Visually Rich Documents (VRDs) are essential in academia, finance, medical
fields, and marketing due to their multimodal information content. Traditional
methods for extracting information from VRDs depend on expert knowledge and
manual labor, making them costly and inefficient. The advent of deep learning
has revolutionized this process, introducing models that leverage multimodal
information vision, text, and layout along with pretraining tasks to develop
comprehensive document representations. These models have achieved
state-of-the-art performance across various downstream tasks, significantly
enhancing the efficiency and accuracy of information extraction from VRDs. In
response to the growing demands and rapid developments in Visually Rich
Document Understanding (VRDU), this paper provides a comprehensive review of
deep learning-based VRDU frameworks. We systematically survey and analyze
existing methods and benchmark datasets, categorizing them based on adopted
strategies and downstream tasks. Furthermore, we compare different techniques
used in VRDU models, focusing on feature representation and fusion, model
architecture, and pretraining methods, while highlighting their strengths,
limitations, and appropriate scenarios. Finally, we identify emerging trends
and challenges in VRDU, offering insights into future research directions and
practical applications. This survey aims to provide a thorough understanding of
VRDU advancements, benefiting both academic and industrial sectors.