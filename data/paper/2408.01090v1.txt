General-purpose Dataflow Model with Neuromorphic Primitives
Weihao Zhang
zwh18@mails.tsinghua.edu.cn
Center for Brain-Inspired Computing
Research (CBICR), Tsinghua
University
Beijing, ChinaYu Du
duyu20@mails.tsinghua.edu.cn
Center for Brain-Inspired Computing
Research (CBICR), Tsinghua
University
Beijing, ChinaHongyi Li
hy-li21@mails.tsinghua.edu.cn
Center for Brain-Inspired Computing
Research (CBICR), Tsinghua
University
Beijing, China
Songchen Ma
msc19@mails.tsinghua.edu.cn
Center for Brain-Inspired Computing
Research (CBICR), Tsinghua
University
Beijing, ChinaRong Zhaoâˆ—
r_zhao@tsinghua.edu.cn
Center for Brain-Inspired Computing
Research (CBICR), Tsinghua
University
Beijing, China
ABSTRACT
Neuromorphic computing exhibits great potential to provide high-
performance benefits in various applications beyond neural net-
works. However, a general-purpose program execution model that
aligns with the features of neuromorphic computing is required
to bridge the gap between program versatility and neuromorphic
hardware efficiency. The dataflow model offers a potential solu-
tion, but it faces high graph complexity and incompatibility with
neuromorphic hardware when dealing with control flow programs,
which decreases the programmability and performance. Here, we
present a dataflow model tailored for neuromorphic hardware,
called neuromorphic dataflow, which provides a compact, concise,
and neuromorphic-compatible program representation for control
logic. The neuromorphic dataflow introduces "when" and "where"
primitives, which restructure the view of control. The neuromor-
phic dataflow embeds these primitives in the dataflow schema with
the plasticity inherited from the spiking algorithms. Our method
enables the deployment of general-purpose programs on neuro-
morphic hardware with both programmability and plasticity, while
fully utilizing the hardwareâ€™s potential.
CCS CONCEPTS
â€¢Computing methodologies â†’Parallel computing method-
ologies ;â€¢Theory of computation â†’Models of computation .
KEYWORDS
neuromorphic computing, dataflow, many-core architecture, spik-
ing neural network, control flow approximation
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICONS, 2023, Santa Fe, New Mexico, USA
Â©2024 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnnACM Reference Format:
Weihao Zhang, Yu Du, Hongyi Li, Songchen Ma, and Rong Zhao. 2024.
General-purpose Dataflow Model with Neuromorphic Primitives. In Proceed-
ings of International Conference on Neuromorphic Systems (Poster) (ICONS).
ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
In recent years, Neuromorphic computing (NC) has attracted much
attention as an alternative computing to von Neumann architecture
in computing[ 1]. Inspired by the biological brain, NC demonstrates
the advantages of ultra-low latency and high energy efficiency in
both neuroscience-oriented and intelligent-oriented applications
due to its in-situ computing, event-driven processing patterns, and
direct utilization of the physical circuitâ€™s functionalities. NC chips
typically adopt a many-core architecture that incorporates digi-
tal or analog crossbars co-located storage for massively parallel
processing. Network-on-chip with routers is used to connect these
processing units[ 21]. Till now, NC has mostly been used for domain-

processing. Network-on-chip with routers is used to connect these
processing units[ 21]. Till now, NC has mostly been used for domain-
specific applications. However, there is now an aspiration to extend
its high efficiency to more versatile applications.
Efforts from various perspectives are underway to achieve this
goal. Theoretically, the NC has been proven to be Turing Com-
plete with corresponding computational model[ 9]. In practice, some
works have used NC infrastructures to support non-neural net-
work applications[ 2], such as solving partial differential equations
through random walking[ 4] and addressing traditional NP-Hard
problems[ 5]. Some neuromorphic chips have also explored the
mutual scheduling mechanism between neuromorphic execution
activities, replacing central processing units (CPU) to enhance
flexibility[ 8], replacing the scheduling responsibility of the CPU. Ad-
ditionally, programming frameworks and compilers have been de-
veloped to provide higher-level abstractions for hardware-agnostic
programming with portability[6, 7].
To support the increasing demand for applications and advance-
ments in theories, a neuromorphic program execution model that
can unify the representation of general programs with hardware
execution abstraction is highly needed. Several mature models
have been proposed for domain-specific NC, such as Corelet for
TrueNorth[ 19] and Rivulet for Tianjic[ 8]. In terms of general-purposearXiv:2408.01090v1  [cs.CL]  2 Aug 2024

ICONS, 2023, Santa Fe, New Mexico, USA Weihao Zhang, Yu Du, Hongyi Li, Songchen Ma, Rong Zhao.
Operator
Gate
Decider
Data Token
Control Token
y = x
v = h(x)
while p(w, v)
if q(v)
v = f(v)
y = g(y)
z = yMerge
T               FT             F
fqT FTh
T             Fw
g
pTT             F
F
zTvx
y
AB
Figure 1: A basic demonstration of conventional dataflow. (A)
A program segment written in Algol-like syntax designed
for von-Neumann architecture. (B) The equivalent dataflow
representation of program A. This example is taken from
[12]
models, there are efforts to utilize the general expressivity or approx-
imation capability of spiking neurons to either construct or approxi-
mate general programs, such as neural engineering framework[ 20],
Fugu[ 6], and neuromorphic completeness representation[ 10]. How-
ever, these approaches require an extensive number of neurons to
construct even a single control operation, and the high-precision
low-redundancy approximation for general control is still chal-
lenging to achieve general approximation. On the other hand, the
dataflow model[ 12], which is Turing complete and shares similar-
ities with high-level features of NC, has been explored for brain-
inspired representation[ 13]. However, the existing dataflow model
has a complex representation for control logic and is incompatible
with most NC chips. Moreover, the conventional dataflow has less
plasticity than neuromorphic models, limiting its learning ability.
To this extent, the main contributions of this paper are:
1)We analyze the control logic of von-Neumann programs
from a neuromorphic perspective and devise the "where"
and "when" primitives.
2)We propose a concise, neuromorphic compatible, programmable,
and learnable neuromorphic dataflow model for general pro-
grams with control flows.
2 CONVENTIONAL DATAFLOW MODEL
A dataflow model is a directed graph in which vertices are called
actors and edges are called arcs. The execution of an actorâ€™s op-
eration is initiated by an event known as a token. Arcs transfer
tokens between actors, where input tokens are consumed by an
actor to generate output tokens based on firing rules. Typically, the
dataflow model contains data tokens that denote arbitrary values,
and control tokens that represent true/false values. A basic example
of the dataflow model is presented in Fig. 1, which include operators
representing integrated functions with one or multiple input data
y = x
v = h(x)
while p(w, v)
if q(v)
v = f(v)
y = g(y)
z = yh
gp
fqOperator
Where Primitive
When PrimitiveData Token
Switch Token
ABFigure 2: A basic demonstration of neuromorphic dataflow.
(A) Same program in Fig. 1 (A). (B) The equivalent neuromor-
phic dataflow model of program A.
arcs, and fire output data tokens when all input tokens are avail-
able. Deciders, representing predicate logic, require one or multiple
data tokens as input and fire a control token when all inputs are
available. The true/false gates permit data tokens to pass through
when receiving a true/false control token, while the merges let the
data token pass through the true side when receiving a true control
token, and vice versa.
In the conventional dataflow model, the presence of gates, merges,
and arcs that carry control tokens significantly increases the com-
plexity of the graph with control logic. In Fig. 1, for example, there
are 10 gates or merges for just one "while" logic and one "if" logic.
These fine-grained and irregular operators in the dataflow model
render it unsuitable for neuromorphic hardware and may cause
a mismatch between the dataflow parallelism and fixed hardware
parallelism.
3 NEUROMORPHIC DATAFLOW MODEL
To address the above issues, we design a neuromorphic dataflow
model (NDF). Classical programs utilize control logic such as condi-
tions, loops, or gotos to manipulate programs based on the program
counter (PC). In contrast, the dataflow model utilizes tokens to trig-
ger operations via an event-driven mechanism. Control is realized

counter (PC). In contrast, the dataflow model utilizes tokens to trig-
ger operations via an event-driven mechanism. Control is realized
in a predicate-decision decoupled framework with control tokens.
The objective of NDF is to further abstract the concept of "con-
trol" from a neuromorphic perspective. Drawing on the dataflow
model and neuroscience, such as the gating mechanism in neuronal
circuits[ 18], the control logic in NDF involves two aspects: where
tokens are directed and when tokens are generated. Following this
philosophy, we designed where and when primitives in NDF.
3.1 Dataflow Model with Where Primitive
The where primitive replaces gates and merges to create a concise
dataflow. It can be viewed as a multi-switch as shown in Fig. 3. The
where primitive hasğ‘šğ‘¤â„ğ‘’ğ‘Ÿğ‘’,ğ‘šğ‘¤â„ğ‘’ğ‘Ÿğ‘’ >=1data token inputs and
ğ‘›ğ‘¤â„ğ‘’ğ‘Ÿğ‘’,ğ‘›ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ >=1data token outputs, and redirects input data
tokens to output with a specific connection pattern. A static where
primitive has fixed connectivity, whereas a dynamic where primitive

General-purpose Dataflow Model with Neuromorphic Primitives ICONS, 2023, Santa Fe, New Mexico, USA
ğ‘‹0ğ‘‹1ğ‘‹2ğ‘‹3
(ğ‘‹0,ğ‘‹2,ğ‘‹3) (ğ‘‹2) (ğ‘‹1)001
100
011
001Switch
TokenInputs
OutputsConnection
Figure 3: A dynamic where primitive takes switch token as
input to change its connections of data in/out arcs.
relies on another input switch token. The switch token is a specially
designed token that determines the connections between inputs and
outputs and is equivalent to a ğ‘›ğ‘¤â„ğ‘’ğ‘Ÿğ‘’Ã—ğ‘šğ‘¤â„ğ‘’ğ‘Ÿğ‘’ adjacency matrix.
Each firing of the dynamic where primitive will consume a switch
token and transfer input data tokens to corresponding output arcs.
With where primitives , the complexity of the original dataflow is
greatly reduced. Shown in Fig. 2, the NDF for the same program
in Fig. 1 only has 8 actors (ignoring the two-to-one data link),
including one static where primitive with a constant three-to-one
connection and two dynamic where primitives that controlled by
thewhen primitives .
3.2 Dataflow Model with When Primitive
Region I Region IIRegion
IIIRegion 
IV
Membrane Potential SpaceExtended Threshold
Region I Region IIRegion III Region IV10
0111
01
01
1010
10
Up to Four 
Connection Pattens Fire With
When Primitiveğ‘£ğ‘¡+12
ğ‘£ğ‘¡2Input
Output
Figure 4: The firing rule of when primitive with two-
dimensional membrane potential and four separate regions.
The when primitive generates switch tokens to determine the
connectivity of the dynamic where primitive . A spiking neuron with
temporal richness is adopted to replace the original decider. For
instance, the predicate 3ğ‘¥âˆ’2<ğ‘¦, i.e. 3ğ‘¥âˆ’ğ‘¦<2can be modeled as
a leaky integrate-and-fire (LIF) neuron that takes two inputs ğ‘¥and
ğ‘¦whose weights are 3andâˆ’1respectively, and a threshold of 2. If
the statement is true, the spiking neuron fires a spike that serves
as a switch token for representing the connection pattern (which
can be stored in advance) under the true situation.
The NDF may have instances where the where primitive has mul-
tiple possible connection patterns. In such cases, the corresponding
when primitive should have an output type beyond just spike and
not spike. Thus, we introduce a modified spiking neuron for when
primitives . The modified spiking neuron has a ğ‘˜-dimensional mem-
brane potential ğ‘£ğ‘˜, inputs vector ğ¼with length ğ‘›ğ‘¤â„ğ‘’ğ‘› , and a weight
matrixğ‘Šwith sizeğ‘›ğ‘¤â„ğ‘’ğ‘›Ã—ğ‘˜. The updated rule for membrane
potential is:
ğ‘£ğ‘˜
ğ‘¡+1=ğ‘“
ğ‘£ğ‘˜
ğ‘¡+ğ¼ğ‘‡ğ‘Š
(1)Here, we introduce a function ğ‘“to increase the non-linearity and
extend the traditional threshold concept to the separation of the
multi-dimensional membrane potential space. Specifically, when
the value of ğ‘£ğ‘˜
ğ‘¡is within a particular region, the neuron fires a cor-
responding token for a particular connection pattern. Accordingly,
thewhen primitive withğ‘šğ‘¤â„ğ‘’ğ‘› separate regions can accommodate
up toğ‘šğ‘¤â„ğ‘’ğ‘› connection patterns of the where primitive . Fig. 4 il-
lustrates such a spiking neuron with a 2-dimensional membrane
potential and four separate regions within the membrane poten-
tial space. The membrane potential shifts among these regions
depending on the input. This type of neuron can be regarded as
a multi-dimensional state machine in continuous space with the
membrane potential updated rule as the state transition equation.
4 HARDWARE COMPATIBILITY OF
NEUROMORPHIC DATAFLOW MODEL
The composability of where primitives .The multi-switch struc-
ture of where primitives makes them compatible with the 2D-mesh
router implementations that are widely adopted by neuromorphic
hardware[ 14]. Alternatively, it can be mapped on a network-on-
chip system with fine-grained functional bio-plausible routing
protocols[ 8,15], allowing for the fusion of multiple adjacent where
primitives into one larger where primitive . This enables the adjust-
ment of the granularity of the where primitives to achieve improved
load-balance on many-core neuromorphic chips, as shown in Fig. 6.
The plasticity of when primitives .When primitives can be
mapped on the soma module (or a modified soma module from co-

load-balance on many-core neuromorphic chips, as shown in Fig. 6.
The plasticity of when primitives .When primitives can be
mapped on the soma module (or a modified soma module from co-
designing), which is primarily responsible for non-linear functions
or dynamic procedures[ 16]. The when primitive has a learning abil-
ity, through STDP[ 22] or BP with surrogate gradient functions[ 17],
to approximate the target functionality. By training the NDF as
a whole with other neuromorphic operators, through the when
primitive , the overall precision can be improved. We demonstrate
this through an experiment.
Theğ‘ ğ‘–ğ‘›andğ‘ğ‘œğ‘ in the target program in Fig. 5 are each approx-
imated by an MLP with 4, 8, and 16 neurons of the hidden layer.
Without the when primitive, the approximation of ğ‘ ğ‘–ğ‘›andğ‘ğ‘œğ‘ are
trained independently. Conversely, the NDF is trained as a whole
with the surrogate gradient of the when primitive. As shown in Fig.
5, using the when primitive can reduce the approximation error of
1.066
0.2790.0341.485
0.283
0.031
00.20.40.60.811.21.41.6
4 8 16Approximation Error
Neuron Numberw/ the when primitive w/o the when primitive
x, y âˆˆ[-10, 10]
If (3x-2 < y)
y = sin(x)
else
y = cos(x)x
cossiny
y
Figure 5: Approximation error of a simple program.

ICONS, 2023, Santa Fe, New Mexico, USA Weihao Zhang, Yu Du, Hongyi Li, Songchen Ma, Rong Zhao.
h
gp
fqh
g fp
qh
g fp + qFuse
Fuse
Axon / Router Chip Core
Soma
Axon/
RouterAdditional MemorySynapse
& DendriteMapComposable Neuromorphic 
Dataflow 
Scalable Neuromorphic 
Hardware A
B
Figure 6: The compatibility between neuromorphic dataflow and neuromorphic hardware. (A) Composable NDF with different
granularity. (B) One of the representative neuromorphic hardware architectures with scalable hierarchy[10].
.
NDF when the number of neurons is small (4 and 8), but little space
is left to reduce the overall error. However, when the number of
neurons is enough to approximate ğ‘ ğ‘–ğ‘›andğ‘ğ‘œğ‘ functions precisely.
5 CONCLUSION
We present a program execution model that combines the dataflow
schema with neuromorphic-compatible primitives, allowing for the
practical programming and deployment of a wide range of applica-
tions on Turing-complete neuromorphic hardware. To achieve this,
we design the where primitive to direct tokens and when primitive
to control when tokens fire. By incorporating these primitives, our
NDF model achieves a compact, concise, and interpretable control-
logic representation. The NDF also exhibits compatibility with neu-
romorphic hardware that has plasticity and multi-grained compos-
ability. Our work introduces a dataflow perspective to practical
general-purpose neuromorphic computing, providing the potential
to combine brain-level efficiency and CPU-level versatility.
ACKNOWLEDGMENTS
This work was partly supported by National Nature Science Foun-
dation of China (nos. 61836004 and 62088102).
REFERENCES
[1]K. Roy, A. Jaiswal, and P. Panda, â€œTowards spike-based machine intelligence with
neuromorphic computing,â€ Nature , vol. 575, no. 7784, pp. 607â€“617, 2019.
[2]J. Aimone, P. Date, G. Fonseca-Guerra, K. Hamilton, K. Henke, B. Kay, G. Kenyon,
S. Kulkarni, S. Mniszewski, M. Parsa et al., â€œA review of non-cognitive applications
for neuromorphic computing,â€ Neuromorphic Computing and Engineering , 2022.
[3]R. AraÃºjo, N. Waniek, and J. Conradt, â€œDevelopment of a dynamically extendable
spinnaker chip computing module,â€ in Artificial Neural Networks and Machine
Learningâ€“ICANN 2014: 24th International Conference on Artificial Neural Networks,
Hamburg, Germany, September 15-19, 2014. Proceedings 24 . Springer, 2014, pp.
821â€“828.
[4]J. D. Smith, A. J. Hill, L. E. Reeder, B. C. Franke, R. B. Lehoucq, O. Parekh, W. Severa,
and J. B. Aimone, â€œNeuromorphic scaling advantages for energy-efficient random
walk computations,â€ Nature Electronics , vol. 5, no. 2, pp. 102â€“112, 2022.
[5]M. Davies, A. Wild, G. Orchard, Y. Sandamirskaya, G. A. F. Guerra, P. Joshi,
P. Plank, and S. R. Risbud, â€œAdvancing neuromorphic computing with loihi: Asurvey of results and outlook, â€ Proceedings of the IEEE , vol. 109, no. 5, pp. 911â€“934,
2021.
[6]J. B. Aimone, W. Severa, and C. M. Vineyard, â€œComposing neural algorithms with
fugu,â€ in Proceedings of the International Conference on Neuromorphic Systems ,
2019, pp. 1â€“8.
[7]Intel, â€œA software framework for neuromorphic computing, â€ https://lava-nc.org/,
2021.
[8]S. Ma, J. Pei, W. Zhang, G. Wang, D. Feng, F. Yu, C. Song, H. Qu, C. Ma, M. Lu
et al. , â€œNeuromorphic computing chip with spatiotemporal elasticity for multi-
intelligent-tasking robots,â€ Science Robotics , vol. 7, no. 67, p. eabk2948, 2022.
[9] P. Date, T. Potok, C. Schuman, and B. Kay, â€œNeuromorphic computing is turing-
complete, â€ in Proceedings of the International Conference on Neuromorphic Systems
2022, 2022, pp. 1â€“10.
[10] Y. Zhang, P. Qu, Y. Ji, W. Zhang, G. Gao, G. Wang, S. Song, G. Li, W. Chen,
W. Zheng et al. , â€œA system hierarchy for brain-inspired computing,â€ Nature , vol.
586, no. 7829, pp. 378â€“384, 2020.
[11] H. Esmaeilzadeh, A. Sampson, L. Ceze, and D. Burger, â€œNeural acceleration for
general-purpose approximate programs,â€ in 2012 45th annual IEEE/ACM interna-
tional symposium on microarchitecture . IEEE, 2012, pp. 449â€“460.

general-purpose approximate programs,â€ in 2012 45th annual IEEE/ACM interna-
tional symposium on microarchitecture . IEEE, 2012, pp. 449â€“460.
[12] J. B. Dennis, J. B. Fosseen, and J. P. Linderman, â€œData flow schemas,â€ in Interna-
tional Symposium on Theoretical Programming . Springer, 1974, pp. 187â€“216.
[13] P. Qu, J. Yan, Y.-H. Zhang, and G. R. Gao, â€œParallel turing machine, a proposal,â€
Journal of Computer Science and Technology , vol. 32, pp. 269â€“285, 2017.
[14] Y. Ji, Y. Zhang, X. Xie, S. Li, P. Wang, X. Hu, Y. Zhang, and Y. Xie, â€œFpsa: A full
system stack solution for reconfigurable reram-based nn accelerator architecture,â€
inProceedings of the Twenty-Fourth International Conference on Architectural
Support for Programming Languages and Operating Systems , 2019, pp. 733â€“747.
[15] S. B. Furber, F. Galluppi, S. Temple, and L. A. Plana, â€œThe spinnaker project,â€
Proceedings of the IEEE , vol. 102, no. 5, pp. 652â€“665, 2014.
[16] G. Indiveri, B. Linares-Barranco, T. J. Hamilton, A. v. Schaik, R. Etienne-
Cummings, T. Delbruck, S.-C. Liu, P. Dudek, P. HÃ¤fliger, S. Renaud et al. , â€œNeuro-
morphic silicon neuron circuits,â€ Frontiers in neuroscience , vol. 5, p. 73, 2011.
[17] E. O. Neftci, H. Mostafa, and F. Zenke, â€œSurrogate gradient learning in spiking
neural networks: Bringing the power of gradient-based optimization to spiking
neural networks,â€ IEEE Signal Processing Magazine , vol. 36, no. 6, pp. 51â€“63, 2019.
[18] L. Luo, â€œArchitectures of neuronal circuits,â€ Science , vol. 373, no. 6559, p. eabg7285,
2021.
[19] A. Amir, P. Datta, W. P. Risk, A. S. Cassidy, J. A. Kusnitz, S. K. Esser, A. An-
dreopoulos, T. M. Wong, M. Flickner, R. Alvarez-Icaza et al. , â€œCognitive com-
puting programming paradigm: a corelet language for composing networks
of neurosynaptic cores,â€ in The 2013 International Joint Conference on Neural
Networks (IJCNN) . IEEE, 2013, pp. 1â€“10.
[20] C. Eliasmith and C. H. Anderson, Neural engineering: Computation, representation,
and dynamics in neurobiological systems . MIT press, 2003.
[21] G. Li, L. Deng, H. Tang, G. Pan, Y. Tian, K. Roy, and W. Maass, â€œBrain inspired
computing: A systematic survey and future trends,â€ 2023.
[22] S. Song, K. D. Miller, and L. F. Abbott, â€œCompetitive hebbian learning through
spike-timing-dependent synaptic plasticity,â€ Nature neuroscience , vol. 3, no. 9, pp.

General-purpose Dataflow Model with Neuromorphic Primitives ICONS, 2023, Santa Fe, New Mexico, USA
919â€“926, 2000.

