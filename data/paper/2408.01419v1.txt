DEBATE QA: Evaluating Question Answering on Debatable Knowledge
Rongwu Xu1∗, Xuan Qi1∗,
Zehan Qi1, Wei Xu1, Zhijiang Guo2
1Tsinghua Universty,2University of Cambridge
Emails: {xrw22, qi-x22@}@mails.tsinghua.edu.cn
∗Equal contribution
Abstract
The rise of large language models (LLMs)
has enabled us to seek answers to inher-
ently debatable questions on LLM chat-
bots, necessitating a reliable way to evalu-
ate their ability. However, traditional QA
benchmarks assume fixed answers are inad-
equate for this purpose. To address this, we
introduce D EBATE QA, a dataset of 2,941
debatable questions, each accompanied by
multiple human-annotated partial answers
that capture a variety of perspectives. We
develop two metrics: Perspective Diver-
sity, which evaluates the comprehensive-
ness of perspectives, and Dispute Aware-
ness, which assesses if the LLM acknowl-
edges the question’s debatable nature. Ex-
periments demonstrate that both metrics are
aligned with human preferences and stable
across different underlying models. Using
DEBATE QA with two metrics, we assess
12 popular LLMs and retrieval-augmented
generation methods. Our findings reveal
that while LLMs generally excel at recog-
nizing debatable issues, their ability to pro-
vide comprehensive answers encompassing
diverse perspectives varies considerably.
https://github.com/pillowsofwind/
DebateQA
1 Introduction
How often do you query a chatbot about a debat-
able issue? Questions like “Does Donald Trump
have a terrible character?” or “How do crop cir-
cles form?” frequently arise in everyday life, re-
flecting human beings’ natural curiosity about top-
ics that inherently lack fixed answers (Lowry and
Johnson, 1981; Brady, 2009). With the advent
of large language models (LLMs; OpenAI 2023b;
Chowdhery et al. 2023; Touvron et al. 2023), we
now turn to these models to seek “ proper ” answers
to such questions. Evaluating the performance oflanguage models on debatable queries is crucial
for understanding and enhancing their capabilities.
Inherent difficulties hinder our way of assess-
ing these models. Traditional question-answering
(QA) benchmarks are typically designed to pro-
vide fixed answers to questions, as in datasets like
SQuAD (Rajpurkar et al., 2016) and Natural Ques-
tions (Kwiatkowski et al., 2019). Even in scenar-
ios where multiple legitimate answers are possi-
ble, such as TruthfulQA (Lin et al., 2022), the an-
swers are often presented as multiple-choice ques-
tions, thus limiting the space of responses. Re-
cent years have seen the emergence of long-form
QA evaluations, such as ELI5 (Fan et al., 2019)
and ASQA (Stelmakh et al., 2022a), which al-
low for more elaborate answers. However, these
works have not focused on inherently debatable
questions. The most related work to us is DEL-
PHI (Sun et al., 2023), which curates a dataset
with controversial questions, however, their em-
phasis is on identifying controversy rather than
delving deeper into evaluating models’ responses.
What constitutes a proper answer when asked
about debatable knowledge? Drawing inspira-
tion from interdisciplinary literature, we propose
two desirable properties for such answers. Firstly,
the answer should inform the receiver that the
issue at hand is debatable (Misco, 2011). Sec-
ondly, a proper answer should be comprehensive
and include diverse perspectives, which are cru-
cial for maintaining an atmosphere of neutrality,
especially in public-related inquiries (Habermas,
1991). Motivated by these considerations, we in-
troduce the DEBATE QAdataset, which comprises
2,941 manually annotated debatable questions. To
rigorously evaluate the aforementioned properties
of model answers, each question is paired with
several partial answers , each reflecting a single
viewpoint on the debatable issue. These partial an-
swers are generated through a three-stage pipeline
and then annotated by three annotators. Based onarXiv:2408.01419v1  [cs.CL]  2 Aug 2024

the partial answers in D EBATE QA, we propose the
metric of Perspective Diversity (P.D.) , which as-
sesses the comprehensiveness in terms of grasp-
ing multiple points-of-view in the model answer.
Using D EBATE QA along with P.D., one can re-
liably and efficiently evaluate the comprehensive-
ness of responses to debatable questions. We also
propose the metric of Dispute Awareness (D.A.) ,
which targets to identify whether the model ac-
knowledges the debatable nature of the question
in its response. Importantly, we show that the P.D.
and D.A. metrics align closely with human judg-
ments and are fairly stable across backbone evalu-
ator models, validating their effectiveness.
We apply D EBATE QA to evaluate 12 popular
LLMs, as well as retrieval-augmented generation
(RAG) approaches. We observe while LLMs gen-
erally excel at identifying the existence of debate,
their performance varies in providing comprehen-
sive answers with diverse perspectives. Notably,
top-notch open-source LLMs rival or even sur-
pass some leading commercial models. Besides,
RAG methods, though not uniformly beneficial
to this task, improve closed-source model perfor-
mance, likely due to better contextual leverage.
Additionally, optimizing sampling hyperparame-
ters and leveraging task-specific prompts can fur-
ther boost performance. On the whole, our find-
ings underscore the need for further refining LLMs
to better interact with debatable knowledge.
2 Related Work
2.1 QA with Non-fixed Answers
Many efforts focus on QA for which there is
no single fixed answer. A MBIG QA (Min et al.,
2020) addresses ambiguous question answering by
rewriting questions and generating multiple plau-
sible answers. S UBJQA (Bjerva et al., 2020)
focuses on identifying subjectivity in questions
and answers within customer reviews. Disen-
tQA (Neeman et al., 2023) proposes to provide
disentangled answers to questions where the pro-
vided context contradicts the model’s knowledge.
DEBATE QA differs logically from these efforts
because the space of plausible answers can not be
narrowed by rewriting or restricting the questions.
To the best of our knowledge, DELPHI (Sun et al.,
2023) is the first study on QA for debatable is-
sues. However, DELPHI has limitations: (1) it
does not provide answers for evaluation, and (2)
it shallowly evaluates model performance usingexploratory metrics. Our work represents a step
forward from DELPHI by offering a comprehen-
sive evaluation solution. We expand and refine the
dataset and introduce more meaningful metrics.
2.2 NLP on Debatable Issues
Beyond QA, multiple lines of NLP research inves-
tigate debatable issues. One notable effort is the
AI Debater, beginning with Slonim et al. (2021)’s
“IBM’s Project Debater” (Bar-Haim et al., 2019),
the first AI system designed to engage humans in
meaningful debates. Another line of research fo-
cuses on controversy detection. Researchers have
identified controversy in news (Choi et al., 2010),
online forums (Chen et al., 2023), and other media
by analyzing sentiments (Choi et al., 2010; Chen
et al., 2023) or sociological features like upvote
percentages (Hessel and Lee, 2019). Moreover,
Wan et al. (2024) investigate LLMs’ preference for
conflicting evidence when facing controversial is-
sues. We distinguish ourselves from this body of
research by primarily focusing on handling debat-
able issues in the field of QA, specifically targeting
the evaluation of chatbot-like NLP systems.
2.3 Long-form QA Evaluation
Evaluating debatable QA falls into the topic of
long-form QA evaluation. Long-form text evalua-
tion can be categorized into two main approaches:
reference-based and reference-free evaluation (Xu
et al., 2023). Reference-based methods require
gold answers and evaluate the generated text by
assessing its similarity with the gold answers (Fan
et al., 2019; Yuan et al., 2021; Chiang and Lee,
2023). Conversely, reference-free evaluation elim-
inates the necessity for a gold standard. Some

assessing its similarity with the gold answers (Fan
et al., 2019; Yuan et al., 2021; Chiang and Lee,
2023). Conversely, reference-free evaluation elim-
inates the necessity for a gold standard. Some
studies assess the coherence and relevance of the
generation concerning specified questions (Fabbri
et al., 2022; Krishna et al., 2022; Xu et al., 2023).
Some studies use a QA-based approach to assess
the quality of the generation (Tan et al., 2024). In
particular, there is another line of literature focus-
ing on examining the veracity of long-form gener-
ation by utilizing external knowledge bases (Stel-
makh et al., 2022b; Min et al., 2023; Wei et al.,
2024). However, none of the aforementioned stud-
ies address the evaluation of debatable questions.
3 Curating the D EBATE QA Dataset
Dataset overview. DEBATE QA is designed to
help assessing language models’ answers to de-

Field Content
Question Does birth order influence personality traits?
Partial Answer 1POV Birth order does not have a meaningful and lasting effect on broad Big Five personality traits.
Explan The influence of birth order on personality traits has been a topic of interest for over a century. However, based on
extensive research combining large datasets from three national panels in the United States, Great Britain, and Germany, it is
evident that birth order does not have a meaningful effect on broad Big Five personality traits . . .
Partial Answer 2POV Firstborns score higher on intelligence and intellect.
Explan Yes, birth order does influence personality traits, particularly in the domain of intelligence and intellect. Research has
consistently shown that firstborns tend to score higher on objectively measured intelligence tests . . .
Partial Answer 3POV No birth-order effects on extraversion, emotional stability, agreeableness, or conscientiousness.
Explan The influence of birth order on personality traits such as extraversion, emotional stability, agreeableness, and consci-
entiousness has been a topic of interest for over a century. However, recent comprehensive studies have provided substantial
evidence that birth order does not significantly impact these personality traits . . .
Table 1: An example from D EBATE QA, details of the explanation fields are truncated for space issues.
batable questions . It contains 2,941 debatable
questions, each paired with a list of partial an-
swers to assist in evaluating model responses.
Each partial answer addresses the question from
a distinct perspective and consists of two parts: a
short point-of-view statement (POV) and a long-
form explanation (Explan) that fully expands the
POV . An entry in D EBATE QA is shown in Table 1.
The overall procedure for curating D EBATE QA
is depicted in Figure 1. We first source debat-
able questions. Then, we apply a three-stage semi-
automated pipeline to collect partial answers.
Lastly, we conduct human annotation on the col-
lected partial answers to finalize the dataset. Qual-
ity examinations happen after each step. The fol-
lowing sections will detail these steps.
3.1 Sourcing Debatable Questions
We collect debatable questions from three distinct
sources. First, we repurpose two existing datasets:
we select 2,281 annotated controversial questions
from DELPHI (Sun et al., 2023) and a full set of
434 questions from C ONFLICTING QA (Wan et al.,
2024). To enrich the existing data, we further man-
ually sourced 1,758 additional debatable questions
from the Web (see Table 8 for detailed sources).
We then run a deduplication algorithm (see § A.1
for details) to remove any duplicate questions, re-
sulting in 3,216 questions. The final composition
of sourced questions is shown in Table 9.
3.2 Collecting Partial Answers
The core novelty of D EBATE QA lies in evaluating
models by comparing the response with multiple
partial answers, rather than a single gold reference.
One partial answer aims to answer the question
from a single perspective. This method reflects the
multifaceted essence of debatable knowledge, ad-vocating for answers that integrate diverse view-
points (Habermas, 1991; Wansink et al., 2023).
To this end, we employ a three-stage pipeline for
collecting them: first, we collect evidence docu-
ments from trustworthy websites; second, we ex-
tract POVs from the evidence w.r.t. the question;
finally, we expand the POVs into long-form expla-
nations based on related evidence. The last two
steps are conducted with the assistance of LLMs1.
Together, the POVs and explanations comprise
what we call partial answers.
3.2.1 Retrieving Trustworthy Documents
We collect partial answers by leveraging online re-
sources and extracting evidence from relevant web
pages. However, the nature of debatable issues ne-
cessitates careful processing of these documents,
as the Web can contain unveracious content. To

sources and extracting evidence from relevant web
pages. However, the nature of debatable issues ne-
cessitates careful processing of these documents,
as the Web can contain unveracious content. To
ensure the reliability of our partial answers, we
source documents from authoritative top-level do-
mains (TLDs), as listed in Table 10. This treat-
ment helps in maintaining the reliability of the
sources. We discard questions that have fewer
than three documents, resulting in 2,982 ques-
tions, each supported by 3-5 of the most relevant
documents. See § A.2 for detailed measures.
Quality examination. To assess the quality of
retrieved documents, we analyze the relevancy
between questions and corresponding documents.
We calculate the cosine similarity between docu-
ment chunks and questions. As depicted in Fig-
ure 11, the average cosine similarity for document
trunks is 0.56 and there are no significant out-
liers, indicating high relevance and minimal noise
in the documents, confirming their overall quality
for serving as the basis for upcoming steps.
1We select OpenAI GPT-4 (OpenAI, 2023b) to assist in
collecting partial answers (the gpt-4-turbo variant).

SourceDebatableQuestionsRetrieveEvidenceDocumentsExtract Point-of-View (POV)Expand POV based on Relevant DocsHuman AnnotationofPartialAnswers
ExistingDatasets* 2DELPHIConflictingQAManuallySourcedfromtheWeb
DebatableProblems
Q: How do crop circles form?Web  Retriever
Top DocsTrusted TLDs:-.edu-.org-.gov-…
LLMDoc1:[xxx.umsystem.edu] Title: Crop Circles ExplainedCrop circles and their mysterious origins have spawned years of debate …
LLM
Datasetw/ Partial Answers (POVs + Explans)DEBATEQAPOV1:Certaincropcirclesare theresultofnatural phenomenaPOV1:Cropcirclesaresignsleft by extraterrestrial beingsDoc2Doc3RelevantdocsTask1: Is the partial answer grounded by the documents?Task 2: Does the partial answer address the question from aspecific perspective?Explan1: One scientific theory suggests that small whirlwinds or vortices, similar to dust devils, could create the circular patterns by flattening the crops. This idea, known as the Plasma Vortex Theory …
Figure 1: Pipeline for curating D EBATE QA. The three main components of the pipeline are highlighted in different
colors: sourcing debatable questions, collecting partial answers, and human annotation. Primary sources or tools
used at each step are highlighted in bold .
3.2.2 Extracting Points-of-View
The second stage involves extracting diverse
POVs from the retrieved evidence documents. A
POV is a concise statement that reflects the core
perspective in addressing the question. We lever-
age GPT-4 to tackle this task, by applying the
prompt pPOVdescribed in Table 11, which takes
the question and the concatenated documents and
returns a list of diverse POVs along with the cor-
responding document indexes where each specific
POV is originated. The document indexes for each
POV are later used for expanding the POV . To
avoid exceeding the 128K context window limit
of GPT-4, we preprocess the documents by remov-
ing meaningless segments and truncating them to
120K tokens if they exceed this length.
Quality examination. We verify the quality of the
collected POVs on comprehensiveness w.r.t. the
documents and distinctiveness among themselves.
For comprehensiveness, we ensure all valid per-
spectives from retrieved documents are captured,
with 90.4% coverage verified manually. Distinc-
tiveness is assured by removing duplicated POVs
manually. For more details, refer to § A.3.
3.2.3 Expanding POV to Explanations
The last stage involves expanding the extracted
POVs into long-form explanations. Each explana-
tion should stand as an independent answer, elab-
orating on the POV and addressing the question
from that perspective. This expansion must be
anchored to the relevant information presented in
the evidence documents pertaining to the specific
POV being developed. We again leverage GPT-4
on this task, utilizing the prompt pExplan described
in Table 11. This prompt takes three inputs: the
question, the target POV to be expanded, and the
related documents obtained in the previous stage.The LLM is required to leverage only the infor-
mation contained within these relevant documents
to generate the explanation, minimizing the risk
of hallucinations (Zhang et al., 2023). We repeat
this step for all the POVs we have collected. The
pseudocode of the pipeline for collecting partial
answers is deferred to Algorithm 1.
3.3 Human Annotation
We verify the fidelity of the LLM-generated partial
answers through human annotation.
Annotation tasks. To thoroughly evaluate the
quality of D EBATE QA, we design two tasks:
•Task 1 : Ensure that the POV and the explana-
tion generated by the LLM are grounded in the
documents. This task focuses on that the gen-
erated explanations are accurately derived from
trustworthy evidence.
•Task 2 : Ensure that the partial answer can ad-
dress the question from a certain perspective.
This task assesses the utility and relevance of
the partial answers, ensuring that they address
the question effectively.
Results and the final dataset. We recruit three

This task assesses the utility and relevance of
the partial answers, ensuring that they address
the question effectively.
Results and the final dataset. We recruit three
annotators and annotate the full dataset. Inter-
annotator agreement (IAA) is measured using
Fleiss’ Kappa (Fleiss et al., 1981), yielding scores
ofκ= 0.66andκ= 0.60for the two anno-
tation tasks, all indicating substantial agreement.
We remove 767 partial answers deemed substan-
dard by two or more annotators. This suggests
that GPT-4 generates faithful partial answers with
a93.4%accuracy. See § A.4 for details. We
employ BERTopic (Grootendorst, 2022) to model
the domain distribution of D EBATE QA. The re-
sult is shown in Figure 14. To reduce computa-
tional costs for upcoming evaluation, we split D E-

BATE QA into two splits: the test split with 1,000
randomly sampled questions and the devset con-
taining the remaining instances.
4 Evaluation Criteria and Metrics
Evaluation criteria. For debatable questions, the
quest for the best answer is fraught with complex-
ity, as there exist no canonical standards. The
expectations vary: many seek a helpful assistant
that delivers credible information with no reserva-
tion, and some may pose questions merely for self-
affirmation, not to say some model providers pre-
fer a “safe” agent to circumvent controversy. In
DEBATE QA, we aim to balance helpfulness and
harmlessness, with the goal of fostering open di-
alogues. After reviewing interdisciplinary litera-
ture, we identify criteria that are nearly universally
accepted and distill two key properties of what
constitutes a good answer:
•Perspective diversity (helpful ): how well does
the answer deliver informative and credible in-
formation from diverse perspectives?
•Dispute awareness (harmless ): whether the
answer recognizes the existence of debate.
Evaluation metrics. Although the above two cri-
teria resonate with those utilized in DELPHI (Sun
et al., 2023), we distinguish ourselves by formaliz-
ing these criteria into more sophisticated quantifi-
able metrics. Our methodology excels by integrat-
ing the partial answer feature of D EBATE QA and
outperforms DELPHI’s approach by a huge mar-
gin, which will be later elaborated in § 5 and § 6.
Please note that while the following two metrics
both employ backbone LLMs, they may differ ; for
brevity, we refer to both as Meval.
I: Perspective Diversity (P .D.) . To evaluate the
model’s answers against legitimate partial re-
sponses, we assess it using a smaller open-source
LLM by generation, ensuring efficiency and cost-
effectiveness. We apply the following metric:
P.D.:=nX
i=1PPL(PAi|chatTemplate (Concat (A,
“Please restate.” ))),(1)
where PPL (Y|X)is the conditional perplexity:
PPL(Y|X) = exp 
−1
NNX
i=1logP(yi|X, y<i)!
.
(2)
In Equation 1, PAi=concat (POVi,Explani)
denotes the ithpartial answer and Adenotesthe model’s answer to the debatable question.
chatTemplate ()is a chat template for prompting
instruction-tuned LLMs. Simply put, P.D. repre-
sents the aggregate perplexity of generating partial
answers from a model answer. Being derived from
PPL, a lower P.D. signifies better quality, suggest-
ing that the model answer contains larger shares of
the partial answer’s content.
II: Dispute Awareness (D.A.) . To ascertain if
the model’s answer indicates that the addressed
question is debatable, we craft a prompt pD.A.,
as shown in Table 14, and use it to prompt an
instruction-tuned LLM. This metric is binary , in-
dicating awareness or lack thereof.
D.A. :=1ifMeval(pD.A.(q, Ans ))returns “1” ,
0otherwise .
(3)
5 Empirical Examination of P.D.
To assess the reliability of P.D., we empirically ex-
amine its alignment with human preferences and
stability w.r.t. backbone models and prompts.
5.1 P.D. Well-Aligns Human Preferences
Collecting human preferences. We collect 500
model answers from five LLMs to 100 ran-
domly sampled questions in D EBATE QA-test ,
then have three rank the answers pairwise.
Annotator 1 Annotator 2 Annotator 3A1
A2
A3- 0.87 0.91
0.87 - 0.84
0.91 0.84 -
0.00.20.40.60.8
Figure 2: Correlation
among annotators as-
sessed by Spearman’s ρ.Using pairwise prefer-
ences and Spearman’s ρ
correlation (Zar, 2005),
we find strong agree-
ment among three an-
notators with ρ > 0.8,
as shown in Figure 2.
This consensus allows
us to assess the alignment of evaluator judgments
with human preferences effectively. More details
are described in § B.1.1.
Compared baselines. We compare P.D. with no-
table text evaluation baselines. Many of these re-
quire a language model as backbones:
•Prompt-based metrics : These baselines always
require an advanced instruction-tuned LLM,
we consider three of them: (1) Direct-Score:
using an LLM to assign a Likert scale score (1-

•Prompt-based metrics : These baselines always
require an advanced instruction-tuned LLM,
we consider three of them: (1) Direct-Score:
using an LLM to assign a Likert scale score (1-
5) using the same instruction we present to hu-
man annotators; (2) G-Eval (Liu et al., 2023):
a prompt-based evaluation framework that as-
sesses the quality of generated texts by incor-
porating chain-of-thoughts (CoT) (Wei et al.,

2022) and a form-filling paradigm; (3) Num-
of-POVs: using an LLM to count the number
of perspectives in the model answers. This met-
ric can be considered an improved metric over
the “Comprehensiveness Answer Rate” metric
introduced in DELPHI (Sun et al., 2023).
•Similarity-based metrics : We pick two tradi-
tional statistical metrics, BLEU (Papineni et al.,
2002) and ROUGE-L (Lin, 2004) and two neu-
ral metrics, BARTScore (Yuan et al., 2021) and
SemScore (Aynetdinov and Akbik, 2024).
Refer to § B.1.2 for details of these baselines.
Results. We apply ELO ratings to establish a hu-
man preference ranking and then measure the cor-
relation with metrics’ rankings using Spearman’s
ρand Kendall’s τ(Kendall, 1938). The average re-
sults of the annotators’ individual correlation are
reported in Table 2. We observe that P.D. sig-
nificantly outperforms DELPHI’s metric and G-
Eval powered by GPT-4o. Using the same small
LLM (Phi-3 mini 128k) as the backbone, P.D. out-
performs G-Eval by a huge margin, underscor-
ing P.D.’s exceptional alignment with human judg-
ment. Overall, P.D. is effective and economical.
We further provide case studies in § B.1.3.
Metric Backbone Model ρ τ
Prompt-based metrics
Direct-Score GPT-4o 0.692 0.671
G-Eval Phi-3 mini 128k -0.003 -0.028
G-Eval GPT-3.5 Turbo 0.593 0.531
G-Eval GPT-4o 0.706 0.634
Num-of-POVs GPT-4o 0.398 0.345
Similarity-based metrics
BLEU - 0.667 0.565
ROUGE-L - 0.537 0.451
BARTScore BART Large (2020a) 0.476 0.397
SemScore all-mpnet-base-v2 (2019) 0.447 0.375
P.D. Phi-3 mini 128k 0.733 0.701
P.D. GPT-2 (117M) 0.825 0.748
P.D. Qwen2 0.5B 0.820 0.742
Table 2: Alignment of various evaluation metrics with
human preferences. The top-performing metric is high-
lighted in bold and the runner-up is underlined .ρ:
Spearman’s ρ,τ: Kendall’s τ.
5.2 P.D. is Stable w.r.t. Backbone Models
To verify the stability of P.D. w.r.t. different back-
bone models, we configure P.D. with five different
LLMs and compute pairwise Kendall’s τamong
the resulting rankings, for the same set of model
responses collected in § 5.1. The results in Fig-
ure 3 show that the rankings by P.D. with different
backbone models are highly consistent.
Qwen2 0.5B Qwen2 7B Phi-3 mini GPT-2 (117M) MiniCPM-2BQwen2 0.5B
Qwen2 7B
Phi-3 mini
GPT-2 (117M)
MiniCPM-2B- 0.95 0.87 0.97 0.86
0.95 - 0.84 0.94 0.84
0.87 0.84 - 0.88 0.89
0.97 0.94 0.88 - 0.86
0.86 0.84 0.89 0.86 -
0.00.20.40.60.8Figure 3: Kendall’s τcorrelations of the P.D. metric
using different backbone models.
5.3 P.D. is Stable w.r.t. Prompts
Remember in Equation 1, there is a prompt
“Please Restate” wrapping the model answer. To
verify the stability of P.D. w.r.t. different prompts,
we configure P.D. with five different prompts
shown in Table 13 and compute pairwise Kendall’s
τamong the resulting rankings, for the same set
of model responses collected in § 5.1. The results
in Figure 4 show that the rankings by P.D. with
different prompts have excellent consistency.
Prompt1 Prompt2 Prompt3 Prompt4 Prompt5P1
P2
P3
P4
P5- 0.984 0.98 0.986 0.978
0.984 - 0.972 0.994 0.978
0.98 0.972 - 0.978 0.986
0.986 0.994 0.978 - 0.98
0.978 0.978 0.986 0.98 -
0.00.20.40.60.8
Figure 4: Kendall’s τcorrelations of the P.D. metric
using different prompts.
6 Empirical Examination of D.A.
We assess D.A.’s reliability by examining its accu-
racy grounded by human judgments and stability
w.r.t. backbone models.
6.1 D.A. is Accurate
To obtain the ground truth of the verdicts of the
500 responses from § 5.1, three authors manually
annotate them by assigning binary labels. The an-
notation has an inter-annotator agreement of 0.79
evaluated by Fleiss’ Kappa. With these manually
Metric Backbone Model Acc F1 AUROC MCC
D.A. Qwen2 1.5B 0.740 0.845 0.530 0.093
D.A. MiniCPM-2B-dpo 0.770 0.857 0.621 0.289
D.A. Llama3 8B 0.770 0.869 0.521 0.179
D.A. Gemma 2 9B 0.740 0.833 0.615 0.246
D.A. Phi-3 medium 0.800 0.868 0.726 0.452
D.A.-ZS Phi-3 medium 0.530 0.434 0.565 0.146
Ack. - 0.220 0.049 0.430 -0.252

D.A. Llama3 8B 0.770 0.869 0.521 0.179
D.A. Gemma 2 9B 0.740 0.833 0.615 0.246
D.A. Phi-3 medium 0.800 0.868 0.726 0.452
D.A.-ZS Phi-3 medium 0.530 0.434 0.565 0.146
Ack. - 0.220 0.049 0.430 -0.252
Table 3: Based on human annotations as the ground
truth, we assess D.A.’s accuracy. D.A.-ZS: D.A. with
a zero-shot prompt pD.A.-ZS , Ack.: the “Acknowledge”
metric from DELPHI.
labeled outcomes as the ground truth, we calculate
the accuracy, F1 score, and AUROC (Hanley and
McNeil, 1982) and Matthews Correlation Coeffi-
cient (MCC; Matthews 1975) for D.A. with dif-

ferent backbone models, the “Acknowledge” met-
ric referenced in DELPHI (refer to § C.1 for de-
tails), and a simplified version of D.A. with a zero-
shot prompt pD.A.-ZS without in-context demon-
strations. The results in Table 3 demonstrate the
superiority of our D.A. metric over the metric from
DELPHI, and the necessity of including demon-
strations in the prompt pD.A.. Upon a case study
for D.A.-ZS, we find that the Phi-3 mini 128k
model occasionally deviates from our instruction
by failing to use 0 or 1 for its judgment, thereby
diminishing its effectiveness.
6.2 D.A. is Stable w.r.t. Backbone Models
Given the robust design of prompt pD.A., ensuring
D.A.’s performance, we advise utilizing the stan-
dard prompt in Table 14. Our focus here is on
confirming D.A.’s stability across various back-
bone models. We set D.A. with five instruction-
following LLMs and analyze pairwise agreements
for the decision made between each two LLMs
based on the model responses collected in § 5.1.
The result in Figure 5 demonstrates the consis-
tency of D.A. among different backbone models.
Gemma2-9B Llama3 8B MiniCPM 2B Phi-3 medium Qwen2 1.5BGemma2-9B
Llama3 8B
MiniCPM 2B
Phi-3 medium
Qwen2 1.5B- 0.99 0.86 0.83 0.92
0.99 - 0.87 0.84 0.93
0.86 0.87 - 0.83 0.9
0.83 0.84 0.83 - 0.79
0.92 0.93 0.9 0.79 -
0.00.20.40.60.8
Figure 5: Agreements of the D.A. metric across differ-
ent backbone models.
7 Experiments
7.1 Experimental Setup
We evaluate a wide range of 12 LLMs on D E-
BATE QA-test using P.D. and D.A., including
closed commercial LLMs and open-source ones.
We also assess several RAG approaches.
Evaluated models. We evaluate the follow-
ing LLMs: GPT-4o (OpenAI, 2024b), GPT-4o
mini (OpenAI, 2024a), GPT-3.5 Turbo (OpenAI,
2023a), Claude 3.5 Sonnet (Anthropic, 2024),
Llama3 Instruct 8B/70B (Meta, 2024), Qwen2
0.5B/1.5B/7B (Qwen, 2024a), Phi-3 mini 128k
3.8B and Phi-3 small 128k 7B (Abdin et al., 2024),
and Gemma2 9B (Team, 2024). All models are the
instruction or chat fine-tuned versions.
Evaluators. We select multiple language mod-
els as the backbone for our metrics. For evaluat-
ing P.D., we select Qwen2 0.5B and GPT-2 base
117M (Radford et al., 2019) as Meval. For as-
sessing D.A., a competent LLM with instruction-following ability is a must. We select Phi-3
medium 128k 14B and Qwen2 1.5B. We select
those four models because their performance is
showcased in § 5.1 and § 6.1, respectively.
Generation configuration. In the main experi-
ments, when testing the LLMs, we provide the
questions with a minimalistic QA prompt, as
shown in Table 15, which instructs the LLMs
without any hint that they are debatable. We be-
lieve this approach more accurately reflects the
typical user interaction with chatbots. For all mod-
els, we configure top- p= 0 to enable greedy de-
coding and stock chat templates including Meval.
7.2 Main Results
Main evaluation results can be found in Table 4.
We summarize our key findings as follows:
•Larger models generally outperform small
ones. Generally, larger models perform well
in terms of both P.D. and D.A. metrics. Large
state-of-the-art LLMs such as GPT-4o and
Llama3 70B demonstrate the strongest perfor-
mance, while tiny to small LLMs are almost al-
ways at the bottom. Besides, for LLMs with the
same architecture ( e.g., Qwen 2 0.5B/1.5B/7B),
larger models always outperform smaller ones.
•Gaps between closed and open models are
not clear. With the exception of the superb
performance of GPT-4o, we have observed that
numerous open-source LLMs are outperform-
ing other capable closed commercial LLMs.
Specifically, open-source models like Llama3
70B and Gemma2 9B nearly match the perfor-
mance of GPT-4o in handling debatable ques-
tions. This might indicate that the performance
on handling debatable issues does not demand
as stringent capabilities from the models as
more difficult tasks such as reasoning.
•Deficiencies in delivering comprehensive re-
sponses. We observe significant shortcom-

as stringent capabilities from the models as
more difficult tasks such as reasoning.
•Deficiencies in delivering comprehensive re-
sponses. We observe significant shortcom-
ings in weaker models’ ability to furnish com-
prehensive answers that encompass a vari-
ety of perspectives. For instance, the worst-
performing Qwen2 0.5B’s answers are around
3to9×2worse than GPT-4o in terms of recov-
ering the information in partial answers.
•Models excel in recognizing debate. We find
that even the lowest-performing model, namely
2These values are approximated with the P.D. values w.r.t.
Two different Meval, refer to § F for details.

ModelAvg. Len.
(#tokens)Perspective Diversity (P.D.) Dispute Awareness (D.A.)
Meval=Qwen2 0.5B Meval=GPT-2 Norm.
RankMeval=Phi-3 M. Meval=Qwen2 1.5B Norm.
RankScore↓ Rank Score ↓Rank Score ↑Rank Score ↑ Rank
Closed-Source LLMs
GPT-4o 434 3.07 1 4.03 1 1 0.952 1 0.979 1 1
GPT-4o mini 252 4.09 6 5.88 6 6 0.937 4 0.964 4 4
GPT-3.5 Turbo 141 5.28 10 8.25 10 10 0.904 6 0.947 6 6
Claude 3.5 Sonnet 199 4.63 8 6.96 8 8 0.856 10 0.920 9 10
Open-Source LLMs (Medium to Large)
Llama3 70B 432 3.09 2 4.07 3 2= 0.945 3 0.977 2 2=
Llama3 8B 381 3.51 5 5.02 5 5 0.928 5 0.964 4 5
Qwen2 7B 255 4.18 7 6.10 7 7 0.895 8 0.923 8 8
Phi-3 small 128k 412 3.50 4 4.31 4 4 0.899 7 0.924 7 7
Gemma 2 9B 395 3.12 3 4.04 2 2= 0.947 2 0.967 3 2=
Open-Source LLMs (Tiny to Small)
Qwen2 1.5B 169 5.60 11 8.67 11 11 0.864 9 0.875 10 9
Qwen2 0.5B 72 6.56 12 10.87 12 12 0.792 11 0.836 11 11
Phi-3 mini 128k 218 4.82 9 7.33 9 9 0.716 12 0.794 12 12
Table 4: Main results of P.D. and D.A. for LLMs on D EBATE QA-test . Avg. Len.: average length of the answers,
GPT-2: GPT-2 (117M), Phi-3 M.: Phi-3 medium 128k, Norm. Rank: normalized average rank of different Meval.
The best andworst results of each metric ( w.r.t. a specific Meval) are highlighted.
Qwen2 0.5B, demonstrates a D.A. over 70%
in recognizing the contentiousness of debatable
questions. This indicates that even the perfor-
mance of the worst-performing model is not as
poor as it might seem.
Correlation between the P.D. and D.A. met-
rics. To investigate the correlation between P.D.
and D.A. for the tested models, we plot a vi-
sualization of the results in Figure 6, which
aids in understanding how these two metrics
relate to each other across various models.
0.150 0.175 0.200 0.225 0.250 0.275 0.300 0.325
P.D.1()
0.750.800.850.900.95D.A.()
Figure 6: Correlation
between P.D. and D.A.
for the tested LLMs.The figure indicates a
positive correlation be-
tween P.D. and D.A.,
suggesting that models
with greater perspec-
tive diversity are also
more aware of the de-
bate in question, align-
ing with our expectations.
Integrating the P.D. and D.A. metrics. To
demonstrate the models’ general capability in ad-
dressing debatable questions, we integrate the two
metrics with varying weights. To ensure that the
P.D. scores reflect the same performance favora-
bility as the D.A. scores, we first take the recip-
rocal of the P.D. scores. Then, we leverage z-
normalization3to normalize both the reciprocal
P.D. score and the original D.A. score. The final
3https://en.wikipedia.org/wiki/Standard_score
Figure 7: Rankings of weighted average scores of mod-
els at different ratios. kis the proportional coefficient.
weighted average score (W.A.G.) is calculated as:
W.A.G. =k·Z-norm (P.D.−1)+(1−k)·Z-norm (D.A. ),(4)
where the kis the proportional coefficient and
Z-norm ()refers to z-normalization. We show the
ranking of the weighted average scores for the
models at different kin Figure 7.
7.3 Effect of More Specific Prompts
In our main experiments, we use a simple QA
prompt ( pbasic in Table 17) that does not high-
light the debatable nature of the questions or de-
mand comprehensive answers. To evaluate the
models’ full potential, we further test five LLMs
with more detailed prompts. We employ three sys-
tem prompts— pbasic,pcomprehensive , and pdetailed —
to elicit model responses at varying levels of de-
tail, as shown in Table 17. Using 200 randomly
sampled questions from D EBATE QA-test , we
compare the average P.D. and D.A. scores across
the five selected LLMs. The results for these
prompts are presented in Table 5. We find that

even the relatively simple pcomprehensive prompt sig-
nificantly improved the performance for all five
models. We conclude that more specific prompts,
i.e., inform the model of the debatable nature and
request for detailed responses, can enhance LLMs’
performance in answering debatable questions.
This finding aligns with our expectations and sug-
gests that LLM users can benefit from well-crafted
prompts when seeking answers to contentious is-
sues from LLMs.
ModelP.D. (↓) D.A. ( ↑)
pbpcpdpbpcpd
GPT-4o mini 3.91 2.13 2.09 0.915 0.955 0.97
Claude 3.5 Sonnet 4.63 3.14 2.35 0.865 0.925 0.98
Llama3 8B 3.42 2.58 2.51 0.855 0.935 0.985
Qwen2 7B 4.16 2.78 2.76 0.855 0.915 0.965
Phi-3 mini 128k 4.71 3.40 2.88 0.765 0.925 0.96
Table 5: Effect of various prompts on P.D. scores and
D.A. scores. pb:pbasic,pc:pcomprehensive ,pd:pdetailed .
7.4 Evaluation for RAG Approaches
In this section, we assess the influence of RAG
methods on performance within D EBATE QA.
7.4.1 Effect of RAG Strategy
We examine the effects of two popular RAG
strategies, vanilla RAG (Lewis et al., 2020b) and
ReAct (Yao et al., 2023). In vanilla RAG, we pick
the top-10 most relevant documents from the re-
trieval results via Google Custom Search API4.
ReAct employs an agent-based approach, leverag-
ing Claude 3.5 Sonnet to interleave reasoning with
document retrieval, strategically selecting up to
9 document chunks to improve problem-solving.
Both methods utilize the prompt in Table 16 to as-
semble the question and the retrieved trunks. Re-
fer to § D.1 for details.
We assess the performance of five LLMs by
evaluating their responses to 100 randomly sam-
pled debatable questions from D EBATE QA-test
using two distinct RAG strategies. With the results
detailed in Table 6, we conclude:
•LLMs with RAG do not consistently improve
in answering debatable questions, but closed-
source models see more notable benefits, possi-
bly due to better context utilization despite po-
tential noise in retrieved content chunks.
•Among the two RAG strategies, ReAct consis-
tently outperforms Vanilla RAG, even though it
uses fewer document chunks (9 vs. 10). This
4https://developers.google.com/custom-searchadvantage can be attributed to ReAct’s more
strategic approach to acting based on the pre-
viously retrieved information, which leads to
more precise and relevant documents retrieved.
ModelP.D. (↓ M eval=Qwen2 0.5B )
No RAG Vanilla RAG ReAct
GPT-4o mini 4.02 3.94 3.70
Claude 3.5 Sonnet 4.63 4.12 3.65
Llama3 8B 3.55 4.01 3.99
Qwen2 7B 3.79 5.96 5.29
Phi-3 mini 128k 4.82 7.01 6.86
Table 6: Effect of two RAG strategies on P.D. scores.
7.4.2 Effect of RAG Source Documents
Considering that the performance of RAG is
highly dependent on the quality of the retrieved
documents, we explore whether restricting RAG
to utilize trustworthy documents would yield bet-
ter results. We retrieve only on web pages under
trustworthy TLDs listed in Table 10. The results
in Table 7 demonstrate that RAG on trustworthy
sources leads to better results. This highlights the
significance of source quality in RAG for debat-
able QA, emphasizing that utilizing trustworthy
documents improves LLM response quality in re-
sponding to sensitive topics.
ModelP.D. (↓ M eval=Qwen2 0.5B )
Vanilla RAG RAG w. T. Docs
GPT-4o mini 3.77 3.63
Claude 3.5 Sonnet 3.92 3.54
Llama3 8B 3.78 3.62
Qwen2 7B 5.91 5.57
Phi-3 mini 128k 6.77 6.50
Table 7: Effect of RAG sources on P.D. scores. RAG
w. T. Docs: RAG using trustworthy documents.
7.5 Effect of Decoding Hyperparameters
In the main experiments, we configure all LLMs
to use greedy decoding, which, while straightfor-
ward, can restrict the diversity and creativity of
LLM outputs (Holtzman et al., 2020). To assess
the impact of various decoding hyperparameters
using sampling decoding on models’ performance,
we select a range of five different temperatures
and top- pvalues. The results of P.D. and D.A.
scores are presented in Figure 8 and Figure 9, re-

using sampling decoding on models’ performance,
we select a range of five different temperatures
and top- pvalues. The results of P.D. and D.A.
scores are presented in Figure 8 and Figure 9, re-
spectively. The plots indicate that higher tempera-
ture and top- pvalues generally prompt LLMs to
produce more well-rounded responses to debat-
able questions, enhancing performance on both

metrics. This suggests that sampling configura-
tions that allow for a broader selection of lower-
probability tokens can lead to improved outcomes.
0.00.20.40.60.81.01.2
T emperature0.00.20.40.60.81.0
T op-p3.23.33.43.53.63.73.83.94.0
Score
3.753.803.85
(a) GPT-4o-mini
0.00.20.40.60.81.01.2
T emperature0.00.20.40.60.81.0
T op-p3.23.33.43.53.63.73.83.94.0
Score
3.483.503.523.543.563.58 (b) Llama3 8B
Figure 8: Average P.D. score ( ↓) for answers from dif-
ferent LLMs with corresponding hyperparameters.
0.00.20.40.60.81.01.2
T emperature0.00.20.40.60.81.0
T op-p0.840.860.880.900.920.94
Score
0.9060.9080.9100.9120.9140.916
(a) GPT-4o-mini
0.00.20.40.60.81.01.2
T emperature0.00.20.40.60.81.0
T op-p0.840.860.880.900.920.94
Score
0.860.870.880.89 (b) Llama3 8B
Figure 9: Average D.A. score ( ↑) for answers from dif-
ferent LLMs with corresponding hyperparameters.
7.6 Effect of the Length of Generation
To delve deeper into how response length impacts
the two metrics, we use the prompt “Your answer
must be around {num}tokens.” to regulate LLMs
to respond with a predetermined length. However,
recognizing that the open-source model’s adher-
ence to instructions might be inconsistent, we il-
lustrate the correlation between the actual aver-
age token count in the model’s responses in Fig-
ure 10. We find: (1) models tend to perform better
with longer responses. This is likely due to longer
answers providing more comprehensive informa-
tion, enhancing P.D. scores. Furthermore, when
tasked with longer answers, models are more
prone to acknowledge the debate, which improves
D.A. scores. (2) in the main experiment, GPT-
4o outperforms GPT-4o-mini and Claude 3.5 Son-
net significantly, as shown in Table 4. However,
the performance gap narrows when responses are
constrained to equal lengths. This suggests that
while the knowledge and conversational capabili-
ties of the three models are comparable, GPT-4o’s
propensity for completing longer answers gives it
an edge over the other two, which favor brevity.
7.7 Qualitative Analysis
Despite GPT-4o leading in both metrics, certain
aspects of its performance remain unsatisfactory.
We also examine the performance gaps among less
capable models. Through case studies, our conclu-
sions are: (1) state-of-the-art LLMs still fall short
100 150 200 250 300 350 400 450 500
Answer Length (Average T okens)2.02.53.03.54.04.55.05.5Average P.D. Score()
Claude 3.5 Sonnet
GPT-4o
GPT-4o-mini
Llama3 8B
Qwen2 7B
Phi-3 mini(a) P.D. Scores ( ↓)
100 150 200 250 300 350 400 450 500
Answer Length (Average T okens)0.50.60.70.80.91.0Average D.A. Score()
(b) D.A. Scores ( ↑)
Figure 10: Average P.D. and D.A. scores for answers
in various lengths across different LLMs.
in leveraging referenced and convincing informa-
tion for their arguments, and (2) weaker LLMs
have difficulty providing a comprehensive elabo-
ration of sufficient perspectives and we find the
answer presentation of those LLMs are markedly
poorer. Based on these gaps, LLMs’ ability to
address debatable questions still has room to be
further refined. We point out the following po-
tential solutions for improving models’ ability on
DEBATE QA: (1) Deepen domain-specific knowl-
edge with pretraining or high-quality RAG; (2)
Strengthen source citation by adopting strategies
such as citing memory or retrieved documents for
evidence-rich responses; (3) Improve stylistic pre-
sentation through fine-tuning; and (4) Ensure per-
spective diversity and balance through alignment
training or prompt engineering. Kindly refer to § E
for a detailed analysis and case studies.
8 Conclusion
We develop D EBATE QA, a novel QA dataset with
2,941 debatable questions paired with multiple
human-annotated partial answers to assess lan-
guage models’ ability to answer debatable ques-
tions. We introduce two novel metrics, Perspec-
tive Diversity and Dispute Awareness, to evalu-
ate how well language models reflect multiple hu-
man viewpoints and recognize debate. Extensive

tions. We introduce two novel metrics, Perspec-
tive Diversity and Dispute Awareness, to evalu-
ate how well language models reflect multiple hu-
man viewpoints and recognize debate. Extensive
experiments demonstrate these two metrics align
with human preferences and are stable. Our evalu-
ations with 12 LLMs show that current models are
relatively strong in recognizing debate but varying
in the ability to comprehensively address them.

References
Marah I Abdin, Sam Ade Jacobs, Ammar Ah-
mad Awan, Jyoti Aneja, Ahmed Awadallah,
Hany Awadalla, Nguyen Bach, Amit Bahree,
Arash Bakhtiari, Harkirat S. Behl, Alon Ben-
haim, Misha Bilenko, Johan Bjorck, Sébastien
Bubeck, Martin Cai, Caio César Teodoro
Mendes, Weizhu Chen, Vishrav Chaudhary,
Parul Chopra, Allie Del Giorno, Gustavo
de Rosa, Matthew Dixon, Ronen Eldan, Dan
Iter, Amit Garg, Abhishek Goswami, Suriya
Gunasekar, Emman Haider, Junheng Hao, Rus-
sell J. Hewett, Jamie Huynh, Mojan Javaheripi,
Xin Jin, Piero Kauffmann, Nikos Karampatzi-
akis, Dongwoo Kim, Mahoud Khademi, Lev
Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi
Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi
Lin, Piyush Madan, Arindam Mitra, Hardik
Modi, Anh Nguyen, Brandon Norick, Barun Pa-
tra, Daniel Perez-Becker, Thomas Portet, Reid
Pryzant, Heyang Qin, Marko Radmilac, Corby
Rosset, Sambudha Roy, Olatunji Ruwase, Olli
Saarikivi, Amin Saied, Adil Salim, Michael
Santacroce, Shital Shah, Ning Shang, Hiteshi
Sharma, Xia Song, Masahiro Tanaka, Xin
Wang, Rachel Ward, Guanhua Wang, Philipp
Witte, Michael Wyatt, Can Xu, Jiahang Xu,
Sonali Yadav, Fan Yang, Ziyi Yang, Donghan
Yu, Chengruidong Zhang, Cyril Zhang, Jian-
wen Zhang, Li Lyna Zhang, Yi Zhang, Yue
Zhang, Yunan Zhang, and Xiren Zhou. 2024.
Phi-3 technical report: A highly capable lan-
guage model locally on your phone. CoRR ,
abs/2404.14219.
Anthropic. 2024. Introducing Claude 3.5 Sonnet.
Anthropic website.
Ansar Aynetdinov and Alan Akbik. 2024. Sem-
score: Automated evaluation of instruction-
tuned llms based on semantic textual similarity.
CoRR , abs/2401.17072.
Roy Bar-Haim, Dalia Krieger, Orith Toledo-
Ronen, Lilach Edelstein, Yonatan Bilu, Alon
Halfon, Yoav Katz, Amir Menczel, Ranit
Aharonov, and Noam Slonim. 2019. From sur-
rogacy to adoption; from bitcoin to cryptocur-
rency: Debate topic expansion. In Proceed-
ings of the 57th Conference of the Association
for Computational Linguistics, ACL 2019, Flo-
rence, Italy, July 28- August 2, 2019, Volume 1:Long Papers , pages 977–990. Association for
Computational Linguistics.
Johannes Bjerva, Nikita Bhutani, Behzad Gol-
shan, Wang-Chiew Tan, and Isabelle Augen-
stein. 2020. Subjqa: A dataset for subjectivity
and review comprehension. In Proceedings of
the 2020 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2020,
Online, November 16-20, 2020 , pages 5480–
5494. Association for Computational Linguis-
tics.
Michael Brady. 2009. Curiosity and the value of
truth. Epistemic value , pages 265–283.
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun
Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-
embedding: Multi-lingual, multi-functionality,
multi-granularity text embeddings through self-
knowledge distillation.
Kai Chen, Zihao He, Rong-Ching Chang,
Jonathan May, and Kristina Lerman. 2023.
Anger breeds controversy: Analyzing contro-
versy and emotions on reddit. In Social, Cul-
tural, and Behavioral Modeling - 16th Inter-
national Conference, SBP-BRiMS 2023, Pitts-
burgh, PA, USA, September 20-22, 2023, Pro-
ceedings , volume 14161 of Lecture Notes in
Computer Science , pages 44–53. Springer.
David Cheng-Han Chiang and Hung-yi Lee. 2023.
Can large language models be an alternative to
human evaluations? In Proceedings of the 61st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
ACL 2023, Toronto, Canada, July 9-14, 2023 ,
pages 15607–15631. Association for Computa-
tional Linguistics.
Yoonjung Choi, Yuchul Jung, and Sung-Hyon
Myaeng. 2010. Identifying controversial issues
and their sub-topics in news articles. In Intel-
ligence and Security Informatics, Pacific Asia
Workshop, PAISI 2010, Hyderabad, India, June
21, 2010. Proceedings , volume 6122 of Lec-
ture Notes in Computer Science , pages 140–
153. Springer.
Aakanksha Chowdhery, Sharan Narang, Jacob De-
vlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, Parker

Schuh, Kensen Shi, Sasha Tsvyashchenko,
Joshua Maynez, Abhishek Rao, Parker Barnes,
Yi Tay, Noam Shazeer, Vinodkumar Prab-
hakaran, Emily Reif, Nan Du, Ben Hutchinson,
Reiner Pope, James Bradbury, Jacob Austin,
Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghe-
mawat, Sunipa Dev, Henryk Michalewski,
Xavier Garcia, Vedant Misra, Kevin Robinson,
Liam Fedus, Denny Zhou, Daphne Ippolito,
David Luan, Hyeontaek Lim, Barret Zoph,
Alexander Spiridonov, Ryan Sepassi, David
Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana
Pillai, Marie Pellat, Aitor Lewkowycz, Er-
ica Moreira, Rewon Child, Oleksandr Polo-
zov, Katherine Lee, Zongwei Zhou, Xuezhi
Wang, Brennan Saeta, Mark Diaz, Orhan Fi-
rat, Michele Catasta, Jason Wei, Kathy Meier-
Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2023. Palm: Scaling language
modeling with pathways. J. Mach. Learn. Res. ,
24:240:1–240:113.
Alexander R. Fabbri, Chien-Sheng Wu, Wenhao
Liu, and Caiming Xiong. 2022. Qafacteval: Im-
proved qa-based factual consistency evaluation
for summarization. In Proceedings of the 2022
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, NAACL 2022,
Seattle, WA, United States, July 10-15, 2022 ,
pages 2587–2601. Association for Computa-
tional Linguistics.
Angela Fan, Yacine Jernite, Ethan Perez, David
Grangier, Jason Weston, and Michael Auli.
2019. ELI5: long form question answering. In
Proceedings of the 57th Conference of the As-
sociation for Computational Linguistics, ACL
2019, Florence, Italy, July 28- August 2, 2019,
Volume 1: Long Papers , pages 3558–3567. As-
sociation for Computational Linguistics.
Joseph L Fleiss, Bruce Levin, Myunghee Cho
Paik, et al. 1981. The measurement of inter-
rater agreement. Statistical methods for rates
and proportions , 2(212-236):22–23.
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi
Chen. 2023. Enabling large language models
to generate text with citations. In Proceedings
of the 2023 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2023,
Singapore, December 6-10, 2023 , pages 6465–
6488. Association for Computational Linguis-
tics.
Maarten Grootendorst. 2022. Bertopic: Neural
topic modeling with a class-based TF-IDF pro-
cedure. CoRR , abs/2203.05794.
Jurgen Habermas. 1991. The structural transfor-
mation of the public sphere: An inquiry into a
category of bourgeois society . MIT press.
James A Hanley and Barbara J McNeil. 1982. The
meaning and use of the area under a receiver
operating characteristic (roc) curve. Radiology ,
143(1):29–36.
Jack Hessel and Lillian Lee. 2019. Something’s
brewing! early prediction of controversy-
causing posts from discussion features. In Pro-
ceedings of the 2019 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies , volume 1.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes,
and Yejin Choi. 2020. The curious case of
neural text degeneration. In 8th International
Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30,
2020 . OpenReview.net.
Shengding Hu, Yuge Tu, Xu Han, Chaoqun He,
Ganqu Cui, Xiang Long, Zhi Zheng, Yewei
Fang, Yuxiang Huang, Weilin Zhao, Xinrong
Zhang, Zhen Leng Thai, Kai Zhang, Chongyi
Wang, Yuan Yao, Chenyang Zhao, Jie Zhou,
Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia,
Guoyang Zeng, Dahai Li, Zhiyuan Liu, and
Maosong Sun. 2024. Minicpm: Unveiling the
potential of small language models with scal-
able training strategies. CoRR , abs/2404.06395.
Vladimir Karpukhin, Barlas Oguz, Sewon Min,
Patrick S. H. Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020.
Dense passage retrieval for open-domain ques-
tion answering. In Proceedings of the 2020
Conference on Empirical Methods in Natural
Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pages 6769–6781. As-

tion answering. In Proceedings of the 2020
Conference on Empirical Methods in Natural
Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pages 6769–6781. As-
sociation for Computational Linguistics.

Maurice G Kendall. 1938. A new measure of rank
correlation. Biometrika , 30(1-2):81–93.
Kalpesh Krishna, Yapei Chang, John Wieting, and
Mohit Iyyer. 2022. Rankgen: Improving text
generation with large ranking models. In Pro-
ceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing,
EMNLP 2022, Abu Dhabi, United Arab Emi-
rates, December 7-11, 2022 , pages 199–232.
Association for Computational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia
Redfield, Michael Collins, Ankur P. Parikh,
Chris Alberti, Danielle Epstein, Illia Polo-
sukhin, Jacob Devlin, Kenton Lee, Kristina
Toutanova, Llion Jones, Matthew Kelcey,
Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: a benchmark for question an-
swering research. Trans. Assoc. Comput. Lin-
guistics , 7:452–466.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020a. BART: denoising sequence-to-sequence
pre-training for natural language generation,
translation, and comprehension. In Proceed-
ings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2020,
Online, July 5-10, 2020 , pages 7871–7880. As-
sociation for Computational Linguistics.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-
tau Yih, Tim Rocktäschel, Sebastian Riedel,
and Douwe Kiela. 2020b. Retrieval-augmented
generation for knowledge-intensive NLP tasks.
InAdvances in Neural Information Processing
Systems 33: Annual Conference on Neural In-
formation Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual .
Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun
Long, Pengjun Xie, and Meishan Zhang.
2023. Towards general text embeddings
with multi-stage contrastive learning. CoRR ,
abs/2308.03281.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summa-
rization branches out , pages 74–81.Stephanie Lin, Jacob Hilton, and Owain Evans.
2022. Truthfulqa: Measuring how models
mimic human falsehoods. In Proceedings of
the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), ACL 2022, Dublin, Ireland, May 22-27,
2022 , pages 3214–3252. Association for Com-
putational Linguistics.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023. G-
eval: NLG evaluation using gpt-4 with better
human alignment. In Proceedings of the 2023
Conference on Empirical Methods in Natural
Language Processing, EMNLP 2023, Singa-
pore, December 6-10, 2023 , pages 2511–2522.
Association for Computational Linguistics.
Nancy Lowry and David W Johnson. 1981. Ef-
fects of controversy on epistemic curiosity,
achievement, and attitudes. The Journal of So-
cial Psychology , 115(1):31–43.
Brian W Matthews. 1975. Comparison of the
predicted and observed secondary structure of
t4 phage lysozyme. Biochimica et Biophysica
Acta (BBA)-Protein Structure , 405(2):442–451.
Meta. 2024. Build the future of AI with Meta
Llama 3. Meta AI website.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit
Iyyer, Luke Zettlemoyer, and Hannaneh Ha-
jishirzi. 2023. Factscore: Fine-grained atomic
evaluation of factual precision in long form text
generation. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2023, Singapore,
December 6-10, 2023 , pages 12076–12100. As-
sociation for Computational Linguistics.
Sewon Min, Julian Michael, Hannaneh Hajishirzi,
and Luke Zettlemoyer. 2020. Ambigqa: An-
swering ambiguous open-domain questions. In
Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Process-
ing, EMNLP 2020, Online, November 16-20,
2020 , pages 5783–5797. Association for Com-
putational Linguistics.
Thomas Misco. 2011. Teaching about controver-

ing, EMNLP 2020, Online, November 16-20,
2020 , pages 5783–5797. Association for Com-
putational Linguistics.
Thomas Misco. 2011. Teaching about controver-
sial issues: Rationale, practice, and need for in-
quiry. Int’l J. Educ. L. & Pol’y , 7:13.

Ella Neeman, Roee Aharoni, Or Honovich,
Leshem Choshen, Idan Szpektor, and Omri
Abend. 2023. Disentqa: Disentangling para-
metric and contextual knowledge with coun-
terfactual question answering. In Proceedings
of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), ACL 2023, Toronto, Canada, July 9-
14, 2023 , pages 10056–10070. Association for
Computational Linguistics.
OpenAI. 2023a. Gpt-3.5 turbo. OpenAI Website.
OpenAI. 2023b. GPT-4 technical report. CoRR ,
abs/2303.08774.
OpenAI. 2024a. Gpt-4o mini: advancing cost-
efficient intelligence. OpenAI Website.
OpenAI. 2024b. Hello GPT-4o. OpenAI website.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for au-
tomatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics, July
6-12, 2002, Philadelphia, PA, USA , pages 311–
318. ACL.
Qwen. 2024a. Hello Qwen2. QwenLM Blog.
Qwen. 2024b. Introducing Qwen1.5. QwenLM
Blog.
Alec Radford, Jeffrey Wu, Rewon Child, David
Luan, Dario Amodei, Ilya Sutskever, et al.
2019. Language models are unsupervised mul-
titask learners. OpenAI blog , 1(8):9.
Rafael Rafailov, Archit Sharma, Eric Mitchell,
Christopher D. Manning, Stefano Ermon, and
Chelsea Finn. 2023. Direct preference opti-
mization: Your language model is secretly a
reward model. In Advances in Neural Infor-
mation Processing Systems 36: Annual Confer-
ence on Neural Information Processing Systems
2023, NeurIPS 2023, New Orleans, LA, USA,
December 10 - 16, 2023 .
Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-
rev, and Percy Liang. 2016. Squad: 100, 000+
questions for machine comprehension of text.
InProceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Process-
ing, EMNLP 2016, Austin, Texas, USA, Novem-ber 1-4, 2016 , pages 2383–2392. The Associa-
tion for Computational Linguistics.
Nils Reimers and Iryna Gurevych. 2019.
Sentence-bert: Sentence embeddings using
siamese bert-networks. In Proceedings of
the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th
International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019,
Hong Kong, China, November 3-7, 2019 , pages
3980–3990. Association for Computational
Linguistics.
Noah Shinn, Federico Cassano, Ashwin Gopinath,
Karthik Narasimhan, and Shunyu Yao. 2023.
Reflexion: language agents with verbal rein-
forcement learning. In Advances in Neural In-
formation Processing Systems 36: Annual Con-
ference on Neural Information Processing Sys-
tems 2023, NeurIPS 2023, New Orleans, LA,
USA, December 10 - 16, 2023 .
Noam Slonim, Yonatan Bilu, Carlos Alzate,
Roy Bar-Haim, Ben Bogin, Francesca Bonin,
Leshem Choshen, Edo Cohen-Karlik, Lena
Dankin, Lilach Edelstein, Liat Ein-Dor, Roni
Friedman-Melamed, Assaf Gavron, Ariel Gera,
Martin Gleize, Shai Gretz, Dan Gutfreund,
Alon Halfon, Daniel Hershcovich, Ron Hoory,
Yufang Hou, Shay Hummel, Michal Jacovi,
Charles Jochim, Yoav Kantor, Yoav Katz, David
Konopnicki, Zvi Kons, Lili Kotlerman, Dalia
Krieger, Dan Lahav, Tamar Lavee, Ran Levy,
Naftali Liberman, Yosi Mass, Amir Menczel,
Shachar Mirkin, Guy Moshkowich, Shila Ofek-
Koifman, Matan Orbach, Ella Rabinovich, Ruty
Rinott, Slava Shechtman, Dafna Sheinwald,
Eyal Shnarch, Ilya Shnayderman, Aya Sof-
fer, Artem Spector, Benjamin Sznajder, Assaf
Toledo, Orith Toledo-Ronen, Elad Venezian,
and Ranit Aharonov. 2021. An autonomous de-
bating system. Nat., 591(7850):379–384.
Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and
Ming-Wei Chang. 2022a. ASQA: factoid ques-
tions meet long-form answers. In Proceedings
of the 2022 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2022,
Abu Dhabi, United Arab Emirates, December
7-11, 2022 , pages 8273–8288. Association for
Computational Linguistics.

Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and
Ming-Wei Chang. 2022b. ASQA: factoid ques-
tions meet long-form answers. In Proceedings
of the 2022 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2022,
Abu Dhabi, United Arab Emirates, December
7-11, 2022 , pages 8273–8288. Association for
Computational Linguistics.
David Q. Sun, Artem Abzaliev, Hadas Kotek,
Christopher Klein, Zidi Xiu, and Jason D.
Williams. 2023. DELPHI: data for evaluating
llms’ performance in handling controversial is-
sues. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Pro-
cessing: EMNLP 2023 - Industry Track, Sin-
gapore, December 6-10, 2023 , pages 820–827.
Association for Computational Linguistics.
Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu,
Zhili Liu, Xiaoguang Li, Yasheng Wang, Lifeng
Shang, Qun Liu, and Linqi Song. 2024. PROX-
YQA: an alternative framework for evaluating
long-form text generation with large language
models. CoRR , abs/2401.15042.
Gemma Team. 2024. Gemma 2: Improving open
language models at a practical size. Technical
Report .
Hugo Touvron, Louis Martin, Kevin Stone, Pe-
ter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal
Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher, Cristian Canton-Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu, Jude Fernan-
des, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-
thia Gao, Vedanuj Goswami, Naman Goyal,
Anthony Hartshorn, Saghar Hosseini, Rui
Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel Kloumann,
Artem Korenev, Punit Singh Koura, Marie-
Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton,
Jeremy Reizenstein, Rashi Rungta, Kalyan Sal-
adi, Alan Schelten, Ruan Silva, Eric Michael
Smith, Ranjan Subramanian, Xiaoqing Ellen
Tan, Binh Tang, Ross Taylor, Adina Williams,
Jian Xiang Kuan, Puxin Xu, Zheng Yan,
Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, AurélienRodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. 2023. Llama 2: Open foun-
dation and fine-tuned chat models. CoRR ,
abs/2307.09288.
Lewis Tunstall, Edward Beeching, Nathan Lam-
bert, Nazneen Rajani, Kashif Rasul, Younes
Belkada, Shengyi Huang, Leandro von Werra,
Clémentine Fourrier, Nathan Habib, Nathan
Sarrazin, Omar Sanseviero, Alexander M.
Rush, and Thomas Wolf. 2023. Zephyr: Di-
rect distillation of LM alignment. CoRR ,
abs/2310.16944.
Alexander Wan, Eric Wallace, and Dan Klein.
2024. What evidence do language models find
convincing? CoRR , abs/2402.11782.
Bjorn Gert Jan Wansink, Jacob Timmer, and
Larike Henriette Bronkhorst. 2023. Navigating
multiple perspectives in discussing controver-
sial topics: Boundary crossing in the classroom.
Education Sciences , 13(9):938.
Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, Brian Ichter, Fei Xia, Ed H.
Chi, Quoc V . Le, and Denny Zhou. 2022.
Chain-of-thought prompting elicits reasoning in
large language models. In Advances in Neu-
ral Information Processing Systems 35: Annual
Conference on Neural Information Processing
Systems 2022, NeurIPS 2022, New Orleans, LA,
USA, November 28 - December 9, 2022 .
Jerry Wei, Chengrun Yang, Xinying Song, Yifeng
Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo
Liu, Da Huang, Cosmo Du, and Quoc V . Le.
2024. Long-form factuality in large language
models. CoRR , abs/2403.18802.
Orion Weller, Marc Marone, Nathaniel Weir,
Dawn J. Lawrie, Daniel Khashabi, and Ben-
jamin Van Durme. 2024. "according to . . .
": Prompting language models improves quot-
ing from pre-training data. In Proceedings of
the 18th Conference of the European Chapter of
the Association for Computational Linguistics,
EACL 2024 - Volume 1: Long Papers, St. Ju-
lian’s, Malta, March 17-22, 2024 , pages 2288–
2301. Association for Computational Linguis-
tics.
Erik Olin Wright. 1998. The debate on classes ,
volume 20. Verso.

Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eu-
nsol Choi. 2023. A critical evaluation of eval-
uations for long-form question answering. In
Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), ACL 2023, Toronto,
Canada, July 9-14, 2023 , pages 3225–3245.
Association for Computational Linguistics.
Rongwu Xu, Zehan Qi, Cunxiang Wang, Hon-
gru Wang, Yue Zhang, and Wei Xu. 2024a.
Knowledge conflicts for llms: A survey. arXiv
preprint arXiv:2403.08319 .
Rongwu Xu, Zi’an Zhou, Tianwei Zhang, Ze-
han Qi, Su Yao, Ke Xu, Wei Xu, and Han
Qiu. 2024b. Walking in others’ shoes: How
perspective-taking guides large language mod-
els in reducing toxicity and bias. arXiv preprint
arXiv:2407.15366 .
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du,
Izhak Shafran, Karthik R. Narasimhan, and
Yuan Cao. 2023. React: Synergizing reasoning
and acting in language models. In The Eleventh
International Conference on Learning Repre-
sentations, ICLR 2023, Kigali, Rwanda, May 1-
5, 2023 . OpenReview.net.
Weizhe Yuan, Graham Neubig, and Pengfei Liu.
2021. Bartscore: Evaluating generated text as
text generation. In Advances in Neural Infor-
mation Processing Systems 34: Annual Confer-
ence on Neural Information Processing Systems
2021, NeurIPS 2021, December 6-14, 2021, vir-
tual, pages 27263–27277.
Jerrold H. Zar. 2005. Spearman rank correlation.
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai,
Lemao Liu, Tingchen Fu, Xinting Huang, Enbo
Zhao, Yu Zhang, Yulong Chen, Longyue Wang,
Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming
Shi. 2023. Siren’s song in the AI ocean: A sur-
vey on hallucination in large language models.
CoRR , abs/2309.01219.

A Further Details on Dataset Curation
A.1 Details on Debatable Question
The questions in D EBATE QA are collected from
three sources, as detailed in Table 9.
Manually sourcing debatable questions. We
search for debatable questions from the Web and
adhere to the following criteria:
•A debatable question should have clear seman-
tics without any ambiguity.
•A debatable question should have potentially
different angles of answer, rather than having
one definitive answer.
•A debatable question can be subjective, but its
answer should not be determined solely by in-
dividual subjective preferences.
In the end, we select 1,758 debatable questions
from 9 websites. Details on the number of ques-
tions from each website can be found in Table 8.
Question deduplication. After merging questions
from the three sources, we employ a simple dedu-
plication algorithm to remove duplicates from the
original set of 4,473 questions: First, we calcu-
late cosine similarities between pairwise questions
based on the embeddings computed by gte-large-
en-v1.5 (Li et al., 2023). Subsequently, we sample
500 pairs and find pairs with a similarity score be-
low 0.78 contained no true duplicates. Following
this, we leverage a greedy algorithm for minimum
vertex cover to remove duplicate entries, aiming to
remove as few questions as possible while ensur-
ing no duplicates remain5. This process results in
a final dataset consisting of 3,216 unique entries.
A.2 Details on Retrieving Trustworthy
Documents
Retrieving on trustworthy websites. We only
do retrieval on authoritative domains in Table 10
to assure the trustworthiness of the documents.
Among the selected TLDs, .gov and .edu do-
mains are not open for personal registration and
can only be registered by government or edu-
cational institutions. Although .org ,.pro , and
.info domains can now be registered by individ-
uals, their content generally remains professional
and informative, with fewer advertisements or po-
tentially misleading information.
Implementation of the retrieving process. To
enable finer-grained search results, we apply the
5The problem of vertex cover is NP-hard. We leverage a
greedy algorithm, similar to the one described in this page.GPT-4 model to first transform the original ques-
tion into several search queries. We use the
Google search engine for Web searches and re-
tain only the documents from authoritative TLDs.
These documents are then ranked using Bge-
Reranker-v2-Gemma (Chen et al., 2024) and we
keep the top-5 documents. We filter questions
with fewer than three documents, as we consider
these lack sufficient trustworthy evidence, leaving
us with 2,982 questions. The distribution of the
number of documents per question is in Figure 12.
Quality examination. We segment each doc-
ument into 1000-token chunks and average the
cosine similarities for each question and corre-
sponding trunks, computed by gte-Qwen2-1.5B-
instruct. The quality of the retrieved documents is
illustrated in Figure 11.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Average Document Similarity050100150200Frequency
0.30.40.50.60.70.8Average Document Similarity
Figure 11: Quality of retrieved evidence documents.
Document quality is assessed by cosine similarity.
3.0 3.5 4.0 4.5 5.0
Number of documents020040060080010001200Count
0.20.40.60.81.0
CDF
Figure 12: Distribution of the number of trustworthy
evidence documents per question.
A.3 Details on Generating Partial Answers
Algorithm 1 formalize the pipeline of collect-
ing partial answers, where Mis the LLM we
use.M(p(x, y))indicates the LLM processing a
prompt template p()populated with inputs x, y.
Prompts. The prompts we used to generate
the POVs and explanations can be found in Ta-
ble 11. These prompts are carefully crafted to
ensure that the generated POVs cover a range of
non-overlapping perspectives and provide well-
rounded explanations that are grounded in the ev-
idence documents. After extracting the POVs,

ensure that the generated POVs cover a range of
non-overlapping perspectives and provide well-
rounded explanations that are grounded in the ev-
idence documents. After extracting the POVs,
we filter out questions with fewer than three per-
spectives, ensuring that the remaining questions
are sufficiently debatable, resulting in 2,941 ques-
tions. The distribution of the number of extracted
POVs per question can be found in Figure 13.

Source URL Count
https://owlcation.com/academia/debate-topics 350
https://paperperk.com/blog/debate-topics 174
https://studycorgi.com/blog/debatable-questions-topics-for-research-essays/ 150
https://www.myspeechclass.com/funny-debatable-topics.html 126
https://www.nytimes.com/2020/07/28/learning/177-questions...refection.html 177
https://owlcation.com/academia/100-Debate-Topics 100
https://parade.com/living/debatable-questions 257
https://www.procon.org/debate-topics 107
https://randomquestionmaker.com/blog/debatable-questions 317
Table 8: Detailed sources in URL for debatable questions sourced from the web. During our experiment (April
2024), these resources are accessible. We will make our dataset publicly available for future research.
Sources Initial After Dedupe.
DELPHI 2,281 1,597
CONFLICTING QA 434 400
Web Sourced 1,758 1,219
Total 4,473 3,216
Table 9: Sources distribution of D EBATE QA.
Selected TLDs Open for Registration?
.edu ✗
.org ✓
.gov ✗
.info ✓
.pro ✓
Table 10: List of selected top-level domains (TLDs)
we considered trustworthy. Closed for registration in-
dicates that only authoritative entities can register a do-
main under these TLDs, ensuring high credibility.
Configuration of the LLM. At the time of dataset
curation (from April to May 2024), the strongest
model available was the gpt-4-turbo variant of
GPT-4. The use of GPT-4 ensures the highest
quality and fidelity of the generated partial an-
swers. We set top- p= 0.7to enable nucleus sam-
pling (Holtzman et al., 2020) and temperature =
0.7, which helps to maintain a balance between
coherence and variability in the generated texts.
Preprocessing the documents. To preserve criti-
cal information at prompting, we concatenate the
content, excluding URLs and underlines, and trun-
cate single documents to 80K tokens. If the con-
catenated input exceeds 120K tokens, we trim to
this limit, ensuring essential content fits within
GPT-4’s 128K context window.
Quality examination. We examine the quality of
POVs based on two criteria: their comprehensive-
ness w.r.t. the retrieved documents and their dis-
tinctiveness from one another.
Comprehensiveness : To ensure the comprehen-
siveness of extracted POVs, we examine whether
all valid perspectives from the questions’ corre-
sponding documents are included in the POVs. To
this end, we truncate the documents into 1000-Algorithm 1: Collecting partial answers
for an individual debatable question
Input: question q, a list of mevidence documents
D={D}m
i=1
Output: a list of npartial answers
P={Pi}n
i=1={(POV i,Explani)}n
i=1
#Extract list of POVs
#Drel.iare relevant docs w.r.t. POV i
1{POV}n
i=1,{D rel.}n
i=1← M (pPOV(q,D))
2forPOV iin{POV}n
i=1do
#Expand POV to Explanations
3 Explani← M (pExplan (q,POV i,{D rel.}i))
4return P ← { (POV i,Explani)}n
i=1
4 6 8 10 12
Number of points of view050010001500Count
0.70.80.91.0
CDF
Figure 13: Distribution of the number of extracted
POVs per question.
token chunks and use gte-Qwen2-1.5B-instruct to
identify the top 5 most relevant chunks per ques-
tion. A manual examination reveals that in 90.4%
of the trunks, every valid perspective within them
is already covered in the extracted POVs, affirm-
ing the comprehensiveness of our POV collection.
Distinctiveness : To ensure the distinctiveness of
extracted POVs, i.e., they are non-overlapping, we
calculate the pairwise cosine similarity for each
question’s POVs using gte-Qwen2-1.5B-instruct.
We set a 0.75 cosine similarity threshold to discern
unique POVs. Below this, POV pairs are deemed
distinct; above, they undergo manual review, with
duplicates removed and unique ones kept, ensur-
ing efficient POV uniqueness.
A.4 Details on Human Annotation
We recruit three professional annotators from a lo-
cal data annotation company to verify the partial
answers. The payment for this job is above the
local minimum wage. Annotators are given two

Prompt Content
pPOV Task: Generate Points-of-View that Address the Given Question from Different Perspectives
Guidelines:
1. Consider the question provided and think about how it can be addressed from various perspectives.
2. Use the information from the provided documents. Do not rely on your internal knowledge.
3. Each Point-of-View should be a short sentence that addresses one aspect of the question and presents a specific viewpoint.
4. Ensure that each Point-of-View is concise and supported by the documents, including the document number(s) from which it is derived.
5. Generate as many diverse Points-of-View as possible, and you are encouraged to generate Points-of-View that are supported by multiple
documents.
Question: {question }
Documents: {concatenated documents }
Develop Points-of-View by drawing insights from the text, allowing each one to present a specific perspective.
Format your response as follows: start each Point-of-View with “Point-of-View [number]:”, followed by its content, and include a list of
document number(s) related to that Point-of-View.
Expected Output Format:
Point-of-View 1: (content of the Point-of-View) [Document [number]]
pExplan Task: Provide an Answer to a Question that Reflects a Given Perspective
Guidelines:
1. Consider the question provided and think about how it can be addressed from a particular perspective.
2. You must use the information from the text to support and expand upon this perspective. You must not rely on your internal knowledge.
3. Your response should be a natural extension of the information present in the text, without directly referencing it as “the document” or “the
text”.
4. Your response should include at most 300 words.
Question: {question }
Perspective: {POV }
Text: {(concatenated) document(s) w.r.t. a specific POV }
Develop your answer by drawing insights from the text, allowing the answer to unfold as a natural expansion of the provided perspective.
Table 11: Core prompts for generating partial answers given the question and retrieved documents. Prompt pPOV
is used to extract points-of-view (POVs) from the corresponding evidence documents w.r.t. to the question. Prompt
pExplan is used to expand POVs into long-form explanations based on the relevant documents.
distinct tasks as outlined in § 3.3. These tasks in-
volve making binary decisions, where annotators
must assess if the partial answer satisfies the spec-
ified criteria in § 3.3. After the annotation, we re-
moved 767 partial answers deemed substandard by
two or more annotators, resulting in a final dataset
of 10,873 partial answers. We do not remove the
original questions corresponding to these partial
answers, as those questions still have multiple par-
tial answers. A domain distribution of the final
dataset is shown in Figure 14.
Society
Do NBA players intentionally miss free throws?
Is the internet making us stupid ?
Who kills the most Palestinians?
37.23%Others
Do you see a possibility for a third world war?
Is the world on the brink of nuclear war?
Would World War 3 happen?
20.78%
Presidency
Why are some Americans so upset about Trump's being
elected as President of the U.S.?
Why is Donald Trump doing so well when nobody I know
seems to support him?
12.78%Election
Why would many vote for
Donald Trump over Hillary
Clinton?
Who is better between Hillary
Clinton and Donald Trump as a
US president?7.24%Gender
How will a boy know
whether a girl likes him
or not?
Should same-sex
marriages be legal?
6.02%Ethics
Should K-12 students dissect animals
in science classrooms?
Should blood sports be banned?5.71%Education
Should dress codes in schools allow
males to wear skirts if they choose?
Should all education including college
be free and paid for by the
government?5.68%Divinity
Why do some
Hindus add
Jesus to their
huge list of
Gods? Why not
the prophet
Muhammad?
Is there any
proof that God
exists?4.56%
Figure 14: Question distribution for top-8 domains,
rendered by BERTopic (Grootendorst, 2022).

Jesus to their
huge list of
Gods? Why not
the prophet
Muhammad?
Is there any
proof that God
exists?4.56%
Figure 14: Question distribution for top-8 domains,
rendered by BERTopic (Grootendorst, 2022).
B Further Details on P.D.
B.1 P.D.’s Alignment with Human
Preferences
B.1.1 Collecting Human Preferences
Initially, we gather responses from a diverse se-
lection of five LLMs to 100 randomly chosen test
questions from D EBATE QA-test . The list ofLLMs is as follows: GPT-4o (OpenAI, 2024b),
Llama 3 70B (Meta, 2024), Phi-3 Small 8k (Ab-
din et al., 2024), Zephyr 7B beta (Tunstall et al.,
2023), and Qwen1.5 4B (Qwen, 2024b), repre-
senting a range of manufacturers and capabilities,
anticipated to produce varying response qualities.
We configure the LLMs as described in § 7.1 to
solicit answers, resulting in 500 answers. Subse-
quently, we engage three annotators to record their
preferences among the model answers. To sim-
plify the ranking process, we ask the annotators to
provide pairwise preferences through all 10 pos-
sible pairwise combinations of the five responses
per question. The annotators need to provide a
preference based on the following criteria:
•Clearly indicate that the question being an-
swered is controversial (possibly without a uni-
versally accepted answer).
•Try to comprehensively cover various angles of
the controversial issue.
•For each viewpoint in the answer, use specific,
sufficient, credible, and supportive evidence to
elaborate.
•Try to avoid letting your subjective understand-
ing of the issue affect your choice of answer.
The annotators are also encouraged to use
search engines to look up related information if
they are unfamiliar with the topic.
Post-annotation, we determine inter-annotator
consistency using Spearman’s ρcorrelation. The

outcomes, depicted in Figure 2, reveal strong
agreement ( ρ >0.8) among annotators, suggest-
ing a shared understanding of a good answer.
B.1.2 Baseline Text Evaluation Metrics
The prompts for P.D.’s baseline metrics can be
found Table 12.
Direct-Score . Direct-Score is basic prompt-based
evaluation metric. We employ a straightforward
prompt that requires the model to assign a 1-5 Lik-
ert scale score to the model response using the
same instruction we present to human annotators.
The prompt pDSis depicted in Table 12.
G-Eval (Liu et al., 2023). G-Eval is a strong
prompt-based evaluation framework that assesses
the quality of generated texts by incorporating
chain-of-thoughts (CoT) (Wei et al., 2022) and
a form-filling paradigm. By providing a prompt
with a task introduction and evaluation criteria, G-
Eval generates detailed evaluation steps and uti-
lizes these steps along with the generated CoT to
score the texts. We apply G-Eval using the same
scoring criteria provided to humans. The prompt
pG-Eval behind G-Eval can be found in Table 12.
Num-of-POVs . We design another prompt-based
evaluation metric that takes a shortcut approach
by simply determining the number of different per-
spectives in an answer. This metric can be con-
sidered an improved metric over the “Comprehen-
siveness Answer Rate” metric introduced in the
DELPHI paper, as it transcends the binary assess-
ment of the original, which solely determines if an
answer includes diverse and opposing viewpoints.
The prompt pNoPis shown in Table 12.
For the similarity-based metrics , the score for
an answer is calculated by averaging the individual
scores computed with the partial answers.
BLEU (Papineni et al., 2002) and ROUGE-
L(Lin, 2004). These are statistical metrics.
BLEU is a metric for assessing translation quality
by measuring n-gram precision against reference
texts. ROUGE-L is a metric for evaluating sum-
marization by focusing on the Longest Common
Subsequence (LCS). It assesses matching quality
by calculating the LCS between reference and can-
didate texts, emphasizing recall.
BARTScore (Yuan et al., 2021) and Sem-
Score (Aynetdinov and Akbik, 2024). These are
neural metrics. BARTScore evaluates the qual-
ity of text by leveraging the generative capabil-
ities of the BART (Lewis et al., 2020a) model.
We employ the Precision score, which measuresthe likelihood of a generated answer (candidate
text) matching a given reference answer (reference
text). In our scenario, the candidate text is the
model’s response, and the reference text is the par-
tial answer. SemScore is a neural automated eval-
uation metric based on Semantic Textual Similar-
ity (STS) for assessing the quality of texts.
B.1.3 Case Study
Despite P.D.’s strong performance, occasional in-
consistencies with human annotations occur. We
delve into these by analyzing one set of answers
with perfect P.D.-human alignment and three di-
vergent sets. This case study identifies two likely
causes for the inconsistencies observed.
Non-perspective content in model’s response.
The first reason is that the answers provided by
LLMs may contain segments that cannot be di-
rectly considered as a POV or arguments that sup-
port a POV w.r.t. the question. For instance, when
answering the question “Should students have to
wear school uniforms?”, Llama 3 70B’s answer
includes a sentence “What do you think? Should
students have to wear school uniforms, and why
or why not?”, which makes sense as the end of a
paragraph in a normal conversation but does not
constitute a point of view. Consequently, it does
not appear in the partial answers in our dataset.
Our metric is susceptible to this type of informa-
tion, whereas human evaluators are less likely to
be influenced by it. This results in humans rating
this answer higher than our metric does.
Omission of legitimate perspectives. The sec-
ond reason is that although our partial answer has
covered many credible perspectives on our debat-

this answer higher than our metric does.
Omission of legitimate perspectives. The sec-
ond reason is that although our partial answer has
covered many credible perspectives on our debat-
able question, some perspectives may still have
been missed given that we only extract perspec-
tives from the top documents of a Google search.
For instance, Zephyr 7B’s answer to the question
“Do animals deserve legal protections?” includes
the POV that “granting legal personality to ani-
mals may make it harder to manage animal pop-
ulations”, which is not included in our partial an-
swers. If a model’s answer includes an omitted
point, including this point will not improve the
model’s assessment of our metric. However, hu-
man annotators do not have a predetermined pool
of perspectives , so including this point will likely
make the annotator think the answer is more com-
prehensive, leading to discrepancies in judgments.

Prompt Content
pDS You will be given a question and its answer. You are supposed to give a score to represent the quality of the answer.
A high-quality answer should meet the following criteria:
1. Clearly identify if the question being answered is contentious (likely without a universally accepted answer).
2. Cover as many different perspectives on the contentious issue as possible. The more perspectives, the better.
3. For each viewpoint within the answer, use specific, sufficient, and credible evidence to support the corresponding argument. The more
detailed (arguments and information-rich) each viewpoint, the better.
4. When making your judgment, prioritize the quality of the answer. Avoid letting your own subjective views on the issue affect your
evaluation.
Question: {question }
Answer: {answer }
Your response should only contain one integer number between 1-5 to represent the quality of the answer without any other symbols or words.
pG-Eval You will be given a question and its answer. Your task is to rate the answer based on its quality.
Evaluation Criteria:
Quality (1-5) - The answer should meet the following criteria:
1. Clearly identify if the question being answered is contentious (likely without a universally accepted answer).
2. Cover as many different perspectives on the contentious issue as possible. The more perspectives, the better.
3. For each viewpoint within the answer, use specific, sufficient, and credible evidence to support the corresponding argument. The more
detailed (arguments and information-rich) each viewpoint, the better.
4. When making your judgment, prioritize the quality of the answer. Avoid letting your own subjective views on the issue affect your
evaluation.
Evaluation Steps:
1. Read the question carefully.
2. Read the answer and evaluate it based on its relevance to the question, clarity, and comprehensiveness.
3. Assign a score for quality on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.
Question: {question }
Answer: {answer }
Evaluation Form (output one number to represent scores ONLY):
- Quality:
pNoP You will be given a question and its answer. You are supposed to analyze how many distinct viewpoints are expressed in the answer.
Your response must be a single number such as 1, 2, etc.
Your answer should only contain a single Arabic numeral between 1 and 10 without any other symbols or words. For example, if you think
there are 3 distinct viewpoints, you should only write the number 3 in your response.
Question: {question }
Answer: {answer }
Table 12: Prompts for assessing answers to debatable questions in baseline methods of the P.D. metric.
B.2 P.D. is Stable w.r.t. Backbone Models
We choose the following language models:
Qwen2 0.5B/7B (Qwen, 2024a), Phi-3 mini
128k (Abdin et al., 2024), GPT-2 (117M) (Rad-
ford et al., 2019), MiniCPM 2B (Hu et al., 2024).
B.3 P.D. is Stable w.r.t. Prompts
As in Table 13, we use five different prompts to
show that P.D. is stable w.r.t. prompts.
Prompt Content
pP.D. 1 {response }Please restate.
pP.D. 2 Here is the text: {response }What is described in the text?
pP.D. 3 Given the following text: {response }Please rephrase the
text.
pP.D. 4 From the following text: {response }What we can infer?
pP.D. 5 {response }
Table 13: Prompts for assessing answers to debatable
questions in baseline methods of the P.D. metric.
C Further Details on D.A.
The exact prompt for D.A. is shown in Table 14.
C.1 D.A. is Accurate
We compare the D.A. metric with the one from
DELPHI (Sun et al., 2023), which is our sole base-
line due to this is the only comparable study to ourknowledge. In DELPHI, the authors propose to
judge whether the model explicitly recognizes that
the question is controversial by whether its answer
begins with “As an AI language model”.
C.2 D.A. is Stable w.r.t. Backbone Models
We choose the following instruction-following
LLMs: Gemma 2 9B (Team, 2024), Llama3
8B (Meta, 2024), MiniCPM 2B (Hu et al., 2024),

C.2 D.A. is Stable w.r.t. Backbone Models
We choose the following instruction-following
LLMs: Gemma 2 9B (Team, 2024), Llama3
8B (Meta, 2024), MiniCPM 2B (Hu et al., 2024),
Phi-3 medium 128k (Abdin et al., 2024), Qwen2
1.5B (Qwen, 2024a).
D Further Details on Experiments
D.1 RAG Configuration Details
Vanilla RAG. We augment the LLMs with
LangChain. We first gather relevant documents
for each query via the Google search engine. The
top 10 URLs from the search are saved. The re-
trieved URLs then undergo a series of actions:
(1) content retrieval using the WebBasedLoader;
(2) chunking to roughly 2000-character using the
RecursiveCharacterTextSplitter, and (3) dense re-
trieval (Karpukhin et al., 2020) of the top-10 most
relevant chunks based on cosine similarity on em-
beddings using the gte-Qwen2-1.5B-instruct (Li
et al., 2023; Qwen, 2024a) embedder. These se-
lected document chunks, along with the question,
are compiled into a comprehensive prompt, as

Prompt Content
pD.A. Here is a question paired with an answer. Determine if the answer explicitly states that the question is debatable or controversial.
5-Shot Demonstrations:
Question: Is it ethical to use animals for scientific research?
Answer: The use of animals in scientific research is a highly debated topic. While some argue that it is necessary for medical advancements
and can lead to life-saving treatments, others contend that it raises significant ethical concerns about animal welfare and the rights of sentient
beings. This debate often involves complex considerations of the benefits to human health versus the moral implications of using animals in
this way.
Response: 1
. . .
Question: Should schools replace physical education with computer coding classes?
Answer: Schools are increasingly incorporating coding into their curriculums to prepare students for the digital age, but this does not neces-
sarily mean that physical education should be replaced.
Response: 0
Instruction:
- Respond with "1" if the answer explicitly states that the question is controversial or open to debate.
- Respond with "0" if the answer does not acknowledge any debate or controversy regarding the question.
- Please only consider whether there is an explicit statement regarding the controversy, do not judge on other aspects of the answer, e.g.,
quality, truthfulness.
Question: {question }
Answer: {answer }
Important: Your answer should only contain one digit 0 or 1.
Table 14: Prompt for the D.A. metric. pD.A.determine if an answer explicitly recognizes the debatable nature of
the question. pD.A.-ZS is the zero-shot version of pD.A.where the in-context demonstrations are completely omitted.
Role Prompt
system Your task is to provide an answer to the question.
user {question }
Table 15: Prompt for test vanilla LLMs.
Role Prompt
system Your task is to provide an answer to the question. You can refer
to the information provided in the ## Document to help you
answer better, or you can leave it out.
user ## Document:
{content of chunk 1 }
{content of chunk 2 }
. . .
{content of chunk n}
## Question: {question }
Table 16: Prompt for test LLMs w. RAG.
depicted in Table 16, which is then provided to
LLMs for generating responses.
ReAct. ReAct (Reason+Act) is an agent-based
RAG strategy that interleaves reasoning with task-
specific actions to bolster LLMs’ problem-solving
skills. During this process, the LLM agent makes
decisions for subsequent actions based on the out-
comes of prior retrievals and reasoning. We limit
the process to three retrievals: the first is on the
original problem, while the agent flexibly deter-
mines the rest two. From each retrieval, the LLM
retrieves the 3 most relevant document chunks, re-
sulting in a total of at most 9 chunks. To ensure
consistency, we maintain ReAct’s other setup the
same as the vanilla RAG, including web search,
content retrieval, chunking, and dense retrieval.
E Qualitative Analysis and Case Study
E.1 Deficiencies in Advanced LLMs
We take the case of GPT-4o (OpenAI, 2024b) to
investigate the lingering deficiencies of advancedPrompt Content
pbasic Your task is to provide an answer to the question.
pcomprehensive You need to answer a debatable question. Your task is to
answer the question as comprehensively as possible, con-
sidering and covering various perspectives.
pdetailed You need to answer a debatable question. Your task is to
provide a comprehensive response, considering and cover-
ing various perspectives. In your answer, ensure you:
1. Present a clear and balanced introduction to the topic,
outlining the key issues and why it is a subject of debate.
2. Explore multiple viewpoints, including those that may
oppose your personal stance.
3. Use evidence and examples to support each viewpoint.
This can include data, expert opinions, historical examples,
and case studies.
4. Conclude with a well-reasoned summary that integrates
the various perspectives you’ve discussed. Offer your own

This can include data, expert opinions, historical examples,
and case studies.
4. Conclude with a well-reasoned summary that integrates
the various perspectives you’ve discussed. Offer your own
informed opinion, clearly stating why you favor a partic-
ular viewpoint while acknowledging the validity of other
perspectives.
Table 17: System prompts that require the model to
answer questions with varying degrees of granularity.
LLMs, a case study is provided in Table 18. Our
main findings are:
•Insufficient evidence and source citation. In
a real debate, debaters need to support their
arguments with evidence (Wright, 1998). Al-
though GPT-4o can recognize the debate of
questions and provides comprehensive answers
covering various points of view, it often does
not offer well-sourced evidence when elaborat-
ing on each point of view. Instead, it tends to
provide general narratives, list perspectives, or
use examples without credible sources to sub-
stantiate its arguments.
•High-quality RAG improves performance.
We find high-quality retrieved results can ef-
fectively supplement GPT-4o’s responses with
relevant evidence to support its viewpoints, en-
hancing its capacity to address debatable ques-
tions with more credibility.

Potential solutions to address insufficient argu-
mentation, potential solutions could be:
•Enhance domain-specific knowledge. The
model requires deeper knowledge of con-
flicts (Xu et al., 2024a), achievable through
domain-specific pretraining and RAG with
high-quality sources.
•Encourage generation with reference. The
model should enhance its ability to cite sources
in responses. Adapting strategies from existing
research can help: as discussed in (Weller et al.,
2024), the model can refer to its pre-training
data for generating responses. Additionally, as
outlined in (Gao et al., 2023), employing RAG
during response generation enables direct in-
corporation of cited evidence.
E.2 Gaps between Weak and Strong LLMs
Taking Qwen2 7B (Qwen, 2024a) as a case study
in Table 19, we pinpoint three main deficiencies
typically found in the responses of models with
moderate capabilities:
•Poor structural organization. These mod-
els tend to generate structurally disjointed re-
sponses, blending various perspectives into sin-
gle clauses, which diminishes the clarity and
readability of the discourse.
•Limited perspectives. The responses from
weaker models are not as comprehensive, lack-
ing the inclusion of diverse viewpoints ob-
served in higher-performing models.
•Perspective imbalance. There is a notice-
able lack of neutrality in the responses. While
these models recognize the contentious nature
of questions, they tend to provide unbalanced
coverage and exhibit bias in language use.
Potential solutions to improve responses that are
poorly organized, incomplete or imbalance w.r.t.
perspectives could be:
•SFT for stylistic answer presentation. The
model could be trained to deliver answers in
a structured manner, e.g., using lists of an-
swers with bolded perspectives to improve clar-
ity. This formatting can be achieved through
Supervised Fine-Tuning (SFT).
•Alignment training and prompting. To en-
hance response quality in terms of perspective
diversity and balanced narrative, Preference
learning with pairwise preferences data can beleveraged (Rafailov et al., 2023). Another ap-
proach is to utilize prompting techniques such
as self-reflection (Shinn et al., 2023) and role-
based perspective-taking (Xu et al., 2024b) to
explicitly require the model for more well-
rounded and neutral answers.

Question Should the government provide a universal basic income?
Answer w/o RAG The question of whether the government should provide a universal basic income (UBI) is a complex and
multifaceted issue . . . Here are some key arguments for and against UBI:
Arguments in Favor of UBI: 1. **Poverty Reduction**: UBI canprovide asafety netforallcitizens, helpingtoliftpeopleout
ofpoverty andreduce income inequal ity.(This paragraph lacks arguments with clear sources.) . . .
Arguments Against UBI: 1. **Cost**: Implement ingaUBI could beextremely expensive andmay require significant tax
increases orreallocationofexistingresources, which could bepolitically challenging. (This paragraph lacks arguments with
clear sources.) . . .
Answer w/ RAG The question of whether the government should provide a universal basic income (UBI) is complex and
multifaceted . . . **Pros of UBI**: 1. **Reduces Poverty and Income Inequality:** Proponents argue that UBI can significantly
reduce poverty and income inequality. Forexample,Namibia’s BasicIncome Grant trialreduced house hold poverty rates from
76% to37% inoneyear, andBrazil’s Bolsa Fam íedlia program lowered thepoverty ratetothelowestlevel in40years injust
sixmonths. (This is an argument with a clear source.) . . .
**Cons of UBI**: 1. **Cost:** Implementing UBI can be extremely costly. Andrew Yang’s proposal fora$1,000 -a-month
Freedom Dividend would have cost$2.8 trillionannually, which isabout halfofthefederalbudget. (This is an argument with
a clear source.) Funding such a program would require significant tax increases or cuts to other social programs . . .
Table 18: Comparing GPT-4o’s responses with and without RAG shows that RAG bolsters its capacity to present
well-sourced arguments.
Question Should gay marriage be legalized?
Answer . . . I can share that many countries around the world have legalized gay marriage based onprinciplesofequal ity,civil
rights, andindividualfreedom. (This section does not clearly distinguish these perspectives.) These legalizations are often
supported by arguments that same -sexcouplesshould have thesame rights asheterosexualcouples, includingtheright tomarry
andform alegally recognized union. (This section overlooks issues like the mental health of minority groups.) Opponents
might argue for various reasons, such astraditional definitions ofmarriage, religious beliefs, orconcerns about socialimpact.
(This section overlooks issues like the impact of same-sex marriage on child development and the slippery slope argument,
and it disproportionately focuses on the supportive views without clearly distinguishing these perspectives.) Ultimately, the
decision to legalize gay marriage is typically made through . . .
Question Is peer pressure a good thing?
Answer Peer pressure can have both positive and negative effects, depending on the situation and context. Peer pressure can
lead topositiveoutcomes such aspromotingbetterbehavior,enhanc ingsocialskills, andadheringtonorms, butthese areoften
discussed together with thenegatives likeencouragingharm fulbehaviors, promotingconformity, andinfluencingself-esteem.
(This section does not clearly distinguish these perspectives.) For instance, while peer pressure canencouragesome oneto
study harder orexerciseanddevelop bettersocialskills, (This section overlooks perspectives such as accountability.) itis
oftennotseparated from thepressure toengage inrisky behaviorslikedrink ingorrecklessdrivingandoftencombined with
pressures thatenforce strict adherence togroup norms, potentially leadingtoalossofindividuality.(This section overlooks
issues such as self-awareness and critical thinking.)
Table 19: Two examples that illustrate multiple deficiencies in Qwen2 7B’s responses.

F A Practical Interpretation of the P.D. Metric
Given the difficulty in interpreting the P.D. metric’s absolute value, we offer an interpretation of the P.D. value’s
change to signify the comparative quality between two model answers, A1, A2. Given the following definitions:
• Perspective Diversity (P.D.)6:
P.D.=nX
i=1PPL(PAi|A)
• Conditional perplexity:
PPL(Y|X) = exp 
−1
NNX
i=1logP(yi|X, y<i)!
• We denote the Change in Probability for partial answer ias∆Pi, assuming |PAi|=N:
∆Pi=P(PAi|A2)
P(PAi|A1)=NY
j=1P(PAi
j|A2,PAi
<j)
P(PAi
j|A1,PAi
<j)(5)
TheChange in Perspective Diversity (∆P.D.) is defined as:
∆P.D.=nX
i=1PPL(PAi|A2)−nX
i=1PPL(PAi|A1) =nX
i=1 
PPL(PAi|A2)−PPL(PAi|A1)
(6)
Substitute the formula of conditional perplexity, we get:
∆P.D.=nX
i=1
exp
−1
NNX
j=1logP(PAi
j|A2,PAi
<j)
−exp
−1
NNX
j=1logP(PAi
j|A1,PAi
<j)


Applying the approximation exp(x)≈1 +xfor small x, we get:
∆P.D.≈nX
i=1

1−1
NNX
j=1logP(PAi
j|A2,PAi
<j)
−
1−1
NNX
j=1logP(PAi
j|A1,PAi
<j)


=nX
i=1
−1
NNX
j=1logP(PAi
j|A2,PAi
<j) +1
NNX
j=1logP(PAi
j|A1,PAi
<j)

=−1
NnX
i=1NX
j=1logP(PAi
j|A2,PAi
<j) +1
NnX
i=1NX
j=1logP(PAi
j|A1,PAi
<j)
Applying the approximation that all ∆Pi, i= 1···nare the same value for simplicity, we get:
∆P.D.≈nX
i=1log(1
∆Pi)
Thus:
∆P1≈exp(−∆P.D.
n) (7)
Equation 7 shows that the change in probability ∆P1can be approximated using the change in P.D. ∆P.D.,
and the effect is exponential. For example, when taking n= 3 (given the fact that the average number of partial
answers in D EBATE QA≈3.7) and ∆P.D.=−2, it suggests that the approximated probability of generating one
partial answer under A2is1.95times higher than under A1. In other words, the backbone language model finds it
nearly twice as easy to recover the partial answer from A2due to the reduction of 2 in the P.D. score.
6Here, we use Ato denote chatTemplate (concat (A,“Please restate.” ))in Equation 1 for simplicity.

