Toward Automatic Relevance Judgment using Vision–Language
Models for Image–Text Retrieval Evaluation
Jheng-Hong Yang, Jimmy Lin
David R. Cheriton School of Computer Science
University of Waterloo, Canada
{jheng-hong.yang, jimmylin}@uwaterloo.ca
Abstract
Vision–Language Models (VLMs) have
demonstrated success across diverse appli-
cations, yet their potential to assist in rele-
vance judgments remains uncertain. This
paper assesses the relevance estimation ca-
pabilities of VLMs, including CLIP, LLaVA,
and GPT-4V, within a large-scale ad hoc
retrieval task tailored for multimedia con-
tent creation in a zero-shot fashion. Pre-
liminary experiments reveal the following:
(1) Both LLaVA and GPT-4V, encompass-
ing open-source and closed-source visual-
instruction-tuned Large Language Models
(LLMs), achieve notable Kendall’s τ∼0.4
when compared to human relevance judg-
ments, surpassing the CLIPScore metric.
(2) While CLIPScore is strongly preferred,
LLMs are less biased towards CLIP-based
retrieval systems. (3) GPT-4V’s score dis-
tribution aligns more closely with human
judgments than other models, achieving
a Cohen’s κvalue of around 0.08, which
outperforms CLIPScore at approximately
-0.096. These findings underscore the poten-
tial of LLM-powered VLMs in enhancing
relevance judgments.
1 Introduction
Cranfield-style test collections, consisting of a
documentcorpus, asetofqueries, andmanually
assessed relevance judgments, have long served
as the foundation of information retrieval re-
search (Cleverdon, 1960). However, evaluating
every document for every query in a substan-
tial corpus often proves cost-prohibitive. To
tackle this challenge, a subset of documents
is selected for assessment through a pooling
process. While this method is cost-effective
compared to user studies, it has limitations due
to its simplifications and struggles to adapt to
complex search scenarios and large document
collections.In this study, we explore the adaptability
of model-based relevance judgments for image–
text retrieval evaluation. Leveraging model-
based retrieval judgments presents an appeal-
ing option. Not only does it provide valuable
insights before undertaking the laborious pro-
cesses of document curation, query creation,
and costly annotation, but it also has the po-
tentialtoextendandscaleuptocomplexsearch
scenarios and large document collections. To
explore opportunities and meet the demands
for large-scale, fine-grained, and long-form text
enrichment scenarios in image-text retrieval
evaluation (Schneider et al., 2021; Kreiss et al.,
2022; Singh et al., 2023; Yang et al., 2023), our
objective is to extend the human-machine col-
laborative framework proposed by Faggioli et al.
(2023) to the context of image-text retrieval
evaluation, alongside widely adopted model-
based image-text evaluation metrics (Hessel
et al., 2021; Park et al., 2021; Kim et al., 2022;
Ruiz et al., 2023; Kreiss* et al., 2023).
Our primary focus is on a fully automatic
evaluation paradigm, where we harness the ca-
pabilities of Vision–Language Models (VLMs),
includingCLIP(Radfordetal.,2021), aswellas
visual instruction-tuned Large Language Mod-
els (LLMs) like LLaVA (Liu et al., 2023b,a)
and GPT-4V (OpenAI, 2023). To evaluate
this approach, we conducted a pilot study us-
ing the TREC-AToMiC 2023 test collection,
which is designed for multimedia content cre-
ation (Yang et al., 2023), based on our instruc-
tion prompt template for VLMs (cf. Table 1
and Section 3.2).
We observe that model-based relevance judg-
ments generated by visual instruction-tuned
LLMs outperform the widely adopted CLIP-
Score (Hessel et al., 2021) in terms of ranking
correlations and agreements when compared to
human annotations. While this discovery holdsarXiv:2408.01363v1  [cs.IR]  2 Aug 2024

Table 1: Prompt template for relevance estimation.
The VLMs are expected to take text qand image
dindependently. The prompts are only applied to
the textual input q, while the VLMs process the
pixel values of image ddirectly.
Text Input: Image Input:
Context:
Page Title: <page_title>
Page Context: <page_context>
Section Title: <section_title>
Section Context: <section_context>
Relevance Instruction:
Think carefully about which images best illus-
trate the SECTION subject matter. Given the
text and the image please answer the following
questions given the criteria listed as follows:
* Images must be significant and relevant in
the topic’s context, not primarily decorative.
They are often an important illustrative aid to
understanding.
* Images should look like what they are meant
to illustrate, whether or not they are provably
authentic.
* Textual information should almost always be
entered as text rather than as an image.
Output Instruction:
Rate the image’s overall relevance (integer,
scale: 1-100) in terms of matching the text.
Output format: "Relevance: <score>"
promise, we also uncover the potential evalu-
ation bias when using model-based relevance
judgments. Our analysis reveals a bias in favor
of CLIP-based retrieval systems in the rankings
when employing model-based relevance judg-
ments, resulting in higher overall effectiveness
assessments for these systems. In summary, our
contributions can be distilled as follows:
•We demonstrate and explore the feasibility
of incorporating VLMs for fully automatic
image–text retrieval evaluation.
•We shed light on the evaluation bias when
utilizing model-based relevance judgments.
2 Related Work
Evaluation Metrics for Image–Text Rel-
evance. Nowadays, model-based evaluation
metrics are widely utilized in various vision–
language tasks, including image caption-
ing (Hessel et al., 2021; Chan et al., 2023b) and
text-to-image synthesis (Park et al., 2021; Hu
et al., 2023). Among model-based approaches,
CLIP-based methods (Park et al., 2021; Kim
etal.,2022;Chanetal.,2023a;Ruizetal.,2023;Kreiss* et al., 2023), such as CLIPScore (Hessel
et al., 2021), are particularly prevalent. How-
ever, while these metrics are capable of measur-
ing coarse text-image similarity, they may fall
short in capturing fine-grained image–text cor-
respondence (Kreiss et al., 2022; Yuksekgonul
et al., 2023). Recent research has highlighted
the effectiveness of enhancing model-based eval-
uation metrics by leveraging LLMs to harness
their reasoning capabilities (Chan et al., 2023b;
Lu et al., 2023; Betti et al., 2023). There exists
significant potential for incorporating LLMs
into model-based approaches, as LLM outputs
are not limited to mere scores but can also pro-
vide free-form texts, e.g., reasons, for further
analysis and many downstream tasks (Zeng
et al., 2023).
Model-based Relevance Judgments. Tra-
ditionally, relevance judgments in retrieval
tasks have adhered to the Cranfield evalua-
tion paradigm due to its cost-effectiveness, re-
producibility, and reliability when compared
to conducting user studies. However, this
approach often relies on simplified assump-
tions and encounters scalability challenges. Re-
searchers have recently explored model-based
automatic relevance estimation as a promising
alternative. This approach aims to optimize
human-machine collaboration to obtain ideal
relevance judgments. Notably, studies of Dietz
and Dalton (2020) and Faggioli et al. (2023)
have revealed high rank correlations between
model-based and human-based judgments. Ad-
ditionally, MacAvaney and Soldaini (2023) have
delved into the task of filling gaps in relevance
judgments using model-based annotations.
3 Methodology
In this study, we investigate techniques for esti-
mating image-text relevance scores, denoted as
F(q, d)∈R, where qrepresentsthetext(query)
anddrepresents the image (document). Our
primary focus is on utilizing VLMs to generate
relevance scores, akin to empirical values anno-

F(q, d)∈R, where qrepresentsthetext(query)
anddrepresents the image (document). Our
primary focus is on utilizing VLMs to generate
relevance scores, akin to empirical values anno-
tated by human assessors denoted as ˆF(q, d).
The main objective is to assess the proximity
between model-based Fand human-based ˆFin
image–text retrieval evaluation. We begin with
a discussion of the setting for human-based
annotations, followed by the process for gener-
ating model-based annotations.

3.1 Human-based Annotations
Our primary focus revolves around a critical
aspect of multimedia content creation, specif-
ically, the image suggestion task, an ad hoc
image retrieval task as part of the AToMiC
track in the TREC conference 2023 (TREC-
AToMiC 2023).1The image suggestion task
aims to identify relevant images from a prede-
fined collection, given a specific section of an
article. Its overarching goal is to enrich textual
content by selecting images that aid readers in
better comprehending the material.
Relevance scores for this task are meticu-
lously annotated by NIST assessors, adhering
to the TREC-style top- kpooling relevance an-
notation process. A total of sixteen valid par-
ticipant runs, generated by diverse image–text
retrieval systems, are considered, encompassing
(CLIP-based) dense retrievers, learned sparse
retrievers, caption-based retrievers, hybrid sys-
tems, and multi-stage retrieval systems. The
pooling depth is set to 25 for eight baseline
systems and 30 for the remaining participant
runs.
NIST assessors classify candidate results into
three graded relevance levels to capture nu-
ances in suitability, guided by the content of
the test query. The test query comprises tex-
tual elements such as the section title, section
context description, page title, and page con-
text description. Assessors base their relevance
judgments on the following criteria:
•0 (Non-relevant) : Candidates deemed ir-
relevant.
•1 (Related) : Candidates that are related
but not relevant to the section context are
categorized as related. They contain perti-
nent information but do not align with the
section’s context.
•2 (Relevant) : These candidates are con-
sidered relevant to the section context and
effectively illustrate it.
3.2 Model-based Annotations
For automatic relevance estimation, we employ
pretrained VLMs as our relevance estimator,
denoted as F(q, d|P). Our relevance estimator
produces relevance scores given a pair of qand
1https://trec-atomic.github.io/
trec-2023-guidelinesd, which is conditioned on P, where Prepre-
sents the prompt template we used to instruct
the models. Prompt engineering is a commonly
adopted technique for enhancing or guiding
VLMs and LLMs in various tasks (Brown et al.,
2020; Radford et al., 2021). It’s important to
note that our current focus is on pointwise esti-
mation, leaving more advanced ranking meth-
ods (such as pairwise or listwise) that consider
multiple qanddfor future exploration (Sun
et al., 2023; Qin et al., 2023).
Prompt Template Design In line with our
approachtorelevancescoreannotation, wehave
created a prompt template designed to guide
models in generating relevance scores. The
prompt template, presented in Table 1, has
been constructed based on our heuristics and
is not an exhaustive search of all possible tem-
plates. Pretrained VLMs are expected to take
both qanddto produce a relevance score fol-
lowing the instructions defined in the prompt
template P. We anticipate that VLMs will
independently process textual and visual in-
formation, and our prompt template is only
applied to textual inputs. Our template com-
prises three essential components:
•Context: This section processes the textual
information from q.2
•Relevance Instruction: It incorporates task-
specific information designed to provide
VLMs with an understanding of the task.
•Output Instruction: This component offers
instructions concerning the expected output,
e.g., output types and format.
From Scores to Relevance Judgments.
We utilize parsing scripts to process the rel-
evance scores generated by the models and con-
vert them into relevance judgments.3Consid-
ering potential score variations across different
models, we apply an additional heuristic rule to
map these scores into graded relevance levels:
0 (non-relevant), 1 (related), and 2 (relevant).
Specifically, scores falling below the median
value are categorized as 0; scores within the
2For VLMs with limited context windows, e.g., CLIP,

0 (non-relevant), 1 (related), and 2 (relevant).
Specifically, scores falling below the median
value are categorized as 0; scores within the
2For VLMs with limited context windows, e.g., CLIP,
we only take the texts in the context part and ignore
all the rest instructions.
3For CLIP, relevance scores are computed using text
and image embeddings directly.

50-75th quantile range are designated as 1; and
scores exceeding the 75th quantile are assigned
a relevance level of 2.
4 Experiments
We have undertaken an empirical comparison
between human assessors and vision-language
models to offer an initial evaluation of their
current capabilities in estimating relevance
judgments. This comparative analysis encom-
passes one embedding-based model (CLIP) and
two LLMs trained by visual instruction tuning
(LLaVA and GPT-4V). The experiments were
carried out in January 2024.
4.1 Setups
Test Collection. Our study focuses on the
image suggestion task in TREC-AToMiC 2023.
Inthistask, queriesaresectionsfromWikipedia
pages, and the corpus contains images from
Wikipedia. We assess VLMs’ ability to assign
relevancelabelsto9,818image–textpairsacross
74 test topics. We predict relevance scores, gen-
erate qrels for 16 retrieval runs, and compare
them with NIST human-assigned qrels. Note
that the test topics consist of Wikipedia text
sections (level-3 vital articles) without accom-
panying images, and NIST qrels are not pub-
licly accessible during the training of VLMs we
study in this work.
Vision–Language Models. Our experi-
ments feature three models: CLIP (Radford
et al., 2021), LLaVA (Liu et al., 2023b,a), and
GPT-4V (OpenAI, 2023). CLIP serves as a ver-
satile baseline model, offering similarity scores
for image–text pairs. We use CLIPScore (Hes-
sel et al., 2021) (referred to as CLIP-S) for cal-
culating relevance with CLIP. However, CLIP
has limitations due to its text encoder’s to-
ken limit (77 tokens), making it less adaptable
for complex tasks with lengthy contexts. In
contrast, LLMs like LLaVA and GPT-4V, fine-
tuned for visual instruction understanding, pos-
sess larger text encoders capable of handling
extended context. These models excel in vari-
ous vision-language tasks, making them more
versatile compared to CLIP.
4.2 Correlation Study
In this subsection, our primary aim is to inves-
tigate the extrinsic properties of relevance judg-ments generated by various approaches, where
we base our analysis on retrieval runs and rank-
ing metrics. While various techniques exist
to enhance the capabilities of vision-language
models, including prompt engineering, few-shot
instructions, and instruction tuning, our cur-
rent focus centers on examining their zero-shot
capabilities. We defer the exploration of other
methods to future research endeavors. Follow-
ing the work of Voorhees (1998), we undertake
an investigation into the system ranking cor-
relation and the agreement between the rele-
vance labels estimated by the model and those
provided by NIST annotators. We evaluate
the ranking correlations concerning the pri-
mary metrics utilized in the AToMiC track:
NDCG@10 and MAP, and calculate Kendall’s
τ, Spearman’s ρs, and Pearson’s ρp. In our
agreement study, we compute Cohen’s κusing
NIST’s qrels as references.
Overall. The primary results are showcased
in Table 2, where rows correspond to the back-
bone model used for relevance judgment genera-
tion. Notably, models leveraging LLMs such as
LLaVA and GPT-4V outperform the CLIP-S
baseline concerning ranking correlation. Specif-
ically, they achieve Kendall’s τvalues of ap-
proximately 0.4 for NDCG@10 and around 0.5
for MAP. For comparison, previous research re-
ported 0.9 for τfor MAP when comparing two
types of human judgments (Voorhees, 1998).
While there is still room for further improve-
ment, our observations already demonstrate
enhancement compared to the CLIP-S baseline:
0.200 (0.333) for NDCG@10 (MAP). More-
over, other correlation coefficients, including
Spearman and Pearson, corroborate the trends
identified by Kendall’s τ. Additionally, we no-
tice a rising trend in agreement levels when
transitioning from CLIP-S (-0.096) to GPT-4V
(0.080), as evidenced by Cohen’s κvalues. The
agreements achieved by the two largest models
(LLaVA-13b and GPT-4V) are categorized as
’slight,’ which represents an improvement over

(0.080), as evidenced by Cohen’s κvalues. The
agreements achieved by the two largest models
(LLaVA-13b and GPT-4V) are categorized as
’slight,’ which represents an improvement over
the smaller LLaVA-7b model and the baseline.
Evaluation Bias Model-based evaluations
can introduce bias, often favoring models that
are closely related to the assessor model (Liu
et al., 2023c; Pangakis et al., 2023). We term
this phenomenon as evaluation bias . This is
distinct from source bias which indicates that

Table 2: Ranking correlation and judgment agreement analysis. Correlations are reported in terms of
Kendall’s τ, Spearman’s ρs, and Pearson’s ρp, whereas judgment agreement is reported in terms of Cohen’s
κwhen comparing to NIST qrels.
NDCG@10 MAP Agreement
Model Version τ ρ s ρp τ ρ s ρp κ
CLIP-S openai/clip-vit-large-patch14 0.200 0.253 0.209 0.333 0.356 0.418 -0.096
LLaVA-7b v1.5-7b 0.400 0.532 0.633 0.483 0.597 0.507 -0.003
LLaVA-13b v1.5-13b 0.433 0.559 0.659 0.517 0.618 0.523 0.010
GPT-4V 1106-vision-preview 0.400 0.544 0.540 0.500 0.594 0.470 0.080
0.0 0.2 0.4 0.6 0.8
NDCG@10 (Human)0.00.20.40.60.8NDCG@10 (Model)
CLIP-S
LLaVA-7b
LLaVA-13b
GPT-4V
Figure 1: Scatter plots of effectiveness (NDCG@10)
for TREC-AToMiC 2023 runs using human-based
and model-based qrels. Each data point represents
themeaneffectivenessofasinglerunevaluatedwith
different qrels. CLIP-based runs are highlighted in
red. Best viewed in color.
neural retrievers might prefer contents gener-
ated by generative models (Dai et al., 2023). To
address this potential concern, we conducted
an initial analysis using the scatter plot pre-
sented in Fig. 1. In this analysis, we compared
the NDCG@10 scores of the 16 submissions
made by participants employing different sets
of qrels. Each data point on the plot corre-
sponds to a specific run, with distinct mark-
ers representing variations in results based on
relevance estimation models. Upon closer ex-
amination of the plot, we identified a positive
correlation between model-based and human-
based qrels. Notably, the effectiveness of sub-
mitted systems appeared slightly higher when
compared to those using human-based qrels.
To gain deeper insights, we’ve visually high-
lighted CLIP-based submissions in red for a
thorough investigation. This visual distinc-
tion underscores the preference for model-based
qrelsforCLIP-basedsystems, especiallyevidentTable 3: Evaluation bias assessment using Relative
∆in terms of NDCG@10 and MAP. A positive
∆favors CLIP-based systems, while a negative ∆
favors other types of systems.
Model ∆(NDCG@10) ∆(MAP)
CLIP-S 114.7 120.5
LLaVA-7b 58.5 86.6
LLaVA-13b 55.8 83.1
GPT-4V 64.0 91.3
Human -11.7 -19.5
with CLIP-S qrels. We quantitatively assess
this bias using a metric adapted from the work
of Dai et al. (2023):
Relative ∆ = 2Metric CLIP-based −Metric Others
Metric CLIP-based +Metric Others×100%,
here Metric stands for a measure, e.g.,
NDCG@ k, averaged across systems. Observing
Table 3, CLIP-S exhibits a strong bias, with
Relative ∆ = 114 .7for NDCG@10 and 120.5
for MAP. LLM-based approaches also display
a slight bias towards CLIP-based systems, pos-
sibly because both LLaVA and GPT-4V rely
on CLIP embeddings for image representations.
In contrast, human-based qrels show the lowest
bias: -11.7 for NDCG@10 and -19.5 for MAP.
4.3 Estimated Relevance Analysis
In this subsection, we aim to explore the intrin-
sic properties of relevance judgments generated
by various systems. We began our analysis by
examining score distributions, visualized in Fig-
ures 2 and 3, to gain insights into model-based
scores.
Figure 2 presents a Cumulative Distribution
Function (CDF) plot of scores before post-
processing into relevance levels (0, 1, and 2).
We included NIST qrels (human) results for ref-
erence. Notably, GPT-4V’s score distribution
closelyalignswiththehumanCDF,whileCLIP-
S exhibits a smoother S-shaped distribution

0.0 0.2 0.4 0.6 0.8 1.0
Score (Min-Max Normalized)0.00.20.40.60.81.0CDF
CLIP-S
LLaVA-7b
LLaVA-13b
GPT-4V
HumanFigure 2: Cumulative distribution function (CDF)
plot of relevance scores from various models. Hu-
man stands for relevance annotations of NIST qrels.
0 1 2
GPT-4V0 1 2Human2716 2570 657
1121 1455 426
227 376 227
0 1 2
LLaVA-13b733 5145 94
319 2624 68
87 718 2610002000300040005000
Figure 3: Confusion matrices comparing human-
based and model-based qrels. Tick labels 0/1/2
represent Non-relevant/Related/Relevant graded
levels. Best viewed in color.
with limited representation of low-relevance
data. LLaVA produces tightly concentrated
scores, adding complexity to post-processing,
particularly when compared to GPT-4V.
Figure 3 illustrates confusion matrices, high-
lighting LLaVA’s tendency to generate more 1
(related) judgments, fewer 2 (relevant), and 0
(non-relevant) judgments compared to GPT-
4V. We anticipate that future models will
strive to produce score distributions that better
match human annotations, thereby addressing
these challenges and limitations. Further stud-
ies (Zhuang et al., 2023) on harnessing LLMs’
relevance prediction capability are necessary.
5 Conclusion
This study delves into the capabilities of VLMs
such as CLIP, LLaVA, and GPT-4V for au-
tomating relevance judgments in image–text
retrieval evaluation. Our findings reveal that
visual-instruction-tuned LLMs outperform tra-ditional metrics like CLIPScore in aligning
with human judgments, with GPT-4V showing
promise due to its closer alignment with human
judgment distributions.
Despite these advancements and low cost of
model-based relevance annotation,4challenges
such as evaluation bias and the complexity of
mimicking human judgments remain. These
issues underscore the need for ongoing model
refinement and exploration of new techniques
to enhance the reliability and scalability of au-
tomated relevance judgments.
In conclusion, our research highlights the po-
tentialofVLMsinstreamliningmultimediacon-
tent creation while also pointing to the critical
areas requiring further investigation. The path
toward fully automated relevance judgment is
complex, necessitating continued collaborative
efforts in the research community to harness
the full potential of VLMs in this domain.
Acknowledgements
This research was supported in part by the
Canada First Research Excellence Fund and
the Natural Sciences and Engineering Research
Council (NSERC) of Canada.
References
Federico Betti, Jacopo Staiano, Lorenzo Baraldi,
Rita Cucchiara, and Niculae Sebe. 2023. Let’s
ViCE! Mimicking human cognitive behavior in
image generation evaluation. In Proceedings of
the 31st ACM International Conference on Mul-
timedia, page 9306–9312.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sas-
try, Amanda Askell, et al. 2020. Language mod-
els are few-shot learners. In Advances in Neu-
ral Information Processing Systems , volume 33,
pages 1877–1901.
David Chan, Austin Myers, Sudheendra Vijaya-
narasimhan, David Ross, and John Canny. 2023a.
IC3: Image captioning by committee consensus.
InProceedings of the 2023 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 8975–9003.
DavidMChan, SuzannePetryk, JosephEGonzalez,
Trevor Darrell, and John Canny. 2023b. CLAIR:
Evaluating image captions with large language
models. In Proceedings of the 2023 Conference
4The cost of using GPT-4V API for the experiments is
around USD 150.

on Empirical Methods in Natural Language Pro-
cessing, pages 13638–13646.
Cyril W Cleverdon. 1960. The ASLIB cranfield
research project on the comparative efficiency
of indexing systems. In ASLIB Proceedings , vol-
ume 12, pages 421–431.
Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao
Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, and
Jun Xu. 2023. LLMs may dominate informa-
tion access: Neural retrievers are biased to-
wards LLM-generated texts. arXiv preprint
arXiv:2310.20501 .
Laura Dietz and Jeff Dalton. 2020. Humans op-
tional? Automatic large-scale test collections
for entity, passage, and entity-passage retrieval.
Datenbank-Spektrum , 20(1):17–28.
GuglielmoFaggioli, LauraDietz, CharlesLAClarke,
Gianluca Demartini, Matthias Hagen, Claudia
Hauff, Noriko Kando, Evangelos Kanoulas, Mar-
tin Potthast, Benno Stein, et al. 2023. Perspec-
tives on large language models for relevance judg-
ment. In Proceedings of the 2023 ACM SIGIR
International Conference on Theory of Informa-
tion Retrieval , pages 39–50.
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan
Le Bras, and Yejin Choi. 2021. CLIPScore: A
reference-free evaluation metric for image cap-
tioning. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 7514–7528.
Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang,
Mari Ostendorf, Ranjay Krishna, and Noah A.
Smith. 2023. TIFA: Accurate and interpretable
text-to-image faithfulness evaluation with ques-
tion answering. In 2023 IEEE/CVF Interna-
tional Conference on Computer Vision , pages
20349–20360.
Jin-Hwa Kim, Yunji Kim, Jiyoung Lee, Kang Min
Yoo, and Sang-Woo Lee. 2022. Mutual informa-
tion divergence: A unified metric for multimodal
generative models. In Advances in Neural In-
formation Processing Systems , volume 35, pages
35072–35086.
Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand,
Eric Zelikman, Meredith Ringel Morris, and
Christopher Potts. 2022. Context matters for im-
age descriptions for accessibility: Challenges for
referenceless evaluation metrics. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 4685–4697.
Elisa Kreiss*, Eric Zelikman*, Christopher Potts,
and Nick Haber. 2023. ContextRef: Evaluat-
ing referenceless metrics for image description
generation. arXiv preprint arxiv:2309.11710 .Haotian Liu, Chunyuan Li, Yuheng Li, and
Yong Jae Lee. 2023a. Improved baselines with vi-
sual instruction tuning. In NeurIPS 2023 Work-
shop on Instruction Tuning and Instruction Fol-
lowing.
Haotian Liu, Chunyuan Li, Qingyang Wu, and
Yong Jae Lee. 2023b. Visual instruction tuning.
InAdvances in Neural Information Processing
Systems.
Yang Liu, Dan Iter, Yichong Xu, Shuohang
Wang, Ruochen Xu, and Chenguang Zhu.
2023c. GPTEval: NLG evaluation using GPT-4
with better human alignment. arXiv preprint
arXiv:2303.16634 .
Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang,
and William Yang Wang. 2023. LLMScore: Un-
veilingthepoweroflargelanguagemodelsintext-
to-image synthesis evaluation. arXiv preprint
arXiv:2305.11116 .
Sean MacAvaney and Luca Soldaini. 2023. One-
shot labeling for automatic relevance estimation.
InProceedings of the 46th International ACM
SIGIR Conference on Research and Development
in Information Retrieval , page 2230–2235.
OpenAI. 2023. GPT-4V(ision) system card.
Nicholas Pangakis, Samuel Wolken, and Neil
Fasching. 2023. Automated annotation with gen-
erative ai requires validation. arXiv preprint
arXiv:2306.00176 .
Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor
Darrell, and Anna Rohrbach. 2021. Benchmark
for compositional text-to-image synthesis. In
Thirty-fifth Conference on Neural Information
Processing Systems Datasets and Benchmarks
Track (Round 1) .
Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,
Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu,
Donald Metzler, Xuanhui Wang, et al. 2023.
Large language models are effective text rankers
with pairwise ranking prompting. arXiv preprint
arXiv:2306.17563 .

Donald Metzler, Xuanhui Wang, et al. 2023.
Large language models are effective text rankers
with pairwise ranking prompting. arXiv preprint
arXiv:2306.17563 .
Alec Radford, Jong Wook Kim, Chris Hallacy,
Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin,
Jack Clark, et al. 2021. Learning transferable
visual models from natural language supervision.
InInternational Conference on Machine Learn-
ing, pages 8748–8763.
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael
Pritch, Michael Rubinstein, and Kfir Aberman.
2023. DreamBooth: Fine tuning text-to-image
diffusion models for subject-driven generation.
InProceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages
22500–22510.

Florian Schneider, Özge Alaçam, Xintong Wang,
and Chris Biemann. 2021. Towards multi-modal
text-image retrieval to improve human reading.
InProceedings of the 2021 Conference of the
North American Chapter of the Association for
Computational Linguistics: Student Research
Workshop .
Janvijay Singh, Vilém Zouhar, and Mrinmaya
Sachan. 2023. Enhancing textbooks with visuals
from the web for improved learning. In Pro-
ceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing , pages
11931–11944.
Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang
Wang, Pengjie Ren, Zhumin Chen, Dawei Yin,
and Zhaochun Ren. 2023. Is ChatGPT good
at search? Investigating large language models
as re-ranking agents. In Proceedings of the 2023
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 14918–14937.
Ellen M Voorhees. 1998. Variations in relevance
judgments and the measurement of retrieval ef-
fectiveness. In Proceedings of the 21st annual in-
ternational ACM SIGIR conference on Research
and development in information retrieval , pages
315–323.
Jheng-Hong Yang, Carlos Lassance, Rafael Sam-
paio De Rezende, Krishna Srinivasan, Miriam
Redi, Stéphane Clinchant, and Jimmy Lin. 2023.
AToMiC: An image/text retrieval test collection
to support multimedia content creation. In Pro-
ceedings of the 46th International ACM SIGIR
conference on research and development in infor-
mation retrieval , pages 2975–2984.
Mert Yuksekgonul, Federico Bianchi, Pratyusha
Kalluri, Dan Jurafsky, and James Zou. 2023.
When and why vision-language models behave
like bags-of-words, and what to do about it?
InThe Eleventh International Conference on
Learning Representations .
Andy Zeng, Maria Attarian, Brian Ichter,
Krzysztof Marcin Choromanski, Adrian Wong,
Stefan Welker, Federico Tombari, Aveek Purohit,
Michael S Ryoo, Vikas Sindhwani, Johnny Lee,
Vincent Vanhoucke, and Pete Florence. 2023. So-
cratic models: Composing zero-shot multimodal
reasoning with language. In The Eleventh In-
ternational Conference on Learning Representa-
tions.
Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu,
Le Yan, Xuanhui Wang, and Michael Berdersky.
2023. Beyond yes and no: Improving zero-shot
LLM rankers via scoring fine-grained relevance
labels.arXiv preprint arXiv:2310.14122 .

