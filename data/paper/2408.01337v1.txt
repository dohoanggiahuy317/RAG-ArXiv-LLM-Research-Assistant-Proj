MUCHOMUSIC: EVALUATING MUSIC UNDERSTANDING IN
MULTIMODAL AUDIO-LANGUAGE MODELS
Benno Weck*1Ilaria Manco*2Emmanouil Benetos2
Elio Quinton3George Fazekas2Dmitry Bogdanov1
1Universitat Pompeu Fabra,2Queen Mary University of London,3Universal Music Group
* equal contribution benno.weck01@estudiant.upf.edu, i.manco@qmul.ac.uk
ABSTRACT
Multimodal models that jointly process audio and language
hold great promise in audio understanding and are increas-
ingly being adopted in the music domain. By allowing
users to query via text and obtain information about a given
audio input, these models have the potential to enable a
variety of music understanding tasks via language-based
interfaces. However, their evaluation poses considerable
challenges, and it remains unclear how to effectively as-
sess their ability to correctly interpret music-related inputs
with current methods. Motivated by this, we introduce
MuChoMusic, a benchmark for evaluating music under-
standing in multimodal language models focused on audio.
MuChoMusic comprises 1,187 multiple-choice questions,
all validated by human annotators, on 644 music tracks
sourced from two publicly available music datasets, and
covering a wide variety of genres. Questions in the bench-
mark are crafted to assess knowledge and reasoning abili-
ties across several dimensions that cover fundamental mu-
sical concepts and their relation to cultural and functional
contexts. Through the holistic analysis afforded by the
benchmark, we evaluate five open-source models and iden-
tify several pitfalls, including an over-reliance on the lan-
guage modality, pointing to a need for better multimodal
integration. Data and code are open-sourced.1
1. INTRODUCTION
Combining the success of large language models (LLMs)
with new advances in machine perception that have led to
image, audio and video foundation models [1], multimodal
LLMs are becoming influential across many fields [2–6].
Recently, models of this kind have started supporting the
audio modality, with a subset also being applied to the mu-
sic domain [7–13]. We refer to such models exhibiting au-
dio understanding capabilities as Audio LLMs . In a nut-
1Data: https://doi.org/10.5281/zenodo.12709974 ,
website: https://mulab-mir.github.io/muchomusic
© B. Weck, I. Manco, E. Benetos, E. Quinton, G. Fazekas,
and D. Bogdanov. Licensed under a Creative Commons Attribution 4.0
International License (CC BY 4.0). Attribution: B. Weck, I. Manco,
E. Benetos, E. Quinton, G. Fazekas, and D. Bogdanov, “MuChoMusic:
Evaluating Music Understanding in Multimodal Audio-Language Mod-
els”, in Proc. of the 25th Int. Society for Music Information Retrieval
Conf., San Francisco, United States, 2024.
Figure 1 .Multiple-choice questions in MuChoMusic
have four answer options of different levels of difficulty.
shell, Audio LLMs consist of pre-trained LLMs whose in-
put space has been expanded beyond text to include tokens
from an audio encoder, granting them the ability to pro-
duce language outputs that require understanding of both
modalities. While promising, these models also inherit
many of the limitations of LLMs and little attention has
so far been given to their evaluation. In most cases, current
automatic evaluation relies on match-based metrics which
measure the semantic or lexical overlap between model
outputs and reference text. However, many works have
pointed out deficiencies in this approach [14], which fails
to capture the large space of acceptable language outputs
admitted by open-ended tasks. For example, the question
“What are some possible uses for this music in a film or
TV show?” may be suitably answered in many different
ways. Secondly, automatic music understanding evalua-
tion via language is only supported by a handful of human-
annotated datasets [15–17], of which only one [15] has
widely been adopted in the context of Audio LLMs. In-
stead, many prior works have created a variety of ad-hoc
datasets built upon synthetically generated captions from

widely been adopted in the context of Audio LLMs. In-
stead, many prior works have created a variety of ad-hoc
datasets built upon synthetically generated captions from
tags and other metadata [8, 9, 18] to train and evaluate
their models, without explicit data validation mechanisms,
which raises questions around their reliability. These three
key issues, lack of standardisation, the inadequacy of text
generation metrics, and the quality of annotations in cur-
rent datasets, pose obstacles to the development of the field
and has prompted some to resort to human evaluations [7],
which can be costly and are hard to scale and reproduce.arXiv:2408.01337v1  [cs.SD]  2 Aug 2024

In this paper, we present MuChoMusic , the first bench-
mark for evaluating music understanding in Audio LLMs.
We design a test that is easy to evaluate by collecting a set
of multiple-choice (MC) questions that are scrutinised by
human annotators, on which simple classification accuracy
can be obtained as a reliable indicator of music understand-
ing over the categories covered by the test. The content
of our benchmark is intended to be challenging, grounded
in factual music knowledge, and tests core understanding
and reasoning skills across several dimensions such as mu-
sic theory, musical styles and traditions, historical and so-
cial contexts, structure and expressive analysis. Using Mu-
ChoMusic, we carry out a comprehensive evaluation of five
existing Audio LLMs with music understanding capabili-
ties. We envision that MuChoMusic will complement prior
efforts to standardise music understanding evaluation [19–
21] by including this new family of models and steering
their early development towards robust progress.
2. RELATED WORK
In the music domain, Audio LLMs are commonly evalu-
ated by assessing their text output in the context of a given
task defined by an instruction template. Tasks are either
designed to test whether the model is able to recognise
predefined musical properties such as key ( “What is the
key of this song?” ), genre, instrumentation, etc., or they
probe for outputs that encompass a variety of musical con-
cepts and that more closely resemble the dialogue format
typical of chatbots. Tasks that fall under the former usu-
ally mirror canonical MIR tasks and their evaluation lever-
ages standard metrics and benchmarks from the MIR liter-
ature. Evaluation of tasks that require broader understand-
ing follows instead less established protocols. Prior works
on Audio LLMs most commonly tackle this via two tasks,
music captioning ( “Describe the contents of the provided
audio in detail” ) [7–9, 11] and music question answering
(“What are some possible uses for this music in a film or
TV show?” ) [8, 9]. To perform this kind of evaluation, the
authors in [7, 9, 11] make use of the MusicCaps dataset
[15], while others [8, 9] employ ad-hoc evaluation datasets
created with the help of LLMs. In particular, Liu et al.
[8] and Deng et al. [9] propose their own datasets for mu-
sic question answering, MusicQA and MusicInstruct re-
spectively. These are derived from captions in the Music-
Caps dataset or tags from the MagnaTagaTune dataset [22]
(MusicQA only), by augmenting them into music-question
pairs via pre-trained LLMs. Similarly to these works, we
also leverage LLMs to generate our set of questions and
answers, but we follow a multiple-choice format to en-
sure meaningful evaluation and validate all generated data
through human annotators to guarantee high data quality.
Finally, we note that concurrent work also proposes
evaluation benchmarks for music understanding in LLM-
based models [23–25], but these all differ from our work in
significant ways: MuChin [23] includes only text in Chi-
nese and does not follow a multiple-choice format, while
both MusicTheoryBench (MTB) and ZIQI-Eval focus on
the symbolic domain and address the evaluation of text-Benchmark Size Source(s) Audio HC MC
MusicQA [8] 4.5k MagnaTagATune ✓ ✗ ✗
MusicInstruct [9] 61k MusicCaps ✓ ✗ ✗
ZIQI-Eval [25] 14k Music books ✗ ✗ ✓
MTB [24] 372 (human-written) ✗✓✓
AIR-Bench [26] 400 MusicCaps ✓ ✗✓
MuChin [23] 1k unknown ✓ ✓ ✗
MuChoMusic 1.2k MusicCaps, SDD ✓ ✓ ✓
Table 1 .Comparison of MuChoMusic to existing
benchmarks. HC: human-curated, MC: multiple-choice.
based LLMs. AIR-Bench [26] includes a small subset of
music-related tasks, but puts its focus on audio understand-
ing more generally. We provide an overview of key differ-
ences with other benchmarks in Table 1.
3. MUCHOMUSIC
Through MuChoMusic, we aim to alleviate three promi-
nent issues in the evaluation of music understanding in
Audio LLMs: a lack of standardisation, the inadequacy

3. MUCHOMUSIC
Through MuChoMusic, we aim to alleviate three promi-
nent issues in the evaluation of music understanding in
Audio LLMs: a lack of standardisation, the inadequacy
of existing text generation metrics, and the quality of cur-
rent evaluation sets. We address the first two by adopt-
ing a multiple-choice format, while our methodical gener-
ation and validation procedure attends to the third issue by
grounding the data in human-written descriptions and en-
suring that the final questions and answers are correct and
contextually relevant, as judged by multiple annotators.2
3.1 Overview
MuChoMusic consists of 1,187 multiple-choice questions
aimed at testing the understanding of 644 unique music
tracks sourced from the MusicCaps [15] and the Song De-
scriber Dataset [16]. We adopt a multiple-choice format
in order to standardise evaluation and follow widespread
practice in LLM-centric evaluation scenarios [28–31]. As
illustrated in Figure 1, each question has four possible an-
swers. One option is the correct answer, the other three
are distractors. Inspired by [32], we structure these as fol-
lows: one does not fit the track of interest but is related to
the question ( incorrect but related ), one correctly fits the
audio, but does not address the question ( correct but un-
related ), and one does not apply to the track and is also
irrelevant to the question ( incorrect and unrelated ).
Evaluation dimensions MuChoMusic is built from a di-
verse set of musical works and their detailed descriptions,
and serves as a foundation for evaluating Audio LLMs
across various dimensions of music comprehension. To de-
lineate the specific evaluation dimensions encompassed by
our benchmark, we develop a taxonomy consisting of two
primary categories: knowledge andreasoning .3Each cat-
egory is further divided into several dimensions, informed
by insights from national music education programs and
existing research on music folksonomies [33]. This struc-
tured approach allows us to assess the depth and breadth
2We provide a datasheet [27] with details about the dataset content,
collection and validation in the online supplementary materials.
3We include the full taxonomy in the online supplementary materials.

Knowledge
Reasoning
Instrumentation
Performance
Metre and Rhythm
Sound Texture
Melody
Harmony
Structure
Mood &
Expression
Genre & Style
Functional Context
Temporal Relations
Lyrics
Cultural ContextFigure 2 .Distribution of evaluation dimensions covered
by MuChoMusic across knowledge and reasoning.
of music-related competencies systematically, offering a
holistic view of models’ capabilities in the music domain.
In the knowledge category, questions probe a model’s
ability to recognise pre-acquired knowledge across various
musical aspects: (i) melody, (ii) harmony, (iii) metre and
rhythm, (iv) instrumentation, (v) sound texture, (vi) per-
formance, and (vii) structure. Questions that test reason-
ingare instead designed to require the synthesis and ana-
lytical processing of multiple musical concepts: (i) mood
and expression, (ii) temporal relations between elements,
(iii) interpretation of lyrics, (iv) genre and style, (v) histor-
ical and cultural context, and (vi) functional context. An
example of reasoning might involve using an understand-
ing of tempo, chord quality, and instrumentation in concert
to ascertain the mood of a music piece. Each question can
cover multiple dimensions and their categorisation is ob-
tained automatically, as described in Section 3.2. Figure 2
shows the coverage of the two categories and their respec-
tive dimensions within the benchmark. Over half the ques-
tions test at least one aspect of musical knowledge, such as
features relating to instrumentation or performance char-
acteristics, while 44% are dedicated to probing reasoning
skills. While the distribution of dimensions within each
category is not balanced, we note that this reflects the dis-
tribution of different musical concepts within music cap-
tions [16], resulting in categories such as instrumentation,
mood and genre appearing more frequently.
3.2 Dataset construction
To build our dataset, we automatically transform human-
written music captions into multiple-choice questions.
These are then carefully validated by multiple human an-
notators, alongside the associated audio, in order to fil-
ter out invalid, ambiguous or irrelevant questions resulting
from inaccuracies or hallucinations in the model output.
Data sources We source our data from music caption
datasets as we aim for elaborate and linguistically diverse
information about the music. Currently, only two caption-
ing datasets provide sufficiently detailed music descrip-
tions, namely the Song Describer Dataset (SDD) and Mu-
Figure 3 .QA generation and validation pipeline . Ex-
ample shown here is from MusicCaps [15].
sicCaps. SDD contains 2-minute-long music clips with
single-sentence captions crowd-sourced from music enthu-
siasts, while the captions in MusicCaps, describing 10-
second audio snippets, are written by professional musi-
cians. From SDD, we select all tracks that have at least
two captions, to ensure enough information is provided to
the model to be able to formulate interesting and challeng-
ing questions. While this is not possible for the MusicCaps
dataset, where only one caption is available for each track,
we note that descriptions are, on average, longer than in
SDD and designed to be more comprehensive. From the
genre-balanced subset of the MusicCaps test split, we ex-
clude all tracks for which the labels indicate a low record-
ing quality, to prevent differences in audio quality from
affecting the results. For both datasets, we employ a state-
of-the-art genre tagging model [34] to identify non-musical
tracks and to sub-sample songs from the most common
genres (e.g. rock and electronic). Through this curation
process, we select 227 unique tracks from SDD and 497
from MusicCaps. We supplement the descriptions with
short text labels taken from the dataset itself in the case
of MusicCaps and from a related dataset for SDD [35].
QA generation We generate the question-answer sets
by instructing Gemini 1.0 Pro [3] to formulate question

of MusicCaps and from a related dataset for SDD [35].
QA generation We generate the question-answer sets
by instructing Gemini 1.0 Pro [3] to formulate question
and answer options for a given human-written caption.
To leverage the model’s in-context learning capability, we
prompt it with a detailed task description and three exam-
ples of input (description and tags) and expected output. In
addition to the question and answer pairs, we ask the model
to start its output with a summary of the provided informa-
tion about the music recording and to interleave the distrac-
tor answer options with explanations of their suitability.
This way of prompting is inspired by the chain-of-thought
methodology and helps to elicit the best model responses
[36, 37]. This way, we obtain three multiple choice ques-
tions from each description on average and collect a total
of 2,091 question-answer pairs. An example of the gener-
ated questions is shown in Figure 3.
Data validation In order to ensure that questions and an-
swers in our benchmark are factually accurate, aptly writ-
ten and that each question can be correctly answered based
on the available audio, we validate all sets of questions via
human annotators. For this step, we recruit 222 partici-
pants via the Prolific platform (www.prolific.com). During

annotation, a question, the corresponding audio clip, and
all four answer options are presented to the participants in
random order, for a total of 30 to 50 question items. Par-
ticipants are then asked to select all options that correctly
answer the question or skip the question by indicating that
they are unable to provide an answer or that the question is
not valid. Following this procedure, for each question, we
collect three to five annotations, stopping early if different
annotators are in agreement. This task setup is intended to
vet questions and detect those that do not adhere to the in-
tended multiple-choice format, either because the expected
correct answer is not the only plausible option or because
any one of the distractors is more likely. Consequently, we
exclude questions from our final dataset for which i) less
than 50% of the annotations indicate the intended correct
answer or ii) more than 50% of the annotations mark any
of the disctractors as a plausible answer. The final dataset
comprises 858 questions from MusicCaps descriptions and
the remaining 329 from SDD captions.
Question categorisation Once questions are validated,
we categorise them according to our taxonomy outlined in
Section 3.1. To achieve this, we employ Gemini 1.0 Pro,
this time prompting it to automatically label each ques-
tion with one or more of the evaluation dimensions. The
prompt includes the full taxonomy including detailed de-
scriptions of all dimensions, a chain-of-thought instruc-
tion, and a single question with only the correct answer.
The produced output contains an explanation of the cate-
gories and dimensions assigned to each question.
4. BENCHMARKING WITH MUCHOMUSIC
We now demonstrate the use of our benchmark, describ-
ing our proposed evaluation protocol and metrics, and then
detailing our experiments on benchmarking Audio LLMs.
4.1 Evaluation Protocol
In multiple-choice-based evaluation, a model is provided
with a question and a set of answer options, and is then
tasked with selecting the most suitable answer. In prac-
tice, this can be accomplished in different ways [30]. In
our experiments, we adopt output-based evaluation: given
a music clip and an associated question-answer set, the lan-
guage output produced by the model is mapped to one of
the candidate options by string matching. Another com-
mon approach in MC evaluation is to determine the se-
lected answer through the conditional log likelihood scores
of the tokens forming each of the different options. While
this can help estimate uncertainty and confidence in the
model predictions, in our experiments, we explore only
the output-based setting, for three reasons: (1) this cor-
responds to real-world use of the models, as interactions
usually take the form of a conversation; (2) it has a lower
computational cost; (3) prior work has demonstrated that
sentence probabilities are not necessarily indicative of the
probabilities assigned to the answers [38]. To extract the
selected answer from the generated outputs, we match ei-
ther the option identifier ( A,B,CorD) or the full answerModel Audio encoder LLM
MusiLingo [9] MERT [40] Vicuna 7B [41]
MuLLaMa [8] MERT [40] LLaMA-2 7B [42]
M2UGen [12] MERT [40] LLaMA-2 7B [42]
SALMONN [11]BEATS [43] &
Whisper large-v2 [44]Vicuna 7B [41]
Qwen-Audio [13] Whisper large-v2 [44] Qwen 7B [45]
Table 2 .Overview of models we evaluate in our study.
text, if one and only one is given in the output.
Metrics We look at two main metrics to measure model
performance on our benchmark: accuracy and instruction
following rate (IFR). Accuracy is given by the percentage
of correctly answered questions out of the total set of ques-
tions. IFR is given by the percentage of generated answers
that correspond to one of the given options. In both cases,
finegrained scores can be obtained by considering only the
subset of questions covering at least one of the available
evaluation dimensions shown in Figure 2.

finegrained scores can be obtained by considering only the
subset of questions covering at least one of the available
evaluation dimensions shown in Figure 2.
Adaptation An important design factor in the evaluation
of LLM-based models is adaptation [30], the process of
adapting the input to a suitable format. While the format
of the audio input is typically fixed by the model design,
text inputs allow for more flexibility and different prompt-
ing techniques have been shown to significantly influence
model’s behaviour [36, 37, 39]. Beyond simply passing
the question and answer options as the input text, corre-
sponding to zero-shot prompting , an effective alternative
strategy is to leverage few-shot in-context learning (ICL),
whereby the model is presented with a set of reference in-
puts that exemplify the task prior to being shown the ques-
tion of interest. We experiment with in-context learning
in our experiments, providing between 0 and 5 examples
in the text input. In the interest of standardisation and to
ensure a fair comparison between the models, unless oth-
erwise specified, we keep the prompt selection fixed in our
final experiments, following an initial exploration.
4.2 Models
In our evaluation, we consider three music-specific mod-
els, MuLLaMA [8], MusiLingo [9], and M2UGen [12],
and two general-audio LLMs which can be applied to mu-
sic, as reported in their respective papers, SALMONN
[11] and Qwen-Audio [13]. To the best of our knowl-
edge, these are all the existing Audio LLMs which can be
applied to music and for which open-source weights are
available. These all share a similar architectural design and
are composed of a backbone LLM, an audio encoder and
a lightweight learnable adapter module to align embed-
dings produced by the audio encoder to the input space of
the LLM, based on either the LLaMA-adapter [46] (MuL-
LaMA, MusiLingo, M2UGen) or a Q-Former network [47]
(SALMONN). An overview of the backbones used in each
model is provided in Table 2. All systems are trained via
instruction tuning [39, 48] and all employ a combination
of different instruction datasets, often in multiple training

ModelAccuracy IFR
All Knowledge Reasoning All
MusiLingo [9] 21.1 22.0 19.2 71.6
MuLLaMa [8] 32.4 32.3 31.3 79.4
M2UGen [12] 42.9 44.9 41.2 96.4
SALMONN [11] 41.8 41.0 43.3 99.8
Qwen-Audio [13] 51.4 51.1 51.0 89.7
Random guessing 25.0 25.0 25.0 100.0
Table 3 . Overall benchmarking results .
stages including pre-training and fine-tuning. For all mod-
els, we follow the official implementation and use default
inference settings. We repeat all experiments 3 times, ran-
domly shuffling the order in which answer options are pre-
sented, and report average performance across all runs.
5. RESULTS AND DISCUSSION
In this section, we first presents findings from our bench-
marking experiments, with the goal of elucidating the cur-
rent state of music understanding in Audio LLMs. We then
illustrate how MuChoMusic can be used to derive new in-
sights via a diagnostic analysis, and discuss key takeaways.
5.1 Benchmarking Results
We report results for all models in Table 3, showing the
overall accuracy score alongside detailed scores on knowl-
edge and reasoning questions, and the instruction follow-
ing rate (IFR). Figure 4 presents a breakdown of accu-
racy scores along all reasoning and knowledge dimen-
sions. Unless otherwise specified, we show one-shot per-
formance for all models, as we find this to be the over-
all optimal setting, as we discuss in more detail in Sec-
tion 5.2. From this, we observe that current models gener-
ally perform poorly across all settings and along all eval-
uation dimensions. Among these, Qwen-Audio stands out
with a score of 51.4%. Surprisingly, with the exception
of M2UGen, music-specialised models generally perform
worse than general-audio ones, in some cases performing
only marginally above or even below random performance.
As evidenced by the IFR, these models struggle to output
answers in the correct format, which in turn negatively im-
pacts their accuracy score. As shown later in Section 5.3,
we find that, when none of the answer options is selected
by the model, this is often due to auditory hallucinations ,
language hallucinations ortraining biases .
5.2 Analysis and Discussion
We now investigate factors influencing performance along
different axes by using our benchmark as a diagnostic tool.
Are models sensitive to prompts? We first study the ef-
fect of varying the number of in-context examples. As
shown in Figure 5, providing a single example is occa-
sionally beneficial to accuracy and IFR, but with both the
difference magnitude and overall impact differing between
models. Additionally, this trend does not hold after the
Figure 4 .Finegrained accuracy across evaluation dimen-
sions in knowledge (labelled in blue) and reasoning (red).
one-shot setting, and we see no consistent improvement
when using a larger number of examples. Interestingly,
we observe that, for M2UGen, Qwen-Audio and MuL-
LaMa, changes in accuracy from zero- to one-shot prompts
are accompanied by a reduction in variance, suggesting
that ICL can help minimise variability in the model out-
put. While we do not explore this in our experiments, we
also hypothesise that the advantages of ICL may become
more prominent through multimodal few-shot prompting
[49, 50], which we leave for future work.
How do models respond to different distractors?
Next, we shift our attention to examining how distrac-
tors in our benchmark influence the difficulty of the task.
To this end, we ablate answer options corresponding to
the different kinds of distractors described in Section 3.2,
and present the model with only two or three answer op-
tions. In Figure 6(a) we show how performance is affected
when using only one distractor alongside the correct op-
tion, always randomising their order. From this, we ob-
serve that the two distractors containing information which
is not related to the question (CU and IU) have a similar
effect, while including the incorrect but related (IR) op-
tion consistently makes the task more challenging. This

is not related to the question (CU and IU) have a similar
effect, while including the incorrect but related (IR) op-
tion consistently makes the task more challenging. This
phenomenon persists when adding a second distractor (not
shown here), with combinations which include IR invari-
ably leading to worse performance. Intuitively, the two
unrelated options can be ruled out based on the text in-
put only, while selecting the correct answer between two
options that appear relevant requires engaging multimodal
understanding to relate information in the audio content to
the text in the question. Crucially, this indicates that mod-
els particularly struggle to discern between options that are
equally plausible based on the text input only, suggesting
that less attention is given to the audio content. This forms
the basis of our hypothesis that current Audio LLMs are
characterised by a strong language bias, leading to poor
performance in tasks that are more audio-dependent. We
test this hypothesis in the next section.

Figure 5 .Effect of the number of in-context examples
on accuracy (left) and instruction-following rate (right).
Do models actually pay attention to the audio? In or-
der to verify whether the audio input is effectively being
ignored or is overshadowed by its text counterpart, we de-
vise a simple test, which we call audio attention test , where
we replace the audio clip corresponding to a given ques-
tion with either white Gaussian noise or a randomly cho-
sen track from the dataset. In order to pass this test, a
model should display a statistically significant drop in per-
formance when either form of audio perturbation is used,
compared to its baseline performance. We showcase re-
sults on this test in Figure 6(b). From this, we clearly see
that, with the exception of SALMONN and Qwen-Audio,
all models fail the audio attention test, and the severity of
this failure is often negatively correlated to their overall
performance on the benchmark (see Table 3). This con-
firms that current Audio LLMs are biased towards textual
information, often choosing answers that score well under
their language prior. Additionally, it provides an explana-
tion for their low performance on the benchmark, as this
is effectively bounded by the maximum score they can at-
tain mostly based on the language input. We argue that this
constitutes a major pitfall in the design and training proce-
dure of these models, which results in music understand-
ing abilities that do not match the expected performance,
as obtained through prior evaluations.
5.3 Failure Modes
While the core goal of our benchmark is to provide stan-
dardised automatic evaluation to objectively measure gen-
eral music understanding capabilities, we argue that it can
also constitute a useful tool for qualitative assessment. We
showcase three examples here, focusing on the two lowest-
performing models. While this is not an exhaustive analy-
sis, these examples offer a bird’s-eye view of how language
pre-training biases percolate through multimodal training,
resulting in failures to attend to the inputs in our evalua-
tion. To describe these, we borrow terminology from [51].
Auditory hallucination One of the ways models fail to
provide a suitable answer falls under the category of audi-
tory hallucination , whereby a response includes references
to musical elements that are not present in the audio. For
example, when asked about an accompaniment instrument,
Figure 6 . (a) Effect of using different types of distrac-
tors: models tend to perform worst when tasked with dis-
tinguishing between two related answers. (b) Audio at-
tention test : only some models display a significant drop
in performance when provided with incorrect audio inputs.
For these experiments, we adopt zero-shot prompting.
models with this type of hallucination may ignore any suit-
able option provided ( “acoustic guitar” or“strings” ), in-
stead answering “The song is accompanied by a piano. ” ,
when the audio clip clearly contains no piano.
Language hallucination Another instance of hallucina-
tion concerns mundane statements that deviate from the
topic of the question altogether. Among others, an ob-
served case of this failure mode is a statement of the form
“The song has a clear and coherent rhythm structure” to a
question specifically asking about the “type of drum beat” .
Training data bias The last failure mode we encounter
is related to a bias towards frequent patterns occurring in
the training data. While some of the benchmarked mod-
els undergo a stage of training that includes instruction-
tuning examples with questions and answers, occasionally
they still produce trivial outputs. For example, when asked
“What is the intended purpose of this song?” , a model with
this type of bias may answer “The intended purpose of this
song is not mentioned in the caption” . Reviewing Mu-
sicQA, used in training MuLLaMa and MusiLingo, reveals
that a high number of the LLM-generated training exam-

song is not mentioned in the caption” . Reviewing Mu-
sicQA, used in training MuLLaMa and MusiLingo, reveals
that a high number of the LLM-generated training exam-
ples mention similar phrases, thus likely biasing the model
towards this type of uninformative but highly likely output.
6. CONCLUSION
We have presented MuChoMusic, a multiple-choice mu-
sic question answering benchmark designed to test mu-
sic understanding in Audio LLMs. From an evaluation of
five state-of-the-art systems, we find that our benchmark
acts as a challenging and informative test, and that current
models do not yet leverage both the audio and text modal-
ities fully. All questions in our benchmark are synthesised
from human-written music descriptions and manually re-
viewed to guarantee high data quality. A categorisation of
the questions highlights that MuChoMusic offers a broad
coverage of areas targeted by current models, and addition-
ally pinpoints gaps that could guide future developments
in the field. While we demonstrate that our approach leads
to new insights, we note that the multiple-choice format

presents some limitations [52]. Therefore evaluation on
MuChoMusic should be complemented via further bench-
marking efforts to address additional aspects of music un-
derstanding through different tasks and formats.
7. ETHICS STATEMENT
7.1 Annotator welfare
Prior to participation, the annotation experiment described
in Section 3.2 was approved by the Queen Mary Ethics
of Research Committee to ensure alignment with ethical
guidelines and protections for human subjects in research.
We did not collect any personal data from our annotators,
safeguarding their privacy and confidentiality. Annotators
were fully informed about the objectives of the research,
the nature of their tasks, and the use of their annotations,
underpinning their informed consent before contributing to
the project. In an effort to provide a fair compensation for
their contributions, annotators were paid £9 per hour.
7.2 Biases and fairness
In constructing the MuChoMusic benchmark, our data col-
lection strategy included sourcing music tracks from a va-
riety of backgrounds, acknowledging the inherent chal-
lenges in representing the rich diversity of global music
cultures within our dataset. We recognise that our initia-
tive does not fully balance the benchmark across all gen-
res, languages, and cultural backgrounds, and annotations
were conducted exclusively in English due to logistical
constraints, highlighting areas for future expansion and im-
provement.
8. ACKNOWLEDGEMENTS
IM is a research student at the UKRI Centre for Doc-
toral Training in Artificial Intelligence and Music, sup-
ported jointly by UK Research and Innovation [grant num-
ber EP/S022694/1] and Universal Music Group. EB is sup-
ported by RAEng/Leverhulme Trust research fellowship
LTRF2223-19-106.
9. REFERENCES
[1] Rishi Bommasani et al. “On the opportunities and
risks of foundation models”. In: arXiv preprint
arXiv:2108.07258 (2021).
[2] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,
Shaohan Huang, Shuming Ma, Qixiang Ye, and
Furu Wei. “Grounding Multimodal Large Language
Models to the World”. In: The Twelfth International
Conference on Learning Representations . 2024.
[3] Gemini Team et al. “Gemini: a family of highly
capable multimodal models”. In: arXiv preprint
arXiv:2312.11805 (2023).
[4] OpenAI et al. “Gpt-4 technical report”. In: arXiv
preprint arXiv:2303.08774 (2023).[5] Jean-Baptiste Alayrac et al. “Flamingo: a Visual
Language Model for Few-Shot Learning”. In: Ad-
vances in Neural Information Processing Systems .
2022.
[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,
Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,
and Jingren Zhou. “Qwen-vl: A frontier large
vision-language model with versatile abilities”. In:
arXiv preprint arXiv:2308.12966 (2023).
[7] Josh Gardner, Simon Durand, Daniel Stoller, and
Rachel Bittner. “LLark: A Multimodal Instruction-
Following Language Model for Music”. In: Pro-
ceedings of the 41st International Conference on
Machine Learning (ICML) . 2024.
[8] Shansong Liu, Atin Sakkeer Hussain, Chen-
shuo Sun, and Ying Shan. “Music Understanding
LLaMA: Advancing Text-to-Music Generation with
Question Answering and Captioning”. In: ICASSP
2024 - 2024 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) .
2024.
[9] Zihao Deng, Yinghao Ma, Yudong Liu, Rongchen
Guo, Ge Zhang, Wenhu Chen, Wenhao Huang, and
Emmanouil Benetos. “MusiLingo: Bridging Music
and Text with Pre-trained Language Models for Mu-
sic Captioning and Query Response”. In: Findings
of the Association for Computational Linguistics:
NAACL 2024 . Association for Computational Lin-
guistics, 2024.
[10] Soham Deshmukh, Benjamin Elizalde, Rita Singh,
and Huaming Wang. “Pengi: An Audio Language
Model for Audio Tasks”. In: Thirty-seventh Con-
ference on Neural Information Processing Systems .
2023.
[11] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao
Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and
Chao Zhang. “SALMONN: Towards Generic Hear-

ference on Neural Information Processing Systems .
2023.
[11] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao
Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and
Chao Zhang. “SALMONN: Towards Generic Hear-
ing Abilities for Large Language Models”. In: The
Twelfth International Conference on Learning Rep-
resentations . 2024.
[12] Atin Sakkeer Hussain, Shansong Liu, Chenshuo
Sun, and Ying Shan. “M2UGen: Multi-modal Mu-
sic Understanding and Generation with the Power
of Large Language Models”. In: arXiv preprint
arXiv:2311.11255 (2023).
[13] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian
Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou,
and Jingren Zhou. “Qwen-audio: Advancing uni-
versal audio understanding via unified large-
scale audio-language models”. In: arXiv preprint
arXiv:2311.07919 (2023).
[14] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin
Shao, Wanrong Zhu, Anas Awadalla, Joshua P.
Gardner, Rohan Taori, and Ludwig Schmidt.
“VisIT-Bench: A Dynamic Benchmark for Eval-
uating Instruction-Following Vision-and-Language

Models”. In: Thirty-seventh Conference on Neu-
ral Information Processing Systems Datasets and
Benchmarks Track . 2023.
[15] Andrea Agostinelli et al. “Musiclm: Generating mu-
sic from text”. In: arXiv preprint arXiv:2301.11325
(2023).
[16] Ilaria Manco et al. “The Song Describer Dataset: a
Corpus of Audio Captions for Music-and-Language
Evaluation”. In: Machine Learning for Audio Work-
shop at NeurIPS 2023 . 2023.
[17] Daniel McKee, Justin Salamon, Josef Sivic, and
Bryan Russell. “Language-Guided Music Recom-
mendation for Video via Prompt Analogies”. In:
2023 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) . IEEE, 2023.
[18] SeungHeon Doh, Keunwoo Choi, Jongpil Lee, and
Juhan Nam. “LP-MusicCaps: LLM-Based Pseudo
Music Captioning”. In: Proceedings of the 24th In-
ternational Society for Music Information Retrieval
Conference, ISMIR 2023 . 2023.
[19] Rachel M. Bittner, Magdalena Fuentes, David Ru-
binstein, Andreas Jansson, Keunwoo Choi, and
Thor Kell. “mirdata: Software for Reproducible Us-
age of Datasets”. In: Proceedings of the 20th In-
ternational Society for Music Information Retrieval
Conference, ISMIR 2019 . 2019.
[20] Ruibin Yuan et al. “MARBLE: Music Audio Rep-
resentation Benchmark for Universal Evaluation”.
In:Thirty-seventh Conference on Neural Informa-
tion Processing Systems Datasets and Benchmarks
Track . 2023.
[21] Christos Plachouras, Pablo Alonso-Jiménez, and
Dmitry Bogdanov. “mir_ref: A Representation
Evaluation Framework for Music Information Re-
trieval Tasks”. In: 37th Conference on Neural In-
formation Processing Systems (NeurIPS), Machine
Learning for Audio Workshop . 2023.
[22] Edith Law, Kris West, Michael Mandel, Mert Bay,
and J. Stephen Downie. “Evaluation of algorithms
using games: The case of music tagging”. In: Pro-
ceedings of the 10th ISMIR Conference . 2009.
[23] Zihao Wang, Shuyu Li, Tao Zhang, Qi Wang,
Pengfei Yu, Jinyang Luo, Yan Liu, Ming Xi, and
Kejun Zhang. “MuChin: A Chinese Colloquial
Description Benchmark for Evaluating Language
Models in the Field of Music”. In: arXiv preprint
arXiv:2402.09871 (2024).
[24] Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian,
Shangda Wu, Tianhao Shen, Ge Zhang, Yuhang Wu,
Cong Liu, Ziya Zhou, et al. “Chatmusician: Un-
derstanding and generating music intrinsically with
llm”. In: arXiv preprint arXiv:2402.16153 (2024).[25] Jiajia Li, Lu Yang, Mingni Tang, Cong Chen,
Zuchao Li, Ping Wang, and Hai Zhao. “The Music
Maestro or The Musically Challenged, A Massive
Music Evaluation Benchmark for Large Language
Models”. In: arXiv preprint arXiv:2406.15885
(2024).
[26] Qian Yang et al. “AIR-Bench: Benchmarking Large
Audio-Language Models via Generative Compre-
hension”. In: CoRR abs/2402.07729 (2024).
[27] Timnit Gebru, Jamie Morgenstern, Briana Vec-
chione, Jennifer Wortman Vaughan, Hanna Wallach,
Hal Daumé III, and Kate Crawford. “Datasheets for
datasets”. In: Communications of the ACM 64.12
(2021).
[28] Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. “Measuring Massive Multitask Language
Understanding”. In: 9th International Conference
on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 . 2021.
[29] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo
Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu
Chen, and Nan Duan. “Agieval: A human-centric
benchmark for evaluating foundation models”. In:
arXiv preprint arXiv:2304.06364 (2023).
[30] Percy Liang et al. “Holistic Evaluation of Language
Models”. In: Transactions on Machine Learning Re-
search (2023).
[31] Aarohi Srivastava et al. “Beyond the Imitation
Game: Quantifying and extrapolating the capabili-
ties of language models”. In: Transactions on Ma-
chine Learning Research (2023).
[32] Yevgeni Berzak, Jonathan Malmaud, and Roger
Levy. “STARC: Structured Annotations for Reading
Comprehension”. In: Proceedings of the 58th An-
nual Meeting of the Association for Computational

[32] Yevgeni Berzak, Jonathan Malmaud, and Roger
Levy. “STARC: Structured Annotations for Reading
Comprehension”. In: Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics . Association for Computational Linguis-
tics, 2020.
[33] Mohamed Sordo, Fabien Gouyon, Luís Sarmento,
Òscar Celma, and Xavier Serra. “Inferring Semantic
Facets of a Music Folksonomy with Wikipedia”. In:
Journal of New Music Research 42.4 (2013).
[34] Pablo Alonso-Jiménez, Xavier Serra, and Dmitry
Bogdanov. “Music Representation Learning Based
on Editorial Metadata from Discogs”. In: Proceed-
ings of the 23rd International Society for Music In-
formation Retrieval Conference, ISMIR 2022, Ben-
galuru, India, December 4-8, 2022 . 2022.
[35] Dmitry Bogdanov, Minz Won, Philip Tovstogan,
Alastair Porter, and Xavier Serra. “The MTG-
Jamendo Dataset for Automatic Music Tagging”.
In:Machine Learning for Music Discovery Work-
shop, International Conference on Machine Learn-
ing (ICML) . 2019.

[36] Takeshi Kojima, Shixiang Shane Gu, Machel Reid,
Yutaka Matsuo, and Yusuke Iwasawa. “Large Lan-
guage Models are Zero-Shot Reasoners”. In: Ad-
vances in Neural Information Processing Systems .
2022.
[37] Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,
Quoc V Le, and Denny Zhou. “Chain of Thought
Prompting Elicits Reasoning in Large Language
Models”. In: Advances in Neural Information Pro-
cessing Systems . 2022.
[38] Joshua Robinson and David Wingate. “Leveraging
Large Language Models for Multiple Choice Ques-
tion Answering”. In: The Eleventh International
Conference on Learning Representations . 2023.
[39] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V Le. “Finetuned Language
Models are Zero-Shot Learners”. In: International
Conference on Learning Representations . 2022.
[40] Yizhi LI et al. “MERT: Acoustic Music Understand-
ing Model with Large-Scale Self-supervised Train-
ing”. In: The Twelfth International Conference on
Learning Representations . 2024.
[41] Wei-Lin Chiang et al. Vicuna: An Open-Source
Chatbot Impressing GPT-4 with 90%* ChatGPT
Quality . 2023.
[42] Hugo Touvron, Louis Martin, Kevin Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, et al. “Llama 2: Open foundation
and fine-tuned chat models”. In: arXiv preprint
arXiv:2307.09288 (2023).
[43] Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu,
Daniel Tompkins, Zhuo Chen, Wanxiang Che, Xi-
angzhan Yu, and Furu Wei. “BEATs: Audio Pre-
Training with Acoustic Tokenizers”. In: Proceed-
ings of the 40th International Conference on Ma-
chine Learning . V ol. 202. Proceedings of Machine
Learning Research. PMLR, 2023.
[44] Alec Radford, Jong Wook Kim, Tao Xu, Greg
Brockman, Christine Mcleavey, and Ilya Sutskever.
“Robust Speech Recognition via Large-Scale Weak
Supervision”. In: Proceedings of the 40th In-
ternational Conference on Machine Learning .
V ol. 202. Proceedings of Machine Learning Re-
search. PMLR, 2023.
[45] Jinze Bai et al. “Qwen Technical Report”. In: arXiv
preprint arXiv:2309.16609 (2023).
[46] Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou,
Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.
“LLaMA-Adapter: Efficient Fine-tuning of Large
Language Models with Zero-initialized Attention”.
In:The Twelfth International Conference on Learn-
ing Representations . 2024.[47] Yiren Jian, Chongyang Gao, and Soroush V osoughi.
“Bootstrapping Vision-Language Learning with De-
coupled Language Pre-training”. In: Thirty-seventh
Conference on Neural Information Processing Sys-
tems. 2023.
[48] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,
Alisa Liu, Noah A. Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. “Self-Instruct: Aligning Lan-
guage Models with Self-Generated Instructions”.
In:Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) . Association for Computational
Linguistics, 2023.
[49] Sivan Doveh, Shaked Perek, M Jehanzeb Mirza,
Amit Alfassy, Assaf Arbelle, Shimon Ullman, and
Leonid Karlinsky. “Towards multimodal in-context
learning for vision & language models”. In: arXiv
preprint arXiv:2403.12736 (2024).
[50] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian
Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng
Wang, Wenjuan Han, and Baobao Chang. “MMICL:
Empowering Vision-language Model with Multi-
Modal In-Context Learning”. In: The Twelfth Inter-
national Conference on Learning Representations .
2024.
[51] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,
Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang
Chen, Furong Huang, Yaser Yacoob, et al. “Hallu-
sionBench: an advanced diagnostic suite for entan-
gled language hallucination and visual illusion in
large vision-language models”. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition . 2024.
[52] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou,

large vision-language models”. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition . 2024.
[52] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou,
and Minlie Huang. “Large Language Models Are
Not Robust Multiple Choice Selectors”. In: The
Twelfth International Conference on Learning Rep-
resentations . 2024.

