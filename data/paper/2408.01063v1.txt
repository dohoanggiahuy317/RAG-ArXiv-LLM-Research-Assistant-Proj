Empirical Software Engineering manuscript No.
(will be inserted by the editor)
Leveraging Large Language Models for Mobile App
Review Feature Extraction
Quim Motger1·Alessio Miaschi2·
Felice Dell’Orletta2·Xavier Franch1·
Jordi Marco3
Received: date / Accepted: date
Abstract Mobile app review analysis presents unique challenges due to the
low quality, subjective bias, and noisy content of user-generated documents.
Extracting features from these reviews is essential for tasks such as feature
prioritization and sentiment analysis, but it remains a challenging task. Mean-
while, encoder-only models based on the Transformer architecture have shown
promising results for classification and information extraction tasks for mul-
tiple software engineering processes. This study explores the hypothesis that
encoder-only large language models can enhance feature extraction from mo-
bile app reviews. By leveraging crowdsourced annotations from an industrial
context, we redefine feature extraction as a supervised token classification
task. Our approach includes extending the pre-training of these models with
a large corpus of user reviews to improve contextual understanding and em-
ploying instance selection techniques to optimize model fine-tuning. Empirical
evaluations demonstrate that this method improves the precision and recall
of extracted features and enhances performance efficiency. Key contributions
include a novel approach to feature extraction, annotated datasets, extended
pre-trained models, and an instance selection mechanism for cost-effective fine-
tuning. This research provides practical methods and empirical evidence in
applying large language models to natural language processing tasks within
mobile app reviews, offering improved performance in feature extraction.
Keywords mobile app reviews ·feature extraction ·named-entity recogni-
tion·large language models ·extended pre-training ·instance selection
BQuim Motger
E-mail: joaquim.motger@upc.edu
1Universitat Polit` ecnica de Catalunya, Department of Service and Information Sys-
tem Engineering, Barcelona, Spain. E-mail: {joaquim.motger,xavier.franch }@upc.edu
2Institute for Computational Linguistics “A. Zampolli” (ILC-CNR), ItaliaNLP Lab, Pisa,
Italy. E-mail: {alessio.miaschi,felice.dellorletta }@ilc.cnr.it
3Universitat Polit` ecnica de Catalunya, Department of Computer Science, Barcelona,
Spain. E-mail: jordi.marco@upc.eduarXiv:2408.01063v1  [cs.CL]  2 Aug 2024

2 Quim Motger1et al.
1 Introduction
Large language models (LLMs) have become a pervasive method for redefin-
ing cognitively challenging software engineering tasks based on the natural
language processing (NLP) of textual documents (Hou et al., 2024). As prac-
titioners explore the potential of introducing these models into their day-to-
day processes, multiple key design and evaluation factors are undermined or
even neglected. This includes proper model analysis and selection (Perez et al.,
2021), exploration of optimization mechanisms (Schick and Sch¨ utze, 2021), ex-
plainability of results (Zini and Awad, 2022) and generalization of research out-
comes (Jiang et al., 2020). Ignoring these dimensions can lead to low functional
suitability, decreased performance efficiency, increased resource consumption
and lack of potential for reusability and knowledge transfer.
In the context of Requirements Engineering (RE), effective adoption and
proper use of LLMs have been elicited as key challenges for future work in the
field (Frattini et al., 2024; Ronanki et al., 2023; Fantechi et al., 2023). As a
text-based document-driven community, both generated by developers (e.g.,
textual requirements, user stories, test cases) and by users (e.g., bug reports,
software issues, app reviews), information extraction from these documents
is key to support requirements elicitation (Ronanki et al., 2023), defect re-
pair (Ferrari et al., 2018), prioritization (Malgaonkar et al., 2022), information
extraction (Sleimi et al., 2021), feedback analysis (Dalpiaz and Parente, 2019),
and release planning (McZara et al., 2015; Sharma and Kumar, 2019). Ex-
tracting descriptors, metadata, entities and categories from these documents
allows practitioners to categorize large amounts of data and build analytical
processes, which is especially useful for continuously analysing large amounts
of user-generated documents (Li et al., 2018; van Vliet et al., 2020). A partic-
ular example are opinion mining methods for mobile app reviews (Dabrowski
et al., 2022), for which three major tasks can be identified from the literature:
(1) review classification (e.g., bug report ,feature request (Maalej and Nabil,
2015)); (2) sentiment analysis (e.g., positive ,negative (Zhang et al., 2014));
and (3) app feature extraction (e.g., send message ,make video call (Johann
et al., 2017)). Concerning the latter, the automatic extraction of mobile app
features (i.e., functions or characteristics of a mobile app from the user per-
spective) supports multiple feature-oriented decision-making tasks, including
feature prioritization (Scalabrino et al., 2019) and feature-oriented sentiment
analysis (Guzman and Maalej, 2014).
While some feature extraction methods have been proposed by leveraging
syntactic-based pattern matching techniques (Johann et al., 2017; Guzman and
Maalej, 2014; Dragoni et al., 2019), several challenges remain (Dabrowski et al.,
2022). For starters, agreement towards what constitutes a feature is typically
low, as it is considered a particularly cognitively subjective task (Dabrowski
et al., 2023). In addition, app reviews are low-quality user-generated doc-
uments, subjectively biased, grammatically incorrect, relatively short, filled
with noisy content and even potentially generated by bots (Araujo et al.,
2022). Hence, traditional and machine learning (ML) methods for feature ex-

Leveraging Large Language Models for Mobile App Review Feature Extraction 3
traction typically struggle especially in the context of app reviews, reporting
an overall low recall and hence missing multiple feature mentions from a large
corpus of reviews (Dabrowski et al., 2023).
In this context, this study aims to validate the following hypothesis:
H1.Encoder-only LLMs can be leveraged to improve the state of the art of
feature extraction in the context of mobile app reviews.
Encoder-only LLMs utilize only the encoder component of the Transformer
architecture without a decoder (Minaee et al., 2024), making them suitable for
tasks like classification, named-entity recognition (NER) and information ex-
traction. To this end, our approach leverages crowdsourced user annotations
from an industrial context to redefine the feature extraction task as a su-
pervised token classification (e.g. NER) task. We compare the performance
of multiple encoder-only LLMs with a classification layer on top to extract
subsets of tokens referring to particular features for a given mobile app.
Using this approach as a baseline, we complement our research with two
additional hypotheses to explore the potential of improving the functional
suitability and performance efficiency of our LLM-based approach:
H2.Mobile app user review context can be better reflected in general-purpose
LLMs by extending the pre-training with a large corpus of user reviews,
improving the functional suitability of feature extraction.
H3.Instance selection of crowdsourced user reviews can optimize model fine-
tuning, improving the performance efficiency of feature extraction while
maintaining - or even improving - functional suitability.
As a result, our research conveys the following contributions1:
C1.A novel approach to redefining feature extraction from mobile app reviews
as a token classification task.
C2.A proposal for automatically leveraging crowdsourced, user-generated fea-
ture annotations from an industrial context into mobile app reviews.
C3.A ground-truth dataset of 23,816 reviews from 468 apps belonging to 10
popular Google Play categories, annotated with 29,383 feature mentions.
C4.A curated dataset of 654,123 mobile app reviews from 832 apps belonging
to 32 popular Google Play categories, used for extended pre-training.
C5.A collection of foundational encoder-only LLMs with extended pre-training
for mobile app reviews NLP-based tasks.
C6.A collection of fine-tuned encoder-only LLMs for feature extraction, com-
bining extended pre-training and instance selection mechanisms.
C7.A document-based instance selection mechanism for token classification
tasks to support cost-effectiveness assessment of fine-tuning LLMs for
NER.
1Source code and datasets for replication of all experiments and full evaluation artefacts
are available in the GitHub repository: https://github.com/gessi-chatbots/t-frex . The
README file includes reference to models published on HuggingFace.

4 Quim Motger1et al.
C8.An empirical analysis of the suitability and the performance variations
across the combination of (1) different types of encoder-only LLMs, (2)
extended pre-training settings, and (3) instance selection data partitions.
This research extends our previously published work (Motger et al., 2024b).
Contributions C 1→C3correspond to those already covered in the original
publication, which we refer to as the T-FREX ( Transformer-based FeatuRe
EXtraction ) baseline design. To minimize overlap, we limit the scope of the
empirical evaluation of the T-FREX baseline to relevant aspects necessary for
self-containment and comparative evaluation in this publication. Additionally,
we expand on the design and development details of our baseline approach,
focusing on LLM-related topics (i.e., annotation transfer, model selection) that
were not covered in detail in the original publication. Contributions C 4→C8
pertain exclusively to new contributions presented in this publication.
The structure of this paper is organized as follows. Section 2 covers back-
ground literature and terminology, including feature extraction, token classifi-
cation, extended pre-training, and instance selection. Section 3 defines the re-
search method, including the definition of the sample study. Section 4 presents
the T-FREX system design, including baseline and extended versions. Sec-
tion 5 presents the evaluation design, the dataset and the experiment results.
Section 6 summarizes the discussion of the research questions. Section 7 sum-
marizes related work. Finally, Section 8 concludes our research.
2 Background
2.1 Feature extraction
Afeature is a specific function or capability within a mobile application that
serves a particular purpose or fulfils a specific need (Dabrowski et al., 2023).
Formal definitions typically refer to features either with software-related termi-
nology (e.g., system capabilities, functional requirements (Wiegers and Beatty,
2013)) or from a user-centered perspective (e.g., characteristics (Kang et al.,
1990), properties (Harman et al., 2012)). Beyond their formalization, features
represent distinct characteristics of a given mobile app, designed to execute a
clear task, comply with a particular qualitative expectation, or meet a specific
user need. For instance, in the following review:
The feature for sharing notes with other users is very handy.
The feature sharing notes refers to a particular function which implies an
actionable use case for a particular interaction between the user and the mobile
app. On the other hand, in the following review:
This is the perfect lightweight app for when you just want radar
without all the other baloney bundled with it.
The feature lightweight refers to a non-functional (i.e., quality) character-
istic of the mobile app. Finally, in the following review:

Leveraging Large Language Models for Mobile App Review Feature Extraction 5
Fig. 1: List of Telegram features upvoted by users in AlternativeTo.
I enjoy meeting new people in my area and creating routes.
The feature meeting new people refers to a particular capability that facil-
itates user engagement and community building within the app.
Features have become a core descriptor for app review mining activi-
ties (Dabrowski et al., 2022). Furthermore, features are also used to sup-
port user-oriented services, including mobile app recommendation (Palomba
et al., 2015), search-based algorithms (Chen et al., 2016) and personaliza-
tion (Laranjo et al., 2021). Figure 1 illustrates a practical example of features
being used as indexation descriptors to support alternative software recom-
mendations in AlternativeTo2. Features are used to cluster and identify simi-
lar mobile apps and provide recommendations based on similar features. Ad-
ditionally, rare features are also highlighted to illustrate the app’s singularity.
However, these features are extracted based on manual user votes, limiting
their generalization, and causing biased, unbalanced representativity and in-
complete feature data.
Alternatively to manual annotation, feature extraction refers to the au-
tomatic identification of features mentioned within a textual document, in-
cluding app descriptions and user reviews (Dabrowski et al., 2022). While
formal methods are mostly based on syntactic-based pattern-matching and
topic modelling (see Section 7), several challenges remain. From a formal-
ization perspective, scoping and stating what constitutes a feature is highly
subjective, leading to discrepancies affecting methodological developments and
even different application scenarios between industrial and research environ-
ments (Johann et al., 2017). From a methodological perspective, due to the
nature and authorship of reviews, these methods tend to struggle when pro-
cessing noisy, grammatically inaccurate and complex documents (Shah et al.,
2019). From an evaluation point of view, this leads to low recall values, produc-
ing high false negatives and therefore limiting the added value of an automatic
approach by missing multiple feature candidates (Dabrowski et al., 2023).
2Source: https://alternativeto.net/software/telegram/about/

6 Quim Motger1et al.
2.2 Token classification and NER
Token classification is a fundamental task in the NLP field where the goal
is to assign a particular label (i.e., class) to each token within a text se-
quence (Naveed et al., 2024). In this context, given a review r, let T(r) de-
note the sequence of tokens in r. Formally, if rconsists of ntokens, then
T(r) = [t1, t2, . . . , t n], where tirepresents the i-th token in the sequence.
LetCbe the set of possible classes for each token. The objective of token
classification is to find a function ϕ:T(r)→Cthat maps each token tito a
class ci∈C. This can be expressed as:
ϕ(ti) =ci,∀ti∈T(r)
The annotated sequence for a review ris thus [( t1, c1),(t2, c2), . . . , (tn, cn)].
NER is a specialized form of token classification where the objective is to
identify and classify specific entities within a text into predefined categories
such as names of people, organizations, locations, dates, and other domain-
specific entities (Hou et al., 2024). Each token in a sequence is assigned a label
that indicates whether it is part of an entity and its role within that entity,
such as the beginning ( B−), inside ( I−), or outside ( O) of a named entity.
In this research, we redefine the feature extraction task from mobile app
reviews as a token classification task to identify and categorize features as
named entities within a given review. In particular, following the structure of
a NER task, we defined the set of possible classes Cas follows:
C={B-feature ,I-feature ,O}
Hence, given a review rwith tokens T(r) = [t1, t2, . . . , t n], the function ϕfor
token-based feature extraction is defined as:
ϕ(ti) =

B-feature if tiis the beginning of a feature ,
I-feature if tiis inside a feature but not the beginning ,
O if tiis not part of any feature .
For example, consider the review rconsisting of the tokens:
T(r) = [“To” ,“do”,“list” ,“function” ,“is”,“not” ,“working”]
If “to-do list ” is identified as a feature, the annotated sequence would be:
[(“To” ,B-feature ),(“do” ,I-feature ),(“list” ,I-feature ),
(“function” ,O),(“is”,O),(“not” ,O),(“working” ,O)]
To address the limitations of syntactic-based approaches, and based on
the validation of H 1, we propose using encoder-only LLMs fine-tuned for
function ϕthrough a supervised learning approach. This method leverages
crowdsourced annotations generated by real users in an industrial context
(i.e., AlternativeTo).

Leveraging Large Language Models for Mobile App Review Feature Extraction 7
2.3 LLMs and extended pre-training
LLMs are pre-trained on large document corpora from various, multidisci-
plinary textual sources (Naveed et al., 2024). These models use a combination
of unsupervised and semi-supervised learning tasks, making them suitable for
specific downstream tasks such as feature extraction. Using a large corpus of
domain-specific documents (e.g., mobile app reviews), the pre-training phase
of these models can be extended - also known as continual pre-training - to
improve their performance on such tasks (Yıldız et al., 2024). Several domains
such as healthcare (Carrino et al., 2022), education (Liu et al., 2023), math-
ematics (Gong et al., 2022), or even less specialized domains such as product
reviews (Jiang et al., 2023) have demonstrated the benefits of domain-specific
extended pre-training. This results in enhanced contextual understanding, en-
abling models to have a finer-grained understanding of language nuances and
context-specific knowledge. It also allows models to adapt to the specific vo-
cabulary and stylistic elements of the domain, which may differ significantly
from the data seen during the initial pre-training phases.
Extending LLM pre-training requires a large dataset within the target do-
main, ranging from 2 to 95 million tokens (Ke et al., 2023; Liu et al., 2023; Jiang
et al., 2020; Carrino et al., 2022), depending on the domain specificity. Fur-
thermore, it requires extensive computational resources due to the increased
amount of data and the complexity of the training processes (Jiang et al.,
2020). In addition, extending the pre-training entails some threats to validity,
such as data bias and generalization to other tasks (Yıldız et al., 2024).
Based on model selection (see Section 4), this paper focuses on two primary
pre-training tasks: masked language modelling (MLM) and permutative lan-
guage modelling (PLM). MLM involves masking a portion of the input tokens
and training the model to predict the masked tokens based on the context pro-
vided by the unmasked tokens. For instance, given the review “ Sleep tracking
is not working ”, the model might be trained with the following masked token:
Sleep [MASK] is not working .
and learn to predict “ tracking ” as a suitable token in the context of mobile
apps and features. On the other hand, PLM shuffles the order of the input
tokens and trains the model to predict the original sequence. For example,
from the same review, the model could be presented with:
tracking Sleep is working not .
and trained to reconstruct the original sequence, especially focusing on the
original order of the “ sleep tracking ” feature.
To enhance the contextual understanding of these models in the mobile app
review domain, and based on the validation of H 2, we propose extending
the pre-training of encoder-only LLMs with a large dataset of reviews from
various popular mobile app categories. This aims to improve the functional
suitability of the feature extraction ϕmethod.

8 Quim Motger1et al.
2.4 Instance selection
Instance selection involves identifying and retaining the most relevant docu-
ments (e.g., mobile app reviews) from a corpus while filtering out non-relevant
or redundant instances (Cunha et al., 2023). This technique aims to enhance
the efficiency and effectiveness of machine- and deep -learning models. Ef-
ficiency can be improved by reducing large datasets filtering non-relevant
documents (i.e., documents not affecting the task performance in terms of
functional suitability), especially when training on the entire corpus may be
computationally prohibitive (Wilson and Martinez, 2000). Additionally, ef-
fectiveness can be improved by removing document instances that might be
noisy, mislabeled, or highly redundant (Carbonera, 2017). There are multiple
instance selection strategies and methods according to the goal for filtering
document instances. This includes removal of mislabeled (Wilson, 1972) or
noisy (Wilson and Martinez, 2000) documents using gold-standards, selection
of highly representative documents through density measures (Carbonera and
Abel, 2015), and clustering-based approaches (Moran et al., 2022).
Instance selection has been widely addressed in multiple NLP tasks such
as text classification (Cunha et al., 2023), sentiment analysis (Onan and Ko-
ruko˘ glu, 2016), text generation (Chang et al., 2021) and information extrac-
tion (Cardellino et al., 2015). However, few studies explore the potential of
instance selection methods for NER tasks. And ultimately, these focus either
on label transfer or propagation (Lu et al., 2021) and random instance selec-
tion (Ferraro et al., 2024).
To optimize resource consumption for fine-tuning methods, and based on
the validation of H 3, we explore the impact of instance selection techniques
based on redundancy reduction and increased representativeness of domain-
specific knowledge. This approach aims to reduce the required dataset for
fine-tuning while maintaining, or even improving, functional quality.
3 Study design
3.1 Objective and research questions
The main goal of this research is to generate insights and empirical ev-
idence about the use of encoder-only LLMs for feature extraction
tasks in the context of mobile app reviews . To this end, we elicited the
following evaluation-oriented research questions (Shaw, 2003):
RQ 1.How effective are encoder-only LLMs at extracting features from mobile
app reviews?
RQ 2.How does extending the pre-training of encoder-only LLMs improve the
effectiveness of feature extraction from mobile app reviews?
RQ 3.How do instance selection methods improve the effectiveness of LLM-
based feature extraction from mobile app reviews?

Leveraging Large Language Models for Mobile App Review Feature Extraction 9
RQ 4.How does the combination of extended pre-training and instance se-
lection improve the effectiveness of LLM-based feature extraction from
mobile app reviews?
RQ1addresses the validation of H 1. RQ 2pertains to the validation of H 2.
Finally, RQ 3and RQ 4focus on the validation of H 3.
3.2 Research method
Figure 2 illustrates a general overview of the research method conducted in this
study. Following a Design Science (DS) methodology for software engineering
research (Engstr¨ om et al., 2020), we refined the main goal of this research
into three scientific sub-objectives focusing on the validation of H 1→H3. We
aligned each Design Science iteration with a particular sub-objective, leading
to three iterations covering ( i) objective refinement, ( ii) design and develop-
ment of the solution, and ( iii) empirical evaluation and verification of results.
Concerning evaluation, we use the ISO/IEC 25010 software product quality
model (International Organization for Standardization, 2023) to focus on two
quality characteristics for the evaluation: functional suitability , for which we
focus on functional correctness with respect to ground truth data and human
evaluation; and performance efficiency , for which we focus on time behaviour.
Extended details of the evaluation design are depicted in Section 5.
Fig. 2: Research method based on DS for software engineering research
3.3 Sample study: Google Play and AlternativeTo
We shaped our research as a sample study using data extracted from the
Google Play App Store3to minimize obtrusiveness and maximize generaliza-
3https://play.google.com/store/apps

10 Quim Motger1et al.
tion of our research findings (Stol and Fitzgerald, 2018). Minimum obtrusive-
ness is achieved by limiting manipulation of the research settings to data collec-
tion from existing repositories (i.e., app stores), constraining instrumentation
to NLP-based pre-processing (see Section 4.1.1). Maximum generalization is
aimed by focusing on cross-domain documents in the context of mobile app
reviews, using data from different mobile app categories, focusing on popular
data sources, and designing evaluation settings exploring the generalization of
research findings to unknown domains (see Section 5.3.1).
Google Play is the largest app store worldwide, hosting more than 3.5 mil-
lion apps, followed by the Apple App Store with 1.6 million apps (Statista,
2024). Its market quote reaches 2,500 million users worldwide (Google, 2024a),
offering a large catalogue of mobile apps from 32 app categories (Google,
2024b), excluding games. The potential of app stores for empirical software
engineering research has not been overlooked, leading to multiple studies ben-
efitting from the available data, including reviews, ratings, app descriptions
and changelogs, among others (McIlroy et al., 2016b,a; Hassan et al., 2018).
In this research, we focus on the collection of mobile app reviews from popular
Google Play categories to evaluate the performance of the feature extraction
process (see Section 5.2 for details on the collected dataset).
In addition to Google Play, we complement our data collection with crowd-
sourced feature annotations generated by users from AlternativeTo4, a soft-
ware recommendation platform for finding alternatives for a given software
product, including web-based, desktop and mobile apps. AlternativeTo has a
catalogue of 120,000 apps and has collected feedback from almost 2 million
users worldwide. As illustrated in Figure 1 (see Section 2.1), users can upvote
features pertaining to a particular software. These features are then used for
various use cases, such as measuring similarity with alternative apps, looking
for apps with a given feature and measuring feature frequency to determine its
popularity or rareness. We extract and use these annotations as ground truth
for the empirical evaluation of our approach. By leveraging crowdsourced user
annotations, we argue that our approach aligns with the concept of features
being used in a real context (i.e., minimizing manipulation to data collection).
Details on the data collection process are presented in Section 4. A sum-
marized overview of the collected dataset of reviews and features is presented
in Section 5.
4 System design
4.1 T-FREX baseline
Figure 3 illustrates a summarized overview of the T-FREX baseline design
proposal. T-FREX baseline is composed of three main stages: 1data collec-
tion and annotation of user reviews and features; 2data pre-processing and
4https://alternativeto.net/

Leveraging Large Language Models for Mobile App Review Feature Extraction 11
Fig. 3: Design of T-FREX baseline
feature transfer from apps to reviews; and 3model selection and fine-tuning
for the token classification function ϕ.
4.1.1 Data collection and annotation
Data collection 1is composed of two data artefacts: reviews and features.
– Reviews dataset. We built on our previous work (Motger et al., 2023)
by reusing a dataset of 639 mobile apps with 622,370 reviews from Google
Play and other Android mobile app repositories belonging to multiple, het-
erogeneous mobile app categories (e.g., communication, health and fitness,
maps and navigation, lifestyle...). We applied the following modifications to
this dataset: (1) we focused on categories exceeding a minimum threshold
for statistical representativeness (i.e., including ≥5 apps); and (2) we re-
moved all categories related to game apps, as the concept of feature in the
scope of this research does not apply to the concept of feature in game mo-
bile apps (Dabrowski et al., 2023). This process resulted in 364,220 reviews
from 468 mobile apps from 10 popular Google Play categories.
– Features dataset. Using the set of reviews, we implemented web scraping
mechanisms to access archived cached versions from AlternativeTo using
the Wayback Machine Internet Archive project5for automatically extract-
ing feature annotations by users for each app in the original dataset. This
process resulted in 198 distinct features for 468 mobile apps.
Extended details on the reviews and features dataset, including a category-
oriented analysis, are presented in Section 5.2.
4.1.2 Data pre-processing and feature transfer
For review and feature pre-processing 2, we apply a common natural lan-
guage pre-processing stage using Stanza’s neural pipeline6for syntactic an-
notation and entity extraction from user reviews and features. The pipeline
was composed of the following steps: ( i) tokenization, ( ii) multi-word token
5http://web.archive.org/
6https://stanfordnlp.github.io/stanza/neural pipeline.html

12 Quim Motger1et al.
Algorithm 1 Feature Annotation Transfer
Require: F={f1, f2, . . . , f q} ▷Set of crowdsourced feature annotations
Require: R={r1, r2, . . . , r m} ▷Set of app reviews
Require: L={O, B-feature, I -feature } ▷Name entity labels
Ensure: Rannotated with labels from L
1:Pre-process FandRusing Stanza’s neural pipeline:
2: F′←PreProcess( F)
3: R′←PreProcess( R)
4:foreach r∈R′do
5: foreach t∈T(r)do
6: t.label ←O ▷ Initialize with default label (i.e., non-feature token)
7: end for
8: appr←r.app ▷ Get the app which rbelongs to
9: foreach f∈F′do
10: appf←f.app ▷ Get the app where fwas annotated
11: ifappf=apprthen
12: ifT(f)⊆T(r)then
13: start←index of first token in T(f) within T(r)
14: end←start +|T(f)| −1
15: T(r)[start ].label←B-feature ▷ Annotate beginning feature tokens
16: fori←start + 1toenddo
17: T(r)[i].label←I-feature ▷ Annotate internal feature tokens
18: end for
19: end if
20: end if
21: end for
22:end for
23:Output the annotated corpus Rin CoNLL-U format
expansion, ( iii) PoS tagging, ( iv) morphological feature extraction, and ( v)
lemmatization. The output of this pipeline is formatted using the CoNLL-U
format7. After the pre-processing step, the corpus is ready for the annotation
process, which we depict in detail in Algorithm 1. Each token t∈T(r),∀r∈R
is initialized with the default label O. The algorithm looks for all matches
between feature tokens and review tokens. If a match is found for a particular
review rand feature ffor a given mobile app app, which means that users from
AlternativeTo voted fas a feature from app, then each token resulted from
the intersection T(r)∩T(f) is annotated with B-feature orI-feature according
to the position of the token within the original feature.
This process resulted in 29,383 feature annotations over 23,816 app reviews.
Section 5.2 provides extended details on the resulting feature annotations after
the feature transfer process.
4.1.3 Model fine-tuning
Stemming from recent literature reviews in the field of LLMs (Hou et al.,
2024; Naveed et al., 2024; Zhao et al., 2023), we compared different encoder-
only LLMs suitable for our evaluation and comparative analysis. We focused
7https://universaldependencies.org/format.html

Leveraging Large Language Models for Mobile App Review Feature Extraction 13
Table 1: Model features and fine-tuning parameters.
model data parameters task epochs learning rate batch size
BERT base 16 GB 110 M MLM 2 2e-5 16
BERT large 16 GB 336 M MLM 2 2e-5 16
RoBERTa base 160 GB 125 M MLM 2 2e-5 16
RoBERTa large 160 GB 355 M MLM 2 2e-5 8
XLNet base 16 GB 110 M PLM 2 3e-5 16
XLNet large 113 GB 340 M PLM 2 3e-5 8
on encoder-only architecture due to their inherent suitability for classification
tasks (Hou et al., 2024). In addition, we also excluded decoder-only models
(also known as generative models) due to their size and resource consumption.
These models present limited applicability in large-scale contexts such as user
review mining, especially in terms of memory, computational resources and
time constraints. Particularly, in this study, we selected the following models:
– BERT , considered the first encoder-only LLM, is renowned for its ad-
vanced contextual understanding due to its bidirectional nature (Devlin
et al., 2019). It is pre-trained using the MLM objective, which enhances
its ability to grasp context from both directions, making it effective for
token-level tasks such as NER (Broscheit, 2019). For these reasons, we use
BERT as a baseline LLM for NER tasks.
– RoBERTa improves upon BERT’s design and training methods through
extended pre-training on a larger dataset with additional data, resulting in
stronger language representations (Liu et al., 2019). It also uses MLM for
pre-training but outperforms BERT in many cases (Liu et al., 2019). We
include RoBERTa in our model evaluation due to its enhanced performance
over BERT.
– XLNet uses a unique approach by combining autoregressive and bidi-
rectional training, considering all possible word permutations during pre-
training (Yang et al., 2019). This improves its ability to understand context
and model token dependencies more effectively than traditional models.
Unlike BERT and RoBERTa, XLNet employs a PLM training objective.
Consequently, token dependencies are modelled differently. We evaluate
XLNet’s performance to compare its innovative training method against
the MLM objectives of BERT and RoBERTa.
Encoder-only models have not significantly evolved over the past few years.
As a result, while generative (decoder-only) models have experienced increased
growth and extended research, models like BERT, RoBERTa, and XLNet still
represent the state-of-the-art for encoder-only LLMs, offering robust perfor-
mance across diverse tasks (Yang et al., 2023). However, their validity in many
specific scenarios still needs thorough assessment (Hou et al., 2024).
Table 1 provides the full lists of models used in this research, as well as some
size-related features. For each model, we use both base and large versions.

14 Quim Motger1et al.
After model selection, the corpus of reviews Ris divided into ksubsets
{R1, R2, . . . , R k}. For each fold j∈ {1,2, . . . , k }, a fine-tuning iteration in-
volves using the subset Rjas the test set, while the remaining subsets R\Rj
are combined to form the training set. This process is repeated ktimes, with
each subset Rjused exactly once as the test set. We design two different
strategies for data preparation:
– In-domain learning . The set of reviews Ris split into kpartitions, each
containing the same proportion of reviews from each mobile app category
as the original dataset. This ensures a balanced representation of each
category in each partition. The in-domain learning setting evaluates the
model’s performance when it is trained on data from all domains, ensuring
a diverse and representative training set.
– Out-of-domain learning . The data is split into kpartitions, where k
is the number of different mobile app categories. Each partition contains
reviews exclusively from one category. This setup evaluates the model’s
performance in extracting features from a domain it was not trained on,
testing its generalizability to new, unseen categories.
After model selection and data preparation, we design the fine-tuning pro-
cess 3as follows:
1.Data processing . Loading of train and test datasets, transformation from
CoNLL-U format to a dataset compatible with the HuggingFace datasets
library, and tokenization of user reviews according to the model architec-
ture used in each evaluation sequence. For BERT, we use WordPiece tok-
enizer, while for RoBERTa and XLNet we use SentencePiece tokenizer. The
main difference involves the management of special tokens (e.g., BERT uses
[CLS] classification token) and tokenization granularity (e.g., RoBERTa
and XLNet employ more fine-grained tokenization where multiple tokens
can belong to the same word).
2.Model loading . Loading the model from the model library. For T-FREX
baseline, checkpoints are directly loaded using the HuggingFace model li-
brary API. This step involves initializing the model architecture and load-
ing original pre-trained weights to leverage transfer learning.
3.Training setting . Configuring the training parameters, including number
of epochs, learning rate and batch size. Table 1 reports experimentation
details used in this study for each model. Variations relate to limitations
of computational resources, assessing the balance between memory usage
and performance efficiency. This configuration is used in all research it-
erations, including T-FREX with extended pre-training (Section 4.2) and
with instance selection (Section 4.3)
4.Training . Fine-tuning process of the proper model (i.e., BERT, RoBERTa,
XLNet) with the training set. This step involves iterative optimization of
model parameters using backpropagation and gradient descent. The train-
ing process aims to minimize the loss function, improving the model’s abil-
ity to predict feature tokens accurately. Regular monitoring of training

Leveraging Large Language Models for Mobile App Review Feature Extraction 15
Fig. 4: Design of T-FREX with extended pre-training
metrics, such as loss and accuracy, is conducted to ensure the model is
learning effectively and to prevent overfitting.
5.Evaluation . Running model for inference to evaluate the performance with
the test set. This includes the collection of metrics used for evaluation in
this study, such as precision, recall, and f-measures (see Section 5.1). The
evaluation process assesses the model’s generalization capability and its
effectiveness in extracting features from unseen user reviews and features.
Evaluation results for T-FREX baseline (RQ 1) are reported in Section 5.3.1.
4.2 T-FREX with extended pre-training
Figure 4 illustrates a summarized overview of the T-FREX design proposal
with extended pre-training of LLMs. This approach is composed of three main
stages: 4data collection and pre-processing; 5extension of the pre-training
task of each LLM used in this research; and 6fine-tuning with extended mod-
els using the annotated ground truth generated during T-FREX baseline 2.
4.2.1 Data collection
We build on our previous work in the generation of a dataset 4of mobile app
reviews extended from the dataset used for T-FREX baseline (Motger et al.,
2024a). This dataset consisted of 13,478,744 user reviews from 832 mobile apps
belonging to 46 categories. To minimize obtrusiveness and guarantee minimal
manipulation of the dataset and the context, we limited the pre-processing
of such reviews to a minimal sanitization pipeline consisting of the following
steps: ( i) converting text to UTF-8 encoding; ( ii) filter non-English reviews;
and ( iii) remove duplicate text. Data is saved in CoNLL format for consis-
tency with T-FREX baseline. In addition, to minimize computational con-
sumption, we limited the dataset to an acceptable minimum size for extending
pre-training using references of related work in the field of domain-specific
extended pre-training (see Section 2.3). As BERT, RoBERTa and XLNet are
pre-trained using token level tasks (i.e., MLM and PLM), we use the length of
the corpus in terms of tokens as a metric to limit the dataset. Consequently,
we reduced the dataset to 8,232,362 tokens pertaining to 622,352 reviews.

16 Quim Motger1et al.
4.2.2 Extending models pre-training
Based on model selection for T-FREX baseline (see Section 4.1.3), we used
the extended dataset to extend the pre-training 5of BERT, RoBERTa (with
MLM) and XLNet (with PLM). Initially, CoNLL formatted data is converted
into a HuggingFace dataset. The dataset is split into training and evaluation
sets, followed by tokenization using a tokenizer specific to the model type
(i.e., WordPiece for BERT, SentencePiece for RoBERTa and XLNet). The
tokenized reviews are grouped into blocks of 128 tokens to ensure efficient
training. Then, we prepare the fine-tuning process for either MLM or PLM
tasks. This fine-tuning process is focused on reducing the value of the evalua-
tion loss after each epoch, which we monitor and report during the evaluation
process (see Section 5). Training arguments are set using the same values as
in Table 1, with the exception of the number of epochs, which we extended
to 10 in order to analyse the evolution of the continual pre-training effect af-
ter several iterations. Hence, we save a model checkpoint after each epoch for
future evaluation with token classification fine-tuning.
4.2.3 Model fine-tuning
For fine-tuning with extended models 6, we repeated steps 1 →5 from the
fine-tuning process as defined for T-FREX baseline (see Section 4.1.3) for
each checkpoint and model saved during the extended pre-training stage 5.
We used the set of annotated reviews from the T-FREX baseline design 2,
and we limited the data preparation of T-FREX with extended pre-training
to the in-domain data analysis. This is motivated by three reasons. First,
the mobile app domain is a highly stable environment in terms of emerging
mobile app categories, making in-domain learning the most common scenario,
as variability in the list of mobile app categories is very limited. Second, the
purpose of the out-of-domain analysis is to test the T-FREX baseline in a
limiting, challenging scenario, exploring the strengths and weaknesses of a
NER-based approach under unexpected circumstances (i.e., generalization to a
new app category). Finally, extended pre-training and fine-tuning processes are
computationally expensive, requiring high energy consumption. Specifically,
the in-domain analysis itself entails a total of 60 fine-tuning processes (10
checkpoints ×6 LLM instances). Hence, in addition to previous considerations
and to promote sustainability, we limit the scope of our research to the most
common scenario in the context of mobile apps (i.e., in-domain learning).
Evaluation results for T-FREX with extended pre-training (RQ 2) are re-
ported in Section 5.3.2.
4.3 T-FREX with instance selection
Figure 5 illustrates the T-FREX design proposal with instance selection of
reviews. T-FREX with instance selection is composed of two main stages:

Leveraging Large Language Models for Mobile App Review Feature Extraction 17
Fig. 5: Design of T-FREX with instance selection
7instance selection of user reviews using a density-based instance selection
mechanism; and 8model fine-tuning with original and extended LLM in-
stances using the annotated ground truth generated during T-FREX baseline.
4.3.1 Central density-based instance selection
We propose an adapted version of a central density-based instance selection
(CDIS) algorithm for classification tasks (Carbonera and Abel, 2016). This
approach 7focuses on redundancy removal for optimal resource consump-
tion and increased accuracy in prediction quality. Specifically, our approach is
focused on two main adaptations: (1) reshaping the instance selection criteria
for a NER task (i.e., using different feature entities as criteria for document
splitting); and (2) leveraging LLMs to generate contextualized embeddings for
each document, compute centroids for a semantic space (i.e., reviews mention-
ing a given feature) and compute distances between documents (i.e., reviews)
and the theoretical centroid.
This process is summarized in Algorithm 2. The algorithm takes the cor-
pus of app reviews Rwith annotated features Fas inputs 2. For each re-
view r∈R, we generate its embedding using BERT base, which we select as
a baseline representative of encoder-only LLMs. Then, we collect all reviews
containing each feature f∈Finto corresponding sets D[f]. For each fea-
ture f, the algorithm aggregates the embeddings of the reviews mentioning
fand computes the centroid Cfof these embeddings. The distance between
each review embedding and the centroid Cfis calculated using the Euclidean
distance. Reviews are then sorted based on their distance to the centroid in
descending order. Finally, the algorithm partitions the sorted reviews into four
subsets based on the specified training data distributions: 12.5%, 25%, 50%,
and 75%. The merged partitions for all features are then output as the result.
Consequently, each partition maximizes semantic representativeness of reviews
at feature level. This optimizes training sets by minimizing highly similar re-
views which might impact negatively the fine-tuning process both from the
functional correctness (i.e., overfitting or unbalanced semantic representation)
and from the performance efficiency (i.e., unnecessarily large datasets) points
of view.

18 Quim Motger1et al.
Algorithm 2 Instance Selection
Require: R={r1, r2, . . . , r m} ▷Corpus of app reviews with annotated features
Require: F={f1, f2, . . . , f q} ▷Set of features
Require: BERT base ▷Pre-trained BERT base model
Ensure: Review subset partitions based on training data distributions
1:foreach r∈Rdo
2: r.embedding ←BERT .compute embedding( r.rawtext)
3:end for
4:D← {}
5:foreach f∈Fdo
6: D[f]← {r∈R|T(f)⊆T(r)} ▷Collect reviews rcontaining f
7:end for
8:foreach f∈Fdo
9: Ef← {}
10: foreach r∈D[f]do
11: Ef←Ef∪ {r.embedding } ▷Collect review embeddings containing f
12: end for
13: Cf←compute centroid( Ef) ▷Compute centroid of embeddings
14: r.distance ←euclidean distance( r.embedding , Cf)
15: D[f]←sort(D[f],key = r.distance ,reverse = True)
16:end for
17:P← {0.125→ {} ,0.25→ {} ,0.50→ {} ,0.75→ {}}
18:foreach f∈Fdo
19: n← |D[f]| ▷Total number of reviews containing f
20: P[0.125] ←P[0.125] ∪D[f][0 :⌈0.125·n⌉]
21: P[0.25]←P[0.25]∪D[f][0 :⌈0.25·n⌉]
22: P[0.50]←P[0.50]∪D[f][0 :⌈0.5·n⌉]
23: P[0.75]←P[0.75]∪D[f][0 :⌈0.75·n⌉]
24:end for
25:Output P ▷ The merged partitions for all features
4.3.2 Model fine-tuning
Similarly to the extended pre-training approach, we repeated steps 1 →5 from
the fine-tuning process as defined for T-FREX baseline with instance selection
training splits 8. To explore the impact of instance selection in isolation
(RQ 3) and in combination with extended pre-training (RQ 4), we fine-tune all
data partitions with both the original and the extended checkpoints generated
during the T-FREX fine-tuning with extended pre-training 5. This leads to a
total of 264 fine-tuning processes (4 training sets ×6 model instances ×1+10
model checkpoints). Using the same criteria as in the extended pre-training,
we limit the scope of analysis to the in-domain data preparation scenario.
Evaluation results for T-FREX with instance selection, both in isolation
(RQ 3) and combined with extended pre-training (RQ 4), are reported in Sec-
tion 5.3.3 and Section 5.3.4, respectively.

Leveraging Large Language Models for Mobile App Review Feature Extraction 19
5 Evaluation
5.1 Design
5.1.1 Functional suitability - Ground truth
Forfunctional correctness , we use collected reviews from Google Play and
features from AlternativeTo as ground truth by assessing the correctness of
ϕ(ti) predictions ∀ti∈T(r) and ∀r∈R, where Ris the corpus of reviews
used for evaluation (see Section 5.2). Based on model predictions, we focus on
the following metrics for measuring functional suitability. First, we focus on
precision:
p=TP/(TP+FP)
where TPrefers to tokens assigned with a feature-related predicted class ci∈
{B-feature ,I-feature }matching the ground truth, and FPrefers to tokens with
a predicted class ci∈ {B-feature ,I-feature }different from the ground truth.
Precision measures how many feature tokens were correctly labelled.
Next, we focus on recall:
r=TP/(TP+FN)
where FN refers to tokens with a predicted class ci=Onot matching the
ground truth class. Recall measures how many feature tokens were missed by
the feature extraction method.
Then, we focus on the f-measure:
f1= 2·p·r
p+r
which measures the harmonic mean between precision and recall.
In the context of evaluating NLP-based RE tasks, app review analysis
is identified as an example of a hairy RE task. Hairy tasks are defined as
document-driven, non-algorithmic, manageable by experts on a small scale
and unmanageable on a large scale (Berry, 2021). Consequently, achieving high
recall is key due to the need for close to 100% recall in identifying all relevant
answers. While precision remains important, we focus on maximizing recall
to minimize overlooked feature mentions. Consequently, we propose using a
different weighting for the f-measure to reflect the greater importance of recall
over precision. In addition to f1, as proposed by Berry (2021), we also measure:
fβ= (1 + β2)·p·r
β2·p+r, β =AT/At
where ATis the average time to find a relevant feature when performing
feature extraction manually, and Atis the average time to determine the valid-
ity of a potential feature. Hence, βis the relative importance of recall (finding
all relevant features) to precision (determining the validity of features) based
on the relative cost between ATandAt. Computation of ATandAtis done
during the human validation process (see Section 5.1.2).

20 Quim Motger1et al.
For the in-domain setting, we measure these metrics using a k-fold cross-
validation analysis with k= 10 and report average values. For the out-of-
domain setting, kis determined by the number of mobile app categories.
We exclude accuracy from the evaluation because we cannot control the
exhaustivity of the annotations. Without certainty that all features of a given
mobile app are annotated in AlternativeTo, we cannot ensure that tokens
labelled by default as non-feature entities ( ci=O) in Algorithm 1 are correct.
5.1.2 Functional suitability - Human evaluation
Stemming from the previous consideration, crowdsourced features from Alter-
nativeTo impose some limitations due to the lack of control of the annota-
tion process (see Section 6.5). As some features might be overlooked, tokens
predicted with a feature class ci∈ {B-feature ,I-feature }annotated with O
might be falsely detected as an FP instance. This limits the assessment of the
precision metric. As a mitigation action, and as an extension to functional
correctness , we decided to extend our evaluation with an external human eval-
uation process strictly on new predicted features, i.e., limiting the evaluation
to predicted features which are not annotated in the original ground truth set.
The human evaluation process consisted of the following steps:
1.Questionnaire design. We designed questionnaires for human assessment
using QuestBase8which were distributed to external annotators with Pro-
lific9. Specifically, we designed two different sets of questionnaires:
(a)Assessment for automatic feature extraction ( QA). Figure 6 illus-
trates a snapshot of the questionnaire to assess the validity of a feature
automatically extracted by T-FREX. Each question presents to the an-
notator: (1) the app name, including a link to Google Play; (2) the app
category; (3) the text of the review; (4) the proposed feature; and (5)
the question. Annotators can confirm the feature proposal ( Yes), reject
it (No) or report it as not clear ( I don’t know ). We conducted itera-
tive internal annotations to adjust the size (100 reviews), required time
(15’) and economic retribution (2 £). Each QAquestionnaire includes
5 control questions to reject annotators not passing a minimum per-
formance requirement (i.e., 4/5 correct feature annotations with trivial
examples from the ground truth).
(b)Assessment for manual feature extraction ( QM). Figure 7 illus-
trates a snapshot of the questionnaire to manually extract features from
a given review. Annotators are presented with the same information as
in previous questionnaires, with the exception of the question and the
answers, which in this case is a free-text area. We reduced the size of
each task (25 reviews) while keeping time (15’) and retribution (2 £).
QMquestionnaires include 3 control questions to reject annotators us-
ing a performance threshold (i.e., 2/3 correct feature annotations).
8https://questbase.com/
9https://www.prolific.com/

Leveraging Large Language Models for Mobile App Review Feature Extraction 21
Fig. 6: Questionnaire for automatic feature extraction ( QA).
Fig. 7: Questionnaire for manual feature extraction ( QM).
2.Guidelines elaboration. We prepared annotation instructions for each
questionnaire. These include (1) the definition of a feature, (2) the context
of the evaluation task, (3) the metadata provided for each annotation task,
and (4) several examples with different feature annotations. Guidelines
were refined during 3 iterative internal annotations to improve the clarity
and representativeness of the examples provided.
3.Evaluation . We conducted three human evaluation iterations:
(a)F-measure weighting factor β(QAand QM).We randomly se-
lected a subset of 100 reviews from the evaluation set. For this subset,
we created one QAquestionnaire with 100 reviews and four QMques-
tionnaires with 25 reviews each. We measured the time required for
each annotator to complete the questionnaire, and we report average
values among all valid annotators to measure AtandAT.
(b)Precision of new features ( QAwith baseline). We use the best
T-FREX baseline model (RQ 1) and we run it for inference with all
reviews available in the evaluation dataset. We selected only those
review-feature annotation pairs where the given feature was not origi-
nally annotated in the ground truth. Then, we split the set of filtered

22 Quim Motger1et al.
review-feature pairs into a subset of questionnaires QAaccording to the
pre-defined size (100 reviews). We required a minimum of 5 valid anno-
tators per task. Among these, we used a voting mechanism to determine
the final label assigned to each feature.
(c)Precision of new features ( QAwith combined extensions). We
repeated the same process as in 3b but with features predicted by the
best-performing T-FREX model with combined extensions (RQ 4).
5.1.3 Performance efficiency
For the time behaviour dimension, we focus on the evaluation of the perfor-
mance efficiency of the fine-tuning processes for generating T-FREX LLM
instances, especially for the assessment of the cost-effectiveness balance with
different data partitions (RQ 3). Consequently, we focus on measuring exe-
cution times10for each of the fine-tuning stages defined in Section 4.1 (i.e.,
data processing, model loading, training setting, training, and evaluation).
We exclude document pre-processing and feature transfer from performance
efficiency analysis as these steps are only executed once before all experiments.
5.2 Dataset
Table 2 reports the details of the dataset used for evaluation11, collected and
generated during T-FREX baseline design (see Section 4.1.1). This includes
the mobile app categories included in the dataset, which covers heterogeneous
categories ranging from generic Communication andSocial apps to specialized
domains such as Health and fitness orMaps and navigation . Reviews included
in this dataset pertain exclusively to those with at least one feature mentioned.
As a summary, our dataset is composed of 23,816 reviews from 468 mobile
apps, leading to 475,382 tokens with the following token class distribution:
29,383 tokens labelled as B-feature , 2,841 tokens labelled as I-feature , and
443,158 tokens labelled as O. The largest category is Productivity , with al-
most 150,000 tokens and up to 77 distinct features ( |features |). On the other
hand, the least represented categories are Maps and navigation ,Lifestyle and
Weather , depending on whether we focus on the number of tokens, number of
features or distinct features. Notice that a distinct feature might be present in
more than one category. For instance, video calling is annotated as a feature
for both Productivity andCommunication apps.
5.3 Results
We structure evaluation results in alignment with research questions, presented
as follows: T-FREX baseline fine-tuning (RQ 1); T-FREX with extended pre-
10Experiments were conducted on two NVIDIA GeForce RTX 4090 GPUs.
11Datasets and source code for replicating the evaluation process are available in the
GitHub repository: https://github.com/gessi-chatbots/t-frex

Leveraging Large Language Models for Mobile App Review Feature Extraction 23
Table 2: Dataset used for evaluation. List of app categories: Productivity (PR),
Communication (CO), Tools (TO), Social (SO), Health and fitness (HE),
Personalization (PE), Travel and local (TR), Maps and Navigation (MA),
Lifestyle (LI), Weather (WE).
metric PR CO TO SO HE PE TR MA LI WE Total
apps 137 51 58 14 75 6 19 31 12 65 468
reviews 7,348 7,003 4,321 819 2,154 112 530 284 344 901 23,816
sentences 8,604 8,135 5,402 899 2,330 118 602 315 391 984 27,780
tokens 148,172 134,833 93,395 15,597 40,907 2,022 11,105 5,868 8,044 15,439 475,382
B-feature 8,801 10,026 5,220 1,016 1,981 111 691 355 346 836 29,383
I-feature 1,495 820 305 60 59 1 17 13 47 24 2,841
O 137,876 123,987 87,870 14,521 38,867 1,910 10,397 5,500 7,651 14,579 443,158
features 8,801 10,026 5,220 1,016 1,981 111 691 355 346 836 29,383
|features | 77 54 50 26 23 19 17 12 10 7 198
training (RQ 2); T-FREX with instance selection (RQ 3); and T-FREX with
combined extensions (RQ 4).
5.3.1 Baseline fine-tuning
Table 3 reports average precision ( p), recall ( r), standard f-measure ( f1) and
weighted f-measure ( fβ) for token classification for both out-of-domain12and
in-domain data preparation settings. As mentioned in Section 5.1, βis com-
puted comparing the performance of AT(time required for assessing automatic
feature extraction) with respect to At(time required for manual feature ex-
traction). Average results from human evaluation (3a) led to AT= 28.29sand
At= 11 .86s, which leads to a β=AT/At= 2.385. We use fβas the gold
metric to select the best-performing models in our research context.
Table 3: Token classification results (baseline fine-tuning)
Out-of-domain In-domain
model p r f 1 fβ p r f 1 fβ
BERT base 0.546 0.314 0.381 0.335 0.596 0.488 0.532 0.502
BERT large 0.577 0.339 0.414 0.361 0.719 0.582 0.637 0.595
RoBERta base 0.531 0.336 0.386 0.356 0.668 0.569 0.611 0.582
RoBERTa large 0.455 0.339 0.374 0.352 0.688 0.509 0.571 0.530
XLNet base 0.627 0.482 0.535 0.499 0.679 0.519 0.582 0.538
XLNet large 0.651 0.374 0.437 0.399 0.761 0.573 0.646 0.599
For out-of-domain analysis, XLNet baseoutperformed all other models with
a recall r= 0.482 and a weighted f-measure fβ= 0.499 (+0 .100 with respect to
the second best-performing model, XLNet large). However, the highest precision
is reported by XLNet large with p= 0.651. On the other hand, RoBERTa large
12Based on the scope of this research, we limit the out-of-domain analysis to average
results, excluding category-oriented evaluation details. These details are extended in the
previous work from which this research stems (Motger et al., 2024b).

24 Quim Motger1et al.
demonstrated the weakest performance for almost every metric, especially pre-
cision. For in-domain analysis, XLNet large achieved the highest performance
with a precision p= 0.761 and weighted f-measure of fβ= 0.599. BERT large
reports similar results, especially due to its highest recall with r= 0.582 and
the weight of recall in computing the weighted f-measure, leading to fβ= 0.595
(only−0.005 with respect to XLNet large). Conversely, BERT baseexhibited the
lowest performance metrics, with a weighted f-measure fβ= 0.502.
Table 4: Feature extraction evaluation (comparison with SAFE baseline)
Out-of-domain In-domain
model p r f 1 fβ p r f 1 fβ
SAFE 0.301 0.321 0.310 0.318 0.193 0.215 0.199 0.209
BERT base 0.471 0.300 0.347 0.311 0.575 0.419 0.485 0.436
XLNet large 0.503 0.417 0.445 0.424 0.631 0.572 0.600 0.572
In addition to token-level effectiveness, we also report and measure effec-
tiveness at the feature level. In this setting, quality metrics are measured using
the feature as a whole (i.e., groups of contiguous tokens B-feature andI-feature
composing a whole feature as labelled in the ground truth). This analysis facil-
itates comparison with SAFE (Johann et al., 2017), a syntactic-based method
in the field of feature extraction which we identify as a baseline method from
related work (see Section 7). Furthermore, this analysis is also intended to fa-
cilitate comparisons by further research in the field of feature extraction. We
build on a replication of SAFE to support this comparative analysis with our
dataset (Shah et al., 2019). Table 4 reports functional correctness results for
the SAFE approach, BERT base(used as the baseline for LLM-based feature
extraction design) and XLNet large (reported as the best-performing model in
T-FREX baseline for token-level effectiveness). Results showcase that T-FREX
outperforms SAFE in all settings, with the only exception of the out-of-domain
recall with BERT base. For in-domain analysis, T-FREX reports significantly
greater precision (+0 .438) and recall (+0 .357) with respect to syntactic-based
mechanisms, especially for the former. Overall, in-domain T-FREX baseline
version showcases to overcome some of the limitations posed by syntactic-based
approaches in the context of reviews, especially when generalizing syntactic
patterns to different datasets, as suggested by Shah et al. (2019).
Table 5 summarizes the results of the human evaluation of new features
(3b). We collected all features predicted by XLNet largefine-tuned model dur-
ing the in-domain k-fold cross-validation analysis for each test set. Then, we
selected those reviews with predicted features which were not originally anno-
tated as features in the ground-truth set. This led to a total of 1,956 reviews,
with 1,067 distinct feature annotations. Given the size of the dataset, we de-
cided to submit for evaluation all reviews, leading to 21 human evaluation
tasks of 100 feature annotation questions (95 for evaluation, 5 for control). As
a result, human evaluation of new features leads to a total average precision of

Leveraging Large Language Models for Mobile App Review Feature Extraction 25
Table 5: Human evaluation of new features (FP) with best-performing T-
FREX baseline model (XLNet large). Total column is weighted based on review
distribution across categories.
PR CO TO SO HE PE TR MA LI WE Total
#reviews 459 643 560 44 218 0 8 29 0 0 1,956
% Yes 68.6% 62.3% 58.4% 63.6% 59.4% - 66.7% 58.6% - - 62.5%
% No 28.8% 35.0% 41.7% 34.1% 39.3% - 33.3% 41.4% - - 36.1%
% Idk 1.6% 2.7% 1.8% 2.2% 0.6% - 0.0% 0.0% - - 1.9%
0.625 (i.e., 62.5% of ‘ Yes’ annotations across the whole dataset). This supports
the hypothesis that the original dataset lacks exhaustive annotations.
5.3.2 Extended pre-training
Figure 8 showcases the evolution of the evaluation loss during the extended
pre-training stage after each epoch 1 →10. All models, especially large model
instances (i.e., BERT large, RoBERTa large, XLNet large) show a general trend
of decreasing evaluation loss over the epochs, with the largest reduction oc-
curring between the first and second epochs. On the other hand, base models
(i.e., BERT base, RoBERTa base, XLNet base) exhibit relatively stable and lower
evaluation losses throughout the epochs. Between epochs 8 →10 all six models
converge into evaluation loss values ≤10−4. The higher initial evaluation loss
for large models may be attributed to their increased complexity and greater
number of parameters, which require more epochs for effective optimization.
Fig. 8: Evaluation loss across 10 epochs.
Figure 9 illustrates the evolution of all functional correctness metrics for
the in-domain fine-tuning using model checkpoints cfrom 1 →10. For a com-
parative analysis, we include T-FREX baseline model as model checkpoint
c= 0. Results showcase that all six model instances increase the maximum

26 Quim Motger1et al.
(a) Results for BERT base
 (b) Results for BERT large
(c) Results for RoBERTa base
 (d) Results for RoBERTa large
(e) Results for XLNet base
 (f) Results for XLNet large
Fig. 9: Token classification evaluation results with extended pre-training over
different checkpoints (train epochs).
value for each metric at some point during the extended pre-training. Precision
is on average the most increased metric, especially for base models BERT base
(c= 2, +0.159), RoBERTa base(c= 5, +0.166) and XLNet base(c= 2, +0.097).
The only exception is precision for XLNet large, which suffers from decay from
0.761 to 0.686 (-0.075). The increase of recall is more modest and is especially
highlighted in base models like XLNet base(c= 4, +0.078) but also in large
models such as RoBERTa large(c= 9, +0.040) or XLNet large(c= 1, +0.047).
If we focus on evolution across epochs, we notice that most models approach
the best metric values between the first and the second epoch. After that, met-
ric values either stabilize (i.e., BERT base, RoBERTa base, XLNet base) or start to

Leveraging Large Language Models for Mobile App Review Feature Extraction 27
decay (i.e., BERT base, RoBERTa large). In addition, precision and recall values
show a common behaviour across all epochs (i.e., increasing one also increases
the other). The only exception to both of these statements is XLNet large. Us-
ing XLNet large extended with just one epoch increases recall (+0.047) and,
consequently, fβ(+0.036). But given the decay in precision, the balanced f-
measure remains almost identical (+0.002). Finally, in terms of evolutionary
behaviour, BERT (both base and large) and RoBERTa baseshow relatively sta-
ble behaviour between consecutive epochs. However, RoBERTa largeand XLNet
(both base and large) showcase erratic behaviour with constants increases and
decays. This behaviour might be a consequence of the pre-training objectives
and the data used for the initial pre-training of these models (see Section 6).
5.3.3 Instance selection
Figure 10 illustrates the evolution of all functional correctness metrics for the
in-domain fine-tuning using T-FREX baseline approach for fine-tuning and the
instance selection algorithm to generate different training data set partitions
d∈ {12.5%,25%,50%,75%}. For a comparative, evolutionary analysis, we
include T-FREX baseline setting using the complete training set as d= 100%.
All T-FREX base models experience an improvement in every metric when
a certain degree of instance selection (i.e., between 12.5% and 75%) is con-
ducted in the ground truth training dataset. For BERT base, the best data
partition is 75% ( fβ= 0.583). For RoBERTa base, using only 25% leads to
the best results ( fβ= 0.615). For XLNet base, the best data partition is 50%
(fβ= 0.631). While this condition mostly prevails in large models, BERT large
(for recall and f-measures) and XLNet large(for precision) report some partic-
ular exceptions. Beyond these, using 50% of the training data results in the
best setting for RoBERTa large(fβ= 0.629) and XLNet large(fβ= 0.677). On
average, for six models and two fundamental metrics (precision and recall), 10
out of 12 evaluations improve with instance selection. Specifically, two model-
metric combinations improve with 25% of the data (RoBERTa base, precision
and recall), two combinations improve with 75% (BERT base, precision and
recall), and the rest improve with a 50% partition.
If we focus on the tendency as we increase the size of the training set, we
observe a non-linear behaviour that suggests the presence of a local minimum.
This indicates an optimal value for the training set size where the model per-
formance is maximized before it starts to decline with further data increase.
This phenomenon is consistent across various models and metrics. For in-
stance, for BERT baseand RoBERTa base, the metrics peak at different points
— 75% and 25% respectively — before showing a decline, which highlights the
importance of instance selection in optimizing model performance. Similarly,
for large models like BERT largeand XLNet large, the optimal training set sizes
are different, with 50% being optimal for RoBERTa largeand XLNet large.
Concerning variations on time behaviour using different data partitions,
Figure 11 reports, for each model and training data partition, the execution
times required for each model fine-tuning stage (steps 1 →6 in Section 4.1.3).

28 Quim Motger1et al.
(a) Results for BERT base
 (b) Results for BERT large
(c) Results for RoBERTa base
 (d) Results for RoBERTa large
(e) Results for XLNet base
 (f) Results for XLNet large
Fig. 10: Token classification evaluation results with instance selection over
different training data partitions (% of review instances).
We report average values obtained from the k-fold cross-validation during the
in-domain analysis. The training stage takes the majority of the total execution
time across all models and data partitions. In comparison, data processing and
model loading phases consume relatively minimal time. On average, using 50%
of the training set entails a speed up of ×1.8 with respect to using 100% of the
dataset. For smaller partitions, the speed up grows linearly. On average, for
base models, using 12.5% of the training set entails a ×4.1 speed up, while for
large models this is increased to ×4.9 on average. If we focus on larger data
partitions, for 75%, all models consistently report a speedup of ×1.3.

Leveraging Large Language Models for Mobile App Review Feature Extraction 29
Fig. 11: Execution times (in seconds) for T-FREX model fine-tuning.
5.3.4 Combining extended pre-training and instance selection
Given the large number of experimentation settings for combined analysis (6 T-
FREX models ×4 data partitions ×11 model checkpoints), we limit the results
included in this paper to the best configuration of extended pre-training and/or
instance selection for each metric. To this end, Table 6 reports functional
correctness metrics for the token classification task with respect to T-FREX
baseline (RQ 1), T-FREX with extended pre-training (RQ 2), T-FREX with
instance selection (RQ 3) and T-FREX with combined extensions (RQ 4). For
each combination, we exclusively report the setting reporting the best metric
by specifying the model checkpoint ( c) for extended pre-training, the data
partition ( d) for instance selection, and both when combining extensions.
If we focus on the combined use of instance selection and extended pre-
training (EP/IS), there are only a few examples where this combination is the
best option for functional correctness: precision for BERT baseand XLNet base;
and recall and fβfor RoBERTa large. If we focus on the use of extended pre-
training only (EP), BERT large, RoBERTa baseand XLNet large report this de-
sign as the best option for increasing effectiveness. In fact, XLNet largeemerges
as the best model, both for recall ( r= 0.700) and weighted f-measure ( fβ=
0.677), with c= 1. For precision, RoBERTa basewith extended pre-training
with c= 5 emerges as the best option. On the other hand, if we focus on the
isolated use of instance selection, it emerges as the best option for BERT base
and RoBERTa base, with some exceptions like precision for BERT base.
To better illustrate the contributions of our research, Table 7 reports metric
variations for each T-FREX extension with respect to the T-FREX baseline
design. On average, we observe that all base models, as well as mostly all

30 Quim Motger1et al.
Table 6: Summary of best-performing approach according to each setting:
baseline (BL), with extended pre-training (EP), with instance selection (IS),
and combining extended pre-training with instance selection (EP/IS). Green
cells represent the best metrics for each model. Pinpointed cells with
 icon
represent the best metrics in all evaluations.
model setting p r f1 f β
BERT baseBL 0.596 0.488 0.532 0.502
EPc=20.755c=20.545c=20.621c=20.569
ISd=75%0.738d=75%0.562d=75%0.632d=75%0.583
EP/ISc=2,d=75%0.768c=1,d=75%0.549c=1,d=75%0.626c=1,d=75%0.573
BERT largeBL 0.719 0.582 0.637 0.599
EPc=20.762c=20.632c=20.691c=20.649
ISd=50%0.727d=75%0.568d=75%0.626d=75%0.587
EP/ISc=7,d=75%0.692c=4,d=75%0.587c=4,d=75%0.630c=4,d=75%0.601
RoBERTa baseBL 0.668 0.569 0.611 0.582
EP
c=50.834c=70.623c=70.694c=70.647
ISd=75%0.739d=25%0.598d=25%0.652d=25%0.616
EP/ISc=5,d=75%0.741c=5,d=25%0.592c=7,d=75%0.650c=7,d=75%0.610
RoBERTa largeBL 0.688 0.509 0.571 0.530
EPc=20.783c=20.555c=20.636c=20.580
ISd=50%0.751d=50%0.612d=50%0.666d=50%0.629
EP/ISc=9,d=75%0.769c=1,d=12.5%0.621c=1,d=12.5%0.664c=1,d=12.5%0.639
XLNet baseBL 0.679 0.519 0.582 0.538
EPc=80.776c=40.597c=40.661c=40.618
ISd=50%0.744d=50%0.615d=50%0.664d=50%0.631
EP/ISc=2,d=75%0.783c=3,d=75%0.595c=2,d=75%0.655c=2,d=75%0.617
XLNet largeBL 0.761 0.573 0.646 0.595
EPc=10.738
c=10.667
c=10.700
c=10.677
ISd=50%0.686d=50%0.620d=50%0.648d=50%0.629
EP/ISc=10,d=75%0.748c=7,d=25%0.641c=7,d=25%0.669c=7,d=25%0.655
large models, improve T-FREX baseline when extended pre-training and/or
instance selection is applied. For large models, there are some minor exceptions
like precision for XLNet large and f-measures for BERT large. Precision is the
most increased metric, experiencing its greatest improvement when extended
pre-training and instance selection are used in combination for BERT base
(+0.172). Recall is also increased, but its maximum improvement is more con-
servative, as observed with RoBERTa largewith combined extensions (+0.112)
or in XLNet largewith instance selection (+0.094).
After analysis of all T-FREX settings, XLNet largewith extended pre-training
(c= 1) emerges as the best-performing13model (based on fβ). For consis-
tency with the evaluation of T-FREX baseline, we complement the analysis
of ground truth annotations with the evaluation of new features reported by
T-FREX which are not present in the ground truth. We use the fine-tuned
c=1XLNet largemodel for inference, collecting 11,120 reviews mentioning 1,311
distinct new features not assessed during evaluation of RQ 1. Given the size
of the dataset of reviews, we limited the set of reviews used for human eval-
uation for resource optimization to 1,311 reviews (i.e., one review instance of
13Notice the criteria for “best-performing model” might vary across different research
contexts and scenarios, for which we provide exhaustive results for precision and recall.

Leveraging Large Language Models for Mobile App Review Feature Extraction 31
Table 7: Delta variations δfor evaluation metrics with respect to T-FREX
baseline (for best setting in each case).
model setting δp δr δf1 δfβ
BERT baseEP +0.159 +0.057 +0.089 +0.037
IS +0.142 +0.074 +0.100 +0.051
EP/IS +0.172 +0.061 +0.094 +0.041
BERT largeEP +0.043 +0.050 +0.054 +0.012
IS +0.008 -0.014 -0.011 -0.050
EP/IS -0.027 0.005 -0.007 -0.036
RoBERTa baseEP +0.166 +0.054 +0.083 +0.036
IS +0.071 +0.029 +0.041 +0.005
EP/IS +0.073 +0.023 +0.039 -0.001
RoBERTa largeEP +0.095 +0.046 +0.065 +0.009
IS +0.063 +0.103 +0.095 +0.058
EP/IS +0.081 +0.112 +0.093 +0.068
XLNet baseEP +0.097 +0.078 +0.079 +0.036
IS +0.065 +0.096 +0.082 +0.049
EP/IS +0.104 +0.076 +0.073 +0.035
XLNet largeEP -0.075 +0.047 +0.002 -0.017
IS -0.023 +0.094 +0.054 +0.031
EP/IS -0.013 +0.068 +0.023 +0.009
each distinct feature). This entails 11.8% of the complete set of reviews and
100% of the newly predicted features. Consequently, human evaluation (3c)
consisted of 14 tasks with 100 questions (95 for evaluation, 5 for control).
Table 8 reports the results for this analysis using the same format as in RQ 1
(Table 5). On average, we observe a consistent precision of features labelled as
‘Yes’ (60.8%), slightly lower with respect to T-FREX baseline (62.5%). How-
ever, the set of features labelled as ‘ No’ is -3% smaller for the extended version
(33.1%) with respect to T-FREX baseline (36.1%). This is due to a major un-
certainty in label predictions annotated during this evaluation process, labelled
as ‘I don’t know ’ (6.1%). In addition, the number of new features reported with
extended pre-training (11,120) is ×5.7 higher than the number of new features
reported with the baseline design (1,956). Consequently, although precision is
not increased, T-FREX withc=1XLNet large significantly reduces the number
of potentially missed features compared to T-FREX baseline.
Table 8: Human evaluation of new features (FP) with best-performing T-
FREX extended model (c=1XLNet large).Total column is weighted based on
review distribution across categories.
PR CO TO SO HE PE TR MA LI WE Total
#reviews 206 228 260 102 282 8 93 46 35 60 1320
% Yes 57.8% 68.0% 60.0% 71.6% 53.2% 87.5% 51.6% 54.3% 88.6% 63.3% 60.8%
% No 35.4% 25.9% 34.6% 22.5% 38.7% 12.5% 43.0% 41.3% 8.6% 33.3% 33.1%
% Idk 6.8% 6.1% 5.4% 5.9% 8.2% 0.0% 5.4% 4.3% 2.9% 3.4% 6.1%

32 Quim Motger1et al.
6 Discussion
6.1 Baseline fine-tuning
RQ 1:The T-FREX baseline design effectively extracts feature mentions
from mobile app reviews, enhancing the performance of syntactic-based
baseline methods in both out-of-domain and in-domain learning settings.
This improvement is observed in terms of both precision and recall.
For the most challenging scenario (i.e., out-of-domain), XLNet basedemon-
strated the strongest performance, achieving a weighted f-measure ( fβ) of
0.499. This model is particularly effective in reducing missed features (false
negatives). However, XLNet large excelled in precision, achieving the highest
score of 0.651, indicating its ability to minimize false features (false positives).
Conversely, RoBERTa large showed the weakest performance, highlighting its
limited generalization capability for out-of-domain scenarios.
For the most common scenario (i.e., in-domain), XLNet largeoutperformed
XLNet baseacross all metrics, with a precision of 0.761 and a weighted f-
measure of 0.599, overcoming its out-of-domain limitations. BERT large also
performed well, especially in recall with a score of 0.582, but slightly lagged in
weighted f-measure at 0.595. Despite the small differences, XLNet largeemerged
as the best overall model for in-domain tasks. Notably, larger RoBERTa mod-
els performed worse than their base versions, consistent with other studies
suggesting that larger models may sometimes hinder performance.
Human evaluation of new features using XLNet largefor in-domain analysis
revealed an average precision of 0.625, confirming the model’s ability to iden-
tify valid, previously unannotated features. This supports the hypothesis that
the original dataset’s annotations were not exhaustive.
Overall, the results highlight the effectiveness of T-FREX models, par-
ticularly the XLNet variants, in various evaluation contexts. n encoder-only
LLM approach showcases better adaptation to the concept of a feature as
used in an industrial setting like AlternativeTo, suppressing the limitations of
deterministic approaches conditioned by the use of specific syntactic patterns
that might not always generalize. Hence, we argue that T-FREX provides an
effective software-based mechanism to reduce missing features in automatic
review-based opinion mining pipelines used by practitioners and researchers.
6.2 Extended pre-training
RQ 2:Extended pre-training consistently improves the precision and re-
call of feature extraction for all encoder-only models analysed in this re-
search. Different models entail different outcomes, requiring different train-
ing epochs according to the size and pre-training task of the model to reach
maximum improvement without forcing the decay of quality metrics.

Leveraging Large Language Models for Mobile App Review Feature Extraction 33
Base models such as BERT base, RoBERTa base, and XLNet basedemonstrate
substantial improvements in performance metrics with extended pre-training,
with precision showing the most significant increases. This suggests that base
models have greater potential for refinement, and extended pre-training effec-
tively reduces false positives, enhancing the likelihood that identified features
are accurate. Similarly, while large models tend to also improve with extended
pre-training, they also tend to experience a decay after the first few epochs,
indicating a limit to the benefits of extended pre-training for these more com-
plex models. This is also supported by the evolution of the evaluation loss,
which is significantly limited after a few epochs of extended pre-training.
In addition, base models tend to stabilize across epochs in comparison to
the erratic behaviour of large models, especially RoBERTa largeand XLNet large.
This instability may be attributed to the complexity of their pre-training tasks
and the data used, which could lead to fluctuations in performance as the
models are fine-tuned. Specifically, XLNet largeshowcases significant variability,
potentially due to its architectural differences and the pre-training objective
(i.e., PLM), which might not align as well with the fine-tuning tasks. This
erratic behaviour underscores the need for careful monitoring and potentially
different strategies when employing large models for extended pre-training.
On average, the best performance improvements are achieved after just one
or two epochs of extended pre-training. Beyond this, additional epochs do not
provide substantial benefits and, in some cases, lead to performance decay. For
similar tasks and settings, this suggests that investing in extensive pre-training
beyond a few epochs may not be adequate in terms of cost-effectiveness bal-
ance, given the diminishing returns and significant computational resources
required. Researchers and practitioners conducting similar experiments (e.g.,
NER fine-tuning with encoder-only LLMs) should consider these factors when
designing their training protocols to ensure efficient and effective use of com-
putational resources.
6.3 Instance selection
RQ 3:On average, instance selection not only improves the performance
efficiency of the fine-tuning process but also leads to a significant improve-
ment in the functional correctness of the token classification task, both in
precision and recall. This improvement is especially observed for data par-
titions between 50%-75%, and it is consistently observed in base models,
while large models present some exceptions.
As illustrated with the analysis of base models, optimal performance is
achieved with different data partitions from the original set of reviews (e.g.,
75% for BERT base, 25% for RoBERTa base, and 50% for XLNet base). This un-
derscores that base models may struggle with redundancies and noise in the
full dataset, and a more targeted data selection can enhance their perfor-
mance. Large models, while also benefiting from instance selection, show some

34 Quim Motger1et al.
variability, particularly with XLNet large, which exhibits erratic behaviour, sug-
gesting that its architecture might be more sensitive to the training set size
and content. Interestingly, the performance across all models tends to peak
at certain data partition sizes and then decline, indicating a local optimum.
Overall, empirical results showcase the validity of the instance selection al-
gorithm (Algorithm 2) proposed for token-level fine-tuning in the context of
mobile app review feature extraction.
In addition to effectiveness, using a smaller, optimal portion of the dataset
significantly reduces execution times. Training with 50% of the dataset, for
instance, nearly halves the execution time while still improving performance
on average. For even smaller partitions, such as 12.5%, the speedup can be as
much as four to five times faster, which is particularly advantageous for large-
scale applications where computational resources and time are critical. This
efficiency gain, combined with improved model performance, makes instance
selection a highly valuable strategy in real-world scenarios, where processing
large batches of reviews for multiple tasks (e.g., sentiment analysis, content
classification, feature extraction) becomes computationally expensive.
6.4 Combining extended pre-training and instance selection
RQ 4:Combined use of extended pre-training and instance selection im-
proves T-FREX baseline design in almost all design scenarios and models.
This entails that a cost-effective assessment is necessary to improve both
the effectiveness and efficiency of the system. However, optimal precision
and recall values are overall achieved with extended pre-training without
the need for instance selection. Results report some exceptions to this, espe-
cially for base models, where either combined extensions or simply instance
selection becomes the best option.
When the precision of feature proposals is the priority, RoBERTa basestands
out as the best model, especially for tasks where false positives are more tol-
erable than missing actual features. Conversely, for applications where recall
is critical, XLNet largeis superior, as missing a feature mention is more costly
than incorrectly identifying one. This underscores the need for decision-making
based on specific application scenarios, which is why detailed reporting of these
metrics is essential. However, in the context of RE hairy tasks (Berry, 2021),
we argue that high recall is more important than high precision, which is
reflected in the weighting of fβ.
On average, base models benefit more from instance selection, particularly
in terms of recall, demonstrating their sensitivity to data quality and rele-
vance. On the other hand, large models show significant improvements with
extended pre-training, whether used alone or in conjunction with instance se-
lection. Notably, base models always experience improvements with some form
of extension. This motivates the need for a cost-effectiveness analysis consider-

Leveraging Large Language Models for Mobile App Review Feature Extraction 35
ing both absolute performance (Table 6) and relative improvements (Table 7)
to determine the practicality of maintaining base versus large models.
Regarding new, unseen features, human evaluation indicates a substantial
increase in the number of identified features with extended T-FREX design.
Due to the large number of feature candidates in the dataset (i.e., tokens),
even a slight improvement in recall (+0 .047 withc=1XLNet large) leads to a
significant increase in potential new features in absolute numbers (1,956 vs.
11,120). Consequently, despite a slight drop in the precision of new features
(−0.017), the much higher number of new feature mentions identified suggests
a lower likelihood of missing important features. This trade-off supports a
more exhaustive feature extraction process, as previously argued.
6.5 Threats to validity
For the elicitation of threats to validity, we rely on the taxonomy proposed
by Wohlin (2014). Concerning construct validity , we rely on functional cor-
rectness metrics based on token classification performance at the token level.
However, the feature extraction process is not evaluated in a real-world set-
ting, where feature predictions are used for a particular purpose. Due to the
extensiveness of the empirical evaluation showcased in this research, we limited
the scope of analysis to the feature extraction task, leading to future work on
additional quality characteristics from ISO 25010, such as functional appropri-
ateness or interaction capability of T-FREX as a component in a real mobile
app review mining pipeline. In addition, the selection of fβis conditioned by
the human evaluation results. Different experiments might lead to different β
values. To reduce the threats imposed by this, we reported extended results
including precision, recall and harmonized f1in addition to fβto facilitate
replication in other contexts. Furthermore, we conducted a human evaluation
with external annotators using a large set of reviews and multiple annotators
(5 per task), increasing the reliability of results by using average values.
Concerning internal validity , the main threat comes from the ground truth
dataset used for evaluation. Collected reviews from Google Play and Alter-
nativeTo might be biased and conditioned by domain-specific particularities.
Furthermore, the formalization of what constitutes a feature entails some bias,
especially given the inconsistencies found in the literature (see Section 2.1).
To mitigate this threat, we relied on and leveraged crowdsourced annotations
made by real users in an industrial setting. We argue that our findings are
then applicable to the concept of feature as being used in real settings, rather
than providing or reusing a synthesized data set. In addition, we focused on
the evaluation of one instance selection algorithm, as well as specific data
partitions. This selection might have introduced some bias in the instance se-
lection process. We limited the scope of analysis based on literature review and
procuring resource optimization, validating the decisions and the design eval-
uated in this research. However, we acknowledge that other instance selection
algorithms might produce similar - or even better - results.

36 Quim Motger1et al.
Concerning external validity , we identify the generalization of research
findings as the most compromised threat. Specifically, evaluation of different
datasets, such as reviews from other repositories beyond Google Play, features
from other sources beyond AlternativeTo, or even for different mobile app
categories, might produce different outcomes. We built on previous, validated
work to construct datasets for both fine-tuning and extended pre-training, in-
creasing the confidence in the quality and representativeness of the domain
of the data used for evaluation. Furthermore, results for model comparative
analysis might not translate with different model architectures even with the
same evaluation settings. Additionally, the generalization of the value of ex-
tended pre-training in the domain of mobile apps to other tasks (e.g., sentiment
analysis) remains unexplored.
Finally, concerning conclusion validity , decisions and recommendations con-
cerning best and worst models and settings for each scenario are conditioned
and restricted to the results presented in this paper. To this end, for the T-
FREX baseline, we designed two different learning scenarios (i.e., in-domain
vs. out-of-domain) to assess the validity of T-FREX across different contexts
of application. Furthermore, we exhaustively reported all metrics to facilitate
decisions based on their applicability. While in this paper, for some settings,
we limited the scope of these results (i.e., results for RQ 4are limited to the
best configuration in each setting), we provide the source code for all exper-
iments and all tasks depicted in this research to replicate our evaluation. In
addition, we provide data artefacts to verify extended results and complement
the analytical insights presented in this research.
7 Related work
We structure related work based on two main areas of research. First, we cover
automatic methods for NLP-based feature extraction from mobile app reviews.
Second, we focus on empirical research depicting approaches to fine-tune LLMs
for domain-specific token classification tasks.
7.1 Feature extraction
Dabrowski et al. recently conducted a systematic literature review in the field
of mining app reviews for feedback analysis (Dabrowski et al., 2022). They
identified feature extraction as a key task, for which they also conducted
replication and comparative studies using multiple relevant approaches in the
field (Dabrowski et al., 2023). The state-of-the-art in the field is mainly repre-
sented by syntactic-based approaches. While the SAFE approach is considered
one of the standards (Johann et al., 2017), there are several related contribu-
tions published either as early work (Iacob et al., 2014; Guzman and Maalej,
2014; Gu and Kim, 2015), replication studies (Shah et al., 2019) or even as
continual evolutions of the same approach (Dragoni et al., 2019). These meth-
ods are based on the use of syntactic properties, such as Part-of-Speech (PoS)

Leveraging Large Language Models for Mobile App Review Feature Extraction 37
tags and syntactic dependencies between elements in a given sentence. Using a
pattern-matching approach, syntactic methods look for a series of predefined
patterns compliant with typical formulations for a feature. Recent work in the
field is still applying these methods for complex opinion mining NLP-based
pipelines (Sutino and Siahaan, 2019; Song et al., 2020; Kasri et al., 2020;
Al-Hawari et al., 2021; Kumari and Memon, 2022).
Recently, there have been some initial proposals based on leveraging LLMs
to support the feature extraction task. TransFeatEx (Motger et al., 2023) uses
a RoBERTa model for extracting syntactic annotations, to which then a syn-
tactic pattern-matching approach can be applied. KEFE (Wu et al., 2021)
uses PoS pattern-extracted features as input to a BERT model for classify-
ing sentences are potential feature mentions. Their focus is on applying this
technique to app descriptions, transferring potential feature matches to user
reviews.
Despite extensive work in the field, several challenges posed by these strate-
gies still remain. Performance on the correctness of extracted features is lim-
ited, especially when processing user reviews, leading to substantial noise (i.e.,
increased false positives) and missed features (i.e., increased false negatives)
due to the various writing styles and grammatical inaccuracies found in re-
views (Guzman and Maalej, 2014; Johann et al., 2017; Dragoni et al., 2019).
Furthermore, evaluation strategies and ground truth are limited to instructed
internal coders (Johann et al., 2017; Dabrowski et al., 2023). Our approach
delves into these challenges by leveraging crowdsourced annotations by real
users and transferring them into real user reviews. In addition to previous con-
siderations, source code for these solutions is scarce (Dabrowski et al., 2022),
and black-box integration is highly limited due to compatibility restrictions
(both for syntactic-based and deep-learning-based solutions). We publish T-
FREX models on a collection of HuggingFace models ready to be used either
for download or with the HuggingFace Inference API14, facilitating reusability
and integration with other software components.
7.2 Token classification with LLMs
Hou et al. recently conducted a systematic literature review in the field of
software engineering practices leveraging LLMs (Hou et al., 2024). Similarly,
context-agnostic literature reviews on the generic use of LLMs index several
contributions for token classification and NER tasks (Naveed et al., 2024; Zhao
et al., 2023; Minaee et al., 2024). Several works showed that Transformer-based
models trained for NER on common entities (e.g., locations, dates, names) have
showcased promising results with respect to traditional ML methods meth-
ods (Xu et al., 2019; Souza et al., 2020). On the other hand, domain-specific
studies have increasingly focused on leveraging LLMs for token classification,
particularly in the medical domain. For instance, Tial et al. evaluated differ-
ent Transformer-based NER models on free-text eligibility criteria from clinical
14https://huggingface.co/docs/api-inference/index

38 Quim Motger1et al.
trials (Tian et al., 2021). Liu et al. proposed Med-BERT, a medical-dictionary-
enhanced BERT specifically tailored for performing NER on medical records
(Liu et al., 2021).
On a closer domain, Malik et al. tested three Transformer models (i.e.
BERT, RoBERTa and ALBERT) for software-specific entity extraction (Ma-
lik et al., 2022). Tabassum et al. instead fine-tuned BERT for a NER task on
20 fine-grained types of computing programming entities from Stack Overflow
posts (Tabassum et al., 2020). Chen et al. proposed a BERT language repre-
sentation model for extracting cybersecurity-related terms such as software,
organizations and vulnerabilities from unstructured texts (Chen et al., 2021).
Beyond these studies, token classification models leveraging LLMs in the field
of software engineering are scarce. To the best of our knowledge, there is no
related work using encoder-only LLMs for token classification in mobile app
review mining to extract app-related entities.
8 Conclusions and future work
In this study, we presented T-FREX, a feature extraction method in the con-
text of mobile app reviews leveraging encoder-only LLMs. We redefined feature
extraction as a NER task using crowdsourced annotations generated by real
users in a real setting. Empirical evaluation of T-FREX baseline (RQ 1) show-
cases the potential of T-FREX with respect to previous approaches, improving
both precision and recall of extracted features (H 1). In addition, extending the
pre-training of such models with a large dataset of reviews (RQ 2) resulted in
improved correctness of the predictions in almost all settings (H 2), with only
a few epochs to achieve best results in the majority of scenarios. Furthermore,
applying our proposal for feature-oriented instance selection (RQ 3) not only
significantly improves the performance efficiency of the fine-tuning process
(H3), but also increases the correctness of feature predictions in multiple sce-
narios, especially in the context of base models. Finally, while the combined
use of instance selection and extended pre-training (RQ 4) is not always the
best approach, it still improves the correctness of feature extraction while also
improving the performance efficiency in most T-FREX settings.
As future work, we plan on evaluating the generalization of T-FREX mod-
els to other mobile app domains. Specifically, we want to explore its ability
to generalize and extract features in emerging, disruptive domains, such as
AI-based applications. Furthermore, we plan to explore generalization beyond
the scope of mobile applications, such as desktop applications or APIs, and
other user-generated documents, such as issues and bug reports. In conclu-
sion, we envisage that both the methodological contributions as well as the
set of T-FREX LLM instances (fine-tuned and with extended pre-training),
which are publicly available, might assist future researchers and practition-
ers by integrating these models into their own review-based NLP pipelines for
opinion mining and decision-making tasks where features are considered a core
descriptor.

Leveraging Large Language Models for Mobile App Review Feature Extraction 39
Acknowledgements With the support from the Secretariat for Universities and Research
of the Ministry of Business and Knowledge of the Government of Catalonia and the European
Social Fund. This paper has been funded by the Spanish Ministerio de Ciencia e Innovaci´ on
under project/funding scheme PID2020-117191RB-I00 / AEI/10.13039/501100011033. This
paper has been also supported by FAIR - Future AI Research (PE00000013) project under
the NRRP MUR program funded by the NextGenerationEU.
Data Availability Statements
The source code and full app review datasets required for the replication of all
experiments and the full evaluation artefacts are publicly available in the lat-
est release of our GitHub repository: https://github.com/gessi-chatbots/
t-frex . The repository’s README file includes references to the models pub-
lished on HuggingFace, including fine-tuned models for feature extraction and
LLMs with extended pre-training in the field of mobile app reviews.
Conflict of Interest
The authors declared that they have no conflict of interest.
References
Al-Hawari A, Najadat H, Shatnawi R (2021) Classification of application re-
views into software maintenance tasks using data mining techniques. Soft-
ware Quality Journal 29(3):667 – 703, DOI 10.1007/s11219-020-09529-8
Araujo AF, Gˆ olo MPS, Marcacini RM (2022) Opinion mining for app reviews:
an analysis of textual representation and predictive models. Automated Soft-
ware Engg 29(1), DOI 10.1007/s10515-021-00301-1
Berry DM (2021) Empirical evaluation of tools for hairy requirements en-
gineering tasks. Empirical Software Engineering 26(6):111, DOI 10.1007/
s10664-021-09986-0
Broscheit S (2019) Investigating entity knowledge in BERT with simple neural
end-to-end entity linking. In: Proceedings of the 23rd Conference on Com-
putational Natural Language Learning (CoNLL), Hong Kong, China, pp
677–685, DOI 10.18653/v1/K19-1063
Carbonera JL (2017) An efficient approach for instance selection. In: Bella-
treche L, Chakravarthy S (eds) Big Data Analytics and Knowledge Discov-
ery, Cham, pp 228–243
Carbonera JL, Abel M (2015) A density-based approach for instance selec-
tion. In: 2015 IEEE 27th International Conference on Tools with Artificial
Intelligence (ICTAI), pp 768–774, DOI 10.1109/ICTAI.2015.114
Carbonera JL, Abel M (2016) A novel density-based approach for instance se-
lection. In: 2016 IEEE 28th International Conference on Tools with Artificial
Intelligence (ICTAI), pp 549–556, DOI 10.1109/ICTAI.2016.0090

40 Quim Motger1et al.
Cardellino C, Villata S, Alemany LA, Cabrio E (2015) Information ex-
traction with active learning: A case study in legal text. Lecture Notes
in Computer Science (including subseries Lecture Notes in Artificial In-
telligence and Lecture Notes in Bioinformatics) 9042:483 – 494, DOI
10.1007/978-3-319-18117-2 36
Carrino CP, Llop J, P` amies M, Guti´ errez-Fandi˜ no A, Armengol-Estap´ e J,
Silveira-Ocampo J, Valencia A, Gonzalez-Agirre A, Villegas M (2022) Pre-
trained biomedical language models for clinical nlp in spanish. In: Proceed-
ings of the Annual Meeting of the Association for Computational Linguistics,
p 193 – 199
Chang E, Shen X, Yeh HS, Demberg V (2021) On training instance selection
for few-shot neural text generation. In: ACL-IJCNLP 2021 - 59th Annual
Meeting of the Association for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language Processing, Proceedings
of the Conference, vol 2, p 8 – 13
Chen N, Hoi SC, Li S, Xiao X (2016) Mobile app tagging. In: WSDM 2016 -
Proceedings of the 9th ACM International Conference on Web Search and
Data Mining, p 63 – 72, DOI 10.1145/2835776.2835812
Chen Y, Ding J, Li D, Chen Z (2021) Joint bert model based cybersecurity
named entity recognition. In: ACM International Conference Proceeding
Series, p 236 – 242, DOI 10.1145/3451471.3451508
Cunha W, Viegas F, Fran¸ ca C, Rosa T, Rocha L, Gon¸ calves MA (2023) A
comparative survey of instance selection methods applied to non-neural and
transformer-based text classification. ACM Comput Surv 55(13s), DOI 10.
1145/3582000
Dabrowski J, Letier E, Perini A, Susi A (2022) Analysing app reviews for soft-
ware engineering: a systematic literature review. Empirical Software Engi-
neering 27(2):43, DOI 10.1007/s10664-021-10065-7
Dabrowski J, Letier E, Perini A, Susi A (2023) Mining and searching app
reviews for requirements engineering: Evaluation and replication studies.
Information Systems 114:102181, DOI 10.1016/j.is.2023.102181
Dalpiaz F, Parente M (2019) Re-swot: From user feedback to requirements via
competitor analysis. Lecture Notes in Computer Science (including subseries
Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
11412 LNCS:55 – 70, DOI 10.1007/978-3-030-15538-4 4
Devlin J, Chang MW, Lee K, Toutanova K (2019) BERT: Pre-training of
deep bidirectional transformers for language understanding. In: Proceedings
of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers), Minneapolis, Minnesota, pp 4171–4186, DOI
10.18653/v1/N19-1423
Dragoni M, Federici M, Rexha A (2019) An unsupervised aspect extraction
strategy for monitoring real-time reviews stream. Information Processing &
Management 56(3):1103–1118, DOI 10.1016/j.ipm.2018.04.010
Engstr¨ om E, Storey MA, Runeson P, H¨ ost M, Baldassarre MT (2020) How
software engineering research aligns with design science: a review. Empirical

Leveraging Large Language Models for Mobile App Review Feature Extraction 41
Software Engineering 25(4):2630 – 2660, DOI 10.1007/s10664-020-09818-7
Fantechi A, Gnesi S, Passaro L, Semini L (2023) Inconsistency detection in
natural language requirements using chatgpt: a preliminary evaluation. In:
2023 IEEE 31st International Requirements Engineering Conference (RE),
pp 335–340, DOI 10.1109/RE57278.2023.00045
Ferrari A, Gori G, Rosadini B, Trotta I, Bacherini S, Fantechi A, Gnesi S
(2018) Detecting requirements defects with nlp patterns: an industrial ex-
perience in the railway domain. Empirical Software Engineering 23(6):3684
– 3733, DOI 10.1007/s10664-018-9596-7
Ferraro A, Galli A, La Gatta V, Minocchi M, Moscato V, Postiglione M (2024)
Few shot ner on augmented unstructured text from cardiology records. Lec-
ture Notes on Data Engineering and Communications Technologies 193:1 –
12, DOI 10.1007/978-3-031-53555-0 1
Frattini J, Unterkalmsteiner M, Fucci D, Mendez D (2024) Nlp4re tools: Classi-
fication, overview, and management. URL https://arxiv.org/abs/2403.
06685 ,2403.06685
Gong Z, Zhou K, Zhao WX, Sha J, Wang S, Wen JR (2022) Continual pre-
training of language models for math problem understanding with syntax-
aware memory network. In: Proceedings of the Annual Meeting of the
Association for Computational Linguistics, vol 1, p 5923 – 5933, DOI
10.18653/v1/2022.acl-long.408
Google (2024a) How google play works. URL https://play.google/
howplayworks/ , accessed: 2024-07-05
Google (2024b) Requesting app category changes in google play. URL
https://support.google.com/googleplay/android-developer/
answer/9859673?hl=en , accessed: 2024-07-05
Gu X, Kim S (2015) ”what parts of your apps are loved by users?” (t). In:
2015 30th IEEE/ACM International Conference on Automated Software
Engineering (ASE), pp 760–770, DOI 10.1109/ASE.2015.57
Guzman E, Maalej W (2014) How do users like this feature? a fine grained
sentiment analysis of app reviews. In: 2014 IEEE 22nd International Re-
quirements Engineering Conference (RE), pp 153–162, DOI 10.1109/RE.
2014.6912257
Harman M, Jia Y, Zhang Y (2012) App store mining and analysis: Msr for
app stores. In: 2012 9th IEEE Working Conference on Mining Software
Repositories (MSR), pp 108–111, DOI 10.1109/MSR.2012.6224306
Hassan S, Tantithamthavorn C, Bezemer CP, Hassan AE (2018) Studying
the dialogue between users and developers of free apps in the google play
store. Empirical Software Engineering 23(3):1275 – 1312, DOI 10.1007/
s10664-017-9538-9
Hou X, Zhao Y, Liu Y, Yang Z, Wang K, Li L, Luo X, Lo D, Grundy J, Wang
H (2024) Large language models for software engineering: A systematic lit-
erature review. URL https://arxiv.org/abs/2308.10620 ,2308.10620
Iacob C, Harrison R, Faily S (2014) Online reviews as first class artifacts in
mobile app development. In: Memmi G, Blanke U (eds) Mobile Computing,
Applications, and Services, pp 47–53

42 Quim Motger1et al.
International Organization for Standardization (2023) ISO/IEC
25010:2023(en) Systems and software engineering — Systems and software
Quality Requirements and Evaluation (SQuaRE) — Product quality model.
URL https://www.iso.org/obp/ui/#iso:std:iso-iec:25010:ed-2:
v1:en , accessed: 2024-07-25
Jiang G, Jiang C, Xue S, Zhang J, Zhou J, Lian D, Wei Y (2023) Towards
anytime fine-tuning: Continually pre-trained language models with hyper-
network prompts. In: Bouamor H, Pino J, Bali K (eds) Findings of the
Association for Computational Linguistics: EMNLP 2023, Singapore, pp
12081–12095, DOI 10.18653/v1/2023.findings-emnlp.808
Jiang H, He P, Chen W, Liu X, Gao J, Zhao T (2020) Smart: Robust and
efficient fine-tuning for pre-trained natural language models through princi-
pled regularized optimization. In: Proceedings of the Annual Meeting of the
Association for Computational Linguistics, p 2177 – 2190
Johann T, Stanik C, Alizadeh B AM, Maalej W (2017) Safe: A simple approach
for feature extraction from app descriptions and app reviews. In: 2017 IEEE
25th International Requirements Engineering Conference (RE), pp 21–30,
DOI 10.1109/RE.2017.71
Kang K, Cohen S, Hess J, Novak W, Peterson A (1990) Feature-Oriented
Domain Analysis (FODA) Feasibility Study. Tech. Rep. CMU/SEI-90-TR-
021, Software Engineering Institute, Carnegie Mellon University
Kasri M, et al. (2020) A Comparison of Features Extraction Methods for
Arabic Sentiment Analysis. In: 4th International Conference on Big Data
and Internet of Things
Ke Z, Shao Y, Lin H, Konishi T, Kim G, Liu B (2023) Continual pre-training of
language models. URL https://arxiv.org/abs/2302.03241 ,2302.03241
Kumari S, Memon ZA (2022) Extracting feature requests from online reviews
of travel industry. Acta Scientiarum - Technology 44
Laranjo L, DIng D, Heleno B, Kocaballi B, Quiroz JC, Tong HL, Chahwan
B, Neves AL, Gabarron E, Dao KP, Rodrigues D, Neves GC, Antunes ML,
Coiera E, Bates DW (2021) Do smartphone applications and activity track-
ers increase physical activity in adults? systematic review, meta-analysis
and metaregression. British Journal of Sports Medicine 55(8):422 – 432,
DOI 10.1136/bjsports-2020-102892
Li C, Huang L, Ge J, Luo B, Ng V (2018) Automatically classifying user
requests in crowdsourcing requirements engineering. Journal of Systems and
Software 138:108 – 123, DOI 10.1016/j.jss.2017.12.028
Liu N, Hu Q, Xu H, Xu X, Chen M (2021) Med-bert: A pretraining frame-
work for medical records named entity recognition. IEEE Transactions on
Industrial Informatics 18(8):5600–5608
Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer
L, Stoyanov V (2019) RoBERTa: A Robustly Optimized BERT Pretraining
Approach. URL https://arxiv.org/abs/1907.11692
Liu Z, He X, Liu L, Liu T, Zhai X (2023) Context matters: A strategy
to pre-train language model for science education. Communications in
Computer and Information Science 1831 CCIS:666 – 674, DOI 10.1007/

Leveraging Large Language Models for Mobile App Review Feature Extraction 43
978-3-031-36336-8 103
Lu T, Gui Y, Gao Z (2021) Learning document-level label propagation and in-
stance selection by deep q-network for interactive named entity annotation.
IEEE Access 9:39568 – 39586, DOI 10.1109/ACCESS.2021.3064054
Maalej W, Nabil H (2015) Bug report, feature request, or simply praise? on
automatically classifying app reviews. In: 2015 IEEE 23rd International Re-
quirements Engineering Conference (RE), pp 116–125, DOI 10.1109/RE.
2015.7320414
Malgaonkar S, Licorish SA, Savarimuthu BTR (2022) Prioritizing user con-
cerns in app reviews – a study of requests for new features, enhance-
ments and bug fixes. Information and Software Technology 144, DOI
10.1016/j.infsof.2021.106798
Malik G, Cevik M, Bera S, Yildirim S, Parikh D, Basar A (2022) Software re-
quirement specific entity extraction using transformer models. In: Canadian
AI
McIlroy S, Ali N, Hassan AE (2016a) Fresh apps: an empirical study of
frequently-updated mobile apps in the google play store. Empirical Soft-
ware Engineering 21(3):1346 – 1370, DOI 10.1007/s10664-015-9388-2
McIlroy S, Ali N, Khalid H, E Hassan A (2016b) Analyzing and automat-
ically labelling the types of user issues that are raised in mobile app re-
views. Empirical Software Engineering 21(3):1067 – 1106, DOI 10.1007/
s10664-015-9375-7
McZara J, Sarkani S, Holzer T, Eveleigh T (2015) Software requirements pri-
oritization and selection using linguistic tools and constraint solvers—a con-
trolled experiment. Empirical Software Engineering 20(6):1721 – 1761, DOI
10.1007/s10664-014-9334-8
Minaee S, Mikolov T, Nikzad N, Chenaghlu M, Socher R, Amatriain X, Gao
J (2024) Large language models: A survey. URL https://arxiv.org/abs/
2402.06196 ,2402.06196
Moran M, Cohen T, Ben-Zion Y, Gordon G (2022) Curious instance selection.
Information Sciences 608:794–808, DOI https://doi.org/10.1016/j.ins.2022.
07.025
Motger Q, Franch X, Marco J (2023) Mobile feature-oriented knowledge base
generation using knowledge graphs. In: New Trends in Database and In-
formation Systems - ADBIS 2023 Short Papers, Doctoral Consortium and
Workshops: AIDMA, DOING, K-Gals, MADEISD, PeRS, Barcelona, Spain,
September 4-7, 2023, Proceedings, Communications in Computer and Infor-
mation Science, vol 1850, pp 269–279, DOI 10.1007/978-3-031-42941-5 \24
Motger Q, Franch X, Marco J (2024a) Mapp-kg: Mobile app knowledge graph
for document-based feature knowledge generation. In: Islam S, Sturm A
(eds) Intelligent Information Systems, pp 129–137
Motger Q, Miaschi A, Dell’Orletta F, Franch X, Marco J (2024b) T-frex: A
transformer-based feature extraction method from mobile app reviews. URL
https://arxiv.org/abs/2401.03833 ,2401.03833
Naveed H, Khan AU, Qiu S, Saqib M, Anwar S, Usman M, Akhtar N, Barnes
N, Mian A (2024) A comprehensive overview of large language models. URL

44 Quim Motger1et al.
https://arxiv.org/abs/2307.06435 ,2307.06435
Onan A, Koruko˘ glu S (2016) Exploring performance of instance selection
methods in text sentiment classification. Advances in Intelligent Systems
and Computing 464:167 – 179, DOI 10.1007/978-3-319-33625-1 16
Palomba F, Linares-Vasquez M, Bavota G, Oliveto R, Di Penta M, Poshy-
vanyk D, De Lucia A (2015) User reviews matter! tracking crowdsourced
reviews to support evolution of successful apps. In: 2015 IEEE 31st Inter-
national Conference on Software Maintenance and Evolution, ICSME 2015
- Proceedings, p 291 – 300, DOI 10.1109/ICSM.2015.7332475
Perez E, Kiela D, Cho K (2021) True few-shot learning with language models.
Advances in Neural Information Processing Systems 14:11054 – 11070
Ronanki K, Berger C, Horkoff J (2023) Investigating chatgpt’s potential to
assist in requirements elicitation processes. In: 2023 49th Euromicro Con-
ference on Software Engineering and Advanced Applications (SEAA), pp
354–361, DOI 10.1109/SEAA60479.2023.00061
Scalabrino S, Bavota G, Russo B, Penta MD, Oliveto R (2019) Listening to
the crowd for the release planning of mobile apps. IEEE Transactions on
Software Engineering 45(1):68 – 86, DOI 10.1109/TSE.2017.2759112
Schick T, Sch¨ utze H (2021) It’s not just size that matters: Small language
models are also few-shot learners. In: NAACL-HLT 2021 - 2021 Conference
of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Proceedings of the Conference, p
2339 – 2352
Shah FA, Sirts K, Pfahl D (2019) Is the safe approach too simple for app
feature extraction? a replication study. In: Knauss E, Goedicke M (eds)
Requirements Engineering: Foundation for Software Quality, Cham, pp 21–
36
Sharma S, Kumar D (2019) Agile release planning using natural language
processing algorithm. In: Proceedings - 2019 Amity International Conference
on Artificial Intelligence, AICAI 2019, p 934 – 938, DOI 10.1109/AICAI.
2019.8701252
Shaw M (2003) Writing good software engineering research papers. In: 25th
International Conference on Software Engineering, 2003. Proceedings., pp
726–736, DOI 10.1109/ICSE.2003.1201262
Sleimi A, Sannier N, Sabetzadeh M, Briand L, Ceci M, Dann J (2021)
An automated framework for the extraction of semantic legal metadata
from legal texts. Empirical Software Engineering 26(3), DOI 10.1007/
s10664-020-09933-5
Song R, Li T, Ding Z (2020) Automatically identifying requirements-oriented
reviews using a top-down feature extraction approach. In: Proceedings -
Asia-Pacific Software Engineering Conference, APSEC, vol 2020-December,
p 450 – 454, DOI 10.1109/APSEC51365.2020.00054
Souza F, Nogueira R, Lotufo R (2020) Portuguese named entity recognition
using bert-crf. URL https://arxiv.org/abs/1909.10649 ,1909.10649
Statista (2024) Number of apps available in leading app stores
as of 2024. URL https://www.statista.com/statistics/276623/

Leveraging Large Language Models for Mobile App Review Feature Extraction 45
number-of-apps-available-in-leading-app-stores/ , accessed: 2024-
07-05
Stol KJ, Fitzgerald B (2018) The ABC of Software Engineering Research.
ACM Trans Softw Eng Methodol 27(3), DOI 10.1145/3241743
Sutino Q, Siahaan D (2019) Feature extraction from app reviews in google
play store by considering infrequent feature and app description. Journal of
Physics: Conference Series 1230, DOI 10.1088/1742-6596/1230/1/012007
Tabassum J, Maddela M, Xu W, Ritter A (2020) Code and named entity recog-
nition in StackOverflow. In: Jurafsky D, Chai J, Schluter N, Tetreault J (eds)
Proceedings of the 58th Annual Meeting of the Association for Computa-
tional Linguistics, Online, pp 4913–4926, DOI 10.18653/v1/2020.acl-main.
443
Tian S, Erdengasileng A, Yang X, Guo Y, Wu Y, Zhang J, Bian J, He Z (2021)
Transformer-based named entity recognition for parsing clinical trial eligibil-
ity criteria. In: Proceedings of the 12th ACM Conference on Bioinformatics,
Computational Biology, and Health Informatics, pp 1–6
van Vliet M, Groen EC, Dalpiaz F, Brinkkemper S (2020) Identifying and
classifying user requirements in online feedback via crowdsourcing. Lecture
Notes in Computer Science (including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics) 12045 LNCS:143 – 159,
DOI 10.1007/978-3-030-44429-7 11
Wiegers KE, Beatty J (2013) Software Requirements 3. Microsoft Press, USA
Wilson DL (1972) Asymptotic properties of nearest neighbor rules using edited
data. IEEE Transactions on Systems, Man, and Cybernetics SMC-2(3):408–
421, DOI 10.1109/TSMC.1972.4309137
Wilson DR, Martinez TR (2000) Reduction techniques for instance-based
learning algorithms. Machine Learning 38(3):257–286, DOI 10.1023/A:
1007626913721
Wohlin C (2014) Guidelines for snowballing in systematic literature studies
and a replication in software engineering. In: Proceedings of the 18th Inter-
national Conference on Evaluation and Assessment in Software Engineering
Wu H, et al. (2021) Identifying key features from app user reviews. In: Inter-
national Conference on Software Engineering
Xu J, Wen J, Sun X, Su Q (2019) A discourse-level named entity recognition
and relation extraction dataset for chinese literature text. URL https://
arxiv.org/abs/1711.07010 ,1711.07010
Yang J, Jin H, Tang R, Han X, Feng Q, Jiang H, Yin B, Hu X (2023) Harness-
ing the power of llms in practice: A survey on chatgpt and beyond. URL
https://arxiv.org/abs/2304.13712 ,2304.13712
Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV (2019) Xl-
net: Generalized autoregressive pretraining for language understanding. Ad-
vances in neural information processing systems 32
Yıldız C, Ravichandran NK, Punia P, Bethge M, Ermis B (2024) Investigating
continual pretraining in large language models: Insights and implications.
URL https://arxiv.org/abs/2402.17400 ,2402.17400

46 Quim Motger1et al.
Zhang L, Hua K, Wang H, Qian G, Zhang L (2014) Sentiment analysis
on reviews of mobile users. Procedia Computer Science 34:458–465, DOI
https://doi.org/10.1016/j.procs.2014.07.013, the 9th International Confer-
ence on Future Networks and Communications (FNC’14)/The 11th Inter-
national Conference on Mobile Systems and Pervasive Computing (Mo-
biSPC’14)/Affiliated Workshops
Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang
J, Dong Z, Du Y, Yang C, Chen Y, Chen Z, Jiang J, Ren R, Li Y, Tang X,
Liu Z, Liu P, Nie JY, Wen JR (2023) A survey of large language models.
URL https://arxiv.org/abs/2303.18223 ,2303.18223
Zini JE, Awad M (2022) On the explainability of natural language processing
deep models. ACM Computing Surveys 55(5), DOI 10.1145/3529755

