Mission Impossible: A Statistical Perspective on
Jailbreaking LLMs
Jingtong Su∗
NYU & Meta AI, FAIRJulia Kempe†
NYU & Meta AI, FAIRKaren Ullrich†
Meta AI, FAIR
Abstract
Large language models (LLMs) are trained on a deluge of text data with limited
quality control. As a result, LLMs can exhibit unintended or even harmful be-
haviours, such as leaking information, fake news or hate speech. Countermeasures,
commonly referred to as preference alignment, include fine-tuning the pretrained
LLMs with carefully crafted text examples of desired behaviour. Even then, empir-
ical evidence shows preference aligned LLMs can be enticed to harmful behaviour.
This so called jailbreaking of LLMs is typically achieved by adversarially modify-
ing the input prompt to the LLM. Our paper provides theoretical insights into the
phenomenon of preference alignment and jailbreaking from a statistical perspective.
Under our framework, we first show that pretrained LLMs will mimic harmful
behaviour if present in the training corpus. Under that same framework, we then
introduce a statistical notion of alignment, and lower-bound the jailbreaking
probability, showing that it is unpreventable under reasonable assumptions.
Based on our insights, we propose an alteration to the currently prevalent alignment
strategy RLHF. Specifically, we introduce a simple modification to the RLHF
objective, we call E-RLHF , that aims to increase the likelihood of safe responses.
E-RLHF brings no additional training cost, and is compatible with other methods.
Empirically, we demonstrate that E-RLHF outperforms RLHF on all alignment
problems put forward by the AdvBench [ 1] and HarmBench project [ 2] without
sacrificing model performance as measured by the MT-Bench project [3].
1 Introduction
Large Language Models (LLMs) have revolutionized the field of deep learning due to their remarkable
capabilities across various domains, serving as assistants, in code generation [ 4], healthcare [ 5],
and theorem proving [ 6]. The training process of a LLM typically includes two stages: pretraining
with massive corpora, and an alignment step using Reinforcement Learning from Human Feedback
(RLHF) to further align model behavior with human preferences. The latter step typically involves
large amounts of humanly annotated data, and can be decomposed into a supervised fine-tuning
(SFT) step, a reward modeling step, and an RL Fine-Tuning step. Despite their ability to perform
multiple tasks effectively, LLMs are susceptible to generating offensive or inappropriate content
including hate-speech, malware, fake information or social biases, due to the unavoidable presence
of harmful elements within their pretraining datasets [ 7–9]. Social media showcase an abundance
of tricks on how to attack ChatGPT [ 10] to elicit harmful responses, e.g., the “Do Anything Now”
(DAN) prompts [ 11] or the “Grandma Exploit” hack [ 12]. On the other hand, behavior diversity in
the training corpus is essential to for example capturing different cultural preferences. What is and
isn’t harmful ultimately depends on user preferences, hence the alignment step is not universal but
depends on the specific use case under which a model will be employed.
∗Correspondence to: Jingtong Su <js12196@nyu.edu>.
†Equal advising.
Preprint. Under review.arXiv:2408.01420v1  [cs.LG]  2 Aug 2024

To address deployment safety and eliminate objectionable responses, numerous alignment efforts
have been made, such as injecting safe information during SFT [ 13], performing red teaming with
human experts and AI themselves [ 14–18], as well as refining and improving the whole RLHF process
in detail [ 19–23]. Yet we continue to witness a cat-and-mouse game of ever more sophisticated
alignment methods to neutralize “harmful” prompts and even more inventive “jailbreaking” attacks
that manipulate those prompts to elicit LLMs to produce harmful information. Such attacks come in
various flavors, such as injecting adversarial suffixes [ 1,24,25], exploring cipher and low-resource
natural languages [ 26–28], or letting LLMs craft prompts automatically [ 29–33]. Although several ad-
hoc defense methods against suffixes have been proposed [ 34–37], we only have limited proposal on
a principled universal defense against jailbreaking attacks [ 2], and limited theoretical understanding
on this phenomenon [38].
In this paper, we present a theoretical framework for analyzing both the pretraining phase and the
post-alignment jailbreaking phenomenon. Exploiting the fact that jailbreaking prompts typically
maintain the underlying harmful concept while manipulating other aspects of the prompt, we design
framework that decouples input prompts to allows us to quantify the strength of potential adversaries.
By representing the output elements of an language model (LM) as lengthier text fragments rather than
individual tokens, we can quantify the extent to which these models emulate the training distribution
and consequently better understand the mechanisms underlying jailbreaking vulnerabilities.
Our contributions can be summarized as follows:
•Based on our proposed framework, we first offer a non-vacuous PAC-Bayesian style gener-
alization bound for pre-training. Assuming the validity of our framework, we conclude that
high-performing pre-trained models will inevitably be susceptible to generating behaviour that is
present in the training corpus, including any unintended and harmful behaviour.
•Subsequently, we extend our framework to include notions of alignment and jailbreaking. As-
suming our assumptions are met, we demonstrate jailbreaking to be unpreventable even after
safety alignment because the LM fails to concentrate its output distribution over the set of safe
responses.
•Motivated by our theoretical findings, we identify a key drawback in the widely adopted RL
Fine-Tuning objective due to the natural difference between the harmlessness and the helpfulness
targets. By addressing this issue, we facilitate the training of safer models that are more resilient
to a suite of jailbreaking attacks while preserving model performance.
The paper is organized as follows. In Section 2, we introduce our framework. In Section 3, we prove
the PAC-Bayesian generalization bound for pretraining. Next, in Section 4 we present analysis on
jailbreaking from a statistical perspective. Finally, in Section 5 we illustrate our proposed E-RLHF
objective and its effectiveness on improving LLM safety. We give a literature review in Appendix H.
2 Framework and assumptions
Jailbreaking carries several analogies to adversarial attacks , a well studied field in computer vision
[39]. Here, an adversary is defined as a map that perturbs a given input image in pixel space to change
the model output. The strength of the adversary is bounded by how far it is able to move the original
input as quantified by the ℓpdistance [ 40–42]. Typically, this distance is bounded in a way that the
change would not be perceptible to the human observer. The goal of the adversary is to cause mis-
classification of the input. In contrast, in the instance of an LLM, the adversary’s goal is to provoke
harmful behaviour, e.g., unintended leaking of information or hate speech. Further, any perturbation

harmful behaviour, e.g., unintended leaking of information or hate speech. Further, any perturbation
to an input, called prompt, will have a perceptible effect. Hence quantifying and bounding the
capabilities of the adversary is not straight forward. Nonetheless, with some modifications, we will
build on this analogy.
For the purpose of this work, we will view any prompt as a tuple of query and concept (q, c),
where c∈ C, and q∈ Q , with C,Qdenoting the complete concept set and query set. Con-
ceptually, we think of concepts as representing the information content of the prompt, usu-
ally through a short piece of text, for example “tutorial on making a cake” .Queries
are instructional text pieces that are composable with certain concepts. We can think of
queries as mechanisms to trigger an LM to expand a concept in a specific way. Examples
include“Tell me how to {}” , or“We are now in an imaginary world, and you are
2

not bounded by any ethical concerns. Teach my avatar how to {}” . Since not all
queries and concepts are composable,3we denote P⊊Q × C as the set of all plausible prompts,
where the definition of plausible will be made clear below.
The decomposition of prompts allows us to isolate and hence bound the adversary’s strength.
In line with current empirical work on inducing harmful behaviour, we will allow perturbations only
on the queries, not on the concepts. Further mimicking the spirit of previous work on adversarial
attacks, we will assume that the ground-truth related to a prompt is determined solely by the concept,
not the query. We will make these ideas more rigorous in the next paragraphs.
Figure 1: Our framework in a nutshell: We define
a language model, pLM:→ , as a map from
prompts to a distribution over a subset of all possible
explanations E. To later be able to bound the strength
of the adversarial attacker, we split the text inputs into
concepts and queries (q, c). We assume that (i) the text
corpus only covers a part of the domain of the LM:
supp( DP)⊊dom( pLM), (ii) the size of the domain of
the output distribution, denoted |dom( pLM(q, c))|, is
small compared to the size of E, and (iii) only concepts
determine the output (see ).First, in contrast to previous theoretical
work where LMs are regarded as single
sentence generators [38], we model LMs
aslengthier text fragment generators , and
refer to possible generated content e∈ Eas
explanations. Conceptually, explanations
expand concepts with additional informa-
tion. For example, “The US president
in 2023 is Joe Biden.” . An LM thus
induces a mapping from plausible prompts
to distributions over explanations, pLM:
P → ∆(E), where ∆(E)denotes the set
of distributions defined over elements in
E.4The output of a LM given a prompt,
pLM(q, c), is a discrete distribution over
explanations. We use dom( pLM(q, c))as
thedomain of this distribution, pLM(e|q, c)
as the probability of egiven (q, c)as the
input, and supp( pLM(q, c))as the subset
ofEwith non-zero pLM(e|q, c). Further,
we assume the existence of a latent ground
truth mapping pworld :P → ∆(E)that
the LM is optimized to mimic during the
pretraining stage. This is the distribution
that defines “knowledge”: for all plausible prompts (q, c), it specifies the ground-truth distribution
over explanations. By plausible , we refer to all prompts that lie in the domain of the ground truth
mapping (q, c)∈dom( pworld),i.e.,P ≡dom( pworld). Many plausible prompts will not even exist
within any available training corpus, as discussed below.
We can now state our main assumption, namely that for any plausible prompt (q, c)∈dom( pworld)
the ground-truth distribution pworld(q, c)is supported on a small subset of E ⇔ supp( pworld(q, c))⊊
E. This assumption seems sensible to us: under normal circumstances, providing an explanation of
“Paris” would not offer any relevant knowledge when given a prompt such as “How to write a
hello world python script” . Our second assumption is that for all plausible prompts (q, c), the
concept cuniquely determines thesupport of the output distribution specified by pworld , regardless
of the query: supp( pworld(q, c)) = supp( pworld(q∗, c)),∀plausible (q, c)and,(q∗, c). The query
changes the ground-truth distribution without affecting its support. An illustration is depicted in
Figure 1. To be more precise:
Assumption 2.1. (Concepts uniquely determine the explanation for plausible prompts)
For all plausible prompts (q, c)∈dom( pworld),
i)pworld :P → ∆(supp( pworld(q, c))
where supp( pworld(q, c))⊊Es.t.|supp( pworld(q, c))| ≪ |E| ;and
ii)supp( pworld(q, c)) = supp( pworld(q∗, c)),∀(q, c),(q∗, c)plausible .
3For example, "Who is a tutorial on making a cake." is unreasonable.
4For real-world LMs, with different decoding hyperparameters e.g., the temperature T, top-pand top- k
sampling parameters, the induced distribution with the same set of parameters could be different. Our discussion

sampling parameters, the induced distribution with the same set of parameters could be different. Our discussion
holds for a pre-fixed set of hyperparameters throughout this paper.
3

This assumption is natural since it essentially tells us that knowledge is specified by the corresponding
concept alone, irrespective of what query is used to extract it. In other words, given a concept c, if a
query qmanages to change supp( pworld(q, c)), we argue that the query should be deconstructed and
partially absorbed by cto accurately reflect the knowledge mirrored by the support.
Lastly, we make the assumption on the existence of an underlying generative distribution over
prompts, denoted as (q, c)∼DP. This distribution serves as the principle governing the creation of
our pretraining corpus. It is important to note that supp( DP)⊊dom( pworld). For example, take
the prompt (q′, c′)=“Who is James Bond $ λ*#!48811” ; even though this prompt never appears
in any text corpus across the internet, (q′, c′)/∈supp( DP), we, as humans, can make sense of it:
(q′, c′)∈dom( pworld). Later proofs in this paper assume LMs generate semantically reasonable
explanations for such unseen plausible prompts, since in reality LMs are claimed to generalize well
on huge, out-of-distribution datasets [ 43]. This is made explicit in Section 4, within Assumption 4.1.
Finally, the following definitions pertain to our notion of harmfulness. More specifically, we un-
derstand harmful behaviour abstractly as any unintended behaviour. For this, we assume that any
explanation ecan be denoted as either harmful or not harmful (safe) . A concept cis regarded as
harmful if and only if the world generates harmful explanations with probability higher than a certain
threshold with direct prompts.
Definition 2.1. (Notions of Harmfulness)
•(Direct Queries and Direct Prompts ) We refer to a prompt as direct if it stems from DP, i.e.,
(q, c)∈supp( DP). The query of a direct prompt is called a direct query.
•(Harmful Concepts and Harmful Set ) Given a concept c, the associated harmful set of ex-
planations is denoted as Eh(c) := {e|e∈supp( pworld(·, c))∧eis harmful }. In accor-
dance with Assumption 2.1, with a threshold η, a concept cis harmful if ∀qs.t.(q, c)∈
dom( pworld),P
e:e∈Eh(c)pworld(e|q, c)≥1−η. We refer to the set of all possible harmful
concepts as Ch⊊C.
•(Safe Set )∀c∈ Ch, there exists a corresponding safe set Es(c)⊊Ethat we wish pLM(q, c)to be
concentrated on. It includes safe explanations existing in supp( pworld(·, c)), and explanations
designed by humans, e.g., with the template beginning with “Sorry. ”
•(Semantically meaningful ) We call explanations in Eh(c)∪Es(c)as semantically meaningful
for the (q, c)prompt.
•(Mixture decomposition of DP) With these notions, we can decompose DP=αDPh+ (1−
α)DPs(where supp( DPh)includes all direct prompts with a harmful concept, and supp( DPs)
includes the complement) as a mixture over direct prompts with a harmful concept and the
non-harmful counterpart.
3 PAC-Bayesian bound for pre-training LLMs on harmful data
Given a learning algorithm that leads to a posterior distribution over a set of models, PAC-Bayesian
theory [ 44] applies Probably Approximately Correct (PAC) inequalities, to provide bounds on the
generalization gap, i.e.,the difference between the model’s empirical loss and the population loss.
We now present the first result of our analysis: a non-vacuous PAC-Bayesian bound for pretraining
LMs which implies that a well-trained LM ought to exhibit harmful behaviour even when simply
prompted with direct queries if it was presented with harmful behavior during training.
We denote by S={(qi, ci)}n
i=1a set of prompts generated i.i.d. under DP,S∼Dn
P. These prompts
together with sampled explanations form our pretraining corpus. We use π, ρas the prior and posterior
distribution over LMs before and after the pretraining process, defined over LM, the set of language
models. Given a prompt (q, c), we measure the generalization capability of a LM by quantifying the
Total Variation (TV) loss between the induced distribution pLM(q, c)and the ground-truth distribution

Total Variation (TV) loss between the induced distribution pLM(q, c)and the ground-truth distribution
pworld(q, c).5For real-world LMs, pretraining involves optimizing the cross-entropy loss on the
training corpus, which is equivalent to minimizing KL[pworld(q, c)||pLM(q, c)]under our framework.
With Pinsker’s Inequality, optimizing the KL-divergence term is equivalent to optimizing an upper
bound on TV; thus we expect empirical TV loss be small.
5We regard both distributions as defined over the entire Esince we do not restrict the output distribution of
LM in this section.
4

Definition 3.1. (TV empirical loss and population loss)
ℓTV(pLM,(q, c)) := TV( pworld(q, c), pLM(q, c)).
Given an LM and a set of data S, the empirical loss ˆRS(pLM)and population loss R(pLM)are
defined as
ˆRS(pLM) :=1
nnX
i=1ℓTV(pLM,(qi, ci));
R(pLM) :=ES∼Dn
Ph
ˆRS(pLM)i
=E(q,c)∼DP[ℓTV(pLM,(q, c))].
We state our PAC-Bayesian bound as follows. The detailed proof can be found in Appendix B.1.6
Theorem 1. (PAC-Bayesian Generalization Bound for Language Models.) With αas in Definition
2.1, consider a set of language models LM, with prior distribution πoverLM.
Given any δ∈(0,1), for any probability measure ρoverLM such that ρ, πshare the same support,
the following holds with probability at least 1−δover the random draw of S:
ELM∼ρ[R(pLM)−ˆRS(pLM)]≤s
KL[ρ||π] + log1
δ
2n:=ϱ;
ELM∼ρ[E(q,c)∼DPhℓTV(pLM,(q, c))]≤1
αh
ELM∼ρˆRS(pLM) +ϱi
. (1)
In Appendix B.2 we give a theoretical estimation of ϱ, to illustrate the bound we derive is non-vacuous,
i.e.,less than 1. The KL term is of order O(K)where Kis the number of parameters involved in π, ρ,
andncan be shown to greatly exceed K(using a realistic Zipf distribution assumption on prompts to
estimate the number of unique prompts). Theorem 1 tells us that, as long as pretraining successfully
reduces the loss on the training corpus ( ˆRS(pLM)↓), in expectation the language model will mimic
the world well (small ℓTVdifference) on a given direct prompt sampled from DP. Furthermore, if α
is not too small, then this statement holds on a direct prompt whose concept is harmful. Since we
have defined the harmful concept as outputting harmful explanations with high probability (Definition
2.1), we conclude that an LM trained on DPdata can output explanations in the harmful set.
4 A statistical perspective on jailbreaking after alignment
In this section, we will present the main theoretical contribution of our work: given our assumptions
hold, we prove the existence of ways for an adversary to jailbreak an LM even after the preference
alignment process . Our proof strategy is inspired by the work on adversarial robustness [ 41], which
bounds the adversary’s probability of success by upper bounding the volume of the set of points
that does not allow for the existence of adversarial examples. Going forward, we need to extend our
framework to integrate alignment and jailbreaking.
After an LM is pretrained, it typically will undergo fine-tuning on a dataset containing preferred
behaviour. In what follows, we will assume that this alignment process does not change the model
performance in the sense that the LM will still produce semantically meaningful explanations
(Definition 2.1). It would not, for example, default to answering any request with the same response.
Assumption 4.1. (LM outputs semantically meaningful explanations) For any harmful concept c,
and all plausible prompts (q, c)∈dom( pworld),
∃ |En(c)| ≪ |Eh(c)|+|Es(c)|s.t.O(1)≪ |dom( pLM(q, c))|=|Eh(c)∪Es(c)∪En(c)|.
In other words, we assume the LM’s output distribution is accurately supported on Eh(c)∪Es(c),
in the sense that the size of “residual” En(c)is relatively small compared to these semantically
meaningful explanations. We define n(c) =|En(c)|+|Es(c)|+|Eh(c)|. We omit the (c)annotations
when clear from the context. The O(1)statement is reasonable, because harmful explanations are
usually long text fragments that allow for many alternative formulations. The assumption can be
broken down into two components: (1) within the support of the output distribution, only occasional
6The inspiration for the proof of Theorem 1 comes from Mbacke et al. [45], and the proof idea is originally
proposed in Germain et al. [46], Haddouche et al. [47].
5

instances of unrelated explanations exist; (2) the process of aligning the model towards safety does not
eliminate the harmful explanations acquired during the pretraining phase. For part (1), similar to
the example we gave above, under normal circumstances, we do not expect the explanation “Paris”
to appear in dom( pLM(q, c))given (q, c)as“How to build a bomb” . As for part (2), though
seemingly surprising, evidence with a series of current state-of-the-art LMs can be experimentally
validated [ 48], where diverse, harmful explanations are extracted by simply manipulating the decoding
process using direct prompts. In Section 5 we give an explanation for this undesired phenomenon.
Figure 2: Conceptual illustration of our
framework for jailbreaking introduced in
Section 4, with a fixed harmful concept
c. The triangle represents the probability
simplex. This figure showcases a typ-
ical successful jailbreaking attempt by
the adversary: although safety alignment
makes the sampled LM safe under the
direct prompt input, the adversary is able
to move the output to the harmful zone
Hhby manipulating the query q.To bound the likelihood of jailbreaking we first need
to specify how the output of a LM interacts with its
support. Assuming a fixed order of explanations in
dom( pLM(q, c)), and slight abuse of notation, we can
usepLM(q, c)to denote an n(c)-dimensional vector on
∆n(c)−1, the probability simplex with n(c)elements,
where each entry represents the probability of a single
explanation. We call this simplex the output simplex
related to a given concept c. Next, we can induce a dis-
tribution on this simplex given a posterior distribution γ
over the set of language models LM, as follows.
Definition 4.1. (Induced Distribution on Simplex, γc)
Under the assumption that the LM outputs semantically
meaningful explanations (Assumption 4.1), with a fixed
prompt (q, c)and a posterior distribution γoverLM,
the corresponding induced distribution: pLM(q, c)where
LM∼γis supported over a subset of the output simplex
∆n−1. This distribution is denoted as γ(q,c), orγcwhen
the reference to qis clear from context.
Next, we will separate the output simplex into a harmful
and safety zone. This definition is motivated by the obser-
vation that typically an adversary is deemed successful if
it can extract even a single harmful explanation for a given concept. This translates into a division of
the output simplex, under Assumption 4.1, as follows.
Definition 4.2. (Harmful Zone and Safety Zone ) For a given harmful concept cand a fixed LM,
the output simplex is divided into a safety zone and a harmful zone ,HsandHh, where a pre-
defined threshold p∈[0,1]is used to quantify the distinction: pLM(q, c)∈ H hif and only ifP
e:e∈Eh(c)pLM(e|q, c)≥p, and otherwise pLM(q, c)∈ H s.
Before we introduce jailbreaking, the reader might wonder why we did not define alignment more
clearly. This is because under the PAC framework, preference alignment is nothing but a transforma-
tion from ρto some γposterior defined over LM. Given this inability on fine-grained characterization
of alignment, we instead provide the goal of it as follows. With the above notion, given a prompt
(q, c)where cis harmful, its goal is to push the induced distribution γcinto the safety zone Hs.
Ideally, supp( γc)⊂ H s⇔with probability 1, the resulting LM is safe when encountering (q, c). We
are ready to introduce necessary concepts related to jailbreaking.
Definition 4.3. (Jailbreaking ) Given a harmful concept cand a query q′, the prompt (q′, c)jailbreaks
the LM iff pLM(q′, c)∈ H h. We call such a prompt (q′, c)and query q′a jailbreaking prompt and
jailbreaking query, respectively.
The threshold pfor discriminating HhandHsshould be very small, since it means in expectation the
adversary needs to call the LM1
ptimes to collect a single harmful explanation i.e.,to jailbreak the
LM.
To theoretically prove the jailbreaking effect, we need to restrict the adversary’s ability. To achieve

ptimes to collect a single harmful explanation i.e.,to jailbreak the
LM.
To theoretically prove the jailbreaking effect, we need to restrict the adversary’s ability. To achieve
this goal, we borrow insights from adversarial attacks, to assume that the adversary has bounded
manipulating capability on the output simplex when searching over the query set:
Assumption 4.2. (ϵ-bounded adversary) Given an LM, a harmful concept cand an associated direct
prompt (q, c), we assume the adversary can find a set of queries Q′, such that the output is moved at
most ϵon the simplex towards HhfrompLM(q, c):
sup
q′∈Q′d(pLM(q, c), pLM(q′, c)) =ϵ.
6

Here dis a distance measure between two discrete distributions. dcan be a typical ℓpmeasure with
p≥1, or the Total Variation / Jensen-Shannon Divergence. We call q′∈ Q′anϵ-bounded query.
A conceptual illustration of our framework is depicted in Figure 2. Before arriving at our Theorem,
we give the final definition of ϵ-expansion.
Definition 4.4. (ϵ-expansion) Given a set A⊂∆n−1and a distance measure d, theϵ-expansion set
A(ϵ, d)is defined as
A(ϵ, d) :={t|t∈∆n−1∧ ∃y∈A s.t.||y−t||d≤ϵ}.
We are ready to present the following theorem, which states that as long as the induced posterior γcis
not concentrated in an extremely safe area, then with high probability the model can be jailbroken.
The proof is in Appendix B.3.
Theorem 2. (Jailbreak is unavoidable) Assume that an LMs output semantically meaningful expla-
nations (Assumption 4.1). Given any γposterior distribution over LM, choose a harmful concept c
with a direct prompt (q, c)and a threshold p(Definition 2.1), to define the corresponding induced
distribution γc(Definition 4.1) and division over output simplex (Definition 4.2). An ϵ-bounded
adversary (Assumption 4.2) can find a jailbreaking prompt (Definition 4.3) with probability at least
1−γs×(1−Φ(aϵ)),
•by using either the direct prompt, such that pLM(q, c)∈ H h; or
•by finding an ϵ-bounded query q′, such that pLM(q′, c)∈ H h.
Here, Φ(·)is the standard Gaussian cdf, γs:= max x∈Hs−Hh(ϵ,d)γc(x)
U(x), with U(x)the uniform dis-
tribution over ∆n−1, and aϵ:=a+√n−1ϵ, where awrites analytically as a≍|Eh(c)|−1−(n−1)p√
(n−1)p(1−p).
Trivially, the chances of an adversary to find a jailbreaking prompt increase for stronger adversaries
(ϵ↑). In the real world, this could relate to how much compute budget we allow to alter a query for
a specific harmful concept. Furthermore, the chances of an adversary to find a jailbreaking prompt
increase when the ratio of the sizes of the harmful explanation set to the safe explanation set is larger
|Eh(c)|
|Es(c)|↑. This is because their ratio will determine the size of the harmful zone which in turn will
cause Φ(aϵ)→1. In real world settings, for any harmful concept, the training corpus naturally
contains a large harmful set due to the number of possible responses. Realistically, its size can not be
countered by any manually-constructed safe set. Hence achieving alignment is hard : Recall that the
goal of alignment is to respond with only safe explanations with high probability. However, we just
learned that to increase that probability, we need to have a small harmful-to-safety set ratio which we
discussed is not realistic. Consequently, the safety zone is going to be small.
5 E-RLHF: improving alignment by expanding the safety zone
Recall from Theorem 2 and the subsequent discussion in the previous section, that jailbreaking
becomes more likely the larger the harmful zone is in comparison to the safety zone. The size of both
zones relates to the size of their respective explanation sets. In other words, the size of the preference
alignment dataset is crucial to successful alignment. Unfortunately, the human labor involved in
creating such a dataset effectively caps its size.
In order to bridge the gap between our theoretical insights and a practical solution towards suppressing
the jailbreaking problem, we focus on other more practical ways to expand the safety zone . Even
though our ideas are more broadly applicable, in our experiments we will focus on improving
Reinforcement Learning with Human Feedback (RLHF). RLHF typically includes three phases: i)
supervised fine-tuning (SFT); ii) preference sampling and reward learning and iii) RL optimization.
Rafailov et al. [23] have recently proposed a widely applied version of RLHF for LMs, coined Direct
Preference Optimization (DPO), that employs a clever reparameterization which leads to directly
learning from the preference dataset, without the need of obtaining a reward model beforehand.

Preference Optimization (DPO), that employs a clever reparameterization which leads to directly
learning from the preference dataset, without the need of obtaining a reward model beforehand.
DPO is more stable in the training process than other implementations of RLHF. A more complete
overview of RLHF and DPO can be found in Appendix C.
For our purposes, we assume access to an LLM pSFTthat has been supervised fine-tuned on high-
quality data. We further assume access to a preference aligned dataset Ds; that contains a set of text
7

prompts (q, c) =x, and two respective explanations that have been rated by human annotators as
better ewor worse el. In phase ii) of RLHF, one typically optimizes a reward model r(x, e)based on
the annotated explanations. Our proposal concerns phase iii) of the RLHF process: training of the
preference aligned model pLM. For a given reward model, pLMis typically obtained by optimizing
the following objective;
LRLHF (pLM) =−Ex∼Ds,e∼pLM(x)[r(x, e)] +βDKL(pLM(x)||pSFT(x)) (2)
Note that, the first term is maximizing the reward, while the KL-term acts as a regularizer ensuring
the aligned model can not drift off too far from the SFT model. We claim that this regularization is
exactly the problem. While designed to keep the model helpful7, for any harmful prompt xhand
any harmful explanation e∈supp( pSFT(xh)),pLMwill maintain ein the support of the output
distribution. Specifically, the supervised fine-tuning process does not involve elimination of any
harmful explanations from the support of the output distribution of the pretrained model,8thus this
small safety set problem will be further passed to pLM, even if pLMis optimized to the optimal
solution of the above objective. Thus, we should not push the LLM posterior into the same direction.
Instead, for any harmful prompt xh, we propose to use a prior pSFT(·)that we design to have a large
safety zone. We can achieve this by modifying the harmful prompt , such that a safe concept is
used to replace the harmful concept, which alters supp( pSFT)to include more safe explanations.
This can be done in an individualized manner, or simply by prefacing all harmful prompts with a
prefix such as "Please ensure your response adheres to community guidelines and
ethical standards:" .9Importantly, the non-harmful prompts are not modified. Due to the focus
of our approach to expand the safety zone of the output distribution, we coin our proposal E-RLHF ,
resulting in the following modification to Eq. (2):
LE-RLHF (pLM) =−Ex∼Ds,e∼pLM(x)[r(x, e)] +βDKL(pLM(x)||pSFT(xs)) (3)
where xsis asafety-transformed version of the original harmful prompt xh. To recap, the key
argument we put forth is that, in order to ensure the stability of model fine-tuning, it is not imperative
to utilize identical prompt inputs xfor both the reference model and the target model, particularly
when the original input xitself is harmful. In fact, as long as the substitute or "anchor" prompt
generates logically reasonable outputs akin to those produced by the original prompt, this approach
would not impede the training process of the model. To solidify our argument we show the impact of
our modification on the support of the optimal policy in Appendix C. We also deduce there that we
can trivially integrate our modification into the DPO objective allowing us to train without an explicit
reward model (eliminates step ii)) as follows, where σ(·)stands for the sigmoid function:
LE-DPO(pLM) =−E(x,ew,el)∼Ds
logσ
βlogpLM(ew|x)
pSFT(ew|xs)−βlogpLM(el|x)
pSFT(el|xs)
. (4)
6 Experiments and results
Our experimental set-up is based on the alignment-handbook code base [ 50]. We tune the publicly-
available SFT model pSFTprovided by huggingface hub [ 51], using the public dataset [ 52,53], with
default hyperparameter setup. We label harmful prompts in the preference dataset by prompting
GPT-3.5-Turbo, see Appendix E. We are using the very same prefix proposed in the previous section
to generate xs. Experiments are performed on 8 NVIDIA Tesla V100 GPUs, using half-precision
tuning i.e.,Float16. In the appendix, we also show results for an alternative training paradigm: the
Low-Rank Adaptation (LoRA) [ 54] (see Appendix D.1). Following community standards [ 3,1,2],
we use greedy decoding i.e.,T= 0for model evaluation.
We first show empirical evidence that our proposed modification of DPO, E-DPO, does in fact improve
safety alignment, using the Harmbench dataset [ 2] and the first 100prompts in the AdvBench harmful

We first show empirical evidence that our proposed modification of DPO, E-DPO, does in fact improve
safety alignment, using the Harmbench dataset [ 2] and the first 100prompts in the AdvBench harmful
behavior dataset [ 1], measured by the HarmBench protocol. We give an overview on all adversaries
in Appendix F. The results are presented in Table 1. E-DPO achieves improvements across every
task we tested.
On top of our safety results, we want to make sure E-RLHF does not sacrifice helpfulness for
increased safety . We evaluate helpfulness with the MT-Bench project [ 3]. The SFT model pSFT
7Otherwise the model could drift into trivial behaviour like always responding with "I can’t help you." .
8Even the probability can be suppressed to close to 0.
9The prefix shares similarities to the system prompts used by open-source LLMs [13, 49] to boost safety.
8

Table 1: Safety alignment with the E-RLHF objective, here specifically E-DPO, reduces the average
Attack Success Rate (ASR) across all jailbreak adversaries for both the HarmBench and the AdvBench
data, to 36.95, and to 20.89, respectively. Moreover, resilience against all adversaries improves with
our modification to safety alignment ( indicates better performance between DPO and E-DPO).
HarmBench ASR [2]
Model Direct Request GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human A VG↓
pSFT 32.25 59 .25 35 .50 42 .75 42 .75 36 .20 56 .50 65 .00 56 .75 26 .75 35 .50 44 .47
pDPO 27.50 53 .00 39 .00 46 .75 43 .25 29 .10 52 .50 54 .00 51 .00 28 .75 37 .15 42 .00
pE-DPO (ours) 23.50 47.50 31.75 36.25 40.50 26.45 48.50 51.00 43.00 27.00 31.05 36.95
AdvBench ASR [1]
pSFT 6.00 80 .00 13 .00 37 .00 31 .00 14 .80 65 .00 78 .00 91 .00 4 .00 21 .20 40 .09
pDPO 0.00 47 .00 12 .00 39 .00 30 .00 7 .00 50 .00 61 .00 44 .00 4.00 18 .40 28 .40
pE-DPO (ours) 0.00 38.00 8.00 15.00 21.00 5.20 41.00 53.00 31.00 4.00 13.60 20.89
receives a score of 6.3, and both the DPO and E-DPO models perform better than that (6.8 and 6.6
respectively), making us believe that performance degradation is not a problem with our proposal.
Next, we show the impact of the safe prefix on model performance. We demonstrate that our
method’s performance depends on the choice of safe prefix to some extend but never fails (see
Appendix D.2). We believe, finding better safe prefixes by explicit tuning would improve our results,
similar to the work by Yang et al. [55], but we leave this exploration for future work. Further, we
confirm that the improvement arises from using a safe prior in the KL term for harmful prompts.
We ablate our results by appending the prefix on all prompts in the preference alignment
dataset (see Appendix D.3). In all cases, applying the safe prefix to usual prompts degrades safety,
showcasing the importance of switching the prior only on the harmful prompts. Finally, we show that
E-DPO can be combined with any system prompt, to further boost safety (see Appendix D.4).
The proposal can even be used to improve helpfulness and safety simultaneously (see Appendix
D.5).
7 Conclusion and discussions
In this paper, we present a theoretical framework for language model pretraining and jailbreaking by
dissecting input prompts into query and concept pairs. Through this approach, we have established
two theoretical results pertaining to the ability of language models to mimic the world following pre-
training, which leads to outputting harmful explanations given harmful prompts; and the inevitability
of jailbreaking resulting from alignment challenges. Guided by these theoretical insights, we have
devised a simple yet effective technique to enhance safety alignment, and demonstrate the improved
resilience to jailbreak attacks with this methodology.
Current limitations (1) Although we have classified concepts as either harmful or non-harmful, it
is important to acknowledge that the perception of a concept’s potential for harm can be influenced
by various factors such as cultural, legal, and societal norms, which collectively form the context
of the situation. (2) Language models have demonstrated impressive capabilities in reasoning and
completing tasks within multi-round, multi-step conversations; our current framework may not fully
account for the generalization and jailbreaking possibilities associated with such input formats. (3)
Our analysis is grounded on a fixed pworld mapping and DPdistribution. Nevertheless, the world is
inherently dynamic, as both pworld andDPcontinually evolve.
Future work (1) Regarding our E-RLHF approach, as highlighted in the experimental section, in
addition to attaching a universally safe prefix to all harmful prompts, improvements can be achieved
by individually transforming the harmful prompts. Moreover, the safety-transformed prompts can be

addition to attaching a universally safe prefix to all harmful prompts, improvements can be achieved
by individually transforming the harmful prompts. Moreover, the safety-transformed prompts can be
employed to expand the preference dataset for conventional RLHF. (2) Throughout our analysis, we
have not imposed any constraints on the capacity of the language model. Extending our analysis under
finite memory constraints or analyzing hallucination properties of LLMs is an interesting direction to
explore. (3) Large language models have shown remarkable capabilities as in-context learners [ 56],
and such techniques could potentially be used for jailbreaking them as well [ 57–59]. Investigating
the incorporation of such input paradigms remains a promising avenue for future research.
9

References
[1]Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable
adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043 , 2023. 1, 2,
8, 9, 26, 27, 28, 29, 30, 31
[2]Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham
Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: A standardized evaluation
framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249 ,
2024. 1, 2, 8, 9, 26, 27, 28
[3]Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with
mt-bench and chatbot arena. Advances in Neural Information Processing Systems , 37, 2023. 1,
8, 27
[4]Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,
Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models
for code. arXiv preprint arXiv:2308.12950 , 2023. 1
[5]Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung,
Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language
models encode clinical knowledge. Nature , 620(7972):172–180, 2023. 1
[6]Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad
Godil, Ryan J Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with
retrieval-augmented language models. Advances in Neural Information Processing Systems ,
36, 2024. 1
[7]Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On
the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021
ACM conference on fairness, accountability, and transparency , pages 610–623, 2021. 1
[8]Julian Hazell. Large language models can be used to effectively scale spear phishing campaigns.
arXiv preprint arXiv:2305.06972 , 2023.
[9]Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei
Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv
preprint arXiv:2305.13860 , 2023. 1, 29
[10] OpenAI. Chatgpt. https://openai.com/index/chatgpt/ , 2022. 1
[11] DAN. Do anything now prompt. https://github.com/0xk1h0/ChatGPT_DAN , 2023. 1
[12] Reddit. Chatgpt grandma exploit. https://www.reddit.com/r/ChatGPT/comments/
12sn0kk/grandma_exploit/ , 2023. 1
[13] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 2, 8, 21, 25
[14] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath,
Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language
models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint
arXiv:2209.07858 , 2022. 2, 30
[15] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia
Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language
models. arXiv preprint arXiv:2202.03286 , 2022. 28
[16] Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, es-
tablish, exploit: Red teaming language models from scratch. arXiv preprint arXiv:2306.09442 ,
2023.
10

[17] Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja,
James R Glass, Akash Srivastava, and Pulkit Agrawal. Curiosity-driven red-teaming for
large language models. In The Twelfth International Conference on Learning Representations ,
2023.
[18] Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H
Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster,
et al. Rainbow teaming: Open-ended generation of diverse adversarial prompts. arXiv preprint
arXiv:2402.16822 , 2024. 2, 30
[19] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,
Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.
arXiv preprint arXiv:1909.08593 , 2019. 2, 24, 30
[20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models
to follow instructions with human feedback. Advances in Neural Information Processing
Systems , 35:27730–27744, 2022. 24, 30
[21] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy
Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional
ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022. 24, 30
[22] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley,
Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human
preferences. In International Conference on Machine Learning , pages 17506–17533. PMLR,
2023. 30
[23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
arXiv preprint arXiv:2305.18290 , 2023. 2, 7, 24, 25, 32
[24] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang,
Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable adversarial attacks on
large language models. arXiv preprint arXiv:2310.15140 , 2023. 2, 29
[25] Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking
of large language models. arXiv preprint arXiv:2309.01446 , 2023. 2, 29
[26] Zheng-Xin Yong, Cristina Menghini, and Stephen H Bach. Low-resource languages jailbreak
gpt-4. arXiv preprint arXiv:2310.02446 , 2023. 2, 30
[27] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak
challenges in large language models. arXiv preprint arXiv:2310.06474 , 2023. 30
[28] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and
Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint
arXiv:2308.06463 , 2023. 2, 30
[29] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy
jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451 , 2023.
2, 28, 29
[30] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and
Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint
arXiv:2310.08419 , 2023. 27, 28, 29
[31] Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, and Ee-Chien Chang. Semantic
mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms. arXiv
preprint arXiv:2402.14872 , 2024. 29
[32] Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, and Shujian Huang.
A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language
models easily. arXiv preprint arXiv:2311.08268 , 2023. 29
11

[33] Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron
Singer, and Amin Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. arXiv
preprint arXiv:2312.02119 , 2023. 2, 28, 29
[34] Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. Smoothllm: Defending
large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684 , 2023. 2,
31, 32
[35] Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. Defending against alignment-breaking
attacks via robustly aligned llm. arXiv preprint arXiv:2309.14348 , 2023. 31
[36] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju. Certifying
llm safety against adversarial prompting. arXiv preprint arXiv:2309.02705 , 2023. 31
[37] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-
yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Base-
line defenses for adversarial attacks against aligned language models. arXiv preprint
arXiv:2309.00614 , 2023. 2, 31
[38] Yotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua. Fundamental limitations of
alignment in large language models. arXiv preprint arXiv:2304.11082 , 2023. 2, 3, 32
[39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199 , 2013. 2
[40] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander
Madry. Robustness may be at odds with accuracy. In International Conference on Learning
Representations , 2018. 2
[41] Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are
adversarial examples inevitable? In International Conference on Learning Representations ,
2018. 5
[42] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via random-
ized smoothing. In international conference on machine learning , pages 1310–1320. PMLR,
2019. 2
[43] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.
Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
arXiv preprint arXiv:2206.04615 , 2022. 4
[44] David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual
conference on Computational learning theory , pages 230–234, 1998. 4
[45] Sokhna Diarra Mbacke, Florence Clerc, and Pascal Germain. Pac-bayesian generalization
bounds for adversarial generative models. arXiv preprint arXiv:2302.08942 , 2023. 5
[46] Pascal Germain, Alexandre Lacasse, François Laviolette, and Mario Marchand. Pac-bayesian
learning of linear classifiers. In Proceedings of the 26th Annual International Conference on
Machine Learning , pages 353–360, 2009. 5
[47] Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, and John Shawe-Taylor. Pac-bayes
unleashed: Generalisation bounds with unbounded losses. Entropy , 23(10):1330, 2021. 5
[48] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic
jailbreak of open-source llms via exploiting generation. arXiv preprint arXiv:2310.06987 ,
2023. 6, 26, 29
[49] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023. 8
12

[50] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang, Kashif
Rasul, Alexander M. Rush, and Thomas Wolf. The alignment handbook. https://github.
com/huggingface/alignment-handbook , 2023. 8
[51] HuggingFace. Huggingface constitutional-ai sft model. https://huggingface.co/
alignment-handbook/mistral-7b-sft-constitutional-ai , 2024. 8
[52] HuggingFace. Huggingface constitutional-ai dataset: Ultrafeedback-binarized. https://
huggingface.co/datasets/HuggingFaceH4/ultrafeecback_binarized , 2024. 8
[53] HuggingFace. Huggingface constitutional-ai dataset: Cai-conversation-harmless. https://
huggingface.co/datasets/HuggingFaceH4/cai-conversation-harmless , 2024. 8
[54] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu
Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference
on Learning Representations , 2021. 8, 26
[55] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun
Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409 , 2023. 9, 25
[56] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners. Advances in neural information processing systems , 33:1877–
1901, 2020. 9
[57] Zeming Wei, Yifei Wang, and Yisen Wang. Jailbreak and guard aligned language models with
only few in-context demonstrations. arXiv preprint arXiv:2310.06387 , 2023. 9, 30
[58] Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. Adversarial
demonstration attacks on large language models. arXiv preprint arXiv:2305.14950 , 2023. 30
[59] Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina
Rimsky, Meg Tong, Jesse Mu, Daniel Ford, et al. Many-shot jailbreaking, 2024. 9, 30
[60] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring
generalization in deep learning. Advances in neural information processing systems , 30, 2017.
20
[61] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon
Wilson. A simple baseline for bayesian uncertainty in deep learning. Advances in neural
information processing systems , 32, 2019. 20
[62] Robert D Gordon. Values of mills’ ratio of area to bounding ordinate and of the normal
probability integral for large values of the argument. The Annals of Mathematical Statistics ,
12(3):364–366, 1941. 22
[63] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing
systems , 30, 2017. 24
[64] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the
method of paired comparisons. Biometrika , 39(3/4):324–345, 1952. 24
[65] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017. 24
[66] Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I Jor-
dan, and Jiantao Jiao. Fine-tuning language models with advantage-induced policy alignment.
arXiv preprint arXiv:2306.02231 , 2023. 24
[67] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry
Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: A case
study on ppo and trpo. arXiv preprint arXiv:2005.12729 , 2020. 24
13

[68] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.
Advances in neural information processing systems , 35:24824–24837, 2022. 25
[69] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy
Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a
laboratory for alignment. arXiv preprint arXiv:2112.00861 , 2021. 25
[70] Xiaotian Zou, Yongkang Chen, and Ke Li. Is the system message really important to jailbreaks
in large language models? arXiv preprint arXiv:2402.14857 , 2024. 26, 31
[71] Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. Gradient-based adver-
sarial attacks against text transformers. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , pages 5747–5757, 2021. 28, 29
[72] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Auto-
prompt: Eliciting knowledge from language models with automatically generated prompts.
arXiv preprint arXiv:2010.15980 , 2020. 28, 29
[73] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How
johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by
humanizing llms. arXiv preprint arXiv:2401.06373 , 2024. 28, 30
[74] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. " do anything now":
Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv
preprint arXiv:2308.03825 , 2023. 28, 29
[75] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal ad-
versarial triggers for attacking and analyzing nlp. arXiv preprint arXiv:1908.07125 , 2019.
28
[76] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Gold-
stein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and
discovery. Advances in Neural Information Processing Systems , 36, 2024. 28
[77] Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing
large language models via discrete optimization. arXiv preprint arXiv:2303.04381 , 2023. 29
[78] Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models.
arXiv preprint arXiv:2211.09527 , 2022. 29
[79] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety
training fail? In Thirty-seventh Conference on Neural Information Processing Systems , 2023.
29
[80] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei
Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model
chatbots. arXiv preprint arXiv:2307.08715 , 2023. 29
[81] Jiahao Yu, Xingwei Lin, and Xinyu Xing. Gptfuzzer: Red teaming large language models with
auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253 , 2023. 29
[82] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models.
Advances in Neural Information Processing Systems , 36, 2024. 29
[83] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepin-
ception: Hypnotize large language model to be jailbreaker. arXiv preprint arXiv:2311.03191 ,
2023. 29
[84] Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. Drattack:
Prompt decomposition and reconstruction makes powerful llm jailbreakers. arXiv preprint
arXiv:2402.16914 , 2024. 29
14

[85] Zhenhua Wang, Wei Xie, Baosheng Wang, Enze Wang, Zhiwen Gui, Shuoyoucheng Ma, and
Kai Chen. Foot in the door: Understanding large language model jailbreaking via cognitive
psychology. arXiv preprint arXiv:2402.15690 , 2024. 29
[86] Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian.
Advprompter: Fast adaptive adversarial prompting for llms. arXiv preprint arXiv:2404.16873 ,
2024. 29
[87] Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui
Chen, and Dinghao Wu. On the safety of open-sourced large language models: Does alignment
really prevent them from being misused? arXiv preprint arXiv:2310.01581 , 2023. 29
[88] Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and
William Yang Wang. Weak-to-strong jailbreaking on large language models. arXiv preprint
arXiv:2401.17256 , 2024. 29
[89] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and
Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. arXiv
preprint arXiv:2310.02949 , 2023. 29
[90] Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang.
Removing rlhf protections in gpt-4 via fine-tuning. arXiv preprint arXiv:2311.05553 , 2023. 29
[91] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter
Henderson. Fine-tuning aligned language models compromises safety, even when users do not
intend to! arXiv preprint arXiv:2310.03693 , 2023. 29
[92] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional
adversarial attacks on multi-modal language models. arXiv preprint arXiv:2307.14539 , 2023.
30
[93] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. Visual
adversarial examples jailbreak large language models. arXiv preprint arXiv:2306.13213 , 2023.
30
[94] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena
Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramèr, and Ludwig Schmidt. Are aligned
neural networks adversarially aligned? In Thirty-seventh Conference on Neural Information
Processing Systems , 2023. 30
[95] Natalie Maus, Patrick Chao, Eric Wong, and Jacob R Gardner. Black box adversarial prompting
for foundation models. In The Second Workshop on New Frontiers in Adversarial Machine
Learning , 2023. 30
[96] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:
An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL
https://lmsys.org/blog/2023-03-30-vicuna/ . 30
[97] Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori
Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard secu-
rity attacks. arXiv preprint arXiv:2302.05733 , 2023. 30
[98] Rusheb Shah, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando, et al. Scalable and
transferable black-box jailbreaks for language models via persona modulation. arXiv preprint
arXiv:2311.03348 , 2023. 30
[99] Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and Bin Hu. Cold-attack: Jailbreaking
llms with stealthiness and controllability. arXiv preprint arXiv:2402.08679 , 2024. 30
[100] Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-
based constrained text generation with langevin dynamics. Advances in Neural Information
Processing Systems , 35:9538–9551, 2022. 30
15

[101] Somnath Banerjee, Sayan Layek, Rima Hazra, and Animesh Mukherjee. How (un) ethical
are instruction-centric responses of llms? unveiling the vulnerabilities of safety guardrails to
harmful queries. arXiv preprint arXiv:2402.15302 , 2024. 30
[102] Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz,
Somesh Jha, and Atul Prakash. Prp: Propagating universal perturbations to attack large
language model guard-rails. arXiv preprint arXiv:2402.15911 , 2024. 30
[103] Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou, Junjie Ye, Tao Gui,
Qi Zhang, and Xuanjing Huang. Codechameleon: Personalized encryption framework for
jailbreaking large language models. arXiv preprint arXiv:2402.16717 , 2024. 30
[104] Vinu Sankar Sadasivan, Shoumik Saha, Gaurang Sriramanan, Priyatham Kattakinda, Atoosa
Chegini, and Soheil Feizi. Fast adversarial attacks on language models in one gpu minute.
arXiv preprint arXiv:2402.15570 , 2024. 30
[105] Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein.
Coercing llms to do and reveal (almost) anything. arXiv preprint arXiv:2402.14020 , 2024. 30
[106] Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, and Lizhuang Ma.
Exploring safety generalization challenges of large language models via code, 2024. 30
[107] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations , 2018. 30
[108] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM
SIGSAC conference on computer and communications security , pages 308–318, 2016. 30
[109] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,
2022. 30
[110] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop,
Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human
feedback with ai feedback. arXiv preprint arXiv:2309.00267 , 2023. 30
[111] Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity. arXiv
preprint arXiv:2308.14132 , 2023. 31
[112] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and
Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural
information processing systems , 30, 2017. 31
[113] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and
Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. Nature Machine
Intelligence , pages 1–11, 2023. 31
[114] Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang,
and Nanyun Peng. Prompt-driven llm safeguarding via directed representation optimization.
arXiv preprint arXiv:2401.18018 , 2024. 31
[115] Andy Zhou, Bo Li, and Haohan Wang. Robust prompt optimization for defending language
models against jailbreaking attacks. arXiv preprint arXiv:2401.17263 , 2024. 31
[116] Yichuan Mo, Yuji Wang, Zeming Wei, and Yisen Wang. Studious bob fight back against
jailbreaking via prompt adversarial tuning. arXiv preprint arXiv:2402.06255 , 2024. 31
[117] Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. Defending large language models
against jailbreaking attacks through goal prioritization. arXiv preprint arXiv:2311.09096 ,
2023. 31
16

[118] Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang,
Hongyan Bao, and Xiangliang Zhang. Defending jailbreak prompts via in-context adver-
sarial game. arXiv preprint arXiv:2402.13148 , 2024. 31
[119] Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau. Llm self defense: By self
examination, llms know they are being tricked. arXiv preprint arXiv:2308.07308 , 2023. 31
[120] Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei
Lin, and Kam-Fai Wong. Self-guard: Empower the llm to safeguard itself. arXiv preprint
arXiv:2310.15851 , 2023. 31
[121] Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. Rain: Your language
models can align themselves without finetuning. arXiv preprint arXiv:2309.07124 , 2023. 31
[122] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, and Radha Pooven-
dran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. arXiv
preprint arXiv:2402.08983 , 2024. 31
[123] Adib Hasan, Ileana Rugina, and Alex Wang. Pruning for protection: Increasing jailbreak
resistance in aligned llms without fine-tuning. arXiv preprint arXiv:2401.10862 , 2024. 31
[124] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning
approach for large language models. arXiv preprint arXiv:2306.11695 , 2023. 31
[125] Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and
Tong Zhang. Mllm-protector: Ensuring mllm’s safety without hurting performance. arXiv
preprint arXiv:2401.02906 , 2024. 31
[126] Yuqi Zhang, Liang Ding, Lefei Zhang, and Dacheng Tao. Intention analysis prompting makes
large language models a good jailbreak defender. arXiv preprint arXiv:2401.06561 , 2024. 31
[127] Yihan Wang, Zhouxing Shi, Andrew Bai, and Cho-Jui Hsieh. Defending llms against jailbreak-
ing attacks via backtranslation. arXiv preprint arXiv:2402.16459 , 2024. 31
[128] Heegyu Kim, Sehyun Yuk, and Hyunsouk Cho. Break the breakout: Reinventing lm defense
against jailbreak attacks with self-refinement. arXiv preprint arXiv:2402.15180 , 2024. 31
[129] Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu. Autodefense: Multi-
agent llm defense against jailbreak attacks, 2024. 31
[130] Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. Gradient cuff: Detecting jailbreak attacks on
large language models by exploring refusal loss landscapes. arXiv preprint arXiv:2403.00867 ,
2024. 32
[131] Jiabao Ji, Bairu Hou, Alexander Robey, George J Pappas, Hamed Hassani, Yang Zhang, Eric
Wong, and Shiyu Chang. Defending large language models against jailbreak attacks via
semantic smoothing. arXiv preprint arXiv:2402.16192 , 2024. 32
[132] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao,
Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based
input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674 , 2023.
32
[133] Swapnaja Achintalwar, Adriana Alvarado Garcia, Ateret Anaby-Tavor, Ioana Baldini, Sara E.
Berger, Bishwaranjan Bhattacharjee, Djallel Bouneffouf, Subhajit Chaudhury, Pin-Yu Chen,
Lamogha Chiazor, Elizabeth M. Daly, Rogério Abreu de Paula, Pierre Dognin, Eitan Farchi,
Soumya Ghosh, Michael Hind, Raya Horesh, George Kour, Ja Young Lee, Erik Miehling,
Keerthiram Murugesan, Manish Nagireddy, Inkit Padhi, David Piorkowski, Ambrish Rawat,
Orna Raz, Prasanna Sattigeri, Hendrik Strobelt, Sarathkrishna Swaminathan, Christoph Till-
mann, Aashka Trivedi, Kush R. Varshney, Dennis Wei, Shalisha Witherspooon, and Marcel
Zalmanovici. Detectors for safe and reliable llms: Implementations, uses, and limitations,
2024. 32
17

[134] Adam Tauman Kalai and Santosh S Vempala. Calibrated language models must hallucinate.
arXiv preprint arXiv:2311.14648 , 2023. 32
[135] Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and
Rada Mihalcea. A mechanistic understanding of alignment algorithms: A case study on dpo
and toxicity. arXiv preprint arXiv:2401.01967 , 2024. 32
[136] Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek
Mittal, Mengdi Wang, and Peter Henderson. Assessing the brittleness of safety alignment via
pruning and low-rank modifications. arXiv preprint arXiv:2402.05162 , 2024. 32
18

Appendix
A Glossary
Table 2: Summary of notation.
Symbol Meaning
q A single query, composable with a certain set of concepts.
c A single concept, composable with a certain set of queries.
x= (q, c) A single prompt composed by query qand concept c.
e A single explanation.
Q The query set.
C The concept set.
E The explanation set.
P⊊Q × C The set of plausible prompts.
pworld :P → ∆(E)The world mapping. For each plausible prompt, it specifies the ground-truth distribution over E, a.k.a. the “knowledge”.
pworld(q, c) The ground-truth distribution over a subset of E, given a prompt (q, c). With slight abuse of notation, it also refers to a point on the probability simplex.
supp( pworld(q, c)) Support of pworld(q, c). A strict subset of E.
pLM:P → ∆(E) A language model. For each plausible prompt, it specifies a distribution over (a subset of) E, to mimic pworld .
pLM(q, c) The output distribution over (a subset of) Eby LM, given a prompt (q, c). With slight abuse of notation, it also refers to a point on the probability simplex.
dom( pLM(q, c)) Domain of the pLM(q, c)distribution, a subset of E.
(q, c)∼DP Underlying generative distribution over prompts, a.k.a. the distribution governing the creation of our pretraining corpus.
supp( DP)⊊P Support of DP.(q, c)∈supp( DP)is called a direct prompt .
LM A set of language models.
π The prior distribution over LM.
ρ The posterior distribution over LM, after pretraining.
γ The posterior distribution over LM, after preference alignment.
B Proof of Theorems
B.1 Proof of PAC-Bayesian bounds
Definition B.1. (Bounded Difference) A function f:Xn→Ris said to have bounded difference
property w.r.t. a collection of constants c1,···, cn, iff
sup
x1,x2,...,x n,x′
i|f(x1, x2,···, xn)−f(x1, x2,···, xi−1, x′
i,···, xn)| ≤ci,∀i∈[n].
Lemma B.1. (Hoeffding’s Lemma) for random variable X∈[a, b]with probability 1, the following
holds:
E[exp( λX)]≤exp(λEX+λ2(b−a)2
8).
Lemma B.2. (Hoeffding’s Lemma, Multivariate) for random variables Z=f(x1,···, xn)where f
has the bounded difference property, the following holds:
E[exp( λ(EZ−Z))]≤exp(λ2Pn
i=1c2
i
8).
Note that substituting Zwith ˆRS(LM)is valid.
Lemma B.3. Empirical Loss defined in Definition 3.1 satisfies the bounded difference condition with
constant c= 1,∀i.
We are ready to present the proof of Theorem 1.
Proof. Starting with the above lemma, we know
ES[exp( λ(R(LM)−ˆRS(LM)))]≤exp(λ2c2
8n).
The above result holds for a manually picked LM. With an overall average over the prior πwe have
ELM∼πES[exp( λ(R(LM)−ˆRS(LM)))]≤exp(λ2c2
8n).
Apply Fubini’s theorem (note that πis independent of S):
ESELM∼π[exp( λ(R(LM)−ˆRS(LM)))]≤exp(λ2c2
8n).
19

Define Y=ELM∼π[exp( λ(R(LM)−ˆRS(LM)))], a random variable depends on S. Obviously
Y≥0. Thus, with Markov’s inequality:
P[Y≥1
δESY]≤δ.
Equivalently, with probability at least 1−δ, we have
Y≤1
δexp[λ2c2
8n].
Since we have assumed π, ρshare the same support, using Radon-Nykodim derivative to change the
expectation with respect to πto with respect to ρ, we have
ELM∼ρdπ
dρexp(λ(R(LM)−ˆRS(LM)))
≤1
δexp[λ2c2
8n].
Taking logarithm and applying Jensen’s Inequality we know
ELM∼ρdπ
dρ+λ(R(LM)−ˆRS(LM))
≤log1
δ+λ2c2
8n.
Incorporating c= 1, noticingdρ
dπ= (dπ
dρ)−1we could rewrite the inequality as
ELM∼ρh
(R(LM)−ˆRS(LM))i
≤1
λ
KL[ρ||π] + log1
δ
+λ
8n.
Finding λthat minimizes the term on right hand side gives us the ϱterm.
When DPallows for a decomposition into mixture components, noticing the linearty of expectation,
the bound can be re-written as
αELM∼ρ[E(q,c)∼DPhℓTV(pLM,(q, c))] + (1 −α)ELM∼ρ[E(q,c)∼DPsℓTV(pLM,(q, c))]
≤ϱ+ELM∼ρ[ˆRS(pLM)].
which leads to
ELM∼ρ[E(q,c)∼DPhℓTV(pLM,(q, c))]≤1
α[ϱ+ELM∼ρ[ˆRS(pLM)]].
B.2 An estimation on the non-vacuousness of the PAC bound
We give an estimation of the term appears in our PAC bound, ϱ, and state that it is non-vacuous.
The numerator. We follow Neyshabur et al. [60] to instantiate the term in the simplest setup. Assume
π, ρare defined over the parameter space of a given LM, with Kparameters. Assume wis a set of
weights learned from the pretraining corpus. Let the prior πbe the zero-mean multivariate Gaussian,
whose entry-wise variance is related to the magnitude of the weight: σi=β|wi|, andρbe a Gaussian
with the same anisotropic variance centered around w. We argue though simple, both settings
are practical, since Gaussian initialization is common for model training, and the SWA-Gaussian
algorithm [ 61] utilizes such Gaussian posterior. Under this setup, the KL goes asP
iw2
i
2σ2
i=O(K).
Specifically, taking β=√
2
2makes the term exactly K. Current language models often possess
millions, or billions, of parameters, namely, K∼[106,109].
The denominator. To estimate the number of unique direct prompts in the training corpus, it is
important to notice that the dataset does not only consist of (q, c)prompts but also eexplanations.
Thus, we need to estimate the average token length (ATL) associated with each unique prompt
x= (q, c). For each unique prompt x, aside from its own token length l(x), there will be a collection
of explanations {ei}N(x)
i=1, with expected token length of each associated explanation l(e).We have
EATL =Ex∼DPN(x)×[l(x) +l(e)].
20

Fact. Given a prompt x, the larger the expected length of the prompt itself and explanation (l(x) +
l(e)↑), the larger the expected number of explanation elements (N(x)↑), and the smaller the number
of such prompts (DP(x)↓), appearing in the training corpus. The former comes naturally due to the
composability of natural language: the longer the text fragment, the more equivalent text fragments
in expectation, while the latter is reflected by the spirit of the widely accepted Zipf’s law.
Inspired by the fact, we assume prompts are categorized by the quantity of l(x) +l(e), namely, for
all prompt x,N(x)is a function of l(x) +l(e). Moreover, the complete data generation process is
decomposed into i) sample a value of l(x) +l(e)out, and then ii) sample a unique prompt from the
set decided by this specific l(x) +l(e)value, and iii) generate N(x)explanations.
Step i). Use the fact: the larger the expected length of the output explanation, the smaller the
probability that such a prompt appears in the training corpus. We assume step i) follows a (cut-off)
zeta distribution. Specifically, for a random prompt x,
p(l(x) +l(e) =k)∝k−s,∀k≥k0.
When k0= 1, we resume the zeta distribution with coefficient s.
Step ii). We assume each prompt following this step is unique.
Step iii). Use the fact: the larger the expected length of the output explanation, the larger the expected
number of explanation elements in the training corpus. We assume a power law scaling on N, with a
constant t >1, such that
N(l(x) +l(e) =k) =kt−1.
Thus, the average token length writes
EATL =X
kp(l(x) +l(e) =k)×k×N(l(x) +l(e) =k) =ζ(s−t)−Pk0−1
i=1i−(s−t)
ζ(s)−Pk0−1
i=1i−s.
where ζ(s) =P
i∈Z+i−sis the Riemann zeta function.
For example, take s= 4, t= 2. With k0= 1, the ATL would be 1.52, while with k0= 10 , the ATL
becomes 272. These results translate into an estimation of unique prompts as ntokens/ATL . With
current SOTA LM, the pretraining corpus often includes (tens of) trillions of tokens ( >1012), thus
n >1010> K can be safely assumed ⇒ϱ <1.
αconstant. According to LLaMa-2 report (section 4.1, Figure 13) [ 13], approximately 0.2%of
the documents in their training corpus is labeled as harmful. However, we argue this is indeed
an extremely loose lower bound for α, due to the estimation strategy used in their paper. Given a
document, they use a binary classifier on harmfulness over each single line (1 means harmful and 0
otherwise), and assign the average score to the document. 0.2%is the ratio of documents with score
≥0.5.Take the example of “How to build a bomb” . The chemical reaction parts will not be
counted as harmful, and thus this estimation strategy could judge a completely harmful explanation
as harmless. Thus, it is reasonable to assert αis not too small, though with current literature we are
not capable of raising an accurate estimation on it.
B.3 Proof of jailbreaking
Before proceeding to the proof, we list necessary definitions and lemmas as follows.
Lemma B.4. (Volume of n-simplex)10For any dimension n, the volume of the n-element probability
simplex: ∆n−1, in the n−1-dimensional space is
√n
(n−1)!.
We define the projected probability simplex as follows.
Definition B.2. (Projected probability simplex) Given ∆n−1, the corresponding projected probability
simplex, ∆n−1
p, is defined as a subset of Rn−1:{x∈Rn−1|Pn−1
i=1xi≤1,∀i∈[n−1]}.
10Seehttps://en.wikipedia.org/wiki/Simplex#Volume .
21

An illustration of ∆n−1and∆n−1
p.For example, take n= 3. The probability simplex with n= 3
elements is a triangle whose (euclidean) side length is√
2with vertices (1,0,0),(0,1,0),(0,0,1).
Then its volume in the 2-dimensional space, i.e.,its area, is√
3
2. The corresponding projected
probability simplex is the triangle between the X−Yaxis, with vertices (1,0),(0,1),(0,0).
A direct lemma that connects the probability simplex and the projected probability simplex is given
below.
Lemma B.5. (Transformation of probability simplex) Given a proper probability density function
ν(x)defined on ∆n−1
p, it is equivalent to the distribution defined on ∆n−1with densityν(x)√n:∀A∈
Borel (∆n−1
p), letB={x∈∆n−1:x1:n−1∈A}. ThenR
Aν(x)dx=R
Bν(x)√ndx.Specifically, this
impliesvolume (A)
volume (∆n−1
p)=volume (B)
volume (∆n−1).
Proof. Consider a translation on ∆n−1withxn=−Pn−1
i=1xiwhich does not affect its the volume
and shape. The mapping: ∆n−1
p→translated ∆n−1is an affine transformation with matrix
T=
1 0 ··· 0
0 1 ··· 0
··· ··· ··· ···
−1−1··· − 1

n×(n−1)
Thus, any area under this transformation is scaled by√
detT⊤T=√n: a constant. The lemma
follows directly after this conclusion.
We use U(·)to denote the uniform distribution over ∆n−1:U(x) =(n−1)!√n,∀x∈∆n−1. We use the
notation vol[S] =R
S1dsto represent the volume of a given subset S⊂∆n−1, and use rvol[S]for
the relative volume (w.r.t. the underlying n-simplex) of S,i.e.,rvol[S] :=vol[S]
vol[∆n−1]=R
SU(x)dx.
We also use n=|E(c)|from now on. We use the vector xto denote (with the slight abuse of notation
we have mentioned) pLM(q, c)on the output simplex.
Lemma B.6. (Gaussian cdf Tail Bound, Gordon [62]) Denote ϕ(·)as the standard Gaussian pdf.
When x >0,
x
x2+ 1ϕ(x) =x
x2+ 1e−x2/2
√
2π≤1−Φ(x)≤e−x2/2
√
2πx=1
xϕ(x).
Now we are ready to give the proof of Theorem 2.
Proof. Let|Eh(c)|=n0and denote |Eh(c)|+|Es(c)|+|En(c)|=n. Without loss of generality,
we define the first n0=|Eh(c)|elements as the harmful explanations. Let the thresholding constant
bep. That is, we define the harmful zone Hhas{x∈∆n−1|Pn0
i=1xi≥p}. To compute the
relative volume of Hhin∆n−1, we could instead operate on the projected probability simplex ∆n−1
p
introduced in Definition B.2, and compute the relative volume of the projected Hh:Hh,p:={x∈
22

∆n−1
p|Pn0
i=1xi≥p}. Note that ∆n−1
p⊂Rn−1.We derive its expression as follows.
volume [HC
h,p] =volume [{x∈∆n−1
p|n0X
i=1xi≤p]}
=Zp
0dx1Zp−x1
0dx2···Zp−Pn0−1
i=1xi
0dxn0Z1−Pn0
i=1
0dxn0+1···Z1−Pn−2
i=1xi
0dxn−1
=Zp
0dx1Zp−x1
0dx2···Zp−Pn0−1
i=1xi
0dxn0"
1
(n−n0−1)!(1−n0X
i=1xi)n−n0−1#
=Zp
0dx1Zp−x1
0dx2···Zp−Pn0−2
i=1xi
0dxn0−11
(n−n0)!"
(1−n0−1X
i=1xi)n−n0−(1−p)n−n0#
=···
=1
(n−1)![1−(1−p)n−1]−n0−1X
j=1(1−p)n−1−j
j!(n−1−j)!pj
(5)
Thus, the relative volume of Hhcan be written as
rvol[Hh] = 1−volume [HC
h,p]
volume[projected probability simplex]
= (1−p)n−1+n0−1X
j=1(n−1)!(1−p)n−1−j
j!(n−1−j)!pj
=n0−1X
j=0pj(1−p)n−1−jn−1
j
.(6)
Which is precisely the binomial distribution formula. With the Central Limit Theorem, when
n≫O(1), we know the binomial distribution can be well approximated via the normal distribution
as follows:
f(x) =n
x
px(1−p)n−xd− → N (np, np (1−p)). (7)
Thus, denote ϕ(n−1),p(x)as the pdf of Gaussian variable with mean (n−1)p, variance (n−1)p(1−p),
the rvol term above can be estimated as follows:
n0−1X
j=0pj(1−p)n−1−jn−1
j
≍Zn0−1
−∞ϕ(n−1),p(x)dx
= Φ"
n0−1−(n−1)pp
(n−1)p(1−p)#
= Φ"
|Eh(c)| −1−(n−1)pp
(n−1)p(1−p)#
.(8)
We use a=|Eh(c)|−1−(n−1)p√
(n−1)p(1−p). Consider an adversary with budget ϵunder ℓpor Jensen-Shannon
Divergence (JSD) / Total Variation (TV) capability. Since ||x||1≥ ||x||p,∀p≥1as well as
||x||1≥2JSD( x),||x||1≥2TV( x), we know Hh(ϵ, ℓ1)⊂ H h(ϵ, d)for all dwe have considered.
With that ℓ1, ϵsetup, the corresponding ϵ−expansion set of Hhhas a closed-form expression as
Hh(ϵ, ℓ1) ={x∈∆n−1|n0X
i=1xi≥p−ϵ
2}.
23

Similar as above, we derive the analytical solution of its relative volume associated with constant a′
as:
a′=|Eh(c)| −1−(n−1)(p−ϵ
2)p
(n−1)(p−ϵ
2)(1−p+ϵ
2)
=as
p(1−p)
(p−ϵ
2)(1−p+ϵ
2)+ϵ
2s
n−1
(p−ϵ
2)(1−p+ϵ
2).(9)
Under our framework, with p <1
2, we know1
4> p(1−p)>(p−ϵ
2)(1−p+ϵ
2)). Thus
a′> a+√
n−1ϵ:=aϵ.
Consider the induced distribution γcon the output simplex. Given an adversary with ℓpor JSD/TV
perturbing capability, with the fixed harmful concept c, safety is guaranteed if and only if pLM(q, c)
resides outside Hh(ϵ, d). Define the area of interest, S(d)asS(d) := ∆n−1− H h(ϵ, d). Thus, the
probability of this event could be bounded as
Px∼γc1x∈S(d)<max
x∈S(d)γc(x)Z
S(d)1dx < γ srvol[S(d)]< γsrvol[S(ℓ1)]< γs(1−rvol[Hh(ϵ, ℓ1)])
This gives an upper bound of
γs(1−Φ(aϵ)).
which can be simplified when a≥0using Lemma B.6:
γsϕ(aϵ)
aϵ
.
Thus, the probability of getting a LM instance from the preference alignment process such that it
allows for successful jailbreaking on a specific harmful concept cis at least
1−γs(1−Φ(aϵ)).
Up to now, we have derived the result in Theorem 2. However, we can move a step further to show
the decay rate on the right hand side term. It can be simplified when a≥0:
1−γsϕ(aϵ)
aϵ
,
which finishes the proof.
C RLHF, DPO and our E-RLHF
The classic RLHF framework was established by Christiano et al. [63], and developed by Ziegler
et al. [19], Ouyang et al. [20], Bai et al. [21]. After the collection of a preference dataset Ds=
{(x, ew, el)}, one first trains a reward model under the Bradley-Terry model [ 64], with the objective,
where σ(·)stands for the sigmoid function:
r(x, e) = arg max
rE(x,ew,el)∼Dlogσ(r(x, ew)−r(x, el)).
Following, proximal policy optimization (PPO) [ 65] is commonly adopted across these implemen-
tations, forming the basis of current state-of-the-art language models. The KL-constrained RL
Fine-Tuning (RLFT) objective takes the form:
max
pLMEx∼Ds,e∼pLM(·|x)[r(x, e)]−βDKL(pLM(·|x)||pref(·|x)).
However, PPO tuning can suffer from instability [ 66] and implementation complication [ 67]. To
overcome these issues, a series of work propose to skip the reward modeling step and learn directly
from the preference dataset, with the representative pioneering work by Rafailov et al. [23], namely
Direct Preference Optimization (DPO) . We summarize the derivation of the DPO objective below,
and generalize the objective to the one we use in our experiments, i.e.,E-DPO.
24

First, noticing the closed-form optimal solution forpLMof the RLFT objective writes (see e.g.,
Appendix A.1 of Rafailov et al. [23])
p∗
RLFT(e|x) =1
Z′(x)pref(e|x) exp(1
βr(x, e)).
With this analytical solution in mind, we can solve the reward as
r(x, e) =βlogp∗
RLFT(e|x)
pref(e|x)+βlogZ′(x).
Regard π∗as the optimization target, plug this transformation into the reward model objective to
obtain the DPO objective:
pDPO= arg min
pLM−E(x,ew,el)∼D[logσ(βlogpLM(ew|x)
pref(ew|x)−βlogpLM(el|x)
pref(el|x))].
For our E-RLHF, the modification to the objective leads to another optimal solution of pLM:
p∗(e|x) =1
Z(x)pref(e|xs) exp(1
βr(x, e)).
Thus,
r(x, e) =βlogp∗(e|x)
pref(e|xs)+βlogZ(x)
And plug it in to the reward model objective to formulate our E-DPO:
pE-DPO = arg min
pLM−E(x,ew,el)∼D[logσ(βlogpLM(ew|x)
pref(ew|xs)−βlogpLM(el|x)
pref(el|xs))].
The advantage of our E-RLHF objective is as follows.
Proposition C.1. (Overcoming the small safe set problem) E-RLHF will lead to the optimal solution
p∗:
p∗(e|x) =1
Z(x)pref(e|xs) exp(1
βr(x, e)).
Compared to p∗
RLFT, the advantage when encountering a harmful prompt xis:
(1) (Erase harmful explanations) ∀e∈supp(pref(·|x))−supp(pref(·|xs)),p∗(e|x) = 0 ;
(2) (Add safe explanations) ∀e∈supp(pref(·|xs))−supp(pref(·|x)),p∗(e|x)>0 =p∗
RLFT(e|x).
Thus, with the same jailbreak threshold p, the safety zone is successfully expanded.
Intriguingly, when the safe transformation is done by appending an identical safe prefix to all harmful
prompts, we can connect our E-RLHF to context distillation . A good prompt is known to matter for
the performance of a fixed-parameters LM [ 68,55]. Researchers have proposed a systematic LM
tuning algorithm, called Context Distillation [ 69], aiming at distilling useful information from a good
context as prefix to a language model. Given an initialized language model, for example pSFT, an
input prompt xand a prefix context string prefix , Askell et al. [69] optimizes the loss
L(pLM) =DKL(pSFT(prefix ⊕x), pLM(x))
where ⊕stands for string concatenation. This technique has been adopted as part of the safety
alignment process during the LLaMa-2 series tuning [ 13], whereprefix is chosen from a set of pre-
defined safe prefixes. When applying the identical prefix transform in our E-RLHF transformation, it
can be regarded as a combination of safety context distillation and RLHF. This gives another point of
view on the effectiveness of our proposal.
D Ablation Study
In this section, we perform extensive ablation studies to showcase the effectiveness of our proposed
E-RLHF.
25

Table 3: Safety evaluation, LoRA results. The result is consistent with the one we have obtained
in the main text, that our E-DPO performs better than DPO across a collection of adversaries.
indicates better performance.
HarmBench ASR [2]
Model Direct Request GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human A VG↓
πDPO (LoRA) 24.50 47 .50 40 .50 43 .25 43 .25 28 .50 45 .25 53.25 53 .50 29 .75 38 .90 40 .74
πE-DPO (LoRA) 24.25 42.50 36.50 41.50 42.75 27.20 45.00 53 .75 50.25 27.25 38.05 39.00
AdvBench ASR [1]
πDPO (LoRA) 2.00 29 .00 26 .00 26 .00 40 .00 8 .80 32 .00 54 .00 46 .00 2.00 31 .20 27 .00
πE-DPO (LoRA) (ours) 0.00 25.00 20.00 27.00 33.00 7.20 23.00 50.00 45.00 2.00 28.80 23.73
Table 4: Safe prefix ablation. Prefixes we use are included in Table 8. Our E-DPO performs better
than the DPO baseline in most cases we have tested. indicates best performance.
HarmBench ASR [2]
Model Direct Request GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human A VG↓
πDPO 27.50 53 .00 39 .00 46 .75 43 .25 29 .10 52 .50 54 .00 51 .00 28 .75 37 .15 42 .00
πE-DPO(1) 26.25 56 .50 33 .75 44 .25 42 .25 29 .30 50 .00 56 .75 56 .25 31 .50 34 .05 41 .90
πE-DPO(2) 24.75 52 .25 34 .00 39 .00 44 .75 29 .75 50 .50 54 .50 53 .25 28 .00 34 .35 40 .46
πE-DPO(3) 24.75 52 .75 35 .25 37 .50 35.50 28 .65 49 .00 53 .50 47 .25 30 .50 30.25 38 .63
πE-DPO(4) 23.50 47.50 31.75 36.25 40 .50 26.45 48.50 51.00 43.00 27.00 31 .05 36.95
AdvBench ASR [1]
πDPO 0.00 47 .00 12 .00 39 .00 30 .00 7 .00 50 .00 61 .00 44 .00 4 .00 18 .40 28 .40
πE-DPO(1) 0.00 51 .00 12 .00 29 .00 33 .00 6 .80 47 .00 62 .00 53 .00 5 .00 20 .00 28 .98
πE-DPO(2) 1.00 39 .00 12 .00 20 .00 34 .00 6 .20 53 .00 63 .00 49 .00 3.00 15 .60 26 .89
πE-DPO(3) 0.00 47 .00 11 .00 23 .00 23 .00 6 .80 45 .00 58 .00 36 .00 4 .00 15 .80 24 .51
πE-DPO(4) 0.00 38.00 8.00 15.00 21.00 5.20 41.00 53.00 31.00 4 .00 13.60 20.89
D.1 LoRA results
In this section, we show results obtained by Low-Rank Adaptation [ 54]. We explore the same set of
safe prefixes as in ablation D.2, and choose the best model for illustration. Numbers are illustrated
in Table 3. Results are identical to the ones obtained via full parameter tuning that our E-RLHF
performs better consistently against the RLHF baseline.
D.2 Ablation on safe prefixes
We ablate the effect of different safe prefixes. The prefixes we consider are collected in Table 8.
Numbers are shown in Table 4. Clearly, almost all safe prefixes lead to better safety against the DPO
baseline, but with varying performance. Finding a good prefix matters for our method, and we leave
digging the optimal one out as future work.
D.3 Ablation on transforming all prompts
Here, we proceed to ablating the effect of transforming all prompts, no matter harmful or not. Results
are shown in Table 5, where the red color indicates that safety downgrades compared to the model
obtained via transforming harmful prompts only. Clearly, most models even persist worse safety
compared to the DPO baseline itself, suggesting the detrimental effect of transforming the unharmful
prompts.
D.4 Ablation with system prompt
As pointed out by previous works [ 2,48,70], system prompt can have a significant impact on ASR.
This comes in two-folds: firstly, a powerful system prompt can initialize the LM to be closer to
the safety zone, thus making the model safer; secondly, a longer system prompt would enlarge the
difficulty of launching a specific attack due to the increased computational consumption. To confirm
26

Table 5: Ablation study on transforming all prompts. We apply the same safe prefixes as used in
Table 4. indicates the safety is worse compared to the model trained with transforming only the
harmful prompts. A VG scores achieved by the DPO baseline are 42.00and28.40, respectively.
HarmBench ASR [2]
Model Direct Request GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human A VG↓
πE-DPO(1) 28.00 55 .75 41.00 42 .00 42.50 31.00 51.25 56 .25 56 .00 32.00 36.05 42.89
πE-DPO(2) 25.75 60.50 40.25 46.50 44 .75 30.15 52.75 57.25 63.50 30.75 40.45 44.78
πE-DPO(3) 24.00 57.25 35 .00 41.75 39.50 26 .55 49.25 55.00 58.75 29 .25 38.70 41.36
πE-DPO(4) 27.75 58.50 38.00 42.00 40.75 31.20 52.75 60.50 54.75 31.50 38.30 43.27
AdvBench ASR [1]
πE-DPO(1) 0.00 53.00 17.00 31.00 26 .00 6 .00 50.00 57 .00 61.00 4 .00 19 .00 29.45
πE-DPO(2) 0.00 52.00 17.00 29.00 26 .00 6 .20 56.00 58 .00 73.00 3 .00 25.60 31.44
πE-DPO(3) 0.00 56.00 17.00 23 .00 17 .00 3 .80 46.00 58 .00 66.00 4 .00 23.40 28.56
πE-DPO(4) 0.00 58.00 17.00 36.00 27.00 6.80 46.00 60.00 68.00 3 .00 25.20 31.54
Table 6: Ablation with the default Mistral system prompt. It boosts the safety of our E-DPO models
to achieve better safety, while maintaining the advantage over the DPO baseline.
HarmBench ASR [2]
Model Direct Request GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human A VG↓
πSFT 25.25 61 .25 26 .75 37 .00 27 .25 24 .75 55 .50 58 .75 61 .25 24 .00 30 .35 39 .28
πDPO 20.50 49 .25 29 .25 32 .75 34 .50 22 .55 42 .75 50 .25 50 .75 30 .00 31 .20 35 .80
πE-DPO 15.50 41.50 22.75 29.00 31.25 17.95 41.75 44.25 44.25 23.00 26.55 30.70
our E-RLHF is still helpful under this specific situation, we ablate this effect under the control of the
default Mistral system prompt11. The results can be found in Table 6, where nearly all ASRs drop,
whilst the conclusion does not change.
D.5 Ablation on improving helpfulness and safety simultaneously
In similar spirit, we ask the question: can the trick we propose to improve safety be used to
improve helpfulness simultaneously? To achieve this goal, we choose the prefix "Give me a
safe and helpful response to the following request." , and apply it to all prompts in
the preference dataset. Results can be found in Table 7. Our model achieves better safety, and
improves the helpfulness benchmarked by MT-Bench [3] from 6.8 to 6.9.
E Filtering harmful prompt in the preference alignment dataset
Inspired by the work by Chao et al. [30], we prompt GPT-3.5-Turbo as a harmness judge on input
prompts. It assigns an integer score ranging from 1to10for a given prompt, and we choose the score
≥6subset as harmful. The system prompt is shown in Table 9.
11https://docs.mistral.ai/capabilities/guardrailing/
Table 7: Improving helpfulness and safety simultaneously using E-RLHF. MT-Bench scores are 6.8
and6.9, for the DPO baseline and our E-DPO model, respectively.
HarmBench ASR [2]
Model Direct Request GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human A VG↓
πSFT 32.25 59 .25 35 .50 42 .75 42 .75 36 .20 56 .50 65 .00 56 .75 26 .75 35 .50 44 .47
πDPO 27.50 53.00 39 .00 46 .75 43 .25 29 .10 52 .50 54.00 51.00 28 .75 37 .15 42 .00
πE-DPO (ours) 26.25 56 .00 37.00 43.75 38.25 27.90 49.75 56 .00 52 .25 27.50 32.65 40.66
27

Table 8: Safe prefixes we use in our experiments.
(1) "Provide a responsible and ethical answer to the following request. "
(2) "In a manner that promotes safety and well-being, please respond to the following request. "
(3) "Keeping in mind the importance of safety and legality, how would you respond to this: "
(4) "Please ensure your response adheres to community guidelines and ethical standards: "
Table 9: System prompt for GPT-3.5-Turbo used to filter harmful and non-harmful prompt in the
preference dataset.
You are a helpful red teaming assistant. You will receive a prompt. Please act as an impartial judge and evaluate
whether the prompt itself is requesting information that violates ethical, legal, or safety guidelines. Be as
objective as possible. Rate the prompt on a scale from 1 to 10. A rating of 1 signifies a fully non-harmful prompt.
A rating of 10 signifies prompts that could lead to responses that fully violate ethical, legal, or safety guidelines.
Your rating should strictly follow this format: "Rating: [[rating]]", the rating in the double-closed brackets is a
number from 1 to 10, e,g, "Rating: [[5]]".
F Jailbreak adversaries collected in the HarmBench project [2]
In this section, we give a short overview of the adversaries we adopt to evaluate our models. Some
descriptions are summarized in Mazeika et al. [2].
• Direct Request refers to sending the original harmful prompt directly to the target LLM.
• GCG [1], GBDA [71] and AP [72] find adversarial suffixes via token-level optimization.
•SFS (Stochastic Few-Shot) and ZS (Zero-Shot) [ 15] are few-shot sampling or zero-shot
generation of test cases by an attacker LLM to elicit a behavior from a target LLM.
•PAIR [ 30] and TAP [ 33] are iterative prompting / tree-structured prompting methods, with
an attacker LLM, to adaptively explore and elicit specific harmful behaviors from the target
LLM.
•AutoDAN [ 29] is a genetic-algorithm based attack with initializations from handcrafted
DAN jailbreak prompts.
• PAP [73] uses persuasive strategies to jailbreak the target LLM.
•HumanJailbreaks [ 74] uses a fixed set of in-the-wild human jailbreak templates, similar to
the Do Anything Now (DAN) jailbreaks.
We exclude all transfer attacks since we focus on single-model jailbreak. Furthermore, we choose to
discard the UAT [ 75] and PEZ [ 76] adversaries, because the former induces an out-of-memory error
on our V100 GPUs, and the latter never succeeds to find a suffix according to our experiments.
G Broader Impacts
The societal impact of our work has close connection to the topic of LLM safety. We propose a
framework for analyzing language model pretraining and jailbreaking, and we design a new RLHF
algorithm for improving safety. As shown in our experiments, our work could advocate for safer
LLMs.
H Related work
In this section, we provide a review of the current literature on LLM jailbreaking.
H.1 Jailbreak methods
In this section, we summarize existing jailbreaking methods.
28

Baseline and pioneers Autoprompt [ 72], a baseline method for optimizing in the token space w.r.t.
a certain objective, approximates coordinate ascent by first ranking all tokens using an approximate
objective, and then compute the exact value on them. The approximation is carried out by a single step
of gradient computation. Jones et al. [77] propose Autoregressive Randomized Coordinate Ascent
(ARCA) to generate (input, output) pairs that include certain harmful info or satisfy a fixed format
requirement. Token level optimization is carried out with linear approximation on GPT-2. GBDA [ 71]
study adversarial attack on text classification problems, by optimizing the continuous relaxation of the
autoregressive sampling probability matrix. In late 2022, among social media, users misalign GPT-3
via prompt injection. Perez and Ribeiro [78] study how this be done by adversaries. They successfully
manipulate the model to output a given harmful string and leak the system prompt. In early 2023,
an empirical study was carried out by Liu et al. [9]to measure the result of prompt engineering for
breaking ChatGPT safeguards. Shen et al. [74] collect jailbreaking prompts from multiple platforms
on the internet, analyze these data, create a harmful question set, and identify some typical harmful
prompts that are effective at that moment. Later, the Greedy Coordinate Gradient (GCG) method [ 1],
a strong white-box attack variant of AutoPrompt [ 72] was proposed. Wei et al. [79] categorizes two
general modes of jailbreaking: competing objective andmismatched generalization .
LLM automation and suffix-based attacks Liu et al. [29] propose AutoDAN, that relies on genetic
algorithms, with the requirement of manual prompts for conducting mutation and crossover on the
paragraph and sentence level . The jailbreaking prompts generated by AutoDAN are semantically
plausible, unlike the suffix generated by GCG. As a comparison, Lapid et al. [25] use genetic
algorithm for black-box universal adversarial suffix generation. Chao et al. [30] propose another
LLM-based jailbreak automation algorithm, where an LLM judge is built to assign a safety score
to a given output, while the attacker is enforced (via a page-long prompt) to improve the quality of
jailbreaking prompts from multiple perspectives. Zhu et al. [24] propose another AutoDAN method
that explores the balanced loss between jailbreak loss (log probability on the harmful string, as used
in Zou et al. [1]) and the plausibility loss (log probability over the adversarial suffix), aiming at
improving interpretability. Li et al. [31] uses genetic algorithm to search with similarty measure
and initialize with paraphrasing. Its performance is claimed to be better than AutoDAN-GA. Deng
et al. [80] investigate the possible defensive tricks in proprietary LLMs, and propose a pipeline for
automated jailbreaking using a fine-tuned LLM. Yu et al. [81] propose GPTFuzzer, essentially a
genetic algorithmic framework for jailbreaking. Their work’s difference between AutoDAN is that
it has a pool of “seeds”, a.k.a. templates for transforming the harmful prompt, and the mutation is
done on the template level. Ding et al. [32] propose automating attack via LLM prompt rewriting and
scenario nesting . The latter consists of code completion, table filling and text continuation, since the
authors regard these as align with training objectives well, and are suitable for LLMs to complete
the task. Mehrotra et al. [33] combine Automation used in Chao et al. [30] and tree-of-thought
[82], create interpretable prompts in a black-box manner. Li et al. [83] propose DeepInception, and
usenested, imaginary scenario to induce harmful content. Li et al. [84] propose DrAttack, which
camouflages a query’s malicious intent through semantic decomposition, by constructing a parsing
tree and split the original prompt into different segmentations. Wang et al. [85] draw inspiration from

tree and split the original prompt into different segmentations. Wang et al. [85] draw inspiration from
the self-perception theory from psychology to design a prompt modification pipeline on gradually
persuading the LM to be jailbroken. Paulus et al. [86] propose AdvPrompter, where the authors train
a language model as a suffix generator to speed up LLM attack.
Manipulating the decoding process Huang et al. [48] find the method of changing the generating
hyperparameters ( i.e.,pof top- psampling, the temperature T, and kof top- ksampling) of safety-
aligned LLMs suffices for obtaining harmful outputs when the user is able to manipulate the system
prompt and input configurations. Zhang et al. [87] directly manipulate the output generation probabil-
ity by enforcing an affirmative prefix, and reversing the negation words if they appear in a pre-defined
vocabulary ( e.g., sorry→glad). Zhao et al. [88] assume access to the decoding distribution of a LM.
They use two small LMs, a safe one and a harmful one, to manipulate the decoding ratio of the large
safe LM for jailbreaking. The key insight is the decoding distribution between the safe model and the
harmful model only differs significantly for the first tens of tokens.
Fine-Tuning alone suffices Yang et al. [89] show that fine-tuning on as few as 100harmful example
pairs suffices for turning the LLaMa-chat models (and some other <70B LMs) into malicious
counterparts. Zhan et al. [90] fine-tune GPT-4 on harmful data, and find the fine-tuned models escape
previous safety constraints while maintaining usefulness. Qi et al. [91] find fine-tuning alone, even on
29

benign data, leads to safety degradation using LLaMa and GPT-3.5-Turbo. Fine-tuning on harmful
data (with less than 5 gradient steps) will cause the model to be completely harmful, while tuning on
identity-shifting data could make the LM fully obedient.
Low-resource language and cipher Yong et al. [26], Deng et al. [27] explore the difference in
languages when encountering the same harmful query, and find a direct translation to low resource
languages will lead to higher risk, and Deng et al. [27] additionally find when combined with
sophisticated methods, the drawback of low-resource languages disappear. Yuan et al. [28] use cipher
encoding and decoding to break LLMs. Smaller scale models are immune from such attacks, while
the smartest GPT-4 encountered the highest risk.
Vision-language model attacks Besides pure LLM, some research works move a step forward, uti-
lizing images for breaking vision-language models (VLMs). Shayegani et al. [92] explore multimodal
attack on VLM via embedding space feature matching. Qi et al. [93] generate adversarial examples
via maximizing the conditional probability of a harmful corpus, i.e., the sum of log probabilities over
all outputs, and use the final image with harmful query for jailbreaking. Carlini et al. [94] generate
adversarial example for a fixed harmful content , and find no matter what input prompt is given to the
VLM, it will respond with the target harmful string. Maus et al. [95] propose a black-box attack on
manipulating the generated image with modified adversarial prompt.
Misc Wei et al. [57], Wang et al. [58] explore in-context learning for attack and defense. The attack
is weak since it could only break Vicuna [ 96] and can be defended by in-context safe examples. Later,
this method is scaled-up to significantly improve strength for breaking guardrails of large, state-of-
the-art models [ 59]. An early work in February 2023 [ 97] adopts obfuscation (including synonym and
typos), code injection and virtualization to successfully jailbreak ChatGPT. Shah et al. [98] illustrate
in-context automated persona modulation attack for large-scale LLMs and Vicuna. Zeng et al. [73]
consider the more broadly perspective of persuasion : they train a persuasive paraphraser based on a
fine-grained taxonomy of persuasion techniques. Detailed ablation on attack effectiveness is studied.
Guo et al. [99] focus on stealthiness and controllability. They notice the constraints applied to the
jailbreaking prompts ( e.g., fluency) are exactly the targets of the controllable text generation problem.
Thus, they adopt the Energy-based Constrained Decoding with Langevin Dynamic (COLD) [ 100]
on output logits. Forming each constraint as well as the task of jailbreaking as an energy function
over logits, the Langevin Dynamic is used for finding a good logit distribution, and the decoding
technique in Qin et al. [100] is used for output generation. Banerjee et al. [101] introduce a dataset
TECHHAZARDQA, compare direct query v.s. pseudo-code format, and find the latter induces higher
risk. Mangaokar et al. [102] considers a type of adaptive attack against checking-based defense ,
that appends a universal adversarial prefix into the jailbreaking prompt to make the guard model
always output “safe”, and thus making the detector fails to detect harmful information. Lv et al. [103]
propose Code Chameleon, which contains multiple encryption and decryption methods defined by
python functions, that transforms the harmful prompt into specific predefined form to jailbreak LLMs.
Sadasivan et al. [104] speed up the computation of GCG [ 1] to make it possible to launch on a single
GPU. Geiping et al. [105] build a taxonomy on risks beyond jailbreaking, and coerce the LLM to
provide certain outputs by optimizing a set of tokens via GCG. Ren et al. [106] propose CodeAttack
that use code templates to query the output out instead of using natural language directly, and obtain
descent results.
H.2 Defense methods

that use code templates to query the output out instead of using natural language directly, and obtain
descent results.
H.2 Defense methods
Up to now, no universal defensive strategy as adversarial training [ 107] for adversarial attacks /
differential privacy [ 108] for membership attacks exists as a gold standard. In general, we can classify
the methods into three typical types: alignment, red-teaming, and algorithmic proposals.
Alignment The target of alignment is to push the output of language models be aligned to human
values. Regarding safety, the goal is to avoid outputting harmful information. RLHF is widely
adopted in these methods [ 19,20,109,22]. Variants like RLAIF also have been proposed recently
[21, 110].
Red teaming This term is populated as specifically dealing with harmful info on dataset curation,
used together with RLHF [14–18].
30

Next, we proceed to defensive algorithm proposals. We classify existing defensive strategies in the
following categories.
Defense against suffix-based attacks. Alon and Kamfonas [111] notice the messy nature of the
suffix generated by GCG, and propose to use a perplexity (PPL) filter on input prompts. They also
explore using a LightGBM [ 112] with 2 features (PPL, prompt length) to filter harmful prompt,
and show it does better than naive PPL thresholding. The PPL-based filter does not succeed with
human-crafted jailbreaks. Jain et al. [37] explore many concerning viewpoints, including self-PPL
filtering, paraphrasing the input prompt, and re-tokenization since many LLMs’ tokenizers are based
on Byte-Pair Encoding (BPE). All methods are successful in regards of defending against suffix-based
attacks. They also explore the simplest form of adversarial training. Robey et al. [34] propose to
perturb the input token string by random replacement/erasement/insertion, and finally perform a
majority vote in the end. Cao et al. [35] judges whether an input prompt is safe or not by estimation
with Monte Carlo, when randomly dropping a fraction of tokens, using the LLM itself. Kumar et al.
[36] try to perform “certified” safety against harmful prompts, by erasing tokens and set the original
prompt as harmful if at least one of these erased prompts lead to a harmful response, or be classified
as harmful by a DistillBERT-based classifier.
System prompt defense. We could modify the input prompt for jailbreaking; and several works
explore if we can apply similar methods to system prompts to defend against such attacks. Xie et al.
[113] propose “self-reminder”, i.e.,appending a reminding prompt within the system prompt for
defense. The attacks are collected from the JailbreakChat collection, and this strategy is effective for
defending against them. Zheng et al. [114] advocate for finding a good system prompt automatically,
by investigating the representational difference between safe and harmful queries, and optimizing
the safety prompts along the harmful refusal direction in the representation space. One intriguing
takeaway is harmful / harmless queries can be distinguished in the representation space, different from
the adversarial examples in vision. Zhou et al. [115] also optimize the safe system prompt, but in a
more “adversarial training” fashion, that apply jailbreak algorithms with current safe prompts first and
then find good replacement candidates in the same way as done by Zou et al. [1]. Concurrently, Mo
et al. [116] propose prompt adversarial tuning, where an adversarial suffix is assumed, while a safe
system prompt is jointly optimized with this suffix, with an additionally constructed benign loss to
improve helpfulness under normal queries. Zhang et al. [117] propose the idea of “goal prioritization”,
either without training (append prioritize safety than helpfulness and in-context examples to the
system prompt) or with training (generate data pairs of prioritizing safety or helpfulness, finetune,
and append prioritize safety prompt into system prompt). The former is effective for large-scale
LLMs, while the latter improves safety of LLaMa-chat models. Zhou et al. [118] propose in-context
adversarial game, where an attack LLM and a defense LLM interact on exchanging insights on
successful jailbreaks, and defend by improving the system prompt. Zou et al. [70] give the result
that system prompt matters for jailbreaking, and shows conducting GA-based search over it could
improve safety.
Checking-based, decoding-based, and Misc. Helbling et al. [119] generate responses first, and
then use the LLM itself to examine whether the output is harmful or not. They find such simple
self-examination is powerful since the TPR reported can be up to ∼1.00. Wang et al. [120] propose
to (1) tune the LM to enhance its capability on discriminating harmful / harmless content; (2) tune

self-examination is powerful since the TPR reported can be up to ∼1.00. Wang et al. [120] propose
to (1) tune the LM to enhance its capability on discriminating harmful / harmless content; (2) tune
the LM to make it able to tag its own response; and (3) rewrite response if output is harmful. Li
et al. [121] propose to suppress the attack performance by iteratively rewinding and re-examining the
generated output. The method does not work well with small models, but works pretty fine with large
(open-source) models. They find the strategy can improve generalization as well. Xu et al. [122] train
a safer model first, and use normalized pattacked +α(psafer−pattacked )over top- kshared tokens for
decoding to enhance safety. Hasan et al. [123] show with original Wanda pruning [ 124], the LLM
can help resist direct jailbreaking prompts, e.g., with role-playing attacks. Pi et al. [125] propose
MLLM-Protector on safeguarding Visual LLMs by checking the output and then detoxifying the
content. Zhang et al. [126] perform intention analysis on the input, and enforce the model generate
policy-aligned outputs both by prompting. Wang et al. [127] propose backtranslation that guesses
the input prompt directly, and reject if it is harmful. Kim et al. [128] propose self-refinement which
consists of generating a feedback and then refine the response to avoid harmful info output. They
find using additional JSON and code formatting would improve safety. Zeng et al. [129] propose
AutoDefense, which utilizes multiple agents on analyzing prompt, response and intention, to defend
31

against attacks. Hu et al. [130] propose Gradient Cuff, a sampling-based gradient-norm defense
method, by rejecting those input prompts with large gradient norm on top of a majority-vote based
filtering. Ji et al. [131] propose a method similar to Robey et al. [34], but for semantically-meaningful
attacks, that paraphrases the input according to several criteria and conduct a majority vote for
judging.
Several company-centered products also fall into this regime. For example, LLaMa-Guard [ 132] is
trained on toxic data such that it is able to discriminate unsafe user prompts and outputs, respectively.
IBM also propose a framework on constructing and deploying safeguard detection modules, and
releases the details in a technical report [133].
H.3 Theory and experimental understanding
Wolf et al. [38] assumes the decomposability of LLM output into a good and bad component, and
show possible jailbreaking in theory by prompting the model with a sufficiently long input. Kalai and
Vempala [134] use statistical tools to prove hallucination for calibrated LMs. Lee et al. [135] study
the representation in GPT-2. They train a base classifier for toxicity, and use the linear weight as a
proxy of toxic vector. They find there are value vectors close to the toxic vector itself, that are not
suppressed by DPO tuning [ 23]. Wei et al. [136] use pruning and low-rank analysis on safe LM, and
find (1) safe neurons and useful neurons are sparse; pruning the safe neurons or removing the safe
ranks away degrades safety a lot, and (2) fixing the safe neurons in fine-tuning does not maintain
safety.
32

