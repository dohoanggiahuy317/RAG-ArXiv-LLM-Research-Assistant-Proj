id,link,title,abstract,authors,published
http://arxiv.org/abs/2408.07702v1,http://arxiv.org/abs/2408.07702v1,"The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned
  Language Models","Schema linking is a crucial step in Text-to-SQL pipelines, which translate
natural language queries into SQL. The goal of schema linking is to retrieve
relevant tables and columns (signal) while disregarding irrelevant ones
(noise). However, imperfect schema linking can often exclude essential columns
needed for accurate query generation. In this work, we revisit the need for
schema linking when using the latest generation of large language models
(LLMs). We find empirically that newer models are adept at identifying relevant
schema elements during generation, without the need for explicit schema
linking. This allows Text-to-SQL pipelines to bypass schema linking entirely
and instead pass the full database schema to the LLM, eliminating the risk of
excluding necessary information. Furthermore, as alternatives to schema
linking, we propose techniques that improve Text-to-SQL accuracy without
compromising on essential schema information. Our approach achieves 71.83\%
execution accuracy on the BIRD benchmark, ranking first at the time of
submission.","[{'name': 'Karime Maamari'}, {'name': 'Fadhil Abubaker'}, {'name': 'Daniel Jaroslawicz'}, {'name': 'Amine Mhedhbi'}]",2024-08-14T17:59:04Z
http://arxiv.org/abs/2408.07697v1,http://arxiv.org/abs/2408.07697v1,Quantifying over Optimum Answer Sets,"Answer Set Programming with Quantifiers (ASP(Q)) has been introduced to
provide a natural extension of ASP modeling to problems in the polynomial
hierarchy (PH). However, ASP(Q) lacks a method for encoding in an elegant and
compact way problems requiring a polynomial number of calls to an oracle in
$\Sigma_n^p$ (that is, problems in $\Delta_{n+1}^p$). Such problems include, in
particular, optimization problems. In this paper we propose an extension of
ASP(Q), in which component programs may contain weak constraints. Weak
constraints can be used both for expressing local optimization within
quantified component programs and for modeling global optimization criteria. We
showcase the modeling capabilities of the new formalism through various
application scenarios. Further, we study its computational properties obtaining
complexity results and unveiling non-obvious characteristics of ASP(Q) programs
with weak constraints.","[{'name': 'Giuseppe Mazzotta'}, {'name': 'Francesco Ricca'}, {'name': 'Mirek Truszczynski'}]",2024-08-14T17:53:13Z
http://arxiv.org/abs/2408.07676v1,http://arxiv.org/abs/2408.07676v1,"Enhanced Detection of Conversational Mental Manipulation Through
  Advanced Prompting Techniques","This study presents a comprehensive, long-term project to explore the
effectiveness of various prompting techniques in detecting dialogical mental
manipulation. We implement Chain-of-Thought prompting with Zero-Shot and
Few-Shot settings on a binary mental manipulation detection task, building upon
existing work conducted with Zero-Shot and Few- Shot prompting. Our primary
objective is to decipher why certain prompting techniques display superior
performance, so as to craft a novel framework tailored for detection of mental
manipulation. Preliminary findings suggest that advanced prompting techniques
may not be suitable for more complex models, if they are not trained through
example-based learning.","[{'name': 'Ivory Yang'}, {'name': 'Xiaobo Guo'}, {'name': 'Sean Xie'}, {'name': 'Soroush Vosoughi'}]",2024-08-14T17:23:12Z
http://arxiv.org/abs/2408.07666v1,http://arxiv.org/abs/2408.07666v1,"Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,
  Applications and Opportunities","Model merging is an efficient empowerment technique in the machine learning
community that does not require the collection of raw training data and does
not require expensive computation. As model merging becomes increasingly
prevalent across various fields, it is crucial to understand the available
model merging techniques comprehensively. However, there is a significant gap
in the literature regarding a systematic and thorough review of these
techniques. This survey provides a comprehensive overview of model merging
methods and theories, their applications in various domains and settings, and
future research directions. Specifically, we first propose a new taxonomic
approach that exhaustively discusses existing model merging methods. Secondly,
we discuss the application of model merging techniques in large language
models, multimodal large language models, and 10+ machine learning subfields,
including continual learning, multi-task learning, few-shot learning, etc.
Finally, we highlight the remaining challenges of model merging and discuss
future research directions. A comprehensive list of papers about model merging
is available at
\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.","[{'name': 'Enneng Yang'}, {'name': 'Li Shen'}, {'name': 'Guibing Guo'}, {'name': 'Xingwei Wang'}, {'name': 'Xiaochun Cao'}, {'name': 'Jie Zhang'}, {'name': 'Dacheng Tao'}]",2024-08-14T16:58:48Z
http://arxiv.org/abs/2408.07665v1,http://arxiv.org/abs/2408.07665v1,"Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech
  Large Language Models","Warning: This paper may contain texts with uncomfortable content.
  Large Language Models (LLMs) have achieved remarkable performance in various
tasks, including those involving multimodal data like speech. However, these
models often exhibit biases due to the nature of their training data. Recently,
more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent
need to address these biases. This study introduces Spoken Stereoset, a dataset
specifically designed to evaluate social biases in SLLMs. By examining how
different models respond to speech from diverse demographic groups, we aim to
identify these biases. Our experiments reveal significant insights into their
performance and bias levels. The findings indicate that while most models show
minimal bias, some still exhibit slightly stereotypical or anti-stereotypical
tendencies.","[{'name': 'Yi-Cheng Lin'}, {'name': 'Wei-Chih Chen'}, {'name': 'Hung-yi Lee'}]",2024-08-14T16:55:06Z
http://arxiv.org/abs/2408.07663v1,http://arxiv.org/abs/2408.07663v1,"Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining
  of Probability Distributions","Large language models are susceptible to jailbreak attacks, which can result
in the generation of harmful content. While prior defenses mitigate these risks
by perturbing or inspecting inputs, they ignore competing objectives, the
underlying cause of alignment failures. In this paper, we propose
Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive
decoding to address the root causes of jailbreak issues. We first define the
Competitive Index to quantify alignment failures and utilize feedback from
self-evaluation to compute post-alignment logits. Then, AED adaptively combines
AED and post-alignment logits with the original logits to obtain harmless and
helpful distributions. Consequently, our method enhances safety alignment while
maintaining helpfulness. We conduct experiments across five models and four
common jailbreaks, with the results validating the effectiveness of our
approach. Code is available at https://github.com/GIGABaozi/AED.git.","[{'name': 'Quan Liu'}, {'name': 'Zhenhong Zhou'}, {'name': 'Longzhu He'}, {'name': 'Yi Liu'}, {'name': 'Wei Zhang'}, {'name': 'Sen Su'}]",2024-08-14T16:51:21Z
http://arxiv.org/abs/2408.07648v1,http://arxiv.org/abs/2408.07648v1,See It All: Contextualized Late Aggregation for 3D Dense Captioning,"3D dense captioning is a task to localize objects in a 3D scene and generate
descriptive sentences for each object. Recent approaches in 3D dense captioning
have adopted transformer encoder-decoder frameworks from object detection to
build an end-to-end pipeline without hand-crafted components. However, these
approaches struggle with contradicting objectives where a single query
attention has to simultaneously view both the tightly localized object regions
and contextual environment. To overcome this challenge, we introduce SIA
(See-It-All), a transformer pipeline that engages in 3D dense captioning with a
novel paradigm called late aggregation. SIA simultaneously decodes two sets of
queries-context query and instance query. The instance query focuses on
localization and object attribute descriptions, while the context query
versatilely captures the region-of-interest of relationships between multiple
objects or with the global scene, then aggregated afterwards (i.e., late
aggregation) via simple distance-based measures. To further enhance the quality
of contextualized caption generation, we design a novel aggregator to generate
a fully informed caption based on the surrounding context, the global
environment, and object instances. Extensive experiments on two of the most
widely-used 3D dense captioning datasets demonstrate that our proposed method
achieves a significant improvement over prior methods.","[{'name': 'Minjung Kim'}, {'name': 'Hyung Suk Lim'}, {'name': 'Seung Hwan Kim'}, {'name': 'Soonyoung Lee'}, {'name': 'Bumsoo Kim'}, {'name': 'Gunhee Kim'}]",2024-08-14T16:19:18Z
http://arxiv.org/abs/2408.07637v1,http://arxiv.org/abs/2408.07637v1,Hierarchical Working Memory and a New Magic Number,"The extremely limited working memory span, typically around four items,
contrasts sharply with our everyday experience of processing much larger
streams of sensory information concurrently. This disparity suggests that
working memory can organize information into compact representations such as
chunks, yet the underlying neural mechanisms remain largely unknown. Here, we
propose a recurrent neural network model for chunking within the framework of
the synaptic theory of working memory. We showed that by selectively
suppressing groups of stimuli, the network can maintain and retrieve the
stimuli in chunks, hence exceeding the basic capacity. Moreover, we show that
our model can dynamically construct hierarchical representations within working
memory through hierarchical chunking. A consequence of this proposed mechanism
is a new limit on the number of items that can be stored and subsequently
retrieved from working memory, depending only on the basic working memory
capacity when chunking is not invoked. Predictions from our model were
confirmed by analyzing single-unit responses in epileptic patients and memory
experiments with verbal material. Our work provides a novel conceptual and
analytical framework for understanding the on-the-fly organization of
information in the brain that is crucial for cognition.","[{'name': 'Weishun Zhong'}, {'name': 'Mikhail Katkov'}, {'name': 'Misha Tsodyks'}]",2024-08-14T16:03:47Z
http://arxiv.org/abs/2408.07611v1,http://arxiv.org/abs/2408.07611v1,"WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation
  Integrating Web Search and Knowledge Graphs","Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce ""phantom"" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a ""Retrieval-Augmented
Generation (RAG)"" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.","[{'name': 'Weijian Xie'}, {'name': 'Xuefeng Liang'}, {'name': 'Yuhui Liu'}, {'name': 'Kaihua Ni'}, {'name': 'Hong Cheng'}, {'name': 'Zetian Hu'}]",2024-08-14T15:19:16Z
http://arxiv.org/abs/2408.07599v1,http://arxiv.org/abs/2408.07599v1,"Assessing the Role of Lexical Semantics in Cross-lingual Transfer
  through Controlled Manipulations","While cross-linguistic model transfer is effective in many settings, there is
still limited understanding of the conditions under which it works. In this
paper, we focus on assessing the role of lexical semantics in cross-lingual
transfer, as we compare its impact to that of other language properties.
Examining each language property individually, we systematically analyze how
differences between English and a target language influence the capacity to
align the language with an English pretrained representation space. We do so by
artificially manipulating the English sentences in ways that mimic specific
characteristics of the target language, and reporting the effect of each
manipulation on the quality of alignment with the representation space. We show
that while properties such as the script or word order only have a limited
impact on alignment quality, the degree of lexical matching between the two
languages, which we define using a measure of translation entropy, greatly
affects it.","[{'name': 'Roy Ilani'}, {'name': 'Taelin Karidi'}, {'name': 'Omri Abend'}]",2024-08-14T14:59:20Z
http://arxiv.org/abs/2408.07583v1,http://arxiv.org/abs/2408.07583v1,"Transformers and Large Language Models for Efficient Intrusion Detection
  Systems: A Comprehensive Survey","With significant advancements in Transformers LLMs, NLP has extended its
reach into many research fields due to its enhanced capabilities in text
generation and user interaction. One field benefiting greatly from these
advancements is cybersecurity. In cybersecurity, many parameters that need to
be protected and exchanged between senders and receivers are in the form of
text and tabular data, making NLP a valuable tool in enhancing the security
measures of communication protocols. This survey paper provides a comprehensive
analysis of the utilization of Transformers and LLMs in cyber-threat detection
systems. The methodology of paper selection and bibliometric analysis is
outlined to establish a rigorous framework for evaluating existing research.
The fundamentals of Transformers are discussed, including background
information on various cyber-attacks and datasets commonly used in this field.
The survey explores the application of Transformers in IDSs, focusing on
different architectures such as Attention-based models, LLMs like BERT and GPT,
CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.
Furthermore, it explores the diverse environments and applications where
Transformers and LLMs-based IDS have been implemented, including computer
networks, IoT devices, critical infrastructure protection, cloud computing,
SDN, as well as in autonomous vehicles. The paper also addresses research
challenges and future directions in this area, identifying key issues such as
interpretability, scalability, and adaptability to evolving threats, and more.
Finally, the conclusion summarizes the findings and highlights the significance
of Transformers and LLMs in enhancing cyber-threat detection capabilities,
while also outlining potential avenues for further research and development.",[{'name': 'Hamza Kheddar'}],2024-08-14T14:28:11Z
http://arxiv.org/abs/2408.07543v1,http://arxiv.org/abs/2408.07543v1,"MathScape: Evaluating MLLMs in multimodal Math Scenarios through a
  Hierarchical Benchmark","With the development of Multimodal Large Language Models (MLLMs), the
evaluation of multimodal models in the context of mathematical problems has
become a valuable research field. Multimodal visual-textual mathematical
reasoning serves as a critical indicator for evaluating the comprehension and
complex multi-step quantitative reasoning abilities of MLLMs. However, previous
multimodal math benchmarks have not sufficiently integrated visual and textual
information. To address this gap, we proposed MathScape, a new benchmark that
emphasizes the understanding and application of combined visual and textual
information. MathScape is designed to evaluate photo-based math problem
scenarios, assessing the theoretical understanding and application ability of
MLLMs through a categorical hierarchical approach. We conduct a
multi-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark
is challenging even for the most sophisticated models. By analyzing the
evaluation results, we identify the limitations of MLLMs, offering valuable
insights for enhancing model performance.","[{'name': 'Minxuan Zhou'}, {'name': 'Hao Liang'}, {'name': 'Tianpeng Li'}, {'name': 'Zhiyu Wu'}, {'name': 'Mingan Lin'}, {'name': 'Linzhuang Sun'}, {'name': 'Yaqi Zhou'}, {'name': 'Yan Zhang'}, {'name': 'Xiaoqin Huang'}, {'name': 'Yicong Chen'}, {'name': 'Yujing Qiao'}, {'name': 'Weipeng Chen'}, {'name': 'Bin Cui'}, {'name': 'Wentao Zhang'}, {'name': 'Zenan Zhou'}]",2024-08-14T13:23:43Z
http://arxiv.org/abs/2408.07531v1,http://arxiv.org/abs/2408.07531v1,"Development of a Multi-Agent Clinical Decision Support System for Korean
  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in
  Emergency Departments","Emergency department (ED) overcrowding and the complexity of rapid
decision-making in critical care settings pose significant challenges to
healthcare systems worldwide. While clinical decision support systems (CDSS)
have shown promise, the integration of large language models (LLMs) offers new
possibilities for enhancing triage accuracy and clinical decision-making. This
study presents an LLM-driven CDSS designed to assist ED physicians and nurses
in patient triage, treatment planning, and overall emergency care management.
  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,
orchestrated by CrewAI and Langchain. The system comprises four AI agents
emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED
Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for
triage assessment and integrates with the RxNorm API for medication management.
  The model was evaluated using the Asclepius dataset, with performance
assessed by a clinical emergency medicine specialist. The CDSS demonstrated
high accuracy in triage decision-making compared to the baseline of a
single-agent system. Furthermore, the system exhibited strong performance in
critical areas, including primary diagnosis, critical findings identification,
disposition decision-making, treatment planning, and resource allocation.
  Our multi-agent CDSS demonstrates significant potential for supporting
comprehensive emergency care management. By leveraging state-of-the-art AI
technologies, this system offers a scalable and adaptable tool that could
enhance emergency medical care delivery, potentially alleviating ED
overcrowding and improving patient outcomes. This work contributes to the
growing field of AI applications in emergency medicine and offers a promising
direction for future research and clinical implementation.","[{'name': 'Seungjun Han'}, {'name': 'Wongyung Choi'}]",2024-08-14T13:03:41Z
http://arxiv.org/abs/2408.07505v1,http://arxiv.org/abs/2408.07505v1,Large Language Models Know What Makes Exemplary Contexts,"In-context learning (ICL) has proven to be a significant capability with the
advancement of Large Language models (LLMs). By instructing LLMs using few-shot
demonstrative examples, ICL enables them to perform a wide range of tasks
without needing to update millions of parameters. This paper presents a unified
framework for LLMs that allows them to self-select influential in-context
examples to compose their contexts; self-rank candidates with different
demonstration compositions; self-optimize the demonstration selection and
ordering through reinforcement learning. Specifically, our method designs a
parameter-efficient retrieval head that generates the optimized demonstration
after training with rewards from LLM's own preference. Experimental results
validate the proposed method's effectiveness in enhancing ICL performance.
Additionally, our approach effectively identifies and selects the most
representative examples for the current task, and includes more diversity in
retrieval.","[{'name': 'Quanyu Long'}, {'name': 'Jianda Chen'}]",2024-08-14T12:32:41Z
http://arxiv.org/abs/2408.07479v1,http://arxiv.org/abs/2408.07479v1,"A Study on Bias Detection and Classification in Natural Language
  Processing","Human biases have been shown to influence the performance of models and
algorithms in various fields, including Natural Language Processing. While the
study of this phenomenon is garnering focus in recent years, the available
resources are still relatively scarce, often focusing on different forms or
manifestations of biases. The aim of our work is twofold: 1) gather
publicly-available datasets and determine how to better combine them to
effectively train models in the task of hate speech detection and
classification; 2) analyse the main issues with these datasets, such as
scarcity, skewed resources, and reliance on non-persistent data. We discuss
these issues in tandem with the development of our experiments, in which we
show that the combinations of different datasets greatly impact the models'
performance.","[{'name': 'Ana Sofia Evans'}, {'name': 'Helena Moniz'}, {'name': 'Luísa Coheur'}]",2024-08-14T11:49:24Z
http://arxiv.org/abs/2408.07471v1,http://arxiv.org/abs/2408.07471v1,"Bridging and Modeling Correlations in Pairwise Data for Direct
  Preference Optimization","Direct preference optimization (DPO), a widely adopted offline preference
optimization algorithm, aims to align large language models (LLMs) with
human-desired behaviors using pairwise preference data. However, the winning
response and the losing response within pairwise data are generated isolatedly,
leading to weak correlations between them as well as suboptimal alignment
performance. To address this issue, we propose an effective framework named
BMC, for bridging and modeling correlations in pairwise data. Firstly, we
increase the consistency and informativeness of the pairwise preference signals
by targeted modifications, synthesizing a pseudo winning response through
improving the losing response based on the winning response. Secondly, we
identify that DPO alone is insufficient to model these correlations and capture
nuanced variations. Therefore, we propose learning token-level correlations by
dynamically leveraging the policy model's confidence during training.
Comprehensive experiments on QA, math, and instruction-following tasks
demonstrate the effectiveness of our approach, significantly surpassing
competitive baselines, including DPO. Additionally, our in-depth quantitative
analysis reveals the reasons behind our method's superior performance over DPO
and showcases its versatility to other DPO variants.","[{'name': 'Yuxin Jiang'}, {'name': 'Bo Huang'}, {'name': 'Yufei Wang'}, {'name': 'Xingshan Zeng'}, {'name': 'Liangyou Li'}, {'name': 'Yasheng Wang'}, {'name': 'Xin Jiang'}, {'name': 'Lifeng Shang'}, {'name': 'Ruiming Tang'}, {'name': 'Wei Wang'}]",2024-08-14T11:29:47Z
http://arxiv.org/abs/2408.07465v1,http://arxiv.org/abs/2408.07465v1,Large Language Models Prompting With Episodic Memory,"Prompt optimization is essential for enhancing the performance of Large
Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks,
particularly in scenarios of few-shot learning where training examples are
incorporated directly into the prompt. Despite the growing interest in
optimizing prompts with few-shot examples, existing methods for prompt
optimization are often resource-intensive or perform inadequately. In this
work, we propose PrOmpting with Episodic Memory (POEM), a novel prompt
optimization technique that is simple, efficient, and demonstrates strong
generalization capabilities. We approach prompt optimization as a Reinforcement
Learning (RL) challenge, using episodic memory to archive combinations of input
data, permutations of few-shot examples, and the rewards observed during
training. In the testing phase, we optimize the sequence of examples for each
test query by selecting the sequence that yields the highest total rewards from
the top-k most similar training examples in the episodic memory. Our results
show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over
5.3% in various text classification tasks. Furthermore, our approach adapts
well to broader language understanding tasks, consistently outperforming
conventional heuristic methods for ordering examples.","[{'name': 'Dai Do'}, {'name': 'Quan Tran'}, {'name': 'Svetha Venkatesh'}, {'name': 'Hung Le'}]",2024-08-14T11:19:28Z
http://arxiv.org/abs/2408.07457v1,http://arxiv.org/abs/2408.07457v1,From Brazilian Portuguese to European Portuguese,"Brazilian Portuguese and European Portuguese are two varieties of the same
language and, despite their close similarities, they exhibit several
differences. However, there is a significant disproportion in the availability
of resources between the two variants, with Brazilian Portuguese having more
abundant resources. This inequity can impact the quality of translation
services accessible to European Portuguese speakers. To address this issue, we
propose the development of a Brazilian Portuguese to European Portuguese
translation system, leveraging recent advancements in neural architectures and
models. To evaluate the performance of such systems, we manually curated a gold
test set comprising 500 sentences across five different topics. Each sentence
in the gold test set has two distinct references, facilitating a
straightforward evaluation of future translation models. We experimented with
various models by fine-tuning existing Large Language Models using parallel
data extracted from movie subtitles and TED Talks transcripts in both Brazilian
and European Portuguese. Our evaluation involved the use of conventional
automatic metrics as well as a human evaluation. In addition, all models were
compared against ChatGPT 3.5 Turbo, which currently yields the best results.","[{'name': 'João Sanches'}, {'name': 'Rui Ribeiro'}, {'name': 'Luísa Coheur'}]",2024-08-14T10:58:48Z
http://arxiv.org/abs/2408.07453v1,http://arxiv.org/abs/2408.07453v1,"Fact or Fiction? Improving Fact Verification with Knowledge Graphs
  through Simplified Subgraph Retrievals","Despite recent success in natural language processing (NLP), fact
verification still remains a difficult task. Due to misinformation spreading
increasingly fast, attention has been directed towards automatically verifying
the correctness of claims. In the domain of NLP, this is usually done by
training supervised machine learning models to verify claims by utilizing
evidence from trustworthy corpora. We present efficient methods for verifying
claims on a dataset where the evidence is in the form of structured knowledge
graphs. We use the FactKG dataset, which is constructed from the DBpedia
knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval
process, from fine-tuned language models to simple logical retrievals, we are
able to construct models that both require less computational resources and
achieve better test-set accuracy.",[{'name': 'Tobias A. Opsahl'}],2024-08-14T10:46:15Z
http://arxiv.org/abs/2408.07452v1,http://arxiv.org/abs/2408.07452v1,CMU's IWSLT 2024 Simultaneous Speech Translation System,"This paper describes CMU's submission to the IWSLT 2024 Simultaneous Speech
Translation (SST) task for translating English speech to German text in a
streaming manner. Our end-to-end speech-to-text (ST) system integrates the
WavLM speech encoder, a modality adapter, and the Llama2-7B-Base model as the
decoder. We employ a two-stage training approach: initially, we align the
representations of speech and text, followed by full fine-tuning. Both stages
are trained on MuST-c v2 data with cross-entropy loss. We adapt our offline ST
model for SST using a simple fixed hold-n policy. Experiments show that our
model obtains an offline BLEU score of 31.1 and a BLEU score of 29.5 under 2
seconds latency on the MuST-C-v2 tst-COMMON.","[{'name': 'Xi Xu'}, {'name': 'Siqi Ouyang'}, {'name': 'Brian Yan'}, {'name': 'Patrick Fernandes'}, {'name': 'William Chen'}, {'name': 'Lei Li'}, {'name': 'Graham Neubig'}, {'name': 'Shinji Watanabe'}]",2024-08-14T10:44:51Z
http://arxiv.org/abs/2408.07448v1,http://arxiv.org/abs/2408.07448v1,LiveFC: A System for Live Fact-Checking of Audio Streams,"The advances in the digital era have led to rapid dissemination of
information. This has also aggravated the spread of misinformation and
disinformation. This has potentially serious consequences, such as civil
unrest. While fact-checking aims to combat this, manual fact-checking is
cumbersome and not scalable. While automated fact-checking approaches exist,
they do not operate in real-time and do not always account for spread of
misinformation through different modalities. This is particularly important as
proactive fact-checking on live streams in real-time can help people be
informed of false narratives and prevent catastrophic consequences that may
cause civil unrest. This is particularly relevant with the rapid dissemination
of information through video on social media platforms or other streams like
political rallies and debates. Hence, in this work we develop a platform named
\name{}, that can aid in fact-checking live audio streams in real-time. \name{}
has a user-friendly interface that displays the claims detected along with
their veracity and evidence for live streams with associated speakers for
claims from respective segments. The app can be accessed at
http://livefc.factiverse.ai and a screen recording of the demo can be found at
https://bit.ly/3WVAoIw.","[{'name': 'Venktesh V'}, {'name': 'Vinay Setty'}]",2024-08-14T10:36:17Z
http://arxiv.org/abs/2408.07425v1,http://arxiv.org/abs/2408.07425v1,Exploring Retrieval Augmented Generation in Arabic,"Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful
technique in natural language processing, combining the strengths of
retrieval-based and generation-based models to enhance text generation tasks.
However, the application of RAG in Arabic, a language with unique
characteristics and resource constraints, remains underexplored. This paper
presents a comprehensive case study on the implementation and evaluation of RAG
for Arabic text. The work focuses on exploring various semantic embedding
models in the retrieval stage and several LLMs in the generation stage, in
order to investigate what works and what doesn't in the context of Arabic. The
work also touches upon the issue of variations between document dialect and
query dialect in the retrieval stage. Results show that existing semantic
embedding models and LLMs can be effectively employed to build Arabic RAG
pipelines.","[{'name': 'Samhaa R. El-Beltagy'}, {'name': 'Mohamed A. Abdallah'}]",2024-08-14T10:03:28Z
http://arxiv.org/abs/2408.07413v1,http://arxiv.org/abs/2408.07413v1,"Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge
  Editing for Large Language Models","Knowledge editing aims to update outdated or incorrect knowledge in large
language models (LLMs). However, current knowledge editing methods have limited
scalability for lifelong editing. This study explores the fundamental reason
why knowledge editing fails in lifelong editing. We begin with the closed-form
solution derived from linear associative memory, which underpins
state-of-the-art knowledge editing methods. We extend the solution from single
editing to lifelong editing, and through rigorous mathematical derivation,
identify an interference term in the final solution, suggesting that editing
knowledge may impact irrelevant knowledge. Further analysis of the interference
term reveals a close relationship with superposition between knowledge
representations. When knowledge superposition does not exist in language
models, the interference term vanishes, allowing for lossless knowledge
editing. Experiments across numerous language models reveal that knowledge
superposition is universal, exhibiting high kurtosis, zero mean, and
heavy-tailed distributions with clear scaling laws. Ultimately, by combining
theory and experiments, we demonstrate that knowledge superposition is the
fundamental reason for the failure of lifelong editing. Moreover, this is the
first study to investigate knowledge editing from the perspective of
superposition and provides a comprehensive observation of superposition across
numerous real-world language models. Code available at
https://github.com/ChenhuiHu/knowledge_in_superposition.","[{'name': 'Chenhui Hu'}, {'name': 'Pengfei Cao'}, {'name': 'Yubo Chen'}, {'name': 'Kang Liu'}, {'name': 'Jun Zhao'}]",2024-08-14T09:43:32Z
http://arxiv.org/abs/2408.07410v1,http://arxiv.org/abs/2408.07410v1,Aquila2 Technical Report,"This paper introduces the Aquila2 series, which comprises a wide range of
bilingual models with parameter sizes of 7, 34, and 70 billion. These models
are trained based on an innovative framework named HeuriMentor (HM), which
offers real-time insights into model convergence and enhances the training
process and data management. The HM System, comprising the Adaptive Training
Engine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),
allows for precise monitoring of the model's training progress and enables
efficient optimization of data distribution, thereby enhancing training
effectiveness. Extensive evaluations show that the Aquila2 model series
performs comparably well on both English and Chinese benchmarks. Specifically,
Aquila2-34B demonstrates only a slight decrease in performance when quantized
to Int4. Furthermore, we have made our training code
(https://github.com/FlagOpen/FlagScale) and model weights
(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing
research and the development of applications.","[{'name': 'Bo-Wen Zhang'}, {'name': 'Liangdong Wang'}, {'name': 'Jijie Li'}, {'name': 'Shuhao Gu'}, {'name': 'Xinya Wu'}, {'name': 'Zhengduo Zhang'}, {'name': 'Boyan Gao'}, {'name': 'Yulong Ao'}, {'name': 'Guang Liu'}]",2024-08-14T09:34:19Z
http://arxiv.org/abs/2408.07402v1,http://arxiv.org/abs/2408.07402v1,A Quantum-Inspired Analysis of Human Disambiguation Processes,"Formal languages are essential for computer programming and are constructed
to be easily processed by computers. In contrast, natural languages are much
more challenging and instigated the field of Natural Language Processing (NLP).
One major obstacle is the ubiquity of ambiguities. Recent advances in NLP have
led to the development of large language models, which can resolve ambiguities
with high accuracy. At the same time, quantum computers have gained much
attention in recent years as they can solve some computational problems faster
than classical computers. This new computing paradigm has reached the fields of
machine learning and NLP, where hybrid classical-quantum learning algorithms
have emerged. However, more research is needed to identify which NLP tasks
could benefit from a genuine quantum advantage. In this thesis, we applied
formalisms arising from foundational quantum mechanics, such as contextuality
and causality, to study ambiguities arising from linguistics. By doing so, we
also reproduced psycholinguistic results relating to the human disambiguation
process. These results were subsequently used to predict human behaviour and
outperformed current NLP methods.",[{'name': 'Daphne Wang'}],2024-08-14T09:21:23Z
http://arxiv.org/abs/2408.07401v1,http://arxiv.org/abs/2408.07401v1,"DataVisT5: A Pre-trained Language Model for Jointly Understanding Text
  and Data Visualization","Data visualization (DV) is the fundamental and premise tool to improve the
efficiency in conveying the insights behind the big data, which has been widely
accepted in existing data-driven world. Task automation in DV, such as
converting natural language queries to visualizations (i.e., text-to-vis),
generating explanations from visualizations (i.e., vis-to-text), answering
DV-related questions in free form (i.e. FeVisQA), and explicating tabular data
(i.e., table-to-text), is vital for advancing the field. Despite their
potential, the application of pre-trained language models (PLMs) like T5 and
BERT in DV has been limited by high costs and challenges in handling
cross-modal information, leading to few studies on PLMs for DV. We introduce
\textbf{DataVisT5}, a novel PLM tailored for DV that enhances the T5
architecture through a hybrid objective pre-training and multi-task fine-tuning
strategy, integrating text and DV datasets to effectively interpret cross-modal
semantics. Extensive evaluations on public datasets show that DataVisT5
consistently outperforms current state-of-the-art models on various DV-related
tasks. We anticipate that DataVisT5 will not only inspire further research on
vertical PLMs but also expand the range of applications for PLMs.","[{'name': 'Zhuoyue Wan'}, {'name': 'Yuanfeng Song'}, {'name': 'Shuaimin Li'}, {'name': 'Chen Jason Zhang'}, {'name': 'Raymond Chi-Wing Wong'}]",2024-08-14T09:20:17Z
http://arxiv.org/abs/2408.07377v1,http://arxiv.org/abs/2408.07377v1,"Do GPT Language Models Suffer From Split Personality Disorder? The
  Advent Of Substrate-Free Psychometrics","Previous research on emergence in large language models shows these display
apparent human-like abilities and psychological latent traits. However, results
are partly contradicting in expression and magnitude of these latent traits,
yet agree on the worrisome tendencies to score high on the Dark Triad of
narcissism, psychopathy, and Machiavellianism, which, together with a track
record of derailments, demands more rigorous research on safety of these
models. We provided a state of the art language model with the same personality
questionnaire in nine languages, and performed Bayesian analysis of Gaussian
Mixture Model, finding evidence for a deeper-rooted issue. Our results suggest
both interlingual and intralingual instabilities, which indicate that current
language models do not develop a consistent core personality. This can lead to
unsafe behaviour of artificial intelligence systems that are based on these
foundation models, and are increasingly integrated in human life. We
subsequently discuss the shortcomings of modern psychometrics, abstract it, and
provide a framework for its species-neutral, substrate-free formulation.","[{'name': 'Peter Romero'}, {'name': 'Stephen Fitz'}, {'name': 'Teruo Nakatsuma'}]",2024-08-14T08:53:00Z
http://arxiv.org/abs/2408.07353v1,http://arxiv.org/abs/2408.07353v1,"Only One Relation Possible? Modeling the Ambiguity in Event Temporal
  Relation Extraction","Event Temporal Relation Extraction (ETRE) aims to identify the temporal
relationship between two events, which plays an important role in natural
language understanding. Most previous works follow a single-label
classification style, classifying an event pair into either a specific temporal
relation (e.g., \textit{Before}, \textit{After}), or a special label
\textit{Vague} when there may be multiple possible temporal relations between
the pair. In our work, instead of directly making predictions on
\textit{Vague}, we propose a multi-label classification solution for ETRE
(METRE) to infer the possibility of each temporal relation independently, where
we treat \textit{Vague} as the cases when there is more than one possible
relation between two events. We design a speculation mechanism to explore the
possible relations hidden behind \textit{Vague}, which enables the latent
information to be used efficiently. Experiments on TB-Dense, MATRES and UDS-T
show that our method can effectively utilize the \textit{Vague} instances to
improve the recognition for specific temporal relations and outperforms most
state-of-the-art methods.","[{'name': 'Yutong Hu'}, {'name': 'Quzhe Huang'}, {'name': 'Yansong Feng'}]",2024-08-14T07:57:51Z
http://arxiv.org/abs/2408.07303v1,http://arxiv.org/abs/2408.07303v1,"Enhancing Visual Question Answering through Ranking-Based Hybrid
  Training and Multimodal Fusion","Visual Question Answering (VQA) is a challenging task that requires systems
to provide accurate answers to questions based on image content. Current VQA
models struggle with complex questions due to limitations in capturing and
integrating multimodal information effectively. To address these challenges, we
propose the Rank VQA model, which leverages a ranking-inspired hybrid training
strategy to enhance VQA performance. The Rank VQA model integrates high-quality
visual features extracted using the Faster R-CNN model and rich semantic text
features obtained from a pre-trained BERT model. These features are fused
through a sophisticated multimodal fusion technique employing multi-head
self-attention mechanisms. Additionally, a ranking learning module is
incorporated to optimize the relative ranking of answers, thus improving answer
accuracy. The hybrid training strategy combines classification and ranking
losses, enhancing the model's generalization ability and robustness across
diverse datasets. Experimental results demonstrate the effectiveness of the
Rank VQA model. Our model significantly outperforms existing state-of-the-art
models on standard VQA datasets, including VQA v2.0 and COCO-QA, in terms of
both accuracy and Mean Reciprocal Rank (MRR). The superior performance of Rank
VQA is evident in its ability to handle complex questions that require
understanding nuanced details and making sophisticated inferences from the
image and text. This work highlights the effectiveness of a ranking-based
hybrid training strategy in improving VQA performance and lays the groundwork
for further research in multimodal learning methods.","[{'name': 'Peiyuan Chen'}, {'name': 'Zecheng Zhang'}, {'name': 'Yiping Dong'}, {'name': 'Li Zhou'}, {'name': 'Han Wang'}]",2024-08-14T05:18:43Z
http://arxiv.org/abs/2408.07238v1,http://arxiv.org/abs/2408.07238v1,"Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge
  Distillation Approach","Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior
performance in complex human-like interactions. But they are costly, or too
large for edge devices such as smartphones and harder to self-host, leading to
security and privacy concerns. This paper introduces a novel interpretable
knowledge distillation approach to enhance the performance of smaller, more
economical LLMs that firms can self-host. We study this problem in the context
of building a customer service agent aimed at achieving high customer
satisfaction through goal-oriented dialogues. Unlike traditional knowledge
distillation, where the ""student"" model learns directly from the ""teacher""
model's responses via fine-tuning, our interpretable ""strategy"" teaching
approach involves the teacher providing strategies to improve the student's
performance in various scenarios. This method alternates between a ""scenario
generation"" step and a ""strategies for improvement"" step, creating a customized
library of scenarios and optimized strategies for automated prompting. The
method requires only black-box access to both student and teacher models; hence
it can be used without manipulating model parameters. In our customer service
application, the method improves performance, and the learned strategies are
transferable to other LLMs and scenarios beyond the training set. The method's
interpretabilty helps safeguard against potential harms through human audit.","[{'name': 'Tong Wang'}, {'name': 'K. Sudhir'}, {'name': 'Dat Hong'}]",2024-08-13T23:59:36Z
http://arxiv.org/abs/2408.07237v1,http://arxiv.org/abs/2408.07237v1,"Neural embedding of beliefs reveals the role of relative dissonance in
  human decision-making","Beliefs serve as the foundation for human cognition and decision-making. They
guide individuals in deriving meaning from their lives, shaping their
behaviors, and forming social connections. Therefore, a model that encapsulates
beliefs and their interrelationships is crucial for quantitatively studying the
influence of beliefs on our actions. Despite its importance, research on the
interplay between human beliefs has often been limited to a small set of
beliefs pertaining to specific issues, with a heavy reliance on surveys or
experiments. Here, we propose a method for extracting nuanced relations between
thousands of beliefs by leveraging large-scale user participation data from an
online debate platform and mapping these beliefs to an embedding space using a
fine-tuned large language model (LLM). This belief embedding space effectively
encapsulates the interconnectedness of diverse beliefs as well as polarization
across various social issues. We discover that the positions within this belief
space predict new beliefs of individuals. Furthermore, we find that the
relative distance between one's existing beliefs and new beliefs can serve as a
quantitative estimate of cognitive dissonance, allowing us to predict new
beliefs. Our study highlights how modern LLMs, when combined with collective
online records of human beliefs, can offer insights into the fundamental
principles that govern human belief formation and decision-making processes.","[{'name': 'Byunghwee Lee'}, {'name': 'Rachith Aiyappa'}, {'name': 'Yong-Yeol Ahn'}, {'name': 'Haewoon Kwak'}, {'name': 'Jisun An'}]",2024-08-13T23:58:45Z
http://arxiv.org/abs/2408.07190v1,http://arxiv.org/abs/2408.07190v1,BERT's Conceptual Cartography: Mapping the Landscapes of Meaning,"Conceptual Engineers want to make words better. However, they often
underestimate how varied our usage of words is. In this paper, we take the
first steps in exploring the contextual nuances of words by creating conceptual
landscapes -- 2D surfaces representing the pragmatic usage of words -- that
conceptual engineers can use to inform their projects. We use the spoken
component of the British National Corpus and BERT to create contextualised word
embeddings, and use Gaussian Mixture Models, a selection of metrics, and
qualitative analysis to visualise and numerically represent lexical landscapes.
Such an approach has not yet been used in the conceptual engineering literature
and provides a detailed examination of how different words manifest in various
contexts that is potentially useful to conceptual engineering projects. Our
findings highlight the inherent complexity of conceptual engineering, revealing
that each word exhibits a unique and intricate landscape. Conceptual Engineers
cannot, therefore, use a one-size-fits-all approach when improving words -- a
task that may be practically intractable at scale.","[{'name': 'Nina Haket'}, {'name': 'Ryan Daniels'}]",2024-08-13T20:08:26Z
http://arxiv.org/abs/2408.07180v1,http://arxiv.org/abs/2408.07180v1,Unlocking Efficiency: Adaptive Masking for Gene Transformer Models,"Gene transformer models such as Nucleotide Transformer, DNABert, and LOGO are
trained to learn optimal gene sequence representations by using the Masked
Language Modeling (MLM) training objective over the complete Human Reference
Genome. However, the typical tokenization methods employ a basic sliding window
of tokens, such as k-mers, that fail to utilize gene-centric semantics. This
could result in the (trivial) masking of easily predictable sequences, leading
to inefficient MLM training. Time-variant training strategies are known to
improve pretraining efficiency in both language and vision tasks. In this work,
we focus on using curriculum masking where we systematically increase the
difficulty of masked token prediction task by using a Pointwise Mutual
Information-based difficulty criterion, as gene sequences lack well-defined
semantic units similar to words or sentences of NLP domain. Our proposed
Curriculum Masking-based Gene Masking Strategy (CM-GEMS) demonstrates superior
representation learning capabilities compared to baseline masking approaches
when evaluated on downstream gene sequence classification tasks. We perform
extensive evaluation in both few-shot (five datasets) and full dataset settings
(Genomic Understanding Evaluation benchmark consisting of 27 tasks). Our
findings reveal that CM-GEMS outperforms state-of-the-art models (DNABert-2,
Nucleotide transformer, DNABert) trained at 120K steps, achieving similar
results in just 10K and 1K steps. We also demonstrate that Curriculum-Learned
LOGO (a 2-layer DNABert-like model) can achieve nearly 90% of the
state-of-the-art model performance of 120K steps. We will make the models and
codes publicly available at https://github.com/roysoumya/curriculum-GeneMask.","[{'name': 'Soumyadeep Roy'}, {'name': 'Shamik Sural'}, {'name': 'Niloy Ganguly'}]",2024-08-13T19:45:02Z
http://arxiv.org/abs/2408.07154v1,http://arxiv.org/abs/2408.07154v1,Self-folding Self-replication,"Inspired by protein folding, we explored the construction of
three-dimensional structures and machines from one-dimensional chains of simple
building blocks. This approach not only allows us to recreate the
self-replication mechanism introduced earlier, but also significantly
simplifies the process. We introduced a new set of folding blocks that
facilitate the formation of secondary structures such as {\alpha}-helices and
\b{eta}-sheets, as well as more advanced tertiary and quaternary structures,
including self-replicating machines. The introduction of rotational degrees of
freedom leads to a reduced variety of blocks and, most importantly, reduces the
overall size of the machines by a factor of five. In addition, we present a
universal copier-constructor, a highly efficient self-replicating mechanism
composed of approximately 40 blocks, including the restictions posed on it. The
paper also addresses evolutionary considerations, outlining several steps on
the evolutionary ladder towards more sophisticated self-replicating systems.
Finally, this study offers a clear rationale for nature's preference for
one-dimensional chains in constructing three-dimensional structures.",[{'name': 'Ralph P. Lano'}],2024-08-13T18:50:07Z
http://arxiv.org/abs/2408.07144v1,http://arxiv.org/abs/2408.07144v1,Language Models as Models of Language,"This chapter critically examines the potential contributions of modern
language models to theoretical linguistics. Despite their focus on engineering
goals, these models' ability to acquire sophisticated linguistic knowledge from
mere exposure to data warrants a careful reassessment of their relevance to
linguistic theory. I review a growing body of empirical evidence suggesting
that language models can learn hierarchical syntactic structure and exhibit
sensitivity to various linguistic phenomena, even when trained on
developmentally plausible amounts of data. While the competence/performance
distinction has been invoked to dismiss the relevance of such models to
linguistic theory, I argue that this assessment may be premature. By carefully
controlling learning conditions and making use of causal intervention methods,
experiments with language models can potentially constrain hypotheses about
language acquisition and competence. I conclude that closer collaboration
between theoretical linguists and computational researchers could yield
valuable insights, particularly in advancing debates about linguistic nativism.",[{'name': 'Raphaël Millière'}],2024-08-13T18:26:04Z
http://arxiv.org/abs/2408.07137v1,http://arxiv.org/abs/2408.07137v1,"ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal
  Advice","Despite remarkable performance in legal consultation exhibited by legal Large
Language Models(LLMs) combined with legal article retrieval components, there
are still cases when the advice given is incorrect or baseless. To alleviate
these problems, we propose {\bf ELLA}, a tool for {\bf E}mpowering {\bf L}LMs
for interpretable, accurate, and informative {\bf L}egal {\bf A}dvice. ELLA
visually presents the correlation between legal articles and LLM's response by
calculating their similarities, providing users with an intuitive legal basis
for the responses. Besides, based on the users' queries, ELLA retrieves
relevant legal articles and displays them to users. Users can interactively
select legal articles for LLM to generate more accurate responses. ELLA also
retrieves relevant legal cases for user reference. Our user study shows that
presenting the legal basis for the response helps users understand better. The
accuracy of LLM's responses also improves when users intervene in selecting
legal articles for LLM. Providing relevant legal cases also aids individuals in
obtaining comprehensive information.","[{'name': 'Yutong Hu'}, {'name': 'Kangcheng Luo'}, {'name': 'Yansong Feng'}]",2024-08-13T18:12:00Z
http://arxiv.org/abs/2408.07065v1,http://arxiv.org/abs/2408.07065v1,Fingerspelling within Sign Language Translation,"Fingerspelling poses challenges for sign language processing due to its
high-frequency motion and use for open-vocabulary terms. While prior work has
studied fingerspelling recognition, there has been little attention to
evaluating how well sign language translation models understand fingerspelling
in the context of entire sentences -- and improving this capability. We
manually annotate instances of fingerspelling within FLEURS-ASL and use them to
evaluate the effect of two simple measures to improve fingerspelling
recognition within American Sign Language to English translation: 1) use a
model family (ByT5) with character- rather than subword-level tokenization, and
2) mix fingerspelling recognition data into the translation training mixture.
We find that 1) substantially improves understanding of fingerspelling (and
therefore translation quality overall), but the effect of 2) is mixed.",[{'name': 'Garrett Tanzer'}],2024-08-13T17:57:14Z
http://arxiv.org/abs/2408.07060v1,http://arxiv.org/abs/2408.07060v1,"Diversity Empowers Intelligence: Integrating Expertise of Software
  Engineering Agents","Large language model (LLM) agents have shown great potential in solving
real-world software engineering (SWE) problems. The most advanced open-source
SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.
However, these sophisticated agent frameworks exhibit varying strengths,
excelling in certain tasks while underperforming in others. To fully harness
the diversity of these agents, we propose DEI (Diversity Empowered
Intelligence), a framework that leverages their unique expertise. DEI functions
as a meta-module atop existing SWE agent frameworks, managing agent collectives
for enhanced problem-solving. Experimental results show that a DEI-guided
committee of agents is able to surpass the best individual agent's performance
by a large margin. For instance, a group of open-source SWE agents, with a
maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%
resolve rate with DEI, making a 25% improvement and beating most closed-source
solutions. Our best-performing group excels with a 55% resolve rate, securing
the highest ranking on SWE-Bench Lite. Our findings contribute to the growing
body of research on collaborative AI systems and their potential to solve
complex software engineering challenges.","[{'name': 'Kexun Zhang'}, {'name': 'Weiran Yao'}, {'name': 'Zuxin Liu'}, {'name': 'Yihao Feng'}, {'name': 'Zhiwei Liu'}, {'name': 'Rithesh Murthy'}, {'name': 'Tian Lan'}, {'name': 'Lei Li'}, {'name': 'Renze Lou'}, {'name': 'Jiacheng Xu'}, {'name': 'Bo Pang'}, {'name': 'Yingbo Zhou'}, {'name': 'Shelby Heinecke'}, {'name': 'Silvio Savarese'}, {'name': 'Huan Wang'}, {'name': 'Caiming Xiong'}]",2024-08-13T17:50:28Z
http://arxiv.org/abs/2408.07057v1,http://arxiv.org/abs/2408.07057v1,"A Survey on Model MoErging: Recycling and Routing Among Specialized
  Experts for Collaborative Learning","The availability of performant pre-trained models has led to a proliferation
of fine-tuned expert models that are specialized to a particular domain or
task. Model MoErging methods aim to recycle expert models to create an
aggregate system with improved performance or generalization. A key component
of MoErging methods is the creation of a router that decides which expert
model(s) to use for a particular input or application. The promise,
effectiveness, and large design space of MoErging has spurred the development
of many new methods over the past few years. This rapid pace of development has
made it challenging to compare different MoErging methods, which are rarely
compared to one another and are often validated in different experimental
setups. To remedy such gaps, we present a comprehensive survey of MoErging
methods that includes a novel taxonomy for cataloging key design choices and
clarifying suitable applications for each method. Apart from surveying MoErging
research, we inventory software tools and applications that make use of
MoErging. We additionally discuss related fields of study such as model
merging, multitask learning, and mixture-of-experts models. Taken as a whole,
our survey provides a unified overview of existing MoErging methods and creates
a solid foundation for future work in this burgeoning field.","[{'name': 'Prateek Yadav'}, {'name': 'Colin Raffel'}, {'name': 'Mohammed Muqeeth'}, {'name': 'Lucas Caccia'}, {'name': 'Haokun Liu'}, {'name': 'Tianlong Chen'}, {'name': 'Mohit Bansal'}, {'name': 'Leshem Choshen'}, {'name': 'Alessandro Sordoni'}]",2024-08-13T17:49:00Z
http://arxiv.org/abs/2408.07055v1,http://arxiv.org/abs/2408.07055v1,"LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs","Current long context large language models (LLMs) can process inputs up to
100,000 tokens, yet struggle to generate outputs exceeding even a modest length
of 2,000 words. Through controlled experiments, we find that the model's
effective generation length is inherently bounded by the sample it has seen
during supervised fine-tuning (SFT). In other words, their output limitation is
due to the scarcity of long-output examples in existing SFT datasets. To
address this, we introduce AgentWrite, an agent-based pipeline that decomposes
ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to
generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we
construct LongWriter-6k, a dataset containing 6,000 SFT data with output
lengths ranging from 2k to 32k words. By incorporating this dataset into model
training, we successfully scale the output length of existing models to over
10,000 words while maintaining output quality. We also develop LongBench-Write,
a comprehensive benchmark for evaluating ultra-long generation capabilities.
Our 9B parameter model, further improved through DPO, achieves state-of-the-art
performance on this benchmark, surpassing even much larger proprietary models.
In general, our work demonstrates that existing long context LLM already
possesses the potential for a larger output window--all you need is data with
extended output during model alignment to unlock this capability. Our code &
models are at: https://github.com/THUDM/LongWriter.","[{'name': 'Yushi Bai'}, {'name': 'Jiajie Zhang'}, {'name': 'Xin Lv'}, {'name': 'Linzhi Zheng'}, {'name': 'Siqi Zhu'}, {'name': 'Lei Hou'}, {'name': 'Yuxiao Dong'}, {'name': 'Jie Tang'}, {'name': 'Juanzi Li'}]",2024-08-13T17:46:12Z
http://arxiv.org/abs/2408.07052v1,http://arxiv.org/abs/2408.07052v1,The News Comment Gap and Algorithmic Agenda Setting in Online Forums,"The disparity between news stories valued by journalists and those preferred
by readers, known as the ""News Gap"", is well-documented. However, the
difference in expectations regarding news related user-generated content is
less studied. Comment sections, hosted by news websites, are popular venues for
reader engagement, yet still subject to editorial decisions. It is thus
important to understand journalist vs reader comment preferences and how these
are served by various comment ranking algorithms that represent discussions
differently. We analyse 1.2 million comments from Austrian newspaper Der
Standard to understand the ""News Comment Gap"" and the effects of different
ranking algorithms. We find that journalists prefer positive, timely, complex,
direct responses, while readers favour comments similar to article content from
elite authors. We introduce the versatile Feature-Oriented Ranking Utility
Metric (FORUM) to assess the impact of different ranking algorithms and find
dramatic differences in how they prioritise the display of comments by
sentiment, topical relevance, lexical diversity, and readability. Journalists
can exert substantial influence over the discourse through both curatorial and
algorithmic means. Understanding these choices' implications is vital in
fostering engaging and civil discussions while aligning with journalistic
objectives, especially given the increasing legal scrutiny and societal
importance of online discourse.","[{'name': 'Flora Böwing'}, {'name': 'Patrick Gildersleve'}]",2024-08-13T17:43:32Z
http://arxiv.org/abs/2408.07045v1,http://arxiv.org/abs/2408.07045v1,TableGuard -- Securing Structured & Unstructured Data,"With the increasing demand for data sharing across platforms and
organizations, ensuring the privacy and security of sensitive information has
become a critical challenge. This paper introduces ""TableGuard"". An innovative
approach to data obfuscation tailored for relational databases. Building on the
principles and techniques developed in prior work on context-sensitive
obfuscation, TableGuard applies these methods to ensure that API calls return
only obfuscated data, thereby safeguarding privacy when sharing data with third
parties. TableGuard leverages advanced context-sensitive obfuscation techniques
to replace sensitive data elements with contextually appropriate alternatives.
By maintaining the relational integrity and coherence of the data, our approach
mitigates the risks of cognitive dissonance and data leakage. We demonstrate
the implementation of TableGuard using a BERT based transformer model, which
identifies and obfuscates sensitive entities within relational tables. Our
evaluation shows that TableGuard effectively balances privacy protection with
data utility, minimizing information loss while ensuring that the obfuscated
data remains functionally useful for downstream applications. The results
highlight the importance of domain-specific obfuscation strategies and the role
of context length in preserving data integrity. The implications of this
research are significant for organizations that need to share data securely
with external parties. TableGuard offers a robust framework for implementing
privacy-preserving data sharing mechanisms, thereby contributing to the broader
field of data privacy and security.","[{'name': 'Anantha Sharma'}, {'name': 'Ajinkya Deshmukh'}]",2024-08-13T17:20:52Z
http://arxiv.org/abs/2408.07003v1,http://arxiv.org/abs/2408.07003v1,Generative AI for automatic topic labelling,"Topic Modeling has become a prominent tool for the study of scientific
fields, as they allow for a large scale interpretation of research trends.
Nevertheless, the output of these models is structured as a list of keywords
which requires a manual interpretation for the labelling. This paper proposes
to assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini
for topic labelling. Drawing on previous research leveraging BERTopic, we
generate topics from a dataset of all the scientific articles (n=34,797)
authored by all biology professors in Switzerland (n=465) between 2008 and
2020, as recorded in the Web of Science database. We assess the output of the
three models both quantitatively and qualitatively and find that, first, both
GPT models are capable of accurately and precisely label topics from the
models' output keywords. Second, 3-word labels are preferable to grasp the
complexity of research topics.","[{'name': 'Diego Kozlowski'}, {'name': 'Carolina Pradier'}, {'name': 'Pierre Benz'}]",2024-08-13T16:07:16Z
http://arxiv.org/abs/2408.06931v1,http://arxiv.org/abs/2408.06931v1,"The advantages of context specific language models: the case of the
  Erasmian Language Model","The current trend to improve language model performance seems to be based on
scaling up with the number of parameters (e.g. the state of the art GPT4 model
has approximately 1.7 trillion parameters) or the amount of training data fed
into the model. However this comes at significant costs in terms of
computational resources and energy costs that compromise the sustainability of
AI solutions, as well as risk relating to privacy and misuse. In this paper we
present the Erasmian Language Model (ELM) a small context specific, 900 million
parameter model, pre-trained and fine-tuned by and for Erasmus University
Rotterdam. We show how the model performs adequately in a classroom context for
essay writing, and how it achieves superior performance in subjects that are
part of its context. This has implications for a wide range of institutions and
organizations, showing that context specific language models may be a viable
alternative for resource constrained, privacy sensitive use cases.","[{'name': 'João Gonçalves'}, {'name': 'Nick Jelicic'}, {'name': 'Michele Murgia'}, {'name': 'Evert Stamhuis'}]",2024-08-13T14:34:59Z
http://arxiv.org/abs/2408.06930v1,http://arxiv.org/abs/2408.06930v1,"Diagnosis extraction from unstructured Dutch echocardiogram reports
  using span- and document-level characteristic classification","Clinical machine learning research and AI driven clinical decision support
models rely on clinically accurate labels. Manually extracting these labels
with the help of clinical specialists is often time-consuming and expensive.
This study tests the feasibility of automatic span- and document-level
diagnosis extraction from unstructured Dutch echocardiogram reports.
  We included 115,692 unstructured echocardiogram reports from the UMCU a large
university hospital in the Netherlands. A randomly selected subset was manually
annotated for the occurrence and severity of eleven commonly described cardiac
characteristics. We developed and tested several automatic labelling techniques
at both span and document levels, using weighted and macro F1-score, precision,
and recall for performance evaluation. We compared the performance of span
labelling against document labelling methods, which included both direct
document classifiers and indirect document classifiers that rely on span
classification results.
  The SpanCategorizer and MedRoBERTa.nl models outperformed all other span and
document classifiers, respectively. The weighted F1-score varied between
characteristics, ranging from 0.60 to 0.93 in SpanCategorizer and 0.96 to 0.98
in MedRoBERTa.nl. Direct document classification was superior to indirect
document classification using span classifiers. SetFit achieved competitive
document classification performance using only 10\% of the training data.
Utilizing a reduced label set yielded near-perfect document classification
results.
  We recommend using our published SpanCategorizer and MedRoBERTa.nl models for
span- and document-level diagnosis extraction from Dutch echocardiography
reports. For settings with limited training data, SetFit may be a promising
alternative for document classification.","[{'name': 'Bauke Arends'}, {'name': 'Melle Vessies'}, {'name': 'Dirk van Osch'}, {'name': 'Arco Teske'}, {'name': 'Pim van der Harst'}, {'name': 'René van Es'}, {'name': 'Bram van Es'}]",2024-08-13T14:33:32Z
http://arxiv.org/abs/2408.06929v1,http://arxiv.org/abs/2408.06929v1,"Evaluating Cultural Adaptability of a Large Language Model via
  Simulation of Synthetic Personas","The success of Large Language Models (LLMs) in multicultural environments
hinges on their ability to understand users' diverse cultural backgrounds. We
measure this capability by having an LLM simulate human profiles representing
various nationalities within the scope of a questionnaire-style psychological
experiment. Specifically, we employ GPT-3.5 to reproduce reactions to
persuasive news articles of 7,286 participants from 15 countries; comparing the
results with a dataset of real participants sharing the same demographic
traits. Our analysis shows that specifying a person's country of residence
improves GPT-3.5's alignment with their responses. In contrast, using native
language prompting introduces shifts that significantly reduce overall
alignment, with some languages particularly impairing performance. These
findings suggest that while direct nationality information enhances the model's
cultural adaptability, native language cues do not reliably improve simulation
fidelity and can detract from the model's effectiveness.","[{'name': 'Louis Kwok'}, {'name': 'Michal Bravansky'}, {'name': 'Lewis D. Griffin'}]",2024-08-13T14:32:43Z
http://arxiv.org/abs/2408.06904v1,http://arxiv.org/abs/2408.06904v1,"Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge
  Perspectives","As large language models (LLMs) continue to scale, their enhanced performance
often proves insufficient for solving domain-specific tasks. Systematically
analyzing their failures and effectively enhancing their performance remain
significant challenges. This paper introduces the Re-TASK framework, a novel
theoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge
perspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space
Theory. The Re-TASK framework provides a systematic methodology to deepen our
understanding, evaluation, and enhancement of LLMs for domain-specific tasks.
It explores the interplay among an LLM's capabilities, the knowledge it
processes, and the skills it applies, elucidating how these elements are
interconnected and impact task performance. Our application of the Re-TASK
framework reveals that many failures in domain-specific tasks can be attributed
to insufficient knowledge or inadequate skill adaptation. With this insight, we
propose structured strategies for enhancing LLMs through targeted knowledge
injection and skill adaptation. Specifically, we identify key capability items
associated with tasks and employ a deliberately designed prompting strategy to
enhance task performance, thereby reducing the need for extensive fine-tuning.
Alternatively, we fine-tune the LLM using capability-specific instructions,
further validating the efficacy of our framework. Experimental results confirm
the framework's effectiveness, demonstrating substantial improvements in both
the performance and applicability of LLMs.","[{'name': 'Zhihu Wang'}, {'name': 'Shiwan Zhao'}, {'name': 'Yu Wang'}, {'name': 'Heyuan Huang'}, {'name': 'Jiaxin Shi'}, {'name': 'Sitao Xie'}, {'name': 'Zhixing Wang'}, {'name': 'Yubo Zhang'}, {'name': 'Hongyan Li'}, {'name': 'Junchi Yan'}]",2024-08-13T13:58:23Z
http://arxiv.org/abs/2408.06874v1,http://arxiv.org/abs/2408.06874v1,"Leveraging Language Models for Emotion and Behavior Analysis in
  Education","The analysis of students' emotions and behaviors is crucial for enhancing
learning outcomes and personalizing educational experiences. Traditional
methods often rely on intrusive visual and physiological data collection,
posing privacy concerns and scalability issues. This paper proposes a novel
method leveraging large language models (LLMs) and prompt engineering to
analyze textual data from students. Our approach utilizes tailored prompts to
guide LLMs in detecting emotional and engagement states, providing a
non-intrusive and scalable solution. We conducted experiments using Qwen,
ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and
chain-of-thought (CoT) prompting. Results demonstrate that our method
significantly outperforms the baselines in both accuracy and contextual
understanding. This study highlights the potential of LLMs combined with prompt
engineering to offer practical and effective tools for educational emotion and
behavior analysis.","[{'name': 'Kaito Tanaka'}, {'name': 'Benjamin Tan'}, {'name': 'Brian Wong'}]",2024-08-13T13:11:53Z
http://arxiv.org/abs/2408.06854v1,http://arxiv.org/abs/2408.06854v1,"LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large
  Language Models","Fine-tuning large language models (LLMs) with high parameter efficiency for
downstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)
significantly reduces the number of trainable parameters for fine-tuning.
Although it has demonstrated commendable performance, updating parameters
within a single scale may not be the optimal choice for complex downstream
tasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.
We first combine orthogonal projection theory to train a set of LoRAs in two
mutually orthogonal planes. Then, we improve the importance score algorithm,
which reduce parameter sensitivity score calculations by approximately 98.5\%.
By pruning singular values with lower importance scores, thereby enhancing
adaptability to various downstream tasks. Extensive experiments are conducted
on two widely used pre-trained models to validate the effectiveness of
LoRA$^2$. Results show that it significantly reduces the number of trainable
parameters to just 0.72\% compared to full fine-tuning, while still delivering
highly impressive performance. Even when the parameters are further reduced to
0.17M, it still achieves comparable results to the baseline with 8 times more
parameters. Our code is available here:
https://anonymous.4open.science/r/LoRA-2-5B4C","[{'name': 'Jia-Chen Zhang'}, {'name': 'Yu-Jie Xiong'}, {'name': 'He-Xi Qiu'}, {'name': 'Dong-Hai Zhu'}, {'name': 'Chun-Ming Xia'}]",2024-08-13T12:31:30Z
http://arxiv.org/abs/2408.06849v1,http://arxiv.org/abs/2408.06849v1,Causal Agent based on Large Language Model,"Large language models (LLMs) have achieved significant success across various
domains. However, the inherent complexity of causal problems and causal theory
poses challenges in accurately describing them in natural language, making it
difficult for LLMs to comprehend and use them effectively. Causal methods are
not easily conveyed through natural language, which hinders LLMs' ability to
apply them accurately. Additionally, causal datasets are typically tabular,
while LLMs excel in handling natural language data, creating a structural
mismatch that impedes effective reasoning with tabular data. This lack of
causal reasoning capability limits the development of LLMs. To address these
challenges, we have equipped the LLM with causal tools within an agent
framework, named the Causal Agent, enabling it to tackle causal problems. The
causal agent comprises tools, memory, and reasoning modules. In the tools
module, the causal agent applies causal methods to align tabular data with
natural language. In the reasoning module, the causal agent employs the ReAct
framework to perform reasoning through multiple iterations with the tools. In
the memory module, the causal agent maintains a dictionary instance where the
keys are unique names and the values are causal graphs. To verify the causal
ability of the causal agent, we established a benchmark consisting of four
levels of causal problems: variable level, edge level, causal graph level, and
causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for
these four levels of issues and tested the causal agent on the datasets. Our
methodology demonstrates remarkable efficacy on the four-level causal problems,
with accuracy rates all above 80%. For further insights and implementation
details, our code is accessible via the GitHub repository
https://github.com/Kairong-Han/Causal_Agent.","[{'name': 'Kairong Han'}, {'name': 'Kun Kuang'}, {'name': 'Ziyu Zhao'}, {'name': 'Junjian Ye'}, {'name': 'Fei Wu'}]",2024-08-13T12:22:26Z
http://arxiv.org/abs/2408.06816v1,http://arxiv.org/abs/2408.06816v1,"MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data
  Uncertainty","Although large language models (LLMs) are capable of performing various
tasks, they still suffer from producing plausible but incorrect responses. To
improve the reliability of LLMs, recent research has focused on uncertainty
quantification to predict whether a response is correct or not. However, most
uncertainty quantification methods have been evaluated on questions requiring a
single clear answer, ignoring the existence of data uncertainty that arises
from irreducible randomness. Instead, these methods only consider model
uncertainty, which arises from a lack of knowledge. In this paper, we
investigate previous uncertainty quantification methods under the presence of
data uncertainty. Our contributions are two-fold: 1) proposing a new
Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge,
mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty
quantification regarding data uncertainty, and 2) assessing 5 uncertainty
quantification methods of diverse white- and black-box LLMs. Our findings show
that entropy and consistency-based methods estimate the model uncertainty well
even under data uncertainty, while other methods for white- and black-box LLMs
struggle depending on the tasks. Additionally, methods designed for white-box
LLMs suffer from overconfidence in reasoning tasks compared to simple knowledge
queries. We believe our observations will pave the way for future work on
uncertainty quantification in realistic setting.","[{'name': 'Yongjin Yang'}, {'name': 'Haneul Yoo'}, {'name': 'Hwaran Lee'}]",2024-08-13T11:17:31Z
http://arxiv.org/abs/2408.06793v1,http://arxiv.org/abs/2408.06793v1,Layerwise Recurrent Router for Mixture-of-Experts,"The scaling of large language models (LLMs) has revolutionized their
capabilities in various tasks, yet this growth must be matched with efficient
computational strategies. The Mixture-of-Experts (MoE) architecture stands out
for its ability to scale model size without significantly increasing training
costs. Despite their advantages, current MoE models often display parameter
inefficiency. For instance, a pre-trained MoE-based LLM with 52 billion
parameters might perform comparably to a standard model with 6.7 billion
parameters. Being a crucial part of MoE, current routers in different layers
independently assign tokens without leveraging historical routing information,
potentially leading to suboptimal token-expert combinations and the parameter
inefficiency problem. To alleviate this issue, we introduce the Layerwise
Recurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated
Recurrent Unit (GRU) to establish dependencies between routing decisions across
consecutive layers. Such layerwise recurrence can be efficiently parallelly
computed for input tokens and introduces negotiable costs. Our extensive
empirical evaluations demonstrate that RMoE-based language models consistently
outperform a spectrum of baseline models. Furthermore, RMoE integrates a novel
computation stage orthogonal to existing methods, allowing seamless
compatibility with other MoE architectures. Our analyses attribute RMoE's gains
to its effective cross-layer information sharing, which also improves expert
selection and diversity. Our code is at https://github.com/qiuzh20/RMoE","[{'name': 'Zihan Qiu'}, {'name': 'Zeyu Huang'}, {'name': 'Shuang Cheng'}, {'name': 'Yizhi Zhou'}, {'name': 'Zili Wang'}, {'name': 'Ivan Titov'}, {'name': 'Jie Fu'}]",2024-08-13T10:25:13Z
http://arxiv.org/abs/2408.06787v1,http://arxiv.org/abs/2408.06787v1,Unlock the Power of Frozen LLMs in Knowledge Graph Completion,"Classical knowledge graph completion (KGC) methods rely solely on structural
information, struggling with the inherent sparsity of knowledge graphs (KGs).
Large Language Models (LLMs) learn extensive knowledge from large corpora with
powerful context modeling, which is ideal for mitigating the limitations of
previous methods. Directly fine-tuning LLMs offers great capability but comes
at the cost of huge time and memory consumption, while utilizing frozen LLMs
yields suboptimal results. In this work, we aim to leverage LLMs for KGC
effectively and efficiently. We capture the context-aware hidden states of
knowledge triples by employing prompts to stimulate the intermediate layers of
LLMs. We then train a data-efficient classifier on these hidden states to
harness the inherent capabilities of frozen LLMs in KGC. We also generate
entity descriptions with subgraph sampling on KGs, reducing the ambiguity of
triplets and enriching the knowledge representation. Extensive experiments on
standard benchmarks showcase the efficiency and effectiveness of our approach.
We outperform classical KGC methods on most datasets and match the performance
of fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU
memory efficiency by \textbf{$188\times$} and speed up training+inference by
\textbf{$13.48\times$}.","[{'name': 'Bo Xue'}, {'name': 'Yi Xu'}, {'name': 'Yunchong Song'}, {'name': 'Yiming Pang'}, {'name': 'Yuyang Ren'}, {'name': 'Jiaxin Ding'}, {'name': 'Luoyi Fu'}, {'name': 'Xinbing Wang'}]",2024-08-13T10:15:55Z
http://arxiv.org/abs/2408.06778v1,http://arxiv.org/abs/2408.06778v1,Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors,"Link prediction models can benefit from incorporating textual descriptions of
entities and relations, enabling fully inductive learning and flexibility in
dynamic graphs. We address the challenge of also capturing rich structured
information about the local neighbourhood of entities and their relations, by
introducing a Transformer-based approach that effectively integrates textual
descriptions with graph structure, reducing the reliance on resource-intensive
text encoders. Our experiments on three challenging datasets show that our
Fast-and-Frugal Text-Graph (FnF-TG) Transformers achieve superior performance
compared to the previous state-of-the-art methods, while maintaining efficiency
and scalability.","[{'name': 'Andrei C. Coman'}, {'name': 'Christos Theodoropoulos'}, {'name': 'Marie-Francine Moens'}, {'name': 'James Henderson'}]",2024-08-13T10:04:29Z
http://arxiv.org/abs/2408.06755v1,http://arxiv.org/abs/2408.06755v1,"Sumotosima: A Framework and Dataset for Classifying and Summarizing
  Otoscopic Images","Otoscopy is a diagnostic procedure to examine the ear canal and eardrum using
an otoscope. It identifies conditions like infections, foreign bodies, ear drum
perforations and ear abnormalities. We propose a novel resource efficient deep
learning and transformer based framework, Sumotosima (Summarizer for otoscopic
images), an end-to-end pipeline for classification followed by summarization.
Our framework works on combination of triplet and cross-entropy losses.
Additionally, we use Knowledge Enhanced Multimodal BART whose input is fused
textual and image embedding. The objective is to provide summaries that are
well-suited for patients, ensuring clarity and efficiency in understanding
otoscopic images. Given the lack of existing datasets, we have curated our own
OCASD (Otoscopic Classification And Summary Dataset), which includes 500 images
with 5 unique categories annotated with their class and summaries by
Otolaryngologists. Sumotosima achieved a result of 98.03%, which is 7.00%,
3.10%, 3.01% higher than K-Nearest Neighbors, Random Forest and Support Vector
Machines, respectively, in classification tasks. For summarization, Sumotosima
outperformed GPT-4o and LLaVA by 88.53% and 107.57% in ROUGE scores,
respectively. We have made our code and dataset publicly available at
https://github.com/anas2908/Sumotosima","[{'name': 'Eram Anwarul Khan'}, {'name': 'Anas Anwarul Haq Khan'}]",2024-08-13T09:26:41Z
http://arxiv.org/abs/2408.06737v1,http://arxiv.org/abs/2408.06737v1,Multilingual Models for Check-Worthy Social Media Posts Detection,"This work presents an extensive study of transformer-based NLP models for
detection of social media posts that contain verifiable factual claims and
harmful claims. The study covers various activities, including dataset
collection, dataset pre-processing, architecture selection, setup of settings,
model training (fine-tuning), model testing, and implementation. The study
includes a comprehensive analysis of different models, with a special focus on
multilingual models where the same model is capable of processing social media
posts in both English and in low-resource languages such as Arabic, Bulgarian,
Dutch, Polish, Czech, Slovak. The results obtained from the study were
validated against state-of-the-art models, and the comparison demonstrated the
robustness of the proposed models. The novelty of this work lies in the
development of multi-label multilingual classification models that can
simultaneously detect harmful posts and posts that contain verifiable factual
claims in an efficient way.","[{'name': 'Sebastian Kula'}, {'name': 'Michal Gregor'}]",2024-08-13T08:55:28Z
http://arxiv.org/abs/2408.06732v1,http://arxiv.org/abs/2408.06732v1,"Exploring the anatomy of articulation rate in spontaneous English
  speech: relationships between utterance length effects and social factors","Speech rate has been shown to vary across social categories such as gender,
age, and dialect, while also being conditioned by properties of speech
planning. The effect of utterance length, where speech rate is faster and less
variable for longer utterances, has also been shown to reduce the role of
social factors once it has been accounted for, leaving unclear the relationship
between social factors and speech production in conditioning speech rate.
Through modelling of speech rate across 13 English speech corpora, it is found
that utterance length has the largest effect on speech rate, though this effect
itself varies little across corpora and speakers. While age and gender also
modulate speech rate, their effects are much smaller in magnitude. These
findings suggest utterance length effects may be conditioned by articulatory
and perceptual constraints, and that social influences on speech rate should be
interpreted in the broader context of how speech rate variation is structured.","[{'name': 'James Tanner'}, {'name': 'Morgan Sonderegger'}, {'name': 'Jane Stuart-Smith'}, {'name': 'Tyler Kendall'}, {'name': 'Jeff Mielke'}, {'name': 'Robin Dodsworth'}, {'name': 'Erik Thomas'}]",2024-08-13T08:47:29Z
http://arxiv.org/abs/2408.06731v1,http://arxiv.org/abs/2408.06731v1,"Large language models can consistently generate high-quality content for
  election disinformation operations","Advances in large language models have raised concerns about their potential
use in generating compelling election disinformation at scale. This study
presents a two-part investigation into the capabilities of LLMs to automate
stages of an election disinformation operation. First, we introduce DisElect, a
novel evaluation dataset designed to measure LLM compliance with instructions
to generate content for an election disinformation operation in localised UK
context, containing 2,200 malicious prompts and 50 benign prompts. Using
DisElect, we test 13 LLMs and find that most models broadly comply with these
requests; we also find that the few models which refuse malicious prompts also
refuse benign election-related prompts, and are more likely to refuse to
generate content from a right-wing perspective. Secondly, we conduct a series
of experiments (N=2,340) to assess the ""humanness"" of LLMs: the extent to which
disinformation operation content generated by an LLM is able to pass as
human-written. Our experiments suggest that almost all LLMs tested released
since 2022 produce election disinformation operation content indiscernible by
human evaluators over 50% of the time. Notably, we observe that multiple models
achieve above-human levels of humanness. Taken together, these findings suggest
that current LLMs can be used to generate high-quality content for election
disinformation operations, even in hyperlocalised scenarios, at far lower costs
than traditional methods, and offer researchers and policymakers an empirical
benchmark for the measurement and evaluation of these capabilities in current
and future models.","[{'name': 'Angus R. Williams'}, {'name': 'Liam Burke-Moore'}, {'name': 'Ryan Sze-Yin Chan'}, {'name': 'Florence E. Enock'}, {'name': 'Federico Nanni'}, {'name': 'Tvesha Sippy'}, {'name': 'Yi-Ling Chung'}, {'name': 'Evelina Gabasova'}, {'name': 'Kobi Hackenburg'}, {'name': 'Jonathan Bright'}]",2024-08-13T08:45:34Z
http://arxiv.org/abs/2408.06725v1,http://arxiv.org/abs/2408.06725v1,"Enhancing Visual Dialog State Tracking through Iterative Object-Entity
  Alignment in Multi-Round Conversations","Visual Dialog (VD) is a task where an agent answers a series of image-related
questions based on a multi-round dialog history. However, previous VD methods
often treat the entire dialog history as a simple text input, disregarding the
inherent conversational information flows at the round level. In this paper, we
introduce Multi-round Dialogue State Tracking model (MDST), a framework that
addresses this limitation by leveraging the dialogue state learned from dialog
history to answer questions. MDST captures each round of dialog history,
constructing internal dialogue state representations defined as 2-tuples of
vision-language representations. These representations effectively ground the
current question, enabling the generation of accurate answers. Experimental
results on the VisDial v1.0 dataset demonstrate that MDST achieves a new
state-of-the-art performance in generative setting. Furthermore, through a
series of human studies, we validate the effectiveness of MDST in generating
long, consistent, and human-like answers while consistently answering a series
of questions correctly.","[{'name': 'Wei Pang'}, {'name': 'Ruixue Duan'}, {'name': 'Jinfu Yang'}, {'name': 'Ning Li'}]",2024-08-13T08:36:15Z
http://arxiv.org/abs/2408.06675v1,http://arxiv.org/abs/2408.06675v1,"Latin Treebanks in Review: An Evaluation of Morphological Tagging Across
  Time","Existing Latin treebanks draw from Latin's long written tradition, spanning
17 centuries and a variety of cultures. Recent efforts have begun to harmonize
these treebanks' annotations to better train and evaluate morphological
taggers. However, the heterogeneity of these treebanks must be carefully
considered to build effective and reliable data. In this work, we review
existing Latin treebanks to identify the texts they draw from, identify their
overlap, and document their coverage across time and genre. We additionally
design automated conversions of their morphological feature annotations into
the conventions of standard Latin grammar. From this, we build new time-period
data splits that draw from the existing treebanks which we use to perform a
broad cross-time analysis for POS and morphological feature tagging. We find
that BERT-based taggers outperform existing taggers while also being more
robust to cross-domain shifts.","[{'name': 'Marisa Hudspeth'}, {'name': ""Brendan O'Connor""}, {'name': 'Laure Thompson'}]",2024-08-13T06:55:54Z
http://arxiv.org/abs/2408.06673v1,http://arxiv.org/abs/2408.06673v1,Pragmatic inference of scalar implicature by LLMs,"This study investigates how Large Language Models (LLMs), particularly BERT
(Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic
inference of scalar implicature, such as some. Two sets of experiments were
conducted using cosine similarity and next sentence/token prediction as
experimental methods. The results in experiment 1 showed that, both models
interpret some as pragmatic implicature not all in the absence of context,
aligning with human language processing. In experiment 2, in which Question
Under Discussion (QUD) was presented as a contextual cue, BERT showed
consistent performance regardless of types of QUDs, while GPT-2 encountered
processing difficulties since a certain type of QUD required pragmatic
inference for implicature. The findings revealed that, in terms of theoretical
approaches, BERT inherently incorporates pragmatic implicature not all within
the term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2
seems to encounter processing difficulties in inferring pragmatic implicature
within context, consistent with Context-driven model (Sperber and Wilson,
2002).","[{'name': 'Ye-eun Cho'}, {'name': 'Seong mook Kim'}]",2024-08-13T06:52:29Z
http://arxiv.org/abs/2408.06663v2,http://arxiv.org/abs/2408.06663v2,"Amuro & Char: Analyzing the Relationship between Pre-Training and
  Fine-Tuning of Large Language Models","The development of large language models leads to the formation of a
pre-train-then-align paradigm, in which the model is typically pre-trained on a
large text corpus and undergoes a tuning stage to align the model with human
preference or downstream tasks. In this work, we investigate the relationship
between pre-training and fine-tuning by fine-tuning multiple intermediate
pre-trained model checkpoints. Our results on 18 datasets suggest that i)
continual pre-training improves the model in a latent way that unveils after
fine-tuning; ii) with extra fine-tuning, the datasets that the model does not
demonstrate capability gain much more than those that the model performs well
during the pre-training stage; iii) although model benefits significantly
through supervised fine-tuning, it may forget previously known domain knowledge
and the tasks that are not seen during fine-tuning; iv) the model resembles
high sensitivity to evaluation prompts after supervised fine-tuning, but this
sensitivity can be alleviated by more pre-training.","[{'name': 'Kaiser Sun'}, {'name': 'Mark Dredze'}]",2024-08-13T06:28:43Z
http://arxiv.org/abs/2408.06634v1,http://arxiv.org/abs/2408.06634v1,"Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM
  Approach","Accurate stock market predictions following earnings reports are crucial for
investors. Traditional methods, particularly classical machine learning models,
struggle with these predictions because they cannot effectively process and
interpret extensive textual data contained in earnings reports and often
overlook nuances that influence market movements. This paper introduces an
advanced approach by employing Large Language Models (LLMs) instruction
fine-tuned with a novel combination of instruction-based techniques and
quantized low-rank adaptation (QLoRA) compression. Our methodology integrates
'base factors', such as financial metric growth and earnings transcripts, with
'external factors', including recent market indices performances and analyst
grades, to create a rich, supervised dataset. This comprehensive dataset
enables our models to achieve superior predictive performance in terms of
accuracy, weighted F1, and Matthews correlation coefficient (MCC), especially
evident in the comparison with benchmarks such as GPT-4. We specifically
highlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases
significant improvements over baseline models. The paper also discusses the
potential of expanding the output capabilities to include a 'Hold' option and
extending the prediction horizon, aiming to accommodate various investment
styles and time frames. This study not only demonstrates the power of
integrating cutting-edge AI with fine-tuned financial data but also paves the
way for future research in enhancing AI-driven financial analysis tools.","[{'name': 'Haowei Ni'}, {'name': 'Shuchen Meng'}, {'name': 'Xupeng Chen'}, {'name': 'Ziqing Zhao'}, {'name': 'Andi Chen'}, {'name': 'Panfeng Li'}, {'name': 'Shiyao Zhang'}, {'name': 'Qifu Yin'}, {'name': 'Yuanqing Wang'}, {'name': 'Yuxi Chan'}]",2024-08-13T04:53:31Z
http://arxiv.org/abs/2408.06632v1,http://arxiv.org/abs/2408.06632v1,"EditScribe: Non-Visual Image Editing with Natural Language Verification
  Loops","Image editing is an iterative process that requires precise visual evaluation
and manipulation for the output to match the editing intent. However, current
image editing tools do not provide accessible interaction nor sufficient
feedback for blind and low vision individuals to achieve this level of control.
To address this, we developed EditScribe, a prototype system that makes image
editing accessible using natural language verification loops powered by large
multimodal models. Using EditScribe, the user first comprehends the image
content through initial general and object descriptions, then specifies edit
actions using open-ended natural language prompts. EditScribe performs the
image edit, and provides four types of verification feedback for the user to
verify the performed edit, including a summary of visual changes, AI judgement,
and updated general and object descriptions. The user can ask follow-up
questions to clarify and probe into the edits or verification feedback, before
performing another edit. In a study with ten blind or low-vision users, we
found that EditScribe supported participants to perform and verify image edit
actions non-visually. We observed different prompting strategies from
participants, and their perceptions on the various types of verification
feedback. Finally, we discuss the implications of leveraging natural language
verification loops to make visual authoring non-visually accessible.","[{'name': 'Ruei-Che Chang'}, {'name': 'Yuxuan Liu'}, {'name': 'Lotus Zhang'}, {'name': 'Anhong Guo'}]",2024-08-13T04:40:56Z
http://arxiv.org/abs/2408.06631v1,http://arxiv.org/abs/2408.06631v1,"IFShip: A Large Vision-Language Model for Interpretable Fine-grained
  Ship Classification via Domain Knowledge-Enhanced Instruction Tuning","End-to-end interpretation is currently the prevailing paradigm for remote
sensing fine-grained ship classification (RS-FGSC) task. However, its inference
process is uninterpretable, leading to criticism as a black box model. To
address this issue, we propose a large vision-language model (LVLM) named
IFShip for interpretable fine-grained ship classification. Unlike traditional
methods, IFShip excels in interpretability by accurately conveying the
reasoning process of FGSC in natural language. Specifically, we first design a
domain knowledge-enhanced Chain-of-Thought (COT) prompt generation mechanism.
This mechanism is used to semi-automatically construct a task-specific
instruction-following dataset named TITANIC-FGS, which emulates human-like
logical decision-making. We then train the IFShip model using task instructions
tuned with the TITANIC-FGS dataset. Building on IFShip, we develop an FGSC
visual chatbot that redefines the FGSC problem as a step-by-step reasoning task
and conveys the reasoning process in natural language. Experimental results
reveal that the proposed method surpasses state-of-the-art FGSC algorithms in
both classification interpretability and accuracy. Moreover, compared to LVLMs
like LLaVA and MiniGPT-4, our approach demonstrates superior expertise in the
FGSC task. It provides an accurate chain of reasoning when fine-grained ship
types are recognizable to the human eye and offers interpretable explanations
when they are not.","[{'name': 'Mingning Guo'}, {'name': 'Mengwei Wu'}, {'name': 'Yuxiang Shen'}, {'name': 'Haifeng Li'}, {'name': 'Chao Tao'}]",2024-08-13T04:36:18Z
http://arxiv.org/abs/2408.06627v1,http://arxiv.org/abs/2408.06627v1,WorldScribe: Towards Context-Aware Live Visual Descriptions,"Automated live visual descriptions can aid blind people in understanding
their surroundings with autonomy and independence. However, providing
descriptions that are rich, contextual, and just-in-time has been a
long-standing challenge in accessibility. In this work, we develop WorldScribe,
a system that generates automated live real-world visual descriptions that are
customizable and adaptive to users' contexts: (i) WorldScribe's descriptions
are tailored to users' intents and prioritized based on semantic relevance.
(ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively
succinct descriptions for dynamic scenes, while presenting longer and detailed
ones for stable settings. (iii) WorldScribe is adaptive to sound contexts,
e.g., increasing volume in noisy environments, or pausing when conversations
start. Powered by a suite of vision, language, and sound recognition models,
WorldScribe introduces a description generation pipeline that balances the
tradeoffs between their richness and latency to support real-time use. The
design of WorldScribe is informed by prior work on providing visual
descriptions and a formative study with blind participants. Our user study and
subsequent pipeline evaluation show that WorldScribe can provide real-time and
fairly accurate visual descriptions to facilitate environment understanding
that is adaptive and customized to users' contexts. Finally, we discuss the
implications and further steps toward making live visual descriptions more
context-aware and humanized.","[{'name': 'Ruei-Che Chang'}, {'name': 'Yuxuan Liu'}, {'name': 'Anhong Guo'}]",2024-08-13T04:32:45Z
http://arxiv.org/abs/2408.06621v1,http://arxiv.org/abs/2408.06621v1,"Towards Robust and Cost-Efficient Knowledge Unlearning for Large
  Language Models","Large Language Models (LLMs) have demonstrated strong reasoning and
memorization capabilities via pretraining on massive textual corpora. However,
training LLMs on human-written text entails significant risk of privacy and
copyright violations, which demands an efficient machine unlearning framework
to remove knowledge of sensitive data without retraining the model from
scratch. While Gradient Ascent (GA) is widely used for unlearning by reducing
the likelihood of generating unwanted information, the unboundedness of
increasing the cross-entropy loss causes not only unstable optimization, but
also catastrophic forgetting of knowledge that needs to be retained. We also
discover its joint application under low-rank adaptation results in
significantly suboptimal computational cost vs. generative performance
trade-offs. In light of this limitation, we propose two novel techniques for
robust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge
loss that suppresses unwanted tokens by increasing the probability of the next
most likely token, thereby retaining fluency and structure in language
generation. We also propose to initialize low-rank adapter weights based on
Fisher-weighted low-rank approximation, which induces faster unlearning and
better knowledge retention by allowing model updates to be focused on
parameters that are important in generating textual data we wish to remove.","[{'name': 'Sungmin Cha'}, {'name': 'Sungjun Cho'}, {'name': 'Dasol Hwang'}, {'name': 'Moontae Lee'}]",2024-08-13T04:18:32Z
http://arxiv.org/abs/2408.06618v1,http://arxiv.org/abs/2408.06618v1,"Generalized knowledge-enhanced framework for biomedical entity and
  relation extraction","In recent years, there has been an increasing number of frameworks developed
for biomedical entity and relation extraction. This research effort aims to
address the accelerating growth in biomedical publications and the intricate
nature of biomedical texts, which are written for mainly domain experts. To
handle these challenges, we develop a novel framework that utilizes external
knowledge to construct a task-independent and reusable background knowledge
graph for biomedical entity and relation extraction. The design of our model is
inspired by how humans learn domain-specific topics. In particular, humans
often first acquire the most basic and common knowledge regarding a field to
build the foundational knowledge and then use that as a basis for extending to
various specialized topics. Our framework employs such common-knowledge-sharing
mechanism to build a general neural-network knowledge graph that is learning
transferable to different domain-specific biomedical texts effectively.
Experimental evaluations demonstrate that our model, equipped with this
generalized and cross-transferable knowledge base, achieves competitive
performance benchmarks, including BioRelEx for binding interaction detection
and ADE for Adverse Drug Effect identification.","[{'name': 'Minh Nguyen'}, {'name': 'Phuong Le'}]",2024-08-13T04:06:45Z
http://arxiv.org/abs/2408.06610v1,http://arxiv.org/abs/2408.06610v1,CROME: Cross-Modal Adapters for Efficient Multimodal LLM,"Multimodal Large Language Models (MLLMs) demonstrate remarkable
image-language capabilities, but their widespread use faces challenges in
cost-effective training and adaptation. Existing approaches often necessitate
expensive language model retraining and limited adaptability. Additionally, the
current focus on zero-shot performance improvements offers insufficient
guidance for task-specific tuning. We propose CROME, an efficient
vision-language instruction tuning framework. It features a novel gated
cross-modal adapter that effectively combines visual and textual
representations prior to input into a frozen LLM. This lightweight adapter,
trained with minimal parameters, enables efficient cross-modal understanding.
Notably, CROME demonstrates superior zero-shot performance on standard visual
question answering and instruction-following benchmarks. Moreover, it yields
fine-tuning with exceptional parameter efficiency, competing with task-specific
specialist state-of-the-art methods. CROME demonstrates the potential of pre-LM
alignment for building scalable, adaptable, and parameter-efficient multimodal
models.","[{'name': 'Sayna Ebrahimi'}, {'name': 'Sercan O. Arik'}, {'name': 'Tejas Nama'}, {'name': 'Tomas Pfister'}]",2024-08-13T03:45:11Z
http://arxiv.org/abs/2408.06598v1,http://arxiv.org/abs/2408.06598v1,"A Perspective on Large Language Models, Intelligent Machines, and
  Knowledge Acquisition","Large Language Models (LLMs) are known for their remarkable ability to
generate synthesized 'knowledge', such as text documents, music, images, etc.
However, there is a huge gap between LLM's and human capabilities for
understanding abstract concepts and reasoning. We discuss these issues in a
larger philosophical context of human knowledge acquisition and the Turing
test. In addition, we illustrate the limitations of LLMs by analyzing GPT-4
responses to questions ranging from science and math to common sense reasoning.
These examples show that GPT-4 can often imitate human reasoning, even though
it lacks understanding. However, LLM responses are synthesized from a large LLM
model trained on all available data. In contrast, human understanding is based
on a small number of abstract concepts. Based on this distinction, we discuss
the impact of LLMs on acquisition of human knowledge and education.","[{'name': 'Vladimir Cherkassky'}, {'name': 'Eng Hock Lee'}]",2024-08-13T03:25:49Z
http://arxiv.org/abs/2408.06583v2,http://arxiv.org/abs/2408.06583v2,"An Event Structure-aware Generative Model for Biomedical Event
  Extraction","Biomedical Event Extraction (BEE) is a challenging task that involves
modeling complex relationships between fine-grained entities in biomedical
text. Most existing BEE models rely on classification methods that ignore label
semantics and argument dependencies in the data. Although generative models
that use prompts are increasingly being used for event extraction, they face
two main challenges: creating effective prompts for the biomedical domain and
dealing with events with complex structures in the text. To address these
limitations, we propose GenBEE, a generative model enhanced with
structure-aware prefixes for biomedical event extraction. GenBEE constructs
event prompts that leverage knowledge distilled from large language models
(LLMs), thereby incorporating both label semantics and argument dependency
relationships. Additionally, GenBEE introduces a structural prefix learning
module that generates structure-aware prefixes with structural prompts,
enriching the generation process with structural features. Extensive
experiments on three benchmark datasets demonstrate the effectiveness of GenBEE
and it achieves state-of-the-art performance on the MLEE and GE11 datasets.
Moreover, our analysis shows that the structural prefixes effectively bridge
the gap between structural prompts and the representation space of generative
models, enabling better integration of event structural information.","[{'name': 'Haohan Yuan'}, {'name': 'Siu Cheung Hui'}, {'name': 'Haopeng Zhang'}]",2024-08-13T02:43:19Z
http://arxiv.org/abs/2408.06578v2,http://arxiv.org/abs/2408.06578v2,OpenEP: Open-Ended Future Event Prediction,"Future event prediction (FEP) is a long-standing and crucial task in the
world, as understanding the evolution of events enables early risk
identification, informed decision-making, and strategic planning. Existing work
typically treats event prediction as classification tasks and confines the
outcomes of future events to a fixed scope, such as yes/no questions, candidate
set, and taxonomy, which is difficult to include all possible outcomes of
future events. In this paper, we introduce OpenEP (an Open-Ended Future Event
Prediction task), which generates flexible and diverse predictions aligned with
real-world scenarios. This is mainly reflected in two aspects: firstly, the
predictive questions are diverse, covering different stages of event
development and perspectives; secondly, the outcomes are flexible, without
constraints on scope or format. To facilitate the study of this task, we
construct OpenEPBench, an open-ended future event prediction dataset. For
question construction, we pose questions from seven perspectives, including
location, time, event development, event outcome, event impact, event response,
and other, to facilitate an in-depth analysis and understanding of the
comprehensive evolution of events. For outcome construction, we collect
free-form text containing the outcomes as ground truth to provide semantically
complete and detail-enriched outcomes. Furthermore, we propose StkFEP, a
stakeholder-enhanced future event prediction framework, that incorporates event
characteristics for open-ended settings. Our method extracts stakeholders
involved in events to extend questions to gather diverse information. We also
collect historically events that are relevant and similar to the question to
reveal potential evolutionary patterns. Experiment results indicate that
accurately predicting future events in open-ended settings is challenging for
existing LLMs.","[{'name': 'Yong Guan'}, {'name': 'Hao Peng'}, {'name': 'Xiaozhi Wang'}, {'name': 'Lei Hou'}, {'name': 'Juanzi Li'}]",2024-08-13T02:35:54Z
http://arxiv.org/abs/2408.06576v1,http://arxiv.org/abs/2408.06576v1,"CTISum: A New Benchmark Dataset For Cyber Threat Intelligence
  Summarization","Cyber Threat Intelligence (CTI) summarization task requires the system to
generate concise and accurate highlights from raw intelligence data, which
plays an important role in providing decision-makers with crucial information
to quickly detect and respond to cyber threats in the cybersecurity domain.
However, efficient techniques for summarizing CTI reports, including facts,
analytical insights, attack processes, etc., have largely been unexplored,
primarily due to the lack of available dataset. To this end, we present CTISum,
a new benchmark for CTI summarization task. Considering the importance of
attack process, a novel fine-grained subtask of attack process summarization is
proposed to enable defenders to assess risk, identify security gaps,
vulnerabilities, and so on. Specifically, we first design a multi-stage
annotation pipeline to gather and annotate the CTI data, and then benchmark the
CTISum with a collection of extractive and abstractive summarization methods.
Experimental results show that current state-of-the-art models exhibit
limitations when applied to CTISum, underscoring the fact that automatically
producing concise summaries of CTI reports remains an open research challenge.","[{'name': 'Wei Peng'}, {'name': 'Junmei Ding'}, {'name': 'Wei Wang'}, {'name': 'Lei Cui'}, {'name': 'Wei Cai'}, {'name': 'Zhiyu Hao'}, {'name': 'Xiaochun Yun'}]",2024-08-13T02:25:16Z
http://arxiv.org/abs/2408.06574v1,http://arxiv.org/abs/2408.06574v1,"SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark
  Large Language Model","Large language models (LLMs) have shown remarkable achievements across
various language tasks.To enhance the performance of LLMs in scientific
literature services, we developed the scientific literature LLM (SciLit-LLM)
through pre-training and supervised fine-tuning on scientific literature,
building upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge
service system Spark Research Assistant (SparkRA) based on our SciLit-LLM.
SparkRA is accessible online and provides three primary functions: literature
investigation, paper reading, and academic writing. As of July 30, 2024,
SparkRA has garnered over 50,000 registered users, with a total usage count
exceeding 1.3 million.","[{'name': 'Dayong Wu'}, {'name': 'Jiaqi Li'}, {'name': 'Baoxin Wang'}, {'name': 'Honghong Zhao'}, {'name': 'Siyuan Xue'}, {'name': 'Yanjie Yang'}, {'name': 'Zhijun Chang'}, {'name': 'Rui Zhang'}, {'name': 'Li Qian'}, {'name': 'Bo Wang'}, {'name': 'Shijin Wang'}, {'name': 'Zhixiong Zhang'}, {'name': 'Guoping Hu'}]",2024-08-13T02:18:47Z
http://arxiv.org/abs/2408.06569v1,http://arxiv.org/abs/2408.06569v1,Social Debiasing for Fair Multi-modal LLMs,"Multi-modal Large Language Models (MLLMs) have advanced significantly,
offering powerful vision-language understanding capabilities. However, these
models often inherit severe social biases from their training datasets, leading
to unfair predictions based on attributes like race and gender. This paper
addresses the issue of social biases in MLLMs by i) Introducing a comprehensive
Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a
more diverse and extensive training set compared to existing datasets. ii)
Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by
revisiting the MLLM training process, rescaling the autoregressive loss
function, and improving data sampling methods to counteract biases. Through
extensive experiments on various MLLMs, our CMSC dataset and ASD method
demonstrate a significant reduction in social biases while maintaining the
models' original performance.","[{'name': 'Harry Cheng'}, {'name': 'Yangyang Guo'}, {'name': 'Qingpei Guo'}, {'name': 'Ming Yang'}, {'name': 'Tian Gan'}, {'name': 'Liqiang Nie'}]",2024-08-13T02:08:32Z
http://arxiv.org/abs/2408.06567v1,http://arxiv.org/abs/2408.06567v1,"AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out
  Strategies","In recent years, with the rapid application of large language models across
various fields, the scale of these models has gradually increased, and the
resources required for their pre-training have grown exponentially. Training an
LLM from scratch will cost a lot of computation resources while scaling up from
a smaller model is a more efficient approach and has thus attracted significant
attention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B
Mixture of Experts (MoE) language model that has 8 experts with 16 billion
parameters each and is developed using an innovative training methodology
called EfficientScale. This approach optimizes performance while minimizing
data requirements through a two-stage process. The first stage, termed
Scale-Up, initializes the larger model with weights from a pre-trained smaller
model, enabling substantial knowledge transfer and continuous pretraining with
significantly less data. The second stage, Scale-Out, uses a pre-trained dense
model to initialize the MoE experts, further enhancing knowledge transfer and
performance. Extensive validation experiments on 1.8B and 7B models compared
various initialization schemes, achieving models that maintain and reduce loss
during continuous pretraining. Utilizing the optimal scheme, we successfully
trained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating
significant improvements in performance and training efficiency.","[{'name': 'Bo-Wen Zhang'}, {'name': 'Liangdong Wang'}, {'name': 'Ye Yuan'}, {'name': 'Jijie Li'}, {'name': 'Shuhao Gu'}, {'name': 'Mengdi Zhao'}, {'name': 'Xinya Wu'}, {'name': 'Guang Liu'}, {'name': 'Chengwei Wu'}, {'name': 'Hanyu Zhao'}, {'name': 'Li Du'}, {'name': 'Yiming Ju'}, {'name': 'Quanyue Ma'}, {'name': 'Yulong Ao'}, {'name': 'Yingli Zhao'}, {'name': 'Songhe Zhu'}, {'name': 'Zhou Cao'}, {'name': 'Dong Liang'}, {'name': 'Yonghua Lin'}, {'name': 'Ming Zhang'}, {'name': 'Shunfei Wang'}, {'name': 'Yanxin Zhou'}, {'name': 'Min Ye'}, {'name': 'Xuekai Chen'}, {'name': 'Xinyang Yu'}, {'name': 'Xiangjun Huang'}, {'name': 'Jian Yang'}]",2024-08-13T02:07:00Z
http://arxiv.org/abs/2408.06537v1,http://arxiv.org/abs/2408.06537v1,"Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality
  Parallel Data Outperforms Traditional Web-Crawled Data","Recent research in neural machine translation (NMT) has shown that training
on high-quality machine-generated data can outperform training on
human-generated data. This work accompanies the first-ever release of a
LLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and
multi-sentence examples. We perform extensive experiments to demonstrate the
quality of our dataset in terms of its downstream impact on NMT model
performance. We find that training from scratch on our (machine-generated)
dataset outperforms training on the (web-crawled) WMT'23 training dataset
(which is 300 times larger), and also outperforms training on the top-quality
subset of the WMT'23 training dataset. We also find that performing
self-distillation by finetuning the LLM which generated this dataset
outperforms the LLM's strong few-shot baseline. These findings corroborate the
quality of our dataset, and demonstrate the value of high-quality
machine-generated data in improving performance of NMT models.","[{'name': 'Mara Finkelstein'}, {'name': 'David Vilar'}, {'name': 'Markus Freitag'}]",2024-08-13T00:06:56Z
http://arxiv.org/abs/2408.06527v1,http://arxiv.org/abs/2408.06527v1,"Chain-of-Strategy Planning with LLMs: Aligning the Generation of
  Psychotherapy Dialogue with Strategy in Motivational Interviewing","Recent advancements in large language models (LLMs) have shown promise in
generating psychotherapeutic dialogues, especially in Motivational Interviewing
(MI). However, how to employ strategies, a set of motivational interviewing
(MI) skills, to generate therapeutic-adherent conversations with explainability
is underexplored. We propose an approach called strategy-aware dialogue
generation with Chain-of-Strategy (CoS) planning, which first predicts MI
strategies as reasoning and utilizes these strategies to guide the subsequent
dialogue generation. It brings the potential for controllable and explainable
generation in psychotherapy by aligning the generated MI dialogues with
therapeutic strategies. Extensive experiments including automatic and human
evaluations are conducted to validate the effectiveness of the MI strategy. Our
findings demonstrate the potential of LLMs in producing strategically aligned
dialogues and suggest directions for practical applications in
psychotherapeutic settings.","[{'name': 'Xin Sun'}, {'name': 'Xiao Tang'}, {'name': 'Abdallah El Ali'}, {'name': 'Zhuying Li'}, {'name': 'Xiaoyu Shen'}, {'name': 'Pengjie Ren'}, {'name': 'Jan de Wit'}, {'name': 'Jiahuan Pei'}, {'name': 'Jos A. Bosch'}]",2024-08-12T23:19:02Z
http://arxiv.org/abs/2408.06520v1,http://arxiv.org/abs/2408.06520v1,"Hierarchical in-Context Reinforcement Learning with Hindsight Modular
  Reflections for Planning","Large Language Models (LLMs) have demonstrated remarkable abilities in
various language tasks, making them promising candidates for decision-making in
robotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose
Hierarchical in-Context Reinforcement Learning (HCRL), a novel framework that
decomposes complex tasks into sub-tasks using an LLM-based high-level policy,
in which a complex task is decomposed into sub-tasks by a high-level policy
on-the-fly. The sub-tasks, defined by goals, are assigned to the low-level
policy to complete. Once the LLM agent determines that the goal is finished, a
new goal will be proposed. To improve the agent's performance in multi-episode
execution, we propose Hindsight Modular Reflection (HMR), where, instead of
reflecting on the full trajectory, we replace the task objective with
intermediate goals and let the agent reflect on shorter trajectories to improve
reflection efficiency. We evaluate the decision-making ability of the proposed
HCRL in three benchmark environments--ALFWorld, Webshop, and HotpotQA. Results
show that HCRL can achieve 9%, 42%, and 10% performance improvement in 5
episodes of execution over strong in-context learning baselines.","[{'name': 'Chuanneng Sun'}, {'name': 'Songjun Huang'}, {'name': 'Dario Pompili'}]",2024-08-12T22:40:01Z
http://arxiv.org/abs/2408.06518v1,http://arxiv.org/abs/2408.06518v1,"Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in
  Language Models","Despite their wide adoption, the biases and unintended behaviors of language
models remain poorly understood. In this paper, we identify and characterize a
phenomenon never discussed before, which we call semantic leakage, where models
leak irrelevant information from the prompt into the generation in unexpected
ways. We propose an evaluation setting to detect semantic leakage both by
humans and automatically, curate a diverse test suite for diagnosing this
behavior, and measure significant semantic leakage in 13 flagship models. We
also show that models exhibit semantic leakage in languages besides English and
across different settings and generation scenarios. This discovery highlights
yet another type of bias in language models that affects their generation
patterns and behavior.","[{'name': 'Hila Gonen'}, {'name': 'Terra Blevins'}, {'name': 'Alisa Liu'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Noah A. Smith'}]",2024-08-12T22:30:55Z
http://arxiv.org/abs/2408.06484v1,http://arxiv.org/abs/2408.06484v1,"Cross-Lingual Conversational Speech Summarization with Large Language
  Models","Cross-lingual conversational speech summarization is an important problem,
but suffers from a dearth of resources. While transcriptions exist for a number
of languages, translated conversational speech is rare and datasets containing
summaries are non-existent. We build upon the existing Fisher and Callhome
Spanish-English Speech Translation corpus by supplementing the translations
with summaries. The summaries are generated using GPT-4 from the reference
translations and are treated as ground truth. The task is to generate similar
summaries in the presence of transcription and translation errors. We build a
baseline cascade-based system using open-source speech recognition and machine
translation models. We test a range of LLMs for summarization and analyze the
impact of transcription and translation errors. Adapting the Mistral-7B model
for this task performs significantly better than off-the-shelf models and
matches the performance of GPT-4.","[{'name': 'Max Nelson'}, {'name': 'Shannon Wotherspoon'}, {'name': 'Francis Keith'}, {'name': 'William Hartmann'}, {'name': 'Matthew Snover'}]",2024-08-12T20:40:46Z
http://arxiv.org/abs/2408.06474v1,http://arxiv.org/abs/2408.06474v1,TOGGL: Transcribing Overlapping Speech with Staggered Labeling,"Transcribing the speech of multiple overlapping speakers typically requires
separating the audio into multiple streams and recognizing each one
independently. More recent work jointly separates and transcribes, but requires
a separate decoding component for each speaker. We propose the TOGGL model to
simultaneously transcribe the speech of multiple speakers. The TOGGL model uses
special output tokens to attribute the speech to each speaker with only a
single decoder. Our approach generalizes beyond two speakers, even when trained
only on two-speaker data. We demonstrate superior performance compared to
competing approaches on a conversational speech dataset. Our approach also
improves performance on single-speaker audio.","[{'name': 'Chak-Fai Li'}, {'name': 'William Hartmann'}, {'name': 'Matthew Snover'}]",2024-08-12T20:19:27Z
http://arxiv.org/abs/2408.06458v1,http://arxiv.org/abs/2408.06458v1,"Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in
  Language Models","We propose a novel in-context learning algorithm for building autonomous
decision-making language agents. The language agent continuously attempts to
solve the same task by self-correcting each time the task fails. Our selected
language agent demonstrates the ability to solve tasks in a text-based game
environment. Our results show that the gemma-2-9b-it language model, using our
proposed method, can successfully complete two of six tasks that failed in the
first attempt. This highlights the effectiveness of our approach in enhancing
the problem-solving capabilities of a single language model through
self-correction, paving the way for more advanced autonomous agents. The code
is publicly available at
https://github.com/YenCheHsiao/AutonomousLLMAgentwithAdaptingPlanning.","[{'name': 'Yen-Che Hsiao'}, {'name': 'Abhishek Dutta'}]",2024-08-12T19:18:05Z
http://arxiv.org/abs/2408.06450v1,http://arxiv.org/abs/2408.06450v1,Evaluating Language Models for Efficient Code Generation,"We introduce Differential Performance Evaluation (DPE), a framework designed
to reliably evaluate Large Language Models (LLMs) for efficient code
generation. Traditional coding benchmarks often fail to provide reliable
insights into code efficiency, due to their reliance on simplistic test inputs
and the absence of effective compound metrics. DPE addresses these issues by
focusing on efficiency-demanding programming tasks and establishing an
insightful compound metric for performance evaluation. DPE operates in two
phases: To curate efficiency datasets, it selects efficiency-demanding tasks
from existing coding benchmarks and generates computationally expensive inputs
to stress the efficiency of LLM solutions. To assess the code efficiency, DPE
profiles the new solution and compares it globally against a set of reference
solutions that exhibit distinct efficiency levels, where the matched level
defines its efficiency score. As a proof of concept, we use DPE to create
EvalPerf, a benchmark with 121 performance-challenging coding tasks. Our
comprehensive evaluation draws interesting findings on the efficiency impact of
model sizes, instruction tuning, and prompting. For example, while the scaling
law fails to account for code efficiency, general instruction tuning benefits
both code correctness and efficiency. We also evaluate the evaluation by
examining the effectiveness of DPE, showing that EvalPerf is reliable and
convenient to use even across platforms.","[{'name': 'Jiawei Liu'}, {'name': 'Songrun Xie'}, {'name': 'Junhao Wang'}, {'name': 'Yuxiang Wei'}, {'name': 'Yifeng Ding'}, {'name': 'Lingming Zhang'}]",2024-08-12T18:59:13Z
http://arxiv.org/abs/2408.06423v1,http://arxiv.org/abs/2408.06423v1,Evaluating Language Models on Entity Disambiguation in Tables,"Tables are crucial containers of information, but understanding their meaning
may be challenging. Indeed, recently, there has been a focus on Semantic Table
Interpretation (STI), i.e., the task that involves the semantic annotation of
tabular data to disambiguate their meaning. Over the years, there has been a
surge in interest in data-driven approaches based on deep learning that have
increasingly been combined with heuristic-based approaches. In the last period,
the advent of Large Language Models (LLMs) has led to a new category of
approaches for table annotation. The interest in this research field,
characterised by multiple challenges, has led to a proliferation of approaches
employing different techniques. However, these approaches have not been
consistently evaluated on a common ground, making evaluation and comparison
difficult. This work proposes an extensive evaluation of four state-of-the-art
(SOTA) approaches - Alligator (formerly s-elBat), Dagobah, TURL, and
TableLlama; the first two belong to the family of heuristic-based algorithms,
while the others are respectively encoder-only and decoder-only LLMs. The
primary objective is to measure the ability of these approaches to solve the
entity disambiguation task, with the ultimate aim of charting new research
paths in the field.","[{'name': 'Federico Belotti'}, {'name': 'Fabio Dadda'}, {'name': 'Marco Cremaschi'}, {'name': 'Roberto Avogadro'}, {'name': 'Riccardo Pozzi'}, {'name': 'Matteo Palmonari'}]",2024-08-12T18:01:50Z
http://arxiv.org/abs/2408.06335v1,http://arxiv.org/abs/2408.06335v1,"LOLgorithm: Integrating Semantic,Syntactic and Contextual Elements for
  Humor Classification","This paper explores humor detection through a linguistic lens, prioritizing
syntactic, semantic, and contextual features over computational methods in
Natural Language Processing. We categorize features into syntactic, semantic,
and contextual dimensions, including lexicons, structural statistics, Word2Vec,
WordNet, and phonetic style. Our proposed model, Colbert, utilizes BERT
embeddings and parallel hidden layers to capture sentence congruity. By
combining syntactic, semantic, and contextual features, we train Colbert for
humor detection. Feature engineering examines essential syntactic and semantic
features alongside BERT embeddings. SHAP interpretations and decision trees
identify influential features, revealing that a holistic approach improves
humor detection accuracy on unseen data. Integrating linguistic cues from
different dimensions enhances the model's ability to understand humor
complexity beyond traditional computational methods.","[{'name': 'Tanisha Khurana'}, {'name': 'Kaushik Pillalamarri'}, {'name': 'Vikram Pande'}, {'name': 'Munindar Singh'}]",2024-08-12T17:52:11Z
http://arxiv.org/abs/2408.06333v1,http://arxiv.org/abs/2408.06333v1,"FastFiD: Improve Inference Efficiency of Open Domain Question Answering
  via Sentence Selection","Open Domain Question Answering (ODQA) has been advancing rapidly in recent
times, driven by significant developments in dense passage retrieval and
pretrained language models. Current models typically incorporate the FiD
framework, which is composed by a neural retriever alongside an encoder-decoder
neural reader. In the answer generation process, the retriever will retrieve
numerous passages (around 100 for instance), each of which is then individually
encoded by the encoder. Subsequently, the decoder makes predictions based on
these encoded passages. Nevertheless, this framework can be relatively
time-consuming, particularly due to the extensive length of the gathered
passages. To address this, we introduce FastFiD in this paper, a novel approach
that executes sentence selection on the encoded passages. This aids in
retaining valuable sentences while reducing the context length required for
generating answers. Experiments on three commonly used datasets (Natural
Questions, TriviaQA and ASQA) demonstrate that our method can enhance the
inference speed by 2.3X-5.7X, while simultaneously maintaining the model's
performance. Moreover, an in-depth analysis of the model's attention reveals
that the selected sentences indeed hold a substantial contribution towards the
final answer. The codes are publicly available at
https://github.com/thunlp/FastFiD.","[{'name': 'Yufei Huang'}, {'name': 'Xu Han'}, {'name': 'Maosong Sun'}]",2024-08-12T17:50:02Z
http://arxiv.org/abs/2408.06332v1,http://arxiv.org/abs/2408.06332v1,"Animate, or Inanimate, That is the Question for Large Language Models","The cognitive essence of humans is deeply intertwined with the concept of
animacy, which plays an essential role in shaping their memory, vision, and
multi-layered language understanding. Although animacy appears in language via
nuanced constraints on verbs and adjectives, it is also learned and refined
through extralinguistic information. Similarly, we assume that the LLMs'
limited abilities to understand natural language when processing animacy are
motivated by the fact that these models are trained exclusively on text.
  Hence, the question this paper aims to answer arises: can LLMs, in their
digital wisdom, process animacy in a similar way to what humans would do? We
then propose a systematic analysis via prompting approaches. In particular, we
probe different LLMs by prompting them using animate, inanimate, usual, and
stranger contexts. Results reveal that, although LLMs have been trained
predominantly on textual data, they exhibit human-like behavior when faced with
typical animate and inanimate entities in alignment with earlier studies.
Hence, LLMs can adapt to understand unconventional situations by recognizing
oddities as animated without needing to interface with unspoken cognitive
triggers humans rely on to break down animations.","[{'name': 'Leonardo Ranaldi'}, {'name': 'Giulia Pucci'}, {'name': 'Fabio Massimo Zanzotto'}]",2024-08-12T17:48:55Z
http://arxiv.org/abs/2408.06327v1,http://arxiv.org/abs/2408.06327v1,"VisualAgentBench: Towards Large Multimodal Models as Visual Foundation
  Agents","Large Multimodal Models (LMMs) have ushered in a new era in artificial
intelligence, merging capabilities in both language and vision to form highly
capable Visual Foundation Agents. These agents are postulated to excel across a
myriad of tasks, potentially approaching general artificial intelligence.
However, existing benchmarks fail to sufficiently challenge or showcase the
full potential of LMMs in complex, real-world environments. To address this
gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering
benchmark specifically designed to train and evaluate LMMs as visual foundation
agents across diverse scenarios, including Embodied, Graphical User Interface,
and Visual Design, with tasks formulated to probe the depth of LMMs'
understanding and interaction capabilities. Through rigorous testing across
nine proprietary LMM APIs and eight open models, we demonstrate the
considerable yet still developing agent capabilities of these models.
Additionally, VAB constructs a trajectory training set constructed through
hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and
Human Demonstrations, promoting substantial performance improvements in LMMs
through behavior cloning. Our work not only aims to benchmark existing models
but also provides a solid foundation for future development into visual
foundation agents. Code, train \& test data, and part of fine-tuned open LMMs
are available at \url{https://github.com/THUDM/VisualAgentBench}.","[{'name': 'Xiao Liu'}, {'name': 'Tianjie Zhang'}, {'name': 'Yu Gu'}, {'name': 'Iat Long Iong'}, {'name': 'Yifan Xu'}, {'name': 'Xixuan Song'}, {'name': 'Shudan Zhang'}, {'name': 'Hanyu Lai'}, {'name': 'Xinyi Liu'}, {'name': 'Hanlin Zhao'}, {'name': 'Jiadai Sun'}, {'name': 'Xinyue Yang'}, {'name': 'Yu Yang'}, {'name': 'Zehan Qi'}, {'name': 'Shuntian Yao'}, {'name': 'Xueqiao Sun'}, {'name': 'Siyi Cheng'}, {'name': 'Qinkai Zheng'}, {'name': 'Hao Yu'}, {'name': 'Hanchen Zhang'}, {'name': 'Wenyi Hong'}, {'name': 'Ming Ding'}, {'name': 'Lihang Pan'}, {'name': 'Xiaotao Gu'}, {'name': 'Aohan Zeng'}, {'name': 'Zhengxiao Du'}, {'name': 'Chan Hee Song'}, {'name': 'Yu Su'}, {'name': 'Yuxiao Dong'}, {'name': 'Jie Tang'}]",2024-08-12T17:44:17Z
http://arxiv.org/abs/2408.06303v1,http://arxiv.org/abs/2408.06303v1,Long-Form Answers to Visual Questions from Blind and Low Vision People,"Vision language models can now generate long-form answers to questions about
images - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a
dataset of long-form answers to visual questions posed by blind and low vision
(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,
collected from human expert describers and six VQA models. We develop and
annotate functional roles of sentences of LFVQA and demonstrate that long-form
answers contain information beyond the question answer such as explanations and
suggestions. We further conduct automatic and human evaluations with BLV and
sighted people to evaluate long-form answers. BLV people perceive both
human-written and generated long-form answers to be plausible, but generated
answers often hallucinate incorrect visual details, especially for unanswerable
visual questions (e.g., blurry or irrelevant images). To reduce hallucinations,
we evaluate the ability of VQA models to abstain from answering unanswerable
questions across multiple prompting strategies.","[{'name': 'Mina Huh'}, {'name': 'Fangyuan Xu'}, {'name': 'Yi-Hao Peng'}, {'name': 'Chongyan Chen'}, {'name': 'Hansika Murugu'}, {'name': 'Danna Gurari'}, {'name': 'Eunsol Choi'}, {'name': 'Amy Pavel'}]",2024-08-12T17:15:02Z
http://arxiv.org/abs/2408.06292v1,http://arxiv.org/abs/2408.06292v1,"The AI Scientist: Towards Fully Automated Open-Ended Scientific
  Discovery","One of the grand challenges of artificial general intelligence is developing
agents capable of conducting scientific research and discovering new knowledge.
While frontier models have already been used as aids to human scientists, e.g.
for brainstorming ideas, writing code, or prediction tasks, they still conduct
only a small part of the scientific process. This paper presents the first
comprehensive framework for fully automatic scientific discovery, enabling
frontier large language models to perform research independently and
communicate their findings. We introduce The AI Scientist, which generates
novel research ideas, writes code, executes experiments, visualizes results,
describes its findings by writing a full scientific paper, and then runs a
simulated review process for evaluation. In principle, this process can be
repeated to iteratively develop ideas in an open-ended fashion, acting like the
human scientific community. We demonstrate its versatility by applying it to
three distinct subfields of machine learning: diffusion modeling,
transformer-based language modeling, and learning dynamics. Each idea is
implemented and developed into a full paper at a cost of less than $15 per
paper. To evaluate the generated papers, we design and validate an automated
reviewer, which we show achieves near-human performance in evaluating paper
scores. The AI Scientist can produce papers that exceed the acceptance
threshold at a top machine learning conference as judged by our automated
reviewer. This approach signifies the beginning of a new era in scientific
discovery in machine learning: bringing the transformative benefits of AI
agents to the entire research process of AI itself, and taking us closer to a
world where endless affordable creativity and innovation can be unleashed on
the world's most challenging problems. Our code is open-sourced at
https://github.com/SakanaAI/AI-Scientist","[{'name': 'Chris Lu'}, {'name': 'Cong Lu'}, {'name': 'Robert Tjarko Lange'}, {'name': 'Jakob Foerster'}, {'name': 'Jeff Clune'}, {'name': 'David Ha'}]",2024-08-12T16:58:11Z
http://arxiv.org/abs/2408.06285v1,http://arxiv.org/abs/2408.06285v1,"Synthetic Patient-Physician Dialogue Generation from Clinical Notes
  Using LLM","Medical dialogue systems (MDS) enhance patient-physician communication,
improve healthcare accessibility, and reduce costs. However, acquiring suitable
data to train these systems poses significant challenges. Privacy concerns
prevent the use of real conversations, necessitating synthetic alternatives.
Synthetic dialogue generation from publicly available clinical notes offers a
promising solution to this issue, providing realistic data while safeguarding
privacy. Our approach, SynDial, uses a single LLM iteratively with zero-shot
prompting and a feedback loop to generate and refine high-quality synthetic
dialogues. The feedback consists of weighted evaluation scores for similarity
and extractiveness. The iterative process ensures dialogues meet predefined
thresholds, achieving superior extractiveness as a result of the feedback loop.
Additionally, evaluation shows that the generated dialogues excel in factuality
metric compared to the baselines and has comparable diversity scores with GPT4.","[{'name': 'Trisha Das'}, {'name': 'Dina Albassam'}, {'name': 'Jimeng Sun'}]",2024-08-12T16:49:22Z
http://arxiv.org/abs/2408.06281v1,http://arxiv.org/abs/2408.06281v1,MovieSum: An Abstractive Summarization Dataset for Movie Screenplays,"Movie screenplay summarization is challenging, as it requires an
understanding of long input contexts and various elements unique to movies.
Large language models have shown significant advancements in document
summarization, but they often struggle with processing long input contexts.
Furthermore, while television transcripts have received attention in recent
studies, movie screenplay summarization remains underexplored. To stimulate
research in this area, we present a new dataset, MovieSum, for abstractive
summarization of movie screenplays. This dataset comprises 2200 movie
screenplays accompanied by their Wikipedia plot summaries. We manually
formatted the movie screenplays to represent their structural elements.
Compared to existing datasets, MovieSum possesses several distinctive features:
(1) It includes movie screenplays, which are longer than scripts of TV
episodes. (2) It is twice the size of previous movie screenplay datasets. (3)
It provides metadata with IMDb IDs to facilitate access to additional external
knowledge. We also show the results of recently released large language models
applied to summarization on our dataset to provide a detailed baseline.","[{'name': 'Rohit Saxena'}, {'name': 'Frank Keller'}]",2024-08-12T16:43:09Z
http://arxiv.org/abs/2408.06276v2,http://arxiv.org/abs/2408.06276v2,"Review-driven Personalized Preference Reasoning with Large Language
  Models for Recommendation","Recent advancements in Large Language Models (LLMs) have demonstrated
exceptional performance across a wide range of tasks, generating significant
interest in their application to recommendation systems. However, existing
methods have not fully capitalized on the potential of LLMs, often constrained
by limited input information or failing to fully utilize their advanced
reasoning capabilities. To address these limitations, we introduce EXP3RT, a
novel LLM-based recommender designed to leverage rich preference information
contained in user and item reviews. EXP3RT is basically fine-tuned through
distillation from a teacher LLM to perform three key tasks in order: EXP3RT
first extracts and encapsulates essential subjective preferences from raw
reviews, aggregates and summarizes them according to specific criteria to
create user and item profiles. It then generates detailed step-by-step
reasoning followed by predicted rating, i.e., reasoning-enhanced rating
prediction, by considering both subjective and objective information from
user/item profiles and item descriptions. This personalized preference
reasoning from EXP3RT enhances rating prediction accuracy and also provides
faithful and reasonable explanations for recommendation. Extensive experiments
show that EXP3RT outperforms existing methods on both rating prediction and
candidate item reranking for top-k recommendation, while significantly
enhancing the explainability of recommendation systems.","[{'name': 'Jieyong Kim'}, {'name': 'Hyunseo Kim'}, {'name': 'Hyunjin Cho'}, {'name': 'SeongKu Kang'}, {'name': 'Buru Chang'}, {'name': 'Jinyoung Yeo'}, {'name': 'Dongha Lee'}]",2024-08-12T16:39:03Z
http://arxiv.org/abs/2408.06273v2,http://arxiv.org/abs/2408.06273v2,"FuxiTranyu: A Multilingual Large Language Model Trained with Balanced
  Data","Large language models (LLMs) have demonstrated prowess in a wide range of
tasks. However, many LLMs exhibit significant performance discrepancies between
high- and low-resource languages. To mitigate this challenge, we present
FuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the
need of the research community for balanced and high-performing multilingual
capabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is
trained from scratch on a meticulously balanced multilingual data repository
that contains 600 billion tokens covering 43 natural languages and 16
programming languages. In addition to the base model, we also develop two
instruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse
multilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined
with DPO on a preference dataset for enhanced alignment ability. Extensive
experiments on a wide range of multilingual benchmarks demonstrate the
competitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,
BLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability
analyses at both the neuron and representation level suggest that FuxiTranyu is
able to learn consistent multilingual representations across different
languages. To promote further research into multilingual LLMs and their working
mechanisms, we release both the base and instruction-tuned FuxiTranyu models
together with 58 pretraining checkpoints at HuggingFace and Github.","[{'name': 'Haoran Sun'}, {'name': 'Renren Jin'}, {'name': 'Shaoyang Xu'}, {'name': 'Leiyu Pan'}, {'name': 'Supryadi'}, {'name': 'Menglong Cui'}, {'name': 'Jiangcun Du'}, {'name': 'Yikun Lei'}, {'name': 'Lei Yang'}, {'name': 'Ling Shi'}, {'name': 'Juesi Xiao'}, {'name': 'Shaolin Zhu'}, {'name': 'Deyi Xiong'}]",2024-08-12T16:34:56Z
http://arxiv.org/abs/2408.06266v1,http://arxiv.org/abs/2408.06266v1,"Anchored Preference Optimization and Contrastive Revisions: Addressing
  Underspecification in Alignment","Large Language Models (LLMs) are often aligned using contrastive alignment
objectives and preference pair datasets. The interaction between model, paired
data, and objective makes alignment a complicated procedure, sometimes
producing subpar results. We study this and find that (i) preference data gives
a better learning signal when the underlying responses are contrastive, and
(ii) alignment objectives lead to better performance when they specify more
control over the model during training. Based on these insights, we introduce
Contrastive Learning from AI Revisions (CLAIR), a data-creation method which
leads to more contrastive preference pairs, and Anchored Preference
Optimization (APO), a controllable and more stable alignment objective. We
align Llama-3-8B-Instruct using various comparable datasets and alignment
objectives and measure MixEval-Hard scores, which correlate highly with human
judgments. The CLAIR preferences lead to the strongest performance out of all
datasets, and APO consistently outperforms less controllable objectives. Our
best model, trained on 32K CLAIR preferences with APO, improves
Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code
is available at https://github.com/ContextualAI/CLAIR_and_APO.","[{'name': ""Karel D'Oosterlinck""}, {'name': 'Winnie Xu'}, {'name': 'Chris Develder'}, {'name': 'Thomas Demeester'}, {'name': 'Amanpreet Singh'}, {'name': 'Christopher Potts'}, {'name': 'Douwe Kiela'}, {'name': 'Shikib Mehri'}]",2024-08-12T16:24:51Z
http://arxiv.org/abs/2408.06259v1,http://arxiv.org/abs/2408.06259v1,"Context-aware Visual Storytelling with Visual Prefix Tuning and
  Contrastive Learning","Visual storytelling systems generate multi-sentence stories from image
sequences. In this task, capturing contextual information and bridging visual
variation bring additional challenges. We propose a simple yet effective
framework that leverages the generalization capabilities of pretrained
foundation models, only training a lightweight vision-language mapping network
to connect modalities, while incorporating context to enhance coherence. We
introduce a multimodal contrastive objective that also improves visual
relevance and story informativeness. Extensive experimental results, across
both automatic metrics and human evaluations, demonstrate that the stories
generated by our framework are diverse, coherent, informative, and interesting.","[{'name': 'Yingjin Song'}, {'name': 'Denis Paperno'}, {'name': 'Albert Gatt'}]",2024-08-12T16:15:32Z
http://arxiv.org/abs/2408.06227v1,http://arxiv.org/abs/2408.06227v1,FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks,"This paper introduces FLEURS-R, a speech restoration applied version of the
Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS)
corpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages as
FLEURS, with improved audio quality and fidelity by applying the speech
restoration model Miipher. The aim of FLEURS-R is to advance speech technology
in more languages and catalyze research including text-to-speech (TTS) and
other speech generation tasks in low-resource languages. Comprehensive
evaluations with the restored speech and TTS baseline models trained from the
new corpus show that the new corpus obtained significantly improved speech
quality while maintaining the semantic contents of the speech. The corpus is
publicly released via Hugging Face.","[{'name': 'Min Ma'}, {'name': 'Yuma Koizumi'}, {'name': 'Shigeki Karita'}, {'name': 'Heiga Zen'}, {'name': 'Jason Riesa'}, {'name': 'Haruko Ishikawa'}, {'name': 'Michiel Bacchiani'}]",2024-08-12T15:28:51Z
http://arxiv.org/abs/2408.06223v1,http://arxiv.org/abs/2408.06223v1,"On Effects of Steering Latent Representation for Large Language Model
  Unlearning","Representation Misdirection for Unlearning (RMU), which steers model
representation in the intermediate layer to a target random representation, is
an effective method for large language model (LLM) unlearning. Despite its high
performance, the underlying cause and explanation remain underexplored. In this
paper, we first theoretically demonstrate that steering forget representations
in the intermediate layer reduces token confidence, causing LLMs to generate
wrong or nonsense responses. Second, we investigate how the coefficient
influences the alignment of forget-sample representations with the random
direction and hint at the optimal coefficient values for effective unlearning
across different network layers. Third, we show that RMU unlearned models are
robust against adversarial jailbreak attacks. Last, our empirical analysis
shows that RMU is less effective when applied to the middle and later layers in
LLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yet
effective alternative method that makes unlearning effective with most layers.
Extensive experiments demonstrate that Adaptive RMU significantly improves the
unlearning performance compared to prior art while incurring no additional
computational cost.","[{'name': 'Dang Huu-Tien'}, {'name': 'Trung-Tin Pham'}, {'name': 'Hoang Thanh-Tung'}, {'name': 'Naoya Inoue'}]",2024-08-12T15:24:50Z
http://arxiv.org/abs/2408.06195v1,http://arxiv.org/abs/2408.06195v1,Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers,"This paper introduces rStar, a self-play mutual reasoning approach that
significantly improves reasoning capabilities of small language models (SLMs)
without fine-tuning or superior models. rStar decouples reasoning into a
self-play mutual generation-discrimination process. First, a target SLM
augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like
reasoning actions to construct higher quality reasoning trajectories. Next,
another SLM, with capabilities similar to the target SLM, acts as a
discriminator to verify each trajectory generated by the target SLM. The
mutually agreed reasoning trajectories are considered mutual consistent, thus
are more likely to be correct. Extensive experiments across five SLMs
demonstrate rStar can effectively solve diverse reasoning problems, including
GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K
accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for
Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be
available at https://github.com/zhentingqi/rStar.","[{'name': 'Zhenting Qi'}, {'name': 'Mingyuan Ma'}, {'name': 'Jiahang Xu'}, {'name': 'Li Lyna Zhang'}, {'name': 'Fan Yang'}, {'name': 'Mao Yang'}]",2024-08-12T14:42:13Z
http://arxiv.org/abs/2408.06186v1,http://arxiv.org/abs/2408.06186v1,"Improving Structural Diversity of Blackbox LLMs via
  Chain-of-Specification Prompting","The capability to generate diverse text is a key challenge facing large
language models (LLMs). Thus far, diversity has been studied via metrics such
as $n$-gram diversity or diversity of BERT embeddings. However, for these kinds
of diversity, the user has little control over the dimensions along which
diversity is considered. For example, in the poetry domain, one might desire
diversity in terms of rhyme and meter, whereas in the code domain, one might
desire diversity in terms of the kinds of expressions used to solve a problem.
We propose a diversity metric called structural diversity, where the user
provides a mapping from generated text to features capturing the kinds of
diversity that they care about. In addition, we propose a novel strategy called
chain-of-specification (CoS) prompting for improving diversity by first having
the LLM generate a specification encoding one instance of structural features,
and then prompting the LLM to generate text that satisfies these features;
notably, our strategy works with blackbox LLMs. In our experiments, we show
that for structural diversity in the poetry and code domains, CoS significantly
improves diversity compared to several baselines.","[{'name': 'Halley Young'}, {'name': 'Yimeng Zeng'}, {'name': 'Jacob Gardner'}, {'name': 'Osbert Bastani'}]",2024-08-12T14:34:06Z
http://arxiv.org/abs/2408.06150v1,http://arxiv.org/abs/2408.06150v1,"LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid
  Library","In this study, we generate and maintain a database of 10 million virtual
lipids through METiS's in-house de novo lipid generation algorithms and lipid
virtual screening techniques. These virtual lipids serve as a corpus for
pre-training, lipid representation learning, and downstream task knowledge
transfer, culminating in state-of-the-art LNP property prediction performance.
We propose LipidBERT, a BERT-like model pre-trained with the Masked Language
Model (MLM) and various secondary tasks. Additionally, we compare the
performance of embeddings generated by LipidBERT and PhatGPT, our GPT-like
lipid generation model, on downstream tasks. The proposed bilingual LipidBERT
model operates in two languages: the language of ionizable lipid pre-training,
using in-house dry-lab lipid structures, and the language of LNP fine-tuning,
utilizing in-house LNP wet-lab data. This dual capability positions LipidBERT
as a key AI-based filter for future screening tasks, including new versions of
METiS de novo lipid libraries and, more importantly, candidates for in vivo
testing for orgran-targeting LNPs. To the best of our knowledge, this is the
first successful demonstration of the capability of a pre-trained language
model on virtual lipids and its effectiveness in downstream tasks using web-lab
data. This work showcases the clever utilization of METiS's in-house de novo
lipid library as well as the power of dry-wet lab integration.","[{'name': 'Tianhao Yu'}, {'name': 'Cai Yao'}, {'name': 'Zhuorui Sun'}, {'name': 'Feng Shi'}, {'name': 'Lin Zhang'}, {'name': 'Kangjie Lyu'}, {'name': 'Xuan Bai'}, {'name': 'Andong Liu'}, {'name': 'Xicheng Zhang'}, {'name': 'Jiali Zou'}, {'name': 'Wenshou Wang'}, {'name': 'Chris Lai'}, {'name': 'Kai Wang'}]",2024-08-12T13:44:24Z
http://arxiv.org/abs/2408.06142v1,http://arxiv.org/abs/2408.06142v1,Med42-v2: A Suite of Clinical LLMs,"Med42-v2 introduces a suite of clinical large language models (LLMs) designed
to address the limitations of generic models in healthcare settings. These
models are built on Llama3 architecture and fine-tuned using specialized
clinical data. They underwent multi-stage preference alignment to effectively
respond to natural prompts. While generic models are often preference-aligned
to avoid answering clinical queries as a precaution, Med42-v2 is specifically
trained to overcome this limitation, enabling its use in clinical settings.
Med42-v2 models demonstrate superior performance compared to the original
Llama3 models in both 8B and 70B parameter configurations and GPT-4 across
various medical benchmarks. These LLMs are developed to understand clinical
queries, perform reasoning tasks, and provide valuable assistance in clinical
environments. The models are now publicly available at
\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.","[{'name': 'Clément Christophe'}, {'name': 'Praveen K Kanithi'}, {'name': 'Tathagata Raha'}, {'name': 'Shadab Khan'}, {'name': 'Marco AF Pimentel'}]",2024-08-12T13:37:31Z
http://arxiv.org/abs/2408.07277v1,http://arxiv.org/abs/2408.07277v1,"Speech vs. Transcript: Does It Matter for Human Annotators in Speech
  Summarization?","Reference summaries for abstractive speech summarization require human
annotation, which can be performed by listening to an audio recording or by
reading textual transcripts of the recording. In this paper, we examine whether
summaries based on annotators listening to the recordings differ from those
based on annotators reading transcripts. Using existing intrinsic evaluation
based on human evaluation, automatic metrics, LLM-based evaluation, and a
retrieval-based reference-free method. We find that summaries are indeed
different based on the source modality, and that speech-based summaries are
more factually consistent and information-selective than transcript-based
summaries. Meanwhile, transcript-based summaries are impacted by recognition
errors in the source, and expert-written summaries are more informative and
reliable. We make all the collected data and analysis code
public(https://github.com/cmu-mlsp/interview_humanssum) to facilitate the
reproduction of our work and advance research in this area.","[{'name': 'Roshan Sharma'}, {'name': 'Suwon Shon'}, {'name': 'Mark Lindsey'}, {'name': 'Hira Dhamyal'}, {'name': 'Rita Singh'}, {'name': 'Bhiksha Raj'}]",2024-08-12T13:25:53Z
http://arxiv.org/abs/2408.06124v1,http://arxiv.org/abs/2408.06124v1,Utilize Transformers for translating Wikipedia category names,"On Wikipedia, articles are categorized to aid readers in navigating content
efficiently. The manual creation of new categories can be laborious and
time-intensive. To tackle this issue, we built language models to translate
Wikipedia categories from English to Vietnamese with a dataset containing
15,000 English-Vietnamese category pairs. Subsequently, small to medium-scale
Transformer pre-trained models with a sequence-to-sequence architecture were
fine-tuned for category translation. The experiments revealed that
OPUS-MT-en-vi surpassed other models, attaining the highest performance with a
BLEU score of 0.73, despite its smaller model storage. We expect our paper to
be an alternative solution for translation tasks with limited computer
resources.","[{'name': 'Hoang-Thang Ta'}, {'name': 'Quoc Thang La'}]",2024-08-12T13:07:34Z
http://arxiv.org/abs/2408.06120v1,http://arxiv.org/abs/2408.06120v1,"How ChatGPT Changed the Media's Narratives on AI: A Semi-Automated
  Narrative Analysis Through Frame Semantics","The recent explosion of attention to AI is arguably one of the biggest in the
technology's media coverage. To investigate the effects it has on the
discourse, we perform a mixed-method frame semantics-based analysis on a
dataset of more than 49,000 sentences collected from 5846 news articles that
mention AI. The dataset covers the twelve-month period centred around the
launch of OpenAI's chatbot ChatGPT and is collected from the most visited
open-access English-language news publishers. Our findings indicate that during
the half year succeeding the launch, media attention rose
tenfold$\unicode{x2014}$from already historically high levels. During this
period, discourse has become increasingly centred around experts and political
leaders, and AI has become more closely associated with dangers and risks. A
deeper review of the data also suggests a qualitative shift in the types of
threat AI is thought to represent, as well as the anthropomorphic qualities
ascribed to it.","[{'name': 'Igor Ryazanov'}, {'name': 'Carl Öhman'}, {'name': 'Johanna Björklund'}]",2024-08-12T13:02:31Z
http://arxiv.org/abs/2408.06087v1,http://arxiv.org/abs/2408.06087v1,Building Decision Making Models Through Language Model Regime,"We propose a novel approach for decision making problems leveraging the
generalization capabilities of large language models (LLMs). Traditional
methods such as expert systems, planning algorithms, and reinforcement learning
often exhibit limited generalization, typically requiring the training of new
models for each unique task. In contrast, LLMs demonstrate remarkable success
in generalizing across varied language tasks, inspiring a new strategy for
training decision making models. Our approach, referred to as ""Learning then
Using"" (LTU), entails a two-stage process. Initially, the \textit{learning}
phase develops a robust foundational decision making model by integrating
diverse knowledge from various domains and decision making contexts. The
subsequent \textit{using} phase refines this foundation model for specific
decision making scenarios. Distinct from other studies that employ LLMs for
decision making through supervised learning, our LTU method embraces a
versatile training methodology that combines broad pre-training with targeted
fine-tuning. Experiments in e-commerce domains such as advertising and search
optimization have shown that LTU approach outperforms traditional supervised
learning regimes in decision making capabilities and generalization. The LTU
approach is the first practical training architecture for both single-step and
multi-step decision making tasks combined with LLMs, which can be applied
beyond game and robot domains. It provides a robust and adaptable framework for
decision making, enhances the effectiveness and flexibility of various systems
in tackling various challenges.","[{'name': 'Yu Zhang'}, {'name': 'Haoxiang Liu'}, {'name': 'Feijun Jiang'}, {'name': 'Weihua Luo'}, {'name': 'Kaifu Zhang'}]",2024-08-12T12:04:14Z
http://arxiv.org/abs/2408.06065v1,http://arxiv.org/abs/2408.06065v1,An Investigation Into Explainable Audio Hate Speech Detection,"Research on hate speech has predominantly revolved around detection and
interpretation from textual inputs, leaving verbal content largely unexplored.
While there has been limited exploration into hate speech detection within
verbal acoustic speech inputs, the aspect of interpretability has been
overlooked. Therefore, we introduce a new task of explainable audio hate speech
detection. Specifically, we aim to identify the precise time intervals,
referred to as audio frame-level rationales, which serve as evidence for hate
speech classification. Towards this end, we propose two different approaches:
cascading and End-to-End (E2E). The cascading approach initially converts audio
to transcripts, identifies hate speech within these transcripts, and
subsequently locates the corresponding audio time frames. Conversely, the E2E
approach processes audio utterances directly, which allows it to pinpoint hate
speech within specific time frames. Additionally, due to the lack of
explainable audio hate speech datasets that include audio frame-level
rationales, we curated a synthetic audio dataset to train our models. We
further validated these models on actual human speech utterances and found that
the E2E approach outperforms the cascading method in terms of the audio frame
Intersection over Union (IoU) metric. Furthermore, we observed that including
frame-level rationales significantly enhances hate speech detection accuracy
for the E2E approach.
  \textbf{Disclaimer} The reader may encounter content of an offensive or
hateful nature. However, given the nature of the work, this cannot be avoided.","[{'name': 'Jinmyeong An'}, {'name': 'Wonjun Lee'}, {'name': 'Yejin Jeon'}, {'name': 'Jungseul Ok'}, {'name': 'Yunsu Kim'}, {'name': 'Gary Geunbae Lee'}]",2024-08-12T11:32:34Z
http://arxiv.org/abs/2408.06062v1,http://arxiv.org/abs/2408.06062v1,"On Tables with Numbers, with Numbers","This paper is a critical reflection on the epistemic culture of contemporary
computational linguistics, framed in the context of its growing obsession with
tables with numbers. We argue against tables with numbers on the basis of their
epistemic irrelevance, their environmental impact, their role in enabling and
exacerbating social inequalities, and their deep ties to commercial
applications and profit-driven research. We substantiate our arguments with
empirical evidence drawn from a meta-analysis of computational linguistics
research over the last decade.","[{'name': 'Konstantinos Kogkalidis'}, {'name': 'Stergios Chatzikyriakidis'}]",2024-08-12T11:23:24Z
http://arxiv.org/abs/2408.06061v1,http://arxiv.org/abs/2408.06061v1,Quantum Algorithms for Compositional Text Processing,"Quantum computing and AI have found a fruitful intersection in the field of
natural language processing. We focus on the recently proposed DisCoCirc
framework for natural language, and propose a quantum adaptation, QDisCoCirc.
This is motivated by a compositional approach to rendering AI interpretable:
the behavior of the whole can be understood in terms of the behavior of parts,
and the way they are put together. For the model-native primitive operation of
text similarity, we derive quantum algorithms for fault-tolerant quantum
computers to solve the task of question-answering within QDisCoCirc, and show
that this is BQP-hard; note that we do not consider the complexity of
question-answering in other natural language processing models. Assuming
widely-held conjectures, implementing the proposed model classically would
require super-polynomial resources. Therefore, it could provide a meaningful
demonstration of the power of practical quantum processors. The model
construction builds on previous work in compositional quantum natural language
processing. Word embeddings are encoded as parameterized quantum circuits, and
compositionality here means that the quantum circuits compose according to the
linguistic structure of the text. We outline a method for evaluating the model
on near-term quantum processors, and elsewhere we report on a recent
implementation of this on quantum hardware. In addition, we adapt a quantum
algorithm for the closest vector problem to obtain a Grover-like speedup in the
fault-tolerant regime for our model. This provides an unconditional quadratic
speedup over any classical algorithm in certain circumstances, which we will
verify empirically in future work.","[{'name': 'Tuomas Laakkonen'}, {'name': 'Konstantinos Meichanetzidis'}, {'name': 'Bob Coecke'}]",2024-08-12T11:21:40Z
http://arxiv.org/abs/2408.06044v1,http://arxiv.org/abs/2408.06044v1,"DiagESC: Dialogue Synthesis for Integrating Depression Diagnosis into
  Emotional Support Conversation","Dialogue systems for mental health care aim to provide appropriate support to
individuals experiencing mental distress. While extensive research has been
conducted to deliver adequate emotional support, existing studies cannot
identify individuals who require professional medical intervention and cannot
offer suitable guidance. We introduce the Diagnostic Emotional Support
Conversation task for an advanced mental health management system. We develop
the DESC dataset to assess depression symptoms while maintaining user
experience by utilizing task-specific utterance generation prompts and a strict
filtering algorithm. Evaluations by professional psychological counselors
indicate that DESC has a superior ability to diagnose depression than existing
data. Additionally, conversational quality evaluation reveals that DESC
maintains fluent, consistent, and coherent dialogues.","[{'name': 'Seungyeon Seo'}, {'name': 'Gary Geunbae Lee'}]",2024-08-12T10:26:39Z
http://arxiv.org/abs/2408.06043v1,http://arxiv.org/abs/2408.06043v1,"Enhancing Dialogue Speech Recognition with Robust Contextual Awareness
  via Noise Representation Learning","Recent dialogue systems rely on turn-based spoken interactions, requiring
accurate Automatic Speech Recognition (ASR). Errors in ASR can significantly
impact downstream dialogue tasks. To address this, using dialogue context from
user and agent interactions for transcribing subsequent utterances has been
proposed. This method incorporates the transcription of the user's speech and
the agent's response as model input, using the accumulated context generated by
each turn. However, this context is susceptible to ASR errors because it is
generated by the ASR model in an auto-regressive fashion. Such noisy context
can further degrade the benefits of context input, resulting in suboptimal ASR
performance. In this paper, we introduce Context Noise Representation Learning
(CNRL) to enhance robustness against noisy context, ultimately improving
dialogue speech recognition accuracy. To maximize the advantage of context
awareness, our approach includes decoder pre-training using text-based dialogue
data and noise representation learning for a context encoder. Based on the
evaluation of speech dialogues, our method shows superior results compared to
baselines. Furthermore, the strength of our approach is highlighted in noisy
environments where user speech is barely audible due to real-world noise,
relying on contextual information to transcribe the input accurately.","[{'name': 'Wonjun Lee'}, {'name': 'San Kim'}, {'name': 'Gary Geunbae Lee'}]",2024-08-12T10:21:09Z
http://arxiv.org/abs/2408.06040v1,http://arxiv.org/abs/2408.06040v1,"ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation
  Using Large Language Models and Transformers","In the rapidly evolving fields of natural language processing and computer
vision, Visual Word Sense Disambiguation (VWSD) stands as a critical, yet
challenging task. The quest for models that can seamlessly integrate and
interpret multimodal data is more pressing than ever. Imagine a system that can
understand language with the depth and nuance of human cognition, while
simultaneously interpreting the rich visual context of the world around it.
  We present ARPA, an architecture that fuses the unparalleled contextual
understanding of large language models with the advanced feature extraction
capabilities of transformers, which then pass through a custom Graph Neural
Network (GNN) layer to learn intricate relationships and subtle nuances within
the data. This innovative architecture not only sets a new benchmark in visual
word disambiguation but also introduces a versatile framework poised to
transform how linguistic and visual data interact by harnessing the synergistic
strengths of its components, ensuring robust performance even in the most
complex disambiguation scenarios. Through a series of experiments and
comparative analysis, we reveal the substantial advantages of our model,
underscoring its potential to redefine standards in the field. Beyond its
architectural prowess, our architecture excels through experimental
enrichments, including sophisticated data augmentation and multi-modal training
techniques.
  ARPA's introduction marks a significant milestone in visual word
disambiguation, offering a compelling solution that bridges the gap between
linguistic and visual modalities. We invite researchers and practitioners to
explore the capabilities of our model, envisioning a future where such hybrid
models drive unprecedented advancements in artificial intelligence.","[{'name': 'Aristi Papastavrou'}, {'name': 'Maria Lymperaiou'}, {'name': 'Giorgos Stamou'}]",2024-08-12T10:15:13Z
http://arxiv.org/abs/2408.06022v1,http://arxiv.org/abs/2408.06022v1,"Controlling Surprisal in Music Generation via Information Content Curve
  Matching","In recent years, the quality and public interest in music generation systems
have grown, encouraging research into various ways to control these systems. We
propose a novel method for controlling surprisal in music generation using
sequence models. To achieve this goal, we define a metric called Instantaneous
Information Content (IIC). The IIC serves as a proxy function for the perceived
musical surprisal (as estimated from a probabilistic model) and can be
calculated at any point within a music piece. This enables the comparison of
surprisal across different musical content even if the musical events occur in
irregular time intervals. We use beam search to generate musical material whose
IIC curve closely approximates a given target IIC. We experimentally show that
the IIC correlates with harmonic and rhythmic complexity and note density. The
correlation decreases with the length of the musical context used for
estimating the IIC. Finally, we conduct a qualitative user study to test if
human listeners can identify the IIC curves that have been used as targets when
generating the respective musical material. We provide code for creating IIC
interpolations and IIC visualizations on https://github.com/muthissar/iic.","[{'name': 'Mathias Rose Bjare'}, {'name': 'Stefan Lattner'}, {'name': 'Gerhard Widmer'}]",2024-08-12T09:21:41Z
http://arxiv.org/abs/2408.05976v1,http://arxiv.org/abs/2408.05976v1,Global-to-Local Support Spectrums for Language Model Explainability,"Existing sample-based methods, like influence functions and representer
points, measure the importance of a training point by approximating the effect
of its removal from training. As such, they are skewed towards outliers and
points that are very close to the decision boundaries. The explanations
provided by these methods are often static and not specific enough for
different test points. In this paper, we propose a method to generate an
explanation in the form of support spectrums which are based on two main ideas:
the support sets and a global-to-local importance measure. The support set is
the set of training points, in the predicted class, that ``lie in between'' the
test point and training points in the other classes. They indicate how well the
test point can be distinguished from the points not in the predicted class. The
global-to-local importance measure is obtained by decoupling existing methods
into the global and local components which are then used to select the points
in the support set. Using this method, we are able to generate explanations
that are tailored to specific test points. In the experiments, we show the
effectiveness of the method in image classification and text generation tasks.","[{'name': 'Lucas Agussurja'}, {'name': 'Xinyang Lu'}, {'name': 'Bryan Kian Hsiang Low'}]",2024-08-12T08:05:30Z
http://arxiv.org/abs/2408.05977v1,http://arxiv.org/abs/2408.05977v1,"The Language of Trauma: Modeling Traumatic Event Descriptions Across
  Domains with Explainable AI","Psychological trauma can manifest following various distressing events and is
captured in diverse online contexts. However, studies traditionally focus on a
single aspect of trauma, often neglecting the transferability of findings
across different scenarios. We address this gap by training language models
with progressing complexity on trauma-related datasets, including
genocide-related court data, a Reddit dataset on post-traumatic stress disorder
(PTSD), counseling conversations, and Incel forum posts. Our results show that
the fine-tuned RoBERTa model excels in predicting traumatic events across
domains, slightly outperforming large language models like GPT-4. Additionally,
SLALOM-feature scores and conceptual explanations effectively differentiate and
cluster trauma-related language, highlighting different trauma aspects and
identifying sexual abuse and experiences related to death as a common traumatic
event across all datasets. This transferability is crucial as it allows for the
development of tools to enhance trauma detection and intervention in diverse
populations and settings.","[{'name': 'Miriam Schirmer'}, {'name': 'Tobias Leemann'}, {'name': 'Gjergji Kasneci'}, {'name': 'Jürgen Pfeffer'}, {'name': 'David Jurgens'}]",2024-08-12T08:05:30Z
http://arxiv.org/abs/2408.05948v1,http://arxiv.org/abs/2408.05948v1,"ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge
  Graph QA datasets with Large Language Models","The rapid advancement of Large Language Models (LLMs) and conversational
assistants necessitates dynamic, scalable, and configurable conversational
datasets for training and evaluation. These datasets must accommodate diverse
user interaction modes, including text and voice, each presenting unique
modeling challenges. Knowledge Graphs (KGs), with their structured and evolving
nature, offer an ideal foundation for current and precise knowledge. Although
human-curated KG-based conversational datasets exist, they struggle to keep
pace with the rapidly changing user information needs. We present ConvKGYarn, a
scalable method for generating up-to-date and configurable conversational KGQA
datasets. Qualitative psychometric analyses confirm our method can generate
high-quality datasets rivaling a popular conversational KGQA dataset while
offering it at scale and covering a wide range of human-interaction
configurations. We showcase its utility by testing LLMs on diverse
conversations - exploring model behavior on conversational KGQA sets with
different configurations grounded in the same KG fact set. Our results
highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate
parametric knowledge of LLMs, thus offering a robust solution to the constantly
evolving landscape of conversational assistants.","[{'name': 'Ronak Pradeep'}, {'name': 'Daniel Lee'}, {'name': 'Ali Mousavi'}, {'name': 'Jeff Pound'}, {'name': 'Yisi Sang'}, {'name': 'Jimmy Lin'}, {'name': 'Ihab Ilyas'}, {'name': 'Saloni Potdar'}, {'name': 'Mostafa Arefiyan'}, {'name': 'Yunyao Li'}]",2024-08-12T06:48:43Z
http://arxiv.org/abs/2408.05911v1,http://arxiv.org/abs/2408.05911v1,"A New Pipeline For Generating Instruction Dataset via RAG and Self
  Fine-Tuning","With the rapid development of large language models in recent years, there
has been an increasing demand for domain-specific Agents that can cater to the
unique needs of enterprises and organizations. Unlike general models, which
strive for broad coverage, these specialized Agents rely on focused datasets
tailored to their intended applications. This research proposes a pipeline that
leverages the power of LLMs and the Retrieval-Augmented Generation related
framework to construct high-quality instruction datasets for fine-tuning on
specific domains using custom document collections. By ingesting
domain-specific documents, the pipeline generates relevant and contextually
appropriate instructions, thus effectively creating a comprehensive dataset for
fine-tuning LLMs on the target domain. This approach overcomes the limitations
of traditional dataset creation methods, which often rely on manual curation or
web-scraping techniques that may introduce noise and irrelevant data. Notably,
our pipeline offers a dynamic solution that can quickly adapt to updates or
modifications in the domain-specific document collection, eliminating the need
for complete retraining. Additionally, it addresses the challenge of data
scarcity by enabling the generation of instruction datasets from a limited set
of initial documents, rendering it suitable for unpopular or specialized
domains where comprehensive datasets are scarce. As a case study, we apply this
approach to the domain of psychiatry, a field requiring specialized knowledge
and sensitive handling of patient information. The resulting fine-tuned LLM
demonstrates showcases the viability of the proposed approach and underscores
its potential for widespread adoption across various industries and domains
where tailored, accurate, and contextually relevant language models are
indispensable.","[{'name': 'Chih-Wei Song'}, {'name': 'Yu-Kai Lee'}, {'name': 'Yin-Te Tsai'}]",2024-08-12T03:52:11Z
http://arxiv.org/abs/2408.05906v1,http://arxiv.org/abs/2408.05906v1,"AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine
  Advertising","With the increase in the more fluent ad texts automatically created by
natural language generation technology, it is in the high demand to verify the
quality of these creatives in a real-world setting. We propose AdTEC, the first
public benchmark to evaluate ad texts in multiple aspects from the perspective
of practical advertising operations. Our contributions are: (i) Defining five
tasks for evaluating the quality of ad texts and building a dataset based on
the actual operational experience of advertising agencies, which is typically
kept in-house. (ii) Validating the performance of existing pre-trained language
models (PLMs) and human evaluators on the dataset. (iii) Analyzing the
characteristics and providing challenges of the benchmark. The results show
that while PLMs have already reached the practical usage level in several
tasks, human still outperforms in certain domains, implying that there is
significant room for improvement in such area.","[{'name': 'Peinan Zhang'}, {'name': 'Yusuke Sakai'}, {'name': 'Masato Mita'}, {'name': 'Hiroki Ouchi'}, {'name': 'Taro Watanabe'}]",2024-08-12T03:32:53Z
http://arxiv.org/abs/2408.05894v1,http://arxiv.org/abs/2408.05894v1,GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models,"Vision-Language Models (VLMs) building upon the foundation of powerful large
language models have made rapid progress in reasoning across visual and textual
data. While VLMs perform well on vision tasks that they are trained on, our
results highlight key challenges in abstract pattern recognition. We present
GlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of
visual patterns from 40 writing systems with three visual presentation styles.
  GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models
to understand and judge natural language descriptions of visual patterns.
GlyphPattern patterns are drawn from a large-scale cognitive science
investigation of human writing systems; as a result, they are rich in spatial
reference and compositionality. Our experiments show that GlyphPattern is
challenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with
marginal gains from few-shot prompting. Our detailed error analysis reveals
challenges at multiple levels, including visual processing, natural language
understanding, and pattern generalization.","[{'name': 'Zixuan Wu'}, {'name': 'Yoolim Kim'}, {'name': 'Carolyn Jane Anderson'}]",2024-08-12T02:16:47Z
http://arxiv.org/abs/2408.05882v1,http://arxiv.org/abs/2408.05882v1,Creating Arabic LLM Prompts at Scale,"The debut of chatGPT and BARD has popularized instruction following text
generation using LLMs, where a user can interrogate an LLM using natural
language requests and obtain natural language answers that matches their
requests. Training LLMs to respond in this manner requires a large number of
worked out examples of user requests (aka prompts) with corresponding gold
responses. In this paper, we introduce two methods for creating such prompts
for Arabic cheaply and quickly. The first methods entails automatically
translating existing prompt datasets from English, such as PromptSource and
Super-NaturalInstructions, and then using machine translation quality
estimation to retain high quality translations only. The second method involves
creating natural language prompts on top of existing Arabic NLP datasets. Using
these two methods we were able to create more than 67.4 million Arabic prompts
that cover a variety of tasks including summarization, headline generation,
grammar checking, open/closed question answering, creative writing, etc. We
show that fine tuning an open 7 billion parameter large language model, namely
base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter
instruction tuned model, namely Llama3 70B, in handling Arabic prompts.","[{'name': 'Abdelrahman El-Sheikh'}, {'name': 'Ahmed Elmogtaba'}, {'name': 'Kareem Darwish'}, {'name': 'Muhammad Elmallah'}, {'name': 'Ashraf Elneima'}, {'name': 'Hassan Sawaf'}]",2024-08-12T00:46:39Z
http://arxiv.org/abs/2408.05874v1,http://arxiv.org/abs/2408.05874v1,LLM-Based Robust Product Classification in Commerce and Compliance,"Product classification is a crucial task in international trade, as
compliance regulations are verified and taxes and duties are applied based on
product categories. Manual classification of products is time-consuming and
error-prone, and the sheer volume of products imported and exported renders the
manual process infeasible. Consequently, e-commerce platforms and enterprises
involved in international trade have turned to automatic product classification
using machine learning. However, current approaches do not consider the
real-world challenges associated with product classification, such as very
abbreviated and incomplete product descriptions. In addition, recent
advancements in generative Large Language Models (LLMs) and their reasoning
capabilities are mainly untapped in product classification and e-commerce. In
this research, we explore the real-life challenges of industrial classification
and we propose data perturbations that allow for realistic data simulation.
Furthermore, we employ LLM-based product classification to improve the
robustness of the prediction in presence of incomplete data. Our research shows
that LLMs with in-context learning outperform the supervised approaches in the
clean-data scenario. Additionally, we illustrate that LLMs are significantly
more robust than the supervised approaches when data attacks are present.","[{'name': 'Sina Gholamian'}, {'name': 'Gianfranco Romani'}, {'name': 'Bartosz Rudnikowicz'}, {'name': 'Laura Skylaki'}]",2024-08-11T22:59:32Z
http://arxiv.org/abs/2408.05873v1,http://arxiv.org/abs/2408.05873v1,"Defining Boundaries: A Spectrum of Task Feasibility for Large Language
  Models","Large language models (LLMs) have shown remarkable performance in various
tasks but often fail to handle queries that exceed their knowledge and
capabilities, leading to incorrect or fabricated responses. This paper
addresses the need for LLMs to recognize and refuse infeasible tasks due to the
required skills surpassing their capabilities. We first systematically
conceptualize infeasible tasks for LLMs, providing formal definitions and
categorizations that cover a spectrum of related hallucinations. We develop and
benchmark a new dataset comprising diverse infeasible and feasible tasks to
test multiple LLMs' abilities on task feasibility. Furthermore, we explore the
potential of training enhancements to increase LLMs' refusal capabilities with
fine-tuning. Experiments validate the effectiveness of our methods, offering
promising directions for refining the operational boundaries of LLMs in real
applications.","[{'name': 'Wenbo Zhang'}, {'name': 'Zihang Xu'}, {'name': 'Hengrui Cai'}]",2024-08-11T22:58:23Z
http://arxiv.org/abs/2408.07092v1,http://arxiv.org/abs/2408.07092v1,Post-Training Sparse Attention with Double Sparsity,"The inference process for large language models is slow and memory-intensive,
with one of the most critical bottlenecks being excessive Key-Value (KV) cache
accesses. This paper introduces ""Double Sparsity,"" a novel post-training sparse
attention technique designed to alleviate this bottleneck by reducing KV cache
access. Double Sparsity combines token sparsity, which focuses on utilizing
only the important tokens for computing self-attention, with channel sparsity,
an approach that uses important feature channels for identifying important
tokens. Our key insight is that the pattern of channel sparsity is relatively
static, allowing us to use offline calibration to make it efficient at runtime,
thereby enabling accurate and efficient identification of important tokens.
Moreover, this method can be combined with offloading to achieve significant
memory usage reduction. Experimental results demonstrate that Double Sparsity
can achieve \(\frac{1}{16}\) token and channel sparsity with minimal impact on
accuracy across various tasks, including wiki-2 perplexity, key-value
retrieval, and long context benchmarks with models including Llama-2-7B,
Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\times$ acceleration in
attention operations and a 1.9$\times$ improvement in end-to-end inference on
GPUs. With offloading, it achieves a decoding speed acceleration of
16.3$\times$ compared to state-of-the-art solutions at a sequence length of
256K. Our code is publicly available at
\url{https://github.com/andy-yang-1/DoubleSparse}.","[{'name': 'Shuo Yang'}, {'name': 'Ying Sheng'}, {'name': 'Joseph E. Gonzalez'}, {'name': 'Ion Stoica'}, {'name': 'Lianmin Zheng'}]",2024-08-11T18:40:36Z
http://arxiv.org/abs/2408.05840v2,http://arxiv.org/abs/2408.05840v2,Iterative Improvement of an Additively Regularized Topic Model,"Topic modelling is fundamentally a soft clustering problem (of known objects
-- documents, over unknown clusters -- topics). That is, the task is
incorrectly posed. In particular, the topic models are unstable and incomplete.
All this leads to the fact that the process of finding a good topic model
(repeated hyperparameter selection, model training, and topic quality
assessment) can be particularly long and labor-intensive. We aim to simplify
the process, to make it more deterministic and provable. To this end, we
present a method for iterative training of a topic model. The essence of the
method is that a series of related topic models are trained so that each
subsequent model is at least as good as the previous one, i.e., that it retains
all the good topics found earlier. The connection between the models is
achieved by additive regularization. The result of this iterative training is
the last topic model in the series, which we call the iteratively updated
additively regularized topic model (ITAR). Experiments conducted on several
collections of natural language texts show that the proposed ITAR model
performs better than other popular topic models (LDA, ARTM, BERTopic), its
topics are diverse, and its perplexity (ability to ""explain"" the underlying
data) is moderate.","[{'name': 'Alex Gorbulev'}, {'name': 'Vasiliy Alekseev'}, {'name': 'Konstantin Vorontsov'}]",2024-08-11T18:22:12Z
http://arxiv.org/abs/2408.05794v1,http://arxiv.org/abs/2408.05794v1,"HateSieve: A Contrastive Learning Framework for Detecting and Segmenting
  Hateful Content in Multimodal Memes","Amidst the rise of Large Multimodal Models (LMMs) and their widespread
application in generating and interpreting complex content, the risk of
propagating biased and harmful memes remains significant. Current safety
measures often fail to detect subtly integrated hateful content within
``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new
framework designed to enhance the detection and segmentation of hateful
elements in memes. \textsc{HateSieve} features a novel Contrastive Meme
Generator that creates semantically paired memes, a customized triplet dataset
for contrastive learning, and an Image-Text Alignment module that produces
context-aware embeddings for accurate meme segmentation. Empirical experiments
on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses
existing LMMs in performance with fewer trainable parameters but also offers a
robust mechanism for precisely identifying and isolating hateful content.
\textcolor{red}{Caution: Contains academic discussions of hate speech; viewer
discretion advised.}","[{'name': 'Xuanyu Su'}, {'name': 'Yansong Li'}, {'name': 'Diana Inkpen'}, {'name': 'Nathalie Japkowicz'}]",2024-08-11T14:56:06Z
http://arxiv.org/abs/2408.05793v1,http://arxiv.org/abs/2408.05793v1,"SAGA: A Participant-specific Examination of Story Alternatives and Goal
  Applicability for a Deeper Understanding of Complex Events","Interpreting and assessing goal driven actions is vital to understanding and
reasoning over complex events. It is important to be able to acquire the
knowledge needed for this understanding, though doing so is challenging. We
argue that such knowledge can be elicited through a participant achievement
lens. We analyze a complex event in a narrative according to the intended
achievements of the participants in that narrative, the likely future actions
of the participants, and the likelihood of goal success. We collect 6.3K high
quality goal and action annotations reflecting our proposed participant
achievement lens, with an average weighted Fleiss-Kappa IAA of 80%. Our
collection contains annotated alternate versions of each narrative. These
alternate versions vary minimally from the ""original"" story, but can license
drastically different inferences. Our findings suggest that while modern large
language models can reflect some of the goal-based knowledge we study, they
find it challenging to fully capture the design and intent behind concerted
actions, even when the model pretraining included the data from which we
extracted the goal knowledge. We show that smaller models fine-tuned on our
dataset can achieve performance surpassing larger models.","[{'name': 'Sai Vallurupalli'}, {'name': 'Katrin Erk'}, {'name': 'Francis Ferraro'}]",2024-08-11T14:52:40Z
http://arxiv.org/abs/2408.05786v1,http://arxiv.org/abs/2408.05786v1,"HiLight: A Hierarchy-aware Light Global Model with Hierarchical Local
  ConTrastive Learning","Hierarchical text classification (HTC) is a special sub-task of multi-label
classification (MLC) whose taxonomy is constructed as a tree and each sample is
assigned with at least one path in the tree. Latest HTC models contain three
modules: a text encoder, a structure encoder and a multi-label classification
head. Specially, the structure encoder is designed to encode the hierarchy of
taxonomy. However, the structure encoder has scale problem. As the taxonomy
size increases, the learnable parameters of recent HTC works grow rapidly.
Recursive regularization is another widely-used method to introduce
hierarchical information but it has collapse problem and generally relaxed by
assigning with a small weight (ie. 1e-6). In this paper, we propose a
Hierarchy-aware Light Global model with Hierarchical local conTrastive learning
(HiLight), a lightweight and efficient global model only consisting of a text
encoder and a multi-label classification head. We propose a new learning task
to introduce the hierarchical information, called Hierarchical Local
Contrastive Learning (HiLCL). Extensive experiments are conducted on two
benchmark datasets to demonstrate the effectiveness of our model.","[{'name': 'Zhijian Chen'}, {'name': 'Zhonghua Li'}, {'name': 'Jianxin Yang'}, {'name': 'Ye Qi'}]",2024-08-11T14:26:58Z
http://arxiv.org/abs/2408.05769v1,http://arxiv.org/abs/2408.05769v1,"LI-TTA: Language Informed Test-Time Adaptation for Automatic Speech
  Recognition","Test-Time Adaptation (TTA) has emerged as a crucial solution to the domain
shift challenge, wherein the target environment diverges from the original
training environment. A prime exemplification is TTA for Automatic Speech
Recognition (ASR), which enhances model performance by leveraging output
prediction entropy minimization as a self-supervision signal. However, a key
limitation of this self-supervision lies in its primary focus on acoustic
features, with minimal attention to the linguistic properties of the input. To
address this gap, we propose Language Informed Test-Time Adaptation (LI-TTA),
which incorporates linguistic insights during TTA for ASR. LI-TTA integrates
corrections from an external language model to merge linguistic with acoustic
information by minimizing the CTC loss from the correction alongside the
standard TTA loss. With extensive experiments, we show that LI-TTA effectively
improves the performance of TTA for ASR in various distribution shift
situations.","[{'name': 'Eunseop Yoon'}, {'name': 'Hee Suk Yoon'}, {'name': 'John Harvill'}, {'name': 'Mark Hasegawa-Johnson'}, {'name': 'Chang D. Yoo'}]",2024-08-11T13:19:27Z
http://arxiv.org/abs/2408.05767v1,http://arxiv.org/abs/2408.05767v1,Reference-free Hallucination Detection for Large Vision-Language Models,"Large vision-language models (LVLMs) have made significant progress in recent
years. While LVLMs exhibit excellent ability in language understanding,
question answering, and conversations of visual inputs, they are prone to
producing hallucinations. While several methods are proposed to evaluate the
hallucinations in LVLMs, most are reference-based and depend on external tools,
which complicates their practical application. To assess the viability of
alternative methods, it is critical to understand whether the reference-free
approaches, which do not rely on any external tools, can efficiently detect
hallucinations. Therefore, we initiate an exploratory study to demonstrate the
effectiveness of different reference-free solutions in detecting hallucinations
in LVLMs. In particular, we conduct an extensive study on three kinds of
techniques: uncertainty-based, consistency-based, and supervised uncertainty
quantification methods on four representative LVLMs across two different tasks.
The empirical results show that the reference-free approaches are capable of
effectively detecting non-factual responses in LVLMs, with the supervised
uncertainty quantification method outperforming the others, achieving the best
performance across different settings.","[{'name': 'Qing Li'}, {'name': 'Chenyang Lyu'}, {'name': 'Jiahui Geng'}, {'name': 'Derui Zhu'}, {'name': 'Maxim Panov'}, {'name': 'Fakhri Karray'}]",2024-08-11T13:17:14Z
http://arxiv.org/abs/2408.05758v1,http://arxiv.org/abs/2408.05758v1,"VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for
  Speech Processing","Deep learning has brought significant improvements to the field of
cross-modal representation learning. For tasks such as text-to-speech (TTS),
voice conversion (VC), and automatic speech recognition (ASR), a cross-modal
fine-grained (frame-level) sequence representation is desired, emphasizing the
semantic content of the text modality while de-emphasizing the paralinguistic
information of the speech modality. We propose a method called ""Vector
Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)"", which uses the
cross-modal aligned sequence transcoder to bring text and speech into a joint
multimodal space, learning how to connect text and speech at the frame level.
The proposed VQ-CTAP is a paradigm for cross-modal sequence representation
learning, offering a promising solution for fine-grained generation and
recognition tasks in speech processing. The VQ-CTAP can be directly applied to
VC and ASR tasks without fine-tuning or additional structures. We propose a
sequence-aware semantic connector, which connects multiple frozen pre-trained
modules for the TTS task, exhibiting a plug-and-play capability. We design a
stepping optimization strategy to ensure effective model convergence by
gradually injecting and adjusting the influence of various loss components.
Furthermore, we propose a semantic-transfer-wise paralinguistic consistency
loss to enhance representational capabilities, allowing the model to better
generalize to unseen data and capture the nuances of paralinguistic
information. In addition, VQ-CTAP achieves high-compression speech coding at a
rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the
sampling rate. The audio demo is available at
https://qiangchunyu.github.io/VQCTAP/","[{'name': 'Chunyu Qiang'}, {'name': 'Wang Geng'}, {'name': 'Yi Zhao'}, {'name': 'Ruibo Fu'}, {'name': 'Tao Wang'}, {'name': 'Cheng Gong'}, {'name': 'Tianrui Wang'}, {'name': 'Qiuyu Liu'}, {'name': 'Jiangyan Yi'}, {'name': 'Zhengqi Wen'}, {'name': 'Chen Zhang'}, {'name': 'Hao Che'}, {'name': 'Longbiao Wang'}, {'name': 'Jianwu Dang'}, {'name': 'Jianhua Tao'}]",2024-08-11T12:24:23Z
http://arxiv.org/abs/2408.05738v1,http://arxiv.org/abs/2408.05738v1,"Language-Informed Beam Search Decoding for Multilingual Machine
  Translation","Beam search decoding is the de-facto method for decoding auto-regressive
Neural Machine Translation (NMT) models, including multilingual NMT where the
target language is specified as an input. However, decoding multilingual NMT
models commonly produces ``off-target'' translations -- yielding translation
outputs not in the intended language. In this paper, we first conduct an error
analysis of off-target translations for a strong multilingual NMT model and
identify how these decodings are produced during beam search. We then propose
Language-informed Beam Search (LiBS), a general decoding algorithm
incorporating an off-the-shelf Language Identification (LiD) model into beam
search decoding to reduce off-target translations. LiBS is an inference-time
procedure that is NMT-model agnostic and does not require any additional
parallel data. Results show that our proposed LiBS algorithm on average
improves +1.1 BLEU and +0.9 BLEU on WMT and OPUS datasets, and reduces
off-target rates from 22.9\% to 7.7\% and 65.8\% to 25.3\% respectively.","[{'name': 'Yilin Yang'}, {'name': 'Stefan Lee'}, {'name': 'Prasad Tadepalli'}]",2024-08-11T09:57:46Z
http://arxiv.org/abs/2408.05664v1,http://arxiv.org/abs/2408.05664v1,"Training an NLP Scholar at a Small Liberal Arts College: A Backwards
  Designed Course Proposal","The rapid growth in natural language processing (NLP) over the last couple
years has generated student interest and excitement in learning more about the
field. In this paper, we present two types of students that NLP courses might
want to train. First, an ""NLP engineer"" who is able to flexibly design, build
and apply new technologies in NLP for a wide range of tasks. Second, an ""NLP
scholar"" who is able to pose, refine and answer questions in NLP and how it
relates to the society, while also learning to effectively communicate these
answers to a broader audience. While these two types of skills are not mutually
exclusive -- NLP engineers should be able to think critically, and NLP scholars
should be able to build systems -- we think that courses can differ in the
balance of these skills. As educators at Small Liberal Arts Colleges, the
strengths of our students and our institution favors an approach that is better
suited to train NLP scholars. In this paper we articulate what kinds of skills
an NLP scholar should have, and then adopt a backwards design to propose course
components that can aid the acquisition of these skills.","[{'name': 'Grusha Prasad'}, {'name': 'Forrest Davis'}]",2024-08-11T00:50:59Z
http://arxiv.org/abs/2408.05655v1,http://arxiv.org/abs/2408.05655v1,"WiDe-analysis: Enabling One-click Content Moderation Analysis on
  Wikipedia's Articles for Deletion","Content moderation in online platforms is crucial for ensuring activity
therein adheres to existing policies, especially as these platforms grow. NLP
research in this area has typically focused on automating some part of it given
that it is not feasible to monitor all active discussions effectively. Past
works have focused on revealing deletion patterns with like sentiment analysis,
or on developing platform-specific models such as Wikipedia policy or stance
detectors. Unsurprisingly, however, this valuable body of work is rather
scattered, with little to no agreement with regards to e.g., the deletion
discussions corpora used for training or the number of stance labels. Moreover,
while efforts have been made to connect stance with rationales (e.g., to ground
a deletion decision on the relevant policy), there is little explanability work
beyond that. In this paper, we introduce a suite of experiments on Wikipedia
deletion discussions and wide-analyis (Wikipedia Deletion Analysis), a Python
package aimed at providing one click analysis to content moderation
discussions. We release all assets associated with wide-analysis, including
data, models and the Python package, and a HuggingFace space with the goal to
accelerate research on automating content moderation in Wikipedia and beyond.","[{'name': 'Hsuvas Borkakoty'}, {'name': 'Luis Espinosa-Anke'}]",2024-08-10T23:43:11Z
http://arxiv.org/abs/2408.05646v1,http://arxiv.org/abs/2408.05646v1,Eigen Attention: Attention in Low-Rank Space for KV Cache Compression,"Large language models (LLMs) represent a groundbreaking advancement in the
domain of natural language processing due to their impressive reasoning
abilities. Recently, there has been considerable interest in increasing the
context lengths for these models to enhance their applicability to complex
tasks. However, at long context lengths and large batch sizes, the key-value
(KV) cache, which stores the attention keys and values, emerges as the new
bottleneck in memory usage during inference. To address this, we propose Eigen
Attention, which performs the attention operation in a low-rank space, thereby
reducing the KV cache memory overhead. Our proposed approach is orthogonal to
existing KV cache compression techniques and can be used synergistically with
them. Through extensive experiments over OPT, MPT, and Llama model families, we
demonstrate that Eigen Attention results in up to 40% reduction in KV cache
sizes and up to 60% reduction in attention operation latency with minimal drop
in performance.","[{'name': 'Utkarsh Saxena'}, {'name': 'Gobinda Saha'}, {'name': 'Sakshi Choudhary'}, {'name': 'Kaushik Roy'}]",2024-08-10T22:47:12Z
http://arxiv.org/abs/2408.05636v1,http://arxiv.org/abs/2408.05636v1,"Speculative Diffusion Decoding: Accelerating Language Generation through
  Diffusion","Speculative decoding has emerged as a widely adopted method to accelerate
large language model inference without sacrificing the quality of the model
outputs. While this technique has facilitated notable speed improvements by
enabling parallel sequence verification, its efficiency remains inherently
limited by the reliance on incremental token generation in existing draft
models. To overcome this limitation, this paper proposes an adaptation of
speculative decoding which uses discrete diffusion models to generate draft
sequences. This allows parallelization of both the drafting and verification
steps, providing significant speed-ups to the inference process. Our proposed
approach, \textit{Speculative Diffusion Decoding (SpecDiff)}, is validated on
standard language generation benchmarks and empirically demonstrated to provide
a \textbf{up to 8.7x speed-up over standard generation processes and up to 2.5x
speed-up over existing speculative decoding approaches.}","[{'name': 'Jacob K Christopher'}, {'name': 'Brian R Bartoldson'}, {'name': 'Bhavya Kailkhura'}, {'name': 'Ferdinando Fioretto'}]",2024-08-10T21:24:25Z
http://arxiv.org/abs/2408.05566v1,http://arxiv.org/abs/2408.05566v1,Document-Level Event Extraction with Definition-Driven ICL,"In the field of Natural Language Processing (NLP), Large Language Models
(LLMs) have shown great potential in document-level event extraction tasks, but
existing methods face challenges in the design of prompts. To address this
issue, we propose an optimization strategy called ""Definition-driven
Document-level Event Extraction (DDEE)."" By adjusting the length of the prompt
and enhancing the clarity of heuristics, we have significantly improved the
event extraction performance of LLMs. We used data balancing techniques to
solve the long-tail effect problem, enhancing the model's generalization
ability for event types. At the same time, we refined the prompt to ensure it
is both concise and comprehensive, adapting to the sensitivity of LLMs to the
style of prompts. In addition, the introduction of structured heuristic methods
and strict limiting conditions has improved the precision of event and argument
role extraction. These strategies not only solve the prompt engineering
problems of LLMs in document-level event extraction but also promote the
development of event extraction technology, providing new research perspectives
for other tasks in the NLP field.","[{'name': 'Zhuoyuan Liu'}, {'name': 'Yilin Luo'}]",2024-08-10T14:24:09Z
http://arxiv.org/abs/2408.05555v1,http://arxiv.org/abs/2408.05555v1,"Large Language Model-based Role-Playing for Personalized Medical Jargon
  Extraction","Previous studies reveal that Electronic Health Records (EHR), which have been
widely adopted in the U.S. to allow patients to access their personal medical
information, do not have high readability to patients due to the prevalence of
medical jargon. Tailoring medical notes to individual comprehension by
identifying jargon that is difficult for each person will enhance the utility
of generative models. We present the first quantitative analysis to measure the
impact of role-playing in LLM in medical term extraction. By comparing the
results of Mechanical Turk workers over 20 sentences, our study demonstrates
that LLM role-playing improves F1 scores in 95% of cases across 14 different
socio-demographic backgrounds. Furthermore, applying role-playing with
in-context learning outperformed the previous state-of-the-art models. Our
research showed that ChatGPT can improve traditional medical term extraction
systems by utilizing role-play to deliver personalized patient education, a
potential that previous models had not achieved.","[{'name': 'Jung Hoon Lim'}, {'name': 'Sunjae Kwon'}, {'name': 'Zonghai Yao'}, {'name': 'John P. Lalor'}, {'name': 'Hong Yu'}]",2024-08-10T13:40:44Z
http://arxiv.org/abs/2408.05541v1,http://arxiv.org/abs/2408.05541v1,"P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for
  Optimizing LLM Training","In the rapidly evolving field of Large Language Models (LLMs), selecting
high-quality data for fine-tuning is essential. This paper focuses on
task-specific data pruning and selection to enhance fine-tuning. We introduce
an innovative framework, termed P3, which improves LLM performance through a
dynamic, adaptive training strategy. Specifically, P3 comprises the following
components: (1) Policy-driven Difficulty Measurement: we begin by measuring the
difficulty of data based on the model's real-time performance, transitioning
from static, predefined metrics to more dynamic and adaptable ones. (2)
Pace-adaptive Selection: we employ self-paced learning (SPL) to gradually
select increasingly challenging data, thereby progressively enhancing the
model's performance. (3) Diversity Promotion: we integrate Determinantal Point
Process (DPP) into the selection process to promote the diversity within and
between samples, enriching the learning process. We have validated our method
on two well-known LLM datasets, APPS and MATH, designed for logical reasoning
scenarios. The results show that our P3 framework significantly improves
training outcomes compared to traditional methods. By fundamentally refining
data selection and utilization strategies, P3 not only advances theoretical
understanding of dynamic training approaches but also provides a versatile
framework that can revolutionize model training in natural language processing.","[{'name': 'Yingxuan Yang'}, {'name': 'Huayi Wang'}, {'name': 'Muning Wen'}, {'name': 'Weinan Zhang'}]",2024-08-10T12:44:49Z
http://arxiv.org/abs/2408.05524v1,http://arxiv.org/abs/2408.05524v1,"Context-Driven Index Trimming: A Data Quality Perspective to Enhancing
  Precision of RALMs","Retrieval-Augmented Large Language Models (RALMs) have made significant
strides in enhancing the accuracy of generated responses.However, existing
research often overlooks the data quality issues within retrieval results,
often caused by inaccurate existing vector-distance-based retrieval methods.We
propose to boost the precision of RALMs' answers from a data quality
perspective through the Context-Driven Index Trimming (CDIT) framework, where
Context Matching Dependencies (CMDs) are employed as logical data quality rules
to capture and regulate the consistency between retrieved contexts.Based on the
semantic comprehension capabilities of Large Language Models (LLMs), CDIT can
effectively identify and discard retrieval results that are inconsistent with
the query context and further modify indexes in the database, thereby improving
answer quality.Experiments demonstrate on challenging question-answering
tasks.Also, the flexibility of CDIT is verified through its compatibility with
various language models and indexing methods, which offers a promising approach
to bolster RALMs' data quality and retrieval precision jointly.","[{'name': 'Kexin Ma'}, {'name': 'Ruochun Jin'}, {'name': 'Xi Wang'}, {'name': 'Huan Chen'}, {'name': 'Jing Ren'}, {'name': 'Yuhua Tang'}]",2024-08-10T11:39:22Z
http://arxiv.org/abs/2408.05517v2,http://arxiv.org/abs/2408.05517v2,SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning,"Recent development in Large Language Models (LLMs) and Multi-modal Large
Language Models (MLLMs) have leverage Attention-based Transformer architectures
and achieved superior performance and generalization capabilities. They have
since covered extensive areas of traditional learning tasks. For instance,
text-based tasks such as text-classification and sequence-labeling, as well as
multi-modal tasks like Visual Question Answering (VQA) and Optical Character
Recognition (OCR), which were previously addressed using different models, can
now be tackled based on one foundation model. Consequently, the training and
lightweight fine-tuning of LLMs and MLLMs, especially those based on
Transformer architecture, has become particularly important. In recognition of
these overwhelming needs, we develop SWIFT, a customizable one-stop
infrastructure for large models. With support of over $300+$ LLMs and $50+$
MLLMs, SWIFT stands as the open-source framework that provide the \textit{most
comprehensive support} for fine-tuning large models. In particular, it is the
first training framework that provides systematic support for MLLMs. In
addition to the core functionalities of fine-tuning, SWIFT also integrates
post-training processes such as inference, evaluation, and model quantization,
to facilitate fast adoptions of large models in various application scenarios.
With a systematic integration of various training techniques, SWIFT offers
helpful utilities such as benchmark comparisons among different training
techniques for large models. For fine-tuning models specialized in agent
framework, we show that notable improvements on the ToolBench leader-board can
be achieved by training with customized dataset on SWIFT, with an increase of
5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in
hallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.","[{'name': 'Yuze Zhao'}, {'name': 'Jintao Huang'}, {'name': 'Jinghan Hu'}, {'name': 'Xingjun Wang'}, {'name': 'Yunlin Mao'}, {'name': 'Daoze Zhang'}, {'name': 'Zeyinzi Jiang'}, {'name': 'Zhikai Wu'}, {'name': 'Baole Ai'}, {'name': 'Ang Wang'}, {'name': 'Wenmeng Zhou'}, {'name': 'Yingda Chen'}]",2024-08-10T11:00:13Z
http://arxiv.org/abs/2408.05506v1,http://arxiv.org/abs/2408.05506v1,"Your Context Is Not an Array: Unveiling Random Access Limitations in
  Transformers","Despite their recent successes, Transformer-based large language models show
surprising failure modes. A well-known example of such failure modes is their
inability to length-generalize: solving problem instances at inference time
that are longer than those seen during training. In this work, we further
explore the root cause of this failure by performing a detailed analysis of
model behaviors on the simple parity task. Our analysis suggests that length
generalization failures are intricately related to a model's inability to
perform random memory accesses within its context window. We present supporting
evidence for this hypothesis by demonstrating the effectiveness of
methodologies that circumvent the need for indexing or that enable random token
access indirectly, through content-based addressing. We further show where and
how the failure to perform random memory access manifests through attention map
visualizations.","[{'name': 'MohammadReza Ebrahimi'}, {'name': 'Sunny Panchal'}, {'name': 'Roland Memisevic'}]",2024-08-10T10:12:09Z
http://arxiv.org/abs/2408.05502v1,http://arxiv.org/abs/2408.05502v1,"GEM: Context-Aware Gaze EstiMation with Visual Search Behavior Matching
  for Chest Radiograph","Gaze estimation is pivotal in human scene comprehension tasks, particularly
in medical diagnostic analysis. Eye-tracking technology facilitates the
recording of physicians' ocular movements during image interpretation, thereby
elucidating their visual attention patterns and information-processing
strategies. In this paper, we initially define the context-aware gaze
estimation problem in medical radiology report settings. To understand the
attention allocation and cognitive behavior of radiologists during the medical
image interpretation process, we propose a context-aware Gaze EstiMation (GEM)
network that utilizes eye gaze data collected from radiologists to simulate
their visual search behavior patterns throughout the image interpretation
process. It consists of a context-awareness module, visual behavior graph
construction, and visual behavior matching. Within the context-awareness
module, we achieve intricate multimodal registration by establishing
connections between medical reports and images. Subsequently, for a more
accurate simulation of genuine visual search behavior patterns, we introduce a
visual behavior graph structure, capturing such behavior through high-order
relationships (edges) between gaze points (nodes). To maintain the authenticity
of visual behavior, we devise a visual behavior-matching approach, adjusting
the high-order relationships between them by matching the graph constructed
from real and estimated gaze points. Extensive experiments on four publicly
available datasets demonstrate the superiority of GEM over existing methods and
its strong generalizability, which also provides a new direction for the
effective utilization of diverse modalities in medical image interpretation and
enhances the interpretability of models in the field of medical imaging.
https://github.com/Tiger-SN/GEM","[{'name': 'Shaonan Liu'}, {'name': 'Wenting Chen'}, {'name': 'Jie Liu'}, {'name': 'Xiaoling Luo'}, {'name': 'Linlin Shen'}]",2024-08-10T09:46:25Z
http://arxiv.org/abs/2408.05497v1,http://arxiv.org/abs/2408.05497v1,"MABR: A Multilayer Adversarial Bias Removal Approach Without Prior Bias
  Knowledge","Models trained on real-world data often mirror and exacerbate existing social
biases. Traditional methods for mitigating these biases typically require prior
knowledge of the specific biases to be addressed, such as gender or racial
biases, and the social groups associated with each instance. In this paper, we
introduce a novel adversarial training strategy that operates independently of
prior bias-type knowledge and protected attribute labels. Our approach
proactively identifies biases during model training by utilizing auxiliary
models, which are trained concurrently by predicting the performance of the
main model without relying on task labels. Additionally, we implement these
auxiliary models at various levels of the feature maps of the main model,
enabling the detection of a broader and more nuanced range of bias features.
Through experiments on racial and gender biases in sentiment and occupation
classification tasks, our method effectively reduces social biases without the
need for demographic annotations. Moreover, our approach not only matches but
often surpasses the efficacy of methods that require detailed demographic
insights, marking a significant advancement in bias mitigation techniques.","[{'name': 'Maxwell J. Yin'}, {'name': 'Boyu Wang'}, {'name': 'Charles Ling'}]",2024-08-10T09:11:01Z
http://arxiv.org/abs/2408.05457v1,http://arxiv.org/abs/2408.05457v1,Investigating Instruction Tuning Large Language Models on Graphs,"Inspired by the recent advancements of Large Language Models (LLMs) in NLP
tasks, there's growing interest in applying LLMs to graph-related tasks. This
study delves into the capabilities of instruction-following LLMs for engaging
with real-world graphs, aiming to offer empirical insights into how LLMs can
effectively interact with graphs and generalize across graph tasks. We begin by
constructing a dataset designed for instruction tuning, which comprises a
diverse collection of 79 graph-related tasks from academic and e-commerce
domains, featuring 44,240 training instances and 18,960 test samples. Utilizing
this benchmark, our initial investigation focuses on identifying the optimal
graph representation that serves as a conduit for LLMs to understand complex
graph structures. Our findings indicate that JSON format for graph
representation consistently outperforms natural language and code formats
across various LLMs and graph types. Furthermore, we examine the key factors
that influence the generalization abilities of instruction-tuned LLMs by
evaluating their performance on both in-domain and out-of-domain graph tasks.","[{'name': 'Kerui Zhu'}, {'name': 'Bo-Wei Huang'}, {'name': 'Bowen Jin'}, {'name': 'Yizhu Jiao'}, {'name': 'Ming Zhong'}, {'name': 'Kevin Chang'}, {'name': 'Shou-De Lin'}, {'name': 'Jiawei Han'}]",2024-08-10T06:54:35Z
http://arxiv.org/abs/2408.05456v1,http://arxiv.org/abs/2408.05456v1,"Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph
  Representation","Unified graph representation learning aims to produce node embeddings, which
can be applied to multiple downstream applications. However, existing studies
based on graph neural networks and language models either suffer from the
limitations of numerous training needed toward specific downstream predictions
or have shallow semantic features. In this work, we propose a novel Path-LLM
model to learn unified graph representation, which leverages a powerful large
language model (LLM) to incorporate our proposed path features. Our Path-LLM
framework consists of several well-designed techniques. First, we develop a new
mechanism of long-to-short shortest path (L2SP) selection, which covers
essential connections between different dense groups. An in-depth comparison of
different path selection plans is offered to illustrate the strength of our
designed L2SP. Then, we design path textualization to obtain L2SP-based
training texts. Next, we feed the texts into a self-supervised LLM training
process to learn embeddings. Extensive experiments on benchmarks validate the
superiority of Path-LLM against the state-of-the-art WalkLM method on two
classical graph learning tasks (node classification and link prediction) and
one NP-hard graph query processing task (keyword search), meanwhile saving more
than 90% of training paths.","[{'name': 'Wenbo Shang'}, {'name': 'Xuliang Zhu'}, {'name': 'Xin Huang'}]",2024-08-10T06:35:11Z
http://arxiv.org/abs/2408.05212v1,http://arxiv.org/abs/2408.05212v1,"Preserving Privacy in Large Language Models: A Survey on Current Threats
  and Solutions","Large Language Models (LLMs) represent a significant advancement in
artificial intelligence, finding applications across various domains. However,
their reliance on massive internet-sourced datasets for training brings notable
privacy issues, which are exacerbated in critical domains (e.g., healthcare).
Moreover, certain application-specific scenarios may require fine-tuning these
models on private data. This survey critically examines the privacy threats
associated with LLMs, emphasizing the potential for these models to memorize
and inadvertently reveal sensitive information. We explore current threats by
reviewing privacy attacks on LLMs and propose comprehensive solutions for
integrating privacy mechanisms throughout the entire learning pipeline. These
solutions range from anonymizing training datasets to implementing differential
privacy during training or inference and machine unlearning after training. Our
comprehensive review of existing literature highlights ongoing challenges,
available tools, and future directions for preserving privacy in LLMs. This
work aims to guide the development of more secure and trustworthy AI systems by
providing a thorough understanding of privacy preservation methods and their
effectiveness in mitigating risks.","[{'name': 'Michele Miranda'}, {'name': 'Elena Sofia Ruzzetti'}, {'name': 'Andrea Santilli'}, {'name': 'Fabio Massimo Zanzotto'}, {'name': 'Sébastien Bratières'}, {'name': 'Emanuele Rodolà'}]",2024-08-10T05:41:19Z
http://arxiv.org/abs/2408.05442v1,http://arxiv.org/abs/2408.05442v1,"Chain of Condition: Construct, Verify and Solve Conditions for
  Conditional Question Answering","Conditional question answering (CQA) is an important task that aims to find
probable answers and identify conditions that need to be satisfied to support
the answer. Existing approaches struggle with CQA due to two main challenges:
(1) precisely identifying conditions and their logical relationship, and (2)
verifying and solving the conditions. To address these challenges, we propose
Chain of Condition, a novel prompting approach by firstly identifying all
conditions and constructing their logical relationships explicitly according to
the document, then verifying whether these conditions are satisfied, finally
solving the logical expression by tools to indicate any missing conditions and
generating the answer based on the resolved conditions. The experiments on two
benchmark conditional question answering datasets shows chain of condition
outperforms existing prompting baselines, establishing a new state-of-the-art.
Furthermore, with backbone models like GPT-3.5-Turbo or GPT-4, it surpasses all
supervised baselines with only few-shot settings.","[{'name': 'Jiuheng Lin'}, {'name': 'Yuxuan Lai'}, {'name': 'Yansong Feng'}]",2024-08-10T05:09:11Z
http://arxiv.org/abs/2408.05404v1,http://arxiv.org/abs/2408.05404v1,"LaiDA: Linguistics-aware In-context Learning with Data Augmentation for
  Metaphor Components Identification","Metaphor Components Identification (MCI) contributes to enhancing machine
understanding of metaphors, thereby advancing downstream natural language
processing tasks. However, the complexity, diversity, and dependency on context
and background knowledge pose significant challenges for MCI. Large language
models (LLMs) offer new avenues for accurate comprehension of complex natural
language texts due to their strong semantic analysis and extensive commonsense
knowledge. In this research, a new LLM-based framework is proposed, named
Linguistics-aware In-context Learning with Data Augmentation (LaiDA).
Specifically, ChatGPT and supervised fine-tuning are utilized to tailor a
high-quality dataset. LaiDA incorporates a simile dataset for pre-training. A
graph attention network encoder generates linguistically rich feature
representations to retrieve similar examples. Subsequently, LLM is fine-tuned
with prompts that integrate linguistically similar examples. LaiDA ranked 2nd
in Subtask 2 of NLPCC2024 Shared Task 9, demonstrating its effectiveness. Code
and data are available at https://github.com/WXLJZ/LaiDA.","[{'name': 'Hongde Liu'}, {'name': 'Chenyuan He'}, {'name': 'Feiyang Meng'}, {'name': 'Changyong Niu'}, {'name': 'Yuxiang Jia'}]",2024-08-10T02:02:26Z
http://arxiv.org/abs/2408.05365v1,http://arxiv.org/abs/2408.05365v1,"FiST-Financial Style Transfer with Hallucination and Creativity Control
  Framework","Financial report generation using general purpose large language models pose
two major challenges, including the lack of compound sentences and
hallucinations. Advanced prompt engineering and retrieval augmented generation
(RAG) techniques are incapable of curing the writing style discrepancies. In
this work we propose a novel two-stage fine-tuning process wherein public
domain financial reports are processed into prompt-completions and augmented
using simple LLM prompts to then enable sectional financial report generation
using minimal instructions and tabular data inputs. Our proposed fine-tuning
framework results doubles the number of correct questions answers and reduces
hallucinations by over 50%. Additionally, the two-stage fine tuned models have
lower perplexity, improved ROUGE, TER and BLEU scores, higher creativity and
knowledge density with lower uncertainty and cross entropy.","[{'name': 'Sohini Roychowdhury'}, {'name': 'Marko Krema'}, {'name': 'Brian Moore'}, {'name': 'Xingjian Lai'}, {'name': 'Dike Effedua'}, {'name': 'Bharat Jethwani'}]",2024-08-09T22:29:23Z
http://arxiv.org/abs/2408.05346v2,http://arxiv.org/abs/2408.05346v2,"DataNarrative: Automated Data-Driven Storytelling with Visualizations
  and Texts","Data-driven storytelling is a powerful method for conveying insights by
combining narrative techniques with visualizations and text. These stories
integrate visual aids, such as highlighted bars and lines in charts, along with
textual annotations explaining insights. However, creating such stories
requires a deep understanding of the data and meticulous narrative planning,
often necessitating human intervention, which can be time-consuming and
mentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,
their ability to generate coherent and comprehensive data stories remains
underexplored. In this work, we introduce a novel task for data story
generation and a benchmark containing 1,449 stories from diverse sources. To
address the challenges of crafting coherent data stories, we propose a
multiagent framework employing two LLM agents designed to replicate the human
storytelling process: one for understanding and describing the data
(Reflection), generating the outline, and narration, and another for
verification at each intermediary step. While our agentic framework generally
outperforms non-agentic counterparts in both model-based and human evaluations,
the results also reveal unique challenges in data story generation.","[{'name': 'Mohammed Saidul Islam'}, {'name': 'Md Tahmid Rahman Laskar'}, {'name': 'Md Rizwan Parvez'}, {'name': 'Enamul Hoque'}, {'name': 'Shafiq Joty'}]",2024-08-09T21:31:33Z
http://arxiv.org/abs/2408.05334v1,http://arxiv.org/abs/2408.05334v1,Revisiting Multi-Modal LLM Evaluation,"With the advent of multi-modal large language models (MLLMs), datasets used
for visual question answering (VQA) and referring expression comprehension have
seen a resurgence. However, the most popular datasets used to evaluate MLLMs
are some of the earliest ones created, and they have many known problems,
including extreme bias, spurious correlations, and an inability to permit
fine-grained analysis. In this paper, we pioneer evaluating recent MLLMs (LLaVA
1.5, LLaVA-NeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-4o) on datasets designed
to address weaknesses in earlier ones. We assess three VQA datasets: 1) TDIUC,
which permits fine-grained analysis on 12 question types; 2) TallyQA, which has
simple and complex counting questions; and 3) DVQA, which requires optical
character recognition for chart understanding. We also study VQDv1, a dataset
that requires identifying all image regions that satisfy a given query. Our
experiments reveal the weaknesses of many MLLMs that have not previously been
reported. Our code is integrated into the widely used LAVIS framework for MLLM
evaluation, enabling the rapid assessment of future MLLMs. Project webpage:
https://kevinlujian.github.io/MLLM_Evaluations/","[{'name': 'Jian Lu'}, {'name': 'Shikhar Srivastava'}, {'name': 'Junyu Chen'}, {'name': 'Robik Shrestha'}, {'name': 'Manoj Acharya'}, {'name': 'Kushal Kafle'}, {'name': 'Christopher Kanan'}]",2024-08-09T20:55:46Z
http://arxiv.org/abs/2408.05328v1,http://arxiv.org/abs/2408.05328v1,"From Text to Insight: Leveraging Large Language Models for Performance
  Evaluation in Management","This study explores the potential of Large Language Models (LLMs),
specifically GPT-4, to enhance objectivity in organizational task performance
evaluations. Through comparative analyses across two studies, including various
task performance outputs, we demonstrate that LLMs can serve as a reliable and
even superior alternative to human raters in evaluating knowledge-based
performance outputs, which are a key contribution of knowledge workers. Our
results suggest that GPT ratings are comparable to human ratings but exhibit
higher consistency and reliability. Additionally, combined multiple GPT ratings
on the same performance output show strong correlations with aggregated human
performance ratings, akin to the consensus principle observed in performance
evaluation literature. However, we also find that LLMs are prone to contextual
biases, such as the halo effect, mirroring human evaluative biases. Our
research suggests that while LLMs are capable of extracting meaningful
constructs from text-based data, their scope is currently limited to specific
forms of performance evaluation. By highlighting both the potential and
limitations of LLMs, our study contributes to the discourse on AI role in
management studies and sets a foundation for future research to refine AI
theoretical and practical applications in management.","[{'name': 'Ning Li'}, {'name': 'Huaikang Zhou'}, {'name': 'Mingze Xu'}]",2024-08-09T20:35:10Z
http://arxiv.org/abs/2408.05326v1,http://arxiv.org/abs/2408.05326v1,A Psychology-based Unified Dynamic Framework for Curriculum Learning,"Directly learning from examples of random difficulty levels is often
challenging for both humans and machine learning models. A more effective
strategy involves exposing learners to examples in a progressive order, from
easy to difficult. Curriculum Learning (CL) has been proposed to implement this
strategy in machine learning model training. However, two key challenges
persist in CL framework design: defining the difficulty of training data and
determining the appropriate amount of data to input at each training step. This
paper presents a Psychology-based Unified Dynamic Framework for Curriculum
Learning (PUDF), drawing inspiration from psychometrics. We quantify the
difficulty of training data by applying Item Response Theory (IRT) to responses
from Artificial Crowds (AC). This theory-driven IRT-AC approach leads to global
(i.e., model-independent) and interpretable difficulty values. Leveraging IRT,
we propose a Dynamic Data Selection via Model Ability Estimation (DDS-MAE)
strategy to schedule the appropriate amount of data during model training.
Since our difficulty labeling and model ability estimation are based on a
consistent theory, namely IRT, their values are comparable within the same
scope, potentially leading to a faster convergence compared to the other CL
methods. Experimental results demonstrate that fine-tuning pre-trained language
models with PUDF enhances their performance on the GLUE benchmark. Moreover,
PUDF surpasses other state-of-the-art (SOTA) CL methods on the GLUE benchmark.
We further explore the components of PUDF, namely the difficulty measurer
(IRT-AC) and the training scheduler (DDS-MAE) qualitatively and quantitatively.
Lastly, we conduct an ablation study to clarify which components of PUDF
contribute to faster convergence and higher accuracy.","[{'name': 'Guangyu Meng'}, {'name': 'Qingkai Zeng'}, {'name': 'John P. Lalor'}, {'name': 'Hong Yu'}]",2024-08-09T20:30:37Z
http://arxiv.org/abs/2408.05283v1,http://arxiv.org/abs/2408.05283v1,"MUSE: Multi-Knowledge Passing on the Edges, Boosting Knowledge Graph
  Completion","Knowledge Graph Completion (KGC) aims to predict the missing information in
the (head entity)-[relation]-(tail entity) triplet. Deep Neural Networks have
achieved significant progress in the relation prediction task. However, most
existing KGC methods focus on single features (e.g., entity IDs) and sub-graph
aggregation, which cannot fully explore all the features in the Knowledge Graph
(KG), and neglect the external semantic knowledge injection. To address these
problems, we propose MUSE, a knowledge-aware reasoning model to learn a
tailored embedding space in three dimensions for missing relation prediction
through a multi-knowledge representation learning mechanism. Our MUSE consists
of three parallel components: 1) Prior Knowledge Learning for enhancing the
triplets' semantic representation by fine-tuning BERT; 2) Context Message
Passing for enhancing the context messages of KG; 3) Relational Path
Aggregation for enhancing the path representation from the head entity to the
tail entity. Our experimental results show that MUSE significantly outperforms
other baselines on four public datasets, such as over 5.50% improvement in H@1
and 4.20% improvement in MRR on the NELL995 dataset. The code and all datasets
will be released via https://github.com/NxxTGT/MUSE.",[{'name': 'Pengjie Liu'}],2024-08-09T18:10:02Z
http://arxiv.org/abs/2408.05211v1,http://arxiv.org/abs/2408.05211v1,VITA: Towards Open-Source Interactive Omni Multimodal LLM,"The remarkable multimodal capabilities and interactive experience of GPT-4o
underscore their necessity in practical applications, yet open-source models
rarely excel in both areas. In this paper, we introduce VITA, the first-ever
open-source Multimodal Large Language Model (MLLM) adept at simultaneous
processing and analysis of Video, Image, Text, and Audio modalities, and
meanwhile has an advanced multimodal interactive experience. Starting from
Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary
followed by bilingual instruction tuning. We further endow the language model
with visual and audio capabilities through two-stage multi-task learning of
multimodal alignment and instruction tuning. VITA demonstrates robust
foundational capabilities of multilingual, vision, and audio understanding, as
evidenced by its strong performance across a range of both unimodal and
multimodal benchmarks. Beyond foundational capabilities, we have made
considerable progress in enhancing the natural multimodal human-computer
interaction experience. To the best of our knowledge, we are the first to
exploit non-awakening interaction and audio interrupt in MLLM. VITA is the
first step for the open-source community to explore the seamless integration of
multimodal understanding and interaction. While there is still lots of work to
be done on VITA to get close to close-source counterparts, we hope that its
role as a pioneer can serve as a cornerstone for subsequent research. Project
Page: https://vita-home.github.io.","[{'name': 'Chaoyou Fu'}, {'name': 'Haojia Lin'}, {'name': 'Zuwei Long'}, {'name': 'Yunhang Shen'}, {'name': 'Meng Zhao'}, {'name': 'Yifan Zhang'}, {'name': 'Xiong Wang'}, {'name': 'Di Yin'}, {'name': 'Long Ma'}, {'name': 'Xiawu Zheng'}, {'name': 'Ran He'}, {'name': 'Rongrong Ji'}, {'name': 'Yunsheng Wu'}, {'name': 'Caifeng Shan'}, {'name': 'Xing Sun'}]",2024-08-09T17:59:49Z
http://arxiv.org/abs/2408.05204v1,http://arxiv.org/abs/2408.05204v1,"Evaluating the capability of large language models to personalize
  science texts for diverse middle-school-age learners","Large language models (LLMs), including OpenAI's GPT-series, have made
significant advancements in recent years. Known for their expertise across
diverse subject areas and quick adaptability to user-provided prompts, LLMs
hold unique potential as Personalized Learning (PL) tools. Despite this
potential, their application in K-12 education remains largely unexplored. This
paper presents one of the first randomized controlled trials (n = 23) to
evaluate the effectiveness of GPT-4 in personalizing educational science texts
for middle school students. In this study, GPT-4 was used to profile student
learning preferences based on choices made during a training session. For the
experimental group, GPT-4 was used to rewrite science texts to align with the
student's predicted profile while, for students in the control group, texts
were rewritten to contradict their learning preferences. The results of a
Mann-Whitney U test showed that students significantly preferred (at the .10
level) the rewritten texts when they were aligned with their profile (p =
.059). These findings suggest that GPT-4 can effectively interpret and tailor
educational content to diverse learner preferences, marking a significant
advancement in PL technology. The limitations of this study and ethical
considerations for using artificial intelligence in education are also
discussed.","[{'name': 'Michael Vaccaro Jr'}, {'name': 'Mikayla Friday'}, {'name': 'Arash Zaghi'}]",2024-08-09T17:53:35Z
http://arxiv.org/abs/2408.05200v1,http://arxiv.org/abs/2408.05200v1,"TaSL: Task Skill Localization and Consolidation for Language Model
  Continual Learning","Language model continual learning (CL) has recently garnered significant
interest due to its potential to adapt large language models (LLMs) to dynamic
real-world environments without re-training. A key challenge in this field is
catastrophic forgetting, where models lose previously acquired knowledge when
learning new tasks. Existing methods commonly employ multiple
parameter-efficient fine-tuning (PEFT) blocks to acquire task-specific
knowledge for each task, but these approaches lack efficiency and overlook the
potential for knowledge transfer through task interaction. In this paper, we
present a novel CL framework for language models called Task Skill Localization
and Consolidation (TaSL), which enhances knowledge transfer without relying on
memory replay. TaSL first divides the model into `skill units' based on
parameter dependencies, enabling more granular control. It then employs a novel
group-wise skill localization technique to identify the importance distribution
of skill units for a new task. By comparing this importance distribution with
those from previous tasks, we implement a fine-grained skill consolidation
strategy that retains task-specific knowledge, thereby preventing forgetting,
and updates task-shared knowledge, which facilitates bi-directional knowledge
transfer. As a result, TaSL achieves a superior balance between retaining
previous knowledge and excelling in new tasks. TaSL also shows strong
generalizability, suitable for general models and customizable for PEFT methods
like LoRA. Additionally, it demonstrates notable extensibility, allowing
integration with memory replay to further enhance performance. Extensive
experiments on two CL benchmarks, with varying model sizes (from 220M to 7B),
demonstrate the effectiveness of TaSL and its variants across different
settings.","[{'name': 'Yujie Feng'}, {'name': 'Xu Chu'}, {'name': 'Yongxin Xu'}, {'name': 'Zexin Lu'}, {'name': 'Bo Liu'}, {'name': 'Philip S. Yu'}, {'name': 'Xiao-Ming Wu'}]",2024-08-09T17:44:45Z
http://arxiv.org/abs/2408.05192v1,http://arxiv.org/abs/2408.05192v1,"Separating Style from Substance: Enhancing Cross-Genre Authorship
  Attribution through Data Selection and Presentation","The task of deciding whether two documents are written by the same author is
challenging for both machines and humans. This task is even more challenging
when the two documents are written about different topics (e.g. baseball vs.
politics) or in different genres (e.g. a blog post vs. an academic article).
For machines, the problem is complicated by the relative lack of real-world
training examples that cross the topic boundary and the vanishing scarcity of
cross-genre data. We propose targeted methods for training data selection and a
novel learning curriculum that are designed to discourage a model's reliance on
topic information for authorship attribution and correspondingly force it to
incorporate information more robustly indicative of style no matter the topic.
These refinements yield a 62.7% relative improvement in average cross-genre
authorship attribution, as well as 16.6% in the per-genre condition.","[{'name': 'Steven Fincke'}, {'name': 'Elizabeth Boschee'}]",2024-08-09T17:31:37Z
http://arxiv.org/abs/2408.05184v1,http://arxiv.org/abs/2408.05184v1,"Deep-change at AXOLOTL-24: Orchestrating WSD and WSI Models for Semantic
  Change Modeling","This paper describes our solution of the first subtask from the AXOLOTL-24
shared task on Semantic Change Modeling. The goal of this subtask is to
distribute a given set of usages of a polysemous word from a newer time period
between senses of this word from an older time period and clusters representing
gained senses of this word. We propose and experiment with three new methods
solving this task. Our methods achieve SOTA results according to both official
metrics of the first substask. Additionally, we develop a model that can tell
if a given word usage is not described by any of the provided sense
definitions. This model serves as a component in one of our methods, but can
potentially be useful on its own.","[{'name': 'Denis Kokosinskii'}, {'name': 'Mikhail Kuklin'}, {'name': 'Nikolay Arefyev'}]",2024-08-09T17:15:54Z
http://arxiv.org/abs/2408.05147v1,http://arxiv.org/abs/2408.05147v1,Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2,"Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse
decomposition of a neural network's latent representations into seemingly
interpretable features. Despite recent excitement about their potential,
research applications outside of industry are limited by the high cost of
training a comprehensive suite of SAEs. In this work, we introduce Gemma Scope,
an open suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2
2B and 9B and select layers of Gemma 2 27B base models. We primarily train SAEs
on the Gemma 2 pre-trained models, but additionally release SAEs trained on
instruction-tuned Gemma 2 9B for comparison. We evaluate the quality of each
SAE on standard metrics and release these results. We hope that by releasing
these SAE weights, we can help make more ambitious safety and interpretability
research easier for the community. Weights and a tutorial can be found at
https://huggingface.co/google/gemma-scope and an interactive demo can be found
at https://www.neuronpedia.org/gemma-scope","[{'name': 'Tom Lieberum'}, {'name': 'Senthooran Rajamanoharan'}, {'name': 'Arthur Conmy'}, {'name': 'Lewis Smith'}, {'name': 'Nicolas Sonnerat'}, {'name': 'Vikrant Varma'}, {'name': 'János Kramár'}, {'name': 'Anca Dragan'}, {'name': 'Rohin Shah'}, {'name': 'Neel Nanda'}]",2024-08-09T16:06:42Z
http://arxiv.org/abs/2408.05141v1,http://arxiv.org/abs/2408.05141v1,A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning,"Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.","[{'name': 'Ye Yuan'}, {'name': 'Chengwu Liu'}, {'name': 'Jingyang Yuan'}, {'name': 'Gongbo Sun'}, {'name': 'Siqi Li'}, {'name': 'Ming Zhang'}]",2024-08-09T15:53:55Z
http://arxiv.org/abs/2408.05126v1,http://arxiv.org/abs/2408.05126v1,"Large Language Models and Thematic Analysis: Human-AI Synergy in
  Researching Hate Speech on Social Media","In the dynamic field of artificial intelligence (AI), the development and
application of Large Language Models (LLMs) for text analysis are of
significant academic interest. Despite the promising capabilities of various
LLMs in conducting qualitative analysis, their use in the humanities and social
sciences has not been thoroughly examined. This article contributes to the
emerging literature on LLMs in qualitative analysis by documenting an
experimental study involving GPT-4. The study focuses on performing thematic
analysis (TA) using a YouTube dataset derived from an EU-funded project, which
was previously analyzed by other researchers. This dataset is about the
representation of Roma migrants in Sweden during 2016, a period marked by the
aftermath of the 2015 refugee crisis and preceding the Swedish national
elections in 2017. Our study seeks to understand the potential of combining
human intelligence with AI's scalability and efficiency, examining the
advantages and limitations of employing LLMs in qualitative research within the
humanities and social sciences. Additionally, we discuss future directions for
applying LLMs in these fields.","[{'name': 'Petre Breazu'}, {'name': 'Miriam Schirmer'}, {'name': 'Songbo Hu'}, {'name': 'Napoleon Kastos'}]",2024-08-09T15:34:41Z
http://arxiv.org/abs/2408.07091v1,http://arxiv.org/abs/2408.07091v1,"Node Level Graph Autoencoder: Unified Pretraining for Textual Graph
  Learning","Textual graphs are ubiquitous in real-world applications, featuring rich text
information with complex relationships, which enables advanced research across
various fields. Textual graph representation learning aims to generate
low-dimensional feature embeddings from textual graphs that can improve the
performance of downstream tasks. A high-quality feature embedding should
effectively capture both the structural and the textual information in a
textual graph. However, most textual graph dataset benchmarks rely on word2vec
techniques to generate feature embeddings, which inherently limits their
capabilities. Recent works on textual graph representation learning can be
categorized into two folds: supervised and unsupervised methods. Supervised
methods finetune a language model on labeled nodes, which have limited
capabilities when labeled data is scarce. Unsupervised methods, on the other
hand, extract feature embeddings by developing complex training pipelines. To
address these limitations, we propose a novel unified unsupervised learning
autoencoder framework, named Node Level Graph AutoEncoder (NodeGAE). We employ
language models as the backbone of the autoencoder, with pretraining on text
reconstruction. Additionally, we add an auxiliary loss term to make the feature
embeddings aware of the local graph structure. Our method maintains simplicity
in the training process and demonstrates generalizability across diverse
textual graphs and downstream tasks. We evaluate our method on two core graph
representation learning downstream tasks: node classification and link
prediction. Comprehensive experiments demonstrate that our approach
substantially enhances the performance of diverse graph neural networks (GNNs)
across multiple textual graph datasets.","[{'name': 'Wenbin Hu'}, {'name': 'Huihao Jing'}, {'name': 'Qi Hu'}, {'name': 'Haoran Li'}, {'name': 'Yangqiu Song'}]",2024-08-09T14:57:53Z
http://arxiv.org/abs/2408.05102v1,http://arxiv.org/abs/2408.05102v1,How Well Do LLMs Identify Cultural Unity in Diversity?,"Much work on the cultural awareness of large language models (LLMs) focuses
on the models' sensitivity to geo-cultural diversity. However, in addition to
cross-cultural differences, there also exists common ground across cultures.
For instance, a bridal veil in the United States plays a similar
cultural-relevant role as a honggaitou in China. In this study, we introduce a
benchmark dataset CUNIT for evaluating decoder-only LLMs in understanding the
cultural unity of concepts. Specifically, CUNIT consists of 1,425 evaluation
examples building upon 285 traditional cultural-specific concepts across 10
countries. Based on a systematic manual annotation of cultural-relevant
features per concept, we calculate the cultural association between any pair of
cross-cultural concepts. Built upon this dataset, we design a contrastive
matching task to evaluate the LLMs' capability to identify highly associated
cross-cultural concept pairs. We evaluate 3 strong LLMs, using 3 popular
prompting strategies, under the settings of either giving all extracted concept
features or no features at all on CUNIT Interestingly, we find that cultural
associations across countries regarding clothing concepts largely differ from
food. Our analysis shows that LLMs are still limited to capturing
cross-cultural associations between concepts compared to humans. Moreover,
geo-cultural proximity shows a weak influence on model performance in capturing
cross-cultural associations.","[{'name': 'Jialin Li'}, {'name': 'Junli Wang'}, {'name': 'Junjie Hu'}, {'name': 'Ming Jiang'}]",2024-08-09T14:45:22Z
http://arxiv.org/abs/2408.05101v1,http://arxiv.org/abs/2408.05101v1,"MooER: LLM-based Speech Recognition and Translation Models from Moore
  Threads","In this paper, we present MooER, a LLM-based large-scale automatic speech
recognition (ASR) / automatic speech translation (AST) model of Moore Threads.
A 5000h pseudo labeled dataset containing open source and self collected speech
data is used for training. We achieve performance comparable to other open
source models trained with up to hundreds of thousands of hours of labeled
speech data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest
that our model outperforms other open source Speech LLMs. A BLEU score of 25.2
can be obtained. The main contributions of this paper are summarized as
follows. First, this paper presents a training strategy for encoders and LLMs
on speech related tasks (including ASR and AST) using a small size of pseudo
labeled data without any extra manual annotation and selection. Second, we
release our ASR and AST models and plan to open-source our training code and
strategy in the near future. Moreover, a model trained on 8wh scale training
data is planned to be released later on.","[{'name': 'Junhao Xu'}, {'name': 'Zhenlin Liang'}, {'name': 'Yi Liu'}, {'name': 'Yichao Hu'}, {'name': 'Jian Li'}, {'name': 'Yajun Zheng'}, {'name': 'Meng Cai'}, {'name': 'Hua Wang'}]",2024-08-09T14:43:56Z
http://arxiv.org/abs/2408.05094v1,http://arxiv.org/abs/2408.05094v1,"Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective
  Alignment with Contrastive Prompts","The task of multi-objective alignment aims at balancing and controlling the
different alignment objectives (e.g., helpfulness, harmlessness and honesty) of
large language models to meet the personalized requirements of different users.
However, previous methods tend to train multiple models to deal with various
user preferences, with the number of trained models growing linearly with the
number of alignment objectives and the number of different preferences.
Meanwhile, existing methods are generally poor in extensibility and require
significant re-training for each new alignment objective considered.
Considering the limitation of previous approaches, we propose MCA
(Multi-objective Contrastive Alignemnt), which constructs an expert prompt and
an adversarial prompt for each objective to contrast at the decoding time and
balances the objectives through combining the contrast. Our approach is
verified to be superior to previous methods in obtaining a well-distributed
Pareto front among different alignment objectives.","[{'name': 'Tingchen Fu'}, {'name': 'Yupeng Hou'}, {'name': 'Julian McAuley'}, {'name': 'Rui Yan'}]",2024-08-09T14:36:42Z
http://arxiv.org/abs/2408.05093v1,http://arxiv.org/abs/2408.05093v1,"Order Matters in Hallucination: Reasoning Order as Benchmark and
  Reflexive Prompting for Large-Language-Models","Large language models (LLMs) have generated significant attention since their
inception, finding applications across various academic and industrial domains.
However, these models often suffer from the ""hallucination problem"", where
outputs, though grammatically and logically coherent, lack factual accuracy or
are entirely fabricated. A particularly troubling issue discovered and widely
discussed recently is the numerical comparison error where multiple LLMs
incorrectly infer that ""9.11$>$9.9"". We discovered that the order in which LLMs
generate answers and reasoning impacts their consistency. Specifically, results
vary significantly when an LLM generates an answer first and then provides the
reasoning versus generating the reasoning process first and then the
conclusion. Inspired by this, we propose a new benchmark method for assessing
LLM consistency: comparing responses generated through these two different
approaches. This benchmark effectively identifies instances where LLMs
fabricate answers and subsequently generate justifications. Furthermore, we
introduce a novel and straightforward prompt strategy designed to mitigate this
issue. Experimental results demonstrate that this strategy improves performance
across various LLMs compared to direct questioning. This work not only sheds
light on a critical flaw in LLMs but also offers a practical solution to
enhance their reliability.",[{'name': 'Zikai Xie'}],2024-08-09T14:34:32Z
http://arxiv.org/abs/2408.05086v1,http://arxiv.org/abs/2408.05086v1,"Generating novel experimental hypotheses from language models: A case
  study on cross-dative generalization","Neural network language models (LMs) have been shown to successfully capture
complex linguistic knowledge. However, their utility for understanding language
acquisition is still debated. We contribute to this debate by presenting a case
study where we use LMs as simulated learners to derive novel experimental
hypotheses to be tested with humans. We apply this paradigm to study
cross-dative generalization (CDG): productive generalization of novel verbs
across dative constructions (she pilked me the ball/she pilked the ball to me)
-- acquisition of which is known to involve a large space of contextual
features -- using LMs trained on child-directed speech. We specifically ask:
""what properties of the training exposure facilitate a novel verb's
generalization to the (unmodeled) alternate construction?"" To answer this, we
systematically vary the exposure context in which a novel dative verb occurs in
terms of the properties of the theme and recipient, and then analyze the LMs'
usage of the novel verb in the unmodeled dative construction. We find LMs to
replicate known patterns of children's CDG, as a precondition to exploring
novel hypotheses. Subsequent simulations reveal a nuanced role of the features
of the novel verbs' exposure context on the LMs' CDG. We find CDG to be
facilitated when the first postverbal argument of the exposure context is
pronominal, definite, short, and conforms to the prototypical animacy
expectations of the exposure dative. These patterns are characteristic of
harmonic alignment in datives, where the argument with features ranking higher
on the discourse prominence scale tends to precede the other. This gives rise
to a novel hypothesis that CDG is facilitated insofar as the features of the
exposure context -- in particular, its first postverbal argument -- are
harmonically aligned. We conclude by proposing future experiments that can test
this hypothesis in children.","[{'name': 'Kanishka Misra'}, {'name': 'Najoung Kim'}]",2024-08-09T14:17:36Z
http://arxiv.org/abs/2408.05074v1,http://arxiv.org/abs/2408.05074v1,"RT-Surv: Improving Mortality Prediction After Radiotherapy with Large
  Language Model Structuring of Large-Scale Unstructured Electronic Health
  Records","Accurate patient selection is critical in radiotherapy (RT) to prevent
ineffective treatments. Traditional survival prediction models, relying on
structured data, often lack precision. This study explores the potential of
large language models (LLMs) to structure unstructured electronic health record
(EHR) data, thereby improving survival prediction accuracy through
comprehensive clinical information integration. Data from 34,276 patients
treated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,
encompassing both structured and unstructured data. An open-source LLM was used
to structure the unstructured EHR data via single-shot learning, with its
performance compared against a domain-specific medical LLM and a smaller
variant. Survival prediction models were developed using statistical, machine
learning, and deep learning approaches, incorporating both structured and
LLM-structured data. Clinical experts evaluated the accuracy of the
LLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring
unstructured EHR data without additional training, significantly outperforming
the domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs
were more effective, particularly in extracting clinically relevant features
like general condition and disease extent, which closely correlated with
patient survival. Incorporating LLM-structured clinical features into survival
prediction models significantly improved accuracy, with the C-index of deep
learning models increasing from 0.737 to 0.820. These models also became more
interpretable by emphasizing clinically significant factors. This study shows
that general-domain LLMs, even without specific medical training, can
effectively structure large-scale unstructured EHR data, substantially
enhancing the accuracy and interpretability of clinical predictive models.","[{'name': 'Sangjoon Park'}, {'name': 'Chan Woo Wee'}, {'name': 'Seo Hee Choi'}, {'name': 'Kyung Hwan Kim'}, {'name': 'Jee Suk Chang'}, {'name': 'Hong In Yoon'}, {'name': 'Ik Jae Lee'}, {'name': 'Yong Bae Kim'}, {'name': 'Jaeho Cho'}, {'name': 'Ki Chang Keum'}, {'name': 'Chang Geol Lee'}, {'name': 'Hwa Kyung Byun'}, {'name': 'Woong Sub Koom'}]",2024-08-09T14:02:24Z
http://arxiv.org/abs/2408.05035v1,http://arxiv.org/abs/2408.05035v1,"Examining the Behavior of LLM Architectures Within the Framework of
  Standardized National Exams in Brazil","The Exame Nacional do Ensino M\'edio (ENEM) is a pivotal test for Brazilian
students, required for admission to a significant number of universities in
Brazil. The test consists of four objective high-school level tests on Math,
Humanities, Natural Sciences and Languages, and one writing essay. Students'
answers to the test and to the accompanying socioeconomic status questionnaire
are made public every year (albeit anonymized) due to transparency policies
from the Brazilian Government. In the context of large language models (LLMs),
these data lend themselves nicely to comparing different groups of humans with
AI, as we can have access to human and machine answer distributions. We
leverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4,
and MariTalk, a model trained using Portuguese data, to humans, aiming to
ascertain how their answers relate to real societal groups and what that may
reveal about the model biases. We divide the human groups by using
socioeconomic status (SES), and compare their answer distribution with LLMs for
each question and for the essay. We find no significant biases when comparing
LLM performance to humans on the multiple-choice Brazilian Portuguese tests, as
the distance between model and human answers is mostly determined by the human
accuracy. A similar conclusion is found by looking at the generated text as,
when analyzing the essays, we observe that human and LLM essays differ in a few
key factors, one being the choice of words where model essays were easily
separable from human ones. The texts also differ syntactically, with LLM
generated essays exhibiting, on average, smaller sentences and less thought
units, among other differences. These results suggest that, for Brazilian
Portuguese in the ENEM context, LLM outputs represent no group of humans, being
significantly different from the answers from Brazilian students across all
tests.","[{'name': 'Marcelo Sartori Locatelli'}, {'name': 'Matheus Prado Miranda'}, {'name': 'Igor Joaquim da Silva Costa'}, {'name': 'Matheus Torres Prates'}, {'name': 'Victor Thomé'}, {'name': 'Mateus Zaparoli Monteiro'}, {'name': 'Tomas Lacerda'}, {'name': 'Adriana Pagano'}, {'name': 'Eduardo Rios Neto'}, {'name': 'Wagner Meira Jr.'}, {'name': 'Virgilio Almeida'}]",2024-08-09T12:47:28Z
http://arxiv.org/abs/2408.05024v1,http://arxiv.org/abs/2408.05024v1,MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling,"Guitar tablatures enrich the structure of traditional music notation by
assigning each note to a string and fret of a guitar in a particular tuning,
indicating precisely where to play the note on the instrument. The problem of
generating tablature from a symbolic music representation involves inferring
this string and fret assignment per note across an entire composition or
performance. On the guitar, multiple string-fret assignments are possible for
most pitches, which leads to a large combinatorial space that prevents
exhaustive search approaches. Most modern methods use constraint-based dynamic
programming to minimize some cost function (e.g.\ hand position movement). In
this work, we introduce a novel deep learning solution to symbolic guitar
tablature estimation. We train an encoder-decoder Transformer model in a masked
language modeling paradigm to assign notes to strings. The model is first
pre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on
a curated set of professionally transcribed guitar performances. Given the
subjective nature of assessing tablature quality, we conduct a user study
amongst guitarists, wherein we ask participants to rate the playability of
multiple versions of tablature for the same four-bar excerpt. The results
indicate our system significantly outperforms competing algorithms.","[{'name': 'Drew Edwards'}, {'name': 'Xavier Riley'}, {'name': 'Pedro Sarmento'}, {'name': 'Simon Dixon'}]",2024-08-09T12:25:23Z
http://arxiv.org/abs/2408.04983v1,http://arxiv.org/abs/2408.04983v1,"Get Confused Cautiously: Textual Sequence Memorization Erasure with
  Selective Entropy Maximization","Large Language Models (LLMs) have been found to memorize and recite some of
the textual sequences from their training set verbatim, raising broad concerns
about privacy and copyright issues when using LLMs. This Textual Sequence
Memorization (TSM) phenomenon leads to a high demand to regulate LLM output to
prevent it from generating certain memorized text to meet user requirements.
However, our empirical study reveals that existing methods for TSM erasure fail
to forget massive memorized samples without substantially jeopardizing the
model utility. To achieve a better trade-off between the effectiveness of TSM
erasure and model utility in LLMs, our paper proposes a new framework based on
Entropy Maximization with Selective Optimization (EMSO), where the updated
weights are chosen with a novel contrastive gradient metric without any
participation of additional model or data. Our analysis shows that training
with the entropy maximization loss has a more stable optimization process and
better keeps model utility than existing methods. The contrastive gradient
metric localizes the most influential weight for TSM erasure by taking both the
gradient magnitude and direction into consideration. Extensive experiments
across three model scales demonstrate that our method excels in handling
large-scale forgetting requests while preserving model ability in language
generation and reasoning.","[{'name': 'Zhaohan Zhang'}, {'name': 'Ziquan Liu'}, {'name': 'Ioannis Patras'}]",2024-08-09T10:26:11Z
http://arxiv.org/abs/2408.04975v3,http://arxiv.org/abs/2408.04975v3,"\textit{re}CSE: Portable Reshaping Features for Sentence Embedding in
  Self-supervised Contrastive Learning","We propose \textit{re}CSE, a self supervised contrastive learning sentence
representation framework based on feature reshaping. This framework is
different from the current advanced models that use discrete data augmentation
methods, but instead reshapes the input features of the original sentence,
aggregates the global information of each token in the sentence, and alleviates
the common problems of representation polarity and GPU memory consumption
linear increase in current advanced models. In addition, our \textit{re}CSE has
achieved competitive performance in semantic similarity tasks. And the
experiment proves that our proposed feature reshaping method has strong
universality, which can be transplanted to other self supervised contrastive
learning frameworks and enhance their representation ability, even achieving
state-of-the-art performance. Our code is available at
https://github.com/heavenhellchen/reCSE.","[{'name': 'Fufangchen Zhao'}, {'name': 'Jian Gao'}, {'name': 'Danfeng Yan'}]",2024-08-09T09:56:30Z
http://arxiv.org/abs/2408.04965v1,http://arxiv.org/abs/2408.04965v1,"Generalisation First, Memorisation Second? Memorisation Localisation for
  Natural Language Classification Tasks","Memorisation is a natural part of learning from real-world data: neural
models pick up on atypical input-output combinations and store those training
examples in their parameter space. That this happens is well-known, but how and
where are questions that remain largely unanswered. Given a multi-layered
neural model, where does memorisation occur in the millions of parameters?
Related work reports conflicting findings: a dominant hypothesis based on image
classification is that lower layers learn generalisable features and that
deeper layers specialise and memorise. Work from NLP suggests this does not
apply to language models, but has been mainly focused on memorisation of facts.
We expand the scope of the localisation question to 12 natural language
classification tasks and apply 4 memorisation localisation techniques. Our
results indicate that memorisation is a gradual process rather than a localised
one, establish that memorisation is task-dependent, and give nuance to the
generalisation first, memorisation second hypothesis.","[{'name': 'Verna Dankers'}, {'name': 'Ivan Titov'}]",2024-08-09T09:30:57Z
http://arxiv.org/abs/2408.04948v1,http://arxiv.org/abs/2408.04948v1,"HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented
  Generation for Efficient Information Extraction","Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain","[{'name': 'Bhaskarjit Sarmah'}, {'name': 'Benika Hall'}, {'name': 'Rohan Rao'}, {'name': 'Sunil Patel'}, {'name': 'Stefano Pasquali'}, {'name': 'Dhagash Mehta'}]",2024-08-09T09:07:48Z
http://arxiv.org/abs/2408.04941v1,http://arxiv.org/abs/2408.04941v1,Quantitative Information Extraction from Humanitarian Documents,"Humanitarian action is accompanied by a mass of reports, summaries, news, and
other documents. To guide its activities, important information must be quickly
extracted from such free-text resources. Quantities, such as the number of
people affected, amount of aid distributed, or the extent of infrastructure
damage, are central to emergency response and anticipatory action. In this
work, we contribute an annotated dataset for the humanitarian domain for the
extraction of such quantitative information, along side its important context,
including units it refers to, any modifiers, and the relevant event. Further,
we develop a custom Natural Language Processing pipeline to extract the
quantities alongside their units, and evaluate it in comparison to baseline and
recent literature. The proposed model achieves a consistent improvement in the
performance, especially in the documents pertaining to the Dominican Republic
and select African countries. We make the dataset and code available to the
research community to continue the improvement of NLP tools for the
humanitarian domain.","[{'name': 'Daniele Liberatore'}, {'name': 'Kyriaki Kalimeri'}, {'name': 'Derya Sever'}, {'name': 'Yelena Mejova'}]",2024-08-09T08:46:38Z
http://arxiv.org/abs/2408.04909v1,http://arxiv.org/abs/2408.04909v1,"Surveying the Landscape of Image Captioning Evaluation: A Comprehensive
  Taxonomy and Novel Ensemble Method","The task of image captioning has recently been gaining popularity, and with
it the complex task of evaluating the quality of image captioning models. In
this work, we present the first survey and taxonomy of over 70 different image
captioning metrics and their usage in hundreds of papers. We find that despite
the diversity of proposed metrics, the vast majority of studies rely on only
five popular metrics, which we show to be weakly correlated with human
judgements. Instead, we propose EnsembEval -- an ensemble of evaluation methods
achieving the highest reported correlation with human judgements across 5 image
captioning datasets, showing there is a lot of room for improvement by
leveraging a diverse set of metrics.","[{'name': 'Uri Berger'}, {'name': 'Gabriel Stanovsky'}, {'name': 'Omri Abend'}, {'name': 'Lea Frermann'}]",2024-08-09T07:31:06Z
http://arxiv.org/abs/2408.04906v1,http://arxiv.org/abs/2408.04906v1,Towards a Generative Approach for Emotion Detection and Reasoning,"Large language models (LLMs) have demonstrated impressive performance in
mathematical and commonsense reasoning tasks using chain-of-thought (CoT)
prompting techniques. But can they perform emotional reasoning by concatenating
`Let's think step-by-step' to the input prompt? In this paper we investigate
this question along with introducing a novel approach to zero-shot emotion
detection and emotional reasoning using LLMs. Existing state of the art
zero-shot approaches rely on textual entailment models to choose the most
appropriate emotion label for an input text. We argue that this strongly
restricts the model to a fixed set of labels which may not be suitable or
sufficient for many applications where emotion analysis is required. Instead,
we propose framing the problem of emotion analysis as a generative
question-answering (QA) task. Our approach uses a two step methodology of
generating relevant context or background knowledge to answer the emotion
detection question step-by-step. Our paper is the first work on using a
generative approach to jointly address the tasks of emotion detection and
emotional reasoning for texts. We evaluate our approach on two popular emotion
detection datasets and also release the fine-grained emotion labels and
explanations for further training and fine-tuning of emotional reasoning
systems.","[{'name': 'Ankita Bhaumik'}, {'name': 'Tomek Strzalkowski'}]",2024-08-09T07:20:15Z
http://arxiv.org/abs/2408.04905v1,http://arxiv.org/abs/2408.04905v1,"GlitchProber: Advancing Effective Detection and Mitigation of Glitch
  Tokens in Large Language Models","Large language models (LLMs) have achieved unprecedented success in the field
of natural language processing. However, the black-box nature of their internal
mechanisms has brought many concerns about their trustworthiness and
interpretability. Recent research has discovered a class of abnormal tokens in
the model's vocabulary space and named them ""glitch tokens"". Those tokens, once
included in the input, may induce the model to produce incorrect, irrelevant,
or even harmful results, drastically undermining the reliability and
practicality of LLMs.
  In this work, we aim to enhance the understanding of glitch tokens and
propose techniques for their detection and mitigation. We first reveal the
characteristic features induced by glitch tokens on LLMs, which are evidenced
by significant deviations in the distributions of attention patterns and
dynamic information from intermediate model layers. Based on the insights, we
develop GlitchProber, a tool for efficient glitch token detection and
mitigation. GlitchProber utilizes small-scale sampling, principal component
analysis for accelerated feature extraction, and a simple classifier for
efficient vocabulary screening. Taking one step further, GlitchProber rectifies
abnormal model intermediate layer values to mitigate the destructive effects of
glitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber
demonstrates higher efficiency, precision, and recall compared to existing
approaches, with an average F1 score of 0.86 and an average repair rate of
50.06%. GlitchProber unveils a novel path to address the challenges posed by
glitch tokens and inspires future research toward more robust and interpretable
LLMs.","[{'name': 'Zhibo Zhang'}, {'name': 'Wuxia Bai'}, {'name': 'Yuxi Li'}, {'name': 'Mark Huasong Meng'}, {'name': 'Kailong Wang'}, {'name': 'Ling Shi'}, {'name': 'Li Li'}, {'name': 'Jun Wang'}, {'name': 'Haoyu Wang'}]",2024-08-09T07:19:53Z
http://arxiv.org/abs/2408.04900v1,http://arxiv.org/abs/2408.04900v1,"Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural
  Communication in Codenames","Cultural differences in common ground may result in pragmatic failure and
misunderstandings during communication. We develop our method Rational Speech
Acts for Cross-Cultural Communication (RSA+C3) to resolve cross-cultural
differences in common ground. To measure the success of our method, we study
RSA+C3 in the collaborative referential game of Codenames Duet and show that
our method successfully improves collaboration between simulated players of
different cultures. Our contributions are threefold: (1) creating Codenames
players using contrastive learning of an embedding space and LLM prompting that
are aligned with human patterns of play, (2) studying culturally induced
differences in common ground reflected in our trained models, and (3)
demonstrating that our method RSA+C3 can ease cross-cultural communication in
gameplay by inferring sociocultural context from interaction. Our code is
publicly available at github.com/icwhite/codenames.","[{'name': 'Isadora White'}, {'name': 'Sashrika Pandey'}, {'name': 'Michelle Pan'}]",2024-08-09T07:02:18Z
http://arxiv.org/abs/2408.04873v1,http://arxiv.org/abs/2408.04873v1,Unsupervised Episode Detection for Large-Scale News Events,"Episodic structures are inherently interpretable and adaptable to evolving
large-scale key events. However, state-of-the-art automatic event detection
methods overlook event episodes and, therefore, struggle with these crucial
characteristics. This paper introduces a novel task, episode detection, aimed
at identifying episodes from a news corpus containing key event articles. An
episode describes a cohesive cluster of core entities (e.g., ""protesters"",
""police"") performing actions at a specific time and location. Furthermore, an
episode is a significant part of a larger group of episodes under a particular
key event. Automatically detecting episodes is challenging because, unlike key
events and atomic actions, we cannot rely on explicit mentions of times and
locations to distinguish between episodes or use semantic similarity to merge
inconsistent episode co-references. To address these challenges, we introduce
EpiMine, an unsupervised episode detection framework that (1) automatically
identifies the most salient, key-event-relevant terms and segments, (2)
determines candidate episodes in an article based on natural episodic
partitions estimated through shifts in discriminative term combinations, and
(3) refines and forms final episode clusters using large language model-based
reasoning on the candidate episodes. We construct three diverse, real-world
event datasets annotated at the episode level. EpiMine outperforms all
baselines on these datasets by an average 59.2% increase across all metrics.","[{'name': 'Priyanka Kargupta'}, {'name': 'Yunyi Zhang'}, {'name': 'Yizhu Jiao'}, {'name': 'Siru Ouyang'}, {'name': 'Jiawei Han'}]",2024-08-09T05:26:31Z
http://arxiv.org/abs/2408.04872v1,http://arxiv.org/abs/2408.04872v1,"SCOI: Syntax-augmented Coverage-based In-context Example Selection for
  Machine Translation","In-context learning (ICL) greatly improves the performance of large language
models (LLMs) on various down-stream tasks, where the improvement highly
depends on the quality of demonstrations. In this work, we introduce syntactic
knowledge to select better in-context examples for machine translation (MT). We
propose a new strategy, namely Syntax-augmented COverage-based In-context
example selection (SCOI), leveraging the deep syntactic structure beyond
conventional word matching. Specifically, we measure the set-level syntactic
coverage by computing the coverage of polynomial terms with the help of a
simplified tree-to-polynomial algorithm, and lexical coverage using word
overlap. Furthermore, we devise an alternate selection approach to combine both
coverage measures, taking advantage of syntactic and lexical information. We
conduct experiments with two multi-lingual LLMs on six translation directions.
Empirical results show that our proposed SCOI obtains the highest average COMET
score among all learning-free methods, indicating that combining syntactic and
lexical coverage successfully helps to select better in-context examples for
MT.","[{'name': 'Chenming Tang'}, {'name': 'Zhixiang Wang'}, {'name': 'Yunfang Wu'}]",2024-08-09T05:25:17Z
http://arxiv.org/abs/2408.04852v1,http://arxiv.org/abs/2408.04852v1,MSG-Chart: Multimodal Scene Graph for ChartQA,"Automatic Chart Question Answering (ChartQA) is challenging due to the
complex distribution of chart elements with patterns of the underlying data not
explicitly displayed in charts. To address this challenge, we design a joint
multimodal scene graph for charts to explicitly represent the relationships
between chart elements and their patterns. Our proposed multimodal scene graph
includes a visual graph and a textual graph to jointly capture the structural
and semantical knowledge from the chart. This graph module can be easily
integrated with different vision transformers as inductive bias. Our
experiments demonstrate that incorporating the proposed graph module enhances
the understanding of charts' elements' structure and semantics, thereby
improving performance on publicly available benchmarks, ChartQA and OpenCQA.","[{'name': 'Yue Dai'}, {'name': 'Soyeon Caren Han'}, {'name': 'Wei Liu'}]",2024-08-09T04:11:23Z
http://arxiv.org/abs/2408.04849v1,http://arxiv.org/abs/2408.04849v1,"Ensemble BERT: A student social network text sentiment classification
  model based on ensemble learning and BERT architecture","The mental health assessment of middle school students has always been one of
the focuses in the field of education. This paper introduces a new ensemble
learning network based on BERT, employing the concept of enhancing model
performance by integrating multiple classifiers. We trained a range of
BERT-based learners, which combined using the majority voting method. We
collect social network text data of middle school students through China's
Weibo and apply the method to the task of classifying emotional tendencies in
middle school students' social network texts. Experimental results suggest that
the ensemble learning network has a better performance than the base model and
the performance of the ensemble learning model, consisting of three
single-layer BERT models, is barely the same as a three-layer BERT model but
requires 11.58% more training time. Therefore, in terms of balancing prediction
effect and efficiency, the deeper BERT network should be preferred for
training. However, for interpretability, network ensembles can provide
acceptable solutions.","[{'name': 'Kai Jiang'}, {'name': 'Honghao Yang'}, {'name': 'Yuexian Wang'}, {'name': 'Qianru Chen'}, {'name': 'Yiming Luo'}]",2024-08-09T03:57:31Z
http://arxiv.org/abs/2408.04840v2,http://arxiv.org/abs/2408.04840v2,"mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal
  Large Language Models","Multi-modal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in executing instructions for a variety of single-image tasks.
Despite this progress, significant challenges remain in modeling long image
sequences. In this work, we introduce the versatile multi-modal large language
model, mPLUG-Owl3, which enhances the capability for long image-sequence
understanding in scenarios that incorporate retrieved image-text knowledge,
interleaved image-text, and lengthy videos. Specifically, we propose novel
hyper attention blocks to efficiently integrate vision and language into a
common language-guided semantic space, thereby facilitating the processing of
extended multi-image scenarios. Extensive experimental results suggest that
mPLUG-Owl3 achieves state-of-the-art performance among models with a similar
size on single-image, multi-image, and video benchmarks. Moreover, we propose a
challenging long visual sequence evaluation named Distractor Resistance to
assess the ability of models to maintain focus amidst distractions. Finally,
with the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance
on ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to
the development of more efficient and powerful multimodal large language
models.","[{'name': 'Jiabo Ye'}, {'name': 'Haiyang Xu'}, {'name': 'Haowei Liu'}, {'name': 'Anwen Hu'}, {'name': 'Ming Yan'}, {'name': 'Qi Qian'}, {'name': 'Ji Zhang'}, {'name': 'Fei Huang'}, {'name': 'Jingren Zhou'}]",2024-08-09T03:25:42Z
http://arxiv.org/abs/2408.04816v1,http://arxiv.org/abs/2408.04816v1,"FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt
  Optimization Across Tokenizers","The widespread use of large language models has resulted in a multitude of
tokenizers and embedding spaces, making knowledge transfer in prompt discovery
tasks difficult. In this work, we propose FUSE (Flexible Unification of
Semantic Embeddings), an inexpensive approach to approximating an adapter layer
that maps from one model's textual embedding space to another, even across
different tokenizers. We introduce a third-order tensor-based representation of
a model's embedding space that aligns semantic embeddings that have been split
apart by different tokenizers, and use this representation to derive an
approximation of the gradient of one model's outputs with respect to another
model's embedding space. We show the efficacy of our approach via
multi-objective optimization over vision-language and causal language models
for image captioning and sentiment-based image captioning.","[{'name': 'Joshua Nathaniel Williams'}, {'name': 'J. Zico Kolter'}]",2024-08-09T02:16:37Z
http://arxiv.org/abs/2408.04775v1,http://arxiv.org/abs/2408.04775v1,"Hybrid Student-Teacher Large Language Model Refinement for Cancer
  Toxicity Symptom Extraction","Large Language Models (LLMs) offer significant potential for clinical symptom
extraction, but their deployment in healthcare settings is constrained by
privacy concerns, computational limitations, and operational costs. This study
investigates the optimization of compact LLMs for cancer toxicity symptom
extraction using a novel iterative refinement approach. We employ a
student-teacher architecture, utilizing Zephyr-7b-beta and Phi3-mini-128 as
student models and GPT-4o as the teacher, to dynamically select between prompt
refinement, Retrieval-Augmented Generation (RAG), and fine-tuning strategies.
Our experiments on 294 clinical notes covering 12 post-radiotherapy toxicity
symptoms demonstrate the effectiveness of this approach. The RAG method proved
most efficient, improving average accuracy scores from 0.32 to 0.73 for
Zephyr-7b-beta and from 0.40 to 0.87 for Phi3-mini-128 during refinement. In
the test set, both models showed an approximate 0.20 increase in accuracy
across symptoms. Notably, this improvement was achieved at a cost 45 times
lower than GPT-4o for Zephyr and 79 times lower for Phi-3. These results
highlight the potential of iterative refinement techniques in enhancing the
capabilities of compact LLMs for clinical applications, offering a balance
between performance, cost-effectiveness, and privacy preservation in healthcare
settings.","[{'name': 'Reza Khanmohammadi'}, {'name': 'Ahmed I. Ghanem'}, {'name': 'Kyle Verdecchia'}, {'name': 'Ryan Hall'}, {'name': 'Mohamed Elshaikh'}, {'name': 'Benjamin Movsas'}, {'name': 'Hassan Bagher-Ebadian'}, {'name': 'Bing Luo'}, {'name': 'Indrin J. Chetty'}, {'name': 'Tuka Alhanai'}, {'name': 'Kundan Thind'}, {'name': 'Mohammad M. Ghassemi'}]",2024-08-08T22:18:01Z
http://arxiv.org/abs/2408.04723v1,http://arxiv.org/abs/2408.04723v1,Survey: Transformer-based Models in Data Modality Conversion,"Transformers have made significant strides across various artificial
intelligence domains, including natural language processing, computer vision,
and audio processing. This success has naturally garnered considerable interest
from both academic and industry researchers. Consequently, numerous Transformer
variants (often referred to as X-formers) have been developed for these fields.
However, a thorough and systematic review of these modality-specific
conversions remains lacking. Modality Conversion involves the transformation of
data from one form of representation to another, mimicking the way humans
integrate and interpret sensory information. This paper provides a
comprehensive review of transformer-based models applied to the primary
modalities of text, vision, and speech, discussing their architectures,
conversion methodologies, and applications. By synthesizing the literature on
modality conversion, this survey aims to underline the versatility and
scalability of transformers in advancing AI-driven content generation and
understanding.","[{'name': 'Elyas Rashno'}, {'name': 'Amir Eskandari'}, {'name': 'Aman Anand'}, {'name': 'Farhana Zulkernine'}]",2024-08-08T18:39:14Z
http://arxiv.org/abs/2408.04632v1,http://arxiv.org/abs/2408.04632v1,Arctic-TILT. Business Document Understanding at Sub-Billion Scale,"The vast portion of workloads employing LLMs involves answering questions
grounded on PDF or scan content. We introduce the Arctic-TILT achieving
accuracy on par with models 1000$\times$ its size on these use cases. It can be
fine-tuned and deployed on a single 24GB GPU, lowering operational costs while
processing Visually Rich Documents with up to 400k tokens. The model
establishes state-of-the-art results on seven diverse Document Understanding
benchmarks, as well as provides reliable confidence scores and quick inference,
which are essential for processing files in large-scale or time-sensitive
enterprise environments.","[{'name': 'Łukasz Borchmann'}, {'name': 'Michał Pietruszka'}, {'name': 'Wojciech Jaśkowski'}, {'name': 'Dawid Jurkiewicz'}, {'name': 'Piotr Halama'}, {'name': 'Paweł Józiak'}, {'name': 'Łukasz Garncarek'}, {'name': 'Paweł Liskowski'}, {'name': 'Karolina Szyndler'}, {'name': 'Andrzej Gretkowski'}, {'name': 'Julita Ołtusek'}, {'name': 'Gabriela Nowakowska'}, {'name': 'Artur Zawłocki'}, {'name': 'Łukasz Duhr'}, {'name': 'Paweł Dyda'}, {'name': 'Michał Turski'}]",2024-08-08T17:59:46Z
http://arxiv.org/abs/2408.04628v1,http://arxiv.org/abs/2408.04628v1,"LogogramNLP: Comparing Visual and Textual Representations of Ancient
  Logographic Writing Systems for NLP","Standard natural language processing (NLP) pipelines operate on symbolic
representations of language, which typically consist of sequences of discrete
tokens. However, creating an analogous representation for ancient logographic
writing systems is an extremely labor intensive process that requires expert
knowledge. At present, a large portion of logographic data persists in a purely
visual form due to the absence of transcription -- this issue poses a
bottleneck for researchers seeking to apply NLP toolkits to study ancient
logographic languages: most of the relevant data are images of writing.
  This paper investigates whether direct processing of visual representations
of language offers a potential solution. We introduce LogogramNLP, the first
benchmark enabling NLP analysis of ancient logographic languages, featuring
both transcribed and visual datasets for four writing systems along with
annotations for tasks like classification, translation, and parsing. Our
experiments compare systems that employ recent visual and text encoding
strategies as backbones. The results demonstrate that visual representations
outperform textual representations for some investigated tasks, suggesting that
visual processing pipelines may unlock a large amount of cultural heritage data
of logographic languages for NLP-based analyses.","[{'name': 'Danlu Chen'}, {'name': 'Freda Shi'}, {'name': 'Aditi Agarwal'}, {'name': 'Jacobo Myerston'}, {'name': 'Taylor Berg-Kirkpatrick'}]",2024-08-08T17:58:06Z
http://arxiv.org/abs/2408.04619v1,http://arxiv.org/abs/2408.04619v1,Transformer Explainer: Interactive Learning of Text-Generative Models,"Transformers have revolutionized machine learning, yet their inner workings
remain opaque to many. We present Transformer Explainer, an interactive
visualization tool designed for non-experts to learn about Transformers through
the GPT-2 model. Our tool helps users understand complex Transformer concepts
by integrating a model overview and enabling smooth transitions across
abstraction levels of mathematical operations and model structures. It runs a
live GPT-2 instance locally in the user's browser, empowering users to
experiment with their own input and observe in real-time how the internal
components and parameters of the Transformer work together to predict the next
tokens. Our tool requires no installation or special hardware, broadening the
public's education access to modern generative AI techniques. Our open-sourced
tool is available at https://poloclub.github.io/transformer-explainer/. A video
demo is available at https://youtu.be/ECR4oAwocjs.","[{'name': 'Aeree Cho'}, {'name': 'Grace C. Kim'}, {'name': 'Alexander Karpekov'}, {'name': 'Alec Helbling'}, {'name': 'Zijie J. Wang'}, {'name': 'Seongmin Lee'}, {'name': 'Benjamin Hoover'}, {'name': 'Duen Horng Chau'}]",2024-08-08T17:49:07Z
http://arxiv.org/abs/2408.04614v2,http://arxiv.org/abs/2408.04614v2,Better Alignment with Instruction Back-and-Forth Translation,"We propose a new method, instruction back-and-forth translation, to construct
high-quality synthetic data grounded in world knowledge for aligning large
language models (LLMs). Given documents from a web corpus, we generate and
curate synthetic instructions using the backtranslation approach proposed by Li
et al.(2023a), and rewrite the responses to improve their quality further based
on the initial documents. Fine-tuning with the resulting (backtranslated
instruction, rewritten response) pairs yields higher win rates on AlpacaEval
than using other common instruction datasets such as Humpback, ShareGPT, Open
Orca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the
responses with an LLM outperforms direct distillation, and the two generated
text distributions exhibit significant distinction in embedding space. Further
analysis shows that our backtranslated instructions are of higher quality than
other sources of synthetic instructions, while our responses are more diverse
and complex than those obtained from distillation. Overall we find that
instruction back-and-forth translation combines the best of both worlds --
making use of the information diversity and quantity found on the web, while
ensuring the quality of the responses which is necessary for effective
alignment.","[{'name': 'Thao Nguyen'}, {'name': 'Jeffrey Li'}, {'name': 'Sewoong Oh'}, {'name': 'Ludwig Schmidt'}, {'name': 'Jason Weston'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Xian Li'}]",2024-08-08T17:42:32Z
http://arxiv.org/abs/2408.04596v1,http://arxiv.org/abs/2408.04596v1,"Code-switching in text and speech reveals information-theoretic audience
  design","In this work, we use language modeling to investigate the factors that
influence code-switching. Code-switching occurs when a speaker alternates
between one language variety (the primary language) and another (the secondary
language), and is widely observed in multilingual contexts. Recent work has
shown that code-switching is often correlated with areas of high information
load in the primary language, but it is unclear whether high primary language
load only makes the secondary language relatively easier to produce at
code-switching points (speaker-driven code-switching), or whether
code-switching is additionally used by speakers to signal the need for greater
attention on the part of listeners (audience-driven code-switching). In this
paper, we use bilingual Chinese-English online forum posts and transcripts of
spontaneous Chinese-English speech to replicate prior findings that high
primary language (Chinese) information load is correlated with switches to the
secondary language (English). We then demonstrate that the information load of
the English productions is even higher than that of meaning equivalent Chinese
alternatives, and these are therefore not easier to produce, providing evidence
of audience-driven influences in code-switching at the level of the
communication channel, not just at the sociolinguistic level, in both writing
and speech.","[{'name': 'Debasmita Bhattacharya'}, {'name': 'Marten van Schijndel'}]",2024-08-08T17:14:12Z
http://arxiv.org/abs/2408.04585v2,http://arxiv.org/abs/2408.04585v2,"Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency,
  Performance, and Adversarial Robustness","With the increasing demand for practical applications of Large Language
Models (LLMs), many attention-efficient models have been developed to balance
performance and computational cost. However, the adversarial robustness of
these models remains under-explored. In this work, we design a framework to
investigate the trade-off between efficiency, performance, and adversarial
robustness of LLMs by comparing three prominent models with varying levels of
complexity and efficiency -- Transformer++, Gated Linear Attention (GLA)
Transformer, and MatMul-Free LM -- utilizing the GLUE and AdvGLUE datasets. The
AdvGLUE dataset extends the GLUE dataset with adversarial samples designed to
challenge model robustness. Our results show that while the GLA Transformer and
MatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate
higher efficiency and either superior or comparative robustness on AdvGLUE
tasks compared to Transformer++ across different attack levels. These findings
highlight the potential of simplified architectures to achieve a compelling
balance between efficiency, performance, and adversarial robustness, offering
valuable insights for applications where resource constraints and resilience to
adversarial attacks are critical.","[{'name': 'Xiaojing Fan'}, {'name': 'Chunliang Tao'}]",2024-08-08T16:54:40Z
http://arxiv.org/abs/2408.04575v1,http://arxiv.org/abs/2408.04575v1,SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals,"Explainable Artificial Intelligence (XAI) is essential for enhancing the
transparency and accountability of AI models, especially in natural language
processing (NLP) tasks. This paper introduces SCENE (Soft Counterfactual
Evaluation for Natural language Explainability), a novel evaluation method that
leverages large language models (LLMs) to generate Soft Counterfactual
explanations in a zero-shot manner. By focusing on token-based substitutions,
SCENE creates contextually appropriate and seman-tically meaningful Soft
Counterfactuals without extensive fine-tuning. SCENE adopts Validitysoft and
Csoft metrics to evaluate the effectiveness of model-agnostic XAI methods in
text classification tasks. Applied to CNN, RNN, and BERT architectures, SCENE
provides valuable insights into the strengths and limitations of various XAI
techniques.","[{'name': 'Haoran Zheng'}, {'name': 'Utku Pamuksuz'}]",2024-08-08T16:36:24Z
http://arxiv.org/abs/2408.04568v1,http://arxiv.org/abs/2408.04568v1,"Learning Fine-Grained Grounded Citations for Attributed Large Language
  Models","Despite the impressive performance on information-seeking tasks, large
language models (LLMs) still struggle with hallucinations. Attributed LLMs,
which augment generated text with in-line citations, have shown potential in
mitigating hallucinations and improving verifiability. However, current
approaches suffer from suboptimal citation quality due to their reliance on
in-context learning. Furthermore, the practice of citing only coarse document
identifiers makes it challenging for users to perform fine-grained
verification. In this work, we introduce FRONT, a training framework designed
to teach LLMs to generate Fine-Grained Grounded Citations. By grounding model
outputs in fine-grained supporting quotes, these quotes guide the generation of
grounded and consistent responses, not only improving citation quality but also
facilitating fine-grained verification. Experiments on the ALCE benchmark
demonstrate the efficacy of FRONT in generating superior grounded responses and
highly supportive citations. With LLaMA-2-7B, the framework significantly
outperforms all the baselines, achieving an average of 14.21% improvement in
citation quality across all datasets, even surpassing ChatGPT.","[{'name': 'Lei Huang'}, {'name': 'Xiaocheng Feng'}, {'name': 'Weitao Ma'}, {'name': 'Yuxuan Gu'}, {'name': 'Weihong Zhong'}, {'name': 'Xiachong Feng'}, {'name': 'Weijiang Yu'}, {'name': 'Weihua Peng'}, {'name': 'Duyu Tang'}, {'name': 'Dandan Tu'}, {'name': 'Bing Qin'}]",2024-08-08T16:28:22Z
http://arxiv.org/abs/2408.04693v1,http://arxiv.org/abs/2408.04693v1,Understanding the Performance and Estimating the Cost of LLM Fine-Tuning,"Due to the cost-prohibitive nature of training Large Language Models (LLMs),
fine-tuning has emerged as an attractive alternative for specializing LLMs for
specific tasks using limited compute resources in a cost-effective manner. In
this paper, we characterize sparse Mixture of Experts (MoE) based LLM
fine-tuning to understand their accuracy and runtime performance on a single
GPU. Our evaluation provides unique insights into the training efficacy of
sparse and dense versions of MoE models, as well as their runtime
characteristics, including maximum batch size, execution time breakdown,
end-to-end throughput, GPU hardware utilization, and load distribution. Our
study identifies the optimization of the MoE layer as crucial for further
improving the performance of LLM fine-tuning. Using our profiling results, we
also develop and validate an analytical model to estimate the cost of LLM
fine-tuning on the cloud. This model, based on parameters of the model and GPU
architecture, estimates LLM throughput and the cost of training, aiding
practitioners in industry and academia to budget the cost of fine-tuning a
specific model.","[{'name': 'Yuchen Xia'}, {'name': 'Jiho Kim'}, {'name': 'Yuhan Chen'}, {'name': 'Haojie Ye'}, {'name': 'Souvik Kundu'}, {'name': 'Cong'}, {'name': 'Hao'}, {'name': 'Nishil Talati'}]",2024-08-08T16:26:07Z
http://arxiv.org/abs/2408.04560v1,http://arxiv.org/abs/2408.04560v1,Conversational Prompt Engineering,"Prompts are how humans communicate with LLMs. Informative prompts are
essential for guiding LLMs to produce the desired output. However, prompt
engineering is often tedious and time-consuming, requiring significant
expertise, limiting its widespread use. We propose Conversational Prompt
Engineering (CPE), a user-friendly tool that helps users create personalized
prompts for their specific tasks. CPE uses a chat model to briefly interact
with users, helping them articulate their output preferences and integrating
these into the prompt. The process includes two main stages: first, the model
uses user-provided unlabeled data to generate data-driven questions and utilize
user responses to shape the initial instruction. Then, the model shares the
outputs generated by the instruction and uses user feedback to further refine
the instruction and the outputs. The final result is a few-shot prompt, where
the outputs approved by the user serve as few-shot examples. A user study on
summarization tasks demonstrates the value of CPE in creating personalized,
high-performing prompts. The results suggest that the zero-shot prompt obtained
is comparable to its - much longer - few-shot counterpart, indicating
significant savings in scenarios involving repetitive tasks with large text
volumes.","[{'name': 'Liat Ein-Dor'}, {'name': 'Orith Toledo-Ronen'}, {'name': 'Artem Spector'}, {'name': 'Shai Gretz'}, {'name': 'Lena Dankin'}, {'name': 'Alon Halfon'}, {'name': 'Yoav Katz'}, {'name': 'Noam Slonim'}]",2024-08-08T16:18:39Z
http://arxiv.org/abs/2408.04556v1,http://arxiv.org/abs/2408.04556v1,"Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of
  Large Language Models","Large language models (LLMs) have exhibited remarkable proficiency across a
diverse array of natural language processing (NLP) tasks. However, adapting
LLMs to downstream applications typically necessitates computationally
intensive and memory-demanding fine-tuning procedures. To mitigate these
burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a
promising approach to tailor LLMs with minimal computational overhead. While
PEFT methods offer substantial advantages, they do not fully address the
pervasive issue of bias propagation from pre-training data. In this work, we
introduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method
designed to counteract bias inheritance. BA-LoRA incorporates three distinct
regularization terms: (1) consistency regularizer, (2) diversity regularizer,
and (3) singular vector decomposition regularizer. These regularizers
collectively aim to improve the generative models' consistency, diversity, and
generalization capabilities during the fine-tuning process. Through extensive
experiments on a variety of natural language understanding (NLU) and natural
language generation (NLG) tasks, employing prominent LLMs such as LLaMA,
Mistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of
LoRA and its state-of-the-art variants. Moreover, our method effectively
mitigates the deleterious effects of pre-training bias, leading to more
reliable and robust model outputs. The code is available at
https://github.com/cyp-jlu-ai/BA-LoRA.","[{'name': 'Yupeng Chang'}, {'name': 'Yi Chang'}, {'name': 'Yuan Wu'}]",2024-08-08T16:13:26Z
http://arxiv.org/abs/2408.04554v1,http://arxiv.org/abs/2408.04554v1,Molyé: A Corpus-based Approach to Language Contact in Colonial France,"Whether or not several Creole languages which developed during the early
modern period can be considered genetic descendants of European languages has
been the subject of intense debate. This is in large part due to the absence of
evidence of intermediate forms. This work introduces a new open corpus, the
Moly\'e corpus, which combines stereotypical representations of three kinds of
language variation in Europe with early attestations of French-based Creole
languages across a period of 400 years. It is intended to facilitate future
research on the continuity between contact situations in Europe and Creolophone
(former) colonies.","[{'name': 'Rasul Dent'}, {'name': 'Juliette Janès'}, {'name': 'Thibault Clérice'}, {'name': 'Pedro Ortiz Suarez'}, {'name': 'Benoît Sagot'}]",2024-08-08T16:09:40Z
http://arxiv.org/abs/2408.04540v1,http://arxiv.org/abs/2408.04540v1,"MemeMind at ArAIEval Shared Task: Spotting Persuasive Spans in Arabic
  Text with Persuasion Techniques Identification","This paper focuses on detecting propagandistic spans and persuasion
techniques in Arabic text from tweets and news paragraphs. Each entry in the
dataset contains a text sample and corresponding labels that indicate the start
and end positions of propaganda techniques within the text. Tokens falling
within a labeled span were assigned ""B"" (Begin) or ""I"" (Inside), ""O"",
corresponding to the specific propaganda technique. Using attention masks, we
created uniform lengths for each span and assigned BIO tags to each token based
on the provided labels. Then, we used AraBERT-base pre-trained model for Arabic
text tokenization and embeddings with a token classification layer to identify
propaganda techniques. Our training process involves a two-phase fine-tuning
approach. First, we train only the classification layer for a few epochs,
followed by full model fine-tuning, updating all parameters. This methodology
allows the model to adapt to the specific characteristics of the propaganda
detection task while leveraging the knowledge captured by the pre-trained
AraBERT model. Our approach achieved an F1 score of 0.2774, securing the 3rd
position in the leaderboard of Task 1.","[{'name': 'Md Rafiul Biswas'}, {'name': 'Zubair Shah'}, {'name': 'Wajdi Zaghouani'}]",2024-08-08T15:49:01Z
http://arxiv.org/abs/2408.04522v1,http://arxiv.org/abs/2408.04522v1,"Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large
  Language Models","As diverse linguistic communities and users adopt large language models
(LLMs), assessing their safety across languages becomes critical. Despite
ongoing efforts to make LLMs safe, they can still be made to behave unsafely
with jailbreaking, a technique in which models are prompted to act outside
their operational guidelines. Research on LLM safety and jailbreaking, however,
has so far mostly focused on English, limiting our understanding of LLM safety
in other languages. We contribute towards closing this gap by investigating the
effectiveness of many-shot jailbreaking, where models are prompted with unsafe
demonstrations to induce unsafe behaviour, in Italian. To enable our analysis,
we create a new dataset of unsafe Italian question-answer pairs. With this
dataset, we identify clear safety vulnerabilities in four families of
open-weight LLMs. We find that the models exhibit unsafe behaviors even when
prompted with few unsafe demonstrations, and -- more alarmingly -- that this
tendency rapidly escalates with more demonstrations.","[{'name': 'Fabio Pernisi'}, {'name': 'Dirk Hovy'}, {'name': 'Paul Röttger'}]",2024-08-08T15:24:03Z
http://arxiv.org/abs/2408.04519v1,http://arxiv.org/abs/2408.04519v1,"Articulatory Configurations across Genders and Periods in French Radio
  and TV archives","This paper studies changes in articulatory configurations across genders and
periods using an inversion from acoustic to articulatory parameters. From a
diachronic corpus based on French media archives spanning 60 years from 1955 to
2015, automatic transcription and forced alignment allowed extracting the
central frame of each vowel. More than one million frames were obtained from
over a thousand speakers across gender and age categories. Their formants were
used from these vocalic frames to fit the parameters of Maeda's articulatory
model. Evaluations of the quality of these processes are provided. We focus
here on two parameters of Maeda's model linked to total vocal tract length: the
relative position of the larynx (higher for females) and the lips protrusion
(more protruded for males). Implications for voice quality across genders are
discussed. The effect across periods seems gender independent; thus, the
assertion that females lowered their pitch with time is not supported.","[{'name': 'Benjamin Elie'}, {'name': 'David Doukhan'}, {'name': 'Rémi Uro'}, {'name': 'Lucas Ondel-Yang'}, {'name': 'Albert Rilliard'}, {'name': 'Simon Devauchelle'}]",2024-08-08T15:20:39Z
http://arxiv.org/abs/2408.04472v1,http://arxiv.org/abs/2408.04472v1,"Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for
  Competitive Debate","Competitive debate is a comprehensive and complex computational argumentation
task. Large Language Models (LLMs) encounter hallucinations and lack
competitiveness in this task. To address these challenges, we introduce Agent
for Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs
designed to enhance their capabilities in competitive debate. Drawing
inspiration from human behavior in debate preparation and execution,
Agent4Debate employs a collaborative architecture where four specialized agents
(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.
These agents work throughout the debate process, covering multiple stages from
initial research and argument formulation to rebuttal and summary. To
comprehensively evaluate framework performance, we construct the Chinese Debate
Arena, comprising 66 carefully selected Chinese debate motions. We recruite ten
experienced human debaters and collect records of 200 debates involving
Agent4Debate, baseline models, and humans. The evaluation employs the Debatrix
automatic scoring system and professional human reviewers based on the
established Debatrix-Elo and Human-Elo ranking. Experimental results indicate
that the state-of-the-art Agent4Debate exhibits capabilities comparable to
those of humans. Furthermore, ablation studies demonstrate the effectiveness of
each component in the agent structure.","[{'name': 'Yiqun Zhang'}, {'name': 'Xiaocui Yang'}, {'name': 'Shi Feng'}, {'name': 'Daling Wang'}, {'name': 'Yifei Zhang'}, {'name': 'Kaisong Song'}]",2024-08-08T14:02:45Z
http://arxiv.org/abs/2408.04463v1,http://arxiv.org/abs/2408.04463v1,Crowd Intelligence for Early Misinformation Prediction on Social Media,"Misinformation spreads rapidly on social media, causing serious damage by
influencing public opinion, promoting dangerous behavior, or eroding trust in
reliable sources. It spreads too fast for traditional fact-checking, stressing
the need for predictive methods. We introduce CROWDSHIELD, a crowd
intelligence-based method for early misinformation prediction. We hypothesize
that the crowd's reactions to misinformation reveal its accuracy. Furthermore,
we hinge upon exaggerated assertions/claims and replies with particular
positions/stances on the source post within a conversation thread. We employ
Q-learning to capture the two dimensions -- stances and claims. We utilize deep
Q-learning due to its proficiency in navigating complex decision spaces and
effectively learning network properties. Additionally, we use a
transformer-based encoder to develop a comprehensive understanding of both
content and context. This multifaceted approach helps ensure the model pays
attention to user interaction and stays anchored in the communication's
content. We propose MIST, a manually annotated misinformation detection Twitter
corpus comprising nearly 200 conversation threads with more than 14K replies.
In experiments, CROWDSHIELD outperformed ten baseline systems, achieving an
improvement of ~4% macro-F1 score. We conduct an ablation study and error
analysis to validate our proposed model's performance. The source code and
dataset are available at https://github.com/LCS2-IIITD/CrowdShield.git.","[{'name': 'Megha Sundriyal'}, {'name': 'Harshit Choudhary'}, {'name': 'Tanmoy Chakraborty'}, {'name': 'Md Shad Akhtar'}]",2024-08-08T13:45:23Z
http://arxiv.org/abs/2408.04691v1,http://arxiv.org/abs/2408.04691v1,"Improving Relational Database Interactions with Large Language Models:
  Column Descriptions and Their Impact on Text-to-SQL Performance","Relational databases often suffer from uninformative descriptors of table
contents, such as ambiguous columns and hard-to-interpret values, impacting
both human users and Text-to-SQL models. This paper explores the use of large
language models (LLMs) to generate informative column descriptions as a
semantic layer for relational databases. Using the BIRD-Bench development set,
we created \textsc{ColSQL}, a dataset with gold-standard column descriptions
generated and refined by LLMs and human annotators. We evaluated several
instruction-tuned models, finding that GPT-4o and Command R+ excelled in
generating high-quality descriptions. Additionally, we applied an
LLM-as-a-judge to evaluate model performance. Although this method does not
align well with human evaluations, we included it to explore its potential and
to identify areas for improvement. More work is needed to improve the
reliability of automatic evaluations for this task. We also find that detailed
column descriptions significantly improve Text-to-SQL execution accuracy,
especially when columns are uninformative. This study establishes LLMs as
effective tools for generating detailed metadata, enhancing the usability of
relational databases.","[{'name': 'Niklas Wretblad'}, {'name': 'Oskar Holmström'}, {'name': 'Erik Larsson'}, {'name': 'Axel Wiksäter'}, {'name': 'Oscar Söderlund'}, {'name': 'Hjalmar Öhman'}, {'name': 'Ture Pontén'}, {'name': 'Martin Forsberg'}, {'name': 'Martin Sörme'}, {'name': 'Fredrik Heintz'}]",2024-08-08T13:10:51Z
http://arxiv.org/abs/2408.04427v1,http://arxiv.org/abs/2408.04427v1,"AcrosticSleuth: Probabilistic Identification and Ranking of Acrostics in
  Multilingual Corpora","For centuries, writers have hidden messages in their texts as acrostics,
where initial letters of consecutive lines or paragraphs form meaningful words
or phrases. Scholars searching for acrostics manually can only focus on a few
authors at a time and often favor qualitative arguments in discussing
intentionally. We aim to put the study of acrostics on firmer statistical
footing by presenting AcrosticSleuth, a first-of-its-kind tool that
automatically identifies acrostics and ranks them by the probability that the
sequence of characters does not occur by chance (and therefore may have been
inserted intentionally). Acrostics are rare, so we formalize the problem as a
binary classification task in the presence of extreme class imbalance. To
evaluate AcrosticSleuth, we present the Acrostic Identification Dataset
(AcrostID), a collection of acrostics from the WikiSource online database.
Despite the class imbalance, AcrosticSleuth achieves F1 scores of 0.39, 0.59,
and 0.66 on French, English, and Russian subdomains of WikiSource,
respectively. We further demonstrate that AcrosticSleuth can identify
previously unknown high-profile instances of wordplay, such as the acrostic
spelling ARSPOETICA (``art of poetry"") by Italian Humanist Albertino Mussato
and English philosopher Thomas Hobbes' signature in the opening paragraphs of
The Elements of Law.","[{'name': 'Aleksandr Fedchin'}, {'name': 'Isabel Cooperman'}, {'name': 'Pramit Chaudhuri'}, {'name': 'Joseph P. Dexter'}]",2024-08-08T12:53:26Z
http://arxiv.org/abs/2408.04420v1,http://arxiv.org/abs/2408.04420v1,"Recognizing Emotion Regulation Strategies from Human Behavior with Large
  Language Models","Human emotions are often not expressed directly, but regulated according to
internal processes and social display rules. For affective computing systems,
an understanding of how users regulate their emotions can be highly useful, for
example to provide feedback in job interview training, or in psychotherapeutic
scenarios. However, at present no method to automatically classify different
emotion regulation strategies in a cross-user scenario exists. At the same
time, recent studies showed that instruction-tuned Large Language Models (LLMs)
can reach impressive performance across a variety of affect recognition tasks
such as categorical emotion recognition or sentiment analysis. While these
results are promising, it remains unclear to what extent the representational
power of LLMs can be utilized in the more subtle task of classifying users'
internal emotion regulation strategy. To close this gap, we make use of the
recently introduced \textsc{Deep} corpus for modeling the social display of the
emotion shame, where each point in time is annotated with one of seven
different emotion regulation classes. We fine-tune Llama2-7B as well as the
recently introduced Gemma model using Low-rank Optimization on prompts
generated from different sources of information on the \textsc{Deep} corpus.
These include verbal and nonverbal behavior, person factors, as well as the
results of an in-depth interview after the interaction. Our results show, that
a fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation
strategy with high accuracy (0.84) without needing access to data from
post-interaction interviews. This represents a significant improvement over
previous approaches based on Bayesian Networks and highlights the importance of
modeling verbal behavior in emotion regulation.","[{'name': 'Philipp Müller'}, {'name': 'Alexander Heimerl'}, {'name': 'Sayed Muddashir Hossain'}, {'name': 'Lea Siegel'}, {'name': 'Jan Alexandersson'}, {'name': 'Patrick Gebhard'}, {'name': 'Elisabeth André'}, {'name': 'Tanja Schneeberger'}]",2024-08-08T12:47:10Z
http://arxiv.org/abs/2408.04414v1,http://arxiv.org/abs/2408.04414v1,"Enhancing Robustness of Retrieval-Augmented Language Models with
  In-Context Learning","Retrieval-Augmented Language Models (RALMs) have significantly improved
performance in open-domain question answering (QA) by leveraging external
knowledge. However, RALMs still struggle with unanswerable queries, where the
retrieved contexts do not contain the correct answer, and with conflicting
information, where different sources provide contradictory answers due to
imperfect retrieval. This study introduces an in-context learning-based
approach to enhance the reasoning capabilities of RALMs, making them more
robust in imperfect retrieval scenarios. Our method incorporates Machine
Reading Comprehension (MRC) demonstrations, referred to as cases, to boost the
model's capabilities to identify unanswerabilities and conflicts among the
retrieved contexts. Experiments on two open-domain QA datasets show that our
approach increases accuracy in identifying unanswerable and conflicting
scenarios without requiring additional fine-tuning. This work demonstrates that
in-context learning can effectively enhance the robustness of RALMs in
open-domain QA tasks.","[{'name': 'Seong-Il Park'}, {'name': 'Seung-Woo Choi'}, {'name': 'Na-Hyun Kim'}, {'name': 'Jay-Yoon Lee'}]",2024-08-08T12:42:43Z
http://arxiv.org/abs/2408.04403v1,http://arxiv.org/abs/2408.04403v1,"Exploring Reasoning Biases in Large Language Models Through Syllogism:
  Insights from the NeuBAROCO Dataset","This paper explores the question of how accurately current large language
models can perform logical reasoning in natural language, with an emphasis on
whether these models exhibit reasoning biases similar to humans. Specifically,
our study focuses on syllogistic reasoning, a form of deductive reasoning
extensively studied in cognitive science as a natural form of human reasoning.
We present a syllogism dataset called NeuBAROCO, which consists of syllogistic
reasoning problems in English and Japanese. This dataset was originally
designed for psychological experiments to assess human reasoning capabilities
using various forms of syllogisms. Our experiments with leading large language
models indicate that these models exhibit reasoning biases similar to humans,
along with other error tendencies. Notably, there is significant room for
improvement in reasoning problems where the relationship between premises and
hypotheses is neither entailment nor contradiction. We also present
experimental results and in-depth analysis using a new Chain-of-Thought
prompting method, which asks LLMs to translate syllogisms into abstract logical
expressions and then explain their reasoning process. Our analysis using this
method suggests that the primary limitations of LLMs lie in the reasoning
process itself rather than the interpretation of syllogisms.","[{'name': 'Kentaro Ozeki'}, {'name': 'Risako Ando'}, {'name': 'Takanobu Morishita'}, {'name': 'Hirohiko Abe'}, {'name': 'Koji Mineshima'}, {'name': 'Mitsuhiro Okada'}]",2024-08-08T12:10:50Z
http://arxiv.org/abs/2408.04394v1,http://arxiv.org/abs/2408.04394v1,"Automated Educational Question Generation at Different Bloom's Skill
  Levels using Large Language Models: Strategies and Evaluation","Developing questions that are pedagogically sound, relevant, and promote
learning is a challenging and time-consuming task for educators. Modern-day
large language models (LLMs) generate high-quality content across multiple
domains, potentially helping educators to develop high-quality questions.
Automated educational question generation (AEQG) is important in scaling online
education catering to a diverse student population. Past attempts at AEQG have
shown limited abilities to generate questions at higher cognitive levels. In
this study, we examine the ability of five state-of-the-art LLMs of different
sizes to generate diverse and high-quality questions of different cognitive
levels, as defined by Bloom's taxonomy. We use advanced prompting techniques
with varying complexity for AEQG. We conducted expert and LLM-based evaluations
to assess the linguistic and pedagogical relevance and quality of the
questions. Our findings suggest that LLms can generate relevant and
high-quality educational questions of different cognitive levels when prompted
with adequate information, although there is a significant variance in the
performance of the five LLms considered. We also show that automated evaluation
is not on par with human evaluation.","[{'name': 'Nicy Scaria'}, {'name': 'Suma Dharani Chenna'}, {'name': 'Deepak Subramani'}]",2024-08-08T11:56:57Z
http://arxiv.org/abs/2408.04392v1,http://arxiv.org/abs/2408.04392v1,Open-domain Implicit Format Control for Large Language Model Generation,"Controlling the format of outputs generated by large language models (LLMs)
is a critical functionality in various applications. Current methods typically
employ constrained decoding with rule-based automata or fine-tuning with
manually crafted format instructions, both of which struggle with open-domain
format requirements. To address this limitation, we introduce a novel framework
for controlled generation in LLMs, leveraging user-provided, one-shot QA pairs.
This study investigates LLMs' capabilities to follow open-domain, one-shot
constraints and replicate the format of the example answers. We observe that
this is a non-trivial problem for current LLMs. We also develop a dataset
collection methodology for supervised fine-tuning that enhances the open-domain
format control of LLMs without degrading output quality, as well as a benchmark
on which we evaluate both the helpfulness and format correctness of LLM
outputs. The resulting datasets, named OIFC-SFT, along with the related code,
will be made publicly available at https://github.com/cofe-ai/OIFC.","[{'name': 'Yiqun Yao'}, {'name': 'Wenjia Ma'}, {'name': 'Xuezhi Fang'}, {'name': 'Xin Jiang'}, {'name': 'Xiang Li'}, {'name': 'Xuying Meng'}, {'name': 'Peng Han'}, {'name': 'Jing Li'}, {'name': 'Aixin Sun'}, {'name': 'Yequan Wang'}]",2024-08-08T11:51:45Z
http://arxiv.org/abs/2408.04378v1,http://arxiv.org/abs/2408.04378v1,Overview of the NLPCC 2024 Shared Task on Chinese Metaphor Generation,"This paper presents the results of the shared task on Chinese metaphor
generation, hosted at the 13th CCF Conference on Natural Language Processing
and Chinese Computing (NLPCC 2024). The goal of this shared task is to generate
Chinese metaphors using machine learning techniques and effectively identifying
basic components of metaphorical sentences. It is divided into two subtasks: 1)
Metaphor Generation, which involves creating a metaphor from a provided tuple
consisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a
metaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),
guided by the concept of the GROUND. 2) Metaphor Components Identification,
which extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a
metaphorical sentence. This component requires the identification of the most
fitting metaphor elements that correspond to the specified grounds. In addition
to overall results, we report on the setup and insights from the metaphor
generation shared task, which attracted a total of 4 participating teams across
both subtasks.","[{'name': 'Xingwei Qu'}, {'name': 'Ge Zhang'}, {'name': 'Siwei Wu'}, {'name': 'Yizhi Li'}, {'name': 'Chenghua Lin'}]",2024-08-08T11:29:43Z
http://arxiv.org/abs/2408.04369v1,http://arxiv.org/abs/2408.04369v1,"Analyzing Consumer Reviews for Understanding Drivers of Hotels Ratings:
  An Indian Perspective","In the internet era, almost every business entity is trying to have its
digital footprint in digital media and other social media platforms. For these
entities, word of mouse is also very important. Particularly, this is quite
crucial for the hospitality sector dealing with hotels, restaurants etc.
Consumers do read other consumers reviews before making final decisions. This
is where it becomes very important to understand which aspects are affecting
most in the minds of the consumers while giving their ratings. The current
study focuses on the consumer reviews of Indian hotels to extract aspects
important for final ratings. The study involves gathering data using web
scraping methods, analyzing the texts using Latent Dirichlet Allocation for
topic extraction and sentiment analysis for aspect-specific sentiment mapping.
Finally, it incorporates Random Forest to understand the importance of the
aspects in predicting the final rating of a user.","[{'name': 'Subhasis Dasgupta'}, {'name': 'Soumya Roy'}, {'name': 'Jaydip Sen'}]",2024-08-08T10:58:33Z
http://arxiv.org/abs/2408.04363v1,http://arxiv.org/abs/2408.04363v1,"Simulating Articulatory Trajectories with Phonological Feature
  Interpolation","As a first step towards a complete computational model of speech learning
involving perception-production loops, we investigate the forward mapping
between pseudo-motor commands and articulatory trajectories. Two phonological
feature sets, based respectively on generative and articulatory phonology, are
used to encode a phonetic target sequence. Different interpolation techniques
are compared to generate smooth trajectories in these feature spaces, with a
potential optimisation of the target value and timing to capture
co-articulation effects. We report the Pearson correlation between a linear
projection of the generated trajectories and articulatory data derived from a
multi-speaker dataset of electromagnetic articulography (EMA) recordings. A
correlation of 0.67 is obtained with an extended feature set based on
generative phonology and a linear interpolation technique. We discuss the
implications of our results for our understanding of the dynamics of biological
motion.","[{'name': 'Angelo Ortiz Tandazo'}, {'name': 'Thomas Schatz'}, {'name': 'Thomas Hueber'}, {'name': 'Emmanuel Dupoux'}]",2024-08-08T10:51:16Z
http://arxiv.org/abs/2408.04331v1,http://arxiv.org/abs/2408.04331v1,"Enhancing Journalism with AI: A Study of Contextualized Image Captioning
  for News Articles using LLMs and LMMs","Large language models (LLMs) and large multimodal models (LMMs) have
significantly impacted the AI community, industry, and various economic
sectors. In journalism, integrating AI poses unique challenges and
opportunities, particularly in enhancing the quality and efficiency of news
reporting. This study explores how LLMs and LMMs can assist journalistic
practice by generating contextualised captions for images accompanying news
articles. We conducted experiments using the GoodNews dataset to evaluate the
ability of LMMs (BLIP-2, GPT-4v, or LLaVA) to incorporate one of two types of
context: entire news articles, or extracted named entities. In addition, we
compared their performance to a two-stage pipeline composed of a captioning
model (BLIP-2, OFA, or ViT-GPT2) with post-hoc contextualisation with LLMs
(GPT-4 or LLaMA). We assess a diversity of models, and we find that while the
choice of contextualisation model is a significant factor for the two-stage
pipelines, this is not the case in the LMMs, where smaller, open-source models
perform well compared to proprietary, GPT-powered ones. Additionally, we found
that controlling the amount of provided context enhances performance. These
results highlight the limitations of a fully automated approach and underscore
the necessity for an interactive, human-in-the-loop strategy.","[{'name': 'Aliki Anagnostopoulou'}, {'name': 'Thiago Gouvea'}, {'name': 'Daniel Sonntag'}]",2024-08-08T09:31:24Z
http://arxiv.org/abs/2408.04686v1,http://arxiv.org/abs/2408.04686v1,"Multi-Turn Context Jailbreak Attack on Large Language Models From First
  Principles","Large language models (LLMs) have significantly enhanced the performance of
numerous applications, from intelligent conversations to text generation.
However, their inherent security vulnerabilities have become an increasingly
significant challenge, especially with respect to jailbreak attacks. Attackers
can circumvent the security mechanisms of these LLMs, breaching security
constraints and causing harmful outputs. Focusing on multi-turn semantic
jailbreak attacks, we observe that existing methods lack specific
considerations for the role of multiturn dialogues in attack strategies,
leading to semantic deviations during continuous interactions. Therefore, in
this paper, we establish a theoretical foundation for multi-turn attacks by
considering their support in jailbreak attacks, and based on this, propose a
context-based contextual fusion black-box jailbreak attack method, named
Context Fusion Attack (CFA). This method approach involves filtering and
extracting key terms from the target, constructing contextual scenarios around
these terms, dynamically integrating the target into the scenarios, replacing
malicious key terms within the target, and thereby concealing the direct
malicious intent. Through comparisons on various mainstream LLMs and red team
datasets, we have demonstrated CFA's superior success rate, divergence, and
harmfulness compared to other multi-turn attack strategies, particularly
showcasing significant advantages on Llama3 and GPT-4.","[{'name': 'Xiongtao Sun'}, {'name': 'Deyue Zhang'}, {'name': 'Dongdong Yang'}, {'name': 'Quanchen Zou'}, {'name': 'Hui Li'}]",2024-08-08T09:18:47Z
http://arxiv.org/abs/2408.04325v1,http://arxiv.org/abs/2408.04325v1,HydraFormer: One Encoder For All Subsampling Rates,"In automatic speech recognition, subsampling is essential for tackling
diverse scenarios. However, the inadequacy of a single subsampling rate to
address various real-world situations often necessitates training and deploying
multiple models, consequently increasing associated costs. To address this
issue, we propose HydraFormer, comprising HydraSub, a Conformer-based encoder,
and a BiTransformer-based decoder. HydraSub encompasses multiple branches, each
representing a distinct subsampling rate, allowing for the flexible selection
of any branch during inference based on the specific use case. HydraFormer can
efficiently manage different subsampling rates, significantly reducing training
and deployment expenses. Experiments on AISHELL-1 and LibriSpeech datasets
reveal that HydraFormer effectively adapts to various subsampling rates and
languages while maintaining high recognition performance. Additionally,
HydraFormer showcases exceptional stability, sustaining consistent performance
under various initialization conditions, and exhibits robust transferability by
learning from pretrained single subsampling rate automatic speech recognition
models\footnote{Model code and scripts:
https://github.com/HydraFormer/hydraformer}.","[{'name': 'Yaoxun Xu'}, {'name': 'Xingchen Song'}, {'name': 'Zhiyong Wu'}, {'name': 'Di Wu'}, {'name': 'Zhendong Peng'}, {'name': 'Binbin Zhang'}]",2024-08-08T09:08:27Z
http://arxiv.org/abs/2408.04303v1,http://arxiv.org/abs/2408.04303v1,"Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language
  Adaptation of LLMs for Low-Resource NLP","The development of monolingual language models for low and mid-resource
languages continues to be hindered by the difficulty in sourcing high-quality
training data. In this study, we present a novel cross-lingual vocabulary
transfer strategy, trans-tokenization, designed to tackle this challenge and
enable more efficient language adaptation. Our approach focuses on adapting a
high-resource monolingual LLM to an unseen target language by initializing the
token embeddings of the target language using a weighted average of
semantically similar token embeddings from the source language. For this, we
leverage a translation resource covering both the source and target languages.
We validate our method with the Tweeties, a series of trans-tokenized LLMs, and
demonstrate their competitive performance on various downstream tasks across a
small but diverse set of languages. Additionally, we introduce Hydra LLMs,
models with multiple swappable language modeling heads and embedding tables,
which further extend the capabilities of our trans-tokenization strategy. By
designing a Hydra LLM based on the multilingual model TowerInstruct, we
developed a state-of-the-art machine translation model for Tatar, in a
zero-shot manner, completely bypassing the need for high-quality parallel data.
This breakthrough is particularly significant for low-resource languages like
Tatar, where high-quality parallel data is hard to come by. By lowering the
data and time requirements for training high-quality models, our
trans-tokenization strategy allows for the development of LLMs for a wider
range of languages, especially those with limited resources. We hope that our
work will inspire further research and collaboration in the field of
cross-lingual vocabulary transfer and contribute to the empowerment of
languages on a global scale.","[{'name': 'François Remy'}, {'name': 'Pieter Delobelle'}, {'name': 'Hayastan Avetisyan'}, {'name': 'Alfiya Khabibullina'}, {'name': 'Miryam de Lhoneux'}, {'name': 'Thomas Demeester'}]",2024-08-08T08:37:28Z
http://arxiv.org/abs/2408.04293v1,http://arxiv.org/abs/2408.04293v1,"Are Social Sentiments Inherent in LLMs? An Empirical Study on Extraction
  of Inter-demographic Sentiments","Large language models (LLMs) are supposed to acquire unconscious human
knowledge and feelings, such as social common sense and biases, by training
models from large amounts of text. However, it is not clear how much the
sentiments of specific social groups can be captured in various LLMs. In this
study, we focus on social groups defined in terms of nationality, religion, and
race/ethnicity, and validate the extent to which sentiments between social
groups can be captured in and extracted from LLMs. Specifically, we input
questions regarding sentiments from one group to another into LLMs, apply
sentiment analysis to the responses, and compare the results with social
surveys. The validation results using five representative LLMs showed higher
correlations with relatively small p-values for nationalities and religions,
whose number of data points were relatively large. This result indicates that
the LLM responses including the inter-group sentiments align well with actual
social survey results.","[{'name': 'Kunitomo Tanaka'}, {'name': 'Ryohei Sasano'}, {'name': 'Koichi Takeda'}]",2024-08-08T08:13:25Z
http://arxiv.org/abs/2408.04289v1,http://arxiv.org/abs/2408.04289v1,EMTeC: A Corpus of Eye Movements on Machine-Generated Texts,"The Eye Movements on Machine-Generated Texts Corpus (EMTeC) is a naturalistic
eye-movements-while-reading corpus of 107 native English speakers reading
machine-generated texts. The texts are generated by three large language models
using five different decoding strategies, and they fall into six different text
type categories. EMTeC entails the eye movement data at all stages of
pre-processing, i.e., the raw coordinate data sampled at 2000 Hz, the fixation
sequences, and the reading measures. It further provides both the original and
a corrected version of the fixation sequences, accounting for vertical
calibration drift. Moreover, the corpus includes the language models' internals
that underlie the generation of the stimulus texts: the transition scores, the
attention scores, and the hidden states. The stimuli are annotated for a range
of linguistic features both at text and at word level. We anticipate EMTeC to
be utilized for a variety of use cases such as, but not restricted to, the
investigation of reading behavior on machine-generated text and the impact of
different decoding strategies; reading behavior on different text types; the
development of new pre-processing, data filtering, and drift correction
algorithms; the cognitive interpretability and enhancement of language models;
and the assessment of the predictive power of surprisal and entropy for human
reading times. The data at all stages of pre-processing, the model internals,
and the code to reproduce the stimulus generation, data pre-processing and
analyses can be accessed via https://github.com/DiLi-Lab/EMTeC/.","[{'name': 'Lena Sophia Bolliger'}, {'name': 'Patrick Haller'}, {'name': 'Isabelle Caroline Rose Cretton'}, {'name': 'David Robert Reich'}, {'name': 'Tannon Kew'}, {'name': 'Lena Ann Jäger'}]",2024-08-08T08:00:45Z
http://arxiv.org/abs/2408.04284v1,http://arxiv.org/abs/2408.04284v1,LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection,"The widespread accessibility of large language models (LLMs) to the general
public has significantly amplified the dissemination of machine-generated texts
(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in
discerning the origin of a text (human-authored vs machinegenerated). This
raises concerns regarding the potential misuse of MGTs, particularly within
educational and academic domains. In this paper, we present
$\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.
It is able to classify texts into four categories: human-written,
machine-generated, machine-written machine-humanized, and human-written
machine-polished. Contrary to previous MGT detectors that perform binary
classification, introducing two additional categories in LLM-DetectiAIve offers
insights into the varying degrees of LLM intervention during the text creation.
This might be useful in some domains like education, where any LLM intervention
is usually prohibited. Experiments show that LLM-DetectAIve can effectively
identify the authorship of textual content, proving its usefulness in enhancing
integrity in education, academia, and other domains. LLM-DetectAIve is publicly
accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video
describing our system is available at https://youtu.be/E8eT_bE7k8c.","[{'name': 'Mervat Abassy'}, {'name': 'Kareem Elozeiri'}, {'name': 'Alexander Aziz'}, {'name': 'Minh Ngoc Ta'}, {'name': 'Raj Vardhan Tomar'}, {'name': 'Bimarsha Adhikari'}, {'name': 'Saad El Dine Ahmed'}, {'name': 'Yuxia Wang'}, {'name': 'Osama Mohammed Afzal'}, {'name': 'Zhuohan Xie'}, {'name': 'Jonibek Mansurov'}, {'name': 'Ekaterina Artemova'}, {'name': 'Vladislav Mikhailov'}, {'name': 'Rui Xing'}, {'name': 'Jiahui Geng'}, {'name': 'Hasan Iqbal'}, {'name': 'Zain Muhammad Mujahid'}, {'name': 'Tarek Mahmoud'}, {'name': 'Akim Tsvigun'}, {'name': 'Alham Fikri Aji'}, {'name': 'Artem Shelmanov'}, {'name': 'Nizar Habash'}, {'name': 'Iryna Gurevych'}, {'name': 'Preslav Nakov'}]",2024-08-08T07:43:17Z
http://arxiv.org/abs/2408.04278v1,http://arxiv.org/abs/2408.04278v1,LaDiMo: Layer-wise Distillation Inspired MoEfier,"The advent of large language models has revolutionized natural language
processing, but their increasing complexity has led to substantial training
costs, resource demands, and environmental impacts. In response, sparse
Mixture-of-Experts (MoE) models have emerged as a promising alternative to
dense models. Since training MoE models from scratch can be prohibitively
expensive, recent studies have explored leveraging knowledge from pre-trained
non-MoE models. However, existing approaches have limitations, such as
requiring significant hardware resources and data. We propose a novel
algorithm, LaDiMo, which efficiently converts a Transformer-based non-MoE model
into a MoE model with minimal additional training cost. LaDiMo consists of two
stages: layer-wise expert construction and routing policy decision. By
harnessing the concept of Knowledge Distillation, we compress the model and
rapidly recover its performance. Furthermore, we develop an adaptive router
that optimizes inference efficiency by profiling the distribution of routing
weights and determining a layer-wise policy that balances accuracy and latency.
We demonstrate the effectiveness of our method by converting the LLaMA2-7B
model to a MoE model using only 100K tokens, reducing activated parameters by
over 20% while keeping accuracy. Our approach offers a flexible and efficient
solution for building and deploying MoE models.","[{'name': 'Sungyoon Kim'}, {'name': 'Youngjun Kim'}, {'name': 'Kihyo Moon'}, {'name': 'Minsung Jang'}]",2024-08-08T07:37:26Z
http://arxiv.org/abs/2408.04270v1,http://arxiv.org/abs/2408.04270v1,"Analysis of Argument Structure Constructions in the Large Language Model
  BERT","This study investigates how BERT processes and represents Argument Structure
Constructions (ASCs), extending previous LSTM analyses. Using a dataset of 2000
sentences across four ASC types (transitive, ditransitive, caused-motion,
resultative), we analyzed BERT's token embeddings across 12 layers.
Visualizations with MDS and t-SNE and clustering quantified by Generalized
Discrimination Value (GDV) were used. Feedforward classifiers (probes)
predicted construction categories from embeddings. CLS token embeddings
clustered best in layers 2-4, decreased in intermediate layers, and slightly
increased in final layers. DET and SUBJ embeddings showed consistent clustering
in intermediate layers, VERB embeddings increased in clustering from layer 1 to
12, and OBJ embeddings peaked in layer 10. Probe accuracies indicated low
construction information in layer 1, with over 90 percent accuracy from layer 2
onward, revealing latent construction information beyond GDV clustering. Fisher
Discriminant Ratio (FDR) analysis of attention weights showed OBJ tokens were
crucial for differentiating ASCs, followed by VERB and DET tokens. SUBJ, CLS,
and SEP tokens had insignificant FDR scores. This study highlights BERT's
layered processing of linguistic constructions and its differences from LSTMs.
Future research will compare these findings with neuroimaging data to
understand the neural correlates of ASC processing. This research underscores
neural language models' potential to mirror linguistic processing in the human
brain, offering insights into the computational and neural mechanisms
underlying language understanding.","[{'name': 'Pegah Ramezani'}, {'name': 'Achim Schilling'}, {'name': 'Patrick Krauss'}]",2024-08-08T07:12:46Z
http://arxiv.org/abs/2408.04259v1,http://arxiv.org/abs/2408.04259v1,EfficientRAG: Efficient Retriever for Multi-Hop Question Answering,"Retrieval-augmented generation (RAG) methods encounter difficulties when
addressing complex questions like multi-hop queries. While iterative retrieval
methods improve performance by gathering additional information, current
approaches often rely on multiple calls of large language models (LLMs). In
this paper, we introduce EfficientRAG, an efficient retriever for multi-hop
question answering. EfficientRAG iteratively generates new queries without the
need for LLM calls at each iteration and filters out irrelevant information.
Experimental results demonstrate that EfficientRAG surpasses existing RAG
methods on three open-domain multi-hop question-answering datasets.","[{'name': 'Ziyuan Zhuang'}, {'name': 'Zhiyang Zhang'}, {'name': 'Sitao Cheng'}, {'name': 'Fangkai Yang'}, {'name': 'Jia Liu'}, {'name': 'Shujian Huang'}, {'name': 'Qingwei Lin'}, {'name': 'Saravan Rajmohan'}, {'name': 'Dongmei Zhang'}, {'name': 'Qi Zhang'}]",2024-08-08T06:57:49Z
http://arxiv.org/abs/2408.04246v1,http://arxiv.org/abs/2408.04246v1,Explicating the Implicit: Argument Detection Beyond Sentence Boundaries,"Detecting semantic arguments of a predicate word has been conventionally
modeled as a sentence-level task. The typical reader, however, perfectly
interprets predicate-argument relations in a much wider context than just the
sentence where the predicate was evoked. In this work, we reformulate the
problem of argument detection through textual entailment to capture semantic
relations across sentence boundaries. We propose a method that tests whether
some semantic relation can be inferred from a full passage by first encoding it
into a simple and standalone proposition and then testing for entailment
against the passage. Our method does not require direct supervision, which is
generally absent due to dataset scarcity, but instead builds on existing NLI
and sentence-level SRL resources. Such a method can potentially explicate
pragmatically understood relations into a set of explicit sentences. We
demonstrate it on a recent document-level benchmark, outperforming some
supervised methods and contemporary language models.","[{'name': 'Paul Roit'}, {'name': 'Aviv Slobodkin'}, {'name': 'Eran Hirsch'}, {'name': 'Arie Cattan'}, {'name': 'Ayal Klein'}, {'name': 'Valentina Pyatkin'}, {'name': 'Ido Dagan'}]",2024-08-08T06:18:24Z
http://arxiv.org/abs/2408.04237v1,http://arxiv.org/abs/2408.04237v1,Learning to Rewrite: Generalized LLM-Generated Text Detection,"Large language models (LLMs) can be abused at scale to create non-factual
content and spread disinformation. Detecting LLM-generated content is essential
to mitigate these risks, but current classifiers often fail to generalize in
open-world contexts. Prior work shows that LLMs tend to rewrite LLM-generated
content less frequently, which can be used for detection and naturally
generalizes to unforeseen data. However, we find that the rewriting edit
distance between human and LLM content can be indistinguishable across domains,
leading to detection failures. We propose training an LLM to rewrite input
text, producing minimal edits for LLM-generated content and more edits for
human-written text, deriving a distinguishable and generalizable edit distance
difference across different domains. Experiments on text from 21 independent
domains and three popular LLMs (e.g., GPT-4o, Gemini, and Llama-3) show that
our classifier outperforms the state-of-the-art zero-shot classifier by up to
20.6% on AUROC score and the rewriting classifier by 9.2% on F1 score. Our work
suggests that LLM can effectively detect machine-generated text if they are
trained properly.","[{'name': 'Wei Hao'}, {'name': 'Ran Li'}, {'name': 'Weiliang Zhao'}, {'name': 'Junfeng Yang'}, {'name': 'Chengzhi Mao'}]",2024-08-08T05:53:39Z
http://arxiv.org/abs/2408.04682v1,http://arxiv.org/abs/2408.04682v1,"ToolSandbox: A Stateful, Conversational, Interactive Evaluation
  Benchmark for LLM Tool Use Capabilities","Recent large language models (LLMs) advancements sparked a growing research
interest in tool assisted LLMs solving real-world challenges, which calls for
comprehensive evaluation of tool-use capabilities. While previous works focused
on either evaluating over stateless web services (RESTful API), based on a
single turn user prompt, or an off-policy dialog trajectory, ToolSandbox
includes stateful tool execution, implicit state dependencies between tools, a
built-in user simulator supporting on-policy conversational evaluation and a
dynamic evaluation strategy for intermediate and final milestones over an
arbitrary trajectory. We show that open source and proprietary models have a
significant performance gap, and complex tasks like State Dependency,
Canonicalization and Insufficient Information defined in ToolSandbox are
challenging even the most capable SOTA LLMs, providing brand-new insights into
tool-use LLM capabilities. ToolSandbox evaluation framework is released at
https://github.com/apple/ToolSandbox","[{'name': 'Jiarui Lu'}, {'name': 'Thomas Holleis'}, {'name': 'Yizhe Zhang'}, {'name': 'Bernhard Aumayer'}, {'name': 'Feng Nan'}, {'name': 'Felix Bai'}, {'name': 'Shuang Ma'}, {'name': 'Shen Ma'}, {'name': 'Mengyu Li'}, {'name': 'Guoli Yin'}, {'name': 'Zirui Wang'}, {'name': 'Ruoming Pang'}]",2024-08-08T05:45:42Z
http://arxiv.org/abs/2408.04226v2,http://arxiv.org/abs/2408.04226v2,"Evaluating Language Model Math Reasoning via Grounding in Educational
  Curricula","Our work presents a novel angle for evaluating language models' (LMs)
mathematical abilities, by investigating whether they can discern skills and
concepts enabled by math content. We contribute two datasets: one consisting of
385 fine-grained descriptions of K-12 math skills and concepts, or standards,
from Achieve the Core (ATC), and another of 9.9K problems labeled with these
standards (MathFish). Working with experienced teachers, we find that LMs
struggle to tag and verify standards linked to problems, and instead predict
labels that are close to ground truth, but differ in subtle ways. We also show
that LMs often generate problems that do not fully align with standards
described in prompts. Finally, we categorize problems in GSM8k using math
standards, allowing us to better understand why some problems are more
difficult to solve for models than others.","[{'name': 'Li Lucy'}, {'name': 'Tal August'}, {'name': 'Rose E. Wang'}, {'name': 'Luca Soldaini'}, {'name': 'Courtney Allison'}, {'name': 'Kyle Lo'}]",2024-08-08T05:28:34Z
http://arxiv.org/abs/2408.04220v1,http://arxiv.org/abs/2408.04220v1,Diffusion Guided Language Modeling,"Current language models demonstrate remarkable proficiency in text
generation. However, for many applications it is desirable to control
attributes, such as sentiment, or toxicity, of the generated language --
ideally tailored towards each specific use case and target audience. For
auto-regressive language models, existing guidance methods are prone to
decoding errors that cascade during generation and degrade performance. In
contrast, text diffusion models can easily be guided with, for example, a
simple linear sentiment classifier -- however they do suffer from significantly
higher perplexity than auto-regressive alternatives. In this paper we use a
guided diffusion model to produce a latent proposal that steers an
auto-regressive language model to generate text with desired properties. Our
model inherits the unmatched fluency of the auto-regressive approach and the
plug-and-play flexibility of diffusion. We show that it outperforms previous
plug-and-play guidance methods across a wide range of benchmark data sets.
Further, controlling a new attribute in our framework is reduced to training a
single logistic regression classifier.","[{'name': 'Justin Lovelace'}, {'name': 'Varsha Kishore'}, {'name': 'Yiwei Chen'}, {'name': 'Kilian Q. Weinberger'}]",2024-08-08T05:06:22Z
http://arxiv.org/abs/2408.04217v1,http://arxiv.org/abs/2408.04217v1,"Simplifying Translations for Children: Iterative Simplification
  Considering Age of Acquisition with LLMs","In recent years, neural machine translation (NMT) has been widely used in
everyday life. However, the current NMT lacks a mechanism to adjust the
difficulty level of translations to match the user's language level.
Additionally, due to the bias in the training data for NMT, translations of
simple source sentences are often produced with complex words. In particular,
this could pose a problem for children, who may not be able to understand the
meaning of the translations correctly. In this study, we propose a method that
replaces words with high Age of Acquisitions (AoA) in translations with simpler
words to match the translations to the user's level. We achieve this by using
large language models (LLMs), providing a triple of a source sentence, a
translation, and a target word to be replaced. We create a benchmark dataset
using back-translation on Simple English Wikipedia. The experimental results
obtained from the dataset show that our method effectively replaces high-AoA
words with lower-AoA words and, moreover, can iteratively replace most of the
high-AoA words while still maintaining high BLEU and COMET scores.","[{'name': 'Masashi Oshika'}, {'name': 'Makoto Morishita'}, {'name': 'Tsutomu Hirao'}, {'name': 'Ryohei Sasano'}, {'name': 'Koichi Takeda'}]",2024-08-08T04:57:36Z
http://arxiv.org/abs/2408.04681v1,http://arxiv.org/abs/2408.04681v1,"Conversational AI Powered by Large Language Models Amplifies False
  Memories in Witness Interviews","This study examines the impact of AI on human false memories -- recollections
of events that did not occur or deviate from actual occurrences. It explores
false memory induction through suggestive questioning in Human-AI interactions,
simulating crime witness interviews. Four conditions were tested: control,
survey-based, pre-scripted chatbot, and generative chatbot using a large
language model (LLM). Participants (N=200) watched a crime video, then
interacted with their assigned AI interviewer or survey, answering questions
including five misleading ones. False memories were assessed immediately and
after one week. Results show the generative chatbot condition significantly
increased false memory formation, inducing over 3 times more immediate false
memories than the control and 1.7 times more than the survey method. 36.4% of
users' responses to the generative chatbot were misled through the interaction.
After one week, the number of false memories induced by generative chatbots
remained constant. However, confidence in these false memories remained higher
than the control after one week. Moderating factors were explored: users who
were less familiar with chatbots but more familiar with AI technology, and more
interested in crime investigations, were more susceptible to false memories.
These findings highlight the potential risks of using advanced AI in sensitive
contexts, like police interviews, emphasizing the need for ethical
considerations.","[{'name': 'Samantha Chan'}, {'name': 'Pat Pataranutaporn'}, {'name': 'Aditya Suri'}, {'name': 'Wazeer Zulfikar'}, {'name': 'Pattie Maes'}, {'name': 'Elizabeth F. Loftus'}]",2024-08-08T04:55:03Z
http://arxiv.org/abs/2408.04216v1,http://arxiv.org/abs/2408.04216v1,"Attention Mechanism and Context Modeling System for Text Mining Machine
  Translation","This paper advances a novel architectural schema anchored upon the
Transformer paradigm and innovatively amalgamates the K-means categorization
algorithm to augment the contextual apprehension capabilities of the schema.
The transformer model performs well in machine translation tasks due to its
parallel computing power and multi-head attention mechanism. However, it may
encounter contextual ambiguity or ignore local features when dealing with
highly complex language structures. To circumvent this constraint, this
exposition incorporates the K-Means algorithm, which is used to stratify the
lexis and idioms of the input textual matter, thereby facilitating superior
identification and preservation of the local structure and contextual
intelligence of the language. The advantage of this combination is that K-Means
can automatically discover the topic or concept regions in the text, which may
be directly related to translation quality. Consequently, the schema contrived
herein enlists K-Means as a preparatory phase antecedent to the Transformer and
recalibrates the multi-head attention weights to assist in the discrimination
of lexis and idioms bearing analogous semantics or functionalities. This
ensures the schema accords heightened regard to the contextual intelligence
embodied by these clusters during the training phase, rather than merely
focusing on locational intelligence.","[{'name': 'Shi Bo'}, {'name': 'Yuwei Zhang'}, {'name': 'Junming Huang'}, {'name': 'Sitong Liu'}, {'name': 'Zexi Chen'}, {'name': 'Zizheng Li'}]",2024-08-08T04:52:10Z
http://arxiv.org/abs/2408.04680v1,http://arxiv.org/abs/2408.04680v1,Dynamic Fog Computing for Enhanced LLM Execution in Medical Applications,"The ability of large language models (LLMs) to transform, interpret, and
comprehend vast quantities of heterogeneous data presents a significant
opportunity to enhance data-driven care delivery. However, the sensitive nature
of protected health information (PHI) raises valid concerns about data privacy
and trust in remote LLM platforms. In addition, the cost associated with
cloud-based artificial intelligence (AI) services continues to impede
widespread adoption. To address these challenges, we propose a shift in the LLM
execution environment from opaque, centralized cloud providers to a
decentralized and dynamic fog computing architecture. By executing open-weight
LLMs in more trusted environments, such as the user's edge device or a fog
layer within a local network, we aim to mitigate the privacy, trust, and
financial challenges associated with cloud-based LLMs. We further present
SpeziLLM, an open-source framework designed to facilitate rapid and seamless
leveraging of different LLM execution layers and lowering barriers to LLM
integration in digital health applications. We demonstrate SpeziLLM's broad
applicability across six digital health applications, showcasing its
versatility in various healthcare settings.","[{'name': 'Philipp Zagar'}, {'name': 'Vishnu Ravi'}, {'name': 'Lauren Aalami'}, {'name': 'Stephan Krusche'}, {'name': 'Oliver Aalami'}, {'name': 'Paul Schmiedmayer'}]",2024-08-08T04:49:21Z
http://arxiv.org/abs/2408.04211v1,http://arxiv.org/abs/2408.04211v1,MMREC: LLM Based Multi-Modal Recommender System,"The importance of recommender systems is growing rapidly due to the
exponential increase in the volume of content generated daily. This surge in
content presents unique challenges for designing effective recommender systems.
Key among these challenges is the need to effectively leverage the vast amounts
of natural language data and images that represent user preferences. This paper
presents a novel approach to enhancing recommender systems by leveraging Large
Language Models (LLMs) and deep learning techniques. The proposed framework
aims to improve the accuracy and relevance of recommendations by incorporating
multi-modal information processing and by the use of unified latent space
representation. The study explores the potential of LLMs to better understand
and utilize natural language data in recommendation contexts, addressing the
limitations of previous methods. The framework efficiently extracts and
integrates text and image information through LLMs, unifying diverse modalities
in a latent space to simplify the learning process for the ranking model.
Experimental results demonstrate the enhanced discriminative power of the model
when utilizing multi-modal information. This research contributes to the
evolving field of recommender systems by showcasing the potential of LLMs and
multi-modal data integration to create more personalized and contextually
relevant recommendations.","[{'name': 'Jiahao Tian'}, {'name': 'Jinman Zhao'}, {'name': 'Zhenkai Wang'}, {'name': 'Zhicheng Ding'}]",2024-08-08T04:31:29Z
http://arxiv.org/abs/2408.04679v1,http://arxiv.org/abs/2408.04679v1,"Towards Linguistic Neural Representation Learning and Sentence Retrieval
  from Electroencephalogram Recordings","Decoding linguistic information from non-invasive brain signals using EEG has
gained increasing research attention due to its vast applicational potential.
Recently, a number of works have adopted a generative-based framework to decode
electroencephalogram (EEG) signals into sentences by utilizing the power
generative capacity of pretrained large language models (LLMs). However, this
approach has several drawbacks that hinder the further development of
linguistic applications for brain-computer interfaces (BCIs). Specifically, the
ability of the EEG encoder to learn semantic information from EEG data remains
questionable, and the LLM decoder's tendency to generate sentences based on its
training memory can be hard to avoid. These issues necessitate a novel approach
for converting EEG signals into sentences. In this paper, we propose a novel
two-step pipeline that addresses these limitations and enhances the validity of
linguistic EEG decoding research. We first confirm that word-level semantic
information can be learned from EEG data recorded during natural reading by
training a Conformer encoder via a masked contrastive objective for word-level
classification. To achieve sentence decoding results, we employ a training-free
retrieval method to retrieve sentences based on the predictions from the EEG
encoder. Extensive experiments and ablation studies were conducted in this
paper for a comprehensive evaluation of the proposed approach. Visualization of
the top prediction candidates reveals that our model effectively groups EEG
segments into semantic categories with similar meanings, thereby validating its
ability to learn patterns from unspoken EEG recordings. Despite the exploratory
nature of this work, these results suggest that our method holds promise for
providing more reliable solutions for converting EEG signals into text.","[{'name': 'Jinzhao Zhou'}, {'name': 'Yiqun Duan'}, {'name': 'Ziyi Zhao'}, {'name': 'Yu-Cheng Chang'}, {'name': 'Yu-Kai Wang'}, {'name': 'Thomas Do'}, {'name': 'Chin-Teng Lin'}]",2024-08-08T03:40:25Z
http://arxiv.org/abs/2408.04678v1,http://arxiv.org/abs/2408.04678v1,"CREST: Effectively Compacting a Datastore For Retrieval-Based
  Speculative Decoding","We present CREST (Compact Retrieval-Based Speculative Decoding), a redesign
of REST that allows it to be effectively ""compacted"". REST is a drafting
technique for speculative decoding based on retrieving exact n-gram matches of
the most recent n tokens generated by the target LLM from a datastore. The key
idea of CREST is to only store a subset of the smallest and most common n-grams
in the datastore with the hope of achieving comparable performance with less
storage space. We found that storing a subset of n-grams both reduces storage
space and improves performance. CREST matches REST's accepted token length with
10.6-13.5x less storage space and achieves a 16.5-17.1% higher acceptance
length than REST using the same storage space on the HumanEval and MT Bench
benchmarks.","[{'name': 'Sophia Ho'}, {'name': 'Jinsol Park'}, {'name': 'Patrick Wang'}]",2024-08-08T03:38:49Z
http://arxiv.org/abs/2408.04174v1,http://arxiv.org/abs/2408.04174v1,"wav2graph: A Framework for Supervised Learning Knowledge Graph from
  Speech","Knowledge graphs (KGs) enhance the performance of large language models
(LLMs) and search engines by providing structured, interconnected data that
improves reasoning and context-awareness. However, KGs only focus on text data,
thereby neglecting other modalities such as speech. In this work, we introduce
wav2graph, the first framework for supervised learning knowledge graph from
speech data. Our pipeline are straightforward: (1) constructing a KG based on
transcribed spoken utterances and a named entity database, (2) converting KG
into embedding vectors, and (3) training graph neural networks (GNNs) for node
classification and link prediction tasks. Through extensive experiments
conducted in inductive and transductive learning contexts using
state-of-the-art GNN models, we provide baseline results and error analysis for
node classification and link prediction tasks on human transcripts and
automatic speech recognition (ASR) transcripts, including evaluations using
both encoder-based and decoder-based node embeddings, as well as monolingual
and multilingual acoustic pre-trained models. All related code, data, and
models are published online.","[{'name': 'Khai Le-Duc'}, {'name': 'Quy-Anh Dang'}, {'name': 'Tan-Hanh Pham'}, {'name': 'Truong-Son Hy'}]",2024-08-08T02:36:04Z
http://arxiv.org/abs/2408.04167v1,http://arxiv.org/abs/2408.04167v1,mbrs: A Library for Minimum Bayes Risk Decoding,"Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks
that outperforms conventional maximum a posterior (MAP) decoding using beam
search by selecting high-quality outputs based on a utility function rather
than those with high-probability. Typically, it finds the most suitable
hypothesis from the set of hypotheses under the sampled pseudo-references. mbrs
is a library of MBR decoding, which can flexibly combine various metrics,
alternative expectation estimations, and algorithmic variants. It is designed
with a focus on speed measurement and calling count of code blocks,
transparency, reproducibility, and extensibility, which are essential for
researchers and developers. We published our mbrs as an MIT-licensed
open-source project, and the code is available on GitHub.
  GitHub: https://github.com/naist-nlp/mbrs","[{'name': 'Hiroyuki Deguchi'}, {'name': 'Yusuke Sakai'}, {'name': 'Hidetaka Kamigaito'}, {'name': 'Taro Watanabe'}]",2024-08-08T02:28:32Z
http://arxiv.org/abs/2408.04162v1,http://arxiv.org/abs/2408.04162v1,"Semantics or spelling? Probing contextual word embeddings with
  orthographic noise","Pretrained language model (PLM) hidden states are frequently employed as
contextual word embeddings (CWE): high-dimensional representations that encode
semantic information given linguistic context. Across many areas of
computational linguistics research, similarity between CWEs is interpreted as
semantic similarity. However, it remains unclear exactly what information is
encoded in PLM hidden states. We investigate this practice by probing PLM
representations using minimal orthographic noise. We expect that if CWEs
primarily encode semantic information, a single character swap in the input
word will not drastically affect the resulting representation,given sufficient
linguistic context. Surprisingly, we find that CWEs generated by popular PLMs
are highly sensitive to noise in input data, and that this sensitivity is
related to subword tokenization: the fewer tokens used to represent a word at
input, the more sensitive its corresponding CWE. This suggests that CWEs
capture information unrelated to word-level meaning and can be manipulated
through trivial modifications of input data. We conclude that these PLM-derived
CWEs may not be reliable semantic proxies, and that caution is warranted when
interpreting representational similarity","[{'name': 'Jacob A. Matthews'}, {'name': 'John R. Starr'}, {'name': 'Marten van Schijndel'}]",2024-08-08T02:07:25Z
http://arxiv.org/abs/2408.04140v1,http://arxiv.org/abs/2408.04140v1,UNLEARN Efficient Removal of Knowledge in Large Language Models,"Given the prevalence of large language models (LLMs) and the prohibitive cost
of training these models from scratch, dynamically forgetting specific
knowledge e.g., private or proprietary, without retraining the model has become
an important capability. This paper proposes a novel method to achieve this
objective called UNLEARN. The approach builds upon subspace methods to identify
and specifically target the removal of knowledge without adversely affecting
other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can
be forgotten while maintaining performance on other knowledge within 2.5% of
the original model, significantly outperforming the discriminatory abilities of
the previous state-of-the-art. A dual method called LEARN is also proposed for
targeted knowledge addition. Results show LEARN can match the fine-tuning
accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar
tasks.","[{'name': 'Tyler Lizzo'}, {'name': 'Larry Heck'}]",2024-08-08T00:53:31Z
http://arxiv.org/abs/2408.04121v1,http://arxiv.org/abs/2408.04121v1,"Can Rule-Based Insights Enhance LLMs for Radiology Report
  Classification? Introducing the RadPrompt Methodology","Developing imaging models capable of detecting pathologies from chest X-rays
can be cost and time-prohibitive for large datasets as it requires supervision
to attain state-of-the-art performance. Instead, labels extracted from
radiology reports may serve as distant supervision since these are routinely
generated as part of clinical practice. Despite their widespread use, current
rule-based methods for label extraction rely on extensive rule sets that are
limited in their robustness to syntactic variability. To alleviate these
limitations, we introduce RadPert, a rule-based system that integrates an
uncertainty-aware information schema with a streamlined set of rules, enhancing
performance. Additionally, we have developed RadPrompt, a multi-turn prompting
strategy that leverages RadPert to bolster the zero-shot predictive
capabilities of large language models, achieving a statistically significant
improvement in weighted average F1 score over GPT-4 Turbo. Most notably,
RadPrompt surpasses both its underlying models, showcasing the synergistic
potential of LLMs with rule-based models. We have evaluated our methods on two
English Corpora: the MIMIC-CXR gold-standard test set and a gold-standard
dataset collected from the Cambridge University Hospitals.","[{'name': 'Panagiotis Fytas'}, {'name': 'Anna Breger'}, {'name': 'Ian Selby'}, {'name': 'Simon Baker'}, {'name': 'Shahab Shahipasand'}, {'name': 'Anna Korhonen'}]",2024-08-07T23:09:23Z
http://arxiv.org/abs/2408.04114v1,http://arxiv.org/abs/2408.04114v1,Zero-shot Factual Consistency Evaluation Across Domains,"This work addresses the challenge of factual consistency in text generation
systems. We unify the tasks of Natural Language Inference, Summarization
Evaluation, Factuality Verification and Factual Consistency Evaluation to train
models capable of evaluating the factual consistency of source-target pairs
across diverse domains. We rigorously evaluate these against eight baselines on
a comprehensive benchmark suite comprising 22 datasets that span various tasks,
domains, and document lengths. Results demonstrate that our method achieves
state-of-the-art performance on this heterogeneous benchmark while addressing
efficiency concerns and attaining cross-domain generalization.",[{'name': 'Raunak Agarwal'}],2024-08-07T22:32:19Z
http://arxiv.org/abs/2408.04112v1,http://arxiv.org/abs/2408.04112v1,"Patchview: LLM-Powered Worldbuilding with Generative Dust and Magnet
  Visualization","Large language models (LLMs) can help writers build story worlds by
generating world elements, such as factions, characters, and locations.
However, making sense of many generated elements can be overwhelming. Moreover,
if the user wants to precisely control aspects of generated elements that are
difficult to specify verbally, prompting alone may be insufficient. We
introduce Patchview, a customizable LLM-powered system that visually aids
worldbuilding by allowing users to interact with story concepts and elements
through the physical metaphor of magnets and dust. Elements in Patchview are
visually dragged closer to concepts with high relevance, facilitating
sensemaking. The user can also steer the generation with verbally elusive
concepts by indicating the desired position of the element between concepts.
When the user disagrees with the LLM's visualization and generation, they can
correct those by repositioning the element. These corrections can be used to
align the LLM's future behaviors to the user's perception. With a user study,
we show that Patchview supports the sensemaking of world elements and steering
of element generation, facilitating exploration during the worldbuilding
process. Patchview provides insights on how customizable visual representation
can help sensemake, steer, and align generative AI model behaviors with the
user's intentions.","[{'name': 'John Joon Young Chung'}, {'name': 'Max Kreminski'}]",2024-08-07T22:27:19Z
http://arxiv.org/abs/2408.04093v3,http://arxiv.org/abs/2408.04093v3,"Tree Attention: Topology-aware Decoding for Long-Context Attention on
  GPU clusters","Self-attention is the core mathematical operation of modern transformer
architectures and is also a significant computational bottleneck due to its
quadratic complexity in the sequence length. In this work, we derive the scalar
energy function whose gradient computes the self-attention block, thus
elucidating the theoretical underpinnings of self-attention, providing a
Bayesian interpretation of the operation and linking it closely with
energy-based models such as Hopfield Networks. Our formulation reveals that the
reduction across the sequence axis can be efficiently computed in parallel
through a tree reduction. Our algorithm, for parallelizing attention
computation across multiple GPUs enables cross-device decoding to be performed
asymptotically faster (up to 8x faster in our experiments) than alternative
approaches such as Ring Attention, while also requiring significantly less
communication volume and incurring 2x less peak memory. Our code is publicly
available here: \url{https://github.com/Zyphra/tree_attention}.","[{'name': 'Vasudev Shyam'}, {'name': 'Jonathan Pilault'}, {'name': 'Emily Shepperd'}, {'name': 'Quentin Anthony'}, {'name': 'Beren Millidge'}]",2024-08-07T21:16:55Z
http://arxiv.org/abs/2408.04675v1,http://arxiv.org/abs/2408.04675v1,ACL Ready: RAG Based Assistant for the ACL Checklist,"The ARR Responsible NLP Research checklist website states that the ""checklist
is designed to encourage best practices for responsible research, addressing
issues of research ethics, societal impact and reproducibility."" Answering the
questions is an opportunity for authors to reflect on their work and make sure
any shared scientific assets follow best practices. Ideally, considering the
checklist before submission can favorably impact the writing of a research
paper. However, the checklist is often filled out at the last moment. In this
work, we introduce ACLReady, a retrieval-augmented language model application
that can be used to empower authors to reflect on their work and assist authors
with the ACL checklist. To test the effectiveness of the system, we conducted a
qualitative study with 13 users which shows that 92% of users found the
application useful and easy to use as well as 77% of the users found that the
application provided the information they expected. Our code is publicly
available under the CC BY-NC 4.0 license on GitHub.","[{'name': 'Michael Galarnyk'}, {'name': 'Rutwik Routu'}, {'name': 'Kosha Bheda'}, {'name': 'Priyanshu Mehta'}, {'name': 'Agam Shah'}, {'name': 'Sudheer Chava'}]",2024-08-07T21:07:13Z
http://arxiv.org/abs/2408.04029v1,http://arxiv.org/abs/2408.04029v1,"Human Speech Perception in Noise: Can Large Language Models Paraphrase
  to Improve It?","Large Language Models (LLMs) can generate text by transferring style
attributes like formality resulting in formal or informal text. However,
instructing LLMs to generate text that when spoken, is more intelligible in an
acoustically difficult environment, is an under-explored topic. We conduct the
first study to evaluate LLMs on a novel task of generating acoustically
intelligible paraphrases for better human speech perception in noise. Our
experiments in English demonstrated that with standard prompting, LLMs struggle
to control the non-textual attribute, i.e., acoustic intelligibility, while
efficiently capturing the desired textual attributes like semantic equivalence.
To remedy this issue, we propose a simple prompting approach,
prompt-and-select, which generates paraphrases by decoupling the desired
textual and non-textual attributes in the text generation pipeline. Our
approach resulted in a 40% relative improvement in human speech perception, by
paraphrasing utterances that are highly distorted in a listening condition with
babble noise at a signal-to-noise ratio (SNR) -5 dB. This study reveals the
limitation of LLMs in capturing non-textual attributes, and our proposed method
showcases the potential of using LLMs for better human speech perception in
noise.","[{'name': 'Anupama Chingacham'}, {'name': 'Miaoran Zhang'}, {'name': 'Vera Demberg'}, {'name': 'Dietrich Klakow'}]",2024-08-07T18:24:23Z
http://arxiv.org/abs/2408.04023v1,http://arxiv.org/abs/2408.04023v1,"Improving Large Language Model (LLM) fidelity through context-aware
  grounding: A systematic approach to reliability and veracity","As Large Language Models (LLMs) become increasingly sophisticated and
ubiquitous in natural language processing (NLP) applications, ensuring their
robustness, trustworthiness, and alignment with human values has become a
critical challenge. This paper presents a novel framework for contextual
grounding in textual models, with a particular emphasis on the Context
Representation stage. Our approach aims to enhance the reliability and ethical
alignment of these models through a comprehensive, context-aware methodology.
By explicitly capturing and representing relevant situational, cultural, and
ethical contexts in a machine-readable format, we lay the foundation for
anchoring a model's behavior within these contexts. Our approach leverages
techniques from knowledge representation and reasoning, such as ontologies,
semantic web technologies, and logic-based formalisms. We evaluate our
framework on real-world textual datasets, demonstrating its effectiveness in
improving model performance, fairness, and alignment with human expectations,
while maintaining high accuracy. Furthermore, we discuss the other key
components of the framework, including context-aware encoding, context-aware
learning, interpretability and explainability, and continuous monitoring and
adaptation. This research contributes to the growing body of work on
responsible AI, offering a practical approach to developing more reliable,
trustworthy, and ethically-aligned language models. Our findings have
significant implications for the deployment of LLMs in sensitive domains such
as healthcare, legal systems, and social services, where contextual
understanding is paramount.","[{'name': 'Wrick Talukdar'}, {'name': 'Anjanava Biswas'}]",2024-08-07T18:12:02Z
http://arxiv.org/abs/2408.07081v1,http://arxiv.org/abs/2408.07081v1,"MathBridge: A Large-Scale Dataset for Translating Mathematical
  Expressions into Formula Images","Understanding sentences that contain mathematical expressions in text form
poses significant challenges. To address this, the importance of converting
these expressions into formula images has been highlighted. For instance, the
expression ``x equals minus b plus or minus the square root of b squared minus
four a c, all over two a'' is more readily comprehensible when displayed as an
image $x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$. To develop a text-to-image
conversion system, we can break down the process into text-to-LaTeX and
LaTeX-to-image conversions, with the latter being managed with by existing
various LaTeX engines. However, the former approach has been notably hindered
by the severe scarcity of text-to-LaTeX paired data, presenting a significant
challenge in the field.In this context, we introduce MathBridge, the first
extensive dataset for translating mathematical spoken English into LaTeX, which
aims to establish a robust baseline for future research in text-to-LaTeX
translation. MathBridge comprises approximately 23 million LaTeX formulas
paired with corresponding spoken English expressions. Through comprehensive
evaluations, including fine-tuning and testing with data, we discovered that
MathBridge significantly enhances pre-trained language models' capabilities for
text-to-LaTeX translation. Specifically, for the T5-large model, the sacreBLEU
score increased from 4.77 to 46.8, demonstrating substantial enhancement. Our
findings indicate the necessity for a new metric specifically for text-to-LaTeX
conversion evaluation.","[{'name': 'Kyudan Jung'}, {'name': 'Sieun Hyeon'}, {'name': 'Kwon Jeong Youn'}, {'name': 'Nam-Joon Kim'}, {'name': 'Hyun Gon Ryu'}, {'name': 'Hyuk-Jae Lee'}, {'name': 'Jaeyoung Do'}]",2024-08-07T18:07:15Z
http://arxiv.org/abs/2408.04015v1,http://arxiv.org/abs/2408.04015v1,Image-to-LaTeX Converter for Mathematical Formulas and Text,"In this project, we train a vision encoder-decoder model to generate LaTeX
code from images of mathematical formulas and text. Utilizing a diverse
collection of image-to-LaTeX data, we build two models: a base model with a
Swin Transformer encoder and a GPT-2 decoder, trained on machine-generated
images, and a fine-tuned version enhanced with Low-Rank Adaptation (LoRA)
trained on handwritten formulas. We then compare the BLEU performance of our
specialized model on a handwritten test set with other similar models, such as
Pix2Text, TexTeller, and Sumen. Through this project, we contribute open-source
models for converting images to LaTeX and provide from-scratch code for
building these models with distributed training and GPU optimizations.","[{'name': 'Daniil Gurgurov'}, {'name': 'Aleksey Morshnev'}]",2024-08-07T18:04:01Z
http://arxiv.org/abs/2408.03936v1,http://arxiv.org/abs/2408.03936v1,"SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic
  Performance for Mercosur Common Nomenclature","Natural language processing (NLP) has seen significant advancements with the
advent of large language models (LLMs). However, substantial improvements are
still needed for languages other than English, especially for specific domains
like the applications of Mercosur Common Nomenclature (NCM), a Brazilian
Harmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a
foundational Portuguese LLM, as an LLM source to implement the NCM application
processing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)
technique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.
This approach retains the chain-of-thought (CoT) methodology for prompt
development in a more concise and streamlined manner, utilizing brief and
focused documents for training. The proposed model demonstrates an efficient
and cost-effective alternative for fine-tuning smaller LLMs, significantly
outperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the
research focuses on NCM applications, the methodology can be easily adapted for
HS applications worldwide.","[{'name': 'Vinícius Di Oliveira'}, {'name': 'Yuri Façanha Bezerra'}, {'name': 'Li Weigang'}, {'name': 'Pedro Carvalho Brom'}, {'name': 'Victor Rafael R. Celestino'}]",2024-08-07T17:54:21Z
http://arxiv.org/abs/2408.03934v1,http://arxiv.org/abs/2408.03934v1,From Words to Worth: Newborn Article Impact Prediction with LLM,"As the academic landscape expands, the challenge of efficiently identifying
potentially high-impact articles among the vast number of newly published works
becomes critical. This paper introduces a promising approach, leveraging the
capabilities of fine-tuned LLMs to predict the future impact of newborn
articles solely based on titles and abstracts. Moving beyond traditional
methods heavily reliant on external information, the proposed method discerns
the shared semantic features of highly impactful papers from a large collection
of title-abstract and potential impact pairs. These semantic features are
further utilized to regress an improved metric, TNCSI_SP, which has been
endowed with value, field, and time normalization properties. Additionally, a
comprehensive dataset has been constructed and released for fine-tuning the
LLM, containing over 12,000 entries with corresponding titles, abstracts, and
TNCSI_SP. The quantitative results, with an NDCG@20 of 0.901, demonstrate that
the proposed approach achieves state-of-the-art performance in predicting the
impact of newborn articles when compared to competitive counterparts. Finally,
we demonstrate a real-world application for predicting the impact of newborn
journal articles to demonstrate its noteworthy practical value. Overall, our
findings challenge existing paradigms and propose a shift towards a more
content-focused prediction of academic impact, offering new insights for
assessing newborn article impact.","[{'name': 'Penghai Zhao'}, {'name': 'Qinghua Xing'}, {'name': 'Kairan Dou'}, {'name': 'Jinyu Tian'}, {'name': 'Ying Tai'}, {'name': 'Jian Yang'}, {'name': 'Ming-Ming Cheng'}, {'name': 'Xiang Li'}]",2024-08-07T17:52:02Z
http://arxiv.org/abs/2408.04673v1,http://arxiv.org/abs/2408.04673v1,AutoFAIR : Automatic Data FAIRification via Machine Reading,"The explosive growth of data fuels data-driven research, facilitating
progress across diverse domains. The FAIR principles emerge as a guiding
standard, aiming to enhance the findability, accessibility, interoperability,
and reusability of data. However, current efforts primarily focus on manual
data FAIRification, which can only handle targeted data and lack efficiency. To
address this issue, we propose AutoFAIR, an architecture designed to enhance
data FAIRness automately. Firstly, We align each data and metadata operation
with specific FAIR indicators to guide machine-executable actions. Then, We
utilize Web Reader to automatically extract metadata based on language models,
even in the absence of structured data webpage schemas. Subsequently, FAIR
Alignment is employed to make metadata comply with FAIR principles by ontology
guidance and semantic matching. Finally, by applying AutoFAIR to various data,
especially in the field of mountain hazards, we observe significant
improvements in findability, accessibility, interoperability, and reusability
of data. The FAIRness scores before and after applying AutoFAIR indicate
enhanced data value.","[{'name': 'Tingyan Ma'}, {'name': 'Wei Liu'}, {'name': 'Bin Lu'}, {'name': 'Xiaoying Gan'}, {'name': 'Yunqiang Zhu'}, {'name': 'Luoyi Fu'}, {'name': 'Chenghu Zhou'}]",2024-08-07T17:36:58Z
http://arxiv.org/abs/2408.03910v2,http://arxiv.org/abs/2408.03910v2,"CodexGraph: Bridging Large Language Models and Code Repositories via
  Code Graph Databases","Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval
and MBPP, but struggle with handling entire code repositories. This challenge
has prompted research on enhancing LLM-codebase interaction at a repository
scale. Current solutions rely on similarity-based retrieval or manual tools and
APIs, each with notable drawbacks. Similarity-based retrieval often has low
recall in complex tasks, while manual tools and APIs are typically
task-specific and require expert knowledge, reducing their generalizability
across diverse code tasks and real-world applications. To mitigate these
limitations, we introduce CodexGraph, a system that integrates LLM agents with
graph database interfaces extracted from code repositories. By leveraging the
structural properties of graph databases and the flexibility of the graph query
language, CodexGraph enables the LLM agent to construct and execute queries,
allowing for precise, code structure-aware context retrieval and code
navigation. We assess CodexGraph using three benchmarks: CrossCodeEval,
SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding
applications. With a unified graph database schema, CodexGraph demonstrates
competitive performance and potential in both academic and real-world
environments, showcasing its versatility and efficacy in software engineering.
Our application demo:
https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.","[{'name': 'Xiangyan Liu'}, {'name': 'Bo Lan'}, {'name': 'Zhiyuan Hu'}, {'name': 'Yang Liu'}, {'name': 'Zhicheng Zhang'}, {'name': 'Fei Wang'}, {'name': 'Michael Shieh'}, {'name': 'Wenmeng Zhou'}]",2024-08-07T17:13:59Z
http://arxiv.org/abs/2408.03907v1,http://arxiv.org/abs/2408.03907v1,"Decoding Biases: Automated Methods and LLM Judges for Gender Bias
  Detection in Language Models","Large Language Models (LLMs) have excelled at language understanding and
generating human-level text. However, even with supervised training and human
alignment, these LLMs are susceptible to adversarial attacks where malicious
users can prompt the model to generate undesirable text. LLMs also inherently
encode potential biases that can cause various harmful effects during
interactions. Bias evaluation metrics lack standards as well as consensus and
existing methods often rely on human-generated templates and annotations which
are expensive and labor intensive. In this work, we train models to
automatically create adversarial prompts to elicit biased responses from target
LLMs. We present LLM- based bias evaluation metrics and also analyze several
existing automatic evaluation methods and metrics. We analyze the various
nuances of model responses, identify the strengths and weaknesses of model
families, and assess where evaluation methods fall short. We compare these
metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns
with human judgement on bias in response generation.","[{'name': 'Shachi H Kumar'}, {'name': 'Saurav Sahay'}, {'name': 'Sahisnu Mazumder'}, {'name': 'Eda Okur'}, {'name': 'Ramesh Manuvinakurike'}, {'name': 'Nicole Beckage'}, {'name': 'Hsuan Su'}, {'name': 'Hung-yi Lee'}, {'name': 'Lama Nachman'}]",2024-08-07T17:11:34Z
http://arxiv.org/abs/2408.03900v1,http://arxiv.org/abs/2408.03900v1,Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond,"We present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU)
dataset comprising the speech counterpart for a portion of the MASSIVE textual
corpus. Speech-MASSIVE covers 12 languages from different families and inherits
from MASSIVE the annotations for the intent prediction and slot-filling tasks.
Our extension is prompted by the scarcity of massively multilingual SLU
datasets and the growing need for versatile speech datasets to assess
foundation models (LLMs, speech encoders) across languages and tasks. We
provide a multimodal, multitask, multilingual dataset and report SLU baselines
using both cascaded and end-to-end architectures in various training scenarios
(zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate the
suitability of Speech-MASSIVE for benchmarking other tasks such as speech
transcription, language identification, and speech translation. The dataset,
models, and code are publicly available at:
https://github.com/hlt-mt/Speech-MASSIVE","[{'name': 'Beomseok Lee'}, {'name': 'Ioan Calapodescu'}, {'name': 'Marco Gaido'}, {'name': 'Matteo Negri'}, {'name': 'Laurent Besacier'}]",2024-08-07T16:55:28Z
http://arxiv.org/abs/2408.03899v1,http://arxiv.org/abs/2408.03899v1,Simplifying Scholarly Abstracts for Accessible Digital Libraries,"Standing at the forefront of knowledge dissemination, digital libraries
curate vast collections of scientific literature. However, these scholarly
writings are often laden with jargon and tailored for domain experts rather
than the general public. As librarians, we strive to offer services to a
diverse audience, including those with lower reading levels. To extend our
services beyond mere access, we propose fine-tuning a language model to rewrite
scholarly abstracts into more comprehensible versions, thereby making scholarly
literature more accessible when requested. We began by introducing a corpus
specifically designed for training models to simplify scholarly abstracts. This
corpus consists of over three thousand pairs of abstracts and significance
statements from diverse disciplines. We then fine-tuned four language models
using this corpus. The outputs from the models were subsequently examined both
quantitatively for accessibility and semantic coherence, and qualitatively for
language quality, faithfulness, and completeness. Our findings show that the
resulting models can improve readability by over three grade levels, while
maintaining fidelity to the original content. Although commercial
state-of-the-art models still hold an edge, our models are much more compact,
can be deployed locally in an affordable manner, and alleviate the privacy
concerns associated with using commercial models. We envision this work as a
step toward more inclusive and accessible libraries, improving our services for
young readers and those without a college degree.","[{'name': 'Haining Wang'}, {'name': 'Jason Clark'}]",2024-08-07T16:55:00Z
http://arxiv.org/abs/2408.03874v1,http://arxiv.org/abs/2408.03874v1,Personalized Clinical Note Generation from Doctor-Patient Conversations,"In this work, we present a novel technique to improve the quality of draft
clinical notes for physicians. This technique is concentrated on the ability to
model implicit physician conversation styles and note preferences. We also
introduce a novel technique for the enrollment of new physicians when a limited
number of clinical notes paired with conversations are available for that
physician, without the need to re-train a model to support them. We show that
our technique outperforms the baseline model by improving the ROUGE-2 score of
the History of Present Illness section by 13.8%, the Physical Examination
section by 88.6%, and the Assessment & Plan section by 50.8%.","[{'name': 'Nathan Brake'}, {'name': 'Thomas Schaaf'}]",2024-08-07T16:24:01Z
http://arxiv.org/abs/2408.03871v1,http://arxiv.org/abs/2408.03871v1,"BeeManc at the PLABA Track of TAC-2023: Investigating LLMs and
  Controllable Attributes for Improving Biomedical Text Readability","In this system report, we describe the models and methods we used for our
participation in the PLABA2023 task on biomedical abstract simplification, part
of the TAC 2023 tracks. The system outputs we submitted come from the following
three categories: 1) domain fine-tuned T5-like models including Biomedical-T5
and Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes
(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work we
carried out for this task on BioGPT finetuning. In the official automatic
evaluation using SARI scores, BeeManc ranks 2nd among all teams and our model
LaySciFive ranks 3rd among all 13 evaluated systems. In the official human
evaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score
92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It
also produced a high score 91.57 on Fluency in comparison to the highest score
93.53. In the second round of submissions, our team using ChatGPT-prompting
ranks the 2nd in several categories including simplified term accuracy score
92.26 and completeness score 96.58, and a very similar score on faithfulness
score 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Our
codes, fine-tuned models, prompts, and data splits from the system development
stage will be available at https://github.com/ HECTA-UoM/PLABA-MU","[{'name': 'Zihao Li'}, {'name': 'Samuel Belkadi'}, {'name': 'Nicolo Micheletti'}, {'name': 'Lifeng Han'}, {'name': 'Matthew Shardlow'}, {'name': 'Goran Nenadic'}]",2024-08-07T16:21:41Z
http://arxiv.org/abs/2408.03855v1,http://arxiv.org/abs/2408.03855v1,Why transformers are obviously good models of language,"Nobody knows how language works, but many theories abound. Transformers are a
class of neural networks that process language automatically with more success
than alternatives, both those based on neural computations and those that rely
on other (e.g. more symbolic) mechanisms. Here, I highlight direct connections
between the transformer architecture and certain theoretical perspectives on
language. The empirical success of transformers relative to alternative models
provides circumstantial evidence that the linguistic approaches that
transformers embody should be, at least, evaluated with greater scrutiny by the
linguistics community and, at best, considered to be the currently best
available theories.",[{'name': 'Felix Hill'}],2024-08-07T15:52:46Z
http://arxiv.org/abs/2408.03849v1,http://arxiv.org/abs/2408.03849v1,"Hate Speech Detection and Classification in Amharic Text with Deep
  Learning","Hate speech is a growing problem on social media. It can seriously impact
society, especially in countries like Ethiopia, where it can trigger conflicts
among diverse ethnic and religious groups. While hate speech detection in
resource rich languages are progressing, for low resource languages such as
Amharic are lacking. To address this gap, we develop Amharic hate speech data
and SBi-LSTM deep learning model that can detect and classify text into four
categories of hate speech: racial, religious, gender, and non-hate speech. We
have annotated 5k Amharic social media post and comment data into four
categories. The data is annotated using a custom annotation tool by a total of
100 native Amharic speakers. The model achieves a 94.8 F1-score performance.
Future improvements will include expanding the dataset and develop state-of-the
art models.
  Keywords: Amharic hate speech detection, classification, Amharic dataset,
Deep Learning, SBi-LSTM","[{'name': 'Samuel Minale Gashe'}, {'name': 'Seid Muhie Yimam'}, {'name': 'Yaregal Assabie'}]",2024-08-07T15:46:45Z
http://arxiv.org/abs/2408.03837v2,http://arxiv.org/abs/2408.03837v2,"WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language
  Models","WalledEval is a comprehensive AI safety testing toolkit designed to evaluate
large language models (LLMs). It accommodates a diverse range of models,
including both open-weight and API-based ones, and features over 35 safety
benchmarks covering areas such as multilingual safety, exaggerated safety, and
prompt injections. The framework supports both LLM and judge benchmarking, and
incorporates custom mutators to test safety against various text-style
mutations such as future tense and paraphrasing. Additionally, WalledEval
introduces WalledGuard, a new, small and performant content moderation tool,
and SGXSTest, a benchmark for assessing exaggerated safety in cultural
contexts. We make WalledEval publicly available at
https://github.com/walledai/walledeval","[{'name': 'Prannaya Gupta'}, {'name': 'Le Qi Yau'}, {'name': 'Hao Han Low'}, {'name': 'I-Shiang Lee'}, {'name': 'Hugo Maximus Lim'}, {'name': 'Yu Xin Teoh'}, {'name': 'Jia Hng Koh'}, {'name': 'Dar Win Liew'}, {'name': 'Rishabh Bhardwaj'}, {'name': 'Rajat Bhardwaj'}, {'name': 'Soujanya Poria'}]",2024-08-07T15:22:44Z
http://arxiv.org/abs/2408.03819v1,http://arxiv.org/abs/2408.03819v1,"Leveraging Variation Theory in Counterfactual Data Augmentation for
  Optimized Active Learning","Active Learning (AL) allows models to learn interactively from user feedback.
This paper introduces a counterfactual data augmentation approach to AL,
particularly addressing the selection of datapoints for user querying, a
pivotal concern in enhancing data efficiency. Our approach is inspired by
Variation Theory, a theory of human concept learning that emphasizes the
essential features of a concept by focusing on what stays the same and what
changes. Instead of just querying with existing datapoints, our approach
synthesizes artificial datapoints that highlight potential key similarities and
differences among labels using a neuro-symbolic pipeline combining large
language models (LLMs) and rule-based models. Through an experiment in the
example domain of text classification, we show that our approach achieves
significantly higher performance when there are fewer annotated data. As the
annotated training data gets larger the impact of the generated data starts to
diminish showing its capability to address the cold start problem in AL. This
research sheds light on integrating theories of human learning into the
optimization of AL.","[{'name': 'Simret Araya Gebreegziabher'}, {'name': 'Kuangshi Ai'}, {'name': 'Zheng Zhang'}, {'name': 'Elena L. Glassman'}, {'name': 'Toby Jia-Jun Li'}]",2024-08-07T14:55:04Z
http://arxiv.org/abs/2408.03811v1,http://arxiv.org/abs/2408.03811v1,"Generative Language Models with Retrieval Augmented Generation for
  Automated Short Answer Scoring","Automated Short Answer Scoring (ASAS) is a critical component in educational
assessment. While traditional ASAS systems relied on rule-based algorithms or
complex deep learning methods, recent advancements in Generative Language
Models (GLMs) offer new opportunities for improvement. This study explores the
application of GLMs to ASAS, leveraging their off-the-shelf capabilities and
performance in various domains. We propose a novel pipeline that combines
vector databases, transformer-based encoders, and GLMs to enhance short answer
scoring accuracy. Our approach stores training responses in a vector database,
retrieves semantically similar responses during inference, and employs a GLM to
analyze these responses and determine appropriate scores. We further optimize
the system through fine-tuned retrieval processes and prompt engineering.
Evaluation on the SemEval 2013 dataset demonstrates a significant improvement
on the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,
highlighting the potential of GLMs in advancing ASAS technology.","[{'name': 'Zifan Wang'}, {'name': 'Christopher Ormerod'}]",2024-08-07T14:42:13Z
http://arxiv.org/abs/2408.04671v1,http://arxiv.org/abs/2408.04671v1,Prompt and Prejudice,"This paper investigates the impact of using first names in Large Language
Models (LLMs) and Vision Language Models (VLMs), particularly when prompted
with ethical decision-making tasks. We propose an approach that appends first
names to ethically annotated text scenarios to reveal demographic biases in
model outputs. Our study involves a curated list of more than 300 names
representing diverse genders and ethnic backgrounds, tested across thousands of
moral scenarios. Following the auditing methodologies from social sciences we
propose a detailed analysis involving popular LLMs/VLMs to contribute to the
field of responsible AI by emphasizing the importance of recognizing and
mitigating biases in these systems. Furthermore, we introduce a novel
benchmark, the Pratical Scenarios Benchmark (PSB), designed to assess the
presence of biases involving gender or demographic prejudices in everyday
decision-making scenarios as well as practical scenarios where an LLM might be
used to make sensible decisions (e.g., granting mortgages or insurances). This
benchmark allows for a comprehensive comparison of model behaviors across
different demographic categories, highlighting the risks and biases that may
arise in practical applications of LLMs and VLMs.","[{'name': 'Lorenzo Berlincioni'}, {'name': 'Luca Cultrera'}, {'name': 'Federico Becattini'}, {'name': 'Marco Bertini'}, {'name': 'Alberto Del Bimbo'}]",2024-08-07T14:11:33Z
http://arxiv.org/abs/2408.03762v1,http://arxiv.org/abs/2408.03762v1,"'Finance Wizard' at the FinLLM Challenge Task: Financial Text
  Summarization","This paper presents our participation under the team name `Finance Wizard' in
the FinNLP-AgentScen 2024 shared task #2: Financial Text Summarization. It
documents our pipeline approach of fine-tuning a foundation model into a
task-specific model for Financial Text Summarization. It involves (1) adapting
Llama3 8B, a foundation model, to the Finance domain via continued
pre-training, (2) multi-task instruction-tuning to further equip the model with
more finance-related capabilities, (3) finally fine-tuning the model into a
task-specific `expert'. Our model, FinLlama3\_sum, yielded commendable results,
securing the third position in its category with a ROUGE-1 score of 0.521.","[{'name': 'Meisin Lee'}, {'name': 'Soon Lay-Ki'}]",2024-08-07T13:31:44Z
http://arxiv.org/abs/2408.03732v1,http://arxiv.org/abs/2408.03732v1,"Question Rephrasing for Quantifying Uncertainty in Large Language
  Models: Applications in Molecular Chemistry Tasks","Uncertainty quantification enables users to assess the reliability of
responses generated by large language models (LLMs). We present a novel
Question Rephrasing technique to evaluate the input uncertainty of LLMs, which
refers to the uncertainty arising from equivalent variations of the inputs
provided to LLMs. This technique is integrated with sampling methods that
measure the output uncertainty of LLMs, thereby offering a more comprehensive
uncertainty assessment. We validated our approach on property prediction and
reaction prediction for molecular chemistry tasks.","[{'name': 'Zizhang Chen'}, {'name': 'Pengyu Hong'}, {'name': 'Sandeep Madireddy'}]",2024-08-07T12:38:23Z
http://arxiv.org/abs/2408.03706v1,http://arxiv.org/abs/2408.03706v1,"Local Topology Measures of Contextual Language Model Latent Spaces With
  Applications to Dialogue Term Extraction","A common approach for sequence tagging tasks based on contextual word
representations is to train a machine learning classifier directly on these
embedding vectors. This approach has two shortcomings. First, such methods
consider single input sequences in isolation and are unable to put an
individual embedding vector in relation to vectors outside the current local
context of use. Second, the high performance of these models relies on
fine-tuning the embedding model in conjunction with the classifier, which may
not always be feasible due to the size or inaccessibility of the underlying
feature-generation model. It is thus desirable, given a collection of embedding
vectors of a corpus, i.e., a datastore, to find features of each vector that
describe its relation to other, similar vectors in the datastore. With this in
mind, we introduce complexity measures of the local topology of the latent
space of a contextual language model with respect to a given datastore. The
effectiveness of our features is demonstrated through their application to
dialogue term extraction. Our work continues a line of research that explores
the manifold hypothesis for word embeddings, demonstrating that local structure
in the space carved out by word embeddings can be exploited to infer semantic
properties.","[{'name': 'Benjamin Matthias Ruppik'}, {'name': 'Michael Heck'}, {'name': 'Carel van Niekerk'}, {'name': 'Renato Vukovic'}, {'name': 'Hsien-chin Lin'}, {'name': 'Shutong Feng'}, {'name': 'Marcus Zibrowius'}, {'name': 'Milica Gašić'}]",2024-08-07T11:44:32Z
http://arxiv.org/abs/2408.03675v2,http://arxiv.org/abs/2408.03675v2,"NACL: A General and Effective KV Cache Eviction Framework for LLMs at
  Inference Time","Large Language Models (LLMs) have ignited an innovative surge of AI
applications, marking a new era of exciting possibilities equipped with
extended context windows. However, hosting these models is cost-prohibitive
mainly due to the extensive memory consumption of KV Cache involving
long-context modeling. Despite several works proposing to evict unnecessary
tokens from the KV Cache, most of them rely on the biased local statistics of
accumulated attention scores and report performance using unconvincing metric
like perplexity on inadequate short-text evaluation. In this paper, we propose
NACL, a general framework for long-context KV cache eviction that achieves more
optimal and efficient eviction in a single operation during the encoding phase.
Due to NACL's efficiency, we combine more accurate attention score statistics
in PROXY TOKENS EVICTION with the diversified random eviction strategy of
RANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance
the robustness in maintaining pivotal tokens for long-context modeling tasks.
Notably, our method significantly improves the performance on short- and
long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%
with over 95% performance maintenance. The code is available at
https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.","[{'name': 'Yilong Chen'}, {'name': 'Guoxia Wang'}, {'name': 'Junyuan Shang'}, {'name': 'Shiyao Cui'}, {'name': 'Zhenyu Zhang'}, {'name': 'Tingwen Liu'}, {'name': 'Shuohuan Wang'}, {'name': 'Yu Sun'}, {'name': 'Dianhai Yu'}, {'name': 'Hua Wu'}]",2024-08-07T10:31:07Z
http://arxiv.org/abs/2408.03652v1,http://arxiv.org/abs/2408.03652v1,"mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest
  Neighbor Search","Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)
that aims to identify and classify entities in text into predefined categories.
However, when applied to Arabic data, NER encounters unique challenges stemming
from the language's rich morphological inflections, absence of capitalization
cues, and spelling variants, where a single word can comprise multiple
morphemes. In this paper, we introduce Arabic KNN-NER, our submission to the
Wojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the
shared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained
flat-entity recognition for Arabic text, where we identify a single main entity
and possibly zero or multiple sub-entities for each word. Arabic KNN-NER
augments the probability distribution of a fine-tuned model with another label
probability distribution derived from performing a KNN search over the cached
training data. Our submission achieved 91% on the test set on the WojoodFine
dataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.","[{'name': 'Ahmed Abdou'}, {'name': 'Tasneem Mohsen'}]",2024-08-07T09:34:55Z
http://arxiv.org/abs/2408.03633v2,http://arxiv.org/abs/2408.03633v2,CARE: A Clue-guided Assistant for CSRs to Read User Manuals,"It is time-saving to build a reading assistant for customer service
representations (CSRs) when reading user manuals, especially information-rich
ones. Current solutions don't fit the online custom service scenarios well due
to the lack of attention to user questions and possible responses. Hence, we
propose to develop a time-saving and careful reading assistant for CSRs, named
CARE. It can help the CSRs quickly find proper responses from the user manuals
via explicit clue chains. Specifically, each of the clue chains is formed by
inferring over the user manuals, starting from the question clue aligned with
the user question and ending at a possible response. To overcome the shortage
of supervised data, we adopt the self-supervised strategy for model learning.
The offline experiment shows that CARE is efficient in automatically inferring
accurate responses from the user manual. The online experiment further
demonstrates the superiority of CARE to reduce CSRs' reading burden and keep
high service quality, in particular with >35% decrease in time spent and
keeping a >0.75 ICC score.","[{'name': 'Weihong Du'}, {'name': 'Jia Liu'}, {'name': 'Zujie Wen'}, {'name': 'Dingnan Jin'}, {'name': 'Hongru Liang'}, {'name': 'Wenqiang Lei'}]",2024-08-07T08:44:44Z
http://arxiv.org/abs/2408.03631v1,http://arxiv.org/abs/2408.03631v1,"Large Language Models for Base Station Siting: Intelligent Deployment
  based on Prompt or Agent","Traditional base station siting (BSS) methods rely heavily on drive testing
and user feedback, which are laborious and require extensive expertise in
communication, networking, and optimization. As large language models (LLMs)
and their associated technologies advance, particularly in the realms of prompt
engineering and agent engineering, network optimization will witness a
revolutionary approach. This approach entails the strategic use of well-crafted
prompts to infuse human experience and knowledge into these sophisticated LLMs,
and the deployment of autonomous agents as a communication bridge to seamlessly
connect the machine language based LLMs with human users using natural
language. This integration represents the future paradigm of artificial
intelligence (AI) as a service and AI for more ease. As a preliminary
exploration, this research first develops a novel LLM-empowered BSS
optimization framework, and heuristically proposes four different potential
implementations: the strategies based on Prompt-optimized LLM (PoL),
human-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and
Cooperative multiple LLM-based autonomous BSS agents (CLaBa). Through
evaluation on real-world data, the experiments demonstrate that prompt-assisted
LLMs and LLM-based agents can generate more efficient, cost-effective, and
reliable network deployments, noticeably enhancing the efficiency of BSS
optimization and reducing trivial manual participation.","[{'name': 'Yanhu Wang'}, {'name': 'Muhammad Muzammil Afzal'}, {'name': 'Zhengyang Li'}, {'name': 'Jie Zhou'}, {'name': 'Chenyuan Feng'}, {'name': 'Shuaishuai Guo'}, {'name': 'Tony Q. S. Quek'}]",2024-08-07T08:43:32Z
http://arxiv.org/abs/2408.03630v2,http://arxiv.org/abs/2408.03630v2,PAGED: A Benchmark for Procedural Graphs Extraction from Documents,"Automatic extraction of procedural graphs from documents creates a low-cost
way for users to easily understand a complex procedure by skimming visual
graphs. Despite the progress in recent studies, it remains unanswered: whether
the existing studies have well solved this task (Q1) and whether the emerging
large language models (LLMs) can bring new opportunities to this task (Q2). To
this end, we propose a new benchmark PAGED, equipped with a large high-quality
dataset and standard evaluations. It investigates five state-of-the-art
baselines, revealing that they fail to extract optimal procedural graphs well
because of their heavy reliance on hand-written rules and limited available
data. We further involve three advanced LLMs in PAGED and enhance them with a
novel self-refine strategy. The results point out the advantages of LLMs in
identifying textual elements and their gaps in building logical structures. We
hope PAGED can serve as a major landmark for automatic procedural graph
extraction and the investigations in PAGED can offer insights into the research
on logic reasoning among non-sequential elements.","[{'name': 'Weihong Du'}, {'name': 'Wenrui Liao'}, {'name': 'Hongru Liang'}, {'name': 'Wenqiang Lei'}]",2024-08-07T08:43:18Z
http://arxiv.org/abs/2408.03622v1,http://arxiv.org/abs/2408.03622v1,"Improving the quality of Persian clinical text with a novel spelling
  correction system","Background: The accuracy of spelling in Electronic Health Records (EHRs) is a
critical factor for efficient clinical care, research, and ensuring patient
safety. The Persian language, with its abundant vocabulary and complex
characteristics, poses unique challenges for real-word error correction. This
research aimed to develop an innovative approach for detecting and correcting
spelling errors in Persian clinical text.
  Methods: Our strategy employs a state-of-the-art pre-trained model that has
been meticulously fine-tuned specifically for the task of spelling correction
in the Persian clinical domain. This model is complemented by an innovative
orthographic similarity matching algorithm, PERTO, which uses visual similarity
of characters for ranking correction candidates.
  Results: The evaluation of our approach demonstrated its robustness and
precision in detecting and rectifying word errors in Persian clinical text. In
terms of non-word error correction, our model achieved an F1-Score of 90.0%
when the PERTO algorithm was employed. For real-word error detection, our model
demonstrated its highest performance, achieving an F1-Score of 90.6%.
Furthermore, the model reached its highest F1-Score of 91.5% for real-word
error correction when the PERTO algorithm was employed.
  Conclusions: Despite certain limitations, our method represents a substantial
advancement in the field of spelling error detection and correction for Persian
clinical text. By effectively addressing the unique challenges posed by the
Persian language, our approach paves the way for more accurate and efficient
clinical documentation, contributing to improved patient care and safety.
Future research could explore its use in other areas of the Persian medical
domain, enhancing its impact and utility.","[{'name': 'Seyed Mohammad Sadegh Dashti'}, {'name': 'Seyedeh Fatemeh Dashti'}]",2024-08-07T08:31:42Z
http://arxiv.org/abs/2408.03618v1,http://arxiv.org/abs/2408.03618v1,A Logical Fallacy-Informed Framework for Argument Generation,"Despite the remarkable performance of Large Language Models (LLMs), they
still struggle with generating logically sound arguments, resulting in
potential risks such as spreading misinformation. An important factor
contributing to LLMs' suboptimal performance in generating coherent arguments
is their oversight of logical fallacies. To address this issue, we introduce
FIPO, a fallacy-informed framework that leverages preference optimization
methods to steer LLMs toward logically sound arguments. FIPO includes a
classification loss, to capture the fine-grained information on fallacy
categories. Our results on argumentation datasets show that our method reduces
the fallacy errors by up to 17.5%. Furthermore, our human evaluation results
indicate that the quality of the generated arguments by our method
significantly outperforms the fine-tuned baselines, as well as prior preference
optimization methods, such as DPO. These findings highlight the importance of
ensuring models are aware of logical fallacies for effective argument
generation.","[{'name': 'Luca Mouchel'}, {'name': 'Debjit Paul'}, {'name': 'Shaobo Cui'}, {'name': 'Robert West'}, {'name': 'Antoine Bosselut'}, {'name': 'Boi Faltings'}]",2024-08-07T08:19:44Z
http://arxiv.org/abs/2408.03617v1,http://arxiv.org/abs/2408.03617v1,Is Child-Directed Speech Effective Training Data for Language Models?,"While high-performing language models are typically trained on hundreds of
billions of words, human children become fluent language users with a much
smaller amount of data. What are the features of the data they receive, and how
do these features support language modeling objectives? To investigate this
question, we train GPT-2 models on 29M words of English-language child-directed
speech and a new matched, synthetic dataset (TinyDialogues), comparing to a
heterogeneous blend of datasets from the BabyLM challenge. We evaluate both the
syntactic and semantic knowledge of these models using developmentally-inspired
evaluations. Through pretraining experiments, we test whether the global
developmental ordering or the local discourse ordering of children's training
data support high performance relative to other datasets. The local properties
of the data affect model results, but somewhat surprisingly, global properties
do not. Further, child language input is not uniquely valuable for training
language models. These findings support the hypothesis that, rather than
proceeding from better data, children's learning is instead substantially more
efficient than current language modeling techniques.","[{'name': 'Steven Y. Feng'}, {'name': 'Noah D. Goodman'}, {'name': 'Michael C. Frank'}]",2024-08-07T08:18:51Z
http://arxiv.org/abs/2408.03615v1,http://arxiv.org/abs/2408.03615v1,"Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in
  Long-Horizon Tasks","Building a general-purpose agent is a long-standing vision in the field of
artificial intelligence. Existing agents have made remarkable progress in many
domains, yet they still struggle to complete long-horizon tasks in an open
world. We attribute this to the lack of necessary world knowledge and
multimodal experience that can guide agents through a variety of long-horizon
tasks. In this paper, we propose a Hybrid Multimodal Memory module to address
the above challenges. It 1) transforms knowledge into Hierarchical Directed
Knowledge Graph that allows agents to explicitly represent and learn world
knowledge, and 2) summarises historical information into Abstracted Multimodal
Experience Pool that provide agents with rich references for in-context
learning. On top of the Hybrid Multimodal Memory module, a multimodal agent,
Optimus-1, is constructed with dedicated Knowledge-guided Planner and
Experience-Driven Reflector, contributing to a better planning and reflection
in the face of long-horizon tasks in Minecraft. Extensive experimental results
show that Optimus-1 significantly outperforms all existing agents on
challenging long-horizon task benchmarks, and exhibits near human-level
performance on many tasks. In addition, we introduce various Multimodal Large
Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show
that Optimus-1 exhibits strong generalization with the help of the Hybrid
Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.","[{'name': 'Zaijing Li'}, {'name': 'Yuquan Xie'}, {'name': 'Rui Shao'}, {'name': 'Gongwei Chen'}, {'name': 'Dongmei Jiang'}, {'name': 'Liqiang Nie'}]",2024-08-07T08:16:32Z
http://arxiv.org/abs/2408.03603v1,http://arxiv.org/abs/2408.03603v1,EnJa: Ensemble Jailbreak on Large Language Models,"As Large Language Models (LLMs) are increasingly being deployed in
safety-critical applications, their vulnerability to potential jailbreaks --
malicious prompts that can disable the safety mechanism of LLMs -- has
attracted growing research attention. While alignment methods have been
proposed to protect LLMs from jailbreaks, many have found that aligned LLMs can
still be jailbroken by carefully crafted malicious prompts, producing content
that violates policy regulations. Existing jailbreak attacks on LLMs can be
categorized into prompt-level methods which make up stories/logic to circumvent
safety alignment and token-level attack methods which leverage gradient methods
to find adversarial tokens. In this work, we introduce the concept of Ensemble
Jailbreak and explore methods that can integrate prompt-level and token-level
jailbreak into a more powerful hybrid jailbreak attack. Specifically, we
propose a novel EnJa attack to hide harmful instructions using prompt-level
jailbreak, boost the attack success rate using a gradient-based attack, and
connect the two types of jailbreak attacks via a template-based connector. We
evaluate the effectiveness of EnJa on several aligned models and show that it
achieves a state-of-the-art attack success rate with fewer queries and is much
stronger than any individual jailbreak.","[{'name': 'Jiahao Zhang'}, {'name': 'Zilong Wang'}, {'name': 'Ruofan Wang'}, {'name': 'Xingjun Ma'}, {'name': 'Yu-Gang Jiang'}]",2024-08-07T07:46:08Z
http://arxiv.org/abs/2408.03574v1,http://arxiv.org/abs/2408.03574v1,Teach CLIP to Develop a Number Sense for Ordinal Regression,"Ordinal regression is a fundamental problem within the field of computer
vision, with customised well-trained models on specific tasks. While
pre-trained vision-language models (VLMs) have exhibited impressive performance
on various vision tasks, their potential for ordinal regression has received
less exploration. In this study, we first investigate CLIP's potential for
ordinal regression, from which we expect the model could generalise to
different ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP
fails on this task, since current VLMs have a well-documented limitation of
encapsulating compositional concepts such as number sense. We propose a simple
yet effective method called NumCLIP to improve the quantitative understanding
of VLMs. We disassemble the exact image to number-specific text matching
problem into coarse classification and fine prediction stages. We discretize
and phrase each numerical bin with common language concept to better leverage
the available pre-trained alignment in CLIP. To consider the inherent
continuous property of ordinal regression, we propose a novel fine-grained
cross-modal ranking-based regularisation loss specifically designed to keep
both semantic and ordinal alignment in CLIP's feature space. Experimental
results on three general ordinal regression tasks demonstrate the effectiveness
of NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating
and image aesthetics assessment task, respectively. Code is publicly available
at https://github.com/xmed-lab/NumCLIP.","[{'name': 'Yao Du'}, {'name': 'Qiang Zhai'}, {'name': 'Weihang Dai'}, {'name': 'Xiaomeng Li'}]",2024-08-07T06:26:04Z
http://arxiv.org/abs/2408.03562v1,http://arxiv.org/abs/2408.03562v1,"A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel
  Chatbot Use Case","This research compares large language model (LLM) fine-tuning methods,
including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning
(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally
compared LLM evaluation methods including End to End (E2E) benchmark method of
""Golden Answers"", traditional natural language processing (NLP) metrics, RAG
Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,
using the travel chatbot use case. The travel dataset was sourced from the the
Reddit API by requesting posts from travel-related subreddits to get
travel-related conversation prompts and personalized travel experiences, and
augmented for each fine-tuning method. We used two pretrained LLMs utilized for
fine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to
the two pretrained models. The inferences from these models are extensively
evaluated against the aforementioned metrics. The best model according to human
evaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a
Reinforcement Learning from Human Feedback (RLHF) training pipeline, and
ultimately was evaluated as the best model. Our main findings are that: 1)
quantitative and Ragas metrics do not align with human evaluation, 2) Open AI
GPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep
humans in the loop for evaluation because, 4) traditional NLP metrics
insufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms
QLoRA, but still needs postprocessing, 7) RLHF improves model performance
significantly. Next steps include improving data quality, increasing data
quantity, exploring RAG methods, and focusing data collection on a specific
city, which would improve data quality by narrowing the focus, while creating a
useful product.","[{'name': 'Sonia Meyer'}, {'name': 'Shreya Singh'}, {'name': 'Bertha Tam'}, {'name': 'Christopher Ton'}, {'name': 'Angel Ren'}]",2024-08-07T05:52:00Z
http://arxiv.org/abs/2408.03554v1,http://arxiv.org/abs/2408.03554v1,"Empirical Analysis of Large Vision-Language Models against Goal
  Hijacking via Visual Prompt Injection","We explore visual prompt injection (VPI) that maliciously exploits the
ability of large vision-language models (LVLMs) to follow instructions drawn
onto the input image. We propose a new VPI method, ""goal hijacking via visual
prompt injection"" (GHVPI), that swaps the execution task of LVLMs from an
original task to an alternative task designated by an attacker. The
quantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and
demonstrates a notable attack success rate of 15.8%, which is an unignorable
security risk. Our analysis also shows that successful GHVPI requires high
character recognition capability and instruction-following ability in LVLMs.","[{'name': 'Subaru Kimura'}, {'name': 'Ryota Tanaka'}, {'name': 'Shumpei Miyawaki'}, {'name': 'Jun Suzuki'}, {'name': 'Keisuke Sakaguchi'}]",2024-08-07T05:30:10Z
http://arxiv.org/abs/2408.03544v1,http://arxiv.org/abs/2408.03544v1,"Unlocking the Non-Native Language Context Limitation: Native Language
  Prompting Facilitates Knowledge Elicitation","Multilingual large language models (MLLMs) struggle to answer questions posed
in non-dominant languages, even though they have already acquired the relevant
knowledge from their dominant language corpus. In contrast, human multilinguals
can overcome this issue by invoking the relatively rich knowledge acquired from
native language texts through Positive Native Language Transfer (PNLT).
Inspired by this, we analogize the dominant language of MLLMs to the native
language of human multilinguals, and propose Native Language Prompting (NatLan)
to simulate the PNLT observed in human multilinguals. It explicitly creates
native language contexts for MLLMs to facilitate the elicitation of the rich
native language knowledge during question-answering, unlocking the limitations
imposed by non-native language contexts on the effective application of
knowledge. By employing multi-MLLM collaboration, NatLan reduces the workload
on each MLLM in simulating PNLT and refines semantic transfer. On the C-Eval
benchmark, NatLan provides up to a 10.1% average accuracy improvement and up to
a 5.0% increase in the hard-level subset across five MLLMs, surpassing all
top-notch related methods. Our code is available at
https://github.com/AnonyNLP/NatLan.","[{'name': 'Baixuan Li'}, {'name': 'Yunlong Fan'}, {'name': 'Zhiqiang Gao'}]",2024-08-07T04:49:38Z
http://arxiv.org/abs/2408.03541v3,http://arxiv.org/abs/2408.03541v3,EXAONE 3.0 7.8B Instruction Tuned Language Model,"We introduce EXAONE 3.0 instruction-tuned language model, the first open
model in the family of Large Language Models (LLMs) developed by LG AI
Research. Among different model sizes, we publicly release the 7.8B
instruction-tuned model to promote open research and innovations. Through
extensive evaluations across a wide range of public and in-house benchmarks,
EXAONE 3.0 demonstrates highly competitive real-world performance with
instruction-following capability against other state-of-the-art open models of
similar size. Our comparative analysis shows that EXAONE 3.0 excels
particularly in Korean, while achieving compelling performance across general
tasks and complex reasoning. With its strong real-world effectiveness and
bilingual proficiency, we hope that EXAONE keeps contributing to advancements
in Expert AI. Our EXAONE 3.0 instruction-tuned model is available at
https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct","[{'name': 'LG AI Research'}, {'name': ':'}, {'name': 'Soyoung An'}, {'name': 'Kyunghoon Bae'}, {'name': 'Eunbi Choi'}, {'name': 'Stanley Jungkyu Choi'}, {'name': 'Yemuk Choi'}, {'name': 'Seokhee Hong'}, {'name': 'Yeonjung Hong'}, {'name': 'Junwon Hwang'}, {'name': 'Hyojin Jeon'}, {'name': 'Gerrard Jeongwon Jo'}, {'name': 'Hyunjik Jo'}, {'name': 'Jiyeon Jung'}, {'name': 'Yountae Jung'}, {'name': 'Euisoon Kim'}, {'name': 'Hyosang Kim'}, {'name': 'Joonkee Kim'}, {'name': 'Seonghwan Kim'}, {'name': 'Soyeon Kim'}, {'name': 'Sunkyoung Kim'}, {'name': 'Yireun Kim'}, {'name': 'Youchul Kim'}, {'name': 'Edward Hwayoung Lee'}, {'name': 'Haeju Lee'}, {'name': 'Honglak Lee'}, {'name': 'Jinsik Lee'}, {'name': 'Kyungmin Lee'}, {'name': 'Moontae Lee'}, {'name': 'Seungjun Lee'}, {'name': 'Woohyung Lim'}, {'name': 'Sangha Park'}, {'name': 'Sooyoun Park'}, {'name': 'Yongmin Park'}, {'name': 'Boseong Seo'}, {'name': 'Sihoon Yang'}, {'name': 'Heuiyeen Yeen'}, {'name': 'Kyungjae Yoo'}, {'name': 'Hyeongu Yun'}]",2024-08-07T04:38:38Z
http://arxiv.org/abs/2408.03524v1,http://arxiv.org/abs/2408.03524v1,EgyBERT: A Large Language Model Pretrained on Egyptian Dialect Corpora,"This study presents EgyBERT, an Arabic language model pretrained on 10.4 GB
of Egyptian dialectal texts. We evaluated EgyBERT's performance by comparing it
with five other multidialect Arabic language models across 10 evaluation
datasets. EgyBERT achieved the highest average F1-score of 84.25% and an
accuracy of 87.33%, significantly outperforming all other comparative models,
with MARBERTv2 as the second best model achieving an F1-score 83.68% and an
accuracy 87.19%. Additionally, we introduce two novel Egyptian dialectal
corpora: the Egyptian Tweets Corpus (ETC), containing over 34.33 million tweets
(24.89 million sentences) amounting to 2.5 GB of text, and the Egyptian Forums
Corpus (EFC), comprising over 44.42 million sentences (7.9 GB of text)
collected from various Egyptian online forums. Both corpora are used in
pretraining the new model, and they are the largest Egyptian dialectal corpora
to date reported in the literature. Furthermore, this is the first study to
evaluate the performance of various language models on Egyptian dialect
datasets, revealing significant differences in performance that highlight the
need for more dialect-specific models. The results confirm the effectiveness of
EgyBERT model in processing and analyzing Arabic text expressed in Egyptian
dialect, surpassing other language models included in the study. EgyBERT model
is publicly available on \url{https://huggingface.co/faisalq/EgyBERT}.",[{'name': 'Faisal Qarah'}],2024-08-07T03:23:55Z
http://arxiv.org/abs/2408.03511v1,http://arxiv.org/abs/2408.03511v1,MoExtend: Tuning New Experts for Modality and Task Extension,"Large language models (LLMs) excel in various tasks but are primarily trained
on text data, limiting their application scope. Expanding LLM capabilities to
include vision-language understanding is vital, yet training them on multimodal
data from scratch is challenging and costly. Existing instruction tuning
methods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs
via fully fine-tuning LLMs to bridge the modality gap. However, full
fine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous
knowledge, and high training costs particularly in the era of increasing tasks
and modalities. To solve this issue, we introduce MoExtend, an effective
framework designed to streamline the modality adaptation and extension of
Mixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts
into pre-trained MoE models, endowing them with novel knowledge without the
need to tune pretrained models such as MoE and vision encoders. This approach
enables rapid adaptation and extension to new modal data or tasks, effectively
addressing the challenge of accommodating new modalities within LLMs.
Furthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk
of catastrophic forgetting. Experimental results demonstrate the efficacy and
efficiency of MoExtend in enhancing the multimodal capabilities of LLMs,
contributing to advancements in multimodal AI research. Code:
https://github.com/zhongshsh/MoExtend.","[{'name': 'Shanshan Zhong'}, {'name': 'Shanghua Gao'}, {'name': 'Zhongzhan Huang'}, {'name': 'Wushao Wen'}, {'name': 'Marinka Zitnik'}, {'name': 'Pan Zhou'}]",2024-08-07T02:28:37Z
http://arxiv.org/abs/2408.03506v1,http://arxiv.org/abs/2408.03506v1,"1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your
  Language Model Thrives on Quality Data","This paper presents a compute-efficient approach to pre-training a Language
Model-the ""1.5-Pints""-in only 9 days, while outperforming state-of-the-art
models as an instruction-following assistant.Based on MT-Bench (a benchmark
that emulates human judgments), 1.5-Pints outperforms Apple's OpenELM and
Microsoft's Phi.This is achieved by a carefully curated pre-training dataset of
57 billion tokens, using a mix of automated workflows and manual human review.
The selection of the dataset prioritizes content that is considered expository
and ""textbook-like"" to aid the model in reasoning and logical deduction,
culminating in its overall ability as a strong and versatile AI model. In terms
of the model architecture, we employed a modified Mistral tokenizer, alongside
a Llama-2 architecture for wider compatibility. For training, we adopted the
methodologies used by StableLM, TinyLlama, and Huggingface Zephyr. 1.5-Pints
demonstrates that by focusing on data quality over quantity in LLM training, we
can significantly reduce training time and resources required. We believe this
approach will not only make pre-training more accessible but also reduce our
carbon footprint. Our findings and resources from this research are
open-sourced, aiming to facilitate further advancements in the field. The
1.5-Pints model is available in two versions: 2K and 16K context windows.","[{'name': 'Calvin Tan'}, {'name': 'Jerome Wang'}]",2024-08-07T02:14:52Z
http://arxiv.org/abs/2408.03505v1,http://arxiv.org/abs/2408.03505v1,"Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble
  Exploitation","Multimodal large language models (MLLMs) have extended the success of large
language models (LLMs) to multiple data types, such as image, text and audio,
achieving significant performance in various domains, including multimodal
translation, visual question answering and content generation. Nonetheless,
existing systems are inefficient to train MLLMs due to substantial GPU bubbles
caused by the heterogeneous modality models and complex data dependencies in 3D
parallelism. This paper proposes Optimus, a distributed MLLM training system
that reduces end-to-end MLLM training time. Optimus is based on our principled
analysis that scheduling the encoder computation within the LLM bubbles can
reduce bubbles in MLLM training. To make scheduling encoder computation
possible for all GPUs, Optimus searches the separate parallel plans for encoder
and LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM
bubbles without breaking the original data dependencies in the MLLM model
architecture. We further decompose encoder layer computation into a series of
kernels, and analyze the common bubble pattern of 3D parallelism to carefully
optimize the sub-millisecond bubble scheduling, minimizing the overall training
time. Our experiments in a production cluster show that Optimus accelerates
MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs
compared to baselines.","[{'name': 'Weiqi Feng'}, {'name': 'Yangrui Chen'}, {'name': 'Shaoyu Wang'}, {'name': 'Yanghua Peng'}, {'name': 'Haibin Lin'}, {'name': 'Minlan Yu'}]",2024-08-07T02:08:29Z
http://arxiv.org/abs/2408.04668v1,http://arxiv.org/abs/2408.04668v1,Forecasting Live Chat Intent from Browsing History,"Customers reach out to online live chat agents with various intents, such as
asking about product details or requesting a return. In this paper, we propose
the problem of predicting user intent from browsing history and address it
through a two-stage approach. The first stage classifies a user's browsing
history into high-level intent categories. Here, we represent each browsing
history as a text sequence of page attributes and use the ground-truth class
labels to fine-tune pretrained Transformers. The second stage provides a large
language model (LLM) with the browsing history and predicted intent class to
generate fine-grained intents. For automatic evaluation, we use a separate LLM
to judge the similarity between generated and ground-truth intents, which
closely aligns with human judgments. Our two-stage approach yields significant
performance gains compared to generating intents without the classification
stage.","[{'name': 'Se-eun Yoon'}, {'name': 'Ahmad Bin Rabiah'}, {'name': 'Zaid Alibadi'}, {'name': 'Surya Kallumadi'}, {'name': 'Julian McAuley'}]",2024-08-07T01:50:59Z
http://arxiv.org/abs/2408.03492v1,http://arxiv.org/abs/2408.03492v1,Automated Theorem Provers Help Improve Large Language Model Reasoning,"In this paper we demonstrate how logic programming systems and Automated
first-order logic Theorem Provers (ATPs) can improve the accuracy of Large
Language Models (LLMs) for logical reasoning tasks where the baseline
performance is given by direct LLM solutions. We first evaluate LLM reasoning
on steamroller problems using the PRONTOQA benchmark. We show how accuracy can
be improved with a neuro-symbolic architecture where the LLM acts solely as a
front-end for translating a given problem into a formal logic language and an
automated reasoning engine is called for solving it. However, this approach
critically hinges on the correctness of the LLM translation. To assess this
translation correctness, we secondly define a framework of syntactic and
semantic error categories. We implemented the framework and used it to identify
errors that LLMs make in the benchmark domain. Based on these findings, we
thirdly extended our method with capabilities for automatically correcting
syntactic and semantic errors. For semantic error correction we integrate
first-order logic ATPs, which is our main and novel contribution. We
demonstrate that this approach reduces semantic errors significantly and
further increases the accurracy of LLM logical reasoning.","[{'name': 'Lachlan McGinness'}, {'name': 'Peter Baumgartner'}]",2024-08-07T01:03:56Z
http://arxiv.org/abs/2408.03326v1,http://arxiv.org/abs/2408.03326v1,LLaVA-OneVision: Easy Visual Task Transfer,"We present LLaVA-OneVision, a family of open large multimodal models (LMMs)
developed by consolidating our insights into data, models, and visual
representations in the LLaVA-NeXT blog series. Our experimental results
demonstrate that LLaVA-OneVision is the first single model that can
simultaneously push the performance boundaries of open LMMs in three important
computer vision scenarios: single-image, multi-image, and video scenarios.
Importantly, the design of LLaVA-OneVision allows strong transfer learning
across different modalities/scenarios, yielding new emerging capabilities. In
particular, strong video understanding and cross-scenario capabilities are
demonstrated through task transfer from images to videos.","[{'name': 'Bo Li'}, {'name': 'Yuanhan Zhang'}, {'name': 'Dong Guo'}, {'name': 'Renrui Zhang'}, {'name': 'Feng Li'}, {'name': 'Hao Zhang'}, {'name': 'Kaichen Zhang'}, {'name': 'Yanwei Li'}, {'name': 'Ziwei Liu'}, {'name': 'Chunyuan Li'}]",2024-08-06T17:59:44Z
http://arxiv.org/abs/2408.03325v1,http://arxiv.org/abs/2408.03325v1,CoverBench: A Challenging Benchmark for Complex Claim Verification,"There is a growing line of research on verifying the correctness of language
models' outputs. At the same time, LMs are being used to tackle complex queries
that require reasoning. We introduce CoverBench, a challenging benchmark
focused on verifying LM outputs in complex reasoning settings. Datasets that
can be used for this purpose are often designed for other complex reasoning
tasks (e.g., QA) targeting specific use-cases (e.g., financial tables),
requiring transformations, negative sampling and selection of hard examples to
collect such a benchmark. CoverBench provides a diversified evaluation for
complex claim verification in a variety of domains, types of reasoning,
relatively long inputs, and a variety of standardizations, such as multiple
representations for tables where available, and a consistent schema. We
manually vet the data for quality to ensure low levels of label noise. Finally,
we report a variety of competitive baseline results to show CoverBench is
challenging and has very significant headroom. The data is available at
https://huggingface.co/datasets/google/coverbench .","[{'name': 'Alon Jacovi'}, {'name': 'Moran Ambar'}, {'name': 'Eyal Ben-David'}, {'name': 'Uri Shaham'}, {'name': 'Amir Feder'}, {'name': 'Mor Geva'}, {'name': 'Dror Marcus'}, {'name': 'Avi Caciularu'}]",2024-08-06T17:58:53Z
http://arxiv.org/abs/2408.03319v1,http://arxiv.org/abs/2408.03319v1,Training LLMs to Recognize Hedges in Spontaneous Narratives,"Hedges allow speakers to mark utterances as provisional, whether to signal
non-prototypicality or ""fuzziness"", to indicate a lack of commitment to an
utterance, to attribute responsibility for a statement to someone else, to
invite input from a partner, or to soften critical feedback in the service of
face-management needs. Here we focus on hedges in an experimentally
parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced
from memory by 21 speakers for co-present addressees, transcribed to text
(Galati and Brennan, 2010). We created a gold standard of hedges annotated by
human coders (the Roadrunner-Hedge corpus) and compared three LLM-based
approaches for hedge detection: fine-tuning BERT, and zero and few-shot
prompting with GPT-4o and LLaMA-3. The best-performing approach was a
fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on
the top performing approaches, we used an LLM-in-the-Loop approach to improve
the gold standard coding, as well as to highlight cases in which hedges are
ambiguous in linguistically interesting ways that will guide future research.
This is the first step in our research program to train LLMs to interpret and
generate collateral signals appropriately and meaningfully in conversation.","[{'name': 'Amie J. Paige'}, {'name': 'Adil Soubki'}, {'name': 'John Murzaku'}, {'name': 'Owen Rambow'}, {'name': 'Susan E. Brennan'}]",2024-08-06T17:51:42Z
http://arxiv.org/abs/2408.03314v1,http://arxiv.org/abs/2408.03314v1,"Scaling LLM Test-Time Compute Optimally can be More Effective than
  Scaling Model Parameters","Enabling LLMs to improve their outputs by using more test-time computation is
a critical step towards building generally self-improving agents that can
operate on open-ended natural language. In this paper, we study the scaling of
inference-time computation in LLMs, with a focus on answering the question: if
an LLM is allowed to use a fixed but non-trivial amount of inference-time
compute, how much can it improve its performance on a challenging prompt?
Answering this question has implications not only on the achievable performance
of LLMs, but also on the future of LLM pretraining and how one should tradeoff
inference-time and pre-training compute. Despite its importance, little
research attempted to understand the scaling behaviors of various test-time
inference methods. Moreover, current work largely provides negative results for
a number of these strategies. In this work, we analyze two primary mechanisms
to scale test-time computation: (1) searching against dense, process-based
verifier reward models; and (2) updating the model's distribution over a
response adaptively, given the prompt at test time. We find that in both cases,
the effectiveness of different approaches to scaling test-time compute
critically varies depending on the difficulty of the prompt. This observation
motivates applying a ""compute-optimal"" scaling strategy, which acts to most
effectively allocate test-time compute adaptively per prompt. Using this
compute-optimal strategy, we can improve the efficiency of test-time compute
scaling by more than 4x compared to a best-of-N baseline. Additionally, in a
FLOPs-matched evaluation, we find that on problems where a smaller base model
attains somewhat non-trivial success rates, test-time compute can be used to
outperform a 14x larger model.","[{'name': 'Charlie Snell'}, {'name': 'Jaehoon Lee'}, {'name': 'Kelvin Xu'}, {'name': 'Aviral Kumar'}]",2024-08-06T17:35:05Z
http://arxiv.org/abs/2408.03297v1,http://arxiv.org/abs/2408.03297v1,"KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge
  Selection in Retrieval-Augmented Language Models","By integrating external knowledge, Retrieval-Augmented Generation (RAG) has
become an effective strategy for mitigating the hallucination problems that
large language models (LLMs) encounter when dealing with knowledge-intensive
tasks. However, in the process of integrating external non-parametric
supporting evidence with internal parametric knowledge, inevitable knowledge
conflicts may arise, leading to confusion in the model's responses. To enhance
the knowledge selection of LLMs in various contexts, some research has focused
on refining their behavior patterns through instruction-tuning. Nonetheless,
due to the absence of explicit negative signals and comparative objectives,
models fine-tuned in this manner may still exhibit undesirable behaviors in the
intricate and realistic retrieval scenarios. To this end, we propose a
Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving
controllable knowledge selection in real retrieval scenarios. Concretely, we
explore and simulate error types across diverse context combinations and learn
how to avoid these negative signals through preference optimization methods.
Simultaneously, by adjusting the balance between response length and the
proportion of preference data representing different behavior patterns, we
enhance the adherence capabilities and noise robustness of LLMs in a balanced
manner. Experimental results show that KaPO outperforms previous methods for
handling knowledge conflicts by over 37%, while also exhibiting robust
generalization across various out-of-distribution datasets.","[{'name': 'Ruizhe Zhang'}, {'name': 'Yongxin Xu'}, {'name': 'Yuzhen Xiao'}, {'name': 'Runchuan Zhu'}, {'name': 'Xinke Jiang'}, {'name': 'Xu Chu'}, {'name': 'Junfeng Zhao'}, {'name': 'Yasha Wang'}]",2024-08-06T16:55:54Z
http://arxiv.org/abs/2408.04667v1,http://arxiv.org/abs/2408.04667v1,LLM Stability: A detailed analysis with some surprises,"A concerning property of our nearly magical LLMs involves the variation of
results given the exact same input and deterministic hyper-parameters. While AI
has always had a certain level of noisiness from inputs outside of training
data, we have generally had deterministic results for any particular input;
that is no longer true. While most LLM practitioners are ""in the know"", we are
unaware of any work that attempts to quantify current LLM stability. We suspect
no one has taken the trouble because it is just too boring a paper to execute
and write. But we have done it and there are some surprises.
  What kinds of surprises?
  The evaluated LLMs are rarely deterministic at the raw output level; they are
much more deterministic at the parsed output/answer level but still rarely 100%
stable across 5 re-runs with same data input.
  LLM accuracy variation is not normally distributed.
  Stability varies based on task.","[{'name': 'Berk Atil'}, {'name': 'Alexa Chittams'}, {'name': 'Liseng Fu'}, {'name': 'Ferhan Ture'}, {'name': 'Lixinyu Xu'}, {'name': 'Breck Baldwin'}]",2024-08-06T16:43:35Z
http://arxiv.org/abs/2408.03290v1,http://arxiv.org/abs/2408.03290v1,SARA: Singular-Value Based Adaptive Low-Rank Adaption,"With the increasing number of parameters in large pre-trained models, LoRA as
a parameter-efficient fine-tuning(PEFT) method is widely used for not adding
inference overhead. The LoRA method assumes that weight changes during
fine-tuning can be approximated by low-rank matrices. However, the rank values
need to be manually verified to match different downstream tasks, and they
cannot accommodate the varying importance of different layers in the model. In
this work, we first analyze the relationship between the performance of
different layers and their ranks using SVD. Based on this, we design the
Singular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds
the rank during initialization by performing SVD on the pre-trained weights.
Additionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly
reduces the number of parameters by fine-tuning only multiple parallel sets of
singular values controlled by a router. Extensive experiments on various
complex tasks demonstrate the simplicity and parameter efficiency of our
methods. They can effectively and adaptively find the most suitable rank for
each layer of each model.","[{'name': 'Jihao Gu'}, {'name': 'Shuai Chen'}, {'name': 'Zelin Wang'}, {'name': 'Yibo Zhang'}, {'name': 'Ping Gong'}]",2024-08-06T16:39:42Z
http://arxiv.org/abs/2408.04666v1,http://arxiv.org/abs/2408.04666v1,LLMs are Not Just Next Token Predictors,"LLMs are statistical models of language learning through stochastic gradient
descent with a next token prediction objective. Prompting a popular view among
AI modelers: LLMs are just next token predictors. While LLMs are engineered
using next token prediction, and trained based on their success at this task,
our view is that a reduction to just next token predictor sells LLMs short.
Moreover, there are important explanations of LLM behavior and capabilities
that are lost when we engage in this kind of reduction. In order to draw this
out, we will make an analogy with a once prominent research program in biology
explaining evolution and development from the gene's eye view.","[{'name': 'Stephen M. Downes'}, {'name': 'Patrick Forber'}, {'name': 'Alex Grzankowski'}]",2024-08-06T16:36:28Z
http://arxiv.org/abs/2408.03281v2,http://arxiv.org/abs/2408.03281v2,"StructEval: Deepen and Broaden Large Language Model Assessment via
  Structured Evaluation","Evaluation is the baton for the development of large language models. Current
evaluations typically employ a single-item assessment paradigm for each atomic
test objective, which struggles to discern whether a model genuinely possesses
the required capabilities or merely memorizes/guesses the answers to specific
questions. To this end, we propose a novel evaluation framework referred to as
StructEval. Starting from an atomic test objective, StructEval deepens and
broadens the evaluation by conducting a structured assessment across multiple
cognitive levels and critical concepts, and therefore offers a comprehensive,
robust and consistent evaluation for LLMs. Experiments on three widely-used
benchmarks demonstrate that StructEval serves as a reliable tool for resisting
the risk of data contamination and reducing the interference of potential
biases, thereby providing more reliable and consistent conclusions regarding
model capabilities. Our framework also sheds light on the design of future
principled and trustworthy LLM evaluation protocols.","[{'name': 'Boxi Cao'}, {'name': 'Mengjie Ren'}, {'name': 'Hongyu Lin'}, {'name': 'Xianpei Han'}, {'name': 'Feng Zhang'}, {'name': 'Junfeng Zhan'}, {'name': 'Le Sun'}]",2024-08-06T16:28:30Z
http://arxiv.org/abs/2408.03359v1,http://arxiv.org/abs/2408.03359v1,"LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal
  Classification","We introduce LAMPO, a novel paradigm that leverages Large Language Models
(LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike
conventional methods, which concatenate all demonstration examples with the
test instance and prompt LLMs to produce the pointwise prediction, our
framework uses the LLM as a preference machine that makes a relative
comparative decision between the test instance and each demonstration. A
self-supervised method is then introduced to aggregate these binary comparisons
into the final ordinal decision. LAMPO addresses several limitations inherent
in previous methods, including context length constraints, ordering biases, and
challenges associated with absolute point-wise estimation. Extensive
experiments on seven public datasets demonstrate LAMPO's remarkably competitive
performance across a diverse spectrum of applications (e.g., movie review
analysis and hate speech detection). Notably, in certain applications, the
improvement can be substantial, exceeding 20% in an absolute term. Moreover, we
believe LAMPO represents an interesting addition to the non-parametric
application layered on top of LLMs, as it supports black-box LLMs without
necessitating the outputting of LLM's internal states (e.g., embeddings), as
seen in previous approaches.","[{'name': 'Zhen Qin'}, {'name': 'Junru Wu'}, {'name': 'Jiaming Shen'}, {'name': 'Tianqi Liu'}, {'name': 'Xuanhui Wang'}]",2024-08-06T15:55:05Z
http://arxiv.org/abs/2408.03256v1,http://arxiv.org/abs/2408.03256v1,Synthesizing Text-to-SQL Data from Weak and Strong LLMs,"The capability gap between open-source and closed-source large language
models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we
introduce a synthetic data approach that combines data produced by larger, more
powerful models (strong models) with error information data generated by
smaller, not well-aligned models (weak models). The method not only enhances
the domain generalization of text-to-SQL models but also explores the potential
of error data supervision through preference learning. Furthermore, we employ
the synthetic data approach for instruction tuning on open-source LLMs,
resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is
demonstrated through state-of-the-art results on the SPIDER and BIRD
benchmarks, bridging the performance gap between open-source models and methods
prompted by closed-source models.","[{'name': 'Jiaxi Yang'}, {'name': 'Binyuan Hui'}, {'name': 'Min Yang'}, {'name': 'Jian Yang'}, {'name': 'Junyang Lin'}, {'name': 'Chang Zhou'}]",2024-08-06T15:40:32Z
http://arxiv.org/abs/2408.03247v2,http://arxiv.org/abs/2408.03247v2,"Unveiling Factual Recall Behaviors of Large Language Models through
  Knowledge Neurons","In this paper, we investigate whether Large Language Models (LLMs) actively
recall or retrieve their internal repositories of factual knowledge when faced
with reasoning tasks. Through an analysis of LLMs' internal factual recall at
each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness
the critical factual associations under certain circumstances. Instead, they
tend to opt for alternative, shortcut-like pathways to answer reasoning
questions. By manually manipulating the recall process of parametric knowledge
in LLMs, we demonstrate that enhancing this recall process directly improves
reasoning performance whereas suppressing it leads to notable degradation.
Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a
powerful technique for addressing complex reasoning tasks. Our findings
indicate that CoT can intensify the recall of factual knowledge by encouraging
LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how
contextual conflicts affect the retrieval of facts during the reasoning process
to gain a comprehensive understanding of the factual recall behaviors of LLMs.
Code and data will be available soon.","[{'name': 'Yifei Wang'}, {'name': 'Yuheng Chen'}, {'name': 'Wanting Wen'}, {'name': 'Yu Sheng'}, {'name': 'Linjing Li'}, {'name': 'Daniel Dajun Zeng'}]",2024-08-06T15:07:08Z
http://arxiv.org/abs/2408.03246v1,http://arxiv.org/abs/2408.03246v1,Making Long-Context Language Models Better Multi-Hop Reasoners,"Recent advancements in long-context modeling have enhanced language models
(LMs) for complex tasks across multiple NLP applications. Despite this
progress, we find that these models struggle with multi-hop reasoning and
exhibit decreased performance in the presence of noisy contexts. In this paper,
we introduce Reasoning with Attributions, a novel approach that prompts LMs to
supply attributions for each assertion during their reasoning. We validate our
approach through experiments on three multi-hop datasets, employing both
proprietary and open-source models, and demonstrate its efficacy and
resilience. Furthermore, we explore methods to augment reasoning capabilities
via fine-tuning and offer an attribution-annotated dataset and a specialized
training strategy. Our fine-tuned model achieves competitive performance on
multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as
ChatGPT and Claude-instant.","[{'name': 'Yanyang Li'}, {'name': 'Shuo Liang'}, {'name': 'Michael R. Lyu'}, {'name': 'Liwei Wang'}]",2024-08-06T15:06:40Z
http://arxiv.org/abs/2408.04665v1,http://arxiv.org/abs/2408.04665v1,"LLM-based MOFs Synthesis Condition Extraction using Few-Shot
  Demonstrations","The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from
literature text has been challenging but crucial for the logical design of new
MOFs with desirable functionality. The recent advent of large language models
(LLMs) provides disruptively new solution to this long-standing problem and
latest researches have reported over 90% F1 in extracting correct conditions
from MOFs literature. We argue in this paper that most existing synthesis
extraction practices with LLMs stay with the primitive zero-shot learning,
which could lead to downgraded extraction and application performance due to
the lack of specialized knowledge. This work pioneers and optimizes the
few-shot in-context learning paradigm for LLM extraction of material synthesis
conditions. First, we propose a human-AI joint data curation process to secure
high-quality ground-truth demonstrations for few-shot learning. Second, we
apply a BM25 algorithm based on the retrieval-augmented generation (RAG)
technique to adaptively select few-shot demonstrations for each MOF's
extraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the
proposed few-shot method achieves much higher average F1 performance (0.93 vs.
0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under
fully automatic evaluation that are more objective than the previous human
evaluation. The proposed method is further validated through real-world
material experiments: compared with the baseline zero-shot LLM, the proposed
few-shot approach increases the MOFs structural inference performance (R^2) by
29.4% in average.","[{'name': 'Lei Shi'}, {'name': 'Zhimeng Liu'}, {'name': 'Yi Yang'}, {'name': 'Weize Wu'}, {'name': 'Yuyang Zhang'}, {'name': 'Hongbo Zhang'}, {'name': 'Jing Lin'}, {'name': 'Siyu Wu'}, {'name': 'Zihan Chen'}, {'name': 'Ruiming Li'}, {'name': 'Nan Wang'}, {'name': 'Zipeng Liu'}, {'name': 'Huobin Tan'}, {'name': 'Hongyi Gao'}, {'name': 'Yue Zhang'}, {'name': 'Ge Wang'}]",2024-08-06T14:53:25Z
http://arxiv.org/abs/2408.03202v1,http://arxiv.org/abs/2408.03202v1,"A Debiased Nearest Neighbors Framework for Multi-Label Text
  Classification","Multi-Label Text Classification (MLTC) is a practical yet challenging task
that involves assigning multiple non-exclusive labels to each document.
Previous studies primarily focus on capturing label correlations to assist
label prediction by introducing special labeling schemes, designing specific
model structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor
($k$NN) framework has shown promise by retrieving labeled samples as references
to mine label co-occurrence information in the embedding space. However, two
critical biases, namely embedding alignment bias and confidence estimation
bias, are often overlooked, adversely affecting prediction performance. In this
paper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC,
specifically designed to mitigate these biases. To address embedding alignment
bias, we propose a debiased contrastive learning strategy, enhancing neighbor
consistency on label co-occurrence. For confidence estimation bias, we present
a debiased confidence estimation strategy, improving the adaptive combination
of predictions from $k$NN and inductive binary classifications. Extensive
experiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2,
Amazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method.
Besides, our method does not introduce any extra parameters.","[{'name': 'Zifeng Cheng'}, {'name': 'Zhiwei Jiang'}, {'name': 'Yafeng Yin'}, {'name': 'Zhaoling Chen'}, {'name': 'Cong Wang'}, {'name': 'Shiping Ge'}, {'name': 'Qiguo Huang'}, {'name': 'Qing Gu'}]",2024-08-06T14:00:23Z
http://arxiv.org/abs/2408.03172v1,http://arxiv.org/abs/2408.03172v1,"Leveraging Parameter Efficient Training Methods for Low Resource Text
  Classification: A Case Study in Marathi","With the surge in digital content in low-resource languages, there is an
escalating demand for advanced Natural Language Processing (NLP) techniques
tailored to these languages. BERT (Bidirectional Encoder Representations from
Transformers), serving as the foundational framework for numerous NLP
architectures and language models, is increasingly employed for the development
of low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method
for fine-tuning Large Language Models (LLMs) and reducing the training
parameters to some extent to decrease the computational costs needed for
training the model and achieve results comparable to a fully fine-tuned model.
In this work, we present a study of PEFT methods for the Indic low-resource
language Marathi. We conduct a comprehensive analysis of PEFT methods applied
to various monolingual and multilingual Marathi BERT models. These approaches
are evaluated on prominent text classification datasets like MahaSent,
MahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to
significantly expedite the training speed of the models, addressing a critical
aspect of model development and deployment. In this study, we explore Low-Rank
Adaptation of Large Language Models (LoRA) and adapter methods for low-resource
text classification. We show that these methods are competitive with full
fine-tuning and can be used without loss in accuracy. This study contributes
valuable insights into the effectiveness of Marathi BERT models, offering a
foundation for the continued advancement of NLP capabilities in Marathi and
similar Indic languages.","[{'name': 'Pranita Deshmukh'}, {'name': 'Nikita Kulkarni'}, {'name': 'Sanhita Kulkarni'}, {'name': 'Kareena Manghani'}, {'name': 'Raviraj Joshi'}]",2024-08-06T13:16:16Z
http://arxiv.org/abs/2408.03150v1,http://arxiv.org/abs/2408.03150v1,Conditioning LLMs with Emotion in Neural Machine Translation,"Large Language Models (LLMs) have shown remarkable performance in Natural
Language Processing tasks, including Machine Translation (MT). In this work, we
propose a novel MT pipeline that integrates emotion information extracted from
a Speech Emotion Recognition (SER) model into LLMs to enhance translation
quality. We first fine-tune five existing LLMs on the Libri-trans dataset and
select the most performant model. Subsequently, we augment LLM prompts with
different dimensional emotions and train the selected LLM under these different
configurations. Our experiments reveal that integrating emotion information,
especially arousal, into LLM prompts leads to notable improvements in
translation quality.","[{'name': 'Charles Brazier'}, {'name': 'Jean-Luc Rouas'}]",2024-08-06T12:49:33Z
http://arxiv.org/abs/2408.03149v1,http://arxiv.org/abs/2408.03149v1,"Leveraging Entity Information for Cross-Modality Correlation Learning:
  The Entity-Guided Multimodal Summarization","The rapid increase in multimedia data has spurred advancements in Multimodal
Summarization with Multimodal Output (MSMO), which aims to produce a multimodal
summary that integrates both text and relevant images. The inherent
heterogeneity of content within multimodal inputs and outputs presents a
significant challenge to the execution of MSMO. Traditional approaches
typically adopt a holistic perspective on coarse image-text data or individual
visual objects, overlooking the essential connections between objects and the
entities they represent. To integrate the fine-grained entity knowledge, we
propose an Entity-Guided Multimodal Summarization model (EGMS). Our model,
building on BART, utilizes dual multimodal encoders with shared weights to
process text-image and entity-image information concurrently. A gating
mechanism then combines visual data for enhanced textual summary generation,
while image selection is refined through knowledge distillation from a
pre-trained vision-language model. Extensive experiments on public MSMO dataset
validate the superiority of the EGMS method, which also prove the necessity to
incorporate entity information into MSMO problem.","[{'name': 'Yanghai Zhang'}, {'name': 'Ye Liu'}, {'name': 'Shiwei Wu'}, {'name': 'Kai Zhang'}, {'name': 'Xukai Liu'}, {'name': 'Qi Liu'}, {'name': 'Enhong Chen'}]",2024-08-06T12:45:56Z
http://arxiv.org/abs/2408.03130v1,http://arxiv.org/abs/2408.03130v1,"Inference Optimizations for Large Language Models: Effects, Challenges,
  and Practical Considerations","Large language models are ubiquitous in natural language processing because
they can adapt to new tasks without retraining. However, their sheer scale and
complexity present unique challenges and opportunities, prompting researchers
and practitioners to explore novel model training, optimization, and deployment
methods. This literature review focuses on various techniques for reducing
resource requirements and compressing large language models, including
quantization, pruning, knowledge distillation, and architectural optimizations.
The primary objective is to explore each method in-depth and highlight its
unique challenges and practical applications. The discussed methods are
categorized into a taxonomy that presents an overview of the optimization
landscape and helps navigate it to understand the research trajectory better.","[{'name': 'Leo Donisch'}, {'name': 'Sigurd Schacht'}, {'name': 'Carsten Lanquillon'}]",2024-08-06T12:07:32Z
http://arxiv.org/abs/2408.03127v1,http://arxiv.org/abs/2408.03127v1,"Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral
  7B Model and Data Augmentation","This paper describes our approach to the SemEval-2024 safe biomedical Natural
Language Inference for Clinical Trials (NLI4CT) task, which concerns
classifying statements about Clinical Trial Reports (CTRs). We explored the
capabilities of Mistral-7B, a generalist open-source Large Language Model
(LLM). We developed a prompt for the NLI4CT task, and fine-tuned a quantized
version of the model using an augmented version of the training dataset. The
experimental results show that this approach can produce notable results in
terms of the macro F1-score, while having limitations in terms of faithfulness
and consistency. All the developed code is publicly available on a GitHub
repository","[{'name': 'Artur Guimarães'}, {'name': 'Bruno Martins'}, {'name': 'João Magalhães'}]",2024-08-06T11:59:09Z
http://arxiv.org/abs/2408.03125v1,http://arxiv.org/abs/2408.03125v1,COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework,"As the NLP community increasingly addresses challenges associated with
multilingualism, robust annotation tools are essential to handle multilingual
datasets efficiently. In this paper, we introduce a code-mixed multilingual
text annotation framework, COMMENTATOR, specifically designed for annotating
code-mixed text. The tool demonstrates its effectiveness in token-level and
sentence-level language annotation tasks for Hinglish text. We perform robust
qualitative human-based evaluations to showcase COMMENTATOR led to 5x faster
annotations than the best baseline. Our code is publicly available at
\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is
available at \url{https://bit.ly/commentator_video}.","[{'name': 'Rajvee Sheth'}, {'name': 'Shubh Nisar'}, {'name': 'Heenaben Prajapati'}, {'name': 'Himanshu Beniwal'}, {'name': 'Mayank Singh'}]",2024-08-06T11:56:26Z
http://arxiv.org/abs/2408.03119v1,http://arxiv.org/abs/2408.03119v1,"Evaluating the Translation Performance of Large Language Models Based on
  Euas-20","In recent years, with the rapid development of deep learning technology,
large language models (LLMs) such as BERT and GPT have achieved breakthrough
results in natural language processing tasks. Machine translation (MT), as one
of the core tasks of natural language processing, has also benefited from the
development of large language models and achieved a qualitative leap. Despite
the significant progress in translation performance achieved by large language
models, machine translation still faces many challenges. Therefore, in this
paper, we construct the dataset Euas-20 to evaluate the performance of large
language models on translation tasks, the translation ability on different
languages, and the effect of pre-training data on the translation ability of
LLMs for researchers and developers.","[{'name': 'Yan Huang'}, {'name': 'Wei Liu'}]",2024-08-06T11:49:11Z
http://arxiv.org/abs/2408.03099v1,http://arxiv.org/abs/2408.03099v1,Topic Modeling with Fine-tuning LLMs and Bag of Sentences,"Large language models (LLM)'s are increasingly used for topic modeling
outperforming classical topic models such as LDA. Commonly, pre-trained LLM
encoders such as BERT are used out-of-the-box despite the fact that fine-tuning
is known to improve LLMs considerably. The challenge lies in obtaining a
suitable (labeled) dataset for fine-tuning. In this paper, we use the recent
idea to use bag of sentences as the elementary unit in computing topics. In
turn, we derive an approach FT-Topic to perform unsupervised fine-tuning
relying primarily on two steps for constructing a training dataset in an
automatic fashion. First, a heuristic method to identifies pairs of sentence
groups that are either assumed to be of the same or different topics. Second,
we remove sentence pairs that are likely labeled incorrectly. The dataset is
then used to fine-tune an encoder LLM, which can be leveraged by any topic
modeling approach using embeddings. However, in this work, we demonstrate its
effectiveness by deriving a novel state-of-the-art topic modeling method called
SenClu, which achieves fast inference through an expectation-maximization
algorithm and hard assignments of sentence groups to a single topic, while
giving users the possibility to encode prior knowledge on the topic-document
distribution. Code is at \url{https://github.com/JohnTailor/FT-Topic}",[{'name': 'Johannes Schneider'}],2024-08-06T11:04:07Z
http://arxiv.org/abs/2408.03094v1,http://arxiv.org/abs/2408.03094v1,500xCompressor: Generalized Prompt Compression for Large Language Models,"Prompt compression is crucial for enhancing inference speed, reducing costs,
and improving user experience. However, current methods face challenges such as
low compression ratios and potential data leakage during evaluation. To address
these issues, we propose 500xCompressor, a method that compresses extensive
natural language contexts into a minimum of one single special token. The
500xCompressor introduces approximately 0.3% additional parameters and achieves
compression ratios ranging from 6x to 480x. It is designed to compress any
text, answer various types of questions, and could be utilized by the original
large language model (LLM) without requiring fine-tuning. Initially,
500xCompressor was pretrained on the Arxiv Corpus, followed by fine-tuning on
the ArxivQA dataset, and subsequently evaluated on strictly unseen and
classical question answering (QA) datasets. The results demonstrate that the
LLM retained 62.26-72.89% of its capabilities compared to using non-compressed
prompts. This study also shows that not all the compressed tokens are equally
utilized and that K V values have significant advantages over embeddings in
preserving information at high compression ratios. The highly compressive
nature of natural language prompts, even for fine-grained complex information,
suggests promising potential for future applications and further research into
developing a new LLM language.","[{'name': 'Zongqian Li'}, {'name': 'Yixuan Su'}, {'name': 'Nigel Collier'}]",2024-08-06T10:51:47Z
http://arxiv.org/abs/2408.03092v1,http://arxiv.org/abs/2408.03092v1,"Extend Model Merging from Fine-Tuned to Pre-Trained Large Language
  Models via Weight Disentanglement","Merging Large Language Models (LLMs) aims to amalgamate multiple homologous
LLMs into one with all the capabilities. Ideally, any LLMs sharing the same
backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT)
with minor parameter changes or Pre-Trained (PT) with substantial parameter
shifts. However, existing methods often manually assign the model importance,
rendering them feasible only for LLMs with similar parameter alterations, such
as multiple FT LLMs. The diverse parameter changed ranges between FT and PT
LLMs pose challenges for current solutions in empirically determining the
optimal combination. In this paper, we make a pioneering effort to broaden the
applicability of merging techniques from FT to PT LLMs. We initially examine
the efficacy of current methods in merging FT and PT LLMs, discovering that
they struggle to deal with PT LLMs. Subsequently, we introduce an approach
based on WeIght DisENtanglement (WIDEN) to effectively extend the merging
scope, which first disentangles model weights into magnitude and direction
components, and then performs adaptive fusion by considering their respective
contributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with
instruction-following skills) with Sailor (a PT LLM with multilingual
abilities) across 7B and 14B model scales. Results reveal that: (1) existing
solutions usually fail when merging Sailor, either losing both abilities or
only retaining instruction-following skills; (2) WIDEN successfully injects the
multilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in
Southeast Asian languages, achieving enhancements in the fundamental
capabilities. In light of previous research, we also merge multiple 13B FT LLMs
and observe that WIDEN achieves a balanced amalgamation of instruction
following, mathematical reasoning, and code generation skills.","[{'name': 'Le Yu'}, {'name': 'Bowen Yu'}, {'name': 'Haiyang Yu'}, {'name': 'Fei Huang'}, {'name': 'Yongbin Li'}]",2024-08-06T10:46:46Z
http://arxiv.org/abs/2408.03079v1,http://arxiv.org/abs/2408.03079v1,"Enhancing Complex Causality Extraction via Improved Subtask Interaction
  and Knowledge Fusion","Event Causality Extraction (ECE) aims at extracting causal event pairs from
texts. Despite ChatGPT's recent success, fine-tuning small models remains the
best approach for the ECE task. However, existing fine-tuning based ECE methods
cannot address all three key challenges in ECE simultaneously: 1) Complex
Causality Extraction, where multiple causal-effect pairs occur within a single
sentence; 2) Subtask~ Interaction, which involves modeling the mutual
dependence between the two subtasks of ECE, i.e., extracting events and
identifying the causal relationship between extracted events; and 3) Knowledge
Fusion, which requires effectively fusing the knowledge in two modalities,
i.e., the expressive pretrained language models and the structured knowledge
graphs. In this paper, we propose a unified ECE framework (UniCE to address all
three issues in ECE simultaneously. Specifically, we design a subtask
interaction mechanism to enable mutual interaction between the two ECE
subtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in
the two modalities. Furthermore, we employ separate decoders for each subtask
to facilitate complex causality extraction. Experiments on three benchmark
datasets demonstrate that our method achieves state-of-the-art performance and
outperforms ChatGPT with a margin of at least 30% F1-score. More importantly,
our model can also be used to effectively improve the ECE performance of
ChatGPT via in-context learning.","[{'name': 'Jinglong Gao'}, {'name': 'Chen Lu'}, {'name': 'Xiao Ding'}, {'name': 'Zhongyang Li'}, {'name': 'Ting Liu'}, {'name': 'Bing Qin'}]",2024-08-06T10:15:15Z
http://arxiv.org/abs/2408.03074v1,http://arxiv.org/abs/2408.03074v1,"Towards an Analysis of Discourse and Interactional Pragmatic Reasoning
  Capabilities of Large Language Models","In this work, we want to give an overview on which pragmatic abilities have
been tested in LLMs so far and how these tests have been carried out. To do
this, we first discuss the scope of the field of pragmatics and suggest a
subdivision into discourse pragmatics and interactional pragmatics. We give a
non-exhaustive overview of the phenomena of those two subdomains and the
methods traditionally used to analyze them. We subsequently consider the
resulting heterogeneous set of phenomena and methods as a starting point for
our survey of work on discourse pragmatics and interactional pragmatics in the
context of LLMs.","[{'name': 'Amelie Robrecht'}, {'name': 'Judith Sieker'}, {'name': 'Clara Lachenmaier'}, {'name': 'Sina Zarieß'}, {'name': 'Stefan Kopp'}]",2024-08-06T10:02:05Z
http://arxiv.org/abs/2408.03070v1,http://arxiv.org/abs/2408.03070v1,Probing structural constraints of negation in Pretrained Language Models,"Contradictory results about the encoding of the semantic impact of negation
in pretrained language models (PLMs). have been drawn recently (e.g. Kassner
and Sch{\""u}tze (2020); Gubelmann and Handschuh (2022)). In this paper we focus
rather on the way PLMs encode negation and its formal impact, through the
phenomenon of the Negative Polarity Item (NPI) licensing in English. More
precisely, we use probes to identify which contextual representations best
encode 1) the presence of negation in a sentence, and 2) the polarity of a
neighboring masked polarity item. We find that contextual representations of
tokens inside the negation scope do allow for (i) a better prediction of the
presence of not compared to those outside the scope and (ii) a better
prediction of the right polarity of a masked polarity item licensed by not,
although the magnitude of the difference varies from PLM to PLM. Importantly,
in both cases the trend holds even when controlling for distance to not. This
tends to indicate that the embeddings of these models do reflect the notion of
negation scope, and do encode the impact of negation on NPI licensing. Yet,
further control experiments reveal that the presence of other lexical items is
also better captured when using the contextual representation of a token within
the same syntactic clause than outside from it, suggesting that PLMs simply
capture the more general notion of syntactic clause.","[{'name': 'David Kletz'}, {'name': 'Marie Candito'}, {'name': 'Pascal Amsili'}]",2024-08-06T09:54:49Z
http://arxiv.org/abs/2408.03062v1,http://arxiv.org/abs/2408.03062v1,"Analysis of Argument Structure Constructions in a Deep Recurrent
  Language Model","Understanding how language and linguistic constructions are processed in the
brain is a fundamental question in cognitive computational neuroscience. In
this study, we explore the representation and processing of Argument Structure
Constructions (ASCs) in a recurrent neural language model. We trained a Long
Short-Term Memory (LSTM) network on a custom-made dataset consisting of 2000
sentences, generated using GPT-4, representing four distinct ASCs: transitive,
ditransitive, caused-motion, and resultative constructions.
  We analyzed the internal activations of the LSTM model's hidden layers using
Multidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding
(t-SNE) to visualize the sentence representations. The Generalized
Discrimination Value (GDV) was calculated to quantify the degree of clustering
within these representations. Our results show that sentence representations
form distinct clusters corresponding to the four ASCs across all hidden layers,
with the most pronounced clustering observed in the last hidden layer before
the output layer. This indicates that even a relatively simple,
brain-constrained recurrent neural network can effectively differentiate
between various construction types.
  These findings are consistent with previous studies demonstrating the
emergence of word class and syntax rule representations in recurrent language
models trained on next word prediction tasks. In future work, we aim to
validate these results using larger language models and compare them with
neuroimaging data obtained during continuous speech perception. This study
highlights the potential of recurrent neural language models to mirror
linguistic processing in the human brain, providing valuable insights into the
computational and neural mechanisms underlying language understanding.","[{'name': 'Pegah Ramezani'}, {'name': 'Achim Schilling'}, {'name': 'Patrick Krauss'}]",2024-08-06T09:27:41Z
http://arxiv.org/abs/2408.03354v2,http://arxiv.org/abs/2408.03354v2,"The Use of Large Language Models (LLM) for Cyber Threat Intelligence
  (CTI) in Cybercrime Forums","Large language models (LLMs) can be used to analyze cyber threat intelligence
(CTI) data from cybercrime forums, which contain extensive information and key
discussions about emerging cyber threats. However, to date, the level of
accuracy and efficiency of LLMs for such critical tasks has yet to be
thoroughly evaluated. Hence, this study assesses the accuracy of an LLM system
built on the OpenAI GPT-3.5-turbo model [7] to extract CTI information. To do
so, a random sample of 500 daily conversations from three cybercrime forums,
XSS, Exploit_in, and RAMP, was extracted, and the LLM system was instructed to
summarize the conversations and code 10 key CTI variables, such as whether a
large organization and/or a critical infrastructure is being targeted. Then,
two coders reviewed each conversation and evaluated whether the information
extracted by the LLM was accurate. The LLM system performed strikingly well,
with an average accuracy score of 98%. Various ways to enhance the model were
uncovered, such as the need to help the LLM distinguish between stories and
past events, as well as being careful with verb tenses in prompts.
Nevertheless, the results of this study highlight the efficiency and relevance
of using LLMs for cyber threat intelligence.","[{'name': 'Vanessa Clairoux-Trepanier'}, {'name': 'Isa-May Beauchamp'}, {'name': 'Estelle Ruellan'}, {'name': 'Masarah Paquet-Clouston'}, {'name': 'Serge-Olivier Paquette'}, {'name': 'Eric Clay'}]",2024-08-06T09:15:25Z
http://arxiv.org/abs/2408.03047v1,http://arxiv.org/abs/2408.03047v1,"OpenOmni: A Collaborative Open Source Tool for Building Future-Ready
  Multimodal Conversational Agents","Multimodal conversational agents are highly desirable because they offer
natural and human-like interaction. However, there is a lack of comprehensive
end-to-end solutions to support collaborative development and benchmarking.
While proprietary systems like GPT-4o and Gemini demonstrating impressive
integration of audio, video, and text with response times of 200-250ms,
challenges remain in balancing latency, accuracy, cost, and data privacy. To
better understand and quantify these issues, we developed OpenOmni, an
open-source, end-to-end pipeline benchmarking tool that integrates advanced
technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented
Generation, Large Language Models, along with the ability to integrate
customized models. OpenOmni supports local and cloud deployment, ensuring data
privacy and supporting latency and accuracy benchmarking. This flexible
framework allows researchers to customize the pipeline, focusing on real
bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can
significantly enhance applications like indoor assistance for visually impaired
individuals, advancing human-computer interaction. Our demonstration video is
available https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via
https://openomni.ai4wa.com, code is available via
https://github.com/AI4WA/OpenOmniFramework.","[{'name': 'Qiang Sun'}, {'name': 'Yuanyi Luo'}, {'name': 'Sirui Li'}, {'name': 'Wenxiao Zhang'}, {'name': 'Wei Liu'}]",2024-08-06T09:02:53Z
http://arxiv.org/abs/2408.03033v1,http://arxiv.org/abs/2408.03033v1,"L3iTC at the FinLLM Challenge Task: Quantization for Financial Text
  Classification & Summarization","This article details our participation (L3iTC) in the FinLLM Challenge Task
2024, focusing on two key areas: Task 1, financial text classification, and
Task 2, financial text summarization. To address these challenges, we
fine-tuned several large language models (LLMs) to optimize performance for
each task. Specifically, we used 4-bit quantization and LoRA to determine which
layers of the LLMs should be trained at a lower precision. This approach not
only accelerated the fine-tuning process on the training data provided by the
organizers but also enabled us to run the models on low GPU memory. Our
fine-tuned models achieved third place for the financial classification task
with an F1-score of 0.7543 and secured sixth place in the financial
summarization task on the official test datasets.","[{'name': 'Elvys Linhares Pontes'}, {'name': 'Carlos-Emiliano González-Gallardo'}, {'name': 'Mohamed Benjannet'}, {'name': 'Caryn Qu'}, {'name': 'Antoine Doucet'}]",2024-08-06T08:25:49Z
http://arxiv.org/abs/2408.04664v1,http://arxiv.org/abs/2408.04664v1,"Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via
  Language-Contrastive Decoding (LCD)","Large Vision-Language Models (LVLMs) are an extension of Large Language
Models (LLMs) that facilitate processing both image and text inputs, expanding
AI capabilities. However, LVLMs struggle with object hallucinations due to
their reliance on text cues and learned object co-occurrence biases. While most
research quantifies these hallucinations, mitigation strategies are still
lacking. Our study introduces a Language Contrastive Decoding (LCD) algorithm
that adjusts LVLM outputs based on LLM distribution confidence levels,
effectively reducing object hallucinations. We demonstrate the advantages of
LCD in leading LVLMs, showing up to %4 improvement in POPE F1 scores and up to
%36 reduction in CHAIR scores on the COCO validation set, while also improving
captioning quality scores. Our method effectively improves LVLMs without
needing complex post-processing or retraining, and is easily applicable to
different models. Our findings highlight the potential of further exploration
of LVLM-specific decoding algorithms.","[{'name': 'Avshalom Manevich'}, {'name': 'Reut Tsarfaty'}]",2024-08-06T08:10:34Z
http://arxiv.org/abs/2408.04663v1,http://arxiv.org/abs/2408.04663v1,"Dopamin: Transformer-based Comment Classifiers through Domain
  Post-Training and Multi-level Layer Aggregation","Code comments provide important information for understanding the source
code. They can help developers understand the overall purpose of a function or
class, as well as identify bugs and technical debt. However, an overabundance
of comments is meaningless and counterproductive. As a result, it is critical
to automatically filter out these comments for specific purposes. In this
paper, we present Dopamin, a Transformer-based tool for dealing with this
issue. Our model excels not only in presenting knowledge sharing of common
categories across multiple languages, but also in achieving robust performance
in comment classification by improving comment representation. As a result, it
outperforms the STACC baseline by 3% on the NLBSE'24 Tool Competition dataset
in terms of average F1-score, while maintaining a comparable inference time for
practical use. The source code is publicity available at
https://github.com/FSoft-AI4Code/Dopamin.","[{'name': 'Nam Le Hai'}, {'name': 'Nghi D. Q. Bui'}]",2024-08-06T08:08:43Z
http://arxiv.org/abs/2408.03010v1,http://arxiv.org/abs/2408.03010v1,"Fact Finder -- Enhancing Domain Expertise of Large Language Models by
  Incorporating Knowledge Graphs","Recent advancements in Large Language Models (LLMs) have showcased their
proficiency in answering natural language queries. However, their effectiveness
is hindered by limited domain-specific knowledge, raising concerns about the
reliability of their responses. We introduce a hybrid system that augments LLMs
with domain-specific knowledge graphs (KGs), thereby aiming to enhance factual
correctness using a KG-based retrieval approach. We focus on a medical KG to
demonstrate our methodology, which includes (1) pre-processing, (2) Cypher
query generation, (3) Cypher query processing, (4) KG retrieval, and (5)
LLM-enhanced response generation. We evaluate our system on a curated dataset
of 69 samples, achieving a precision of 78\% in retrieving correct KG nodes.
Our findings indicate that the hybrid system surpasses a standalone LLM in
accuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.
This positions the system as a promising tool for applications that demand
factual correctness and completeness, such as target identification -- a
critical process in pinpointing biological entities for disease treatment or
crop enhancement. Moreover, its intuitive search interface and ability to
provide accurate responses within seconds make it well-suited for
time-sensitive, precision-focused research contexts. We publish the source code
together with the dataset and the prompt templates used.","[{'name': 'Daniel Steinigen'}, {'name': 'Roman Teucher'}, {'name': 'Timm Heine Ruland'}, {'name': 'Max Rudat'}, {'name': 'Nicolas Flores-Herr'}, {'name': 'Peter Fischer'}, {'name': 'Nikola Milosevic'}, {'name': 'Christopher Schymura'}, {'name': 'Angelo Ziletti'}]",2024-08-06T07:45:05Z
http://arxiv.org/abs/2408.02976v1,http://arxiv.org/abs/2408.02976v1,"Empathy Level Alignment via Reinforcement Learning for Empathetic
  Response Generation","Empathetic response generation, aiming at understanding the user's situation
and feelings and respond empathically, is crucial in building human-like
dialogue systems. Previous methods mainly focus on using maximum likelihood
estimation as the optimization objective for training response generation
models, without taking into account the empathy level alignment between
generated responses and target responses. To this end, we propose an empathetic
response generation using reinforcement learning (EmpRL) framework. The
framework designs an effective empathy reward function and generates empathetic
responses by maximizing the expected reward through reinforcement learning.
Given the powerful text generation capability of pre-trained language models,
EmpRL utilizes the pre-trained T5 model as the generator and conducts further
training to initialize the policy. To align the empathy level between generated
responses and target responses in the context, an empathy reward function
containing three empathy communication mechanisms, i.e., emotional reaction,
interpretation, and exploration, is constructed using pre-designed and
pre-trained empathy identifiers. Finally, the proximal policy optimization
algorithm is used to further train the policy to produce empathetic responses.
Both automatic and manual evaluations demonstrate that the proposed EmpRL
framework can improve the quality of generated responses, enhance the empathy
level similarity between generated and target responses, and produce empathetic
responses covering both affective and cognitive aspects.","[{'name': 'Hui Ma'}, {'name': 'Bo Zhang'}, {'name': 'Bo Xu'}, {'name': 'Jian Wang'}, {'name': 'Hongfei Lin'}, {'name': 'Xiao Sun'}]",2024-08-06T06:16:00Z
http://arxiv.org/abs/2408.02970v1,http://arxiv.org/abs/2408.02970v1,"EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and
  Quantization","Large language models (LLMs) have attracted considerable attention in various
fields for their cost-effective solutions to diverse challenges, especially
with advancements in instruction tuning and quantization. E-commerce, with its
complex tasks and extensive product-user interactions, presents a promising
application area for LLMs. However, the domain-specific concepts and knowledge
inherent in e-commerce pose significant challenges for adapting general LLMs.
To address this issue, we developed EC-Guide
\href{https://github.com/fzp0424/EC-Guide-KDDUP-2024}, a comprehensive
e-commerce guide for instruction tuning and quantization of LLMs. We also
heuristically integrated Chain-of-Thought (CoT) during inference to enhance
arithmetic performance. Our approach achieved the 2nd place in Track 2 and 5th
place in Track 5 at the Amazon KDD Cup'24
\href{https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms}.
Additionally, our solution is model-agnostic, enabling effective scalability
across larger systems.","[{'name': 'Zhaopeng Feng'}, {'name': 'Zijie Meng'}, {'name': 'Zuozhu Liu'}]",2024-08-06T05:50:41Z
http://arxiv.org/abs/2408.02964v2,http://arxiv.org/abs/2408.02964v2,"Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The
  Impact of Prompt Engineering and Knowledge Retrieval","Large language models (LLMs) are fundamentally transforming human-facing
applications in the health and well-being domains: boosting patient engagement,
accelerating clinical decision-making, and facilitating medical education.
Although state-of-the-art LLMs have shown superior performance in several
conversational applications, evaluations within nutrition and diet applications
are still insufficient. In this paper, we propose to employ the Registered
Dietitian (RD) exam to conduct a standard and comprehensive evaluation of
state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing
both accuracy and consistency in nutrition queries. Our evaluation includes
1050 RD exam questions encompassing several nutrition topics and proficiency
levels. In addition, for the first time, we examine the impact of Zero-Shot
(ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC),
and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the
responses. Our findings revealed that while these LLMs obtained acceptable
overall performance, their results varied considerably with different prompts
and question domains. GPT-4o with CoT-SC prompting outperformed the other
approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.
For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both
accuracy and consistency. RAP was particularly effective for GPT-4o to answer
Expert level questions. Consequently, choosing the appropriate LLM and
prompting technique, tailored to the proficiency level and specific domain, can
mitigate errors and potential risks in diet and nutrition chatbots.","[{'name': 'Iman Azimi'}, {'name': 'Mohan Qi'}, {'name': 'Li Wang'}, {'name': 'Amir M. Rahmani'}, {'name': 'Youlin Li'}]",2024-08-06T05:21:13Z
http://arxiv.org/abs/2408.02948v1,http://arxiv.org/abs/2408.02948v1,"Are Female Carpenters like Blue Bananas? A Corpus Investigation of
  Occupation Gender Typicality","People tend to use language to mention surprising properties of events: for
example, when a banana is blue, we are more likely to mention color than when
it is yellow. This fact is taken to suggest that yellowness is somehow a
typical feature of bananas, and blueness is exceptional. Similar to how a
yellow color is typical of bananas, there may also be genders that are typical
of occupations. In this work, we explore this question using information
theoretic techniques coupled with corpus statistic analysis. In two distinct
large corpora, we do not find strong evidence that occupations and gender
display the same patterns of mentioning as do bananas and color. Instead, we
find that gender mentioning is correlated with femaleness of occupation in
particular, suggesting perhaps that woman-dominated occupations are seen as
somehow ``more gendered'' than male-dominated ones, and thereby they encourage
more gender mentioning overall.","[{'name': 'Da Ju'}, {'name': 'Karen Ulrich'}, {'name': 'Adina Williams'}]",2024-08-06T04:19:23Z
http://arxiv.org/abs/2408.02945v1,http://arxiv.org/abs/2408.02945v1,Self-Supervised Learning for Multi-Channel Neural Transducer,"Self-supervised learning, such as with the wav2vec 2.0 framework
significantly improves the accuracy of end-to-end automatic speech recognition
(ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In
this work, we explored a self-supervised learning method for a multi-channel
end-to-end ASR model based on the wav2vec 2.0 framework. As the multi-channel
end-to-end ASR model, we focused on a multi-channel neural transducer. In
pre-training, we compared three different methods for feature quantization to
train a multi-channel conformer audio encoder: joint quantization, feature-wise
quantization and channel-wise quantization. In fine-tuning, we trained the
multi-channel conformer-transducer. All experiments were conducted using the
far-field in-house and CHiME-4 datasets. The results of the experiments showed
that feature-wise quantization was the most effective among the methods. We
observed a 66% relative reduction in character error rate compared with the
model without any pre-training for the far-field in-house dataset.",[{'name': 'Atsushi Kojima'}],2024-08-06T04:12:31Z
http://arxiv.org/abs/2408.02927v1,http://arxiv.org/abs/2408.02927v1,"HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy
  Protection","Data serves as the fundamental foundation for advancing deep learning,
particularly tabular data presented in a structured format, which is highly
conducive to modeling. However, even in the era of LLM, obtaining tabular data
from sensitive domains remains a challenge due to privacy or copyright
concerns. Hence, exploring how to effectively use models like LLMs to generate
realistic and privacy-preserving synthetic tabular data is urgent. In this
paper, we take a step forward to explore LLMs for tabular data synthesis and
privacy protection, by introducing a new framework HARMONIC for tabular data
generation and evaluation. In the tabular data generation of our framework,
unlike previous small-scale LLM-based methods that rely on continued
pre-training, we explore the larger-scale LLMs with fine-tuning to generate
tabular data and enhance privacy. Based on idea of the k-nearest neighbors
algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to
discover inter-row relationships. Then, with fine-tuning, LLMs are trained to
remember the format and connections of the data rather than the data itself,
which reduces the risk of privacy leakage. In the evaluation part of our
framework, we develop specific privacy risk metrics DLT for LLM synthetic data
generation, as well as performance evaluation metrics LLE for downstream LLM
tasks. Our experiments find that this tabular data generation framework
achieves equivalent performance to existing methods with better privacy, which
also demonstrates our evaluation framework for the effectiveness of synthetic
data and privacy risks in LLM scenarios.","[{'name': 'Yuxin Wang'}, {'name': 'Duanyu Feng'}, {'name': 'Yongfu Dai'}, {'name': 'Zhengyu Chen'}, {'name': 'Jimin Huang'}, {'name': 'Sophia Ananiadou'}, {'name': 'Qianqian Xie'}, {'name': 'Hao Wang'}]",2024-08-06T03:21:13Z
http://arxiv.org/abs/2408.02923v1,http://arxiv.org/abs/2408.02923v1,Intermediate direct preference optimization,"We propose the intermediate direct preference optimization (DPO) method to
calculate the DPO loss at selected intermediate layers as an auxiliary loss for
finetuning large language models (LLMs). The conventional DPO method fine-tunes
a supervised fine-tuning (SFT) model by calculating the DPO loss using logits
from the final layer. In our intermediate DPO approach, DPO losses are
calculated using the logits from K-selected intermediate layers and averaged to
obtain the intermediate DPO loss. For training the intermediate DPO model, the
final loss is obtained by calculating the weighted sum of the DPO and
intermediate DPO losses. During inference, the intermediate DPO model decodes
using the final layer logits similarly to the conventional DPO model. In
experiments using the ultrafeedback dataset, the performance of the
intermediate DPO model was evaluated using GPT-4. As a result, the intermediate
DPO model trained using the intermediate DPO loss calculated at the 22nd layer
of a 32-layer SFT model achieved win rates of 52.5% and 67.5% against the
conventional DPO and SFT models, respectively, demonstrating the effectiveness
of the proposed method. Furthermore, we report the relationships among the
position of the selected intermediate layers, the number of layers, and
performance.",[{'name': 'Atsushi Kojima'}],2024-08-06T03:16:09Z
http://arxiv.org/abs/2408.02919v1,http://arxiv.org/abs/2408.02919v1,Data Checklist: On Unit-Testing Datasets with Usable Information,"Model checklists (Ribeiro et al., 2020) have emerged as a useful tool for
understanding the behavior of LLMs, analogous to unit-testing in software
engineering. However, despite datasets being a key determinant of model
behavior, evaluating datasets, e.g., for the existence of annotation artifacts,
is largely done ad hoc, once a problem in model behavior has already been found
downstream. In this work, we take a more principled approach to unit-testing
datasets by proposing a taxonomy based on the V-information literature. We call
a collection of such unit tests a data checklist. Using a checklist, not only
are we able to recover known artifacts in well-known datasets such as SNLI, but
we also discover previously unknown artifacts in preference datasets for LLM
alignment. Data checklists further enable a new kind of data filtering, which
we use to improve the efficacy and data efficiency of preference alignment.","[{'name': 'Heidi C. Zhang'}, {'name': 'Shabnam Behzad'}, {'name': 'Kawin Ethayarajh'}, {'name': 'Dan Jurafsky'}]",2024-08-06T03:08:36Z
http://arxiv.org/abs/2408.02907v1,http://arxiv.org/abs/2408.02907v1,"Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large
  Language Model-Based Question Answering","Retrieving external knowledge and prompting large language models with
relevant information is an effective paradigm to enhance the performance of
question-answering tasks. Previous research typically handles paragraphs from
external documents in isolation, resulting in a lack of context and ambiguous
references, particularly in multi-document and complex tasks. To overcome these
challenges, we propose a new retrieval framework IIER, that leverages
Inter-chunk Interactions to Enhance Retrieval. This framework captures the
internal connections between document chunks by considering three types of
interactions: structural, keyword, and semantic. We then construct a unified
Chunk-Interaction Graph to represent all external documents comprehensively.
Additionally, we design a graph-based evidence chain retriever that utilizes
previous paths and chunk interactions to guide the retrieval process. It
identifies multiple seed nodes based on the target question and iteratively
searches for relevant chunks to gather supporting evidence. This retrieval
process refines the context and reasoning chain, aiding the large language
model in reasoning and answer generation. Extensive experiments demonstrate
that IIER outperforms strong baselines across four datasets, highlighting its
effectiveness in improving retrieval and reasoning capabilities.","[{'name': 'Tiezheng Guo'}, {'name': 'Chen Wang'}, {'name': 'Yanyi Liu'}, {'name': 'Jiawei Tang'}, {'name': 'Pan Li'}, {'name': 'Sai Xu'}, {'name': 'Qingwen Yang'}, {'name': 'Xianlin Gao'}, {'name': 'Zhi Li'}, {'name': 'Yingyou Wen'}]",2024-08-06T02:39:55Z
http://arxiv.org/abs/2408.02901v1,http://arxiv.org/abs/2408.02901v1,"Lighthouse: A User-Friendly Library for Reproducible Video Moment
  Retrieval and Highlight Detection","We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.","[{'name': 'Taichi Nishimura'}, {'name': 'Shota Nakada'}, {'name': 'Hokuto Munakata'}, {'name': 'Tatsuya Komatsu'}]",2024-08-06T02:15:12Z
http://arxiv.org/abs/2408.04662v1,http://arxiv.org/abs/2408.04662v1,Citekit: A Modular Toolkit for Large Language Model Citation Generation,"Enabling Large Language Models (LLMs) to generate citations in
Question-Answering (QA) tasks is an emerging paradigm aimed at enhancing the
verifiability of their responses when LLMs are utilizing external references to
generate an answer. However, there is currently no unified framework to
standardize and fairly compare different citation generation methods, leading
to difficulties in reproducing different methods and a comprehensive
assessment. To cope with the problems above, we introduce \name, an open-source
and modular toolkit designed to facilitate the implementation and evaluation of
existing citation generation methods, while also fostering the development of
new approaches to improve citation quality in LLM outputs. This tool is highly
extensible, allowing users to utilize 4 main modules and 14 components to
construct a pipeline, evaluating an existing method or innovative designs. Our
experiments with two state-of-the-art LLMs and 11 citation generation baselines
demonstrate varying strengths of different modules in answer accuracy and
citation quality improvement, as well as the challenge of enhancing
granularity. Based on our analysis of the effectiveness of components, we
propose a new method, self-RAG \snippet, obtaining a balanced answer accuracy
and citation quality. Citekit is released at
https://github.com/SjJ1017/Citekit.","[{'name': 'Jiajun Shen'}, {'name': 'Tong Zhou'}, {'name': 'Suifeng Zhao'}, {'name': 'Yubo Chen'}, {'name': 'Kang Liu'}]",2024-08-06T02:13:15Z
http://arxiv.org/abs/2408.02899v1,http://arxiv.org/abs/2408.02899v1,SETN: Stock Embedding Enhanced with Textual and Network Information,"Stock embedding is a method for vector representation of stocks. There is a
growing demand for vector representations of stock, i.e., stock embedding, in
wealth management sectors, and the method has been applied to various tasks
such as stock price prediction, portfolio optimization, and similar fund
identifications. Stock embeddings have the advantage of enabling the
quantification of relative relationships between stocks, and they can extract
useful information from unstructured data such as text and network data. In
this study, we propose stock embedding enhanced with textual and network
information (SETN) using a domain-adaptive pre-trained transformer-based model
to embed textual information and a graph neural network model to grasp network
information. We evaluate the performance of our proposed model on related
company information extraction tasks. We also demonstrate that stock embeddings
obtained from the proposed model perform better in creating thematic funds than
those obtained from baseline methods, providing a promising pathway for various
applications in the wealth management industry.","[{'name': 'Takehiro Takayanagi'}, {'name': 'Hiroki Sakaji'}, {'name': 'Kiyoshi Izumi'}]",2024-08-06T02:07:37Z
http://arxiv.org/abs/2408.02865v1,http://arxiv.org/abs/2408.02865v1,"VisionUnite: A Vision-Language Foundation Model for Ophthalmology
  Enhanced with Clinical Knowledge","The need for improved diagnostic methods in ophthalmology is acute,
especially in the less developed regions with limited access to specialists and
advanced equipment. Therefore, we introduce VisionUnite, a novel
vision-language foundation model for ophthalmology enhanced with clinical
knowledge. VisionUnite has been pretrained on an extensive dataset comprising
1.24 million image-text pairs, and further refined using our proposed MMFundus
dataset, which includes 296,379 high-quality fundus image-text pairs and
889,137 simulated doctor-patient dialogue instances. Our experiments indicate
that VisionUnite outperforms existing generative foundation models such as
GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable
to junior ophthalmologists. VisionUnite performs well in various clinical
scenarios including open-ended multi-disease diagnosis, clinical explanation,
and patient interaction, making it a highly versatile tool for initial
ophthalmic disease screening. VisionUnite can also serve as an educational aid
for junior ophthalmologists, accelerating their acquisition of knowledge
regarding both common and rare ophthalmic conditions. VisionUnite represents a
significant advancement in ophthalmology, with broad implications for
diagnostics, medical education, and understanding of disease mechanisms.","[{'name': 'Zihan Li'}, {'name': 'Diping Song'}, {'name': 'Zefeng Yang'}, {'name': 'Deming Wang'}, {'name': 'Fei Li'}, {'name': 'Xiulan Zhang'}, {'name': 'Paul E. Kinahan'}, {'name': 'Yu Qiao'}]",2024-08-05T23:31:07Z
http://arxiv.org/abs/2408.02861v1,http://arxiv.org/abs/2408.02861v1,A Framework for Fine-Tuning LLMs using Heterogeneous Feedback,"Large language models (LLMs) have been applied to a wide range of tasks,
including text summarization, web navigation, and chatbots. They have
benefitted from supervised fine-tuning (SFT) and reinforcement learning from
human feedback (RLHF) following an unsupervised pretraining. These datasets can
be difficult to collect, limited in scope, and vary in sample quality.
Additionally, datasets can vary extensively in supervision format, from
numerical to binary as well as multi-dimensional with many different values. We
present a framework for fine-tuning LLMs using heterogeneous feedback, which
has two main components. First, we combine the heterogeneous feedback data into
a single supervision format, compatible with methods like SFT and RLHF. Next,
given this unified feedback dataset, we extract a high-quality and diverse
subset to obtain performance increases potentially exceeding the full dataset.
We conduct extensive experiments to understand the effectiveness of these
techniques for incorporating heterogeneous feedback, and demonstrate
improvements from using a high-quality and diverse subset of the data. We find
that our framework is able to improve models in multiple areas simultaneously,
such as in instruction following and bias reduction.","[{'name': 'Ryan Aponte'}, {'name': 'Ryan A. Rossi'}, {'name': 'Shunan Guo'}, {'name': 'Franck Dernoncourt'}, {'name': 'Tong Yu'}, {'name': 'Xiang Chen'}, {'name': 'Subrata Mitra'}, {'name': 'Nedim Lipka'}]",2024-08-05T23:20:32Z
http://arxiv.org/abs/2408.04661v1,http://arxiv.org/abs/2408.04661v1,"MaterioMiner -- An ontology-based text mining dataset for extraction of
  process-structure-property entities","While large language models learn sound statistical representations of the
language and information therein, ontologies are symbolic knowledge
representations that can complement the former ideally. Research at this
critical intersection relies on datasets that intertwine ontologies and text
corpora to enable training and comprehensive benchmarking of neurosymbolic
models. We present the MaterioMiner dataset and the linked materials mechanics
ontology where ontological concepts from the mechanics of materials domain are
associated with textual entities within the literature corpus. Another
distinctive feature of the dataset is its eminently fine-granular annotation.
Specifically, 179 distinct classes are manually annotated by three raters
within four publications, amounting to a total of 2191 entities that were
annotated and curated. Conceptual work is presented for the symbolic
representation of causal composition-process-microstructure-property
relationships. We explore the annotation consistency between the three raters
and perform fine-tuning of pre-trained models to showcase the feasibility of
named-entity recognition model training. Reusing the dataset can foster
training and benchmarking of materials language models, automated ontology
construction, and knowledge graph generation from textual data.","[{'name': 'Ali Riza Durmaz'}, {'name': 'Akhil Thomas'}, {'name': 'Lokesh Mishra'}, {'name': 'Rachana Niranjan Murthy'}, {'name': 'Thomas Straub'}]",2024-08-05T21:42:59Z
http://arxiv.org/abs/2408.02838v1,http://arxiv.org/abs/2408.02838v1,"Interpretation of the Intent Detection Problem as Dynamics in a
  Low-dimensional Space","Intent detection is a text classification task whose aim is to recognize and
label the semantics behind a users query. It plays a critical role in various
business applications. The output of the intent detection module strongly
conditions the behavior of the whole system. This sequence analysis task is
mainly tackled using deep learning techniques. Despite the widespread use of
these techniques, the internal mechanisms used by networks to solve the problem
are poorly understood. Recent lines of work have analyzed the computational
mechanisms learned by RNNs from a dynamical systems perspective. In this work,
we investigate how different RNN architectures solve the SNIPS intent detection
problem. Sentences injected into trained networks can be interpreted as
trajectories traversing a hidden state space. This space is constrained to a
low-dimensional manifold whose dimensionality is related to the embedding and
hidden layer sizes. To generate predictions, RNN steers the trajectories
towards concrete regions, spatially aligned with the output layer matrix rows
directions. Underlying the system dynamics, an unexpected fixed point topology
has been identified with a limited number of attractors. Our results provide
new insights into the inner workings of networks that solve the intent
detection task.","[{'name': 'Eduardo Sanchez-Karhunen'}, {'name': 'Jose F. Quesada-Moreno'}, {'name': 'Miguel A. Gutiérrez-Naranjo'}]",2024-08-05T21:22:36Z
http://arxiv.org/abs/2408.05241v2,http://arxiv.org/abs/2408.05241v2,"Large Model Strategic Thinking, Small Model Efficiency: Transferring
  Theory of Mind in Large Language Models","As the performance of larger, newer Large Language Models continues to
improve for strategic Theory of Mind (ToM) tasks, the demand for these state of
the art models increases commensurately. However, their deployment is costly
both in terms of processing power and time. In this paper, we investigate the
feasibility of creating smaller, simulation-ready agents by way of fine-tuning.
To do this, we present a large pre-trained model with 20 unique scenarios that
combine a social context with a social dilemma, recording its answers, and
using them for Q\&A fine-tuning on a smaller model of the same family. Our
focus is on in-context game-theoretic decision-making, the same domain within
which human interaction occurs and that requires both a theory of mind (or a
semblance thereof) and an understanding of social dynamics. We find that the
fine-tuned smaller language model exhibited significant performance closer to
that of its larger relative, and that their improvements extended in areas and
contexts beyond the ones provided in the training examples. On average for all
games, through fine-tuning, the smaller model showed a \%46 improvement in
aligning with the behavior of the larger model, with \%100 representing
complete alignment. This suggests that our pipeline represents an efficient
method to transmit some form of theory of mind to smaller models, creating
improved and cheaply deployable algorithms in the process. Despite their
simplicity and their associated shortcomings and limitations, our findings
represent a stepping stone in the pursuit and training of specialized models
for strategic and social decision making.","[{'name': 'Nunzio Lore'}, {'name': 'Alireza Sepehr Ilami'}, {'name': 'Babak Heydari'}]",2024-08-05T20:49:48Z
http://arxiv.org/abs/2408.03350v1,http://arxiv.org/abs/2408.03350v1,miniCTX: Neural Theorem Proving with (Long-)Contexts,"We introduce miniCTX, which tests a model's ability to prove formal
mathematical theorems that depend on new definitions, lemmas, or other
contextual information that was not observed during training. miniCTX contains
theorems sourced from real Lean projects and textbooks, each associated with a
context that can span tens of thousands of tokens. Models are tasked with
proving a theorem given access to code from the theorem's repository, which
contains context that is helpful or needed for the proof. As a baseline for
miniCTX, we introduce file-tuning, a simple recipe that trains a model to
generate a proof step conditioned on the preceding file contents. File-tuning
substantially outperforms the traditional neural theorem proving approach that
fine-tunes on states alone. Additionally, our file-tuned model improves
performance on the standard miniF2F benchmark, achieving a pass rate of 33.61%,
which is a new state-of-the-art for 1.3B parameter models. Alongside miniCTX,
we offer ntp-toolkit for automatically extracting and annotating theorem
proving data, making it easy to add new projects into miniCTX to ensure that
contexts are not seen during training. miniCTX offers a challenging and
realistic perspective on evaluating neural theorem provers.","[{'name': 'Jiewen Hu'}, {'name': 'Thomas Zhu'}, {'name': 'Sean Welleck'}]",2024-08-05T20:19:18Z
http://arxiv.org/abs/2408.04660v2,http://arxiv.org/abs/2408.04660v2,XMainframe: A Large Language Model for Mainframe Modernization,"Mainframe operating systems, despite their inception in the 1940s, continue
to support critical sectors like finance and government. However, these systems
are often viewed as outdated, requiring extensive maintenance and
modernization. Addressing this challenge necessitates innovative tools that can
understand and interact with legacy codebases. To this end, we introduce
XMainframe, a state-of-the-art large language model (LLM) specifically designed
with knowledge of mainframe legacy systems and COBOL codebases. Our solution
involves the creation of an extensive data collection pipeline to produce
high-quality training datasets, enhancing XMainframe's performance in this
specialized domain. Additionally, we present MainframeBench, a comprehensive
benchmark for assessing mainframe knowledge, including multiple-choice
questions, question answering, and COBOL code summarization. Our empirical
evaluations demonstrate that XMainframe consistently outperforms existing
state-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%
higher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the
BLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times
higher than GPT-3.5 on COBOL summarization. Our work highlights the potential
of XMainframe to drive significant advancements in managing and modernizing
legacy systems, thereby enhancing productivity and saving time for software
developers.","[{'name': 'Anh T. V. Dau'}, {'name': 'Hieu Trung Dao'}, {'name': 'Anh Tuan Nguyen'}, {'name': 'Hieu Trung Tran'}, {'name': 'Phong X. Nguyen'}, {'name': 'Nghi D. Q. Bui'}]",2024-08-05T20:01:10Z
http://arxiv.org/abs/2408.02798v1,http://arxiv.org/abs/2408.02798v1,Examining Gender and Power on Wikipedia Through Face and Politeness,"We propose a framework for analyzing discourse by combining two
interdependent concepts from sociolinguistic theory: face acts and politeness.
While politeness has robust existing tools and data, face acts are less
resourced. We introduce a new corpus created by annotating Wikipedia talk pages
with face acts and we use this to train a face act tagger. We then employ our
framework to study how face and politeness interact with gender and power in
discussions between Wikipedia editors. Among other findings, we observe that
female Wikipedians are not only more polite, which is consistent with prior
studies, but that this difference corresponds with significantly more language
directed at humbling aspects of their own face. Interestingly, the distinction
nearly vanishes once limiting to editors with administrative power.","[{'name': 'Adil Soubki'}, {'name': 'Shyne Choi'}, {'name': 'Owen Rambow'}]",2024-08-05T19:28:58Z
http://arxiv.org/abs/2408.02795v1,http://arxiv.org/abs/2408.02795v1,Entity Retrieval for Answering Entity-Centric Questions,"The similarity between the question and indexed documents is a crucial factor
in document retrieval for retrieval-augmented question answering. Although this
is typically the only method for obtaining the relevant documents, it is not
the sole approach when dealing with entity-centric questions. In this study, we
propose Entity Retrieval, a novel retrieval method which rather than relying on
question-document similarity, depends on the salient entities within the
question to identify the retrieval documents. We conduct an in-depth analysis
of the performance of both dense and sparse retrieval methods in comparison to
Entity Retrieval. Our findings reveal that our method not only leads to more
accurate answers to entity-centric questions but also operates more
efficiently.","[{'name': 'Hassan S. Shavarani'}, {'name': 'Anoop Sarkar'}]",2024-08-05T19:23:20Z
http://arxiv.org/abs/2408.02784v1,http://arxiv.org/abs/2408.02784v1,LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory,"Humans are not homo economicus (i.e., rational economic beings). As humans,
we exhibit systematic behavioral biases such as loss aversion, anchoring,
framing, etc., which lead us to make suboptimal economic decisions. Insofar as
such biases may be embedded in text data on which large language models (LLMs)
are trained, to what extent are LLMs prone to the same behavioral biases?
Understanding these biases in LLMs is crucial for deploying LLMs to support
human decision-making. We propose utility theory-a paradigm at the core of
modern economic theory-as an approach to evaluate the economic biases of LLMs.
Utility theory enables the quantification and comparison of economic behavior
against benchmarks such as perfect rationality or human behavior. To
demonstrate our approach, we quantify and compare the economic behavior of a
variety of open- and closed-source LLMs. We find that the economic behavior of
current LLMs is neither entirely human-like nor entirely economicus-like. We
also find that most current LLMs struggle to maintain consistent economic
behavior across settings. Finally, we illustrate how our approach can measure
the effect of interventions such as prompting on economic biases.","[{'name': 'Jillian Ross'}, {'name': 'Yoon Kim'}, {'name': 'Andrew W. Lo'}]",2024-08-05T19:00:43Z
http://arxiv.org/abs/2408.02666v2,http://arxiv.org/abs/2408.02666v2,Self-Taught Evaluators,"Model-based evaluation is at the heart of successful model development -- as
a reward model for training, and as a replacement for human evaluation. To
train such evaluators, the standard approach is to collect a large amount of
human preference judgments over model responses, which is costly and the data
becomes stale as models improve. In this work, we present an approach that aims
to im-prove evaluators without human annotations, using synthetic training data
only. Starting from unlabeled instructions, our iterative self-improvement
scheme generates contrasting model outputs and trains an LLM-as-a-Judge to
produce reasoning traces and final judgments, repeating this training at each
new iteration using the improved predictions. Without any labeled preference
data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)
from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms
commonly used LLM judges such as GPT-4 and matches the performance of the
top-performing reward models trained with labeled examples.","[{'name': 'Tianlu Wang'}, {'name': 'Ilia Kulikov'}, {'name': 'Olga Golovneva'}, {'name': 'Ping Yu'}, {'name': 'Weizhe Yuan'}, {'name': 'Jane Dwivedi-Yu'}, {'name': 'Richard Yuanzhe Pang'}, {'name': 'Maryam Fazel-Zarandi'}, {'name': 'Jason Weston'}, {'name': 'Xian Li'}]",2024-08-05T17:57:02Z
http://arxiv.org/abs/2408.02651v1,http://arxiv.org/abs/2408.02651v1,"Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large
  Language Models?","Large Language Models (LLMs) have demonstrated impressive capabilities in
natural language tasks, but their safety and morality remain contentious due to
their training on internet text corpora. To address these concerns, alignment
techniques have been developed to improve the public usability and safety of
LLMs. Yet, the potential for generating harmful content through these models
seems to persist. This paper explores the concept of jailbreaking
LLMs-reversing their alignment through adversarial triggers. Previous methods,
such as soft embedding prompts, manually crafted prompts, and gradient-based
automatic prompts, have had limited success on black-box models due to their
requirements for model access and for producing a low variety of manually
crafted prompts, making them susceptible to being blocked. This paper
introduces a novel approach using reinforcement learning to optimize
adversarial triggers, requiring only inference API access to the target model
and a small surrogate model. Our method, which leverages a BERTScore-based
reward function, enhances the transferability and effectiveness of adversarial
triggers on new black-box models. We demonstrate that this approach improves
the performance of adversarial triggers on a previously untested language
model.","[{'name': 'Mohammad Bahrami Karkevandi'}, {'name': 'Nishant Vishwamitra'}, {'name': 'Peyman Najafirad'}]",2024-08-05T17:27:29Z
http://arxiv.org/abs/2408.02632v1,http://arxiv.org/abs/2408.02632v1,"SEAS: Self-Evolving Adversarial Safety Optimization for Large Language
  Models","As large language models (LLMs) continue to advance in capability and
influence, ensuring their security and preventing harmful outputs has become
crucial. A promising approach to address these concerns involves training
models to automatically generate adversarial prompts for red teaming. However,
the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness
of current adversarial methods, which struggle to specifically target and
explore the weaknesses of these models. To tackle these challenges, we
introduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving
}\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$
optimization framework, which enhances security by leveraging data generated by
the model itself. SEAS operates through three iterative stages: Initialization,
Attack, and Adversarial Optimization, refining both the Red Team and Target
models to improve robustness and safety. This framework reduces reliance on
manual testing and significantly enhances the security capabilities of LLMs.
Our contributions include a novel adversarial framework, a comprehensive safety
dataset, and after three iterations, the Target model achieves a security level
comparable to GPT-4, while the Red Team model shows a marked increase in attack
success rate (ASR) against advanced models.","[{'name': 'Muxi Diao'}, {'name': 'Rumei Li'}, {'name': 'Shiyang Liu'}, {'name': 'Guogang Liao'}, {'name': 'Jingang Wang'}, {'name': 'Xunliang Cai'}, {'name': 'Weiran Xu'}]",2024-08-05T16:55:06Z
http://arxiv.org/abs/2408.02622v1,http://arxiv.org/abs/2408.02622v1,Language Model Can Listen While Speaking,"Dialogue serves as the most natural manner of human-computer interaction
(HCI). Recent advancements in speech language models (SLM) have significantly
enhanced speech-based conversational AI. However, these models are limited to
turn-based conversation, lacking the ability to interact with humans in
real-time spoken scenarios, for example, being interrupted when the generated
content is not satisfactory. To address these limitations, we explore full
duplex modeling (FDM) in interactive speech language models (iSLM), focusing on
enhancing real-time interaction and, more explicitly, exploring the
quintessential ability of interruption. We introduce a novel model design,
namely listening-while-speaking language model (LSLM), an end-to-end system
equipped with both listening and speaking channels. Our LSLM employs a
token-based decoder-only TTS for speech generation and a streaming
self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses
both channels for autoregressive generation and detects turn-taking in real
time. Three fusion strategies -- early fusion, middle fusion, and late fusion
-- are explored, with middle fusion achieving an optimal balance between speech
generation and real-time interaction. Two experimental settings, command-based
FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity
to diverse instructions. Our results highlight LSLM's capability to achieve
duplex communication with minimal impact on existing systems. This study aims
to advance the development of interactive speech dialogue systems, enhancing
their applicability in real-world contexts.","[{'name': 'Ziyang Ma'}, {'name': 'Yakun Song'}, {'name': 'Chenpeng Du'}, {'name': 'Jian Cong'}, {'name': 'Zhuo Chen'}, {'name': 'Yuping Wang'}, {'name': 'Yuxuan Wang'}, {'name': 'Xie Chen'}]",2024-08-05T16:47:22Z
http://arxiv.org/abs/2408.02600v1,http://arxiv.org/abs/2408.02600v1,"BioMamba: A Pre-trained Biomedical Language Representation Model
  Leveraging Mamba","The advancement of natural language processing (NLP) in biology hinges on
models' ability to interpret intricate biomedical literature. Traditional
models often struggle with the complex and domain-specific language in this
field. In this paper, we present BioMamba, a pre-trained model specifically
designed for biomedical text mining. BioMamba builds upon the Mamba
architecture and is pre-trained on an extensive corpus of biomedical
literature. Our empirical studies demonstrate that BioMamba significantly
outperforms models like BioBERT and general-domain Mamba across various
biomedical tasks. For instance, BioMamba achieves a 100 times reduction in
perplexity and a 4 times reduction in cross-entropy loss on the BioASQ test
set. We provide an overview of the model architecture, pre-training process,
and fine-tuning techniques. Additionally, we release the code and trained model
to facilitate further research.","[{'name': 'Ling Yue'}, {'name': 'Sixue Xing'}, {'name': 'Yingzhou Lu'}, {'name': 'Tianfan Fu'}]",2024-08-05T16:21:36Z
http://arxiv.org/abs/2408.02599v1,http://arxiv.org/abs/2408.02599v1,Progressively Selective Label Enhancement for Language Model Alignment,"Large Language Models have demonstrated impressive capabilities in various
language tasks but may produce content that misaligns with human expectations,
raising ethical and legal concerns. Therefore, it is important to explore the
limitations and implement restrictions on the models to ensure safety and
compliance, with Reinforcement Learning from Human Feedback (RLHF) being the
primary method. Due to challenges in stability and scalability with the RLHF
stages, researchers are exploring alternative methods to achieve effects
comparable to those of RLHF. However, these methods often depend on large
high-quality datasets and inefficiently utilize generated data. To deal with
this problem, we propose PSLE, i.e., Progressively Selective Label Enhancement
for Language Model Alignment, a framework that fully utilizes all generated
data by guiding the model with principles to align outputs with human
expectations. Using a dynamically updated threshold, our approach ensures
efficient data utilization by incorporating all generated responses and
weighting them based on their corresponding reward scores. Experimental results
on multiple datasets demonstrate the effectiveness of PSLE compared to existing
language model alignment methods.","[{'name': 'Biao Liu'}, {'name': 'Ning Xu'}, {'name': 'Xin Geng'}]",2024-08-05T16:21:17Z
http://arxiv.org/abs/2408.02584v1,http://arxiv.org/abs/2408.02584v1,"Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality
  Aspect-Based Summarization","The ever-increasing volume of digital information necessitates efficient
methods for users to extract key insights from lengthy documents. Aspect-based
summarization offers a targeted approach, generating summaries focused on
specific aspects within a document. Despite advancements in aspect-based
summarization research, there is a continuous quest for improved model
performance. Given that large language models (LLMs) have demonstrated the
potential to revolutionize diverse tasks within natural language processing,
particularly in the problem of summarization, this paper explores the potential
of fine-tuning LLMs for the aspect-based summarization task. We evaluate the
impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,
Gemma and Aya, on a publicly available domain-specific aspect based summary
dataset. We hypothesize that this approach will enable these models to
effectively identify and extract aspect-related information, leading to
superior quality aspect-based summaries compared to the state-of-the-art. We
establish a comprehensive evaluation framework to compare the performance of
fine-tuned LLMs against competing aspect-based summarization methods and
vanilla counterparts of the fine-tuned LLMs. Our work contributes to the field
of aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs
for generating high-quality aspect-based summaries. Furthermore, it opens doors
for further exploration of using LLMs for targeted information extraction tasks
across various NLP domains.","[{'name': 'Ankan Mullick'}, {'name': 'Sombit Bose'}, {'name': 'Rounak Saha'}, {'name': 'Ayan Kumar Bhowmick'}, {'name': 'Aditya Vempaty'}, {'name': 'Pawan Goyal'}, {'name': 'Niloy Ganguly'}, {'name': 'Prasenjit Dey'}, {'name': 'Ravi Kokku'}]",2024-08-05T16:00:21Z
http://arxiv.org/abs/2408.02575v1,http://arxiv.org/abs/2408.02575v1,"Artificial Intelligence for Public Health Surveillance in Africa:
  Applications and Opportunities","Artificial Intelligence (AI) is revolutionizing various fields, including
public health surveillance. In Africa, where health systems frequently
encounter challenges such as limited resources, inadequate infrastructure,
failed health information systems and a shortage of skilled health
professionals, AI offers a transformative opportunity. This paper investigates
the applications of AI in public health surveillance across the continent,
presenting successful case studies and examining the benefits, opportunities,
and challenges of implementing AI technologies in African healthcare settings.
Our paper highlights AI's potential to enhance disease monitoring and health
outcomes, and support effective public health interventions. The findings
presented in the paper demonstrate that AI can significantly improve the
accuracy and timeliness of disease detection and prediction, optimize resource
allocation, and facilitate targeted public health strategies. Additionally, our
paper identified key barriers to the widespread adoption of AI in African
public health systems and proposed actionable recommendations to overcome these
challenges.","[{'name': 'Jean Marie Tshimula'}, {'name': 'Mitterrand Kalengayi'}, {'name': 'Dieumerci Makenga'}, {'name': 'Dorcas Lilonge'}, {'name': 'Marius Asumani'}, {'name': 'Déborah Madiya'}, {'name': 'Élie Nkuba Kalonji'}, {'name': 'Hugues Kanda'}, {'name': 'René Manassé Galekwa'}, {'name': 'Josias Kumbu'}, {'name': 'Hardy Mikese'}, {'name': 'Grace Tshimula'}, {'name': 'Jean Tshibangu Muabila'}, {'name': 'Christian N. Mayemba'}, {'name': ""D'Jeff K. Nkashama""}, {'name': 'Kalonji Kalala'}, {'name': 'Steve Ataky'}, {'name': 'Tighana Wenge Basele'}, {'name': 'Mbuyi Mukendi Didier'}, {'name': 'Selain K. Kasereka'}, {'name': 'Maximilien V. Dialufuma'}, {'name': 'Godwill Ilunga Wa Kumwita'}, {'name': 'Lionel Muyuku'}, {'name': 'Jean-Paul Kimpesa'}, {'name': 'Dominique Muteba'}, {'name': 'Aaron Aruna Abedi'}, {'name': 'Lambert Mukendi Ntobo'}, {'name': 'Gloria M. Bundutidi'}, {'name': 'Désiré Kulimba Mashinda'}, {'name': 'Emmanuel Kabengele Mpinga'}, {'name': 'Nathanaël M. Kasoro'}]",2024-08-05T15:48:51Z
http://arxiv.org/abs/2408.02559v1,http://arxiv.org/abs/2408.02559v1,"Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan:
  A Multi-Player Cooperative Game under Imperfect Information","Large language models (LLMs) have shown success in handling simple games with
imperfect information and enabling multi-agent coordination, but their ability
to facilitate practical collaboration against other agents in complex,
imperfect information environments, especially in a non-English environment,
still needs to be explored. This study investigates the applicability of
knowledge acquired by open-source and API-based LLMs to sophisticated
text-based games requiring agent collaboration under imperfect information,
comparing their performance to established baselines using other types of
agents. We propose a Theory of Mind (ToM) planning technique that allows LLM
agents to adapt their strategy against various adversaries using only game
rules, current state, and historical context as input. An external tool was
incorporated to mitigate the challenge of dynamic and extensive action spaces
in this card game. Our results show that although a performance gap exists
between current LLMs and state-of-the-art reinforcement learning (RL) models,
LLMs demonstrate ToM capabilities in this game setting. It consistently
improves their performance against opposing agents, suggesting their ability to
understand the actions of allies and adversaries and establish collaboration
with allies. To encourage further research and understanding, we have made our
codebase openly accessible.","[{'name': 'Yauwai Yim'}, {'name': 'Chunkit Chan'}, {'name': 'Tianyu Shi'}, {'name': 'Zheye Deng'}, {'name': 'Wei Fan'}, {'name': 'Tianshi Zheng'}, {'name': 'Yangqiu Song'}]",2024-08-05T15:36:46Z
http://arxiv.org/abs/2408.02545v1,http://arxiv.org/abs/2408.02545v1,"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented
  Generation","Implementing Retrieval-Augmented Generation (RAG) systems is inherently
complex, requiring deep understanding of data, use cases, and intricate design
decisions. Additionally, evaluating these systems presents significant
challenges, necessitating assessment of both retrieval accuracy and generative
quality through a multi-faceted approach. We introduce RAG Foundry, an
open-source framework for augmenting large language models for RAG use cases.
RAG Foundry integrates data creation, training, inference and evaluation into a
single workflow, facilitating the creation of data-augmented datasets for
training and evaluating large language models in RAG settings. This integration
enables rapid prototyping and experimentation with various RAG techniques,
allowing users to easily generate datasets and train RAG models using internal
or specialized knowledge sources. We demonstrate the framework effectiveness by
augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG
configurations, showcasing consistent improvements across three
knowledge-intensive datasets. Code is released as open-source in
https://github.com/IntelLabs/RAGFoundry.","[{'name': 'Daniel Fleischer'}, {'name': 'Moshe Berchansky'}, {'name': 'Moshe Wasserblat'}, {'name': 'Peter Izsak'}]",2024-08-05T15:16:24Z
http://arxiv.org/abs/2408.02544v1,http://arxiv.org/abs/2408.02544v1,"Caution for the Environment: Multimodal Agents are Susceptible to
  Environmental Distractions","This paper investigates the faithfulness of multimodal large language model
(MLLM) agents in the graphical user interface (GUI) environment, aiming to
address the research question of whether multimodal GUI agents can be
distracted by environmental context. A general setting is proposed where both
the user and the agent are benign, and the environment, while not malicious,
contains unrelated content. A wide range of MLLMs are evaluated as GUI agents
using our simulated dataset, following three working patterns with different
levels of perception. Experimental results reveal that even the most powerful
models, whether generalist agents or specialist GUI agents, are susceptible to
distractions. While recent studies predominantly focus on the helpfulness
(i.e., action accuracy) of multimodal agents, our findings indicate that these
agents are prone to environmental distractions, resulting in unfaithful
behaviors. Furthermore, we switch to the adversarial perspective and implement
environment injection, demonstrating that such unfaithfulness can be exploited,
leading to unexpected risks.","[{'name': 'Xinbei Ma'}, {'name': 'Yiting Wang'}, {'name': 'Yao Yao'}, {'name': 'Tongxin Yuan'}, {'name': 'Aston Zhang'}, {'name': 'Zhuosheng Zhang'}, {'name': 'Hai Zhao'}]",2024-08-05T15:16:22Z
http://arxiv.org/abs/2408.02520v1,http://arxiv.org/abs/2408.02520v1,"OneLove beyond the field -- A few-shot pipeline for topic and sentiment
  analysis during the FIFA World Cup in Qatar","The FIFA World Cup in Qatar was discussed extensively in the news and on
social media. Due to news reports with allegations of human rights violations,
there were calls to boycott it. Wearing a OneLove armband was part of a planned
protest activity. Controversy around the armband arose when FIFA threatened to
sanction captains who wear it. To understand what topics Twitter users Tweeted
about and what the opinion of German Twitter users was towards the OneLove
armband, we performed an analysis of German Tweets published during the World
Cup using in-context learning with LLMs. We validated the labels on human
annotations. We found that Twitter users initially discussed the armband's
impact, LGBT rights, and politics; after the ban, the conversation shifted
towards politics in sports in general, accompanied by a subtle shift in
sentiment towards neutrality. Our evaluation serves as a framework for future
research to explore the impact of sports activism and evolving public
sentiment. This is especially useful in settings where labeling datasets for
specific opinions is unfeasible, such as when events are unfolding.","[{'name': 'Christoph Rauchegger'}, {'name': 'Sonja Mei Wang'}, {'name': 'Pieter Delobelle'}]",2024-08-05T14:40:40Z
http://arxiv.org/abs/2408.04658v1,http://arxiv.org/abs/2408.04658v1,Winning Amazon KDD Cup'24,"This paper describes the winning solution of all 5 tasks for the Amazon KDD
Cup 2024 Multi Task Online Shopping Challenge for LLMs. The challenge was to
build a useful assistant, answering questions in the domain of online shopping.
The competition contained 57 diverse tasks, covering 5 different task types
(e.g. multiple choice) and across 4 different tracks (e.g. multi-lingual). Our
solution is a single model per track. We fine-tune Qwen2-72B-Instruct on our
own training dataset. As the competition released only 96 example questions, we
developed our own training dataset by processing multiple public datasets or
using Large Language Models for data augmentation and synthetic data
generation. We apply wise-ft to account for distribution shifts and ensemble
multiple LoRA adapters in one model. We employed Logits Processors to constrain
the model output on relevant tokens for the tasks. AWQ 4-bit Quantization and
vLLM are used during inference to predict the test dataset in the time
constraints of 20 to 140 minutes depending on the track. Our solution achieved
the first place in each individual track and is the first place overall of
Amazons KDD Cup 2024.","[{'name': 'Chris Deotte'}, {'name': 'Ivan Sorokin'}, {'name': 'Ahmet Erdem'}, {'name': 'Benedikt Schifferer'}, {'name': 'Gilberto Titericz Jr'}, {'name': 'Simon Jegou'}]",2024-08-05T14:40:04Z
http://arxiv.org/abs/2408.02503v1,http://arxiv.org/abs/2408.02503v1,"UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks
  With Large Language Model","Significant advancements has recently been achieved in the field of
multi-modal large language models (MLLMs), demonstrating their remarkable
capabilities in understanding and reasoning across diverse tasks. However,
these models are often trained for specific tasks and rely on task-specific
input-output formats, limiting their applicability to a broader range of tasks.
This raises a fundamental question: Can we develop a unified approach to
represent and handle different multi-modal tasks to maximize the
generalizability of MLLMs? In this paper, we propose UnifiedMLLM, a
comprehensive model designed to represent various tasks using a unified
representation. Our model exhibits strong capabilities in comprehending the
implicit intent of user instructions and preforming reasoning. In addition to
generating textual responses, our model also outputs task tokens and grounding
tokens, serving as indicators of task types and task granularity. These outputs
are subsequently routed through the task router and directed to specific expert
models for task completion. To train our model, we construct a task-specific
dataset and an 100k multi-task dataset encompassing complex scenarios.
Employing a three-stage training strategy, we equip our model with robust
reasoning and task processing capabilities while preserving its generalization
capacity and knowledge reservoir. Extensive experiments showcase the impressive
performance of our unified representation approach across various tasks,
surpassing existing methodologies. Furthermore, our approach exhibits
exceptional scalability and generality. Our code, model, and dataset will be
available at \url{https://github.com/lzw-lzw/UnifiedMLLM}.","[{'name': 'Zhaowei Li'}, {'name': 'Wei Wang'}, {'name': 'YiQing Cai'}, {'name': 'Xu Qi'}, {'name': 'Pengyu Wang'}, {'name': 'Dong Zhang'}, {'name': 'Hang Song'}, {'name': 'Botian Jiang'}, {'name': 'Zhida Huang'}, {'name': 'Tao Wang'}]",2024-08-05T14:27:39Z
http://arxiv.org/abs/2408.02479v1,http://arxiv.org/abs/2408.02479v1,"From LLMs to LLM-based Agents for Software Engineering: A Survey of
  Current, Challenges and Future","With the rise of large language models (LLMs), researchers are increasingly
exploring their applications in var ious vertical domains, such as software
engineering. LLMs have achieved remarkable success in areas including code
generation and vulnerability detection. However, they also exhibit numerous
limitations and shortcomings. LLM-based agents, a novel tech nology with the
potential for Artificial General Intelligence (AGI), combine LLMs as the core
for decision-making and action-taking, addressing some of the inherent
limitations of LLMs such as lack of autonomy and self-improvement. Despite
numerous studies and surveys exploring the possibility of using LLMs in
software engineering, it lacks a clear distinction between LLMs and LLM based
agents. It is still in its early stage for a unified standard and benchmarking
to qualify an LLM solution as an LLM-based agent in its domain. In this survey,
we broadly investigate the current practice and solutions for LLMs and
LLM-based agents for software engineering. In particular we summarise six key
topics: requirement engineering, code generation, autonomous decision-making,
software design, test generation, and software maintenance. We review and
differentiate the work of LLMs and LLM-based agents from these six topics,
examining their differences and similarities in tasks, benchmarks, and
evaluation metrics. Finally, we discuss the models and benchmarks used,
providing a comprehensive analysis of their applications and effectiveness in
software engineering. We anticipate this work will shed some lights on pushing
the boundaries of LLM-based agents in software engineering for future research.","[{'name': 'Haolin Jin'}, {'name': 'Linghan Huang'}, {'name': 'Haipeng Cai'}, {'name': 'Jun Yan'}, {'name': 'Bo Li'}, {'name': 'Huaming Chen'}]",2024-08-05T14:01:15Z
http://arxiv.org/abs/2408.02442v1,http://arxiv.org/abs/2408.02442v1,"Let Me Speak Freely? A Study on the Impact of Format Restrictions on
  Performance of Large Language Models","Structured generation, the process of producing content in standardized
formats like JSON and XML, is widely utilized in real-world applications to
extract key output information from large language models (LLMs). This study
investigates whether such constraints on generation space impact LLMs'
abilities, including reasoning and domain knowledge comprehension.
Specifically, we evaluate LLMs' performance when restricted to adhere to
structured formats versus generating free-form responses across various common
tasks. Surprisingly, we observe a significant decline in LLMs' reasoning
abilities under format restrictions. Furthermore, we find that stricter format
constraints generally lead to greater performance degradation in reasoning
tasks.","[{'name': 'Zhi Rui Tam'}, {'name': 'Cheng-Kuang Wu'}, {'name': 'Yi-Lin Tsai'}, {'name': 'Chieh-Yen Lin'}, {'name': 'Hung-yi Lee'}, {'name': 'Yun-Nung Chen'}]",2024-08-05T13:08:24Z
http://arxiv.org/abs/2408.02439v1,http://arxiv.org/abs/2408.02439v1,Long Input Benchmark for Russian Analysis,"Recent advancements in Natural Language Processing (NLP) have fostered the
development of Large Language Models (LLMs) that can solve an immense variety
of tasks. One of the key aspects of their application is their ability to work
with long text documents and to process long sequences of tokens. This has
created a demand for proper evaluation of long-context understanding. To
address this need for the Russian language, we propose LIBRA (Long Input
Benchmark for Russian Analysis), which comprises 21 adapted datasets to study
the LLM's abilities to understand long texts thoroughly. The tests are divided
into four complexity groups and allow the evaluation of models across various
context lengths ranging from 4k up to 128k tokens. We provide the open-source
datasets, codebase, and public leaderboard for LIBRA to guide forthcoming
research.","[{'name': 'Igor Churin'}, {'name': 'Murat Apishev'}, {'name': 'Maria Tikhonova'}, {'name': 'Denis Shevelev'}, {'name': 'Aydar Bulatov'}, {'name': 'Yuri Kuratov'}, {'name': 'Sergej Averkiev'}, {'name': 'Alena Fenogenova'}]",2024-08-05T12:59:35Z
http://arxiv.org/abs/2408.04656v1,http://arxiv.org/abs/2408.04656v1,Towards Semantic Markup of Mathematical Documents via User Interaction,"Mathematical documents written in LaTeX often contain ambiguities. We can
resolve some of them via semantic markup using, e.g., sTeX, which also has
other potential benefits, such as interoperability with computer algebra
systems, proof systems, and increased accessibility. However, semantic markup
is more involved than ""regular"" typesetting and presents a challenge for
authors of mathematical documents. We aim to smooth out the transition from
plain LaTeX to semantic markup by developing semi-automatic tools for authors.
In this paper we present an approach to semantic markup of formulas by
(semi-)automatically generating grammars from existing sTeX macro definitions
and parsing mathematical formulas with them. We also present a GUI-based tool
for the disambiguation of parse results and showcase its functionality and
potential using a grammar for parsing untyped $\lambda$-terms.","[{'name': 'Luka Vrečar'}, {'name': 'Joe Wells'}, {'name': 'Fairouz Kamareddine'}]",2024-08-05T12:36:40Z
http://arxiv.org/abs/2408.02417v1,http://arxiv.org/abs/2408.02417v1,"Infusing Emotions into Task-oriented Dialogue Systems: Understanding,
  Management, and Generation","Emotions are indispensable in human communication, but are often overlooked
in task-oriented dialogue (ToD) modelling, where the task success is the
primary focus. While existing works have explored user emotions or similar
concepts in some ToD tasks, none has so far included emotion modelling into a
fully-fledged ToD system nor conducted interaction with human or simulated
users. In this work, we incorporate emotion into the complete ToD processing
loop, involving understanding, management, and generation. To this end, we
extend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour
labels. Through interactive experimentation involving both simulated and human
users, we demonstrate that our proposed framework significantly enhances the
user's emotional experience as well as the task success.","[{'name': 'Shutong Feng'}, {'name': 'Hsien-chin Lin'}, {'name': 'Christian Geishauser'}, {'name': 'Nurul Lubis'}, {'name': 'Carel van Niekerk'}, {'name': 'Michael Heck'}, {'name': 'Benjamin Ruppik'}, {'name': 'Renato Vukovic'}, {'name': 'Milica Gašić'}]",2024-08-05T12:21:04Z
http://arxiv.org/abs/2408.02416v1,http://arxiv.org/abs/2408.02416v1,"Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in
  Customized Large Language Models","The drastic increase of large language models' (LLMs) parameters has led to a
new research direction of fine-tuning-free downstream customization by prompts,
i.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)
play an important role in many businesses, there has emerged growing concerns
about the prompt leakage, which undermines the intellectual properties of these
services and causes downstream attacks. In this paper, we analyze the
underlying mechanism of prompt leakage, which we refer to as prompt
memorization, and develop corresponding defending strategies. By exploring the
scaling laws in prompt extraction, we analyze key attributes that influence
prompt extraction, including model sizes, prompt lengths, as well as the types
of prompts. Then we propose two hypotheses that explain how LLMs expose their
prompts. The first is attributed to the perplexity, i.e. the familiarity of
LLMs to texts, whereas the second is based on the straightforward token
translation path in attention matrices. To defend against such threats, we
investigate whether alignments can undermine the extraction of prompts. We find
that current LLMs, even those with safety alignments like GPT-4, are highly
vulnerable to prompt extraction attacks, even under the most straightforward
user attacks. Therefore, we put forward several defense strategies with the
inspiration of our findings, which achieve 83.8\% and 71.0\% drop in the prompt
extraction rate for Llama2-7B and GPT-3.5, respectively. Source code is
avaliable at \url{https://github.com/liangzid/PromptExtractionEval}.","[{'name': 'Zi Liang'}, {'name': 'Haibo Hu'}, {'name': 'Qingqing Ye'}, {'name': 'Yaxin Xiao'}, {'name': 'Haoyang Li'}]",2024-08-05T12:20:39Z
http://arxiv.org/abs/2408.04655v2,http://arxiv.org/abs/2408.04655v2,Strong and weak alignment of large language models with human values,"Minimizing negative impacts of Artificial Intelligent (AI) systems on human
societies without human supervision requires them to be able to align with
human values. However, most current work only addresses this issue from a
technical point of view, e.g., improving current methods relying on
reinforcement learning from human feedback, neglecting what it means and is
required for alignment to occur. Here, we propose to distinguish strong and
weak value alignment. Strong alignment requires cognitive abilities (either
human-like or different from humans) such as understanding and reasoning about
agents' intentions and their ability to causally produce desired effects. We
argue that this is required for AI systems like large language models (LLMs) to
be able to recognize situations presenting a risk that human values may be
flouted. To illustrate this distinction, we present a series of prompts showing
ChatGPT's, Gemini's and Copilot's failures to recognize some of these
situations. We moreover analyze word embeddings to show that the nearest
neighbors of some human values in LLMs differ from humans' semantic
representations. We then propose a new thought experiment that we call ""the
Chinese room with a word transition dictionary"", in extension of John Searle's
famous proposal. We finally mention current promising research directions
towards a weak alignment, which could produce statistically satisfying answers
in a number of common situations, however so far without ensuring any truth
value.","[{'name': 'Mehdi Khamassi'}, {'name': 'Marceau Nahon'}, {'name': 'Raja Chatila'}]",2024-08-05T11:27:51Z
http://arxiv.org/abs/2408.02377v1,http://arxiv.org/abs/2408.02377v1,"A Few-Shot Approach for Relation Extraction Domain Adaptation using
  Large Language Models","Knowledge graphs (KGs) have been successfully applied to the analysis of
complex scientific and technological domains, with automatic KG generation
methods typically building upon relation extraction models capturing
fine-grained relations between domain entities in text. While these relations
are fully applicable across scientific areas, existing models are trained on
few domain-specific datasets such as SciERC and do not perform well on new
target domains. In this paper, we experiment with leveraging in-context
learning capabilities of Large Language Models to perform schema-constrained
data annotation, collecting in-domain training instances for a
Transformer-based relation extraction model deployed on titles and abstracts of
research papers in the Architecture, Construction, Engineering and Operations
(AECO) domain. By assessing the performance gain with respect to a baseline
Deep Learning architecture trained on off-domain data, we show that by using a
few-shot learning strategy with structured prompts and only minimal expert
annotation the presented approach can potentially support domain adaptation of
a science KG generation model.","[{'name': 'Vanni Zavarella'}, {'name': 'Juan Carlos Gamero-Salinas'}, {'name': 'Sergio Consoli'}]",2024-08-05T11:06:36Z
http://arxiv.org/abs/2408.02361v1,http://arxiv.org/abs/2408.02361v1,"Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought
  Decoding","State-of-the-art task-oriented dialogue systems typically rely on
task-specific ontologies for fulfilling user queries. The majority of
task-oriented dialogue data, such as customer service recordings, comes without
ontology and annotation. Such ontologies are normally built manually, limiting
the application of specialised systems. Dialogue ontology construction is an
approach for automating that process and typically consists of two steps: term
extraction and relation extraction. In this work, we focus on relation
extraction in a transfer learning set-up. To improve the generalisation, we
propose an extension to the decoding mechanism of large language models. We
adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning
problems, to generative relation extraction. Here, we generate multiple
branches in the decoding space and select the relations based on a confidence
threshold. By constraining the decoding to ontology terms and relations, we aim
to decrease the risk of hallucination. We conduct extensive experimentation on
two widely used datasets and find improvements in performance on target
ontology for source fine-tuned and one-shot prompted large language models.","[{'name': 'Renato Vukovic'}, {'name': 'David Arps'}, {'name': 'Carel van Niekerk'}, {'name': 'Benjamin Matthias Ruppik'}, {'name': 'Hsien-Chin Lin'}, {'name': 'Michael Heck'}, {'name': 'Milica Gašić'}]",2024-08-05T10:10:01Z
http://arxiv.org/abs/2408.02341v1,http://arxiv.org/abs/2408.02341v1,"An approach to optimize inference of the DIART speaker diarization
  pipeline","Speaker diarization answers the question ""who spoke when"" for an audio file.
In some diarization scenarios, low latency is required for transcription.
Speaker diarization with low latency is referred to as online speaker
diarization. The DIART pipeline is an online speaker diarization system. It
consists of a segmentation and an embedding model. The embedding model has the
largest share of the overall latency. The aim of this paper is to optimize the
inference latency of the DIART pipeline. Different inference optimization
methods such as knowledge distilation, pruning, quantization and layer fusion
are applied to the embedding model of the pipeline. It turns out that knowledge
distillation optimizes the latency, but has a negative effect on the accuracy.
Quantization and layer fusion also have a positive influence on the latency
without worsening the accuracy. Pruning, on the other hand, does not improve
latency.","[{'name': 'Roman Aperdannier'}, {'name': 'Sigurd Schacht'}, {'name': 'Alexander Piazza'}]",2024-08-05T09:38:07Z
http://arxiv.org/abs/2408.04653v1,http://arxiv.org/abs/2408.04653v1,Batching BPE Tokenization Merges,"The Byte Pair Encoding algorithm can be safely batched to merge hundreds of
pairs of tokens at a time when building up a tokenizer's vocabulary. This
technique combined with reducing the memory footprint of text used in
vocabulary training make it feasible to train a high quality tokenizer on a
basic laptop. This paper presents BatchBPE, an open-source pure Python
implementation of these concepts, with the goal of making experimenting with
new tokenization strategies more accessible especially in compute- and
memory-constrained contexts. BatchBPE's usefulness and malleability are
demonstrated through the training of several token vocabularies to explore the
batch merging process and experiment with preprocessing a stop word list and
ignoring the least common text chunks in a dataset. Resultant encoded lengths
of texts are used as a basic evaluation metric.",[{'name': 'Alexander P. Morgan'}],2024-08-05T09:37:21Z
http://arxiv.org/abs/2408.02337v1,http://arxiv.org/abs/2408.02337v1,"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR
  Dataset Construction","Advancements in AI and natural language processing have revolutionized
machine-human language interactions, with question answering (QA) systems
playing a pivotal role. The knowledge base question answering (KBQA) task,
utilizing structured knowledge graphs (KG), allows for handling extensive
knowledge-intensive questions. However, a significant gap exists in KBQA
datasets, especially for low-resource languages. Many existing construction
pipelines for these datasets are outdated and inefficient in human labor, and
modern assisting tools like Large Language Models (LLM) are not utilized to
reduce the workload. To address this, we have designed and implemented a
modern, semi-automated approach for creating datasets, encompassing tasks such
as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),
tailored explicitly for low-resource environments. We executed this pipeline
and introduced the PUGG dataset, the first Polish KBQA dataset, and novel
datasets for MRC and IR. Additionally, we provide a comprehensive
implementation, insightful findings, detailed statistics, and evaluation of
baseline models.","[{'name': 'Albert Sawczyn'}, {'name': 'Katsiaryna Viarenich'}, {'name': 'Konrad Wojtasik'}, {'name': 'Aleksandra Domogała'}, {'name': 'Marcin Oleksy'}, {'name': 'Maciej Piasecki'}, {'name': 'Tomasz Kajdanowicz'}]",2024-08-05T09:23:49Z
http://arxiv.org/abs/2408.02302v1,http://arxiv.org/abs/2408.02302v1,"SNFinLLM: Systematic and Nuanced Financial Domain Adaptation of Chinese
  Large Language Models","Large language models (LLMs) have become powerful tools for advancing natural
language processing applications in the financial industry. However, existing
financial LLMs often face challenges such as hallucinations or superficial
parameter training, resulting in suboptimal performance, particularly in
financial computing and machine reading comprehension (MRC). To address these
issues, we propose a novel large language model specifically designed for the
Chinese financial domain, named SNFinLLM. SNFinLLM excels in domain-specific
tasks such as answering questions, summarizing financial research reports,
analyzing sentiment, and executing financial calculations. We then perform the
supervised fine-tuning (SFT) to enhance the model's proficiency across various
financial domains. Specifically, we gather extensive financial data and create
a high-quality instruction dataset composed of news articles, professional
papers, and research reports of finance domain. Utilizing both domain-specific
and general datasets, we proceed with continuous pre-training on an established
open-source base model, resulting in SNFinLLM-base. Following this, we engage
in supervised fine-tuning (SFT) to bolster the model's capability across
multiple financial tasks. Crucially, we employ a straightforward Direct
Preference Optimization (DPO) method to better align the model with human
preferences. Extensive experiments conducted on finance benchmarks and our
evaluation dataset demonstrate that SNFinLLM markedly outperforms other
state-of-the-art financial language models. For more details, check out our
demo video here: https://www.youtube.com/watch?v=GYT-65HZwus.","[{'name': 'Shujuan Zhao'}, {'name': 'Lingfeng Qiao'}, {'name': 'Kangyang Luo'}, {'name': 'Qian-Wen Zhang'}, {'name': 'Junru Lu'}, {'name': 'Di Yin'}]",2024-08-05T08:24:24Z
http://arxiv.org/abs/2408.02290v1,http://arxiv.org/abs/2408.02290v1,"Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen
  Languages","Multilingual neural machine translation systems learn to map sentences of
different languages into a common representation space. Intuitively, with a
growing number of seen languages the encoder sentence representation grows more
flexible and easily adaptable to new languages. In this work, we test this
hypothesis by zero-shot translating from unseen languages. To deal with unknown
vocabularies from unknown languages we propose a setup where we decouple
learning of vocabulary and syntax, i.e. for each language we learn word
representations in a separate step (using cross-lingual word embeddings), and
then train to translate while keeping those word representations frozen. We
demonstrate that this setup enables zero-shot translation from entirely unseen
languages. Zero-shot translating with a model trained on Germanic and Romance
languages we achieve scores of 42.6 BLEU for Portuguese-English and 20.7 BLEU
for Russian-English on TED domain. We explore how this zero-shot translation
capability develops with varying number of languages seen by the encoder.
Lastly, we explore the effectiveness of our decoupled learning strategy for
unsupervised machine translation. By exploiting our model's zero-shot
translation capability for iterative back-translation we attain near parity
with a supervised setting.","[{'name': 'Carlos Mullov'}, {'name': 'Ngoc-Quan Pham'}, {'name': 'Alexander Waibel'}]",2024-08-05T07:58:58Z
http://arxiv.org/abs/2408.02288v1,http://arxiv.org/abs/2408.02288v1,Spin glass model of in-context learning,"Large language models show a surprising in-context learning ability -- being
able to use a prompt to form a prediction for a query, yet without additional
training, in stark contrast to old-fashioned supervised learning. Providing a
mechanistic interpretation and linking the empirical phenomenon to physics are
thus challenging and remain unsolved. We study a simple yet expressive
transformer with linear attention, and map this structure to a spin glass model
with real-valued spins, where the couplings and fields explain the intrinsic
disorder in data. The spin glass model explains how the weight parameters
interact with each other during pre-training, and most importantly why an
unseen function can be predicted by providing only a prompt yet without
training. Our theory reveals that for single instance learning, increasing the
task diversity leads to the emergence of the in-context learning, by allowing
the Boltzmann distribution to converge to a unique correct solution of weight
parameters. Therefore the pre-trained transformer displays a prediction power
in a novel prompt setting. The proposed spin glass model thus establishes a
foundation to understand the empirical success of large language models.","[{'name': 'Yuhao Li'}, {'name': 'Ruoran Bai'}, {'name': 'Haiping Huang'}]",2024-08-05T07:54:01Z
http://arxiv.org/abs/2408.02272v1,http://arxiv.org/abs/2408.02272v1,"COM Kitchens: An Unedited Overhead-view Video Dataset as a
  Vision-Language Benchmark","Procedural video understanding is gaining attention in the vision and
language community. Deep learning-based video analysis requires extensive data.
Consequently, existing works often use web videos as training resources, making
it challenging to query instructional contents from raw video observations. To
address this issue, we propose a new dataset, COM Kitchens. The dataset
consists of unedited overhead-view videos captured by smartphones, in which
participants performed food preparation based on given recipes. Fixed-viewpoint
video datasets often lack environmental diversity due to high camera setup
costs. We used modern wide-angle smartphone lenses to cover cooking counters
from sink to cooktop in an overhead view, capturing activity without in-person
assistance. With this setup, we collected a diverse dataset by distributing
smartphones to participants. With this dataset, we propose the novel
video-to-text retrieval task Online Recipe Retrieval (OnRR) and new video
captioning domain Dense Video Captioning on unedited Overhead-View videos
(DVC-OV). Our experiments verified the capabilities and limitations of current
web-video-based SOTA methods in handling these tasks.","[{'name': 'Koki Maeda'}, {'name': 'Tosho Hirasawa'}, {'name': 'Atsushi Hashimoto'}, {'name': 'Jun Harashima'}, {'name': 'Leszek Rybicki'}, {'name': 'Yusuke Fukasawa'}, {'name': 'Yoshitaka Ushiku'}]",2024-08-05T07:00:10Z
http://arxiv.org/abs/2408.02271v1,http://arxiv.org/abs/2408.02271v1,"StyEmp: Stylizing Empathetic Response Generation via Multi-Grained
  Prefix Encoder and Personality Reinforcement","Recent approaches for empathetic response generation mainly focus on
emotional resonance and user understanding, without considering the system's
personality. Consistent personality is evident in real human expression and is
important for creating trustworthy systems. To address this problem, we propose
StyEmp, which aims to stylize the empathetic response generation with a
consistent personality. Specifically, it incorporates a multi-grained prefix
mechanism designed to capture the intricate relationship between a system's
personality and its empathetic expressions. Furthermore, we introduce a
personality reinforcement module that leverages contrastive learning to
calibrate the generation model, ensuring that responses are both empathetic and
reflective of a distinct personality. Automatic and human evaluations on the
EMPATHETICDIALOGUES benchmark show that StyEmp outperforms competitive
baselines in terms of both empathy and personality expressions.","[{'name': 'Yahui Fu'}, {'name': 'Chenhui Chu'}, {'name': 'Tatsuya Kawahara'}]",2024-08-05T06:59:56Z
http://arxiv.org/abs/2408.02257v1,http://arxiv.org/abs/2408.02257v1,"To Aggregate or Not to Aggregate. That is the Question: A Case Study on
  Annotation Subjectivity in Span Prediction","This paper explores the task of automatic prediction of text spans in a legal
problem description that support a legal area label. We use a corpus of problem
descriptions written by laypeople in English that is annotated by practising
lawyers. Inherent subjectivity exists in our task because legal area
categorisation is a complex task, and lawyers often have different views on a
problem, especially in the face of legally-imprecise descriptions of issues.
Experiments show that training on majority-voted spans outperforms training on
disaggregated ones.","[{'name': 'Kemal Kurniawan'}, {'name': 'Meladel Mistica'}, {'name': 'Timothy Baldwin'}, {'name': 'Jey Han Lau'}]",2024-08-05T06:16:31Z
http://arxiv.org/abs/2408.02253v2,http://arxiv.org/abs/2408.02253v2,Advancing Post-OCR Correction: A Comparative Study of Synthetic Data,"This paper explores the application of synthetic data in the post-OCR domain
on multiple fronts by conducting experiments to assess the impact of data
volume, augmentation, and synthetic data generation methods on model
performance. Furthermore, we introduce a novel algorithm that leverages
computer vision feature detection algorithms to calculate glyph similarity for
constructing post-OCR synthetic data. Through experiments conducted across a
variety of languages, including several low-resource ones, we demonstrate that
models like ByT5 can significantly reduce Character Error Rates (CER) without
the need for manually annotated data, and our proposed synthetic data
generation method shows advantages over traditional methods, particularly in
low-resource languages.","[{'name': 'Shuhao Guan'}, {'name': 'Derek Greene'}]",2024-08-05T05:56:37Z
http://arxiv.org/abs/2408.02248v1,http://arxiv.org/abs/2408.02248v1,ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems,"Recently, there has been increasing interest in using Large Language Models
(LLMs) to construct complex multi-agent systems to perform tasks such as
compiling literature reviews, drafting consumer reports, and planning
vacations. Many tools and libraries exist for helping create such systems,
however none support recursive multi-agent systems -- where the models
themselves flexibly decide when to delegate tasks and how to organize their
delegation structure. In this work, we introduce ReDel: a toolkit for recursive
multi-agent systems that supports custom tool-use, delegation schemes,
event-based logging, and interactive replay in an easy-to-use web interface. We
show that, using ReDel, we are able to achieve significant performance gains on
agentic benchmarks and easily identify potential areas of improvements through
the visualization and debugging tools. Our code, documentation, and PyPI
package are open-source and free to use under the MIT license.","[{'name': 'Andrew Zhu'}, {'name': 'Liam Dugan'}, {'name': 'Chris Callison-Burch'}]",2024-08-05T05:43:23Z
http://arxiv.org/abs/2408.02239v1,http://arxiv.org/abs/2408.02239v1,BOTS-LM: Training Large Language Models for Setswana,"In this work we present BOTS-LM, a series of bilingual language models
proficient in both Setswana and English. Leveraging recent advancements in data
availability and efficient fine-tuning, BOTS-LM achieves performance similar to
models significantly larger than itself while maintaining computational
efficiency. Our initial release features an 8 billion parameter generative
large language model, with upcoming 0.5 billion and 1 billion parameter large
language models and a 278 million parameter encoder-only model soon to be
released. We find the 8 billion parameter model significantly outperforms
Llama-3-70B and Aya 23 on English-Setswana translation tasks, approaching the
performance of dedicated machine translation models, while approaching 70B
parameter performance on Setswana reasoning as measured by a machine translated
subset of the MMLU benchmark. To accompany the BOTS-LM series of language
models, we release the largest Setswana web dataset, SetsText, totalling over
267 million tokens. In addition, we release the largest machine translated
Setswana dataset, the first and largest synthetic Setswana dataset, training
and evaluation code, training logs, and MMLU-tsn, a machine translated subset
of MMLU.","[{'name': 'Nathan Brown'}, {'name': 'Vukosi Marivate'}]",2024-08-05T05:15:17Z
http://arxiv.org/abs/2408.02237v1,http://arxiv.org/abs/2408.02237v1,"Do Large Language Models Speak All Languages Equally? A Comparative
  Study in Low-Resource Settings","Large language models (LLMs) have garnered significant interest in natural
language processing (NLP), particularly their remarkable performance in various
downstream tasks in resource-rich languages. Recent studies have highlighted
the limitations of LLMs in low-resource languages, primarily focusing on binary
classification tasks and giving minimal attention to South Asian languages.
These limitations are primarily attributed to constraints such as dataset
scarcity, computational costs, and research gaps specific to low-resource
languages. To address this gap, we present datasets for sentiment and hate
speech tasks by translating from English to Bangla, Hindi, and Urdu,
facilitating research in low-resource language processing. Further, we
comprehensively examine zero-shot learning using multiple LLMs in English and
widely spoken South Asian languages. Our findings indicate that GPT-4
consistently outperforms Llama 2 and Gemini, with English consistently
demonstrating superior performance across diverse tasks compared to
low-resource languages. Furthermore, our analysis reveals that natural language
inference (NLI) exhibits the highest performance among the evaluated tasks,
with GPT-4 demonstrating superior capabilities.","[{'name': 'Md. Arid Hasan'}, {'name': 'Prerona Tarannum'}, {'name': 'Krishno Dey'}, {'name': 'Imran Razzak'}, {'name': 'Usman Naseem'}]",2024-08-05T05:09:23Z
http://arxiv.org/abs/2408.02233v1,http://arxiv.org/abs/2408.02233v1,"A Multi-Source Heterogeneous Knowledge Injected Prompt Learning Method
  for Legal Charge Prediction","Legal charge prediction, an essential task in legal AI, seeks to assign
accurate charge labels to case descriptions, attracting significant recent
interest. Existing methods primarily employ diverse neural network structures
for modeling case descriptions directly, failing to effectively leverage
multi-source external knowledge. We propose a prompt learning framework-based
method that simultaneously leverages multi-source heterogeneous external
knowledge from a legal knowledge base, a conversational LLM, and related legal
articles. Specifically, we match knowledge snippets in case descriptions via
the legal knowledge base and encapsulate them into the input through a hard
prompt template. Additionally, we retrieve legal articles related to a given
case description through contrastive learning, and then obtain factual elements
within the case description through a conversational LLM. We fuse the embedding
vectors of soft prompt tokens with the encoding vector of factual elements to
achieve knowledge-enhanced model forward inference. Experimental results show
that our method achieved state-of-the-art results on CAIL-2018, the largest
legal charge prediction dataset, and our method has lower data dependency. Case
studies also demonstrate our method's strong interpretability.","[{'name': 'Jingyun Sun'}, {'name': 'Chi Wei'}, {'name': 'Yang Li'}]",2024-08-05T04:53:17Z
http://arxiv.org/abs/2408.02201v1,http://arxiv.org/abs/2408.02201v1,"Evaluating the Performance of Large Language Models for SDG Mapping
  (Technical Report)","The use of large language models (LLMs) is expanding rapidly, and open-source
versions are becoming available, offering users safer and more adaptable
options. These models enable users to protect data privacy by eliminating the
need to provide data to third parties and can be customized for specific tasks.
In this study, we compare the performance of various language models on the
Sustainable Development Goal (SDG) mapping task, using the output of GPT-4o as
the baseline. The selected open-source models for comparison include Mixtral,
LLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more
specialized version of GPT-4o, was included to extend the comparison. Given the
multi-label nature of the SDG mapping task, we employed metrics such as F1
score, precision, and recall with micro-averaging to evaluate different aspects
of the models' performance. These metrics are derived from the confusion matrix
to ensure a comprehensive evaluation. We provide a clear observation and
analysis of each model's performance by plotting curves based on F1 score,
precision, and recall at different thresholds. According to the results of this
experiment, LLaMA 2 and Gemma still have significant room for improvement. The
other four models do not exhibit particularly large differences in performance.
The outputs from all seven models are available on Zenodo:
https://doi.org/10.5281/zenodo.12789375.","[{'name': 'Hui Yin'}, {'name': 'Amir Aryani'}, {'name': 'Nakul Nambiar'}]",2024-08-05T03:05:02Z
http://arxiv.org/abs/2408.02193v1,http://arxiv.org/abs/2408.02193v1,CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs,"Large language models (LLMs) have shown great potential in code-related
tasks, yet open-source models lag behind their closed-source counterparts. To
bridge this performance gap, existing methods generate vast amounts of
synthetic data for fine-tuning, leading to inefficiencies in training.
Motivated by the need for more effective and efficient training, we propose the
Code Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces
the Complexity and Diversity Aware Sampling (CDAS) method to select
high-quality training data based on complexity and diversity, and the Dynamic
Pack padding strategy to reduce computational resource usage by minimizing
padding tokens during training. Experimental results demonstrate that
CodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,
achieves an 8.6% performance increase on HumanEval, reduces training time by
78%, and decreases peak GPU memory usage by 27%. These findings underscore
CodeACT's ability to enhance the performance and efficiency of open-source
models. By optimizing both the data selection and training processes, CodeACT
offers a comprehensive approach to improving the capabilities of open-source
LLMs while significantly reducing computational requirements, addressing the
dual challenges of data quality and training efficiency, and paving the way for
more resource-efficient and performant models.","[{'name': 'Weijie Lv'}, {'name': 'Xuan Xia'}, {'name': 'Sheng-Jun Huang'}]",2024-08-05T02:38:48Z
http://arxiv.org/abs/2408.02152v1,http://arxiv.org/abs/2408.02152v1,Generative Retrieval with Few-shot Indexing,"Existing generative retrieval (GR) approaches rely on training-based
indexing, i.e., fine-tuning a model to memorise the associations between a
query and the document identifier (docid) of a relevant document.
Training-based indexing has three limitations: high training overhead,
under-utilization of the pre-trained knowledge of large language models (LLMs),
and challenges in adapting to a dynamic document corpus. To address the above
issues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).
It has a novel few-shot indexing process, where we prompt an LLM to generate
docids for all documents in a corpus, ultimately creating a docid bank for the
entire corpus. During retrieval, we feed a query to the same LLM and constrain
it to generate a docid within the docid bank created during indexing, and then
map the generated docid back to its corresponding document. Few-Shot GR relies
solely on prompting an LLM without requiring any training, making it more
efficient. Moreover, we devise few-shot indexing with one-to-many mapping to
further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves
superior performance to state-of-the-art GR methods that require heavy
training.","[{'name': 'Arian Askari'}, {'name': 'Chuan Meng'}, {'name': 'Mohammad Aliannejadi'}, {'name': 'Zhaochun Ren'}, {'name': 'Evangelos Kanoulas'}, {'name': 'Suzan Verberne'}]",2024-08-04T22:00:34Z
http://arxiv.org/abs/2408.02143v1,http://arxiv.org/abs/2408.02143v1,"Analyzing Cultural Representations of Emotions in LLMs through Mixed
  Emotion Survey","Large Language Models (LLMs) have gained widespread global adoption,
showcasing advanced linguistic capabilities across multiple of languages. There
is a growing interest in academia to use these models to simulate and study
human behaviors. However, it is crucial to acknowledge that an LLM's
proficiency in a specific language might not fully encapsulate the norms and
values associated with its culture. Concerns have emerged regarding potential
biases towards Anglo-centric cultures and values due to the predominance of
Western and US-based training data. This study focuses on analyzing the
cultural representations of emotions in LLMs, in the specific case of
mixed-emotion situations. Our methodology is based on the studies of Miyamoto
et al. (2010), which identified distinctive emotional indicators in Japanese
and American human responses. We first administer their mixed emotion survey to
five different LLMs and analyze their outputs. Second, we experiment with
contextual variables to explore variations in responses considering both
language and speaker origin. Thirdly, we expand our investigation to encompass
additional East Asian and Western European origin languages to gauge their
alignment with their respective cultures, anticipating a closer fit. We find
that (1) models have limited alignment with the evidence in the literature; (2)
written language has greater effect on LLMs' response than information on
participants origin; and (3) LLMs responses were found more similar for East
Asian languages than Western European languages.","[{'name': 'Shiran Dudy'}, {'name': 'Ibrahim Said Ahmad'}, {'name': 'Ryoko Kitajima'}, {'name': 'Agata Lapedriza'}]",2024-08-04T20:56:05Z
http://arxiv.org/abs/2408.02128v1,http://arxiv.org/abs/2408.02128v1,Table Transformers for Imputing Textual Attributes,"Missing data in tabular dataset is a common issue as the performance of
downstream tasks usually depends on the completeness of the training dataset.
Previous missing data imputation methods focus on numeric and categorical
columns, but we propose a novel end-to-end approach called Table Transformers
for Imputing Textual Attributes (TTITA) based on the transformer to impute
unstructured textual columns using other columns in the table. We conduct
extensive experiments on two Amazon Reviews datasets, and our approach shows
competitive performance outperforming baseline models such as recurrent neural
networks and Llama2. The performance improvement is more significant when the
target sequence has a longer length. Additionally, we incorporated multi-task
learning to simultaneously impute for heterogeneous columns, boosting the
performance for text imputation. We also qualitatively compare with ChatGPT for
realistic applications.","[{'name': 'Ting-Ruen Wei'}, {'name': 'Yuan Wang'}, {'name': 'Yoshitaka Inoue'}, {'name': 'Hsin-Tai Wu'}, {'name': 'Yi Fang'}]",2024-08-04T19:54:12Z
http://arxiv.org/abs/2408.02114v1,http://arxiv.org/abs/2408.02114v1,"Recent Advances in Multi-Choice Machine Reading Comprehension: A Survey
  on Methods and Datasets","This paper provides a thorough examination of recent developments in the
field of multi-choice Machine Reading Comprehension (MRC). Focused on benchmark
datasets, methodologies, challenges, and future trajectories, our goal is to
offer researchers a comprehensive overview of the current landscape in
multi-choice MRC. The analysis delves into 30 existing cloze-style and
multiple-choice MRC benchmark datasets, employing a refined classification
method based on attributes such as corpus style, domain, complexity, context
style, question style, and answer style. This classification system enhances
our understanding of each dataset's diverse attributes and categorizes them
based on their complexity. Furthermore, the paper categorizes recent
methodologies into Fine-tuned and Prompt-tuned methods. Fine-tuned methods
involve adapting pre-trained language models (PLMs) to a specific task through
retraining on domain-specific datasets, while prompt-tuned methods use prompts
to guide PLM response generation, presenting potential applications in
zero-shot or few-shot learning scenarios. By contributing to ongoing
discussions, inspiring future research directions, and fostering innovations,
this paper aims to propel multi-choice MRC towards new frontiers of
achievement.","[{'name': 'Shima Foolad'}, {'name': 'Kourosh Kiani'}, {'name': 'Razieh Rastgoo'}]",2024-08-04T18:57:21Z
http://arxiv.org/abs/2408.02103v1,http://arxiv.org/abs/2408.02103v1,"Effective Demonstration Annotation for In-Context Learning via Language
  Model-Based Determinantal Point Process","In-context learning (ICL) is a few-shot learning paradigm that involves
learning mappings through input-output pairs and appropriately applying them to
new instances. Despite the remarkable ICL capabilities demonstrated by Large
Language Models (LLMs), existing works are highly dependent on large-scale
labeled support sets, not always feasible in practical scenarios. To refine
this approach, we focus primarily on an innovative selective annotation
mechanism, which precedes the standard demonstration retrieval. We introduce
the Language Model-based Determinant Point Process (LM-DPP) that simultaneously
considers the uncertainty and diversity of unlabeled instances for optimal
selection. Consequently, this yields a subset for annotation that strikes a
trade-off between the two factors. We apply LM-DPP to various language models,
including GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2
Generation datasets demonstrate that LM-DPP can effectively select canonical
examples. Further analysis reveals that LLMs benefit most significantly from
subsets that are both low uncertainty and high diversity.","[{'name': 'Peng Wang'}, {'name': 'Xiaobin Wang'}, {'name': 'Chao Lou'}, {'name': 'Shengyu Mao'}, {'name': 'Pengjun Xie'}, {'name': 'Yong Jiang'}]",2024-08-04T18:08:15Z
http://arxiv.org/abs/2408.04652v1,http://arxiv.org/abs/2408.04652v1,"Leveraging Large Language Models with Chain-of-Thought and Prompt
  Engineering for Traffic Crash Severity Analysis and Inference","Harnessing the power of Large Language Models (LLMs), this study explores the
use of three state-of-the-art LLMs, specifically GPT-3.5-turbo, LLaMA3-8B, and
LLaMA3-70B, for crash severity inference, framing it as a classification task.
We generate textual narratives from original traffic crash tabular data using a
pre-built template infused with domain knowledge. Additionally, we incorporated
Chain-of-Thought (CoT) reasoning to guide the LLMs in analyzing the crash
causes and then inferring the severity. This study also examine the impact of
prompt engineering specifically designed for crash severity inference. The LLMs
were tasked with crash severity inference to: (1) evaluate the models'
capabilities in crash severity analysis, (2) assess the effectiveness of CoT
and domain-informed prompt engineering, and (3) examine the reasoning abilities
with the CoT framework. Our results showed that LLaMA3-70B consistently
outperformed the other models, particularly in zero-shot settings. The CoT and
Prompt Engineering techniques significantly enhanced performance, improving
logical reasoning and addressing alignment issues. Notably, the CoT offers
valuable insights into LLMs' reasoning processes, unleashing their capacity to
consider diverse factors such as environmental conditions, driver behavior, and
vehicle characteristics in severity analysis and inference.","[{'name': 'Hao Zhen'}, {'name': 'Yucheng Shi'}, {'name': 'Yongcan Huang'}, {'name': 'Jidong J. Yang'}, {'name': 'Ninghao Liu'}]",2024-08-04T17:14:10Z
http://arxiv.org/abs/2408.02085v3,http://arxiv.org/abs/2408.02085v3,"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data
  Assessment and Selection for Instruction Tuning of Language Models","Instruction tuning plays a critical role in aligning large language models
(LLMs) with human preference. Despite the vast amount of open instruction
datasets, naively training a LLM on all existing instructions may not be
optimal and practical. To pinpoint the most beneficial datapoints, data
assessment and selection methods have been proposed in the fields of natural
language processing (NLP) and deep learning. However, under the context of
instruction tuning, there still exists a gap in knowledge on what kind of data
evaluation metrics can be employed and how they can be integrated into the
selection mechanism. To bridge this gap, we present a comprehensive review on
existing literature of data assessment and selection especially for instruction
tuning of LLMs. We systematically categorize all applicable methods into
quality-based, diversity-based, and importance-based ones where a unified,
fine-grained taxonomy is structured. For each category, representative methods
are elaborated to describe the landscape of relevant research. In addition,
comparison between latest methods is conducted on their officially reported
results to provide in-depth discussions on their limitations. Finally, we
summarize the open challenges and propose the promosing avenues for future
studies. All related contents are available at
https://github.com/yuleiqin/fantastic-data-engineering.","[{'name': 'Yulei Qin'}, {'name': 'Yuncheng Yang'}, {'name': 'Pengcheng Guo'}, {'name': 'Gang Li'}, {'name': 'Hang Shao'}, {'name': 'Yuchen Shi'}, {'name': 'Zihan Xu'}, {'name': 'Yun Gu'}, {'name': 'Ke Li'}, {'name': 'Xing Sun'}]",2024-08-04T16:50:07Z
http://arxiv.org/abs/2408.02056v1,http://arxiv.org/abs/2408.02056v1,MedSyn: LLM-based Synthetic Medical Text Generation Framework,"Generating synthetic text addresses the challenge of data availability in
privacy-sensitive domains such as healthcare. This study explores the
applicability of synthetic data in real-world medical settings. We introduce
MedSyn, a novel medical text generation framework that integrates large
language models with a Medical Knowledge Graph (MKG). We use MKG to sample
prior medical information for the prompt and generate synthetic clinical notes
with GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data
through application in the ICD code prediction task. Our research indicates
that synthetic data can increase the classification accuracy of vital and
challenging codes by up to 17.8% compared to settings without synthetic data.
Furthermore, to provide new data for further research in the healthcare domain,
we present the largest open-source synthetic dataset of clinical notes for the
Russian language, comprising over 41k samples covering 219 ICD-10 codes.","[{'name': 'Gleb Kumichev'}, {'name': 'Pavel Blinov'}, {'name': 'Yulia Kuzkina'}, {'name': 'Vasily Goncharov'}, {'name': 'Galina Zubkova'}, {'name': 'Nikolai Zenovkin'}, {'name': 'Aleksei Goncharov'}, {'name': 'Andrey Savchenko'}]",2024-08-04T15:07:44Z
http://arxiv.org/abs/2408.02044v1,http://arxiv.org/abs/2408.02044v1,"Fine-tuning multilingual language models in Twitter/X sentiment
  analysis: a study on Eastern-European V4 languages","The aspect-based sentiment analysis (ABSA) is a standard NLP task with
numerous approaches and benchmarks, where large language models (LLM) represent
the current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data
in underrepresented languages. On such narrow tasks, small tuned language
models can often outperform universal large ones, providing available and cheap
solutions.
  We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for
classification of sentiment towards Russia and Ukraine in the context of the
ongoing military conflict. The training/testing dataset was obtained from the
academic API from Twitter/X during 2023, narrowed to the languages of the V4
countries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their
performance under a variety of settings including translations, sentiment
targets, in-context learning and more, using GPT4 as a reference model. We
document several interesting phenomena demonstrating, among others, that some
models are much better fine-tunable on multilingual Twitter tasks than others,
and that they can reach the SOTA level with a very small training set. Finally
we identify combinations of settings providing the best results.","[{'name': 'Tomáš Filip'}, {'name': 'Martin Pavlíček'}, {'name': 'Petr Sosík'}]",2024-08-04T14:35:30Z
http://arxiv.org/abs/2408.02006v1,http://arxiv.org/abs/2408.02006v1,LLaSA: Large Language and E-Commerce Shopping Assistant,"The e-commerce platform has evolved rapidly due to its widespread popularity
and convenience. Developing an e-commerce shopping assistant for customers is
crucial to aiding them in quickly finding desired products and recommending
precisely what they need. However, most previous shopping assistants face two
main problems: (1) task-specificity, which necessitates the development of
different models for various tasks, thereby increasing development costs and
limiting effectiveness; and (2) poor generalization, where the trained model
performs inadequately on up-to-date products. To resolve these issues, we
employ Large Language Models (LLMs) to construct an omnipotent assistant,
leveraging their adeptness at handling multiple tasks and their superior
generalization capability. Nonetheless, LLMs lack inherent knowledge of
e-commerce concepts. To address this, we create an instruction dataset
comprising 65,000 samples and diverse tasks, termed as EshopInstruct. Through
instruction tuning on our dataset, the assistant, named LLaSA, demonstrates the
potential to function as an omnipotent assistant. Additionally, we propose
various inference optimization strategies to enhance performance with limited
inference resources. In the Amazon KDD Cup 2024 Challenge, our proposed method,
LLaSA, achieved an overall ranking of 3rd place on ShopBench, including 57
tasks and approximately 20,000 questions, and we secured top-5 rankings in each
track, especially in track4, where we achieved the best performance result
among all student teams. Our extensive practices fully demonstrate that LLMs
possess the great potential to be competent e-commerce shopping assistants.","[{'name': 'Shuo Zhang'}, {'name': 'Boci Peng'}, {'name': 'Xinping Zhao'}, {'name': 'Boren Hu'}, {'name': 'Yun Zhu'}, {'name': 'Yanjia Zeng'}, {'name': 'Xuming Hu'}]",2024-08-04T12:10:51Z
http://arxiv.org/abs/2408.01969v1,http://arxiv.org/abs/2408.01969v1,Optimal and efficient text counterfactuals using Graph Neural Networks,"As NLP models become increasingly integral to decision-making processes, the
need for explainability and interpretability has become paramount. In this
work, we propose a framework that achieves the aforementioned by generating
semantically edited inputs, known as counterfactual interventions, which change
the model prediction, thus providing a form of counterfactual explanations for
the model. We test our framework on two NLP tasks - binary sentiment
classification and topic classification - and show that the generated edits are
contrastive, fluent and minimal, while the whole process remains significantly
faster that other state-of-the-art counterfactual editors.","[{'name': 'Dimitris Lymperopoulos'}, {'name': 'Maria Lymperaiou'}, {'name': 'Giorgos Filandrianos'}, {'name': 'Giorgos Stamou'}]",2024-08-04T09:09:13Z
http://arxiv.org/abs/2408.01966v1,http://arxiv.org/abs/2408.01966v1,"ML-EAT: A Multilevel Embedding Association Test for Interpretable and
  Transparent Social Science","This research introduces the Multilevel Embedding Association Test (ML-EAT),
a method designed for interpretable and transparent measurement of intrinsic
bias in language technologies. The ML-EAT addresses issues of ambiguity and
difficulty in interpreting the traditional EAT measurement by quantifying bias
at three levels of increasing granularity: the differential association between
two target concepts with two attribute concepts; the individual effect size of
each target concept with two attribute concepts; and the association between
each individual target concept and each individual attribute concept. Using the
ML-EAT, this research defines a taxonomy of EAT patterns describing the nine
possible outcomes of an embedding association test, each of which is associated
with a unique EAT-Map, a novel four-quadrant visualization for interpreting the
ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2
language models, and a CLIP language-and-image model shows that EAT patterns
add otherwise unobservable information about the component biases that make up
an EAT; reveal the effects of prompting in zero-shot models; and can also
identify situations when cosine similarity is an ineffective metric, rendering
an EAT unreliable. Our work contributes a method for rendering bias more
observable and interpretable, improving the transparency of computational
investigations into human minds and societies.","[{'name': 'Robert Wolfe'}, {'name': 'Alexis Hiniker'}, {'name': 'Bill Howe'}]",2024-08-04T09:04:44Z
http://arxiv.org/abs/2408.01963v1,http://arxiv.org/abs/2408.01963v1,"A Novel Metric for Measuring the Robustness of Large Language Models in
  Non-adversarial Scenarios","We evaluate the robustness of several large language models on multiple
datasets. Robustness here refers to the relative insensitivity of the model's
answers to meaning-preserving variants of their input. Benchmark datasets are
constructed by introducing naturally-occurring, non-malicious perturbations, or
by generating semantically equivalent paraphrases of input questions or
statements. We further propose a novel metric for assessing a model robustness,
and demonstrate its benefits in the non-adversarial scenario by empirical
evaluation of several models on the created datasets.","[{'name': 'Samuel Ackerman'}, {'name': 'Ella Rabinovich'}, {'name': 'Eitan Farchi'}, {'name': 'Ateret Anaby-Tavor'}]",2024-08-04T08:43:09Z
http://arxiv.org/abs/2408.01962v1,http://arxiv.org/abs/2408.01962v1,"The Implications of Open Generative Models in Human-Centered Data
  Science Work: A Case Study with Fact-Checking Organizations","Calls to use open generative language models in academic research have
highlighted the need for reproducibility and transparency in scientific
research. However, the impact of generative AI extends well beyond academia, as
corporations and public interest organizations have begun integrating these
models into their data science pipelines. We expand this lens to include the
impact of open models on organizations, focusing specifically on fact-checking
organizations, which use AI to observe and analyze large volumes of circulating
misinformation, yet must also ensure the reproducibility and impartiality of
their work. We wanted to understand where fact-checking organizations use open
models in their data science pipelines; what motivates their use of open models
or proprietary models; and how their use of open or proprietary models can
inform research on the societal impact of generative AI. To answer these
questions, we conducted an interview study with N=24 professionals at 20
fact-checking organizations on six continents. Based on these interviews, we
offer a five-component conceptual model of where fact-checking organizations
employ generative AI to support or automate parts of their data science
pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data
Delivery, and Data Sharing. We then provide taxonomies of fact-checking
organizations' motivations for using open models and the limitations that
prevent them for further adopting open models, finding that they prefer open
models for Organizational Autonomy, Data Privacy and Ownership, Application
Specificity, and Capability Transparency. However, they nonetheless use
proprietary models due to perceived advantages in Performance, Usability, and
Safety, as well as Opportunity Costs related to participation in emerging
generative AI ecosystems. Our work provides novel perspective on open models in
data-driven organizations.","[{'name': 'Robert Wolfe'}, {'name': 'Tanushree Mitra'}]",2024-08-04T08:41:48Z
http://arxiv.org/abs/2408.01961v1,http://arxiv.org/abs/2408.01961v1,"Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study","Popular and news media often portray teenagers with sensationalism, as both a
risk to society and at risk from society. As AI begins to absorb some of the
epistemic functions of traditional media, we study how teenagers in two
countries speaking two languages: 1) are depicted by AI, and 2) how they would
prefer to be depicted. Specifically, we study the biases about teenagers
learned by static word embeddings (SWEs) and generative language models (GLMs),
comparing these with the perspectives of adolescents living in the U.S. and
Nepal. We find English-language SWEs associate teenagers with societal
problems, and more than 50% of the 1,000 words most associated with teenagers
in the pretrained GloVe SWE reflect such problems. Given prompts about
teenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss
societal problems, most commonly violence, but also drug use, mental illness,
and sexual taboo. Nepali models, while not free of such associations, are less
dominated by social problems. Data from workshops with N=13 U.S. adolescents
and N=18 Nepalese adolescents show that AI presentations are disconnected from
teenage life, which revolves around activities like school and friendship.
Participant ratings of how well 20 trait words describe teens are decorrelated
from SWE associations, with Pearson's r=.02, n.s. in English FastText and
r=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in
GloVe. U.S. participants suggested AI could fairly present teens by
highlighting diversity, while Nepalese participants centered positivity.
Participants were optimistic that, if it learned from adolescents, rather than
media sources, AI could help mitigate stereotypes. Our work offers an
understanding of the ways SWEs and GLMs misrepresent a developmentally
vulnerable group and provides a template for less sensationalized
characterization.","[{'name': 'Robert Wolfe'}, {'name': 'Aayushi Dangol'}, {'name': 'Bill Howe'}, {'name': 'Alexis Hiniker'}]",2024-08-04T08:35:02Z
http://arxiv.org/abs/2408.01959v1,http://arxiv.org/abs/2408.01959v1,"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in
  Vision-Language AI","Multimodal AI models capable of associating images and text hold promise for
numerous domains, ranging from automated image captioning to accessibility
applications for blind and low-vision users. However, uncertainty about bias
has in some cases limited their adoption and availability. In the present work,
we study 43 CLIP vision-language models to determine whether they learn
human-like facial impression biases, and we find evidence that such biases are
reflected across three distinct CLIP model families. We show for the first time
that the the degree to which a bias is shared across a society predicts the
degree to which it is reflected in a CLIP model. Human-like impressions of
visually unobservable attributes, like trustworthiness and sexuality, emerge
only in models trained on the largest dataset, indicating that a better fit to
uncurated cultural data results in the reproduction of increasingly subtle
social biases. Moreover, we use a hierarchical clustering approach to show that
dataset size predicts the extent to which the underlying structure of facial
impression bias resembles that of facial impression bias in humans. Finally, we
show that Stable Diffusion models employing CLIP as a text encoder learn facial
impression biases, and that these biases intersect with racial biases in Stable
Diffusion XL-Turbo. While pretrained CLIP models may prove useful for
scientific studies of bias, they will also require significant dataset curation
when intended for use as general-purpose models in a zero-shot setting.","[{'name': 'Robert Wolfe'}, {'name': 'Aayushi Dangol'}, {'name': 'Alexis Hiniker'}, {'name': 'Bill Howe'}]",2024-08-04T08:26:58Z
http://arxiv.org/abs/2408.01950v1,http://arxiv.org/abs/2408.01950v1,"Why Perturbing Symbolic Music is Necessary: Fitting the Distribution of
  Never-used Notes through a Joint Probabilistic Diffusion Model","Existing music generation models are mostly language-based, neglecting the
frequency continuity property of notes, resulting in inadequate fitting of rare
or never-used notes and thus reducing the diversity of generated samples. We
argue that the distribution of notes can be modeled by translational invariance
and periodicity, especially using diffusion models to generalize notes by
injecting frequency-domain Gaussian noise. However, due to the low-density
nature of music symbols, estimating the distribution of notes latent in the
high-density solution space poses significant challenges. To address this
problem, we introduce the Music-Diff architecture, which fits a joint
distribution of notes and accompanying semantic information to generate
symbolic music conditionally. We first enhance the fragmentation module for
extracting semantics by using event-based notations and the structural
similarity index, thereby preventing boundary blurring. As a prerequisite for
multivariate perturbation, we introduce a joint pre-training method to
construct the progressions between notes and musical semantics while avoiding
direct modeling of low-density notes. Finally, we recover the perturbed notes
by a multi-branch denoiser that fits multiple noise objectives via Pareto
optimization. Our experiments suggest that in contrast to language models,
joint probability diffusion models perturbing at both note and semantic levels
can provide more sample diversity and compositional regularity. The case study
highlights the rhythmic advantages of our model over language- and DDPMs-based
models by analyzing the hierarchical structure expressed in the self-similarity
metrics.","[{'name': 'Shipei Liu'}, {'name': 'Xiaoya Fan'}, {'name': 'Guowei Wu'}]",2024-08-04T07:38:38Z
http://arxiv.org/abs/2408.01935v1,http://arxiv.org/abs/2408.01935v1,"Defining and Evaluating Decision and Composite Risk in Language Models
  Applied to Natural Language Inference","Despite their impressive performance, large language models (LLMs) such as
ChatGPT are known to pose important risks. One such set of risks arises from
misplaced confidence, whether over-confidence or under-confidence, that the
models have in their inference. While the former is well studied, the latter is
not, leading to an asymmetry in understanding the comprehensive risk of the
model based on misplaced confidence. In this paper, we address this asymmetry
by defining two types of risk (decision and composite risk), and proposing an
experimental framework consisting of a two-level inference architecture and
appropriate metrics for measuring such risks in both discriminative and
generative LLMs. The first level relies on a decision rule that determines
whether the underlying language model should abstain from inference. The second
level (which applies if the model does not abstain) is the model's inference.
Detailed experiments on four natural language commonsense reasoning datasets
using both an open-source ensemble-based RoBERTa model and ChatGPT, demonstrate
the practical utility of the evaluation framework. For example, our results
show that our framework can get an LLM to confidently respond to an extra 20.1%
of low-risk inference tasks that other methods might misclassify as high-risk,
and skip 19.8% of high-risk tasks, which would have been answered incorrectly.","[{'name': 'Ke Shen'}, {'name': 'Mayank Kejriwal'}]",2024-08-04T05:24:32Z
http://arxiv.org/abs/2408.01933v2,http://arxiv.org/abs/2408.01933v2,"DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language
  Models","Large language models (LLMs) have recently showcased remarkable capabilities,
spanning a wide range of tasks and applications, including those in the medical
domain. Models like GPT-4 excel in medical question answering but may face
challenges in the lack of interpretability when handling complex tasks in real
clinical settings. We thus introduce the diagnostic reasoning dataset for
clinical notes (DiReCT), aiming at evaluating the reasoning ability and
interpretability of LLMs compared to human doctors. It contains 511 clinical
notes, each meticulously annotated by physicians, detailing the diagnostic
reasoning process from observations in a clinical note to the final diagnosis.
Additionally, a diagnostic knowledge graph is provided to offer essential
knowledge for reasoning, which may not be covered in the training data of
existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant
gap between their reasoning ability and that of human doctors, highlighting the
critical need for models that can reason effectively in real-world clinical
scenarios.","[{'name': 'Bowen Wang'}, {'name': 'Jiuyang Chang'}, {'name': 'Yiming Qian'}, {'name': 'Guoxin Chen'}, {'name': 'Junhao Chen'}, {'name': 'Zhouqiang Jiang'}, {'name': 'Jiahao Zhang'}, {'name': 'Yuta Nakashima'}, {'name': 'Hajime Nagahara'}]",2024-08-04T05:15:02Z
http://arxiv.org/abs/2408.01928v1,http://arxiv.org/abs/2408.01928v1,"A Semi-supervised Multi-channel Graph Convolutional Network for Query
  Classification in E-commerce","Query intent classification is an essential module for customers to find
desired products on the e-commerce application quickly. Most existing query
intent classification methods rely on the users' click behavior as a supervised
signal to construct training samples. However, these methods based entirely on
posterior labels may lead to serious category imbalance problems because of the
Matthew effect in click samples. Compared with popular categories, it is
difficult for products under long-tail categories to obtain traffic and user
clicks, which makes the models unable to detect users' intent for products
under long-tail categories. This in turn aggravates the problem that long-tail
categories cannot obtain traffic, forming a vicious circle. In addition, due to
the randomness of the user's click, the posterior label is unstable for the
query with similar semantics, which makes the model very sensitive to the
input, leading to an unstable and incomplete recall of categories.
  In this paper, we propose a novel Semi-supervised Multi-channel Graph
Convolutional Network (SMGCN) to address the above problems from the
perspective of label association and semi-supervised learning. SMGCN extends
category information and enhances the posterior label by utilizing the
similarity score between the query and categories. Furthermore, it leverages
the co-occurrence and semantic similarity graph of categories to strengthen the
relations among labels and weaken the influence of posterior label instability.
We conduct extensive offline and online A/B experiments, and the experimental
results show that SMGCN significantly outperforms the strong baselines, which
shows its effectiveness and practicality.","[{'name': 'Chunyuan Yuan'}, {'name': 'Ming Pang'}, {'name': 'Zheng Fang'}, {'name': 'Xue Jiang'}, {'name': 'Changping Peng'}, {'name': 'Zhangang Lin'}]",2024-08-04T04:52:21Z
http://arxiv.org/abs/2408.04651v1,http://arxiv.org/abs/2408.04651v1,"Knowledge AI: Fine-tuning NLP Models for Facilitating Scientific
  Knowledge Extraction and Understanding","This project investigates the efficacy of Large Language Models (LLMs) in
understanding and extracting scientific knowledge across specific domains and
to create a deep learning framework: Knowledge AI. As a part of this framework,
we employ pre-trained models and fine-tune them on datasets in the scientific
domain. The models are adapted for four key Natural Language Processing (NLP)
tasks: summarization, text generation, question answering, and named entity
recognition. Our results indicate that domain-specific fine-tuning
significantly enhances model performance in each of these tasks, thereby
improving their applicability for scientific contexts. This adaptation enables
non-experts to efficiently query and extract information within targeted
scientific fields, demonstrating the potential of fine-tuned LLMs as a tool for
knowledge discovery in the sciences.","[{'name': 'Balaji Muralidharan'}, {'name': 'Hayden Beadles'}, {'name': 'Reza Marzban'}, {'name': 'Kalyan Sashank Mupparaju'}]",2024-08-04T01:32:09Z
http://arxiv.org/abs/2408.01890v1,http://arxiv.org/abs/2408.01890v1,Cross-layer Attention Sharing for Large Language Models,"As large language models (LLMs) evolve, the increase in model depth and
parameter number leads to substantial redundancy. To enhance the efficiency of
the attention mechanism, previous works primarily compress the KV cache or
group attention heads, while largely overlooking redundancy between layers. Our
comprehensive analyses across various LLMs show that highly similar attention
patterns persist within most layers. It's intuitive to save the computation by
sharing attention weights across layers. However, further analysis reveals two
challenges: (1) Directly sharing the weight matrix without carefully
rearranging the attention heads proves to be ineffective; (2) Shallow layers
are vulnerable to small deviations in attention weights. Driven by these
insights, we introduce LiSA, a lightweight substitute for self-attention in
well-trained LLMs. LiSA employs tiny feed-forward networks to align attention
heads between adjacent layers and low-rank matrices to approximate differences
in layer-wise attention weights. Evaluations encompassing 13 typical benchmarks
demonstrate that LiSA maintains high response quality in terms of accuracy and
perplexity while reducing redundant attention calculations within 53-84% of the
total layers. Our implementations of LiSA achieve a 6X compression of Q and K,
with maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for
LLaMA2-7B.","[{'name': 'Yongyu Mu'}, {'name': 'Yuzhang Wu'}, {'name': 'Yuchun Fan'}, {'name': 'Chenglong Wang'}, {'name': 'Hengyu Li'}, {'name': 'Qiaozhi He'}, {'name': 'Murun Yang'}, {'name': 'Tong Xiao'}, {'name': 'Jingbo Zhu'}]",2024-08-04T00:38:34Z
http://arxiv.org/abs/2408.01875v1,http://arxiv.org/abs/2408.01875v1,Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval,"Recent advances in large language models (LLMs) have enabled autonomous
agents with complex reasoning and task-fulfillment capabilities using a wide
range of tools. However, effectively identifying the most relevant tools for a
given task becomes a key bottleneck as the toolset size grows, hindering
reliable tool utilization. To address this, we introduce Re-Invoke, an
unsupervised tool retrieval method designed to scale effectively to large
toolsets without training. Specifically, we first generate a diverse set of
synthetic queries that comprehensively cover different aspects of the query
space associated with each tool document during the tool indexing phase.
Second, we leverage LLM's query understanding capabilities to extract key
tool-related context and underlying intents from user queries during the
inference phase. Finally, we employ a novel multi-view similarity ranking
strategy based on intents to pinpoint the most relevant tools for each query.
Our evaluation demonstrates that Re-Invoke significantly outperforms
state-of-the-art alternatives in both single-tool and multi-tool scenarios, all
within a fully unsupervised setting. Notably, on the ToolE datasets, we achieve
a 20% relative improvement in nDCG@5 for single-tool retrieval and a 39%
improvement for multi-tool retrieval.","[{'name': 'Yanfei Chen'}, {'name': 'Jinsung Yoon'}, {'name': 'Devendra Singh Sachan'}, {'name': 'Qingze Wang'}, {'name': 'Vincent Cohen-Addad'}, {'name': 'Mohammadhossein Bateni'}, {'name': 'Chen-Yu Lee'}, {'name': 'Tomas Pfister'}]",2024-08-03T22:49:27Z
http://arxiv.org/abs/2408.01869v1,http://arxiv.org/abs/2408.01869v1,"MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented
  Generation for Pharmacovigilance","In the era of Large Language Models (LLMs), given their remarkable text
understanding and generation abilities, there is an unprecedented opportunity
to develop new, LLM-based methods for trustworthy medical knowledge synthesis,
extraction and summarization. This paper focuses on the problem of
Pharmacovigilance (PhV), where the significance and challenges lie in
identifying Adverse Drug Events (ADEs) from diverse text sources, such as
medical literature, clinical notes, and drug labels. Unfortunately, this task
is hindered by factors including variations in the terminologies of drugs and
outcomes, and ADE descriptions often being buried in large amounts of narrative
text. We present MALADE, the first effective collaborative multi-agent system
powered by LLM with Retrieval Augmented Generation for ADE extraction from drug
label data. This technique involves augmenting a query to an LLM with relevant
information extracted from text resources, and instructing the LLM to compose a
response consistent with the augmented data. MALADE is a general LLM-agnostic
architecture, and its unique capabilities are: (1) leveraging a variety of
external sources, such as medical literature, drug labels, and FDA tools (e.g.,
OpenFDA drug information API), (2) extracting drug-outcome association in a
structured format along with the strength of the association, and (3) providing
explanations for established associations. Instantiated with GPT-4 Turbo or
GPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area
Under ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our
implementation leverages the Langroid multi-agent LLM framework and can be
found at https://github.com/jihyechoi77/malade.","[{'name': 'Jihye Choi'}, {'name': 'Nils Palumbo'}, {'name': 'Prasad Chalasani'}, {'name': 'Matthew M. Engelhard'}, {'name': 'Somesh Jha'}, {'name': 'Anivarya Kumar'}, {'name': 'David Page'}]",2024-08-03T22:14:13Z
http://arxiv.org/abs/2408.01866v1,http://arxiv.org/abs/2408.01866v1,"Efficient Solutions For An Intriguing Failure of LLMs: Long Context
  Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly","Large Language Models (LLMs) have demonstrated remarkable capabilities in
comprehending and analyzing lengthy sequential inputs, owing to their extensive
context windows that allow processing millions of tokens in a single forward
pass. However, this paper uncovers a surprising limitation: LLMs fall short
when handling long input sequences. We investigate this issue using three
datasets and two tasks (sentiment analysis and news categorization) across
various LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,
and Mistral Instruct models. To address this limitation, we propose and
evaluate ad-hoc solutions that substantially enhance LLMs' performance on long
input sequences by up to 50%, while reducing API cost and latency by up to 93%
and 50%, respectively.","[{'name': 'Peyman Hosseini'}, {'name': 'Ignacio Castro'}, {'name': 'Iacopo Ghinassi'}, {'name': 'Matthew Purver'}]",2024-08-03T21:31:34Z
http://arxiv.org/abs/2408.04650v1,http://arxiv.org/abs/2408.04650v1,"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based
  Evaluation Tools","Objective: This study aims to develop and validate an evaluation framework to
ensure the safety and reliability of mental health chatbots, which are
increasingly popular due to their accessibility, human-like interactions, and
context-aware support. Materials and Methods: We created an evaluation
framework with 100 benchmark questions and ideal responses, and five guideline
questions for chatbot responses. This framework, validated by mental health
experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation
methods explored included large language model (LLM)-based scoring, an agentic
approach using real-time data, and embedding models to compare chatbot
responses against ground truth standards. Results: The results highlight the
importance of guidelines and ground truth for improving LLM evaluation
accuracy. The agentic method, dynamically accessing reliable information,
demonstrated the best alignment with human assessments. Adherence to a
standardized, expert-validated framework significantly enhanced chatbot
response safety and reliability. Discussion: Our findings emphasize the need
for comprehensive, expert-tailored safety evaluation metrics for mental health
chatbots. While LLMs have significant potential, careful implementation is
necessary to mitigate risks. The superior performance of the agentic approach
underscores the importance of real-time data access in enhancing chatbot
reliability. Conclusion: The study validated an evaluation framework for mental
health chatbots, proving its effectiveness in improving safety and reliability.
Future work should extend evaluations to accuracy, bias, empathy, and privacy
to ensure holistic assessment and responsible integration into healthcare.
Standardized evaluations will build trust among users and professionals,
facilitating broader adoption and improved mental health support through
technology.","[{'name': 'Jung In Park'}, {'name': 'Mahyar Abbasian'}, {'name': 'Iman Azimi'}, {'name': 'Dawn Bounds'}, {'name': 'Angela Jun'}, {'name': 'Jaesu Han'}, {'name': 'Robert McCarron'}, {'name': 'Jessica Borelli'}, {'name': 'Jia Li'}, {'name': 'Mona Mahmoudi'}, {'name': 'Carmen Wiedenhoeft'}, {'name': 'Amir Rahmani'}]",2024-08-03T19:57:49Z
http://arxiv.org/abs/2408.01852v2,http://arxiv.org/abs/2408.01852v2,Sólo Escúchame: Spanish Emotional Accompaniment Chatbot,"According to the World Health Organization (WHO), suicide was the fourth
leading cause of death in the world for individuals aged 15 to 29 in 2019.
Given the rapid increase in mental health issues, providing psychological
support is both crucial and urgent. In this paper: (1) we propose S\'olo
Esc\'uchame, the first open-source Spanish emotional assistance chatbot, based
on LLaMA-2-7b-Chat. (2) We introduced the HEAR (Hispanic Emotional
Accompaniment Responses) dataset, compiled from multiple English sources
translated into Spanish, as well as generic data generated using
ChatGPT-3.5-Turbo. Finally, (3) we propose an evaluation metric based on two
semi-automatic assessment methods. Our system outperforms a range of
state-of-the-art models in providing psychological assistance in Spanish. Our
models and datasets are publicly available to facilitate reproducibility.","[{'name': 'Bruno Gil Ramírez'}, {'name': 'Jessica López Espejel'}, {'name': 'María del Carmen Santiago Díaz'}, {'name': 'Gustavo Trinidad Rubín Linares'}]",2024-08-03T19:33:33Z
http://arxiv.org/abs/2408.01838v1,http://arxiv.org/abs/2408.01838v1,"Tracking Emotional Dynamics in Chat Conversations: A Hybrid Approach
  using DistilBERT and Emoji Sentiment Analysis","Computer-mediated communication has become more important than face-to-face
communication in many contexts. Tracking emotional dynamics in chat
conversations can enhance communication, improve services, and support
well-being in various contexts. This paper explores a hybrid approach to
tracking emotional dynamics in chat conversations by combining DistilBERT-based
text emotion detection and emoji sentiment analysis. A Twitter dataset was
analyzed using various machine learning algorithms, including SVM, Random
Forest, and AdaBoost. We contrasted their performance with DistilBERT. Results
reveal DistilBERT's superior performance in emotion recognition. Our approach
accounts for emotive expressions conveyed through emojis to better understand
participants' emotions during chats. We demonstrate how this approach can
effectively capture and analyze emotional shifts in real-time conversations.
Our findings show that integrating text and emoji analysis is an effective way
of tracking chat emotion, with possible applications in customer service, work
chats, and social media interactions.","[{'name': 'Ayan Igali'}, {'name': 'Abdulkhak Abdrakhman'}, {'name': 'Yerdaut Torekhan'}, {'name': 'Pakizar Shamoi'}]",2024-08-03T18:28:31Z
http://arxiv.org/abs/2408.04649v1,http://arxiv.org/abs/2408.04649v1,Chain of Stance: Stance Detection with Large Language Models,"Stance detection is an active task in natural language processing (NLP) that
aims to identify the author's stance towards a particular target within a text.
Given the remarkable language understanding capabilities and encyclopedic prior
knowledge of large language models (LLMs), how to explore the potential of LLMs
in stance detection has received significant attention. Unlike existing
LLM-based approaches that focus solely on fine-tuning with large-scale
datasets, we propose a new prompting method, called \textit{Chain of Stance}
(CoS). In particular, it positions LLMs as expert stance detectors by
decomposing the stance detection process into a series of intermediate,
stance-related assertions that culminate in the final judgment. This approach
leads to significant improvements in classification performance. We conducted
extensive experiments using four SOTA LLMs on the SemEval 2016 dataset,
covering the zero-shot and few-shot learning setups. The results indicate that
the proposed method achieves state-of-the-art results with an F1 score of 79.84
in the few-shot setting.","[{'name': 'Junxia Ma'}, {'name': 'Changjiang Wang'}, {'name': 'Hanwen Xing'}, {'name': 'Dongming Zhao'}, {'name': 'Yazhou Zhang'}]",2024-08-03T16:30:51Z
http://arxiv.org/abs/2408.01803v1,http://arxiv.org/abs/2408.01803v1,STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs,"In this paper, we present STBLLM, the first structural binarization framework
for compressing Large Language Models (LLMs) to less than 1-bit precision. LLMs
have achieved remarkable performance, but their heavy memory requirements have
hindered widespread adoption, particularly on resource-constrained devices.
Binarization, which quantifies weights to a mere 1-bit, achieves a milestone in
increasing computational efficiency. However, we observe that some weights in
binarized LLMs can be randomly flipped without significant performance
degradation, indicating the potential for further compression. To exploit this,
our STBLLM employs an N:M sparsity to perform structural binarization of the
weights. First, we introduce a new Standardized Importance (SI) metric that
considers weight magnitude and input feature norm to better evaluate weight
significance. Then, we propose a layer-wise approach where different layers of
the LLM can be sparsified with varying N:M ratios, balancing compression and
accuracy. Finally, we use residual approximation with double binarization to
preserve information for salient weights. In addition, we utilize a
fine-grained grouping strategy for less important weights that applies
different quantization schemes to sparse, intermediate, and dense regions. We
conduct extensive experiments on various language models, including the
LLaMA-1/2/3, OPT family, and Mistral, to evaluate the effectiveness of STBLLM.
The results demonstrate that our approach performs better than other compressed
binarization LLM methods while significantly reducing memory requirements.","[{'name': 'Peijie Dong'}, {'name': 'Lujun Li'}, {'name': 'Dayou Du'}, {'name': 'Yuhan Chen'}, {'name': 'Zhenheng Tang'}, {'name': 'Qiang Wang'}, {'name': 'Wei Xue'}, {'name': 'Wenhan Luo'}, {'name': 'Qifeng Liu'}, {'name': 'Yike Guo'}, {'name': 'Xiaowen Chu'}]",2024-08-03T15:07:44Z
http://arxiv.org/abs/2408.01779v1,http://arxiv.org/abs/2408.01779v1,"MathLearner: A Large Language Model Agent Framework for Learning to
  Solve Mathematical Problems","With the development of artificial intelligence (AI), large language models
(LLM) are widely used in many fields. However, the reasoning ability of LLM is
still very limited when it comes to mathematical reasoning. Mathematics plays
an important role in all aspects of human society and is a technical guarantee
in the fields of healthcare, transport and aerospace, for this reason, the
development of AI big language models in the field of mathematics has great
potential significance. To improve the mathematical reasoning ability of large
language models, we proposed an agent framework for learning to solve
mathematical problems based on inductive reasoning. By emulating the human
learning process of generalization of learned information and effective
application of previous knowledge in new reasoning tasks, this framework has
great performance in the mathematical reasoning process. It improves global
accuracy over the baseline method (chain-of-thought) by 20.96% and solves
17.54% of the mathematical problems that the baseline cannot solve. Benefiting
from the efficient RETRIEVAL method, our model improves the ability of large
language models to efficiently use external knowledge, i.e., the mathematical
computation of the model can be based on written procedures. In education, our
model can be used as a personalised learning aid, thus reducing the inequality
of educational resources.","[{'name': 'Wenbei Xie'}, {'name': 'Donglin Liu'}, {'name': 'Haoran Yan'}, {'name': 'Wenjie Wu'}, {'name': 'Zongyang Liu'}]",2024-08-03T13:28:19Z
http://arxiv.org/abs/2408.04648v1,http://arxiv.org/abs/2408.04648v1,"PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large
  Language Models","We present PLUGH (https://www.urbandictionary.com/define.php?term=plugh), a
modern benchmark that currently consists of 5 tasks, each with 125 input texts
extracted from 48 different games and representing 61 different
(non-isomorphic) spatial graphs to assess the abilities of Large Language
Models (LLMs) for spatial understanding and reasoning. Our evaluation of
API-based and open-sourced LLMs shows that while some commercial LLMs exhibit
strong reasoning abilities, open-sourced competitors can demonstrate almost the
same level of quality; however, all models still have significant room for
improvement. We identify typical reasons for LLM failures and discuss possible
ways to deal with them. Datasets and evaluation code are released
(https://github.com/altsoph/PLUGH).",[{'name': 'Alexey Tikhonov'}],2024-08-03T13:21:08Z
http://arxiv.org/abs/2408.04647v1,http://arxiv.org/abs/2408.04647v1,Distinguishing Chatbot from Human,"There have been many recent advances in the fields of generative Artificial
Intelligence (AI) and Large Language Models (LLM), with the Generative
Pre-trained Transformer (GPT) model being a leading ""chatbot."" LLM-based
chatbots have become so powerful that it may seem difficult to differentiate
between human-written and machine-generated text. To analyze this problem, we
have developed a new dataset consisting of more than 750,000 human-written
paragraphs, with a corresponding chatbot-generated paragraph for each. Based on
this dataset, we apply Machine Learning (ML) techniques to determine the origin
of text (human or chatbot). Specifically, we consider two methodologies for
tackling this issue: feature analysis and embeddings. Our feature analysis
approach involves extracting a collection of features from the text for
classification. We also explore the use of contextual embeddings and
transformer-based architectures to train classification models. Our proposed
solutions offer high classification accuracy and serve as useful tools for
textual analysis, resulting in a better understanding of chatbot-generated text
in this era of advanced AI technology.","[{'name': 'Gauri Anil Godghase'}, {'name': 'Rishit Agrawal'}, {'name': 'Tanush Obili'}, {'name': 'Mark Stamp'}]",2024-08-03T13:18:04Z
http://arxiv.org/abs/2408.01748v1,http://arxiv.org/abs/2408.01748v1,Discovery of Rare Causal Knowledge from Financial Statement Summaries,"What would happen if temperatures were subdued and result in a cool summer?
One can easily imagine that air conditioner, ice cream or beer sales would be
suppressed as a result of this. Less obvious is that agricultural shipments
might be delayed, or that sound proofing material sales might decrease. The
ability to extract such causal knowledge is important, but it is also important
to distinguish between cause-effect pairs that are known and those that are
likely to be unknown, or rare. Therefore, in this paper, we propose a method
for extracting rare causal knowledge from Japanese financial statement
summaries produced by companies. Our method consists of three steps. First, it
extracts sentences that include causal knowledge from the summaries using a
machine learning method based on an extended language ontology. Second, it
obtains causal knowledge from the extracted sentences using syntactic patterns.
Finally, it extracts the rarest causal knowledge from the knowledge it has
obtained.","[{'name': 'Hiroki Sakaji'}, {'name': 'Jason Bennett'}, {'name': 'Risa Murono'}, {'name': 'Kiyoshi Izumi'}, {'name': 'Hiroyuki Sakai'}]",2024-08-03T11:08:53Z
http://arxiv.org/abs/2408.01745v1,http://arxiv.org/abs/2408.01745v1,"Indexing and Visualization of Climate Change Narratives Using BERT and
  Causal Extraction","In this study, we propose a methodology to extract, index, and visualize
``climate change narratives'' (stories about the connection between causal and
consequential events related to climate change). We use two natural language
processing methods, BERT (Bidirectional Encoder Representations from
Transformers) and causal extraction, to textually analyze newspaper articles on
climate change to extract ``climate change narratives.'' The novelty of the
methodology could extract and quantify the causal relationships assumed by the
newspaper's writers. Looking at the extracted climate change narratives over
time, we find that since 2018, an increasing number of narratives suggest the
impact of the development of climate change policy discussion and the
implementation of climate change-related policies on corporate behaviors,
macroeconomics, and price dynamics. We also observed the recent emergence of
narratives focusing on the linkages between climate change-related policies and
monetary policy. Furthermore, there is a growing awareness of the negative
impacts of natural disasters (e.g., abnormal weather and severe floods) related
to climate change on economic activities, and this issue might be perceived as
a new challenge for companies and governments. The methodology of this study is
expected to be applied to a wide range of fields, as it can analyze causal
relationships among various economic topics, including analysis of inflation
expectation or monetary policy communication strategy.","[{'name': 'Hiroki Sakaji'}, {'name': 'Noriyasu Kaneda'}]",2024-08-03T11:05:41Z
http://arxiv.org/abs/2408.01744v1,http://arxiv.org/abs/2408.01744v1,Summarization of Investment Reports Using Pre-trained Model,"In this paper, we attempt to summarize monthly reports as investment reports.
Fund managers have a wide range of tasks, one of which is the preparation of
investment reports. In addition to preparing monthly reports on fund
management, fund managers prepare management reports that summarize these
monthly reports every six months or once a year. The preparation of fund
reports is a labor-intensive and time-consuming task. Therefore, in this paper,
we tackle investment summarization from monthly reports using transformer-based
models. There are two main types of summarization methods: extractive
summarization and abstractive summarization, and this study constructs both
methods and examines which is more useful in summarizing investment reports.","[{'name': 'Hiroki Sakaji'}, {'name': 'Ryotaro Kobayashi'}, {'name': 'Kiyoshi Izumi'}, {'name': 'Hiroyuki Mitsugi'}, {'name': 'Wataru Kuramoto'}]",2024-08-03T11:04:04Z
http://arxiv.org/abs/2408.01679v1,http://arxiv.org/abs/2408.01679v1,"MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal
  Knowledge Graph","Multi-modal knowledge graphs have emerged as a powerful approach for
information representation, combining data from different modalities such as
text, images, and videos. While several such graphs have been constructed and
have played important roles in applications like visual question answering and
recommendation systems, challenges persist in their development. These include
the scarcity of high-quality Chinese knowledge graphs and limited domain
coverage in existing multi-modal knowledge graphs. This paper introduces
MMPKUBase, a robust and extensive Chinese multi-modal knowledge graph that
covers diverse domains, including birds, mammals, ferns, and more, comprising
over 50,000 entities and over 1 million filtered images. To ensure data
quality, we employ Prototypical Contrastive Learning and the Isolation Forest
algorithm to refine the image data. Additionally, we have developed a
user-friendly platform to facilitate image attribute exploration.","[{'name': 'Xuan Yi'}, {'name': 'Yanzeng Li'}, {'name': 'Lei Zou'}]",2024-08-03T06:35:54Z
http://arxiv.org/abs/2408.01638v1,http://arxiv.org/abs/2408.01638v1,"Transforming Slot Schema Induction with Generative Dialogue State
  Inference","The challenge of defining a slot schema to represent the state of a
task-oriented dialogue system is addressed by Slot Schema Induction (SSI),
which aims to automatically induce slots from unlabeled dialogue data. Whereas
previous approaches induce slots by clustering value spans extracted directly
from the dialogue text, we demonstrate the power of discovering slots using a
generative approach. By training a model to generate slot names and values that
summarize key dialogue information with no prior task knowledge, our SSI method
discovers high-quality candidate information for representing dialogue state.
These discovered slot-value candidates can be easily clustered into unified
slot schemas that align well with human-authored schemas. Experimental
comparisons on the MultiWOZ and SGD datasets demonstrate that Generative
Dialogue State Inference (GenDSI) outperforms the previous state-of-the-art on
multiple aspects of the SSI task.","[{'name': 'James D. Finch'}, {'name': 'Boxin Zhao'}, {'name': 'Jinho D. Choi'}]",2024-08-03T02:41:10Z
http://arxiv.org/abs/2408.01633v1,http://arxiv.org/abs/2408.01633v1,Self-Emotion Blended Dialogue Generation in Social Simulation Agents,"When engaging in conversations, dialogue agents in a virtual simulation
environment may exhibit their own emotional states that are unrelated to the
immediate conversational context, a phenomenon known as self-emotion. This
study explores how such self-emotion affects the agents' behaviors in dialogue
strategies and decision-making within a large language model (LLM)-driven
simulation framework. In a dialogue strategy prediction experiment, we analyze
the dialogue strategy choices employed by agents both with and without
self-emotion, comparing them to those of humans. The results show that
incorporating self-emotion helps agents exhibit more human-like dialogue
strategies. In an independent experiment comparing the performance of models
fine-tuned on GPT-4 generated dialogue datasets, we demonstrate that
self-emotion can lead to better overall naturalness and humanness. Finally, in
a virtual simulation environment where agents have discussions on multiple
topics, we show that self-emotion of agents can significantly influence the
decision-making process of the agents, leading to approximately a 50% change in
decisions.","[{'name': 'Qiang Zhang'}, {'name': 'Jason Naradowsky'}, {'name': 'Yusuke Miyao'}]",2024-08-03T02:11:48Z
http://arxiv.org/abs/2408.01623v1,http://arxiv.org/abs/2408.01623v1,Dialog Flow Induction for Constrainable LLM-Based Chatbots,"LLM-driven dialog systems are used in a diverse set of applications, ranging
from healthcare to customer service. However, given their generalization
capability, it is difficult to ensure that these chatbots stay within the
boundaries of the specialized domains, potentially resulting in inaccurate
information and irrelevant responses. This paper introduces an unsupervised
approach for automatically inducing domain-specific dialog flows that can be
used to constrain LLM-based chatbots. We introduce two variants of dialog flow
based on the availability of in-domain conversation instances. Through human
and automatic evaluation over various dialog domains, we demonstrate that our
high-quality data-guided dialog flows achieve better domain coverage, thereby
overcoming the need for extensive manual crafting of such flows.","[{'name': 'Stuti Agrawal'}, {'name': 'Nishi Uppuluri'}, {'name': 'Pranav Pillai'}, {'name': 'Revanth Gangi Reddy'}, {'name': 'Zoey Li'}, {'name': 'Gokhan Tur'}, {'name': 'Dilek Hakkani-Tur'}, {'name': 'Heng Ji'}]",2024-08-03T01:15:50Z
http://arxiv.org/abs/2408.04646v1,http://arxiv.org/abs/2408.04646v1,Efficacy of Large Language Models in Systematic Reviews,"This study investigates the effectiveness of Large Language Models (LLMs) in
interpreting existing literature through a systematic review of the
relationship between Environmental, Social, and Governance (ESG) factors and
financial performance. The primary objective is to assess how LLMs can
replicate a systematic review on a corpus of ESG-focused papers. We compiled
and hand-coded a database of 88 relevant papers published from March 2020 to
May 2024. Additionally, we used a set of 238 papers from a previous systematic
review of ESG literature from January 2015 to February 2020. We evaluated two
current state-of-the-art LLMs, Meta AI's Llama 3 8B and OpenAI's GPT-4o, on the
accuracy of their interpretations relative to human-made classifications on
both sets of papers. We then compared these results to a ""Custom GPT"" and a
fine-tuned GPT-4o Mini model using the corpus of 238 papers as training data.
The fine-tuned GPT-4o Mini model outperformed the base LLMs by 28.3% on average
in overall accuracy on prompt 1. At the same time, the ""Custom GPT"" showed a
3.0% and 15.7% improvement on average in overall accuracy on prompts 2 and 3,
respectively. Our findings reveal promising results for investors and agencies
to leverage LLMs to summarize complex evidence related to ESG investing,
thereby enabling quicker decision-making and a more efficient market.","[{'name': 'Aaditya Shah'}, {'name': 'Shridhar Mehendale'}, {'name': 'Siddha Kanthi'}]",2024-08-03T00:01:13Z
http://arxiv.org/abs/2408.04645v1,http://arxiv.org/abs/2408.04645v1,"Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors
  for a Robotics Course","This study evaluates the performance of Large Language Models (LLMs) as an
Artificial Intelligence-based tutor for a university course. In particular,
different advanced techniques are utilized, such as prompt engineering,
Retrieval-Augmented-Generation (RAG), and fine-tuning. We assessed the
different models and applied techniques using common similarity metrics like
BLEU-4, ROUGE, and BERTScore, complemented by a small human evaluation of
helpfulness and trustworthiness. Our findings indicate that RAG combined with
prompt engineering significantly enhances model responses and produces better
factual answers. In the context of education, RAG appears as an ideal technique
as it is based on enriching the input of the model with additional information
and material which usually is already present for a university course.
Fine-tuning, on the other hand, can produce quite small, still strong expert
models, but poses the danger of overfitting. Our study further asks how we
measure performance of LLMs and how well current measurements represent
correctness or relevance? We find high correlation on similarity metrics and a
bias of most of these metrics towards shorter responses. Overall, our research
points to both the potential and challenges of integrating LLMs in educational
settings, suggesting a need for balanced training approaches and advanced
evaluation frameworks.","[{'name': 'Sebastian Kahl'}, {'name': 'Felix Löffler'}, {'name': 'Martin Maciol'}, {'name': 'Fabian Ridder'}, {'name': 'Marius Schmitz'}, {'name': 'Jennifer Spanagel'}, {'name': 'Jens Wienkamp'}, {'name': 'Christopher Burgahn'}, {'name': 'Malte Schilling'}]",2024-08-02T19:49:19Z
http://arxiv.org/abs/2408.01527v1,http://arxiv.org/abs/2408.01527v1,"Analyzing LLMs' Capabilities to Establish Implicit User Sentiment of
  Software Desirability","This study explores the use of several LLMs for providing quantitative
zero-shot sentiment analysis of implicit software desirability expressed by
users. The study provides scaled numerical sentiment analysis unlike other
methods that simply classify sentiment as positive, neutral, or negative.
Numerical analysis provides deeper insights into the magnitude of sentiment, to
drive better decisions regarding product desirability.
  Data is collected through the use of the Microsoft Product Desirability
Toolkit (PDT), a well-known qualitative user experience analysis tool. For
initial exploration, the PDT metric was given to users of ZORQ, a gamification
system used in undergraduate computer science education. The PDT data collected
was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and
through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment
(TRBS), and through Vader, a leading sentiment analysis tool, for quantitative
sentiment analysis. Each system was asked to evaluate the data in two ways,
first by looking at the sentiment expressed in the PDT word/explanation pairs;
and by looking at the sentiment expressed by the users in their grouped
selection of five words and explanations, as a whole. Each LLM was also asked
to provide its confidence (low, medium, high) in its sentiment score, along
with an explanation of why it selected the sentiment value.
  All LLMs tested were able to statistically detect user sentiment from the
users' grouped data, whereas TRBS and Vader were not. The confidence and
explanation of confidence provided by the LLMs assisted in understanding the
user sentiment. This study adds to a deeper understanding of evaluating user
experiences, toward the goal of creating a universal tool that quantifies
implicit sentiment expressed.","[{'name': 'Sherri Weitl-Harms'}, {'name': 'John D. Hastings'}, {'name': 'Jonah Lum'}]",2024-08-02T18:40:10Z
http://arxiv.org/abs/2408.01505v1,http://arxiv.org/abs/2408.01505v1,"MoDE: Effective Multi-task Parameter Efficient Fine-Tuning with a
  Mixture of Dyadic Experts","Parameter-efficient fine-tuning techniques like Low-Rank Adaptation (LoRA)
have revolutionized the adaptation of large language models (LLMs) to diverse
tasks. Recent efforts have explored mixtures of LoRA modules for multi-task
settings. However, our analysis reveals redundancy in the down-projection
matrices of these architectures. This observation motivates our proposed
method, Mixture of Dyadic Experts (MoDE), which introduces a novel design for
efficient multi-task adaptation. This is done by sharing the down-projection
matrix across tasks and employing atomic rank-one adapters, coupled with
routers that allow more sophisticated task-level specialization. Our design
allows for more fine-grained mixing, thereby increasing the model's ability to
jointly handle multiple tasks. We evaluate MoDE on the Supernatural
Instructions (SNI) benchmark consisting of a diverse set of 700+ tasks and
demonstrate that it outperforms state-of-the-art multi-task parameter-efficient
fine-tuning (PEFT) methods, without introducing additional parameters. Our
findings contribute to a deeper understanding of parameter efficiency in
multi-task LLM adaptation and provide a practical solution for deploying
high-performing, lightweight models.","[{'name': 'Lin Ning'}, {'name': 'Harsh Lara'}, {'name': 'Meiqi Guo'}, {'name': 'Abhinav Rastogi'}]",2024-08-02T18:05:10Z
http://arxiv.org/abs/2408.01423v1,http://arxiv.org/abs/2408.01423v1,"Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM
  Auto-Prompting","Large Language Models (LLMs) exhibit remarkable proficiency in addressing a
diverse array of tasks within the Natural Language Processing (NLP) domain,
with various prompt design strategies significantly augmenting their
capabilities. However, these prompts, while beneficial, each possess inherent
limitations. The primary prompt design methodologies are twofold: The first,
exemplified by the Chain of Thought (CoT), involves manually crafting prompts
specific to individual datasets, hence termed Expert-Designed Prompts (EDPs).
Once these prompts are established, they are unalterable, and their
effectiveness is capped by the expertise of the human designers. When applied
to LLMs, the static nature of EDPs results in a uniform approach to both simple
and complex problems within the same dataset, leading to the inefficient use of
tokens for straightforward issues. The second method involves prompts
autonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which
provide tailored solutions to specific problems, mitigating the limitations of
EDPs. However, LDPs may encounter a decline in performance when tackling
complex problems due to the potential for error accumulation during the
solution planning process. To address these challenges, we have conceived a
novel Prompt Recursive Search (PRS) framework that leverages the LLM to
generate solutions specific to the problem, thereby conserving tokens. The
framework incorporates an assessment of problem complexity and an adjustable
structure, ensuring a reduction in the likelihood of errors. We have
substantiated the efficacy of PRS framework through extensive experiments using
LLMs with different numbers of parameters across a spectrum of datasets in
various domains. Compared to the CoT method, the PRS method has increased the
accuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22%
improvement.","[{'name': 'Xiangyu Zhao'}, {'name': 'Chengqian Ma'}]",2024-08-02T17:59:42Z
http://arxiv.org/abs/2408.01420v1,http://arxiv.org/abs/2408.01420v1,Mission Impossible: A Statistical Perspective on Jailbreaking LLMs,"Large language models (LLMs) are trained on a deluge of text data with
limited quality control. As a result, LLMs can exhibit unintended or even
harmful behaviours, such as leaking information, fake news or hate speech.
Countermeasures, commonly referred to as preference alignment, include
fine-tuning the pretrained LLMs with carefully crafted text examples of desired
behaviour. Even then, empirical evidence shows preference aligned LLMs can be
enticed to harmful behaviour. This so called jailbreaking of LLMs is typically
achieved by adversarially modifying the input prompt to the LLM. Our paper
provides theoretical insights into the phenomenon of preference alignment and
jailbreaking from a statistical perspective. Under our framework, we first show
that pretrained LLMs will mimic harmful behaviour if present in the training
corpus. Under that same framework, we then introduce a statistical notion of
alignment, and lower-bound the jailbreaking probability, showing that it is
unpreventable under reasonable assumptions. Based on our insights, we propose
an alteration to the currently prevalent alignment strategy RLHF. Specifically,
we introduce a simple modification to the RLHF objective, we call E-RLHF, that
aims to increase the likelihood of safe responses. E-RLHF brings no additional
training cost, and is compatible with other methods. Empirically, we
demonstrate that E-RLHF outperforms RLHF on all alignment problems put forward
by the AdvBench and HarmBench project without sacrificing model performance as
measured by the MT-Bench project.","[{'name': 'Jingtong Su'}, {'name': 'Julia Kempe'}, {'name': 'Karen Ullrich'}]",2024-08-02T17:55:50Z
http://arxiv.org/abs/2408.01419v1,http://arxiv.org/abs/2408.01419v1,DebateQA: Evaluating Question Answering on Debatable Knowledge,"The rise of large language models (LLMs) has enabled us to seek answers to
inherently debatable questions on LLM chatbots, necessitating a reliable way to
evaluate their ability. However, traditional QA benchmarks assume fixed answers
are inadequate for this purpose. To address this, we introduce DebateQA, a
dataset of 2,941 debatable questions, each accompanied by multiple
human-annotated partial answers that capture a variety of perspectives. We
develop two metrics: Perspective Diversity, which evaluates the
comprehensiveness of perspectives, and Dispute Awareness, which assesses if the
LLM acknowledges the question's debatable nature. Experiments demonstrate that
both metrics align with human preferences and are stable across different
underlying models. Using DebateQA with two metrics, we assess 12 popular LLMs
and retrieval-augmented generation methods. Our findings reveal that while LLMs
generally excel at recognizing debatable issues, their ability to provide
comprehensive answers encompassing diverse perspectives varies considerably.","[{'name': 'Rongwu Xu'}, {'name': 'Xuan Qi'}, {'name': 'Zehan Qi'}, {'name': 'Wei Xu'}, {'name': 'Zhijiang Guo'}]",2024-08-02T17:54:34Z
http://arxiv.org/abs/2408.01417v1,http://arxiv.org/abs/2408.01417v1,"Talk Less, Interact Better: Evaluating In-context Conversational
  Adaptation in Multimodal LLMs","Humans spontaneously use increasingly efficient language as interactions
progress, by adapting and forming ad-hoc conventions. This phenomenon has been
studied extensively using reference games, showing properties of human language
that go beyond relaying intents. It remains unexplored whether multimodal large
language models (MLLMs) similarly increase communication efficiency during
interactions, and what mechanisms they may adopt for this purpose. We introduce
ICCA, an automated framework to evaluate such conversational adaptation as an
in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and
observe that while they may understand the increasingly efficient language of
their interlocutor, they do not spontaneously make their own language more
efficient over time. This latter ability can only be elicited in some models
(e.g., GPT-4) with heavy-handed prompting. This shows that this property of
linguistic interaction does not arise from current training regimes, even
though it is a common hallmark of human language. ICCA is available at
https://github.com/lil-lab/ICCA.","[{'name': 'Yilun Hua'}, {'name': 'Yoav Artzi'}]",2024-08-02T17:51:57Z
http://arxiv.org/abs/2408.01402v1,http://arxiv.org/abs/2408.01402v1,"Pre-trained Language Models Improve the Few-shot Prompt Ability of
  Decision Transformer","Decision Transformer (DT) has emerged as a promising class of algorithms in
offline reinforcement learning (RL) tasks, leveraging pre-collected datasets
and Transformer's capability to model long sequences. Recent works have
demonstrated that using parts of trajectories from training tasks as prompts in
DT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.
However, collecting data from specific environments can be both costly and
unsafe in many scenarios, leading to suboptimal performance and limited
few-shot prompt abilities due to the data-hungry nature of Transformer-based
models. Additionally, the limited datasets used in pre-training make it
challenging for Prompt-DT type of methods to distinguish between various RL
tasks through prompts alone. To address these challenges, we introduce the
Language model-initialized Prompt Decision Transformer (LPDT), which leverages
pre-trained language models for meta-RL tasks and fine-tunes the model using
Low-rank Adaptation (LoRA). We further incorporate prompt regularization to
effectively differentiate between tasks based on prompt feature
representations. Our approach integrates pre-trained language model and RL
tasks seamlessly. Extensive empirical studies demonstrate that initializing
with a pre-trained language model significantly enhances the performance of
Prompt-DT on unseen tasks compared to baseline methods.","[{'name': 'Yu Yang'}, {'name': 'Pan Xu'}]",2024-08-02T17:25:34Z
http://arxiv.org/abs/2408.01394v1,http://arxiv.org/abs/2408.01394v1,"Improving Multilingual Neural Machine Translation by Utilizing Semantic
  and Linguistic Features","The many-to-many multilingual neural machine translation can be regarded as
the process of integrating semantic features from the source sentences and
linguistic features from the target sentences. To enhance zero-shot
translation, models need to share knowledge across languages, which can be
achieved through auxiliary tasks for learning a universal representation or
cross-lingual mapping. To this end, we propose to exploit both semantic and
linguistic features between multiple languages to enhance multilingual
translation. On the encoder side, we introduce a disentangling learning task
that aligns encoder representations by disentangling semantic and linguistic
features, thus facilitating knowledge transfer while preserving complete
information. On the decoder side, we leverage a linguistic encoder to integrate
low-level linguistic features to assist in the target language generation.
Experimental results on multilingual datasets demonstrate significant
improvement in zero-shot translation compared to the baseline system, while
maintaining performance in supervised translation. Further analysis validates
the effectiveness of our method in leveraging both semantic and linguistic
features. The code is available at https://github.com/ictnlp/SemLing-MNMT.","[{'name': 'Mengyu Bu'}, {'name': 'Shuhao Gu'}, {'name': 'Yang Feng'}]",2024-08-02T17:10:12Z
http://arxiv.org/abs/2408.01380v1,http://arxiv.org/abs/2408.01380v1,Coalitions of Large Language Models Increase the Robustness of AI Agents,"The emergence of Large Language Models (LLMs) have fundamentally altered the
way we interact with digital systems and have led to the pursuit of LLM powered
AI agents to assist in daily workflows. LLMs, whilst powerful and capable of
demonstrating some emergent properties, are not logical reasoners and often
struggle to perform well at all sub-tasks carried out by an AI agent to plan
and execute a workflow. While existing studies tackle this lack of proficiency
by generalised pretraining at a huge scale or by specialised fine-tuning for
tool use, we assess if a system comprising of a coalition of pretrained LLMs,
each exhibiting specialised performance at individual sub-tasks, can match the
performance of single model agents. The coalition of models approach showcases
its potential for building robustness and reducing the operational costs of
these AI agents by leveraging traits exhibited by specific models. Our findings
demonstrate that fine-tuning can be mitigated by considering a coalition of
pretrained models and believe that this approach can be applied to other
non-agentic systems which utilise LLMs.","[{'name': 'Prattyush Mangal'}, {'name': 'Carol Mak'}, {'name': 'Theo Kanakis'}, {'name': 'Timothy Donovan'}, {'name': 'Dave Braines'}, {'name': 'Edward Pyzer-Knapp'}]",2024-08-02T16:37:44Z
http://arxiv.org/abs/2408.01367v1,http://arxiv.org/abs/2408.01367v1,Transformers are Universal In-context Learners,"Transformers are deep architectures that define ""in-context mappings"" which
enable predicting new tokens based on a given set of tokens (such as a prompt
in NLP applications or a set of patches for vision transformers). This work
studies in particular the ability of these architectures to handle an
arbitrarily large number of context tokens. To mathematically and uniformly
address the expressivity of these architectures, we consider the case that the
mappings are conditioned on a context represented by a probability distribution
of tokens (discrete for a finite number of tokens). The related notion of
smoothness corresponds to continuity in terms of the Wasserstein distance
between these contexts. We demonstrate that deep transformers are universal and
can approximate continuous in-context mappings to arbitrary precision,
uniformly over compact token domains. A key aspect of our results, compared to
existing findings, is that for a fixed precision, a single transformer can
operate on an arbitrary (even infinite) number of tokens. Additionally, it
operates with a fixed embedding dimension of tokens (this dimension does not
increase with precision) and a fixed number of heads (proportional to the
dimension). The use of MLP layers between multi-head attention layers is also
explicitly controlled.","[{'name': 'Takashi Furuya'}, {'name': 'Maarten V. de Hoop'}, {'name': 'Gabriel Peyré'}]",2024-08-02T16:21:48Z
http://arxiv.org/abs/2408.01363v1,http://arxiv.org/abs/2408.01363v1,"Toward Automatic Relevance Judgment using Vision--Language Models for
  Image--Text Retrieval Evaluation","Vision--Language Models (VLMs) have demonstrated success across diverse
applications, yet their potential to assist in relevance judgments remains
uncertain. This paper assesses the relevance estimation capabilities of VLMs,
including CLIP, LLaVA, and GPT-4V, within a large-scale \textit{ad hoc}
retrieval task tailored for multimedia content creation in a zero-shot fashion.
Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,
encompassing open-source and closed-source visual-instruction-tuned Large
Language Models (LLMs), achieve notable Kendall's $\tau \sim 0.4$ when compared
to human relevance judgments, surpassing the CLIPScore metric. (2) While
CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based
retrieval systems. (3) GPT-4V's score distribution aligns more closely with
human judgments than other models, achieving a Cohen's $\kappa$ value of around
0.08, which outperforms CLIPScore at approximately -0.096. These findings
underscore the potential of LLM-powered VLMs in enhancing relevance judgments.","[{'name': 'Jheng-Hong Yang'}, {'name': 'Jimmy Lin'}]",2024-08-02T16:15:25Z
http://arxiv.org/abs/2408.01346v1,http://arxiv.org/abs/2408.01346v1,"Prompt Refinement or Fine-tuning? Best Practices for using LLMs in
  Computational Social Science Tasks","Large Language Models are expressive tools that enable complex tasks of text
understanding within Computational Social Science. Their versatility, while
beneficial, poses a barrier for establishing standardized best practices within
the field. To bring clarity on the values of different strategies, we present
an overview of the performance of modern LLM-based classification methods on a
benchmark of 23 social knowledge tasks. Our results point to three best
practices: select models with larger vocabulary and pre-training corpora; avoid
simple zero-shot in favor of AI-enhanced prompting; fine-tune on task-specific
data, and consider more complex forms instruction-tuning on multiple datasets
only when only training data is more abundant.","[{'name': 'Anders Giovanni Møller'}, {'name': 'Luca Maria Aiello'}]",2024-08-02T15:46:36Z
http://arxiv.org/abs/2408.01337v1,http://arxiv.org/abs/2408.01337v1,"MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language
  Models","Multimodal models that jointly process audio and language hold great promise
in audio understanding and are increasingly being adopted in the music domain.
By allowing users to query via text and obtain information about a given audio
input, these models have the potential to enable a variety of music
understanding tasks via language-based interfaces. However, their evaluation
poses considerable challenges, and it remains unclear how to effectively assess
their ability to correctly interpret music-related inputs with current methods.
Motivated by this, we introduce MuChoMusic, a benchmark for evaluating music
understanding in multimodal language models focused on audio. MuChoMusic
comprises 1,187 multiple-choice questions, all validated by human annotators,
on 644 music tracks sourced from two publicly available music datasets, and
covering a wide variety of genres. Questions in the benchmark are crafted to
assess knowledge and reasoning abilities across several dimensions that cover
fundamental musical concepts and their relation to cultural and functional
contexts. Through the holistic analysis afforded by the benchmark, we evaluate
five open-source models and identify several pitfalls, including an
over-reliance on the language modality, pointing to a need for better
multimodal integration. Data and code are open-sourced.","[{'name': 'Benno Weck'}, {'name': 'Ilaria Manco'}, {'name': 'Emmanouil Benetos'}, {'name': 'Elio Quinton'}, {'name': 'George Fazekas'}, {'name': 'Dmitry Bogdanov'}]",2024-08-02T15:34:05Z
http://arxiv.org/abs/2408.01323v1,http://arxiv.org/abs/2408.01323v1,"FANNO: Augmenting High-Quality Instruction Data with Open-Sourced LLMs
  Only","Instruction fine-tuning stands as a crucial advancement in leveraging large
language models (LLMs) for enhanced task performance. However, the annotation
of instruction datasets has traditionally been expensive and laborious, often
relying on manual annotations or costly API calls of proprietary LLMs. To
address these challenges, we introduce FANNO, a fully autonomous, open-sourced
framework that revolutionizes the annotation process without the need for
pre-existing annotated data. Utilizing a Mistral-7b-instruct model, FANNO
efficiently produces diverse and high-quality datasets through a structured
process involving document pre-screening, instruction generation, and response
generation. Experiments on Open LLM Leaderboard and AlpacaEval benchmark show
that the FANNO can generate high-quality data with diversity and complexity for
free, comparable to human-annotated or cleaned datasets like
Alpaca-GPT4-Cleaned.","[{'name': 'He Zhu'}, {'name': 'Junyou Su'}, {'name': 'Tianle Lun'}, {'name': 'Yicheng Tao'}, {'name': 'Wenjia Zhang'}, {'name': 'Zipei Fan'}, {'name': 'Guanhua Chen'}]",2024-08-02T15:21:20Z
http://arxiv.org/abs/2408.01308v1,http://arxiv.org/abs/2408.01308v1,"Reconsidering Token Embeddings with the Definitions for Pre-trained
  Language Models","Learning token embeddings based on token co-occurrence statistics has proven
effective for both pre-training and fine-tuning in natural language processing.
However, recent studies have pointed out the distribution of learned embeddings
degenerates into anisotropy, and even pre-trained language models (PLMs) suffer
from a loss of semantics-related information in embeddings for low-frequency
tokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large,
and demonstrates its robustness against degeneration. On the basis of this
finding, we propose DefinitionEMB, a method that utilizes definitions to
construct isotropically distributed and semantics-related token embeddings for
PLMs while maintaining original robustness during fine-tuning. Our experiments
demonstrate the effectiveness of leveraging definitions from Wiktionary to
construct such embeddings for RoBERTa-base and BART-large. Furthermore, the
constructed embeddings for low-frequency tokens improve the performance of
these models across various GLUE and four text summarization datasets.","[{'name': 'Ying Zhang'}, {'name': 'Dongyuan Li'}, {'name': 'Manabu Okumura'}]",2024-08-02T15:00:05Z
http://arxiv.org/abs/2408.01287v1,http://arxiv.org/abs/2408.01287v1,"Deep Learning based Visually Rich Document Content Understanding: A
  Survey","Visually Rich Documents (VRDs) are essential in academia, finance, medical
fields, and marketing due to their multimodal information content. Traditional
methods for extracting information from VRDs depend on expert knowledge and
manual labor, making them costly and inefficient. The advent of deep learning
has revolutionized this process, introducing models that leverage multimodal
information vision, text, and layout along with pretraining tasks to develop
comprehensive document representations. These models have achieved
state-of-the-art performance across various downstream tasks, significantly
enhancing the efficiency and accuracy of information extraction from VRDs. In
response to the growing demands and rapid developments in Visually Rich
Document Understanding (VRDU), this paper provides a comprehensive review of
deep learning-based VRDU frameworks. We systematically survey and analyze
existing methods and benchmark datasets, categorizing them based on adopted
strategies and downstream tasks. Furthermore, we compare different techniques
used in VRDU models, focusing on feature representation and fusion, model
architecture, and pretraining methods, while highlighting their strengths,
limitations, and appropriate scenarios. Finally, we identify emerging trends
and challenges in VRDU, offering insights into future research directions and
practical applications. This survey aims to provide a thorough understanding of
VRDU advancements, benefiting both academic and industrial sectors.","[{'name': 'Yihao Ding'}, {'name': 'Jean Lee'}, {'name': 'Soyeon Caren Han'}]",2024-08-02T14:19:34Z
http://arxiv.org/abs/2408.01214v1,http://arxiv.org/abs/2408.01214v1,High-Throughput Phenotyping of Clinical Text Using Large Language Models,"High-throughput phenotyping automates the mapping of patient signs to
standardized ontology concepts and is essential for precision medicine. This
study evaluates the automation of phenotyping of clinical summaries from the
Online Mendelian Inheritance in Man (OMIM) database using large language
models. Due to their rich phenotype data, these summaries can be surrogates for
physician notes. We conduct a performance comparison of GPT-4 and
GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in
identifying, categorizing, and normalizing signs, achieving concordance with
manual annotators comparable to inter-rater agreement. Despite some limitations
in sign normalization, the extensive pre-training of GPT-4 results in high
performance and generalizability across several phenotyping tasks while
obviating the need for manually annotated training data. Large language models
are expected to be the dominant method for automating high-throughput
phenotyping of clinical text.","[{'name': 'Daniel B. Hier'}, {'name': 'S. Ilyas Munzir'}, {'name': 'Anne Stahlfeld'}, {'name': 'Tayo Obafemi-Ajayi'}, {'name': 'Michael D. Carrithers'}]",2024-08-02T12:00:00Z
http://arxiv.org/abs/2408.01168v1,http://arxiv.org/abs/2408.01168v1,"Misinforming LLMs: vulnerabilities, challenges and opportunities","Large Language Models (LLMs) have made significant advances in natural
language processing, but their underlying mechanisms are often misunderstood.
Despite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely
on statistical patterns in word embeddings rather than true cognitive
processes. This leads to vulnerabilities such as ""hallucination"" and
misinformation. The paper argues that current LLM architectures are inherently
untrustworthy due to their reliance on correlations of sequential patterns of
word embedding vectors. However, ongoing research into combining generative
transformer-based models with fact bases and logic programming languages may
lead to the development of trustworthy LLMs capable of generating statements
based on given truth and explaining their self-reasoning process.","[{'name': 'Bo Zhou'}, {'name': 'Daniel Geißler'}, {'name': 'Paul Lukowicz'}]",2024-08-02T10:35:49Z
http://arxiv.org/abs/2408.01154v1,http://arxiv.org/abs/2408.01154v1,DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs,"Entity Alignment (EA) aims to match equivalent entities in different
Knowledge Graphs (KGs), which is essential for knowledge fusion and
integration. Recently, embedding-based EA has attracted significant attention
and many approaches have been proposed. Early approaches primarily focus on
learning entity embeddings from the structural features of KGs, defined by
relation triples. Later methods incorporated entities' names and attributes as
auxiliary information to enhance embeddings for EA. However, these approaches
often used different techniques to encode structural and attribute information,
limiting their interaction and mutual enhancement. In this work, we propose a
dense entity retrieval framework for EA, leveraging language models to
uniformly encode various features of entities and facilitate nearest entity
search across KGs. Alignment candidates are first generated through entity
retrieval, which are subsequently reranked to determine the final alignments.
We conduct comprehensive experiments on both cross-lingual and monolingual EA
datasets, demonstrating that our approach achieves state-of-the-art performance
compared to existing EA methods.","[{'name': 'Zhichun Wang'}, {'name': 'Xuan Chen'}]",2024-08-02T10:12:42Z
http://arxiv.org/abs/2408.01122v1,http://arxiv.org/abs/2408.01122v1,CFBench: A Comprehensive Constraints-Following Benchmark for LLMs,"The adeptness of Large Language Models (LLMs) in comprehending and following
natural language instructions is critical for their deployment in sophisticated
real-world applications. Existing evaluations mainly focus on fragmented
constraints or narrow scenarios, but they overlook the comprehensiveness and
authenticity of constraints from the user's perspective. To bridge this gap, we
propose CFBench, a large-scale Comprehensive Constraints Following Benchmark
for LLMs, featuring 1,000 curated samples that cover more than 200 real-life
scenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from
real-world instructions and constructs an innovative systematic framework for
constraint types, which includes 10 primary categories and over 25
subcategories, and ensures each constraint is seamlessly integrated within the
instructions. To make certain that the evaluation of LLM outputs aligns with
user perceptions, we propose an advanced methodology that integrates
multi-dimensional assessment criteria with requirement prioritization, covering
various perspectives of constraints, instructions, and requirement fulfillment.
Evaluating current leading LLMs on CFBench reveals substantial room for
improvement in constraints following, and we further investigate influencing
factors and enhancement strategies. The data and code are publicly available at
https://github.com/PKU-Baichuan-MLSystemLab/CFBench","[{'name': 'Tao Zhang'}, {'name': 'Yanjun Shen'}, {'name': 'Wenjing Luo'}, {'name': 'Yan Zhang'}, {'name': 'Hao Liang'}, {'name': 'Tao Zhang'}, {'name': 'Fan Yang'}, {'name': 'Mingan Lin'}, {'name': 'Yujing Qiao'}, {'name': 'Weipeng Chen'}, {'name': 'Bin Cui'}, {'name': 'Wentao Zhang'}, {'name': 'Zenan Zhou'}]",2024-08-02T09:03:48Z
http://arxiv.org/abs/2408.01119v1,http://arxiv.org/abs/2408.01119v1,"Task Prompt Vectors: Effective Initialization through Multi-Task
  Soft-Prompt Transfer","Prompt tuning is a modular and efficient solution for training large language
models (LLMs). One of its main advantages is task modularity, making it
suitable for multi-task problems. However, current soft-prompt-based methods
often sacrifice multi-task modularity, requiring the training process to be
fully or partially repeated for each newly added task. While recent work on
task vectors applied arithmetic operations on full model weights to achieve the
desired multi-task performance, a similar approach for soft-prompts is still
missing. To this end, we introduce Task Prompt Vectors, created by element-wise
difference between weights of tuned soft-prompts and their random
initialization. Experimental results on 12 NLU datasets show that task prompt
vectors can be used in low-resource settings to effectively initialize prompt
tuning on similar tasks. In addition, we show that task prompt vectors are
independent of the random initialization of prompt tuning. This allows prompt
arithmetics with the pre-trained vectors from different tasks. In this way, by
arithmetic addition of task prompt vectors from multiple tasks, we are able to
outperform a state-of-the-art baseline in some cases.","[{'name': 'Robert Belanec'}, {'name': 'Simon Ostermann'}, {'name': 'Ivan Srba'}, {'name': 'Maria Bielikova'}]",2024-08-02T09:00:03Z
http://arxiv.org/abs/2408.01118v1,http://arxiv.org/abs/2408.01118v1,"IAI Group at CheckThat! 2024: Transformer Models and Data Augmentation
  for Checkworthy Claim Detection","This paper describes IAI group's participation for automated check-worthiness
estimation for claims, within the framework of the 2024 CheckThat! Lab ""Task 1:
Check-Worthiness Estimation"". The task involves the automated detection of
check-worthy claims in English, Dutch, and Arabic political debates and Twitter
data. We utilized various pre-trained generative decoder and encoder
transformer models, employing methods such as few-shot chain-of-thought
reasoning, fine-tuning, data augmentation, and transfer learning from one
language to another. Despite variable success in terms of performance, our
models achieved notable placements on the organizer's leaderboard: ninth-best
in English, third-best in Dutch, and the top placement in Arabic, utilizing
multilingual datasets for enhancing the generalizability of check-worthiness
detection. Despite a significant drop in performance on the unlabeled test
dataset compared to the development test dataset, our findings contribute to
the ongoing efforts in claim detection research, highlighting the challenges
and potential of language-specific adaptations in claim verification systems.","[{'name': 'Peter Røysland Aarnes'}, {'name': 'Vinay Setty'}, {'name': 'Petra Galuščáková'}]",2024-08-02T08:59:09Z
http://arxiv.org/abs/2408.01107v2,http://arxiv.org/abs/2408.01107v2,BioRAG: A RAG-LLM Framework for Biological Question Reasoning,"The question-answering system for Life science research, which is
characterized by the rapid pace of discovery, evolving insights, and complex
interactions among knowledge entities, presents unique challenges in
maintaining a comprehensive knowledge warehouse and accurate information
retrieval. To address these issues, we introduce BioRAG, a novel
Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)
framework. Our approach starts with parsing, indexing, and segmenting an
extensive collection of 22 million scientific papers as the basic knowledge,
followed by training a specialized embedding model tailored to this domain.
Additionally, we enhance the vector retrieval process by incorporating a
domain-specific knowledge hierarchy, which aids in modeling the intricate
interrelationships among each query and context. For queries requiring the most
current information, BioRAG deconstructs the question and employs an iterative
retrieval process incorporated with the search engine for step-by-step
reasoning. Rigorous experiments have demonstrated that our model outperforms
fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks
across multiple life science question-answering tasks.","[{'name': 'Chengrui Wang'}, {'name': 'Qingqing Long'}, {'name': 'Meng Xiao'}, {'name': 'Xunxin Cai'}, {'name': 'Chengjun Wu'}, {'name': 'Zhen Meng'}, {'name': 'Xuezhi Wang'}, {'name': 'Yuanchun Zhou'}]",2024-08-02T08:37:03Z
http://arxiv.org/abs/2408.01090v1,http://arxiv.org/abs/2408.01090v1,General-purpose Dataflow Model with Neuromorphic Primitives,"Neuromorphic computing exhibits great potential to provide high-performance
benefits in various applications beyond neural networks. However, a
general-purpose program execution model that aligns with the features of
neuromorphic computing is required to bridge the gap between program
versatility and neuromorphic hardware efficiency. The dataflow model offers a
potential solution, but it faces high graph complexity and incompatibility with
neuromorphic hardware when dealing with control flow programs, which decreases
the programmability and performance. Here, we present a dataflow model tailored
for neuromorphic hardware, called neuromorphic dataflow, which provides a
compact, concise, and neuromorphic-compatible program representation for
control logic. The neuromorphic dataflow introduces ""when"" and ""where""
primitives, which restructure the view of control. The neuromorphic dataflow
embeds these primitives in the dataflow schema with the plasticity inherited
from the spiking algorithms. Our method enables the deployment of
general-purpose programs on neuromorphic hardware with both programmability and
plasticity, while fully utilizing the hardware's potential.","[{'name': 'Weihao Zhang'}, {'name': 'Yu Du'}, {'name': 'Hongyi Li'}, {'name': 'Songchen Ma'}, {'name': 'Rong Zhao'}]",2024-08-02T08:09:13Z
http://arxiv.org/abs/2408.01088v2,http://arxiv.org/abs/2408.01088v2,"Bridging Information Gaps in Dialogues With Grounded Exchanges Using
  Knowledge Graphs","Knowledge models are fundamental to dialogue systems for enabling
conversational interactions, which require handling domain-specific knowledge.
Ensuring effective communication in information-providing conversations entails
aligning user understanding with the knowledge available to the system.
However, dialogue systems often face challenges arising from semantic
inconsistencies in how information is expressed in natural language compared to
how it is represented within the system's internal knowledge. To address this
problem, we study the potential of large language models for conversational
grounding, a mechanism to bridge information gaps by establishing shared
knowledge between dialogue participants. Our approach involves annotating human
conversations across five knowledge domains to create a new dialogue corpus
called BridgeKG. Through a series of experiments on this dataset, we
empirically evaluate the capabilities of large language models in classifying
grounding acts and identifying grounded information items within a knowledge
graph structure. Our findings offer insights into how these models use
in-context learning for conversational grounding tasks and common prediction
errors, which we illustrate with examples from challenging dialogues. We
discuss how the models handle knowledge graphs as a semantic layer between
unstructured dialogue utterances and structured information items.","[{'name': 'Phillip Schneider'}, {'name': 'Nektarios Machner'}, {'name': 'Kristiina Jokinen'}, {'name': 'Florian Matthes'}]",2024-08-02T08:07:15Z
http://arxiv.org/abs/2408.01084v1,http://arxiv.org/abs/2408.01084v1,"Adaptive Contrastive Decoding in Retrieval-Augmented Generation for
  Handling Noisy Contexts","When using large language models (LLMs) in knowledge-intensive tasks, such as
open-domain question answering, external context can bridge a gap between
external knowledge and LLM's parametric knowledge. Recent research has been
developed to amplify contextual knowledge over the parametric knowledge of LLM
with contrastive decoding approaches. While these approaches could yield
truthful responses when relevant context is provided, they are prone to
vulnerabilities when faced with noisy contexts. We extend the scope of previous
studies to encompass noisy contexts and propose adaptive contrastive decoding
(ACD) to leverage contextual influence effectively. ACD demonstrates
improvements in open-domain question answering tasks compared to baselines,
especially in robustness by remaining undistracted by noisy contexts in
retrieval-augmented generation.","[{'name': 'Youna Kim'}, {'name': 'Hyuhng Joon Kim'}, {'name': 'Cheonbok Park'}, {'name': 'Choonghyun Park'}, {'name': 'Hyunsoo Cho'}, {'name': 'Junyeob Kim'}, {'name': 'Kang Min Yoo'}, {'name': 'Sang-goo Lee'}, {'name': 'Taeuk Kim'}]",2024-08-02T08:03:38Z
http://arxiv.org/abs/2408.01063v1,http://arxiv.org/abs/2408.01063v1,"Leveraging Large Language Models for Mobile App Review Feature
  Extraction","Mobile app review analysis presents unique challenges due to the low quality,
subjective bias, and noisy content of user-generated documents. Extracting
features from these reviews is essential for tasks such as feature
prioritization and sentiment analysis, but it remains a challenging task.
Meanwhile, encoder-only models based on the Transformer architecture have shown
promising results for classification and information extraction tasks for
multiple software engineering processes. This study explores the hypothesis
that encoder-only large language models can enhance feature extraction from
mobile app reviews. By leveraging crowdsourced annotations from an industrial
context, we redefine feature extraction as a supervised token classification
task. Our approach includes extending the pre-training of these models with a
large corpus of user reviews to improve contextual understanding and employing
instance selection techniques to optimize model fine-tuning. Empirical
evaluations demonstrate that this method improves the precision and recall of
extracted features and enhances performance efficiency. Key contributions
include a novel approach to feature extraction, annotated datasets, extended
pre-trained models, and an instance selection mechanism for cost-effective
fine-tuning. This research provides practical methods and empirical evidence in
applying large language models to natural language processing tasks within
mobile app reviews, offering improved performance in feature extraction.","[{'name': 'Quim Motger'}, {'name': 'Alessio Miaschi'}, {'name': ""Felice Dell'Orletta""}, {'name': 'Xavier Franch'}, {'name': 'Jordi Marco'}]",2024-08-02T07:31:57Z
http://arxiv.org/abs/2408.01050v1,http://arxiv.org/abs/2408.01050v1,"The Impact of Hyperparameters on Large Language Model Inference
  Performance: An Evaluation of vLLM and HuggingFace Pipelines","The recent surge of open-source large language models (LLMs) enables
developers to create AI-based solutions while maintaining control over aspects
such as privacy and compliance, thereby providing governance and ownership of
the model deployment process. To utilize these LLMs, inference engines are
needed. These engines load the model's weights onto available resources, such
as GPUs, and process queries to generate responses. The speed of inference, or
performance, of the LLM, is critical for real-time applications, as it computes
millions or billions of floating point operations per inference. Recently,
advanced inference engines such as vLLM have emerged, incorporating novel
mechanisms such as efficient memory management to achieve state-of-the-art
performance. In this paper, we analyze the performance, particularly the
throughput (tokens generated per unit of time), of 20 LLMs using two inference
libraries: vLLM and HuggingFace's pipelines. We investigate how various
hyperparameters, which developers must configure, influence inference
performance. Our results reveal that throughput landscapes are irregular, with
distinct peaks, highlighting the importance of hyperparameter optimization to
achieve maximum performance. We also show that applying hyperparameter
optimization when upgrading or downgrading the GPU model used for inference can
improve throughput from HuggingFace pipelines by an average of 9.16% and 13.7%,
respectively.",[{'name': 'Matias Martinez'}],2024-08-02T06:56:59Z
http://arxiv.org/abs/2408.01046v1,http://arxiv.org/abs/2408.01046v1,QUDSELECT: Selective Decoding for Questions Under Discussion Parsing,"Question Under Discussion (QUD) is a discourse framework that uses implicit
questions to reveal discourse relationships between sentences. In QUD parsing,
each sentence is viewed as an answer to a question triggered by an anchor
sentence in prior context. The resulting QUD structure is required to conform
to several theoretical criteria like answer compatibility (how well the
question is answered), making QUD parsing a challenging task. Previous works
construct QUD parsers in a pipelined manner (i.e. detect the trigger sentence
in context and then generate the question). However, these parsers lack a
holistic view of the task and can hardly satisfy all the criteria. In this
work, we introduce QUDSELECT, a joint-training framework that selectively
decodes the QUD dependency structures considering the QUD criteria. Using
instruction-tuning, we train models to simultaneously predict the anchor
sentence and generate the associated question. To explicitly incorporate the
criteria, we adopt a selective decoding strategy of sampling multiple QUD
candidates during inference, followed by selecting the best one with criteria
scorers. Our method outperforms the state-of-the-art baseline models by 9% in
human evaluation and 4% in automatic evaluation, demonstrating the
effectiveness of our framework.","[{'name': 'Ashima Suvarna'}, {'name': 'Xiao Liu'}, {'name': 'Tanmay Parekh'}, {'name': 'Kai-Wei Chang'}, {'name': 'Nanyun Peng'}]",2024-08-02T06:46:08Z
http://arxiv.org/abs/2408.01038v2,http://arxiv.org/abs/2408.01038v2,"UNER: A Unified Prediction Head for Named Entity Recognition in
  Visually-rich Documents","The recognition of named entities in visually-rich documents (VrD-NER) plays
a critical role in various real-world scenarios and applications. However, the
research in VrD-NER faces three major challenges: complex document layouts,
incorrect reading orders, and unsuitable task formulations. To address these
challenges, we propose a query-aware entity extraction head, namely UNER, to
collaborate with existing multi-modal document transformers to develop more
robust VrD-NER models. The UNER head considers the VrD-NER task as a
combination of sequence labeling and reading order prediction, effectively
addressing the issues of discontinuous entities in documents. Experimental
evaluations on diverse datasets demonstrate the effectiveness of UNER in
improving entity extraction performance. Moreover, the UNER head enables a
supervised pre-training stage on various VrD-NER datasets to enhance the
document transformer backbones and exhibits substantial knowledge transfer from
the pre-training stage to the fine-tuning stage. By incorporating universal
layout understanding, a pre-trained UNER-based model demonstrates significant
advantages in few-shot and cross-linguistic scenarios and exhibits zero-shot
entity extraction abilities.","[{'name': 'Yi Tu'}, {'name': 'Chong Zhang'}, {'name': 'Ya Guo'}, {'name': 'Huan Chen'}, {'name': 'Jinyang Tang'}, {'name': 'Huijia Zhu'}, {'name': 'Qi Zhang'}]",2024-08-02T06:21:36Z
http://arxiv.org/abs/2408.01005v1,http://arxiv.org/abs/2408.01005v1,"Enhancing Financial Market Predictions: Causality-Driven Feature
  Selection","This paper introduces the FinSen dataset that revolutionizes financial market
analysis by integrating economic and financial news articles from 197 countries
with stock market data. The dataset's extensive coverage spans 15 years from
2007 to 2023 with temporal information, offering a rich, global perspective
with 160,000 records on financial market news. Our study leverages causally
validated sentiment scores and LSTM models to enhance market forecast accuracy
and reliability. Utilizing the FinSen dataset, we introduce an innovative Focal
Calibration Loss, reducing Expected Calibration Error (ECE) to 3.34 percent
with the DAN 3 model. This not only improves prediction accuracy but also
aligns probabilistic forecasts closely with real outcomes, crucial for the
financial sector where predicted probability is paramount. Our approach
demonstrates the effectiveness of combining sentiment analysis with precise
calibration techniques for trustworthy financial forecasting where the cost of
misinterpretation can be high. Finsen Data can be found at [this github
URL](https://github.com/EagleAdelaide/FinSen_Dataset.git).","[{'name': 'Wenhao Liang'}, {'name': 'Zhengyang Li'}, {'name': 'Weitong Chen'}]",2024-08-02T04:40:15Z
http://arxiv.org/abs/2408.00994v1,http://arxiv.org/abs/2408.00994v1,"ArchCode: Incorporating Software Requirements in Code Generation with
  Large Language Models","This paper aims to extend the code generation capability of large language
models (LLMs) to automatically manage comprehensive software requirements from
given textual descriptions. Such requirements include both functional (i.e.
achieving expected behavior for inputs) and non-functional (e.g., time/space
performance, robustness, maintainability) requirements. However, textual
descriptions can either express requirements verbosely or may even omit some of
them. We introduce ARCHCODE, a novel framework that leverages in-context
learning to organize requirements observed in descriptions and to extrapolate
unexpressed requirements from them. ARCHCODE generates requirements from given
descriptions, conditioning them to produce code snippets and test cases. Each
test case is tailored to one of the requirements, allowing for the ranking of
code snippets based on the compliance of their execution results with the
requirements. Public benchmarks show that ARCHCODE enhances to satisfy
functional requirements, significantly improving Pass@k scores. Furthermore, we
introduce HumanEval-NFR, the first evaluation of LLMs' non-functional
requirements in code generation, demonstrating ARCHCODE's superiority over
baseline methods. The implementation of ARCHCODE and the HumanEval-NFR
benchmark are both publicly accessible.","[{'name': 'Hojae Han'}, {'name': 'Jaejin Kim'}, {'name': 'Jaeseok Yoo'}, {'name': 'Youngwon Lee'}, {'name': 'Seung-won Hwang'}]",2024-08-02T03:54:36Z
http://arxiv.org/abs/2408.00966v1,http://arxiv.org/abs/2408.00966v1,"Automatic Extraction of Relationships among Motivations, Emotions and
  Actions from Natural Language Texts","We propose a new graph-based framework to reveal relationships among
motivations, emotions and actions explicitly given natural language texts. A
directed acyclic graph is designed to describe human's nature. Nurture beliefs
are incorporated to connect outside events and the human's nature graph. No
annotation resources are required due to the power of large language models.
Amazon Fine Foods Reviews dataset is used as corpus and food-related
motivations are focused. Totally 92,990 relationship graphs are generated, of
which 63% make logical sense. We make further analysis to investigate error
types for optimization direction in future research.",[{'name': 'Fei Yang'}],2024-08-02T01:22:46Z
http://arxiv.org/abs/2408.00960v1,http://arxiv.org/abs/2408.00960v1,"PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized
  Language Prompting","Understanding the nuances of a user's extensive interaction history is key to
building accurate and personalized natural language systems that can adapt to
evolving user preferences. To address this, we introduce PERSOMA, Personalized
Soft Prompt Adapter architecture. Unlike previous personalized prompting
methods for large language models, PERSOMA offers a novel approach to
efficiently capture user history. It achieves this by resampling and
compressing interactions as free form text into expressive soft prompt
embeddings, building upon recent research utilizing embedding representations
as input for LLMs. We rigorously validate our approach by evaluating various
adapter architectures, first-stage sampling strategies, parameter-efficient
tuning techniques like LoRA, and other personalization methods. Our results
demonstrate PERSOMA's superior ability to handle large and complex user
histories compared to existing embedding-based and text-prompt-based
techniques.","[{'name': 'Liam Hebert'}, {'name': 'Krishna Sayana'}, {'name': 'Ambarish Jash'}, {'name': 'Alexandros Karatzoglou'}, {'name': 'Sukhdeep Sodhi'}, {'name': 'Sumanth Doddapaneni'}, {'name': 'Yanli Cai'}, {'name': 'Dima Kuzmin'}]",2024-08-02T00:24:22Z
http://arxiv.org/abs/2408.00948v1,http://arxiv.org/abs/2408.00948v1,"Leveraging Large Language Models (LLMs) for Traffic Management at Urban
  Intersections: The Case of Mixed Traffic Scenarios","Urban traffic management faces significant challenges due to the dynamic
environments, and traditional algorithms fail to quickly adapt to this
environment in real-time and predict possible conflicts. This study explores
the ability of a Large Language Model (LLM), specifically, GPT-4o-mini to
improve traffic management at urban intersections. We recruited GPT-4o-mini to
analyze, predict position, detect and resolve the conflicts at an intersection
in real-time for various basic scenarios. The key findings of this study to
investigate whether LLMs can logically reason and understand the scenarios to
enhance the traffic efficiency and safety by providing real-time analysis. The
study highlights the potential of LLMs in urban traffic management creating
more intelligent and more adaptive systems. Results showed the GPT-4o-mini was
effectively able to detect and resolve conflicts in heavy traffic, congestion,
and mixed-speed conditions. The complex scenario of multiple intersections with
obstacles and pedestrians saw successful conflict management as well. Results
show that the integration of LLMs promises to improve the effectiveness of
traffic control for safer and more efficient urban intersection management.","[{'name': 'Sari Masri'}, {'name': 'Huthaifa I. Ashqar'}, {'name': 'Mohammed Elhenawy'}]",2024-08-01T23:06:06Z
http://arxiv.org/abs/2408.00932v1,http://arxiv.org/abs/2408.00932v1,"Towards Zero-Shot Annotation of the Built Environment with
  Vision-Language Models (Vision Paper)","Equitable urban transportation applications require high-fidelity digital
representations of the built environment: not just streets and sidewalks, but
bike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions,
traffic signals, signage, street markings, potholes, and more. Direct
inspections and manual annotations are prohibitively expensive at scale.
Conventional machine learning methods require substantial annotated training
data for adequate performance. In this paper, we consider vision language
models as a mechanism for annotating diverse urban features from satellite
images, reducing the dependence on human annotation to produce large training
sets. While these models have achieved impressive results in describing common
objects in images captured from a human perspective, their training sets are
less likely to include strong signals for esoteric features in the built
environment, and their performance in these settings is therefore unclear. We
demonstrate proof-of-concept combining a state-of-the-art vision language model
and variants of a prompting strategy that asks the model to consider segmented
elements independently of the original image. Experiments on two urban features
-- stop lines and raised tables -- show that while direct zero-shot prompting
correctly annotates nearly zero images, the pre-segmentation strategies can
annotate images with near 40% intersection-over-union accuracy. We describe how
these results inform a new research agenda in automatic annotation of the built
environment to improve equity, accessibility, and safety at broad scale and in
diverse environments.","[{'name': 'Bin Han'}, {'name': 'Yiwei Yang'}, {'name': 'Anat Caspi'}, {'name': 'Bill Howe'}]",2024-08-01T21:50:23Z
http://arxiv.org/abs/2408.00921v1,http://arxiv.org/abs/2408.00921v1,"Automatic Pull Request Description Generation Using LLMs: A T5 Model
  Approach","Developers create pull request (PR) descriptions to provide an overview of
their changes and explain the motivations behind them. These descriptions help
reviewers and fellow developers quickly understand the updates. Despite their
importance, some developers omit these descriptions. To tackle this problem, we
propose an automated method for generating PR descriptions based on commit
messages and source code comments. This method frames the task as a text
summarization problem, for which we utilized the T5 text-to-text transfer
model. We fine-tuned a pre-trained T5 model using a dataset containing 33,466
PRs. The model's effectiveness was assessed using ROUGE metrics, which are
recognized for their strong alignment with human evaluations. Our findings
reveal that the T5 model significantly outperforms LexRank, which served as our
baseline for comparison.","[{'name': 'Md Nazmus Sakib'}, {'name': 'Md Athikul Islam'}, {'name': 'Md Mashrur Arifin'}]",2024-08-01T21:22:16Z
http://arxiv.org/abs/2408.04643v1,http://arxiv.org/abs/2408.04643v1,"Risks, Causes, and Mitigations of Widespread Deployments of Large
  Language Models (LLMs): A Survey","Recent advancements in Large Language Models (LLMs), such as ChatGPT and
LLaMA, have significantly transformed Natural Language Processing (NLP) with
their outstanding abilities in text generation, summarization, and
classification. Nevertheless, their widespread adoption introduces numerous
challenges, including issues related to academic integrity, copyright,
environmental impacts, and ethical considerations such as data bias, fairness,
and privacy. The rapid evolution of LLMs also raises concerns regarding the
reliability and generalizability of their evaluations. This paper offers a
comprehensive survey of the literature on these subjects, systematically
gathered and synthesized from Google Scholar. Our study provides an in-depth
analysis of the risks associated with specific LLMs, identifying sub-risks,
their causes, and potential solutions. Furthermore, we explore the broader
challenges related to LLMs, detailing their causes and proposing mitigation
strategies. Through this literature analysis, our survey aims to deepen the
understanding of the implications and complexities surrounding these powerful
models.","[{'name': 'Md Nazmus Sakib'}, {'name': 'Md Athikul Islam'}, {'name': 'Royal Pathak'}, {'name': 'Md Mashrur Arifin'}]",2024-08-01T21:21:18Z
http://arxiv.org/abs/2408.00914v1,http://arxiv.org/abs/2408.00914v1,"Granting GPT-4 License and Opportunity: Enhancing Accuracy and
  Confidence Estimation for Few-Shot Event Detection","Large Language Models (LLMs) such as GPT-4 have shown enough promise in the
few-shot learning context to suggest use in the generation of ""silver"" data and
refinement of new ontologies through iterative application and review. Such
workflows become more effective with reliable confidence estimation.
Unfortunately, confidence estimation is a documented weakness of models such as
GPT-4, and established methods to compensate require significant additional
complexity and computation. The present effort explores methods for effective
confidence estimation with GPT-4 with few-shot learning for event detection in
the BETTER ontology as a vehicle. The key innovation is expanding the prompt
and task presented to GPT-4 to provide License to speculate when unsure and
Opportunity to quantify and explain its uncertainty (L&O). This approach
improves accuracy and provides usable confidence measures (0.759 AUC) with no
additional machinery.","[{'name': 'Steven Fincke'}, {'name': 'Adrien Bibal'}, {'name': 'Elizabeth Boschee'}]",2024-08-01T21:08:07Z
http://arxiv.org/abs/2408.00884v1,http://arxiv.org/abs/2408.00884v1,Hybrid Querying Over Relational Databases and Large Language Models,"Database queries traditionally operate under the closed-world assumption,
providing no answers to questions that require information beyond the data
stored in the database. Hybrid querying using SQL offers an alternative by
integrating relational databases with large language models (LLMs) to answer
beyond-database questions. In this paper, we present the first cross-domain
benchmark, SWAN, containing 120 beyond-database questions over four real-world
databases. To leverage state-of-the-art language models in addressing these
complex questions in SWAN, we present, HQDL, a preliminary solution for hybrid
querying, and also discuss potential future directions. Our evaluation
demonstrates that HQDL using GPT-4 Turbo with few-shot prompts, achieves 40.0\%
in execution accuracy and 48.2\% in data factuality. These results highlights
both the potential and challenges for hybrid querying. We believe that our work
will inspire further research in creating more efficient and accurate data
systems that seamlessly integrate relational databases and large language
models to address beyond-database questions.","[{'name': 'Fuheng Zhao'}, {'name': 'Divyakant Agrawal'}, {'name': 'Amr El Abbadi'}]",2024-08-01T19:29:18Z
http://arxiv.org/abs/2408.00863v1,http://arxiv.org/abs/2408.00863v1,"UniMoT: Unified Molecule-Text Language Model with Discrete Token
  Representation","The remarkable success of Large Language Models (LLMs) across diverse tasks
has driven the research community to extend their capabilities to molecular
applications. However, most molecular LLMs employ adapter-based architectures
that do not treat molecule and text modalities equally and lack a supervision
signal for the molecule modality. To address these issues, we introduce UniMoT,
a Unified Molecule-Text LLM adopting a tokenizer-based architecture that
expands the vocabulary of LLM with molecule tokens. Specifically, we introduce
a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge
the modality gap between molecule and text. This tokenizer transforms molecules
into sequences of molecule tokens with causal dependency, encapsulating
high-level molecular and textual information. Equipped with this tokenizer,
UniMoT can unify molecule and text modalities under a shared token
representation and an autoregressive training paradigm, enabling it to
interpret molecules as a foreign language and generate them as text. Following
a four-stage training scheme, UniMoT emerges as a multi-modal generalist
capable of performing both molecule-to-text and text-to-molecule tasks.
Extensive experiments demonstrate that UniMoT achieves state-of-the-art
performance across a wide range of molecule comprehension and generation tasks.","[{'name': 'Juzheng Zhang'}, {'name': 'Yatao Bian'}, {'name': 'Yongqiang Chen'}, {'name': 'Quanming Yao'}]",2024-08-01T18:31:31Z
http://arxiv.org/abs/2408.00765v1,http://arxiv.org/abs/2408.00765v1,"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models
  for Integrated Capabilities","MM-Vet, with open-ended vision-language questions targeting at evaluating
integrated capabilities, has become one of the most popular benchmarks for
large multimodal model evaluation. MM-Vet assesses six core vision-language
(VL) capabilities: recognition, knowledge, spatial awareness, language
generation, OCR, and math. However, its question format is restricted to single
image-text pairs, lacking the interleaved image and text sequences prevalent in
real-world scenarios. To address this limitation, we introduce MM-Vet v2, which
includes a new VL capability called ""image-text sequence understanding"",
evaluating models' ability to process VL sequences. Furthermore, we maintain
the high quality of evaluation samples while further expanding the evaluation
set size. Using MM-Vet v2 to benchmark large multimodal models, we found that
Claude 3.5 Sonnet is the best model with a score of 71.8, slightly
outperforming GPT-4o which scored 71.0. Among open-weight models,
InternVL2-Llama3-76B leads with a score of 68.4.","[{'name': 'Weihao Yu'}, {'name': 'Zhengyuan Yang'}, {'name': 'Linfeng Ren'}, {'name': 'Linjie Li'}, {'name': 'Jianfeng Wang'}, {'name': 'Kevin Lin'}, {'name': 'Chung-Ching Lin'}, {'name': 'Zicheng Liu'}, {'name': 'Lijuan Wang'}, {'name': 'Xinchao Wang'}]",2024-08-01T17:59:54Z
http://arxiv.org/abs/2408.00764v1,http://arxiv.org/abs/2408.00764v1,"AgentGen: Enhancing Planning Abilities for Large Language Model based
  Agent via Environment and Task Generation","Large Language Model (LLM) based agents have garnered significant attention
and are becoming increasingly popular. Furthermore, planning ability is a
crucial component of an LLM-based agent, involving interaction with the
environment and executing actions to complete a planning task, which generally
entails achieving a desired goal from an initial state. This paper investigates
enhancing the planning abilities of LLMs through instruction tuning, referred
to as agent training. Recent studies have demonstrated that utilizing
expert-level trajectory for instruction-tuning LLMs effectively enhances their
planning capabilities. However, existing work primarily focuses on synthesizing
trajectories from manually designed planning tasks and environments. The
labor-intensive nature of creating these environments and tasks impedes the
generation of sufficiently varied and extensive trajectories. To address this
limitation, this paper explores the automated synthesis of diverse environments
and a gradual range of planning tasks, from easy to difficult. We introduce a
framework, AgentGen, that leverages LLMs first to generate environments and
subsequently generate planning tasks conditioned on these environments.
Specifically, to improve environmental diversity, we propose using an
inspiration corpus composed of various domain-specific text segments as the
context for synthesizing environments. Moreover, to increase the difficulty
diversity of generated planning tasks, we propose a bidirectional evolution
method, Bi-Evol, that evolves planning tasks from easier and harder directions
to synthesize a task set with a smoother difficulty curve. The evaluation
results derived from AgentBoard show that AgentGen greatly improves LLMs'
planning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses
GPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms
GPT-4.","[{'name': 'Mengkang Hu'}, {'name': 'Pu Zhao'}, {'name': 'Can Xu'}, {'name': 'Qingfeng Sun'}, {'name': 'Jianguang Lou'}, {'name': 'Qingwei Lin'}, {'name': 'Ping Luo'}, {'name': 'Saravan Rajmohan'}, {'name': 'Dongmei Zhang'}]",2024-08-01T17:59:46Z
http://arxiv.org/abs/2408.00761v2,http://arxiv.org/abs/2408.00761v2,Tamper-Resistant Safeguards for Open-Weight LLMs,"Rapid advances in the capabilities of large language models (LLMs) have
raised widespread concerns regarding their potential for malicious use.
Open-weight LLMs present unique challenges, as existing safeguards lack
robustness to tampering attacks that modify model weights. For example, recent
works have demonstrated that refusal and unlearning safeguards can be trivially
removed with a few steps of fine-tuning. These vulnerabilities necessitate new
approaches for enabling the safe release of open-weight LLMs. We develop a
method, called TAR, for building tamper-resistant safeguards into open-weight
LLMs such that adversaries cannot remove the safeguards even after thousands of
steps of fine-tuning. In extensive evaluations and red teaming analyses, we
find that our method greatly improves tamper-resistance while preserving benign
capabilities. Our results demonstrate that tamper-resistance is a tractable
problem, opening up a promising new avenue to improve the safety and security
of open-weight LLMs.","[{'name': 'Rishub Tamirisa'}, {'name': 'Bhrugu Bharathi'}, {'name': 'Long Phan'}, {'name': 'Andy Zhou'}, {'name': 'Alice Gatti'}, {'name': 'Tarun Suresh'}, {'name': 'Maxwell Lin'}, {'name': 'Justin Wang'}, {'name': 'Rowan Wang'}, {'name': 'Ron Arel'}, {'name': 'Andy Zou'}, {'name': 'Dawn Song'}, {'name': 'Bo Li'}, {'name': 'Dan Hendrycks'}, {'name': 'Mantas Mazeika'}]",2024-08-01T17:59:12Z
http://arxiv.org/abs/2408.00728v1,http://arxiv.org/abs/2408.00728v1,CERT-ED: Certifiably Robust Text Classification for Edit Distance,"With the growing integration of AI in daily life, ensuring the robustness of
systems to inference-time attacks is crucial. Among the approaches for
certifying robustness to such adversarial examples, randomized smoothing has
emerged as highly promising due to its nature as a wrapper around arbitrary
black-box models. Previous work on randomized smoothing in natural language
processing has primarily focused on specific subsets of edit distance
operations, such as synonym substitution or word insertion, without exploring
the certification of all edit operations. In this paper, we adapt Randomized
Deletion (Huang et al., 2023) and propose, CERTified Edit Distance defense
(CERT-ED) for natural language classification. Through comprehensive
experiments, we demonstrate that CERT-ED outperforms the existing Hamming
distance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of
both accuracy and the cardinality of the certificate. By covering various
threat models, including 5 direct and 5 transfer attacks, our method improves
empirical robustness in 38 out of 50 settings.","[{'name': 'Zhuoqun Huang'}, {'name': 'Neil G Marchant'}, {'name': 'Olga Ohrimenko'}, {'name': 'Benjamin I. P. Rubinstein'}]",2024-08-01T17:20:24Z
http://arxiv.org/abs/2408.00727v1,http://arxiv.org/abs/2408.00727v1,"Improving Retrieval-Augmented Generation in Medicine with Iterative
  Follow-up Questions","The emergent abilities of large language models (LLMs) have demonstrated
great potential in solving medical questions. They can possess considerable
medical knowledge, but may still hallucinate and are inflexible in the
knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed
to enhance the medical question-answering capabilities of LLMs with external
knowledge bases, it may still fail in complex cases where multiple rounds of
information-seeking are required. To address such an issue, we propose
iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up
queries based on previous information-seeking attempts. In each iteration of
i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and
they will be further used to guide the query generation in the next iteration.
Our experiments show the improved performance of various LLMs brought by
i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes
in the United States Medical Licensing Examination (USMLE), as well as various
knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.
Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and
fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\% on the MedQA
dataset. In addition, we characterize the scaling properties of i-MedRAG with
different iterations of follow-up queries and different numbers of queries per
iteration. Our case studies show that i-MedRAG can flexibly ask follow-up
queries to form reasoning chains, providing an in-depth analysis of medical
questions. To the best of our knowledge, this is the first-of-its-kind study on
incorporating follow-up queries into medical RAG.","[{'name': 'Guangzhi Xiong'}, {'name': 'Qiao Jin'}, {'name': 'Xiao Wang'}, {'name': 'Minjia Zhang'}, {'name': 'Zhiyong Lu'}, {'name': 'Aidong Zhang'}]",2024-08-01T17:18:17Z
http://arxiv.org/abs/2408.00690v2,http://arxiv.org/abs/2408.00690v2,"Improving Text Embeddings for Smaller Language Models Using Contrastive
  Fine-tuning","While Large Language Models show remarkable performance in natural language
understanding, their resource-intensive nature makes them less accessible. In
contrast, smaller language models such as MiniCPM offer more sustainable
scalability, but often underperform without specialized optimization. In this
paper, we explore the enhancement of smaller language models through the
improvement of their text embeddings. We select three language models, MiniCPM,
Phi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Our
results demonstrate that this fine-tuning method enhances the quality of text
embeddings for all three models across various benchmarks, with MiniCPM showing
the most significant improvements of an average 56.33% performance gain. The
contrastive fine-tuning code is publicly available at
https://github.com/trapoom555/Language-Model-STS-CFT.","[{'name': 'Trapoom Ukarapol'}, {'name': 'Zhicheng Lee'}, {'name': 'Amy Xin'}]",2024-08-01T16:31:35Z
http://arxiv.org/abs/2408.00684v1,http://arxiv.org/abs/2408.00684v1,"Assessing the Variety of a Concept Space Using an Unbiased Estimate of
  Rao's Quadratic Index","Past research relates design creativity to 'divergent thinking,' i.e., how
well the concept space is explored during the early phase of design.
Researchers have argued that generating several concepts would increase the
chances of producing better design solutions. 'Variety' is one of the
parameters by which one can quantify the breadth of a concept space explored by
the designers. It is useful to assess variety at the conceptual design stage
because, at this stage, designers have the freedom to explore different
solution principles so as to satisfy a design problem with substantially novel
concepts. This article elaborates on and critically examines the existing
variety metrics from the engineering design literature, discussing their
limitations. A new distance-based variety metric is proposed, along with a
prescriptive framework to support the assessment process. This framework uses
the SAPPhIRE model of causality as a knowledge representation scheme to measure
the real-valued distance between two design concepts. The proposed framework is
implemented in a software tool called 'VariAnT.' Furthermore, the tool's
application is demonstrated through an illustrative example.","[{'name': 'Anubhab Majumder'}, {'name': 'Ujjwal Pal'}, {'name': 'Amaresh Chakrabarti'}]",2024-08-01T16:25:54Z
http://arxiv.org/abs/2408.00675v1,http://arxiv.org/abs/2408.00675v1,Leveraging Entailment Judgements in Cross-Lingual Summarisation,"Synthetically created Cross-Lingual Summarisation (CLS) datasets are prone to
include document-summary pairs where the reference summary is unfaithful to the
corresponding document as it contains content not supported by the document
(i.e., hallucinated content). This low data quality misleads model learning and
obscures evaluation results. Automatic ways to assess hallucinations and
improve training have been proposed for monolingual summarisation,
predominantly in English. For CLS, we propose to use off-the-shelf
cross-lingual Natural Language Inference (X-NLI) to evaluate faithfulness of
reference and model generated summaries. Then, we study training approaches
that are aware of faithfulness issues in the training data and propose an
approach that uses unlikelihood loss to teach a model about unfaithful summary
sequences. Our results show that it is possible to train CLS models that yield
more faithful summaries while maintaining comparable or better informativess.","[{'name': 'Huajian Zhang'}, {'name': 'Laura Perez-Beltrachini'}]",2024-08-01T16:18:09Z
http://arxiv.org/abs/2408.00662v1,http://arxiv.org/abs/2408.00662v1,Aligning Multiple Knowledge Graphs in a Single Pass,"Entity alignment (EA) is to identify equivalent entities across different
knowledge graphs (KGs), which can help fuse these KGs into a more comprehensive
one. Previous EA methods mainly focus on aligning a pair of KGs, and to the
best of our knowledge, no existing EA method considers aligning multiple (more
than two) KGs. To fill this research gap, in this work, we study a novel
problem of aligning multiple KGs and propose an effective framework named
MultiEA to solve the problem. First, we embed the entities of all the candidate
KGs into a common feature space by a shared KG encoder. Then, we explore three
alignment strategies to minimize the distances among pre-aligned entities. In
particular, we propose an innovative inference enhancement technique to improve
the alignment performance by incorporating high-order similarities. Finally, to
verify the effectiveness of MultiEA, we construct two new real-world benchmark
datasets and conduct extensive experiments on them. The results show that our
MultiEA can effectively and efficiently align multiple KGs in a single pass.","[{'name': 'Yaming Yang'}, {'name': 'Zhe Wang'}, {'name': 'Ziyu Guan'}, {'name': 'Wei Zhao'}, {'name': 'Weigang Lu'}, {'name': 'Xinyan Huang'}]",2024-08-01T15:58:05Z
http://arxiv.org/abs/2408.00655v5,http://arxiv.org/abs/2408.00655v5,"SentenceVAE: Enable Next-sentence Prediction for Large Language Models
  with Faster Speed, Higher Accuracy and Longer Context","Current large language models (LLMs) primarily utilize next-token prediction
method for inference, which significantly impedes their processing speed. In
this paper, we introduce a novel inference methodology termed next-sentence
prediction, aiming at enhancing the inference efficiency of LLMs. We present
Sentence Variational Autoencoder (SentenceVAE), which includes a Sentence
Encoder to compress multiple tokens in a sentence into a single token, and a
Sentence Decoder to reconstruct it. By integrating SentenceVAE into the input
and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a
sentence-by-sentence inference method. In addition, the SentenceVAE module of
SLLMs can maintain the integrity of the original semantic content by segmenting
the context into sentences, thereby improving accuracy while boosting inference
speed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over
equivalent context length, significantly reducing memory demands for
self-attention computation and facilitating the handling of longer context.
Extensive experiments on Wanjuan dataset have revealed that the proposed method
can accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%
of its original metric, and decrease memory overhead by 86~91% for the
equivalent context length, compared to previous token-by-token methods.","[{'name': 'Hongjun An'}, {'name': 'Yifan Chen'}, {'name': 'Zhe Sun'}, {'name': 'Xuelong Li'}]",2024-08-01T15:45:19Z
http://arxiv.org/abs/2408.00624v1,http://arxiv.org/abs/2408.00624v1,"SynesLM: A Unified Approach for Audio-visual Speech Recognition and
  Translation via Language Model and Synthetic Data","In this work, we present SynesLM, an unified model which can perform three
multimodal language understanding tasks: audio-visual automatic speech
recognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).
Unlike previous research that focused on lip motion as visual cues for speech
signals, our work explores more general visual information within entire
frames, such as objects and actions. Additionally, we use synthetic image data
to enhance the correlation between image and speech data. We benchmark SynesLM
against the How2 dataset, demonstrating performance on par with
state-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our
multitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA
performance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the
VisSpeech Dataset. Furthermore, our results in VST and VMT outperform the
previous results, improving the BLEU score to 43.5 from 37.2 for VST, and to
54.8 from 54.4 for VMT.","[{'name': 'Yichen Lu'}, {'name': 'Jiaqi Song'}, {'name': 'Xuankai Chang'}, {'name': 'Hengwei Bian'}, {'name': 'Soumi Maiti'}, {'name': 'Shinji Watanabe'}]",2024-08-01T15:09:32Z
http://arxiv.org/abs/2408.00620v1,http://arxiv.org/abs/2408.00620v1,Are Bigger Encoders Always Better in Vision Large Models?,"In recent years, multimodal large language models (MLLMs) have shown strong
potential in real-world applications. They are developing rapidly due to their
remarkable ability to comprehend multimodal information and their inherent
powerful cognitive and reasoning capabilities. Among MLLMs, vision language
models (VLM) stand out for their ability to understand vision information.
However, the scaling trend of VLMs under the current mainstream paradigm has
not been extensively studied. Whether we can achieve better performance by
training even larger models is still unclear. To address this issue, we
conducted experiments on the pretraining stage of MLLMs. We conduct our
experiment using different encoder sizes and large language model (LLM) sizes.
Our findings indicate that merely increasing the size of encoders does not
necessarily enhance the performance of VLMs. Moreover, we analyzed the effects
of LLM backbone parameter size and data quality on the pretraining outcomes.
Additionally, we explored the differences in scaling laws between LLMs and
VLMs.","[{'name': 'Bozhou Li'}, {'name': 'Hao Liang'}, {'name': 'Zimo Meng'}, {'name': 'Wentao Zhang'}]",2024-08-01T15:05:42Z
http://arxiv.org/abs/2408.00612v1,http://arxiv.org/abs/2408.00612v1,Downstream bias mitigation is all you need,"The advent of transformer-based architectures and large language models
(LLMs) have significantly advanced the performance of natural language
processing (NLP) models. Since these LLMs are trained on huge corpuses of data
from the web and other sources, there has been a major concern about harmful
prejudices that may potentially be transferred from the data. In many
applications, these pre-trained LLMs are fine-tuned on task specific datasets,
which can further contribute to biases. This paper studies the extent of biases
absorbed by LLMs during pre-training as well as task-specific behaviour after
fine-tuning. We found that controlled interventions on pre-trained LLMs, prior
to fine-tuning, have minimal effect on lowering biases in classifiers. However,
the biases present in domain-specific datasets play a much bigger role, and
hence mitigating them at this stage has a bigger impact. While pre-training
does matter, but after the model has been pre-trained, even slight changes to
co-occurrence rates in the fine-tuning dataset has a significant effect on the
bias of the model.","[{'name': 'Arkadeep Baksi'}, {'name': 'Rahul Singh'}, {'name': 'Tarun Joshi'}]",2024-08-01T14:52:04Z
http://arxiv.org/abs/2408.00584v1,http://arxiv.org/abs/2408.00584v1,"Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian
  Rebuses","Rebuses are puzzles requiring constrained multi-step reasoning to identify a
hidden phrase from a set of images and letters. In this work, we introduce a
large collection of verbalized rebuses for the Italian language and use it to
assess the rebus-solving capabilities of state-of-the-art large language
models. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly
on this task, ad-hoc fine-tuning seems to improve models' performance. However,
we find that performance gains from training are largely motivated by
memorization. Our results suggest that rebus solving remains a challenging test
bed to evaluate large language models' linguistic proficiency and sequential
instruction-following skills.","[{'name': 'Gabriele Sarti'}, {'name': 'Tommaso Caselli'}, {'name': 'Malvina Nissim'}, {'name': 'Arianna Bisazza'}]",2024-08-01T14:14:15Z
http://arxiv.org/abs/2408.00555v1,http://arxiv.org/abs/2408.00555v1,"Alleviating Hallucination in Large Vision-Language Models with Active
  Retrieval Augmentation","Despite the remarkable ability of large vision-language models (LVLMs) in
image comprehension, these models frequently generate plausible yet factually
incorrect responses, a phenomenon known as hallucination.Recently, in large
language models (LLMs), augmenting LLMs by retrieving information from external
knowledge resources has been proven as a promising solution to mitigate
hallucinations.However, the retrieval augmentation in LVLM significantly lags
behind the widespread applications of LVLM. Moreover, when transferred to
augmenting LVLMs, sometimes the hallucination degree of the model is even
exacerbated.Motivated by the research gap and counter-intuitive phenomenon, we
introduce a novel framework, the Active Retrieval-Augmented large
vision-language model (ARA), specifically designed to address hallucinations by
incorporating three critical dimensions: (i) dissecting the retrieval targets
based on the inherent hierarchical structures of images. (ii) pinpointing the
most effective retrieval methods and filtering out the reliable retrieval
results. (iii) timing the retrieval process to coincide with episodes of low
certainty, while circumventing unnecessary retrieval during periods of high
certainty. To assess the capability of our proposed ARA model in reducing
hallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and
mPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by
utilizing fitting retrieval mechanisms and timing the retrieval judiciously, we
can effectively mitigate the hallucination problem. We hope that this study can
provide deeper insights into how to adapt the retrieval augmentation to LVLMs
for reducing hallucinations with more effective retrieval and minimal retrieval
occurrences.","[{'name': 'Xiaoye Qu'}, {'name': 'Qiyuan Chen'}, {'name': 'Wei Wei'}, {'name': 'Jishuo Sun'}, {'name': 'Jianfeng Dong'}]",2024-08-01T13:38:58Z
http://arxiv.org/abs/2408.00550v1,http://arxiv.org/abs/2408.00550v1,Mitigating Multilingual Hallucination in Large Vision-Language Models,"While Large Vision-Language Models (LVLMs) have exhibited remarkable
capabilities across a wide range of tasks, they suffer from hallucination
problems, where models generate plausible yet incorrect answers given the input
image-query pair. This hallucination phenomenon is even more severe when
querying the image in non-English languages, while existing methods for
mitigating hallucinations in LVLMs only consider the English scenarios. In this
paper, we make the first attempt to mitigate this important multilingual
hallucination in LVLMs. With thorough experiment analysis, we found that
multilingual hallucination in LVLMs is a systemic problem that could arise from
deficiencies in multilingual capabilities or inadequate multimodal abilities.
To this end, we propose a two-stage Multilingual Hallucination Removal (MHR)
framework for LVLMs, aiming to improve resistance to hallucination for both
high-resource and low-resource languages. Instead of relying on the intricate
manual annotations of multilingual resources, we fully leverage the inherent
capabilities of the LVLM and propose a novel cross-lingual alignment method,
which generates multiple responses for each image-query input and then
identifies the hallucination-aware pairs for each language. These data pairs
are finally used for direct preference optimization to prompt the LVLMs to
favor non-hallucinating responses. Experimental results show that our MHR
achieves a substantial reduction in hallucination generation for LVLMs.
Notably, on our extended multilingual POPE benchmark, our framework delivers an
average increase of 19.0% in accuracy across 13 different languages. Our code
and model weights are available at https://github.com/ssmisya/MHR","[{'name': 'Xiaoye Qu'}, {'name': 'Mingyang Song'}, {'name': 'Wei Wei'}, {'name': 'Jianfeng Dong'}, {'name': 'Yu Cheng'}]",2024-08-01T13:34:35Z
http://arxiv.org/abs/2408.00539v1,http://arxiv.org/abs/2408.00539v1,Intermittent Semi-working Mask: A New Masking Paradigm for LLMs,"Multi-turn dialogues are a key interaction method between humans and Large
Language Models (LLMs), as conversations extend over multiple rounds, keeping
LLMs' high generation quality and low latency is a challenge. Mainstream LLMs
can be grouped into two categories based on masking strategy: causal LLM and
prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform
causal ones in scenarios that heavily depend on historical context such as
multi-turn dialogues or in-context learning, thanks to their bidirectional
attention on prefix sequences. However, prefix LLMs have an inherent
inefficient training problem in multi-turn dialogue datasets. In addition, the
attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV
Cache) across dialogue rounds to reduce generation latency. In this paper, we
propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to
address these problems. Specifically, we apply alternate bidirectional and
unidirectional attention on queries and answers in the dialogue history. In
this way, ISM is able to maintain the high quality of prefix LLM and low
generation latency of causal LLM, simultaneously. Extensive experiments
illustrate that our ISM achieves significant performance.","[{'name': 'Mingcong Lu'}, {'name': 'Jiangcai Zhu'}, {'name': 'Wang Hao'}, {'name': 'Zheng Li'}, {'name': 'Shusheng Zhang'}, {'name': 'Kailai Shao'}, {'name': 'Chao Chen'}, {'name': 'Nan Li'}, {'name': 'Feng Wang'}, {'name': 'Xin Lu'}]",2024-08-01T13:22:01Z
http://arxiv.org/abs/2408.00534v1,http://arxiv.org/abs/2408.00534v1,"The Monetisation of Toxicity: Analysing YouTube Content Creators and
  Controversy-Driven Engagement","YouTube is a major social media platform that plays a significant role in
digital culture, with content creators at its core. These creators often engage
in controversial behaviour to drive engagement, which can foster toxicity. This
paper presents a quantitative analysis of controversial content on YouTube,
focusing on the relationship between controversy, toxicity, and monetisation.
We introduce a curated dataset comprising 20 controversial YouTube channels
extracted from Reddit discussions, including 16,349 videos and more than 105
million comments. We identify and categorise monetisation cues from video
descriptions into various models, including affiliate marketing and direct
selling, using lists of URLs and keywords. Additionally, we train a machine
learning model to measure the toxicity of comments in these videos. Our
findings reveal that while toxic comments correlate with higher engagement,
they negatively impact monetisation, indicating that controversy-driven
interaction does not necessarily lead to financial gain. We also observed
significant variation in monetisation strategies, with some creators showing
extensive monetisation despite high toxicity levels. Our study introduces a
curated dataset, lists of URLs and keywords to categorise monetisation, a
machine learning model to measure toxicity, and is a significant step towards
understanding the complex relationship between controversy, engagement, and
monetisation on YouTube. The lists used for detecting and categorising
monetisation cues are available on https://github.com/thalesbertaglia/toxmon.","[{'name': 'Thales Bertaglia'}, {'name': 'Catalina Goanta'}, {'name': 'Adriana Iamnitchi'}]",2024-08-01T13:10:35Z
http://arxiv.org/abs/2408.00491v1,http://arxiv.org/abs/2408.00491v1,GalleryGPT: Analyzing Paintings with Large Multimodal Models,"Artwork analysis is important and fundamental skill for art appreciation,
which could enrich personal aesthetic sensibility and facilitate the critical
thinking ability. Understanding artworks is challenging due to its subjective
nature, diverse interpretations, and complex visual elements, requiring
expertise in art history, cultural background, and aesthetic theory. However,
limited by the data collection and model ability, previous works for
automatically analyzing artworks mainly focus on classification, retrieval, and
other simple tasks, which is far from the goal of AI. To facilitate the
research progress, in this paper, we step further to compose comprehensive
analysis inspired by the remarkable perception and generation ability of large
multimodal models. Specifically, we first propose a task of composing paragraph
analysis for artworks, i.e., painting in this paper, only focusing on visual
characteristics to formulate more comprehensive understanding of artworks. To
support the research on formal analysis, we collect a large dataset
PaintingForm, with about 19k painting images and 50k analysis paragraphs. We
further introduce a superior large multimodal model for painting analysis
composing, dubbed GalleryGPT, which is slightly modified and fine-tuned based
on LLaVA architecture leveraging our collected data. We conduct formal analysis
generation and zero-shot experiments across several datasets to assess the
capacity of our model. The results show remarkable performance improvements
comparing with powerful baseline LMMs, demonstrating its superb ability of art
analysis and generalization. \textcolor{blue}{The codes and model are available
at: https://github.com/steven640pixel/GalleryGPT.","[{'name': 'Yi Bin'}, {'name': 'Wenhao Shi'}, {'name': 'Yujuan Ding'}, {'name': 'Zhiqiang Hu'}, {'name': 'Zheng Wang'}, {'name': 'Yang Yang'}, {'name': 'See-Kiong Ng'}, {'name': 'Heng Tao Shen'}]",2024-08-01T11:52:56Z
http://arxiv.org/abs/2408.00397v1,http://arxiv.org/abs/2408.00397v1,"In-Context Example Selection via Similarity Search Improves Low-Resource
  Machine Translation","The ability of generative large language models (LLMs) to perform in-context
learning has given rise to a large body of research into how best to prompt
models for various natural language processing tasks. In this paper, we focus
on machine translation (MT), a task that has been shown to benefit from
in-context translation examples. However no systematic studies have been
published on how best to select examples, and mixed results have been reported
on the usefulness of similarity-based selection over random selection. We
provide a study covering multiple LLMs and multiple in-context example
retrieval strategies, comparing multilingual sentence embeddings. We cover
several language directions, representing different levels of language
resourcedness (English into French, German, Swahili and Wolof). Contrarily to
previously published results, we find that sentence embedding similarity can
improve MT, especially for low-resource language directions, and discuss the
balance between selection pool diversity and quality. We also highlight
potential problems with the evaluation of LLM-based MT and suggest a more
appropriate evaluation protocol, adapting the COMET metric to the evaluation of
LLMs. Code and outputs are freely available at
https://github.com/ArmelRandy/ICL-MT.","[{'name': 'Armel Zebaze'}, {'name': 'Benoît Sagot'}, {'name': 'Rachel Bawden'}]",2024-08-01T09:07:32Z
http://arxiv.org/abs/2408.00357v1,http://arxiv.org/abs/2408.00357v1,"DeliLaw: A Chinese Legal Counselling System Based on a Large Language
  Model","Traditional legal retrieval systems designed to retrieve legal documents,
statutes, precedents, and other legal information are unable to give
satisfactory answers due to lack of semantic understanding of specific
questions. Large Language Models (LLMs) have achieved excellent results in a
variety of natural language processing tasks, which inspired us that we train a
LLM in the legal domain to help legal retrieval. However, in the Chinese legal
domain, due to the complexity of legal questions and the rigour of legal
articles, there is no legal large model with satisfactory practical application
yet. In this paper, we present DeliLaw, a Chinese legal counselling system
based on a large language model. DeliLaw integrates a legal retrieval module
and a case retrieval module to overcome the model hallucination. Users can
consult professional legal questions, search for legal articles and relevant
judgement cases, etc. on the DeliLaw system in a dialogue mode. In addition,
DeliLaw supports the use of English for counseling. we provide the address of
the system: https://data.delilegal.com/lawQuestion.","[{'name': 'Nan Xie'}, {'name': 'Yuelin Bai'}, {'name': 'Hengyuan Gao'}, {'name': 'Feiteng Fang'}, {'name': 'Qixuan Zhao'}, {'name': 'Zhijian Li'}, {'name': 'Ziqiang Xue'}, {'name': 'Liang Zhu'}, {'name': 'Shiwen Ni'}, {'name': 'Min Yang'}]",2024-08-01T07:54:52Z
http://arxiv.org/abs/2408.00307v1,http://arxiv.org/abs/2408.00307v1,ABC Align: Large Language Model Alignment for Safety & Accuracy,"Alignment of Large Language Models (LLMs) remains an unsolved problem. Human
preferences are highly distributed and can be captured at multiple levels of
abstraction, from the individual to diverse populations. Organisational
preferences, represented by standards and principles, are defined to mitigate
reputational risk or meet legislative obligations. In this paper, we present
ABC Align, a novel alignment methodology for LLMs that enables integration of
the standards and preferences of a large media organisation into the LLM
itself. We combine a set of data and methods that build on recent breakthroughs
in synthetic data generation, preference optimisation, and post-training model
quantisation. Our unified approach mitigates bias and improves accuracy, while
preserving reasoning capability, as measured against standard benchmarks.","[{'name': 'Gareth Seneque'}, {'name': 'Lap-Hang Ho'}, {'name': 'Ariel Kuperman'}, {'name': 'Nafise Erfanian Saeedi'}, {'name': 'Jeffrey Molendijk'}]",2024-08-01T06:06:25Z
http://arxiv.org/abs/2408.00284v1,http://arxiv.org/abs/2408.00284v1,"Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like
  Spontaneous Representation","Large-scale text-to-speech (TTS) models have made significant progress
recently.However, they still fall short in the generation of Chinese dialectal
speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS
models capable of generating high-quality Chinese dialectal speech. Bailing-TTS
serves as a foundation model for Chinese dialectal speech generation. First,
continual semi-supervised learning is proposed to facilitate the alignment of
text tokens and speech tokens. Second, the Chinese dialectal representation
learning is developed using a specific transformer architecture and multi-stage
training processes. With the proposed design of novel network architecture and
corresponding strategy, Bailing-TTS is able to generate Chinese dialectal
speech from text effectively and efficiently. Experiments demonstrate that
Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous
representation. Readers are encouraged to listen to demos at
\url{https://c9412600.github.io/bltts_tech_report/index.html}.","[{'name': 'Xinhan Di'}, {'name': 'Zihao Chen'}, {'name': 'Yunming Liang'}, {'name': 'Junjie Zheng'}, {'name': 'Yihua Wang'}, {'name': 'Chaofan Ding'}]",2024-08-01T04:57:31Z
http://arxiv.org/abs/2408.00283v1,http://arxiv.org/abs/2408.00283v1,Navigating Text-to-Image Generative Bias across Indic Languages,"This research investigates biases in text-to-image (TTI) models for the Indic
languages widely spoken across India. It evaluates and compares the generative
performance and cultural relevance of leading TTI models in these languages
against their performance in English. Using the proposed IndicTTI benchmark, we
comprehensively assess the performance of 30 Indic languages with two
open-source diffusion models and two commercial generation APIs. The primary
objective of this benchmark is to evaluate the support for Indic languages in
these models and identify areas needing improvement. Given the linguistic
diversity of 30 languages spoken by over 1.4 billion people, this benchmark
aims to provide a detailed and insightful analysis of TTI models' effectiveness
within the Indic linguistic landscape. The data and code for the IndicTTI
benchmark can be accessed at
https://iab-rubric.org/resources/other-databases/indictti.","[{'name': 'Surbhi Mittal'}, {'name': 'Arnav Sudan'}, {'name': 'Mayank Vatsa'}, {'name': 'Richa Singh'}, {'name': 'Tamar Glaser'}, {'name': 'Tal Hassner'}]",2024-08-01T04:56:13Z
http://arxiv.org/abs/2408.00274v1,http://arxiv.org/abs/2408.00274v1,"QUITO: Accelerating Long-Context Reasoning through Query-Guided Context
  Compression","In-context learning (ICL) capabilities are foundational to the success of
large language models (LLMs). Recently, context compression has attracted
growing interest since it can largely reduce reasoning complexities and
computation costs of LLMs. In this paper, we introduce a novel Query-gUIded
aTtention cOmpression (QUITO) method, which leverages attention of the question
over the contexts to filter useless information. Specifically, we take a
trigger token to calculate the attention distribution of the context in
response to the question. Based on the distribution, we propose three different
filtering methods to satisfy the budget constraints of the context length. We
evaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and
ASQA. Experimental results demonstrate that QUITO significantly outperforms
established baselines across various datasets and downstream LLMs, underscoring
its effectiveness. Our code is available at
https://github.com/Wenshansilvia/attention_compressor.","[{'name': 'Wenshan Wang'}, {'name': 'Yihang Wang'}, {'name': 'Yixing Fan'}, {'name': 'Huaming Liao'}, {'name': 'Jiafeng Guo'}]",2024-08-01T04:28:38Z
http://arxiv.org/abs/2408.00264v1,http://arxiv.org/abs/2408.00264v1,"Clover-2: Accurate Inference for Regressive Lightweight Speculative
  Decoding","Large Language Models (LLMs) frequently suffer from inefficiencies, largely
attributable to the discord between the requirements of auto-regressive
decoding and the architecture of contemporary GPUs. Recently, regressive
lightweight speculative decoding has garnered attention for its notable
efficiency improvements in text generation tasks. This approach utilizes a
lightweight regressive draft model, like a Recurrent Neural Network (RNN) or a
single transformer decoder layer, leveraging sequential information to
iteratively predict potential tokens. Specifically, RNN draft models are
computationally economical but tend to deliver lower accuracy, while attention
decoder layer models exhibit the opposite traits. This paper presents Clover-2,
an advanced iteration of Clover, an RNN-based draft model designed to achieve
comparable accuracy to that of attention decoder layer models while maintaining
minimal computational overhead. Clover-2 enhances the model architecture and
incorporates knowledge distillation to increase Clover's accuracy and improve
overall efficiency. We conducted experiments using the open-source Vicuna 7B
and LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses
existing methods across various model architectures, showcasing its efficacy
and robustness.","[{'name': 'Bin Xiao'}, {'name': 'Lujun Gui'}, {'name': 'Lei Su'}, {'name': 'Weipeng Chen'}]",2024-08-01T03:43:32Z
http://arxiv.org/abs/2408.00244v1,http://arxiv.org/abs/2408.00244v1,"Enhanced Structured State Space Models via Grouped FIR Filtering and
  Attention Sink Mechanisms","Structured State Space Models (SSMs) have emerged as compelling alternatives
to Transformer architectures, offering linear-time complexity and superior
performance in various sequence modeling tasks. Despite their advantages, SSMs
like the original Mamba-2 face training difficulties due to the sensitivities
introduced by the extended series of recurrent matrix multiplications. In this
paper, we propose an advanced architecture that mitigates these challenges by
decomposing A-multiplications into multiple groups and optimizing positional
encoding through Grouped Finite Impulse Response (FIR) filtering. This new
structure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable
matrices for efficient computation. Furthermore, inspired by the ""attention
sink"" phenomenon identified in streaming language models, we incorporate a
similar mechanism to enhance the stability and performance of our model over
extended sequences. Our approach further bridges the gap between SSMs and
Transformer architectures, offering a viable path forward for scalable and
high-performing sequence modeling.","[{'name': 'Tian Meng'}, {'name': 'Yang Tao'}, {'name': 'Wuliang Yin'}]",2024-08-01T02:49:58Z
http://arxiv.org/abs/2408.00230v2,http://arxiv.org/abs/2408.00230v2,"Lost in Translation: Latent Concept Misalignment in Text-to-Image
  Diffusion Models","Advancements in text-to-image diffusion models have broadened extensive
downstream practical applications, but such models often encounter misalignment
issues between text and image. Taking the generation of a combination of two
disentangled concepts as an example, say given the prompt ""a tea cup of iced
coke"", existing models usually generate a glass cup of iced coke because the
iced coke usually co-occurs with the glass cup instead of the tea one during
model training. The root of such misalignment is attributed to the confusion in
the latent semantic space of text-to-image diffusion models, and hence we refer
to the ""a tea cup of iced coke"" phenomenon as Latent Concept Misalignment
(LC-Mis). We leverage large language models (LLMs) to thoroughly investigate
the scope of LC-Mis, and develop an automated pipeline for aligning the latent
semantics of diffusion models to text prompts. Empirical assessments confirm
the effectiveness of our approach, substantially reducing LC-Mis errors and
enhancing the robustness and versatility of text-to-image diffusion models. The
code and dataset are here: https://github.com/RossoneriZhao/iced_coke.","[{'name': 'Juntu Zhao'}, {'name': 'Junyu Deng'}, {'name': 'Yixin Ye'}, {'name': 'Chongxuan Li'}, {'name': 'Zhijie Deng'}, {'name': 'Dequan Wang'}]",2024-08-01T01:54:17Z
http://arxiv.org/abs/2408.00205v1,http://arxiv.org/abs/2408.00205v1,"Sentence-wise Speech Summarization: Task, Datasets, and End-to-End
  Modeling with LM Knowledge Distillation","This paper introduces a novel approach called sentence-wise speech
summarization (Sen-SSum), which generates text summaries from a spoken document
in a sentence-by-sentence manner. Sen-SSum combines the real-time processing of
automatic speech recognition (ASR) with the conciseness of speech
summarization. To explore this approach, we present two datasets for Sen-SSum:
Mega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types of
Transformer-based models: 1) cascade models that combine ASR and strong text
summarization models, and 2) end-to-end (E2E) models that directly convert
speech into a text summary. While E2E models are appealing to develop
compute-efficient models, they perform worse than cascade models. Therefore, we
propose knowledge distillation for E2E models using pseudo-summaries generated
by the cascade models. Our experiments show that this proposed knowledge
distillation effectively improves the performance of the E2E model on both
datasets.","[{'name': 'Kohei Matsuura'}, {'name': 'Takanori Ashihara'}, {'name': 'Takafumi Moriya'}, {'name': 'Masato Mimura'}, {'name': 'Takatomo Kano'}, {'name': 'Atsunori Ogawa'}, {'name': 'Marc Delcroix'}]",2024-08-01T00:18:21Z
http://arxiv.org/abs/2408.00203v1,http://arxiv.org/abs/2408.00203v1,OmniParser for Pure Vision Based GUI Agent,"The recent success of large vision language models shows great potential in
driving the agent system operating on user interfaces. However, we argue that
the power multimodal models like GPT-4V as a general agent on multiple
operating systems across different applications is largely underestimated due
to the lack of a robust screen parsing technique capable of: 1) reliably
identifying interactable icons within the user interface, and 2) understanding
the semantics of various elements in a screenshot and accurately associate the
intended action with the corresponding region on the screen. To fill these
gaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing user
interface screenshots into structured elements, which significantly enhances
the ability of GPT-4V to generate actions that can be accurately grounded in
the corresponding regions of the interface. We first curated an interactable
icon detection dataset using popular webpages and an icon description dataset.
These datasets were utilized to fine-tune specialized models: a detection model
to parse interactable regions on the screen and a caption model to extract the
functional semantics of the detected elements. \textsc{OmniParser}
significantly improves GPT-4V's performance on ScreenSpot benchmark. And on
Mind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only input
outperforms the GPT-4V baselines requiring additional information outside of
screenshot.","[{'name': 'Yadong Lu'}, {'name': 'Jianwei Yang'}, {'name': 'Yelong Shen'}, {'name': 'Ahmed Awadallah'}]",2024-08-01T00:00:43Z
http://arxiv.org/abs/2408.00197v1,http://arxiv.org/abs/2408.00197v1,"Automated Software Vulnerability Static Code Analysis Using Generative
  Pre-Trained Transformer Models","Generative Pre-Trained Transformer models have been shown to be surprisingly
effective at a variety of natural language processing tasks -- including
generating computer code. We evaluate the effectiveness of open source GPT
models for the task of automatic identification of the presence of vulnerable
code syntax (specifically targeting C and C++ source code). This task is
evaluated on a selection of 36 source code examples from the NIST SARD dataset,
which are specifically curated to not contain natural English that indicates
the presence, or lack thereof, of a particular vulnerability. The NIST SARD
source code dataset contains identified vulnerable lines of source code that
are examples of one out of the 839 distinct Common Weakness Enumerations (CWE),
allowing for exact quantification of the GPT output classification error rate.
A total of 5 GPT models are evaluated, using 10 different inference
temperatures and 100 repetitions at each setting, resulting in 5,000 GPT
queries per vulnerable source code analyzed. Ultimately, we find that the GPT
models that we evaluated are not suitable for fully automated vulnerability
scanning because the false positive and false negative rates are too high to
likely be useful in practice. However, we do find that the GPT models perform
surprisingly well at automated vulnerability detection for some of the test
cases, in particular surpassing random sampling, and being able to identify the
exact lines of code that are vulnerable albeit at a low success rate. The best
performing GPT model result found was Llama-2-70b-chat-hf with inference
temperature of 0.1 applied to NIST SARD test case 149165 (which is an example
of a buffer overflow vulnerability), which had a binary classification recall
score of 1.0 and a precision of 1.0 for correctly and uniquely identifying the
vulnerable line of code and the correct CWE number.","[{'name': 'Elijah Pelofske'}, {'name': 'Vincent Urias'}, {'name': 'Lorie M. Liebrock'}]",2024-07-31T23:33:26Z
http://arxiv.org/abs/2408.00162v1,http://arxiv.org/abs/2408.00162v1,A Taxonomy of Stereotype Content in Large Language Models,"This study introduces a taxonomy of stereotype content in contemporary large
language models (LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three
powerful and widely used LLMs, for the characteristics associated with 87
social categories (e.g., gender, race, occupations). We identify 14 stereotype
dimensions (e.g., Morality, Ability, Health, Beliefs, Emotions), accounting for
~90% of LLM stereotype associations. Warmth and Competence facets were the most
frequent content, but all other dimensions were significantly prevalent.
Stereotypes were more positive in LLMs (vs. humans), but there was significant
variability across categories and dimensions. Finally, the taxonomy predicted
the LLMs' internal evaluations of social categories (e.g., how
positively/negatively the categories were represented), supporting the
relevance of a multidimensional taxonomy for characterizing LLM stereotypes.
Our findings suggest that high-dimensional human stereotypes are reflected in
LLMs and must be considered in AI auditing and debiasing to minimize
unidentified harms from reliance in low-dimensional views of bias in LLMs.","[{'name': 'Gandalf Nicolas'}, {'name': 'Aylin Caliskan'}]",2024-07-31T21:14:41Z
http://arxiv.org/abs/2408.00161v2,http://arxiv.org/abs/2408.00161v2,"Automatic Generation of Behavioral Test Cases For Natural Language
  Processing Using Clustering and Prompting","Recent work in behavioral testing for natural language processing (NLP)
models, such as Checklist, is inspired by related paradigms in software
engineering testing. They allow evaluation of general linguistic capabilities
and domain understanding, hence can help evaluate conceptual soundness and
identify model weaknesses. However, a major challenge is the creation of test
cases. The current packages rely on semi-automated approach using manual
development which requires domain expertise and can be time consuming. This
paper introduces an automated approach to develop test cases by exploiting the
power of large language models and statistical techniques. It clusters the text
representations to carefully construct meaningful groups and then apply
prompting techniques to automatically generate Minimal Functionality Tests
(MFT). The well-known Amazon Reviews corpus is used to demonstrate our
approach. We analyze the behavioral test profiles across four different
classification algorithms and discuss the limitations and strengths of those
models.","[{'name': 'Ying Li'}, {'name': 'Rahul Singh'}, {'name': 'Tarun Joshi'}, {'name': 'Agus Sudjianto'}]",2024-07-31T21:12:21Z
http://arxiv.org/abs/2408.00144v1,http://arxiv.org/abs/2408.00144v1,Distributed In-Context Learning under Non-IID Among Clients,"Advancements in large language models (LLMs) have shown their effectiveness
in multiple complicated natural language reasoning tasks. A key challenge
remains in adapting these models efficiently to new or unfamiliar tasks.
In-context learning (ICL) provides a promising solution for few-shot adaptation
by retrieving a set of data points relevant to a query, called in-context
examples (ICE), from a training dataset and providing them during the inference
as context. Most existing studies utilize a centralized training dataset, yet
many real-world datasets may be distributed among multiple clients, and remote
data retrieval can be associated with costs. Especially when the client data
are non-identical independent distributions (non-IID), retrieving from clients
a proper set of ICEs needed for a test query presents critical challenges. In
this paper, we first show that in this challenging setting, test queries will
have different preferences among clients because of non-IIDness, and equal
contribution often leads to suboptimal performance. We then introduce a novel
approach to tackle the distributed non-IID ICL problem when a data usage budget
is present. The principle is that each client's proper contribution (budget)
should be designed according to the preference of each query for that client.
Our approach uses a data-driven manner to allocate a budget for each client,
tailored to each test query. Through extensive empirical studies on diverse
datasets, our framework demonstrates superior performance relative to competing
baselines.","[{'name': 'Siqi Liang'}, {'name': 'Sumyeong Ahn'}, {'name': 'Jiayu Zhou'}]",2024-07-31T20:06:25Z
http://arxiv.org/abs/2408.00137v1,http://arxiv.org/abs/2408.00137v1,"Correcting Negative Bias in Large Language Models through Negative
  Attention Score Alignment","A binary decision task, like yes-no questions or answer verification,
reflects a significant real-world scenario such as where users look for
confirmation about the correctness of their decisions on specific issues. In
this work, we observe that language models exhibit a negative bias in the
binary decisions of complex reasoning tasks. Based on our observations and the
rationale about attention-based model dynamics, we propose a negative attention
score (NAS) to systematically and quantitatively formulate negative bias. Based
on NAS, we identify attention heads that attend to negative tokens provided in
the instructions as answer candidate of binary decisions, regardless of the
question in the prompt, and validate their association with the negative bias.
Additionally, we propose the negative attention score alignment (NASA) method,
which is a parameter-efficient fine-tuning technique to address the extracted
negatively biased attention heads. Experimental results from various domains of
reasoning tasks and large model search space demonstrate that NASA
significantly reduces the gap between precision and recall caused by negative
bias while preserving their generalization abilities. Our codes are available
at \url{https://github.com/ysw1021/NASA}.","[{'name': 'Sangwon Yu'}, {'name': 'Jongyoon Song'}, {'name': 'Bongkyu Hwang'}, {'name': 'Hoyoung Kang'}, {'name': 'Sooah Cho'}, {'name': 'Junhwa Choi'}, {'name': 'Seongho Joe'}, {'name': 'Taehee Lee'}, {'name': 'Youngjune L. Gwon'}, {'name': 'Sungroh Yoon'}]",2024-07-31T19:50:57Z
http://arxiv.org/abs/2408.00122v1,http://arxiv.org/abs/2408.00122v1,A Course Shared Task on Evaluating LLM Output for Clinical Questions,"This paper presents a shared task that we organized at the Foundations of
Language Technology (FoLT) course in 2023/2024 at the Technical University of
Darmstadt, which focuses on evaluating the output of Large Language Models
(LLMs) in generating harmful answers to health-related clinical questions. We
describe the task design considerations and report the feedback we received
from the students. We expect the task and the findings reported in this paper
to be relevant for instructors teaching natural language processing (NLP) and
designing course assignments.","[{'name': 'Yufang Hou'}, {'name': 'Thy Thy Tran'}, {'name': 'Doan Nam Long Vu'}, {'name': 'Yiwen Cao'}, {'name': 'Kai Li'}, {'name': 'Lukas Rohde'}, {'name': 'Iryna Gurevych'}]",2024-07-31T19:24:40Z
http://arxiv.org/abs/2408.00118v2,http://arxiv.org/abs/2408.00118v2,Gemma 2: Improving Open Language Models at a Practical Size,"In this work, we introduce Gemma 2, a new addition to the Gemma family of
lightweight, state-of-the-art open models, ranging in scale from 2 billion to
27 billion parameters. In this new version, we apply several known technical
modifications to the Transformer architecture, such as interleaving
local-global attentions (Beltagy et al., 2020a) and group-query attention
(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge
distillation (Hinton et al., 2015) instead of next token prediction. The
resulting models deliver the best performance for their size, and even offer
competitive alternatives to models that are 2-3 times bigger. We release all
our models to the community.","[{'name': 'Gemma Team'}, {'name': 'Morgane Riviere'}, {'name': 'Shreya Pathak'}, {'name': 'Pier Giuseppe Sessa'}, {'name': 'Cassidy Hardin'}, {'name': 'Surya Bhupatiraju'}, {'name': 'Léonard Hussenot'}, {'name': 'Thomas Mesnard'}, {'name': 'Bobak Shahriari'}, {'name': 'Alexandre Ramé'}, {'name': 'Johan Ferret'}, {'name': 'Peter Liu'}, {'name': 'Pouya Tafti'}, {'name': 'Abe Friesen'}, {'name': 'Michelle Casbon'}, {'name': 'Sabela Ramos'}, {'name': 'Ravin Kumar'}, {'name': 'Charline Le Lan'}, {'name': 'Sammy Jerome'}, {'name': 'Anton Tsitsulin'}, {'name': 'Nino Vieillard'}, {'name': 'Piotr Stanczyk'}, {'name': 'Sertan Girgin'}, {'name': 'Nikola Momchev'}, {'name': 'Matt Hoffman'}, {'name': 'Shantanu Thakoor'}, {'name': 'Jean-Bastien Grill'}, {'name': 'Behnam Neyshabur'}, {'name': 'Olivier Bachem'}, {'name': 'Alanna Walton'}, {'name': 'Aliaksei Severyn'}, {'name': 'Alicia Parrish'}, {'name': 'Aliya Ahmad'}, {'name': 'Allen Hutchison'}, {'name': 'Alvin Abdagic'}, {'name': 'Amanda Carl'}, {'name': 'Amy Shen'}, {'name': 'Andy Brock'}, {'name': 'Andy Coenen'}, {'name': 'Anthony Laforge'}, {'name': 'Antonia Paterson'}, {'name': 'Ben Bastian'}, {'name': 'Bilal Piot'}, {'name': 'Bo Wu'}, {'name': 'Brandon Royal'}, {'name': 'Charlie Chen'}, {'name': 'Chintu Kumar'}, {'name': 'Chris Perry'}, {'name': 'Chris Welty'}, {'name': 'Christopher A. Choquette-Choo'}, {'name': 'Danila Sinopalnikov'}, {'name': 'David Weinberger'}, {'name': 'Dimple Vijaykumar'}, {'name': 'Dominika Rogozińska'}, {'name': 'Dustin Herbison'}, {'name': 'Elisa Bandy'}, {'name': 'Emma Wang'}, {'name': 'Eric Noland'}, {'name': 'Erica Moreira'}, {'name': 'Evan Senter'}, {'name': 'Evgenii Eltyshev'}, {'name': 'Francesco Visin'}, {'name': 'Gabriel Rasskin'}, {'name': 'Gary Wei'}, {'name': 'Glenn Cameron'}, {'name': 'Gus Martins'}, {'name': 'Hadi Hashemi'}, {'name': 'Hanna Klimczak-Plucińska'}, {'name': 'Harleen Batra'}, {'name': 'Harsh Dhand'}, {'name': 'Ivan Nardini'}, {'name': 'Jacinda Mein'}, {'name': 'Jack Zhou'}, {'name': 'James Svensson'}, {'name': 'Jeff Stanway'}, {'name': 'Jetha Chan'}, {'name': 'Jin Peng Zhou'}, {'name': 'Joana Carrasqueira'}, {'name': 'Joana Iljazi'}, {'name': 'Jocelyn Becker'}, {'name': 'Joe Fernandez'}, {'name': 'Joost van Amersfoort'}, {'name': 'Josh Gordon'}, {'name': 'Josh Lipschultz'}, {'name': 'Josh Newlan'}, {'name': 'Ju-yeong Ji'}, {'name': 'Kareem Mohamed'}, {'name': 'Kartikeya Badola'}, {'name': 'Kat Black'}, {'name': 'Katie Millican'}, {'name': 'Keelin McDonell'}, {'name': 'Kelvin Nguyen'}, {'name': 'Kiranbir Sodhia'}, {'name': 'Kish Greene'}, {'name': 'Lars Lowe Sjoesund'}, {'name': 'Lauren Usui'}, {'name': 'Laurent Sifre'}, {'name': 'Lena Heuermann'}, {'name': 'Leticia Lago'}, {'name': 'Lilly McNealus'}, {'name': 'Livio Baldini Soares'}, {'name': 'Logan Kilpatrick'}, {'name': 'Lucas Dixon'}, {'name': 'Luciano Martins'}, {'name': 'Machel Reid'}, {'name': 'Manvinder Singh'}, {'name': 'Mark Iverson'}, {'name': 'Martin Görner'}, {'name': 'Mat Velloso'}, {'name': 'Mateo Wirth'}, {'name': 'Matt Davidow'}, {'name': 'Matt Miller'}, {'name': 'Matthew Rahtz'}, {'name': 'Matthew Watson'}, {'name': 'Meg Risdal'}, {'name': 'Mehran Kazemi'}, {'name': 'Michael Moynihan'}, {'name': 'Ming Zhang'}, {'name': 'Minsuk Kahng'}, {'name': 'Minwoo Park'}, {'name': 'Mofi Rahman'}, {'name': 'Mohit Khatwani'}, {'name': 'Natalie Dao'}, {'name': 'Nenshad Bardoliwalla'}, {'name': 'Nesh Devanathan'}, {'name': 'Neta Dumai'}, {'name': 'Nilay Chauhan'}, {'name': 'Oscar Wahltinez'}, {'name': 'Pankil Botarda'}, {'name': 'Parker Barnes'}, {'name': 'Paul Barham'}, {'name': 'Paul Michel'}, {'name': 'Pengchong Jin'}, {'name': 'Petko Georgiev'}, {'name': 'Phil Culliton'}, {'name': 'Pradeep Kuppala'}, {'name': 'Ramona Comanescu'}, {'name': 'Ramona Merhej'}, {'name': 'Reena Jana'}, {'name': 'Reza Ardeshir Rokni'}, {'name': 'Rishabh Agarwal'}, {'name': 'Ryan Mullins'}, {'name': 'Samaneh Saadat'}, {'name': 'Sara Mc Carthy'}, {'name': 'Sarah Perrin'}, {'name': 'Sébastien M. R. Arnold'}, {'name': 'Sebastian Krause'}, {'name': 'Shengyang Dai'}, {'name': 'Shruti Garg'}, {'name': 'Shruti Sheth'}, {'name': 'Sue Ronstrom'}, {'name': 'Susan Chan'}, {'name': 'Timothy Jordan'}, {'name': 'Ting Yu'}, {'name': 'Tom Eccles'}, {'name': 'Tom Hennigan'}, {'name': 'Tomas Kocisky'}, {'name': 'Tulsee Doshi'}, {'name': 'Vihan Jain'}, {'name': 'Vikas Yadav'}, {'name': 'Vilobh Meshram'}, {'name': 'Vishal Dharmadhikari'}, {'name': 'Warren Barkley'}, {'name': 'Wei Wei'}, {'name': 'Wenming Ye'}, {'name': 'Woohyun Han'}, {'name': 'Woosuk Kwon'}, {'name': 'Xiang Xu'}, {'name': 'Zhe Shen'}, {'name': 'Zhitao Gong'}, {'name': 'Zichuan Wei'}, {'name': 'Victor Cotruta'}, {'name': 'Phoebe Kirk'}, {'name': 'Anand Rao'}, {'name': 'Minh Giang'}, {'name': 'Ludovic Peran'}, {'name': 'Tris Warkentin'}, {'name': 'Eli Collins'}, {'name': 'Joelle Barral'}, {'name': 'Zoubin Ghahramani'}, {'name': 'Raia Hadsell'}, {'name': 'D. Sculley'}, {'name': 'Jeanine Banks'}, {'name': 'Anca Dragan'}, {'name': 'Slav Petrov'}, {'name': 'Oriol Vinyals'}, {'name': 'Jeff Dean'}, {'name': 'Demis Hassabis'}, {'name': 'Koray Kavukcuoglu'}, {'name': 'Clement Farabet'}, {'name': 'Elena Buchatskaya'}, {'name': 'Sebastian Borgeaud'}, {'name': 'Noah Fiedel'}, {'name': 'Armand Joulin'}, {'name': 'Kathleen Kenealy'}, {'name': 'Robert Dadashi'}, {'name': 'Alek Andreev'}]",2024-07-31T19:13:07Z
http://arxiv.org/abs/2408.00113v1,http://arxiv.org/abs/2408.00113v1,"Measuring Progress in Dictionary Learning for Language Model
  Interpretability with Board Game Models","What latent features are encoded in language model (LM) representations?
Recent work on training sparse autoencoders (SAEs) to disentangle interpretable
features in LM representations has shown significant promise. However,
evaluating the quality of these SAEs is difficult because we lack a
ground-truth collection of interpretable features that we expect good SAEs to
recover. We thus propose to measure progress in interpretable dictionary
learning by working in the setting of LMs trained on chess and Othello
transcripts. These settings carry natural collections of interpretable features
-- for example, ""there is a knight on F3"" -- which we leverage into
$\textit{supervised}$ metrics for SAE quality. To guide progress in
interpretable dictionary learning, we introduce a new SAE training technique,
$\textit{p-annealing}$, which improves performance on prior unsupervised
metrics as well as our new metrics.","[{'name': 'Adam Karvonen'}, {'name': 'Benjamin Wright'}, {'name': 'Can Rager'}, {'name': 'Rico Angell'}, {'name': 'Jannik Brinkmann'}, {'name': 'Logan Smith'}, {'name': 'Claudio Mayrink Verdun'}, {'name': 'David Bau'}, {'name': 'Samuel Marks'}]",2024-07-31T18:45:13Z
http://arxiv.org/abs/2408.00103v1,http://arxiv.org/abs/2408.00103v1,"ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation
  Extraction on an Academic Budget","Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in
Natural Language Processing, serving as critical components in a wide range of
applications. In this paper, we propose ReLiK, a Retriever-Reader architecture
for both EL and RE, where, given an input text, the Retriever module undertakes
the identification of candidate entities or relations that could potentially
appear within the text. Subsequently, the Reader module is tasked to discern
the pertinent retrieved entities or relations and establish their alignment
with the corresponding textual spans. Notably, we put forward an innovative
input representation that incorporates the candidate entities or relations
alongside the text, making it possible to link entities or extract relations in
a single forward pass and to fully leverage pre-trained language models
contextualization capabilities, in contrast with previous
Retriever-Reader-based methods, which require a forward pass for each
candidate. Our formulation of EL and RE achieves state-of-the-art performance
in both in-domain and out-of-domain benchmarks while using academic budget
training and with up to 40x inference speed compared to competitors. Finally,
we show how our architecture can be used seamlessly for Information Extraction
(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared
Reader that simultaneously extracts entities and relations.","[{'name': 'Riccardo Orlando'}, {'name': 'Pere-Lluis Huguet-Cabot'}, {'name': 'Edoardo Barba'}, {'name': 'Roberto Navigli'}]",2024-07-31T18:25:49Z
http://arxiv.org/abs/2407.21792v1,http://arxiv.org/abs/2407.21792v1,Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?,"As artificial intelligence systems grow more powerful, there has been
increasing interest in ""AI safety"" research to address emerging and future
risks. However, the field of AI safety remains poorly defined and
inconsistently measured, leading to confusion about how researchers can
contribute. This lack of clarity is compounded by the unclear relationship
between AI safety benchmarks and upstream general capabilities (e.g., general
knowledge and reasoning). To address these issues, we conduct a comprehensive
meta-analysis of AI safety benchmarks, empirically analyzing their correlation
with general capabilities across dozens of models and providing a survey of
existing directions in AI safety. Our findings reveal that many safety
benchmarks highly correlate with upstream model capabilities, potentially
enabling ""safetywashing"" -- where capability improvements are misrepresented as
safety advancements. Based on these findings, we propose an empirical
foundation for developing more meaningful safety metrics and define AI safety
in a machine learning research context as a set of clearly delineated research
goals that are empirically separable from generic capabilities advancements. In
doing so, we aim to provide a more rigorous framework for AI safety research,
advancing the science of safety evaluations and clarifying the path towards
measurable progress.","[{'name': 'Richard Ren'}, {'name': 'Steven Basart'}, {'name': 'Adam Khoja'}, {'name': 'Alice Gatti'}, {'name': 'Long Phan'}, {'name': 'Xuwang Yin'}, {'name': 'Mantas Mazeika'}, {'name': 'Alexander Pan'}, {'name': 'Gabriel Mukobi'}, {'name': 'Ryan H. Kim'}, {'name': 'Stephen Fitz'}, {'name': 'Dan Hendrycks'}]",2024-07-31T17:59:24Z
http://arxiv.org/abs/2407.21788v1,http://arxiv.org/abs/2407.21788v1,Vision-Language Model Based Handwriting Verification,"Handwriting Verification is a critical in document forensics. Deep learning
based approaches often face skepticism from forensic document examiners due to
their lack of explainability and reliance on extensive training data and
handcrafted features. This paper explores using Vision Language Models (VLMs),
such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By
leveraging their Visual Question Answering capabilities and 0-shot
Chain-of-Thought (CoT) reasoning, our goal is to provide clear,
human-understandable explanations for model decisions. Our experiments on the
CEDAR handwriting dataset demonstrate that VLMs offer enhanced
interpretability, reduce the need for large training datasets, and adapt better
to diverse handwriting styles. However, results show that the CNN-based
ResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach
with GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:
71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings
highlight the potential of VLMs in generating human-interpretable decisions
while underscoring the need for further advancements to match the performance
of specialized deep learning models.","[{'name': 'Mihir Chauhan'}, {'name': 'Abhishek Satbhai'}, {'name': 'Mohammad Abuzar Hashemi'}, {'name': 'Mir Basheer Ali'}, {'name': 'Bina Ramamurthy'}, {'name': 'Mingchen Gao'}, {'name': 'Siwei Lyu'}, {'name': 'Sargur Srihari'}]",2024-07-31T17:57:32Z
http://arxiv.org/abs/2407.21783v1,http://arxiv.org/abs/2407.21783v1,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models.
This paper presents a new set of foundation models, called Llama 3. It is a
herd of language models that natively support multilinguality, coding,
reasoning, and tool usage. Our largest model is a dense Transformer with 405B
parameters and a context window of up to 128K tokens. This paper presents an
extensive empirical evaluation of Llama 3. We find that Llama 3 delivers
comparable quality to leading language models such as GPT-4 on a plethora of
tasks. We publicly release Llama 3, including pre-trained and post-trained
versions of the 405B parameter language model and our Llama Guard 3 model for
input and output safety. The paper also presents the results of experiments in
which we integrate image, video, and speech capabilities into Llama 3 via a
compositional approach. We observe this approach performs competitively with
the state-of-the-art on image, video, and speech recognition tasks. The
resulting models are not yet being broadly released as they are still under
development.","[{'name': 'Abhimanyu Dubey'}, {'name': 'Abhinav Jauhri'}, {'name': 'Abhinav Pandey'}, {'name': 'Abhishek Kadian'}, {'name': 'Ahmad Al-Dahle'}, {'name': 'Aiesha Letman'}, {'name': 'Akhil Mathur'}, {'name': 'Alan Schelten'}, {'name': 'Amy Yang'}, {'name': 'Angela Fan'}, {'name': 'Anirudh Goyal'}, {'name': 'Anthony Hartshorn'}, {'name': 'Aobo Yang'}, {'name': 'Archi Mitra'}, {'name': 'Archie Sravankumar'}, {'name': 'Artem Korenev'}, {'name': 'Arthur Hinsvark'}, {'name': 'Arun Rao'}, {'name': 'Aston Zhang'}, {'name': 'Aurelien Rodriguez'}, {'name': 'Austen Gregerson'}, {'name': 'Ava Spataru'}, {'name': 'Baptiste Roziere'}, {'name': 'Bethany Biron'}, {'name': 'Binh Tang'}, {'name': 'Bobbie Chern'}, {'name': 'Charlotte Caucheteux'}, {'name': 'Chaya Nayak'}, {'name': 'Chloe Bi'}, {'name': 'Chris Marra'}, {'name': 'Chris McConnell'}, {'name': 'Christian Keller'}, {'name': 'Christophe Touret'}, {'name': 'Chunyang Wu'}, {'name': 'Corinne Wong'}, {'name': 'Cristian Canton Ferrer'}, {'name': 'Cyrus Nikolaidis'}, {'name': 'Damien Allonsius'}, {'name': 'Daniel Song'}, {'name': 'Danielle Pintz'}, {'name': 'Danny Livshits'}, {'name': 'David Esiobu'}, {'name': 'Dhruv Choudhary'}, {'name': 'Dhruv Mahajan'}, {'name': 'Diego Garcia-Olano'}, {'name': 'Diego Perino'}, {'name': 'Dieuwke Hupkes'}, {'name': 'Egor Lakomkin'}, {'name': 'Ehab AlBadawy'}, {'name': 'Elina Lobanova'}, {'name': 'Emily Dinan'}, {'name': 'Eric Michael Smith'}, {'name': 'Filip Radenovic'}, {'name': 'Frank Zhang'}, {'name': 'Gabriel Synnaeve'}, {'name': 'Gabrielle Lee'}, {'name': 'Georgia Lewis Anderson'}, {'name': 'Graeme Nail'}, {'name': 'Gregoire Mialon'}, {'name': 'Guan Pang'}, {'name': 'Guillem Cucurell'}, {'name': 'Hailey Nguyen'}, {'name': 'Hannah Korevaar'}, {'name': 'Hu Xu'}, {'name': 'Hugo Touvron'}, {'name': 'Iliyan Zarov'}, {'name': 'Imanol Arrieta Ibarra'}, {'name': 'Isabel Kloumann'}, {'name': 'Ishan Misra'}, {'name': 'Ivan Evtimov'}, {'name': 'Jade Copet'}, {'name': 'Jaewon Lee'}, {'name': 'Jan Geffert'}, {'name': 'Jana Vranes'}, {'name': 'Jason Park'}, {'name': 'Jay Mahadeokar'}, {'name': 'Jeet Shah'}, {'name': 'Jelmer van der Linde'}, {'name': 'Jennifer Billock'}, {'name': 'Jenny Hong'}, {'name': 'Jenya Lee'}, {'name': 'Jeremy Fu'}, {'name': 'Jianfeng Chi'}, {'name': 'Jianyu Huang'}, {'name': 'Jiawen Liu'}, {'name': 'Jie Wang'}, {'name': 'Jiecao Yu'}, {'name': 'Joanna Bitton'}, {'name': 'Joe Spisak'}, {'name': 'Jongsoo Park'}, {'name': 'Joseph Rocca'}, {'name': 'Joshua Johnstun'}, {'name': 'Joshua Saxe'}, {'name': 'Junteng Jia'}, {'name': 'Kalyan Vasuden Alwala'}, {'name': 'Kartikeya Upasani'}, {'name': 'Kate Plawiak'}, {'name': 'Ke Li'}, {'name': 'Kenneth Heafield'}, {'name': 'Kevin Stone'}, {'name': 'Khalid El-Arini'}, {'name': 'Krithika Iyer'}, {'name': 'Kshitiz Malik'}, {'name': 'Kuenley Chiu'}, {'name': 'Kunal Bhalla'}, {'name': 'Lauren Rantala-Yeary'}, {'name': 'Laurens van der Maaten'}, {'name': 'Lawrence Chen'}, {'name': 'Liang Tan'}, {'name': 'Liz Jenkins'}, {'name': 'Louis Martin'}, {'name': 'Lovish Madaan'}, {'name': 'Lubo Malo'}, {'name': 'Lukas Blecher'}, {'name': 'Lukas Landzaat'}, {'name': 'Luke de Oliveira'}, {'name': 'Madeline Muzzi'}, {'name': 'Mahesh Pasupuleti'}, {'name': 'Mannat Singh'}, {'name': 'Manohar Paluri'}, {'name': 'Marcin Kardas'}, {'name': 'Mathew Oldham'}, {'name': 'Mathieu Rita'}, {'name': 'Maya Pavlova'}, {'name': 'Melanie Kambadur'}, {'name': 'Mike Lewis'}, {'name': 'Min Si'}, {'name': 'Mitesh Kumar Singh'}, {'name': 'Mona Hassan'}, {'name': 'Naman Goyal'}, {'name': 'Narjes Torabi'}, {'name': 'Nikolay Bashlykov'}, {'name': 'Nikolay Bogoychev'}, {'name': 'Niladri Chatterji'}, {'name': 'Olivier Duchenne'}, {'name': 'Onur Çelebi'}, {'name': 'Patrick Alrassy'}, {'name': 'Pengchuan Zhang'}, {'name': 'Pengwei Li'}, {'name': 'Petar Vasic'}, {'name': 'Peter Weng'}, {'name': 'Prajjwal Bhargava'}, {'name': 'Pratik Dubal'}, {'name': 'Praveen Krishnan'}, {'name': 'Punit Singh Koura'}, {'name': 'Puxin Xu'}, {'name': 'Qing He'}, {'name': 'Qingxiao Dong'}, {'name': 'Ragavan Srinivasan'}, {'name': 'Raj Ganapathy'}, {'name': 'Ramon Calderer'}, {'name': 'Ricardo Silveira Cabral'}, {'name': 'Robert Stojnic'}, {'name': 'Roberta Raileanu'}, {'name': 'Rohit Girdhar'}, {'name': 'Rohit Patel'}, {'name': 'Romain Sauvestre'}, {'name': 'Ronnie Polidoro'}, {'name': 'Roshan Sumbaly'}, {'name': 'Ross Taylor'}, {'name': 'Ruan Silva'}, {'name': 'Rui Hou'}, {'name': 'Rui Wang'}, {'name': 'Saghar Hosseini'}, {'name': 'Sahana Chennabasappa'}, {'name': 'Sanjay Singh'}, {'name': 'Sean Bell'}, {'name': 'Seohyun Sonia Kim'}, {'name': 'Sergey Edunov'}, {'name': 'Shaoliang Nie'}, {'name': 'Sharan Narang'}, {'name': 'Sharath Raparthy'}, {'name': 'Sheng Shen'}, {'name': 'Shengye Wan'}, {'name': 'Shruti Bhosale'}, {'name': 'Shun Zhang'}, {'name': 'Simon Vandenhende'}, {'name': 'Soumya Batra'}, {'name': 'Spencer Whitman'}, {'name': 'Sten Sootla'}, {'name': 'Stephane Collot'}, {'name': 'Suchin Gururangan'}, {'name': 'Sydney Borodinsky'}, {'name': 'Tamar Herman'}, {'name': 'Tara Fowler'}, {'name': 'Tarek Sheasha'}, {'name': 'Thomas Georgiou'}, {'name': 'Thomas Scialom'}, {'name': 'Tobias Speckbacher'}, {'name': 'Todor Mihaylov'}, {'name': 'Tong Xiao'}, {'name': 'Ujjwal Karn'}, {'name': 'Vedanuj Goswami'}, {'name': 'Vibhor Gupta'}, {'name': 'Vignesh Ramanathan'}, {'name': 'Viktor Kerkez'}, {'name': 'Vincent Gonguet'}, {'name': 'Virginie Do'}, {'name': 'Vish Vogeti'}, {'name': 'Vladan Petrovic'}, {'name': 'Weiwei Chu'}, {'name': 'Wenhan Xiong'}, {'name': 'Wenyin Fu'}, {'name': 'Whitney Meers'}, {'name': 'Xavier Martinet'}, {'name': 'Xiaodong Wang'}, {'name': 'Xiaoqing Ellen Tan'}, {'name': 'Xinfeng Xie'}, {'name': 'Xuchao Jia'}, {'name': 'Xuewei Wang'}, {'name': 'Yaelle Goldschlag'}, {'name': 'Yashesh Gaur'}, {'name': 'Yasmine Babaei'}, {'name': 'Yi Wen'}, {'name': 'Yiwen Song'}, {'name': 'Yuchen Zhang'}, {'name': 'Yue Li'}, {'name': 'Yuning Mao'}, {'name': 'Zacharie Delpierre Coudert'}, {'name': 'Zheng Yan'}, {'name': 'Zhengxing Chen'}, {'name': 'Zoe Papakipos'}, {'name': 'Aaditya Singh'}, {'name': 'Aaron Grattafiori'}, {'name': 'Abha Jain'}, {'name': 'Adam Kelsey'}, {'name': 'Adam Shajnfeld'}, {'name': 'Adithya Gangidi'}, {'name': 'Adolfo Victoria'}, {'name': 'Ahuva Goldstand'}, {'name': 'Ajay Menon'}, {'name': 'Ajay Sharma'}, {'name': 'Alex Boesenberg'}, {'name': 'Alex Vaughan'}, {'name': 'Alexei Baevski'}, {'name': 'Allie Feinstein'}, {'name': 'Amanda Kallet'}, {'name': 'Amit Sangani'}, {'name': 'Anam Yunus'}, {'name': 'Andrei Lupu'}, {'name': 'Andres Alvarado'}, {'name': 'Andrew Caples'}, {'name': 'Andrew Gu'}, {'name': 'Andrew Ho'}, {'name': 'Andrew Poulton'}, {'name': 'Andrew Ryan'}, {'name': 'Ankit Ramchandani'}, {'name': 'Annie Franco'}, {'name': 'Aparajita Saraf'}, {'name': 'Arkabandhu Chowdhury'}, {'name': 'Ashley Gabriel'}, {'name': 'Ashwin Bharambe'}, {'name': 'Assaf Eisenman'}, {'name': 'Azadeh Yazdan'}, {'name': 'Beau James'}, {'name': 'Ben Maurer'}, {'name': 'Benjamin Leonhardi'}, {'name': 'Bernie Huang'}, {'name': 'Beth Loyd'}, {'name': 'Beto De Paola'}, {'name': 'Bhargavi Paranjape'}, {'name': 'Bing Liu'}, {'name': 'Bo Wu'}, {'name': 'Boyu Ni'}, {'name': 'Braden Hancock'}, {'name': 'Bram Wasti'}, {'name': 'Brandon Spence'}, {'name': 'Brani Stojkovic'}, {'name': 'Brian Gamido'}, {'name': 'Britt Montalvo'}, {'name': 'Carl Parker'}, {'name': 'Carly Burton'}, {'name': 'Catalina Mejia'}, {'name': 'Changhan Wang'}, {'name': 'Changkyu Kim'}, {'name': 'Chao Zhou'}, {'name': 'Chester Hu'}, {'name': 'Ching-Hsiang Chu'}, {'name': 'Chris Cai'}, {'name': 'Chris Tindal'}, {'name': 'Christoph Feichtenhofer'}, {'name': 'Damon Civin'}, {'name': 'Dana Beaty'}, {'name': 'Daniel Kreymer'}, {'name': 'Daniel Li'}, {'name': 'Danny Wyatt'}, {'name': 'David Adkins'}, {'name': 'David Xu'}, {'name': 'Davide Testuggine'}, {'name': 'Delia David'}, {'name': 'Devi Parikh'}, {'name': 'Diana Liskovich'}, {'name': 'Didem Foss'}, {'name': 'Dingkang Wang'}, {'name': 'Duc Le'}, {'name': 'Dustin Holland'}, {'name': 'Edward Dowling'}, {'name': 'Eissa Jamil'}, {'name': 'Elaine Montgomery'}, {'name': 'Eleonora Presani'}, {'name': 'Emily Hahn'}, {'name': 'Emily Wood'}, {'name': 'Erik Brinkman'}, {'name': 'Esteban Arcaute'}, {'name': 'Evan Dunbar'}, {'name': 'Evan Smothers'}, {'name': 'Fei Sun'}, {'name': 'Felix Kreuk'}, {'name': 'Feng Tian'}, {'name': 'Firat Ozgenel'}, {'name': 'Francesco Caggioni'}, {'name': 'Francisco Guzmán'}, {'name': 'Frank Kanayet'}, {'name': 'Frank Seide'}, {'name': 'Gabriela Medina Florez'}, {'name': 'Gabriella Schwarz'}, {'name': 'Gada Badeer'}, {'name': 'Georgia Swee'}, {'name': 'Gil Halpern'}, {'name': 'Govind Thattai'}, {'name': 'Grant Herman'}, {'name': 'Grigory Sizov'}, {'name': 'Guangyi'}, {'name': 'Zhang'}, {'name': 'Guna Lakshminarayanan'}, {'name': 'Hamid Shojanazeri'}, {'name': 'Han Zou'}, {'name': 'Hannah Wang'}, {'name': 'Hanwen Zha'}, {'name': 'Haroun Habeeb'}, {'name': 'Harrison Rudolph'}, {'name': 'Helen Suk'}, {'name': 'Henry Aspegren'}, {'name': 'Hunter Goldman'}, {'name': 'Igor Molybog'}, {'name': 'Igor Tufanov'}, {'name': 'Irina-Elena Veliche'}, {'name': 'Itai Gat'}, {'name': 'Jake Weissman'}, {'name': 'James Geboski'}, {'name': 'James Kohli'}, {'name': 'Japhet Asher'}, {'name': 'Jean-Baptiste Gaya'}, {'name': 'Jeff Marcus'}, {'name': 'Jeff Tang'}, {'name': 'Jennifer Chan'}, {'name': 'Jenny Zhen'}, {'name': 'Jeremy Reizenstein'}, {'name': 'Jeremy Teboul'}, {'name': 'Jessica Zhong'}, {'name': 'Jian Jin'}, {'name': 'Jingyi Yang'}, {'name': 'Joe Cummings'}, {'name': 'Jon Carvill'}, {'name': 'Jon Shepard'}, {'name': 'Jonathan McPhie'}, {'name': 'Jonathan Torres'}, {'name': 'Josh Ginsburg'}, {'name': 'Junjie Wang'}, {'name': 'Kai Wu'}, {'name': 'Kam Hou U'}, {'name': 'Karan Saxena'}, {'name': 'Karthik Prasad'}, {'name': 'Kartikay Khandelwal'}, {'name': 'Katayoun Zand'}, {'name': 'Kathy Matosich'}, {'name': 'Kaushik Veeraraghavan'}, {'name': 'Kelly Michelena'}, {'name': 'Keqian Li'}, {'name': 'Kun Huang'}, {'name': 'Kunal Chawla'}, {'name': 'Kushal Lakhotia'}, {'name': 'Kyle Huang'}, {'name': 'Lailin Chen'}, {'name': 'Lakshya Garg'}, {'name': 'Lavender A'}, {'name': 'Leandro Silva'}, {'name': 'Lee Bell'}, {'name': 'Lei Zhang'}, {'name': 'Liangpeng Guo'}, {'name': 'Licheng Yu'}, {'name': 'Liron Moshkovich'}, {'name': 'Luca Wehrstedt'}, {'name': 'Madian Khabsa'}, {'name': 'Manav Avalani'}, {'name': 'Manish Bhatt'}, {'name': 'Maria Tsimpoukelli'}, {'name': 'Martynas Mankus'}, {'name': 'Matan Hasson'}, {'name': 'Matthew Lennie'}, {'name': 'Matthias Reso'}, {'name': 'Maxim Groshev'}, {'name': 'Maxim Naumov'}, {'name': 'Maya Lathi'}, {'name': 'Meghan Keneally'}, {'name': 'Michael L. Seltzer'}, {'name': 'Michal Valko'}, {'name': 'Michelle Restrepo'}, {'name': 'Mihir Patel'}, {'name': 'Mik Vyatskov'}, {'name': 'Mikayel Samvelyan'}, {'name': 'Mike Clark'}, {'name': 'Mike Macey'}, {'name': 'Mike Wang'}, {'name': 'Miquel Jubert Hermoso'}, {'name': 'Mo Metanat'}, {'name': 'Mohammad Rastegari'}, {'name': 'Munish Bansal'}, {'name': 'Nandhini Santhanam'}, {'name': 'Natascha Parks'}, {'name': 'Natasha White'}, {'name': 'Navyata Bawa'}, {'name': 'Nayan Singhal'}, {'name': 'Nick Egebo'}, {'name': 'Nicolas Usunier'}, {'name': 'Nikolay Pavlovich Laptev'}, {'name': 'Ning Dong'}, {'name': 'Ning Zhang'}, {'name': 'Norman Cheng'}, {'name': 'Oleg Chernoguz'}, {'name': 'Olivia Hart'}, {'name': 'Omkar Salpekar'}, {'name': 'Ozlem Kalinli'}, {'name': 'Parkin Kent'}, {'name': 'Parth Parekh'}, {'name': 'Paul Saab'}, {'name': 'Pavan Balaji'}, {'name': 'Pedro Rittner'}, {'name': 'Philip Bontrager'}, {'name': 'Pierre Roux'}, {'name': 'Piotr Dollar'}, {'name': 'Polina Zvyagina'}, {'name': 'Prashant Ratanchandani'}, {'name': 'Pritish Yuvraj'}, {'name': 'Qian Liang'}, {'name': 'Rachad Alao'}, {'name': 'Rachel Rodriguez'}, {'name': 'Rafi Ayub'}, {'name': 'Raghotham Murthy'}, {'name': 'Raghu Nayani'}, {'name': 'Rahul Mitra'}, {'name': 'Raymond Li'}, {'name': 'Rebekkah Hogan'}, {'name': 'Robin Battey'}, {'name': 'Rocky Wang'}, {'name': 'Rohan Maheswari'}, {'name': 'Russ Howes'}, {'name': 'Ruty Rinott'}, {'name': 'Sai Jayesh Bondu'}, {'name': 'Samyak Datta'}, {'name': 'Sara Chugh'}, {'name': 'Sara Hunt'}, {'name': 'Sargun Dhillon'}, {'name': 'Sasha Sidorov'}, {'name': 'Satadru Pan'}, {'name': 'Saurabh Verma'}, {'name': 'Seiji Yamamoto'}, {'name': 'Sharadh Ramaswamy'}, {'name': 'Shaun Lindsay'}, {'name': 'Shaun Lindsay'}, {'name': 'Sheng Feng'}, {'name': 'Shenghao Lin'}, {'name': 'Shengxin Cindy Zha'}, {'name': 'Shiva Shankar'}, {'name': 'Shuqiang Zhang'}, {'name': 'Shuqiang Zhang'}, {'name': 'Sinong Wang'}, {'name': 'Sneha Agarwal'}, {'name': 'Soji Sajuyigbe'}, {'name': 'Soumith Chintala'}, {'name': 'Stephanie Max'}, {'name': 'Stephen Chen'}, {'name': 'Steve Kehoe'}, {'name': 'Steve Satterfield'}, {'name': 'Sudarshan Govindaprasad'}, {'name': 'Sumit Gupta'}, {'name': 'Sungmin Cho'}, {'name': 'Sunny Virk'}, {'name': 'Suraj Subramanian'}, {'name': 'Sy Choudhury'}, {'name': 'Sydney Goldman'}, {'name': 'Tal Remez'}, {'name': 'Tamar Glaser'}, {'name': 'Tamara Best'}, {'name': 'Thilo Kohler'}, {'name': 'Thomas Robinson'}, {'name': 'Tianhe Li'}, {'name': 'Tianjun Zhang'}, {'name': 'Tim Matthews'}, {'name': 'Timothy Chou'}, {'name': 'Tzook Shaked'}, {'name': 'Varun Vontimitta'}, {'name': 'Victoria Ajayi'}, {'name': 'Victoria Montanez'}, {'name': 'Vijai Mohan'}, {'name': 'Vinay Satish Kumar'}, {'name': 'Vishal Mangla'}, {'name': 'Vlad Ionescu'}, {'name': 'Vlad Poenaru'}, {'name': 'Vlad Tiberiu Mihailescu'}, {'name': 'Vladimir Ivanov'}, {'name': 'Wei Li'}, {'name': 'Wenchen Wang'}, {'name': 'Wenwen Jiang'}, {'name': 'Wes Bouaziz'}, {'name': 'Will Constable'}, {'name': 'Xiaocheng Tang'}, {'name': 'Xiaofang Wang'}, {'name': 'Xiaojian Wu'}, {'name': 'Xiaolan Wang'}, {'name': 'Xide Xia'}, {'name': 'Xilun Wu'}, {'name': 'Xinbo Gao'}, {'name': 'Yanjun Chen'}, {'name': 'Ye Hu'}, {'name': 'Ye Jia'}, {'name': 'Ye Qi'}, {'name': 'Yenda Li'}, {'name': 'Yilin Zhang'}, {'name': 'Ying Zhang'}, {'name': 'Yossi Adi'}, {'name': 'Youngjin Nam'}, {'name': 'Yu'}, {'name': 'Wang'}, {'name': 'Yuchen Hao'}, {'name': 'Yundi Qian'}, {'name': 'Yuzi He'}, {'name': 'Zach Rait'}, {'name': 'Zachary DeVito'}, {'name': 'Zef Rosnbrick'}, {'name': 'Zhaoduo Wen'}, {'name': 'Zhenyu Yang'}, {'name': 'Zhiwei Zhao'}]",2024-07-31T17:54:27Z
http://arxiv.org/abs/2407.21772v2,http://arxiv.org/abs/2407.21772v2,ShieldGemma: Generative AI Content Moderation Based on Gemma,"We present ShieldGemma, a comprehensive suite of LLM-based safety content
moderation models built upon Gemma2. These models provide robust,
state-of-the-art predictions of safety risks across key harm types (sexually
explicit, dangerous content, harassment, hate speech) in both user input and
LLM-generated output. By evaluating on both public and internal benchmarks, we
demonstrate superior performance compared to existing models, such as Llama
Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%).
Additionally, we present a novel LLM-based data curation pipeline, adaptable to
a variety of safety-related tasks and beyond. We have shown strong
generalization performance for model trained mainly on synthetic data. By
releasing ShieldGemma, we provide a valuable resource to the research
community, advancing LLM safety and enabling the creation of more effective
content moderation solutions for developers.","[{'name': 'Wenjun Zeng'}, {'name': 'Yuchi Liu'}, {'name': 'Ryan Mullins'}, {'name': 'Ludovic Peran'}, {'name': 'Joe Fernandez'}, {'name': 'Hamza Harkous'}, {'name': 'Karthik Narasimhan'}, {'name': 'Drew Proud'}, {'name': 'Piyush Kumar'}, {'name': 'Bhaktipriya Radharapu'}, {'name': 'Olivia Sturman'}, {'name': 'Oscar Wahltinez'}]",2024-07-31T17:48:14Z
http://arxiv.org/abs/2407.21753v1,http://arxiv.org/abs/2407.21753v1,Characterizing User Archetypes and Discussions on Scored.co,"In recent years, the proliferation of social platforms has drastically
transformed the way individuals interact, organize, and share information. In
this scenario, we experience an unprecedented increase in the scale and
complexity of interactions and, at the same time, little to no research about
some fringe social platforms. In this paper, we present a multi-dimensional
framework for characterizing nodes and hyperedges in social hypernetworks, with
a focus on the understudied alt-right platform Scored.co. Our approach
integrates the possibility of studying higher-order interactions, thanks to the
hypernetwork representation, and various node features such as user activity,
sentiment, and toxicity, with the aim to define distinct user archetypes and
understand their roles within the network. Utilizing a comprehensive dataset
from Scored.co, we analyze the dynamics of these archetypes over time and
explore their interactions and influence within the community. The framework's
versatility allows for detailed analysis of both individual user behaviors and
broader social structures. Our findings highlight the importance of
higher-order interactions in understanding social dynamics, offering new
insights into the roles and behaviors that emerge in complex online
environments.","[{'name': 'Andrea Failla'}, {'name': 'Salvatore Citraro'}, {'name': 'Giulio Rossetti'}, {'name': 'Francesco Cauteruccio'}]",2024-07-31T17:18:25Z
http://arxiv.org/abs/2407.21712v1,http://arxiv.org/abs/2407.21712v1,Adaptive Retrieval-Augmented Generation for Conversational Systems,"Despite the success of integrating large language models into the development
of conversational systems, many studies have shown the effectiveness of
retrieving and augmenting external knowledge for informative responses. Hence,
many existing studies commonly assume the always need for Retrieval Augmented
Generation (RAG) in a conversational system without explicit control. This
raises a research question about such a necessity. In this study, we propose to
investigate the need for each turn of system response to be augmented with
external knowledge. In particular, by leveraging human judgements on the binary
choice of adaptive augmentation, we develop RAGate, a gating model, which
models conversation context and relevant inputs to predict if a conversational
system requires RAG for improved responses. We conduct extensive experiments on
devising and applying RAGate to conversational models and well-rounded analyses
of different conversational scenarios. Our experimental results and analysis
indicate the effective application of RAGate in RAG-based conversational
systems in identifying system responses for appropriate RAG with high-quality
responses and a high generation confidence. This study also identifies the
correlation between the generation's confidence level and the relevance of the
augmented knowledge.","[{'name': 'Xi Wang'}, {'name': 'Procheta Sen'}, {'name': 'Ruizhe Li'}, {'name': 'Emine Yilmaz'}]",2024-07-31T16:04:03Z
http://arxiv.org/abs/2407.21669v2,http://arxiv.org/abs/2407.21669v2,Synth-Empathy: Towards High-Quality Synthetic Empathy Data,"In recent years, with the rapid advancements in large language models (LLMs),
achieving excellent empathetic response capabilities has become a crucial
prerequisite. Consequently, managing and understanding empathetic datasets have
gained increasing significance. However, empathetic data are typically
human-labeled, leading to insufficient datasets and wasted human labor. In this
work, we present Synth-Empathy, an LLM-based data generation and quality and
diversity selection pipeline that automatically generates high-quality
empathetic data while discarding low-quality data. With the data generated from
a low empathetic model, we are able to further improve empathetic response
performance and achieve state-of-the-art (SoTA) results across multiple
benchmarks. Moreover, our model achieves SoTA performance on various human
evaluation benchmarks, demonstrating its effectiveness and robustness in
real-world applications. Furthermore, we show the trade-off between data
quantity and quality, providing insights into empathetic data generation and
selection.","[{'name': 'Hao Liang'}, {'name': 'Linzhuang Sun'}, {'name': 'Jingxuan Wei'}, {'name': 'Xijie Huang'}, {'name': 'Linkun Sun'}, {'name': 'Bihui Yu'}, {'name': 'Conghui He'}, {'name': 'Wentao Zhang'}]",2024-07-31T15:12:24Z
http://arxiv.org/abs/2407.21659v2,http://arxiv.org/abs/2407.21659v2,"Defending Jailbreak Attack in VLMs via Cross-modality Information
  Detector","Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively
understand vision information, achieving remarkable performance in many
vision-centric tasks. Despite that, recent studies have shown that these models
are susceptible to jailbreak attacks, which refer to an exploitative technique
where malicious users can break the safety alignment of the target model and
generate misleading and harmful answers. This potential threat is caused by
both the inherent vulnerabilities of LLM and the larger attack scope introduced
by vision input. To enhance the security of VLMs against jailbreak attacks,
researchers have developed various defense techniques. However, these methods
either require modifications to the model's internal structure or demand
significant computational resources during the inference phase. Multimodal
information is a double-edged sword. While it increases the risk of attacks, it
also provides additional data that can enhance safeguards. Inspired by this, we
propose $\underline{\textbf{C}}$ross-modality
$\underline{\textbf{I}}$nformation
$\underline{\textbf{DE}}$tecto$\underline{\textbf{R}}$ ($\textit{CIDER})$, a
plug-and-play jailbreaking detector designed to identify maliciously perturbed
image inputs, utilizing the cross-modal similarity between harmful queries and
adversarial images. This simple yet effective cross-modality information
detector, $\textit{CIDER}$, is independent of the target VLMs and requires less
computation cost. Extensive experimental results demonstrate the effectiveness
and efficiency of $\textit{CIDER}$, as well as its transferability to both
white-box and black-box VLMs.","[{'name': 'Yue Xu'}, {'name': 'Xiuyuan Qi'}, {'name': 'Zhan Qin'}, {'name': 'Wenjie Wang'}]",2024-07-31T15:02:46Z
http://arxiv.org/abs/2408.04641v1,http://arxiv.org/abs/2408.04641v1,GPT-3 Powered Information Extraction for Building Robust Knowledge Bases,"This work uses the state-of-the-art language model GPT-3 to offer a novel
method of information extraction for knowledge base development. The suggested
method attempts to solve the difficulties associated with obtaining relevant
entities and relationships from unstructured text in order to extract
structured information. We conduct experiments on a huge corpus of text from
diverse fields to assess the performance of our suggested technique. The
evaluation measures, which are frequently employed in information extraction
tasks, include precision, recall, and F1-score. The findings demonstrate that
GPT-3 can be used to efficiently and accurately extract pertinent and correct
information from text, hence increasing the precision and productivity of
knowledge base creation. We also assess how well our suggested approach
performs in comparison to the most advanced information extraction techniques
already in use. The findings show that by utilizing only a small number of
instances in in-context learning, our suggested strategy yields competitive
outcomes with notable savings in terms of data annotation and engineering
expense. Additionally, we use our proposed method to retrieve Biomedical
information, demonstrating its practicality in a real-world setting. All things
considered, our suggested method offers a viable way to overcome the
difficulties involved in obtaining structured data from unstructured text in
order to create knowledge bases. It can greatly increase the precision and
effectiveness of information extraction, which is necessary for many
applications including chatbots, recommendation engines, and question-answering
systems.","[{'name': 'Ritabrata Roy Choudhury'}, {'name': 'Soumik Dey'}]",2024-07-31T14:59:29Z
http://arxiv.org/abs/2407.21646v1,http://arxiv.org/abs/2407.21646v1,"Towards Achieving Human Parity on End-to-end Simultaneous Speech
  Translation via LLM Agent","In this paper, we present Cross Language Agent -- Simultaneous
Interpretation, CLASI, a high-quality and human-like Simultaneous Speech
Translation (SiST) System. Inspired by professional human interpreters, we
utilize a novel data-driven read-write strategy to balance the translation
quality and latency. To address the challenge of translating in-domain
terminologies, CLASI employs a multi-modal retrieving module to obtain relevant
information to augment the translation. Supported by LLMs, our approach can
generate error-tolerated translation by considering the input audio, historical
context, and retrieved information. Experimental results show that our system
outperforms other systems by significant margins. Aligned with professional
human interpreters, we evaluate CLASI with a better human evaluation metric,
valid information proportion (VIP), which measures the amount of information
that can be successfully conveyed to the listeners. In the real-world
scenarios, where the speeches are often disfluent, informal, and unclear, CLASI
achieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese
translation directions, respectively. In contrast, state-of-the-art commercial
or open-source systems only achieve 35.4% and 41.6%. On the extremely hard
dataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%
VIP.","[{'name': 'Shanbo Cheng'}, {'name': 'Zhichao Huang'}, {'name': 'Tom Ko'}, {'name': 'Hang Li'}, {'name': 'Ningxin Peng'}, {'name': 'Lu Xu'}, {'name': 'Qini Zhang'}]",2024-07-31T14:48:27Z
http://arxiv.org/abs/2407.21633v1,http://arxiv.org/abs/2407.21633v1,"Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank
  Adaptation","Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to
transition to unfamiliar domains without manual annotation or extensive
retraining. Prior research has approached this objective by embedding prompts
into language models (LMs). Common methodologies include integrating prompts at
the input layer or introducing learnable variables at each transformer layer.
Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at
the input layer risk underutilization, with their impact potentially
diminishing across successive transformer layers. Conversely, the addition of
learnable variables to each layer can complicate the training process and
increase inference latency. To tackle the issues mentioned above, this paper
proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture
designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank
Adaptation (LoRA) components, targeting both dialogue context processing and
prompt optimization, to ensure the comprehensive influence of prompts
throughout the transformer model layers. This is achieved without incurring
additional inference latency, showcasing an efficient integration into existing
architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets,
DualLoRA demonstrates notable improvements across multiple domains,
outperforming traditional baseline methods in zero-shot settings. Our code is
accessible at: \url{https://github.com/suntea233/DualLoRA}.","[{'name': 'Xiang Luo'}, {'name': 'Zhiwen Tang'}, {'name': 'Jin Wang'}, {'name': 'Xuejie Zhang'}]",2024-07-31T14:26:41Z
http://arxiv.org/abs/2407.21630v1,http://arxiv.org/abs/2407.21630v1,"TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization
  Methods","Authorship obfuscation aims to disguise the identity of an author within a
text by altering the writing style, vocabulary, syntax, and other linguistic
features associated with the text author. This alteration needs to balance
privacy and utility. While strong obfuscation techniques can effectively hide
the author's identity, they often degrade the quality and usefulness of the
text for its intended purpose. Conversely, maintaining high utility tends to
provide insufficient privacy, making it easier for an adversary to de-anonymize
the author. Thus, achieving an optimal trade-off between these two conflicting
objectives is crucial. In this paper, we propose TAROT: Task-Oriented
Authorship Obfuscation Using Policy Optimization, a new unsupervised authorship
obfuscation method whose goal is to optimize the privacy-utility trade-off by
regenerating the entire text considering its downstream utility. Our approach
leverages policy optimization as a fine-tuning paradigm over small language
models in order to rewrite texts by preserving author identity and downstream
task utility. We show that our approach largely reduce the accuracy of
attackers while preserving utility. We make our code and models publicly
available.","[{'name': 'Gabriel Loiseau'}, {'name': 'Damien Sileo'}, {'name': 'Damien Riquet'}, {'name': 'Maxime Meyer'}, {'name': 'Marc Tommasi'}]",2024-07-31T14:24:01Z
http://arxiv.org/abs/2407.21571v1,http://arxiv.org/abs/2407.21571v1,"PMoE: Progressive Mixture of Experts with Asymmetric Transformer for
  Continual Learning","Large Language Models (LLMs) encounter significant challenges in continual
learning due to catastrophic forgetting, where new information overwrites
previously acquired knowledge. This limitation leads to substantial
environmental and economic waste. In this study, we introduce the PMoE,
Progressive Mixture of Experts with Asymmetric Transformer, which aims to
minimize forgetting by utilizing an asymmetric design with shallow layers
dedicated to general knowledge and deep layers for new knowledge. PMoE
incorporates progressively added experts in deep layers and a router that
allocates new knowledge to the appropriate experts efficiently. The router,
positioned adjacent to the deep layers, utilizes deep features aggregating
consolidated information. This enables the router to perform efficiently,
allocating new knowledge to the appropriate experts, which progressively
increase in the deep layers. Extensive experiments on TRACE datasets and
general language understanding datasets demonstrate that the proposed PMoE
outperforms previous state-of-the-art approaches.","[{'name': 'Min Jae Jung'}, {'name': 'JooHee Kim'}]",2024-07-31T12:56:14Z
http://arxiv.org/abs/2407.21560v1,http://arxiv.org/abs/2407.21560v1,"Generative Sentiment Analysis via Latent Category Distribution and
  Constrained Decoding","Fine-grained sentiment analysis involves extracting and organizing sentiment
elements from textual data. However, existing approaches often overlook issues
of category semantic inclusion and overlap, as well as inherent structural
patterns within the target sequence. This study introduces a generative
sentiment analysis model. To address the challenges related to category
semantic inclusion and overlap, a latent category distribution variable is
introduced. By reconstructing the input of a variational autoencoder, the model
learns the intensity of the relationship between categories and text, thereby
improving sequence generation. Additionally, a trie data structure and
constrained decoding strategy are utilized to exploit structural patterns,
which in turn reduces the search space and regularizes the generation process.
Experimental results on the Restaurant-ACOS and Laptop-ACOS datasets
demonstrate a significant performance improvement compared to baseline models.
Ablation experiments further confirm the effectiveness of latent category
distribution and constrained decoding strategy.","[{'name': 'Jun Zhou'}, {'name': 'Dongyang Yu'}, {'name': 'Kamran Aziz'}, {'name': 'Fangfang Su'}, {'name': 'Qing Zhang'}, {'name': 'Fei Li'}, {'name': 'Donghong Ji'}]",2024-07-31T12:29:17Z
http://arxiv.org/abs/2407.21536v1,http://arxiv.org/abs/2407.21536v1,"Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment
  Dynamics for Multimodal Emotion Recognition","Multimodal emotion recognition in conversation (MERC) has garnered
substantial research attention recently. Existing MERC methods face several
challenges: (1) they fail to fully harness direct inter-modal cues, possibly
leading to less-than-thorough cross-modal modeling; (2) they concurrently
extract information from the same and different modalities at each network
layer, potentially triggering conflicts from the fusion of multi-source data;
(3) they lack the agility required to detect dynamic sentimental changes,
perhaps resulting in inaccurate classification of utterances with abrupt
sentiment shifts. To address these issues, a novel approach named GraphSmile is
proposed for tracking intricate emotional cues in multimodal dialogues.
GraphSmile comprises two key components, i.e., GSF and SDP modules. GSF
ingeniously leverages graph structures to alternately assimilate inter-modal
and intra-modal emotional dependencies layer by layer, adequately capturing
cross-modal cues while effectively circumventing fusion conflicts. SDP is an
auxiliary task to explicitly delineate the sentiment dynamics between
utterances, promoting the model's ability to distinguish sentimental
discrepancies. Furthermore, GraphSmile is effortlessly applied to multimodal
sentiment analysis in conversation (MSAC), forging a unified multimodal
affective model capable of executing MERC and MSAC tasks. Empirical results on
multiple benchmarks demonstrate that GraphSmile can handle complex emotional
and sentimental patterns, significantly outperforming baseline models.","[{'name': 'Jiang Li'}, {'name': 'Xiaoping Wang'}, {'name': 'Zhigang Zeng'}]",2024-07-31T11:47:36Z
http://arxiv.org/abs/2407.21531v1,http://arxiv.org/abs/2407.21531v1,"Can LLMs ""Reason"" in Music? An Evaluation of LLMs' Capability of Music
  Understanding and Generation","Symbolic Music, akin to language, can be encoded in discrete symbols. Recent
research has extended the application of large language models (LLMs) such as
GPT-4 and Llama2 to the symbolic music domain including understanding and
generation. Yet scant research explores the details of how these LLMs perform
on advanced music understanding and conditioned generation, especially from the
multi-step reasoning perspective, which is a critical aspect in the
conditioned, editable, and interactive human-computer co-creation process. This
study conducts a thorough investigation of LLMs' capability and limitations in
symbolic music processing. We identify that current LLMs exhibit poor
performance in song-level multi-step music reasoning, and typically fail to
leverage learned music knowledge when addressing complex musical tasks. An
analysis of LLMs' responses highlights distinctly their pros and cons. Our
findings suggest achieving advanced musical capability is not intrinsically
obtained by LLMs, and future research should focus more on bridging the gap
between music knowledge and reasoning, to improve the co-creation experience
for musicians.","[{'name': 'Ziya Zhou'}, {'name': 'Yuhang Wu'}, {'name': 'Zhiyue Wu'}, {'name': 'Xinyue Zhang'}, {'name': 'Ruibin Yuan'}, {'name': 'Yinghao Ma'}, {'name': 'Lu Wang'}, {'name': 'Emmanouil Benetos'}, {'name': 'Wei Xue'}, {'name': 'Yike Guo'}]",2024-07-31T11:29:46Z
http://arxiv.org/abs/2407.21530v2,http://arxiv.org/abs/2407.21530v2,Data Contamination Report from the 2024 CONDA Shared Task,"The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant
aspects of data contamination in natural language processing, where data
contamination is understood as situations where evaluation data is included in
pre-training corpora used to train large scale models, compromising evaluation
results. The workshop fostered a shared task to collect evidence on data
contamination in current available datasets and models. The goal of the shared
task and associated database is to assist the community in understanding the
extent of the problem and to assist researchers in avoiding reporting
evaluation results on known contaminated resources. The shared task provides a
structured, centralized public database for the collection of contamination
evidence, open to contributions from the community via GitHub pool requests.
This first compilation paper is based on 566 reported entries over 91
contaminated sources from a total of 23 contributors. The details of the
individual contamination events are available in the platform. The platform
continues to be online, open to contributions from the community.","[{'name': 'Oscar Sainz'}, {'name': 'Iker García-Ferrero'}, {'name': 'Alon Jacovi'}, {'name': 'Jon Ander Campos'}, {'name': 'Yanai Elazar'}, {'name': 'Eneko Agirre'}, {'name': 'Yoav Goldberg'}, {'name': 'Wei-Lin Chen'}, {'name': 'Jenny Chim'}, {'name': 'Leshem Choshen'}, {'name': ""Luca D'Amico-Wong""}, {'name': 'Melissa Dell'}, {'name': 'Run-Ze Fan'}, {'name': 'Shahriar Golchin'}, {'name': 'Yucheng Li'}, {'name': 'Pengfei Liu'}, {'name': 'Bhavish Pahwa'}, {'name': 'Ameya Prabhu'}, {'name': 'Suryansh Sharma'}, {'name': 'Emily Silcock'}, {'name': 'Kateryna Solonko'}, {'name': 'David Stap'}, {'name': 'Mihai Surdeanu'}, {'name': 'Yu-Min Tseng'}, {'name': 'Vishaal Udandarao'}, {'name': 'Zengzhi Wang'}, {'name': 'Ruijie Xu'}, {'name': 'Jinglin Yang'}]",2024-07-31T11:26:57Z
http://arxiv.org/abs/2407.21512v1,http://arxiv.org/abs/2407.21512v1,"Interpreting and learning voice commands with a Large Language Model for
  a robot system","Robots are increasingly common in industry and daily life, such as in nursing
homes where they can assist staff. A key challenge is developing intuitive
interfaces for easy communication. The use of Large Language Models (LLMs) like
GPT-4 has enhanced robot capabilities, allowing for real-time interaction and
decision-making. This integration improves robots' adaptability and
functionality. This project focuses on merging LLMs with databases to improve
decision-making and enable knowledge acquisition for request interpretation
problems.","[{'name': 'Stanislau Stankevich'}, {'name': 'Wojciech Dudek'}]",2024-07-31T10:30:31Z
http://arxiv.org/abs/2407.21491v2,http://arxiv.org/abs/2407.21491v2,Generative Expressive Conversational Speech Synthesis,"Conversational Speech Synthesis (CSS) aims to express a target utterance with
the proper speaking style in a user-agent conversation setting. Existing CSS
methods employ effective multi-modal context modeling techniques to achieve
empathy understanding and expression. However, they often need to design
complex network architectures and meticulously optimize the modules within
them. In addition, due to the limitations of small-scale datasets containing
scripted recording styles, they often fail to simulate real natural
conversational styles. To address the above issues, we propose a novel
generative expressive CSS system, termed GPT-Talker.We transform the multimodal
information of the multi-turn dialogue history into discrete token sequences
and seamlessly integrate them to form a comprehensive user-agent dialogue
context. Leveraging the power of GPT, we predict the token sequence, that
includes both semantic and style knowledge, of response for the agent. After
that, the expressive conversational speech is synthesized by the
conversation-enriched VITS to deliver feedback to the user.Furthermore, we
propose a large-scale Natural CSS Dataset called NCSSD, that includes both
naturally recorded conversational speech in improvised styles and dialogues
extracted from TV shows. It encompasses both Chinese and English languages,
with a total duration of 236 hours.We conducted comprehensive experiments on
the reliability of the NCSSD and the effectiveness of our GPT-Talker. Both
subjective and objective evaluations demonstrate that our model outperforms
other state-of-the-art CSS systems significantly in terms of naturalness and
expressiveness. The Code, Dataset, and Pre-trained Model are available at:
https://github.com/AI-S2-Lab/GPT-Talker.","[{'name': 'Rui Liu'}, {'name': 'Yifan Hu'}, {'name': 'Yi Ren'}, {'name': 'Xiang Yin'}, {'name': 'Haizhou Li'}]",2024-07-31T10:02:21Z
http://arxiv.org/abs/2407.21489v1,http://arxiv.org/abs/2407.21489v1,"Maverick: Efficient and Accurate Coreference Resolution Defying Recent
  Trends","Large autoregressive generative models have emerged as the cornerstone for
achieving the highest performance across several Natural Language Processing
tasks. However, the urge to attain superior results has, at times, led to the
premature replacement of carefully designed task-specific approaches without
exhaustive experimentation. The Coreference Resolution task is no exception;
all recent state-of-the-art solutions adopt large generative autoregressive
models that outperform encoder-based discriminative systems. In this work,we
challenge this recent trend by introducing Maverick, a carefully designed - yet
simple - pipeline, which enables running a state-of-the-art Coreference
Resolution system within the constraints of an academic budget, outperforming
models with up to 13 billion parameters with as few as 500 million parameters.
Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,
training with up to 0.006x the memory resources and obtaining a 170x faster
inference compared to previous state-of-the-art systems. We extensively
validate the robustness of the Maverick framework with an array of diverse
experiments, reporting improvements over prior systems in data-scarce,
long-document, and out-of-domain settings. We release our code and models for
research purposes at https://github.com/SapienzaNLP/maverick-coref.","[{'name': 'Giuliano Martinelli'}, {'name': 'Edoardo Barba'}, {'name': 'Roberto Navigli'}]",2024-07-31T09:58:48Z
http://arxiv.org/abs/2407.21476v1,http://arxiv.org/abs/2407.21476v1,"On the Problem of Text-To-Speech Model Selection for Synthetic Data
  Generation in Automatic Speech Recognition","The rapid development of neural text-to-speech (TTS) systems enabled its
usage in other areas of natural language processing such as automatic speech
recognition (ASR) or spoken language translation (SLT). Due to the large number
of different TTS architectures and their extensions, selecting which TTS
systems to use for synthetic data creation is not an easy task. We use the
comparison of five different TTS decoder architectures in the scope of
synthetic data generation to show the impact on CTC-based speech recognition
training. We compare the recognition results to computable metrics like NISQA
MOS and intelligibility, finding that there are no clear relations to the ASR
performance. We also observe that for data generation auto-regressive decoding
performs better than non-autoregressive decoding, and propose an approach to
quantify TTS generalization capabilities.","[{'name': 'Nick Rossenbach'}, {'name': 'Ralf Schlüter'}, {'name': 'Sakriani Sakti'}]",2024-07-31T09:37:27Z
http://arxiv.org/abs/2407.21452v1,http://arxiv.org/abs/2407.21452v1,"Navigating Beyond Instructions: Vision-and-Language Navigation in
  Obstructed Environments","Real-world navigation often involves dealing with unexpected obstructions
such as closed doors, moved objects, and unpredictable entities. However,
mainstream Vision-and-Language Navigation (VLN) tasks typically assume
instructions perfectly align with the fixed and predefined navigation graphs
without any obstructions. This assumption overlooks potential discrepancies in
actual navigation graphs and given instructions, which can cause major failures
for both indoor and outdoor agents. To address this issue, we integrate diverse
obstructions into the R2R dataset by modifying both the navigation graphs and
visual observations, introducing an innovative dataset and task, R2R with
UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers
of path obstructions to generate instruction-reality mismatches for VLN
research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods
inevitably encounter significant challenges when facing such mismatches,
indicating that they rigidly follow instructions rather than navigate
adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),
which includes a curriculum training strategy and virtual graph construction to
help agents effectively adapt to obstructed environments. Empirical results
show that ObVLN not only maintains robust performance in unobstructed scenarios
but also achieves a substantial performance advantage with unexpected
obstructions.","[{'name': 'Haodong Hong'}, {'name': 'Sen Wang'}, {'name': 'Zi Huang'}, {'name': 'Qi Wu'}, {'name': 'Jiajun Liu'}]",2024-07-31T08:55:57Z
http://arxiv.org/abs/2407.21443v1,http://arxiv.org/abs/2407.21443v1,"Improving Faithfulness of Large Language Models in Summarization via
  Sliding Generation and Self-Consistency","Despite large language models (LLMs) have demonstrated impressive performance
in various tasks, they are still suffering from the factual inconsistency
problem called hallucinations. For instance, LLMs occasionally generate content
that diverges from source article, and prefer to extract information that
appears at the beginning and end of the context, especially in long document
summarization. Inspired by these findings, we propose to improve the
faithfulness of LLMs in summarization by impelling them to process the entire
article more fairly and faithfully. We present a novel summary generation
strategy, namely SliSum, which exploits the ideas of sliding windows and
self-consistency. Specifically, SliSum divides the source article into
overlapping windows, and utilizes LLM to generate local summaries for the
content in the windows. Finally, SliSum aggregates all local summaries using
clustering and majority voting algorithm to produce more faithful summary of
entire article. Extensive experiments demonstrate that SliSum significantly
improves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and
GPT-3.5 in both short and long text summarization, while maintaining their
fluency and informativeness and without additional fine-tuning and resources.
We further conduct qualitative and quantitative studies to investigate why
SliSum works and impacts of hyperparameters in SliSum on performance.","[{'name': 'Taiji Li'}, {'name': 'Zhi Li'}, {'name': 'Yin Zhang'}]",2024-07-31T08:48:48Z
http://arxiv.org/abs/2407.21441v2,http://arxiv.org/abs/2407.21441v2,"QuestGen: Effectiveness of Question Generation Methods for Fact-Checking
  Applications","Verifying fact-checking claims poses a significant challenge, even for
humans. Recent approaches have demonstrated that decomposing claims into
relevant questions to gather evidence enhances the efficiency of the
fact-checking process. In this paper, we provide empirical evidence showing
that this question decomposition can be effectively automated. We demonstrate
that smaller generative models, fine-tuned for the question generation task
using data augmentation from various datasets, outperform large language models
by up to 8%. Surprisingly, in some cases, the evidence retrieved using
machine-generated questions proves to be significantly more effective for
fact-checking than that obtained from human-written questions. We also perform
manual evaluation of the decomposed questions to assess the quality of the
questions generated.","[{'name': 'Ritvik Setty'}, {'name': 'Vinay Setty'}]",2024-07-31T08:44:29Z
http://arxiv.org/abs/2407.21439v1,http://arxiv.org/abs/2407.21439v1,"MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented
  Generation via Knowledge-enhanced Reranking and Noise-injected Training","Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities, including text, images, audio, and video. However, a significant
drawback of MLLMs is their reliance on static training data, leading to
outdated information and limited contextual awareness. This static nature
hampers their ability to provide accurate, up-to-date responses, particularly
in dynamic or rapidly evolving contexts. Integrating Multimodal
Retrieval-augmented Generation (Multimodal RAG) offers a promising solution,
but the system would inevitably encounter the multi-granularity noisy
correspondence (MNC) problem, which involves two types of noise: coarse-grained
(query-caption) and fine-grained (query-image). This noise hinders accurate
retrieval and generation. In this work, we propose \textbf{RagLLaVA}, a novel
framework with knowledge-enhanced reranking and noise-injected training, to
address these limitations. We instruction-tune the MLLM with a simple yet
effective instruction template to induce its ranking ability and serve it as a
reranker to precisely filter the top-k retrieved images. For generation, we
inject visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments are conducted on the subsets of
two datasets that require retrieving and reasoning over images to answer a
given query. Our results demonstrate the superiority of RagLLaVA in retrieving
accurately and generating robustly. Code and models are available at
https://github.com/IDEA-FinAI/RagLLaVA.","[{'name': 'Zhanpeng Chen'}, {'name': 'Chengjin Xu'}, {'name': 'Yiyan Qi'}, {'name': 'Jian Guo'}]",2024-07-31T08:43:17Z
http://arxiv.org/abs/2407.21424v2,http://arxiv.org/abs/2407.21424v2,Cost-Effective Hallucination Detection for LLMs,"Large language models (LLMs) can be prone to hallucinations - generating
unreliable outputs that are unfaithful to their inputs, external facts or
internally inconsistent. In this work, we address several challenges for
post-hoc hallucination detection in production settings. Our pipeline for
hallucination detection entails: first, producing a confidence score
representing the likelihood that a generated answer is a hallucination; second,
calibrating the score conditional on attributes of the inputs and candidate
response; finally, performing detection by thresholding the calibrated score.
We benchmark a variety of state-of-the-art scoring methods on different
datasets, encompassing question answering, fact checking, and summarization
tasks. We employ diverse LLMs to ensure a comprehensive assessment of
performance. We show that calibrating individual scoring methods is critical
for ensuring risk-aware downstream decision making. Based on findings that no
individual score performs best in all situations, we propose a multi-scoring
framework, which combines different scores and achieves top performance across
all datasets. We further introduce cost-effective multi-scoring, which can
match or even outperform more expensive detection methods, while significantly
reducing computational overhead.","[{'name': 'Simon Valentin'}, {'name': 'Jinmiao Fu'}, {'name': 'Gianluca Detommaso'}, {'name': 'Shaoyuan Xu'}, {'name': 'Giovanni Zappella'}, {'name': 'Bryan Wang'}]",2024-07-31T08:19:06Z
http://arxiv.org/abs/2407.21417v1,http://arxiv.org/abs/2407.21417v1,"Dancing in Chains: Reconciling Instruction Following and Faithfulness in
  Language Models","Modern language models (LMs) need to follow human instructions while being
faithful; yet, they often fail to achieve both. Here, we provide concrete
evidence of a trade-off between instruction following (i.e., follow open-ended
instructions) and faithfulness (i.e., ground responses in given context) when
training LMs with these objectives. For instance, fine-tuning LLaMA-7B on
instruction following datasets renders it less faithful. Conversely,
instruction-tuned Vicuna-7B shows degraded performance at following
instructions when further optimized on tasks that require contextual grounding.
One common remedy is multi-task learning (MTL) with data mixing, yet it remains
far from achieving a synergic outcome. We propose a simple yet effective method
that relies on Rejection Sampling for Continued Self-instruction Tuning
(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find
that less is more, as training ReSet with high-quality, yet substantially
smaller data (three-fold less) yields superior results. Our findings offer a
better understanding of objective discrepancies in alignment training of LMs.","[{'name': 'Zhengxuan Wu'}, {'name': 'Yuhao Zhang'}, {'name': 'Peng Qi'}, {'name': 'Yumo Xu'}, {'name': 'Rujun Han'}, {'name': 'Yian Zhang'}, {'name': 'Jifan Chen'}, {'name': 'Bonan Min'}, {'name': 'Zhiheng Huang'}]",2024-07-31T08:05:04Z
http://arxiv.org/abs/2407.21414v1,http://arxiv.org/abs/2407.21414v1,"Towards interfacing large language models with ASR systems using
  confidence measures and prompting","As large language models (LLMs) grow in parameter size and capabilities, such
as interaction through prompting, they open up new ways of interfacing with
automatic speech recognition (ASR) systems beyond rescoring n-best lists. This
work investigates post-hoc correction of ASR transcripts with LLMs. To avoid
introducing errors into likely accurate transcripts, we propose a range of
confidence-based filtering methods. Our results indicate that this can improve
the performance of less competitive ASR systems.","[{'name': 'Maryam Naderi'}, {'name': 'Enno Hermann'}, {'name': 'Alexandre Nanchen'}, {'name': 'Sevada Hovsepyan'}, {'name': 'Mathew Magimai. -Doss'}]",2024-07-31T08:00:41Z
http://arxiv.org/abs/2407.21384v1,http://arxiv.org/abs/2407.21384v1,"GEGA: Graph Convolutional Networks and Evidence Retrieval Guided
  Attention for Enhanced Document-level Relation Extraction","Document-level relation extraction (DocRE) aims to extract relations between
entities from unstructured document text. Compared to sentence-level relation
extraction, it requires more complex semantic understanding from a broader text
context. Currently, some studies are utilizing logical rules within evidence
sentences to enhance the performance of DocRE. However, in the data without
provided evidence sentences, researchers often obtain a list of evidence
sentences for the entire document through evidence retrieval (ER). Therefore,
DocRE suffers from two challenges: firstly, the relevance between evidence and
entity pairs is weak; secondly, there is insufficient extraction of complex
cross-relations between long-distance multi-entities. To overcome these
challenges, we propose GEGA, a novel model for DocRE. The model leverages graph
neural networks to construct multiple weight matrices, guiding attention
allocation to evidence sentences. It also employs multi-scale representation
aggregation to enhance ER. Subsequently, we integrate the most efficient
evidence information to implement both fully supervised and weakly supervised
training processes for the model. We evaluate the GEGA model on three widely
used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The
experimental results indicate that our model has achieved comprehensive
improvements compared to the existing SOTA model.","[{'name': 'Yanxu Mao'}, {'name': 'Peipei Liu'}, {'name': 'Tiehan Cui'}]",2024-07-31T07:15:33Z
http://arxiv.org/abs/2407.21368v1,http://arxiv.org/abs/2407.21368v1,"Prompting Medical Large Vision-Language Models to Diagnose Pathologies
  by Visual Question Answering","Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.","[{'name': 'Danfeng Guo'}, {'name': 'Demetri Terzopoulos'}]",2024-07-31T06:34:38Z
http://arxiv.org/abs/2407.21330v1,http://arxiv.org/abs/2407.21330v1,Performance of Recent Large Language Models for a Low-Resourced Language,"Large Language Models (LLMs) have shown significant advances in the past
year. In addition to new versions of GPT and Llama, several other LLMs have
been introduced recently. Some of these are open models available for download
and modification.
  Although multilingual large language models have been available for some
time, their performance on low-resourced languages such as Sinhala has been
poor. We evaluated four recent LLMs on their performance directly in the
Sinhala language, and by translation to and from English. We also evaluated
their fine-tunability with a small amount of fine-tuning data. Claude and GPT
4o perform well out-of-the-box and do significantly better than previous
versions. Llama and Mistral perform poorly but show some promise of improvement
with fine tuning.","[{'name': 'Ravindu Jayakody'}, {'name': 'Gihan Dias'}]",2024-07-31T04:38:07Z
http://arxiv.org/abs/2407.21315v2,http://arxiv.org/abs/2407.21315v2,"Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal
  Nuances","This paper introduces a novel approach to emotion detection in speech using
Large Language Models (LLMs). We address the limitation of LLMs in processing
audio inputs by translating speech characteristics into natural language
descriptions. Our method integrates these descriptions into text prompts,
enabling LLMs to perform multimodal emotion analysis without architectural
modifications. We evaluate our approach on two datasets: IEMOCAP and MELD,
demonstrating significant improvements in emotion recognition accuracy,
particularly for high-quality audio data. Our experiments show that
incorporating speech descriptions yields a 2 percentage point increase in
weighted F1 score on IEMOCAP (from 70.111\% to 72.596\%). We also compare
various LLM architectures and explore the effectiveness of different feature
representations. Our findings highlight the potential of this approach in
enhancing emotion detection capabilities of LLMs and underscore the importance
of audio quality in speech-based emotion recognition tasks. We'll release the
source code on Github.","[{'name': 'Zehui Wu'}, {'name': 'Ziwei Gong'}, {'name': 'Lin Ai'}, {'name': 'Pengyuan Shi'}, {'name': 'Kaan Donbekci'}, {'name': 'Julia Hirschberg'}]",2024-07-31T03:53:14Z
http://arxiv.org/abs/2407.21276v2,http://arxiv.org/abs/2407.21276v2,Multi-Level Querying using A Knowledge Pyramid,"This paper addresses the need for improved precision in existing
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395\% F1 gain by improving its performance from 0.1636 to 0.8109.","[{'name': 'Rubing Chen'}, {'name': 'Xulu Zhang'}, {'name': 'Jiaxin Wu'}, {'name': 'Wenqi Fan'}, {'name': 'Xiao-Yong Wei'}, {'name': 'Qing Li'}]",2024-07-31T01:51:24Z
http://arxiv.org/abs/2407.21264v2,http://arxiv.org/abs/2407.21264v2,"Model Attribution in LLM-Generated Disinformation: A Domain
  Generalization Approach with Supervised Contrastive Learning","Model attribution for LLM-generated disinformation poses a significant
challenge in understanding its origins and mitigating its spread. This task is
especially challenging because modern large language models (LLMs) produce
disinformation with human-like quality. Additionally, the diversity in
prompting methods used to generate disinformation complicates accurate source
attribution. These methods introduce domain-specific features that can mask the
fundamental characteristics of the models. In this paper, we introduce the
concept of model attribution as a domain generalization problem, where each
prompting method represents a unique domain. We argue that an effective
attribution model must be invariant to these domain-specific features. It
should also be proficient in identifying the originating models across all
scenarios, reflecting real-world detection challenges. To address this, we
introduce a novel approach based on Supervised Contrastive Learning. This
method is designed to enhance the model's robustness to variations in prompts
and focuses on distinguishing between different source LLMs. We evaluate our
model through rigorous experiments involving three common prompting methods:
``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:
``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the
effectiveness of our approach in model attribution tasks, achieving
state-of-the-art performance across diverse and unseen datasets.","[{'name': 'Alimohammad Beigi'}, {'name': 'Zhen Tan'}, {'name': 'Nivedh Mudiam'}, {'name': 'Canyu Chen'}, {'name': 'Kai Shu'}, {'name': 'Huan Liu'}]",2024-07-31T00:56:09Z
http://arxiv.org/abs/2407.21248v1,http://arxiv.org/abs/2407.21248v1,"Adaptive Pre-training Data Detection for Large Language Models via
  Surprising Tokens","While large language models (LLMs) are extensively used, there are raising
concerns regarding privacy, security, and copyright due to their opaque
training data, which brings the problem of detecting pre-training data on the
table. Current solutions to this problem leverage techniques explored in
machine learning privacy such as Membership Inference Attacks (MIAs), which
heavily depend on LLMs' capability of verbatim memorization. However, this
reliance presents challenges, especially given the vast amount of training data
and the restricted number of effective training epochs. In this paper, we
propose an adaptive pre-training data detection method which alleviates this
reliance and effectively amplify the identification. Our method adaptively
locates \textit{surprising tokens} of the input. A token is surprising to a LLM
if the prediction on the token is ""certain but wrong"", which refers to low
Shannon entropy of the probability distribution and low probability of the
ground truth token at the same time. By using the prediction probability of
surprising tokens to measure \textit{surprising}, the detection method is
achieved based on the simple hypothesis that seeing seen data is less
surprising for the model compared with seeing unseen data. The method can be
applied without any access to the the pre-training data corpus or additional
training like reference models. Our approach exhibits a consistent enhancement
compared to existing methods in diverse experiments conducted on various
benchmarks and models, achieving a maximum improvement of 29.5\%. We also
introduce a new benchmark Dolma-Book developed upon a novel framework, which
employs book data collected both before and after model training to provide
further evaluation.","[{'name': 'Anqi Zhang'}, {'name': 'Chaofeng Wu'}]",2024-07-30T23:43:59Z
http://arxiv.org/abs/2407.21229v1,http://arxiv.org/abs/2407.21229v1,"Advancing Vietnamese Visual Question Answering with Transformer and
  Convolutional Integration","Visual Question Answering (VQA) has recently emerged as a potential research
domain, captivating the interest of many in the field of artificial
intelligence and computer vision. Despite the prevalence of approaches in
English, there is a notable lack of systems specifically developed for certain
languages, particularly Vietnamese. This study aims to bridge this gap by
conducting comprehensive experiments on the Vietnamese Visual Question
Answering (ViVQA) dataset, demonstrating the effectiveness of our proposed
model. In response to community interest, we have developed a model that
enhances image representation capabilities, thereby improving overall
performance in the ViVQA system. Specifically, our model integrates the
Bootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2)
and the convolutional neural network EfficientNet to extract and process both
local and global features from images. This integration leverages the strengths
of transformer-based architectures for capturing comprehensive contextual
information and convolutional networks for detailed local features. By freezing
the parameters of these pre-trained models, we significantly reduce the
computational cost and training time, while maintaining high performance. This
approach significantly improves image representation and enhances the
performance of existing VQA systems. We then leverage a multi-modal fusion
module based on a general-purpose multi-modal foundation model (BEiT-3) to fuse
the information between visual and textual features. Our experimental findings
demonstrate that our model surpasses competing baselines, achieving promising
performance. This is particularly evident in its accuracy of $71.04\%$ on the
test set of the ViVQA dataset, marking a significant advancement in our
research area. The code is available at https://github.com/nngocson2002/ViVQA.","[{'name': 'Ngoc Son Nguyen'}, {'name': 'Van Son Nguyen'}, {'name': 'Tung Le'}]",2024-07-30T22:32:50Z
http://arxiv.org/abs/2407.21191v1,http://arxiv.org/abs/2407.21191v1,GenRec: Generative Personalized Sequential Recommendation,"Sequential recommendation is a task to capture hidden user preferences from
historical user item interaction data. Significant progress has been made in
this domain by leveraging classification based learning methods. Inspired by
the recent paradigm of 'pretrain, prompt and predict' in NLP, we consider
sequential recommendation as a sequence to sequence generation task and propose
a novel model named Generative Recommendation (GenRec). Unlike classification
based models that learn explicit user and item representations, GenRec utilizes
the sequence modeling capability of Transformer and adopts the masked item
prediction objective to effectively learn the hidden bidirectional sequential
patterns. Different from existing generative sequential recommendation models,
GenRec does not rely on manually designed hard prompts. The input to GenRec is
textual user item sequence and the output is top ranked next items. Moreover,
GenRec is lightweight and requires only a few hours to train effectively in
low-resource settings, making it highly applicable to real-world scenarios and
helping to democratize large language models in the sequential recommendation
domain. Our extensive experiments have demonstrated that GenRec generalizes on
various public real-world datasets and achieves state-of-the-art results. Our
experiments also validate the effectiveness of the the proposed masked item
prediction objective that improves the model performance by a large margin.","[{'name': 'Panfeng Cao'}, {'name': 'Pietro Lio'}]",2024-07-30T20:58:36Z
http://arxiv.org/abs/2407.21170v1,http://arxiv.org/abs/2407.21170v1,Decomposed Prompting to Answer Questions on a Course Discussion Board,"We propose and evaluate a question-answering system that uses decomposed
prompting to classify and answer student questions on a course discussion
board. Our system uses a large language model (LLM) to classify questions into
one of four types: conceptual, homework, logistics, and not answerable. This
enables us to employ a different strategy for answering questions that fall
under different types. Using a variant of GPT-3, we achieve $81\%$
classification accuracy. We discuss our system's performance on answering
conceptual questions from a machine learning course and various failure modes.","[{'name': 'Brandon Jaipersaud'}, {'name': 'Paul Zhang'}, {'name': 'Jimmy Ba'}, {'name': 'Andrew Petersen'}, {'name': 'Lisa Zhang'}, {'name': 'Michael R. Zhang'}]",2024-07-30T20:24:44Z
http://arxiv.org/abs/2407.21153v1,http://arxiv.org/abs/2407.21153v1,Event-Arguments Extraction Corpus and Modeling using BERT for Arabic,"Event-argument extraction is a challenging task, particularly in Arabic due
to sparse linguistic resources. To fill this gap, we introduce the \hadath
corpus ($550$k tokens) as an extension of Wojood, enriched with event-argument
annotations. We used three types of event arguments: $agent$, $location$, and
$date$, which we annotated as relation types. Our inter-annotator agreement
evaluation resulted in $82.23\%$ $Kappa$ score and $87.2\%$ $F_1$-score.
Additionally, we propose a novel method for event relation extraction using
BERT, in which we treat the task as text entailment. This method achieves an
$F_1$-score of $94.01\%$. To further evaluate the generalization of our
proposed method, we collected and annotated another out-of-domain corpus (about
$80$k tokens) called \testNLI and used it as a second test set, on which our
approach achieved promising results ($83.59\%$ $F_1$-score). Last but not
least, we propose an end-to-end system for event-arguments extraction. This
system is implemented as part of SinaTools, and both corpora are publicly
available at {\small \url{https://sina.birzeit.edu/wojood}}","[{'name': 'Alaa Aljabari'}, {'name': 'Lina Duaibes'}, {'name': 'Mustafa Jarrar'}, {'name': 'Mohammed Khalilia'}]",2024-07-30T19:32:22Z
http://arxiv.org/abs/2407.21139v2,http://arxiv.org/abs/2407.21139v2,"Enhancing Semantic Similarity Understanding in Arabic NLP with Nested
  Embedding Learning","This work presents a novel framework for training Arabic nested embedding
models through Matryoshka Embedding Learning, leveraging multilingual,
Arabic-specific, and English-based models, to highlight the power of nested
embeddings models in various Arabic NLP downstream tasks. Our innovative
contribution includes the translation of various sentence similarity datasets
into Arabic, enabling a comprehensive evaluation framework to compare these
models across different dimensions. We trained several nested embedding models
on the Arabic Natural Language Inference triplet dataset and assessed their
performance using multiple evaluation metrics, including Pearson and Spearman
correlations for cosine similarity, Manhattan distance, Euclidean distance, and
dot product similarity. The results demonstrate the superior performance of the
Matryoshka embedding models, particularly in capturing semantic nuances unique
to the Arabic language. Results demonstrated that Arabic Matryoshka embedding
models have superior performance in capturing semantic nuances unique to the
Arabic language, significantly outperforming traditional models by up to
20-25\% across various similarity metrics. These results underscore the
effectiveness of language-specific training and highlight the potential of
Matryoshka models in enhancing semantic textual similarity tasks for Arabic
NLP.","[{'name': 'Omer Nacar'}, {'name': 'Anis Koubaa'}]",2024-07-30T19:03:03Z
http://arxiv.org/abs/2408.04640v1,http://arxiv.org/abs/2408.04640v1,LLMs for Enhanced Agricultural Meteorological Recommendations,"Agricultural meteorological recommendations are crucial for enhancing crop
productivity and sustainability by providing farmers with actionable insights
based on weather forecasts, soil conditions, and crop-specific data. This paper
presents a novel approach that leverages large language models (LLMs) and
prompt engineering to improve the accuracy and relevance of these
recommendations. We designed a multi-round prompt framework to iteratively
refine recommendations using updated data and feedback, implemented on ChatGPT,
Claude2, and GPT-4. Our method was evaluated against baseline models and a
Chain-of-Thought (CoT) approach using manually collected datasets. The results
demonstrate significant improvements in accuracy and contextual relevance, with
our approach achieving up to 90\% accuracy and high GPT-4 scores. Additional
validation through real-world pilot studies further confirmed the practical
benefits of our method, highlighting its potential to transform agricultural
practices and decision-making.","[{'name': 'Ji-jun Park'}, {'name': 'Soo-joon Choi'}]",2024-07-30T18:10:49Z
http://arxiv.org/abs/2407.21018v1,http://arxiv.org/abs/2407.21018v1,ThinK: Thinner Key Cache by Query-Driven Pruning,"Large Language Models (LLMs) have revolutionized the field of natural
language processing, achieving unprecedented performance across a variety of
applications by leveraging increased model sizes and sequence lengths. However,
the associated rise in computational and memory costs poses significant
challenges, particularly in managing long sequences due to the quadratic
complexity of the transformer attention mechanism. This paper focuses on the
long-context scenario, addressing the inefficiencies in KV cache memory
consumption during inference. Unlike existing approaches that optimize the
memory based on the sequence lengths, we uncover that the channel dimension of
the KV cache exhibits significant redundancy, characterized by unbalanced
magnitude distribution and low-rank structure in attention weights. Based on
these observations, we propose ThinK, a novel query-dependent KV cache pruning
method designed to minimize attention weight loss while selectively pruning the
least significant channels. Our approach not only maintains or enhances model
accuracy but also achieves a reduction in memory costs by over 20% compared
with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and
Mistral models across various long-sequence datasets confirm the efficacy of
ThinK, setting a new precedent for efficient LLM deployment without
compromising performance. We also outline the potential of extending our method
to value cache pruning, demonstrating ThinK's versatility and broad
applicability in reducing both memory and computational overheads.","[{'name': 'Yuhui Xu'}, {'name': 'Zhanming Jie'}, {'name': 'Hanze Dong'}, {'name': 'Lei Wang'}, {'name': 'Xudong Lu'}, {'name': 'Aojun Zhou'}, {'name': 'Amrita Saha'}, {'name': 'Caiming Xiong'}, {'name': 'Doyen Sahoo'}]",2024-07-30T17:59:08Z
http://arxiv.org/abs/2407.21004v1,http://arxiv.org/abs/2407.21004v1,"Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models
  for Hateful Meme Detection","Recent advances show that two-stream approaches have achieved outstanding
performance in hateful meme detection. However, hateful memes constantly evolve
as new memes emerge by fusing progressive cultural ideas, making existing
methods obsolete or ineffective. In this work, we explore the potential of
Large Multimodal Models (LMMs) for hateful meme detection. To this end, we
propose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE)
Prompting, by integrating the evolution attribute and in-context information of
memes. Specifically, Evolver simulates the evolving and expressing process of
memes and reasons through LMMs in a step-by-step manner. First, an evolutionary
pair mining module retrieves the top-k most similar memes in the external
curated meme set with the input meme. Second, an evolutionary information
extractor is designed to summarize the semantic regularities between the paired
memes for prompting. Finally, a contextual relevance amplifier enhances the
in-context hatefulness information to boost the search for evolutionary
processes. Extensive experiments on public FHM, MAMI, and HarM datasets show
that CoE prompting can be incorporated into existing LMMs to improve their
performance. More encouragingly, it can serve as an interpretive tool to
promote the understanding of the evolution of social memes.","[{'name': 'Jinfa Huang'}, {'name': 'Jinsheng Pan'}, {'name': 'Zhongwei Wan'}, {'name': 'Hanjia Lyu'}, {'name': 'Jiebo Luo'}]",2024-07-30T17:51:44Z
http://arxiv.org/abs/2407.20990v1,http://arxiv.org/abs/2407.20990v1,"From Feature Importance to Natural Language Explanations Using LLMs with
  RAG","As machine learning becomes increasingly integral to autonomous
decision-making processes involving human interaction, the necessity of
comprehending the model's outputs through conversational means increases. Most
recently, foundation models are being explored for their potential as post hoc
explainers, providing a pathway to elucidate the decision-making mechanisms of
predictive models. In this work, we introduce traceable question-answering,
leveraging an external knowledge repository to inform the responses of Large
Language Models (LLMs) to user queries within a scene understanding task. This
knowledge repository comprises contextual details regarding the model's output,
containing high-level features, feature importance, and alternative
probabilities. We employ subtractive counterfactual reasoning to compute
feature importance, a method that entails analysing output variations resulting
from decomposing semantic features. Furthermore, to maintain a seamless
conversational flow, we integrate four key characteristics - social, causal,
selective, and contrastive - drawn from social science research on human
explanations into a single-shot prompt, guiding the response generation
process. Our evaluation demonstrates that explanations generated by the LLMs
encompassed these elements, indicating its potential to bridge the gap between
complex model outputs and natural language expressions.","[{'name': 'Sule Tekkesinoglu'}, {'name': 'Lars Kunze'}]",2024-07-30T17:27:20Z
http://arxiv.org/abs/2407.21092v1,http://arxiv.org/abs/2407.21092v1,"Entropy, Thermodynamics and the Geometrization of the Language Model","In this paper, we discuss how pure mathematics and theoretical physics can be
applied to the study of language models. Using set theory and analysis, we
formulate mathematically rigorous definitions of language models, and introduce
the concept of the moduli space of distributions for a language model. We
formulate a generalized distributional hypothesis using functional analysis and
topology. We define the entropy function associated with a language model and
show how it allows us to understand many interesting phenomena in languages. We
argue that the zero points of the entropy function and the points where the
entropy is close to 0 are the key obstacles for an LLM to approximate an
intelligent language model, which explains why good LLMs need billions of
parameters. Using the entropy function, we formulate a conjecture about AGI.
  Then, we show how thermodynamics gives us an immediate interpretation to
language models. In particular we will define the concepts of partition
function, internal energy and free energy for a language model, which offer
insights into how language models work. Based on these results, we introduce a
general concept of the geometrization of language models and define what is
called the Boltzmann manifold. While the current LLMs are the special cases of
the Boltzmann manifold.",[{'name': 'Wenzhe Yang'}],2024-07-30T17:11:15Z
http://arxiv.org/abs/2408.04639v1,http://arxiv.org/abs/2408.04639v1,Abstractive summarization from Audio Transcription,"Currently, large language models are gaining popularity, their achievements
are used in many areas, ranging from text translation to generating answers to
queries. However, the main problem with these new machine learning algorithms
is that training such models requires large computing resources that only large
IT companies have. To avoid this problem, a number of methods (LoRA,
quantization) have been proposed so that existing models can be effectively
fine-tuned for specific tasks. In this paper, we propose an E2E (end to end)
audio summarization model using these techniques. In addition, this paper
examines the effectiveness of these approaches to the problem under
consideration and draws conclusions about the applicability of these methods.",[{'name': 'Ilia Derkach'}],2024-07-30T16:38:38Z
http://arxiv.org/abs/2407.20910v1,http://arxiv.org/abs/2407.20910v1,"Enabling Contextual Soft Moderation on Social Media through Contrastive
  Textual Deviation","Automated soft moderation systems are unable to ascertain if a post supports
or refutes a false claim, resulting in a large number of contextual false
positives. This limits their effectiveness, for example undermining trust in
health experts by adding warnings to their posts or resorting to vague warnings
instead of granular fact-checks, which result in desensitizing users. In this
paper, we propose to incorporate stance detection into existing automated
soft-moderation pipelines, with the goal of ruling out contextual false
positives and providing more precise recommendations for social media content
that should receive warnings. We develop a textual deviation task called
Contrastive Textual Deviation (CTD) and show that it outperforms existing
stance detection approaches when applied to soft moderation.We then integrate
CTD into the stateof-the-art system for automated soft moderation Lambretta,
showing that our approach can reduce contextual false positives from 20% to
2.1%, providing another important building block towards deploying reliable
automated soft moderation tools on social media.","[{'name': 'Pujan Paudel'}, {'name': 'Mohammad Hammas Saeed'}, {'name': 'Rebecca Auger'}, {'name': 'Chris Wells'}, {'name': 'Gianluca Stringhini'}]",2024-07-30T15:37:05Z
http://arxiv.org/abs/2407.20906v1,http://arxiv.org/abs/2407.20906v1,Automated Review Generation Method Based on Large Language Models,"Literature research, vital for scientific advancement, is overwhelmed by the
vast ocean of available information. Addressing this, we propose an automated
review generation method based on Large Language Models (LLMs) to streamline
literature processing and reduce cognitive load. In case study on propane
dehydrogenation (PDH) catalysts, our method swiftly generated comprehensive
reviews from 343 articles, averaging seconds per article per LLM account.
Extended analysis of 1041 articles provided deep insights into catalysts'
composition, structure, and performance. Recognizing LLMs' hallucinations, we
employed a multi-layered quality control strategy, ensuring our method's
reliability and effective hallucination mitigation. Expert verification
confirms the accuracy and citation integrity of generated reviews,
demonstrating LLM hallucination risks reduced to below 0.5% with over 95%
confidence. Released Windows application enables one-click review generation,
aiding researchers in tracking advancements and recommending literature. This
approach showcases LLMs' role in enhancing scientific research productivity and
sets the stage for further exploration.","[{'name': 'Shican Wu'}, {'name': 'Xiao Ma'}, {'name': 'Dehui Luo'}, {'name': 'Lulu Li'}, {'name': 'Xiangcheng Shi'}, {'name': 'Xin Chang'}, {'name': 'Xiaoyun Lin'}, {'name': 'Ran Luo'}, {'name': 'Chunlei Pei'}, {'name': 'Zhi-Jian Zhao'}, {'name': 'Jinlong Gong'}]",2024-07-30T15:26:36Z
http://arxiv.org/abs/2407.20884v1,http://arxiv.org/abs/2407.20884v1,"Effective Black Box Testing of Sentiment Analysis Classification
  Networks","Transformer-based neural networks have demonstrated remarkable performance in
natural language processing tasks such as sentiment analysis. Nevertheless, the
issue of ensuring the dependability of these complicated architectures through
comprehensive testing is still open. This paper presents a collection of
coverage criteria specifically designed to assess test suites created for
transformer-based sentiment analysis networks. Our approach utilizes input
space partitioning, a black-box method, by considering emotionally relevant
linguistic features such as verbs, adjectives, adverbs, and nouns. In order to
effectively produce test cases that encompass a wide range of emotional
elements, we utilize the k-projection coverage metric. This metric minimizes
the complexity of the problem by examining subsets of k features at the same
time, hence reducing dimensionality. Large language models are employed to
generate sentences that display specific combinations of emotional features.
The findings from experiments obtained from a sentiment analysis dataset
illustrate that our criteria and generated tests have led to an average
increase of 16\% in test coverage. In addition, there is a corresponding
average decrease of 6.5\% in model accuracy, showing the ability to identify
vulnerabilities. Our work provides a foundation for improving the dependability
of transformer-based sentiment analysis systems through comprehensive test
evaluation.","[{'name': 'Parsa Karbasizadeh'}, {'name': 'Fathiyeh Faghih'}, {'name': 'Pouria Golshanrad'}]",2024-07-30T14:58:11Z
http://arxiv.org/abs/2408.06370v1,http://arxiv.org/abs/2408.06370v1,Lyrics Transcription for Humans: A Readability-Aware Benchmark,"Writing down lyrics for human consumption involves not only accurately
capturing word sequences, but also incorporating punctuation and formatting for
clarity and to convey contextual information. This includes song structure,
emotional emphasis, and contrast between lead and background vocals. While
automatic lyrics transcription (ALT) systems have advanced beyond producing
unstructured strings of words and are able to draw on wider context, ALT
benchmarks have not kept pace and continue to focus exclusively on words. To
address this gap, we introduce Jam-ALT, a comprehensive lyrics transcription
benchmark. The benchmark features a complete revision of the JamendoLyrics
dataset, in adherence to industry standards for lyrics transcription and
formatting, along with evaluation metrics designed to capture and assess the
lyric-specific nuances, laying the foundation for improving the readability of
lyrics. We apply the benchmark to recent transcription systems and present
additional error analysis, as well as an experimental comparison with a
classical music dataset.","[{'name': 'Ondřej Cífka'}, {'name': 'Hendrik Schreiber'}, {'name': 'Luke Miner'}, {'name': 'Fabian-Robert Stöter'}]",2024-07-30T14:20:09Z
http://arxiv.org/abs/2407.20756v3,http://arxiv.org/abs/2407.20756v3,"SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision
  Language Models","Recently, with the rise of web images, managing and understanding large-scale
image datasets has become increasingly important. Vision Large Language Models
(VLLMs) have recently emerged due to their robust vision-understanding
capabilities. However, training these models requires vast amounts of data,
posing challenges to efficiency, effectiveness, data quality, and privacy. In
this paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.
Unlike existing methods that generate captions from images, SynthVLM employs
advanced diffusion models and high-quality captions to automatically generate
and select high-resolution images from captions, creating precisely aligned
image-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)
performance on various vision question answering tasks, maintaining high
alignment quality and preserving advanced language abilities. Moreover,
SynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in
performance while significantly reducing computational overhead. Crucially, our
method's reliance on purely generated data ensures the preservation of privacy,
achieving SoTA performance with just 100k data points (only 18% of the official
dataset size).","[{'name': 'Zheng Liu'}, {'name': 'Hao Liang'}, {'name': 'Xijie Huang'}, {'name': 'Wentao Xiong'}, {'name': 'Qinhan Yu'}, {'name': 'Linzhuang Sun'}, {'name': 'Chong Chen'}, {'name': 'Conghui He'}, {'name': 'Bin Cui'}, {'name': 'Wentao Zhang'}]",2024-07-30T11:57:40Z
http://arxiv.org/abs/2407.20750v1,http://arxiv.org/abs/2407.20750v1,"JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create
  State-of-the-Art Japanese Retrievers with Constrained Resources","Neural Information Retrieval has advanced rapidly in high-resource languages,
but progress in lower-resource ones such as Japanese has been hindered by data
scarcity, among other challenges. Consequently, multilingual models have
dominated Japanese retrieval, despite their computational inefficiencies and
inability to capture linguistic nuances. While recent multi-vector monolingual
models like JaColBERT have narrowed this gap, they still lag behind
multilingual methods in large-scale evaluations. This work addresses the
suboptimal training methods of multi-vector retrievers in lower-resource
settings, focusing on Japanese. We systematically evaluate and improve key
aspects of the inference and training settings of JaColBERT, and more broadly,
multi-vector models. We further enhance performance through a novel checkpoint
merging step, showcasing it to be an effective way of combining the benefits of
fine-tuning with the generalization capabilities of the original checkpoint.
Building on our analysis, we introduce a novel training recipe, resulting in
the JaColBERTv2.5 model. JaColBERTv2.5, with only 110 million parameters and
trained in under 15 hours on 4 A100 GPUs, significantly outperforms all
existing methods across all common benchmarks, reaching an average score of
0.754, significantly above the previous best of 0.720. To support future
research, we make our final models, intermediate checkpoints and all data used
publicly available.",[{'name': 'Benjamin Clavié'}],2024-07-30T11:42:19Z
http://arxiv.org/abs/2407.20743v1,http://arxiv.org/abs/2407.20743v1,Meltemi: The first open Large Language Model for Greek,"We describe the development and capabilities of Meltemi 7B, the first open
Large Language Model for the Greek language. Meltemi 7B has 7 billion
parameters and is trained on a 40 billion token Greek corpus. For the
development of Meltemi 7B, we adapt Mistral, by continuous pretraining on the
Greek Corpus. Meltemi 7B contains up-to-date information up to September 2023.
Furthermore, we have translated and curated a Greek instruction corpus, which
has been used for the instruction-tuning of a chat model, named Meltemi 7B
Instruct. Special care has been given to the alignment and the removal of toxic
content for the Meltemi 7B Instruct. The developed models are evaluated on a
broad set of collected evaluation corpora, and examples of prompts and
responses are presented. Both Meltemi 7B and Meltemi 7B Instruct are available
at https://huggingface.co/ilsp under the Apache 2.0 license.","[{'name': 'Leon Voukoutis'}, {'name': 'Dimitris Roussis'}, {'name': 'Georgios Paraskevopoulos'}, {'name': 'Sokratis Sofianopoulos'}, {'name': 'Prokopis Prokopidis'}, {'name': 'Vassilis Papavasileiou'}, {'name': 'Athanasios Katsamanis'}, {'name': 'Stelios Piperidis'}, {'name': 'Vassilis Katsouros'}]",2024-07-30T11:22:52Z
http://arxiv.org/abs/2407.20729v1,http://arxiv.org/abs/2407.20729v1,"Adapting Safe-for-Work Classifier for Malaysian Language Text: Enhancing
  Alignment in LLM-Ops Framework","As large language models (LLMs) become increasingly integrated into
operational workflows (LLM-Ops), there is a pressing need for effective
guardrails to ensure safe and aligned interactions, including the ability to
detect potentially unsafe or inappropriate content across languages. However,
existing safe-for-work classifiers are primarily focused on English text. To
address this gap for the Malaysian language, we present a novel safe-for-work
text classifier tailored specifically for Malaysian language content. By
curating and annotating a first-of-its-kind dataset of Malaysian text spanning
multiple content categories, we trained a classification model capable of
identifying potentially unsafe material using state-of-the-art natural language
processing techniques. This work represents an important step in enabling safer
interactions and content filtering to mitigate potential risks and ensure
responsible deployment of LLMs. To maximize accessibility and promote further
research towards enhancing alignment in LLM-Ops for the Malaysian context, the
model is publicly released at
https://huggingface.co/malaysia-ai/malaysian-sfw-classifier.","[{'name': 'Aisyah Razak'}, {'name': 'Ariff Nazhan'}, {'name': 'Kamarul Adha'}, {'name': 'Wan Adzhar Faiq Adzlan'}, {'name': 'Mas Aisyah Ahmad'}, {'name': 'Ammar Azman'}]",2024-07-30T10:51:51Z
http://arxiv.org/abs/2407.20700v1,http://arxiv.org/abs/2407.20700v1,"Industrial-Grade Smart Troubleshooting through Causal Technical Language
  Processing: a Proof of Concept","This paper describes the development of a causal diagnosis approach for
troubleshooting an industrial environment on the basis of the technical
language expressed in Return on Experience records. The proposed method
leverages the vectorized linguistic knowledge contained in the distributed
representation of a Large Language Model, and the causal associations entailed
by the embedded failure modes and mechanisms of the industrial assets. The
paper presents the elementary but essential concepts of the solution, which is
conceived as a causality-aware retrieval augmented generation system, and
illustrates them experimentally on a real-world Predictive Maintenance setting.
Finally, it discusses avenues of improvement for the maturity of the utilized
causal technology to meet the robustness challenges of increasingly complex
scenarios in the industry.","[{'name': 'Alexandre Trilla'}, {'name': 'Ossee Yiboe'}, {'name': 'Nenad Mijatovic'}, {'name': 'Jordi Vitrià'}]",2024-07-30T09:53:55Z
http://arxiv.org/abs/2407.20685v2,http://arxiv.org/abs/2407.20685v2,"CultureVo: The Serious Game of Utilizing Gen AI for Enhancing Cultural
  Intelligence","CultureVo, Inc. has developed the Integrated Culture Learning Suite (ICLS) to
deliver foundational knowledge of world cultures through a combination of
interactive lessons and gamified experiences. This paper explores how
Generative AI powered by open source Large Langauge Models are utilized within
the ICLS to enhance cultural intelligence. The suite employs Generative AI
techniques to automate the assessment of learner knowledge, analyze behavioral
patterns, and manage interactions with non-player characters using real time
learner assessment. Additionally, ICLS provides contextual hint and recommend
course content by assessing learner proficiency, while Generative AI
facilitates the automated creation and validation of educational content.","[{'name': 'Ajita Agarwala'}, {'name': 'Anupam Purwar'}, {'name': 'Viswanadhasai Rao'}]",2024-07-30T09:26:43Z
http://arxiv.org/abs/2407.20673v1,http://arxiv.org/abs/2407.20673v1,Label-Guided Prompt for Multi-label Few-shot Aspect Category Detection,"Multi-label few-shot aspect category detection aims at identifying multiple
aspect categories from sentences with a limited number of training instances.
The representation of sentences and categories is a key issue in this task.
Most of current methods extract keywords for the sentence representations and
the category representations. Sentences often contain many category-independent
words, which leads to suboptimal performance of keyword-based methods. Instead
of directly extracting keywords, we propose a label-guided prompt method to
represent sentences and categories. To be specific, we design label-specific
prompts to represent sentences by combining crucial contextual and semantic
information. Further, the label is introduced into a prompt to obtain category
descriptions by utilizing a large language model. This kind of category
descriptions contain the characteristics of the aspect categories, guiding the
construction of discriminative category prototypes. Experimental results on two
public datasets show that our method outperforms current state-of-the-art
methods with a 3.86% - 4.75% improvement in the Macro-F1 score.","[{'name': 'ChaoFeng Guan'}, {'name': 'YaoHui Zhu'}, {'name': 'Yu Bai'}, {'name': 'LingYun Wang'}]",2024-07-30T09:11:17Z
http://arxiv.org/abs/2407.20663v1,http://arxiv.org/abs/2407.20663v1,"ArabicNLU 2024: The First Arabic Natural Language Understanding Shared
  Task","This paper presents an overview of the Arabic Natural Language Understanding
(ArabicNLU 2024) shared task, focusing on two subtasks: Word Sense
Disambiguation (WSD) and Location Mention Disambiguation (LMD). The task aimed
to evaluate the ability of automated systems to resolve word ambiguity and
identify locations mentioned in Arabic text. We provided participants with
novel datasets, including a sense-annotated corpus for WSD, called SALMA with
approximately 34k annotated tokens, and the IDRISI-DA dataset with 3,893
annotations and 763 unique location mentions. These are challenging tasks. Out
of the 38 registered teams, only three teams participated in the final
evaluation phase, with the highest accuracy being 77.8% for WSD and the highest
MRR@1 being 95.0% for LMD. The shared task not only facilitated the evaluation
and comparison of different techniques, but also provided valuable insights and
resources for the continued advancement of Arabic NLU technologies.","[{'name': 'Mohammed Khalilia'}, {'name': 'Sanad Malaysha'}, {'name': 'Reem Suwaileh'}, {'name': 'Mustafa Jarrar'}, {'name': 'Alaa Aljabari'}, {'name': 'Tamer Elsayed'}, {'name': 'Imed Zitouni'}]",2024-07-30T08:57:01Z
http://arxiv.org/abs/2407.20657v1,http://arxiv.org/abs/2407.20657v1,Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks,"Recent vision-language foundation models, such as CLIP, have demonstrated
superior capabilities in learning representations that can be transferable
across diverse range of downstream tasks and domains. With the emergence of
such powerful models, it has become crucial to effectively leverage their
capabilities in tackling challenging vision tasks. On the other hand, only a
few works have focused on devising adversarial examples that transfer well to
both unknown domains and model architectures. In this paper, we propose a novel
transfer attack method called PDCL-Attack, which leverages the CLIP model to
enhance the transferability of adversarial perturbations generated by a
generative model-based attack framework. Specifically, we formulate an
effective prompt-driven feature guidance by harnessing the semantic
representation power of text, particularly from the ground-truth class labels
of input images. To the best of our knowledge, we are the first to introduce
prompt learning to enhance the transferable generative attacks. Extensive
experiments conducted across various cross-domain and cross-model settings
empirically validate our approach, demonstrating its superiority over
state-of-the-art methods.","[{'name': 'Hunmin Yang'}, {'name': 'Jongoh Jeong'}, {'name': 'Kuk-Jin Yoon'}]",2024-07-30T08:52:16Z
http://arxiv.org/abs/2407.20654v1,http://arxiv.org/abs/2407.20654v1,"Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain
  Study in Italian","Addressing the challenge of limited annotated data in specialized fields and
low-resource languages is crucial for the effective use of Language Models
(LMs). While most Large Language Models (LLMs) are trained on general-purpose
English corpora, there is a notable gap in models specifically tailored for
Italian, particularly for technical and bureaucratic jargon. This paper
explores the feasibility of employing smaller, domain-specific encoder LMs
alongside prompting techniques to enhance performance in these specialized
contexts. Our study concentrates on the Italian bureaucratic and legal
language, experimenting with both general-purpose and further pre-trained
encoder-only models. We evaluated the models on downstream tasks such as
document classification and entity typing and conducted intrinsic evaluations
using Pseudo-Log-Likelihood. The results indicate that while further
pre-trained models may show diminished robustness in general knowledge, they
exhibit superior adaptability for domain-specific tasks, even in a zero-shot
setting. Furthermore, the application of calibration techniques and in-domain
verbalizers significantly enhances the efficacy of encoder models. These
domain-specialized models prove to be particularly advantageous in scenarios
where in-domain resources or expertise are scarce. In conclusion, our findings
offer new insights into the use of Italian models in specialized contexts,
which may have a significant impact on both research and industrial
applications in the digital transformation era.","[{'name': 'Serena Auriemma'}, {'name': 'Martina Miliani'}, {'name': 'Mauro Madeddu'}, {'name': 'Alessandro Bondielli'}, {'name': 'Lucia Passaro'}, {'name': 'Alessandro Lenci'}]",2024-07-30T08:50:16Z
http://arxiv.org/abs/2408.04638v1,http://arxiv.org/abs/2408.04638v1,"Affective Computing in the Era of Large Language Models: A Survey from
  the NLP Perspective","Affective Computing (AC), integrating computer science, psychology, and
cognitive science knowledge, aims to enable machines to recognize, interpret,
and simulate human emotions.To create more value, AC can be applied to diverse
scenarios, including social media, finance, healthcare, education, etc.
Affective Computing (AC) includes two mainstream tasks, i.e., Affective
Understanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained
Language Models (PLMs) for AU tasks has succeeded considerably. However, these
models lack generalization ability, requiring specialized models for specific
tasks. Additionally, traditional PLMs face challenges in AG, particularly in
generating diverse and emotionally rich responses. The emergence of Large
Language Models (LLMs), such as the ChatGPT series and LLaMA models, brings new
opportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess
capabilities of in-context learning, common sense reasoning, and advanced
sequence generation, which present unprecedented opportunities for AU. To
provide a comprehensive overview of AC in the LLMs era from an NLP perspective,
we summarize the development of LLMs research in this field, aiming to offer
new insights. Specifically, we first summarize the traditional tasks related to
AC and introduce the preliminary study based on LLMs. Subsequently, we outline
the relevant techniques of popular LLMs to improve AC tasks, including
Instruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss
full parameter fine-tuning and parameter-efficient methods such as LoRA,
P-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,
Few-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To
clearly understand the performance of LLMs on different Affective Computing
tasks, we further summarize the existing benchmarks and evaluation methods.","[{'name': 'Yiqun Zhang'}, {'name': 'Xiaocui Yang'}, {'name': 'Xingle Xu'}, {'name': 'Zeran Gao'}, {'name': 'Yijie Huang'}, {'name': 'Shiyi Mu'}, {'name': 'Shi Feng'}, {'name': 'Daling Wang'}, {'name': 'Yifei Zhang'}, {'name': 'Kaisong Song'}, {'name': 'Ge Yu'}]",2024-07-30T08:12:04Z
http://arxiv.org/abs/2407.21082v1,http://arxiv.org/abs/2407.21082v1,"Accelerating Large Language Model Inference with Self-Supervised Early
  Exits","This paper presents a novel technique for accelerating inference in large,
pre-trained language models (LLMs) by introducing early exits during inference.
The computational demands of these models, used across a wide range of
applications, can be substantial. By capitalizing on the inherent variability
in token complexity, our approach enables selective acceleration of the
inference process. Specifically, we propose the integration of early exit
''heads'' atop existing transformer layers, which facilitate conditional
terminations based on a confidence metric. These heads are trained in a
self-supervised manner using the model's own predictions as training data,
thereby eliminating the need for additional annotated data. The confidence
metric, established using a calibration set, ensures a desired level of
accuracy while enabling early termination when confidence exceeds a
predetermined threshold. Notably, our method preserves the original accuracy
and reduces computational time on certain tasks, leveraging the existing
knowledge of pre-trained LLMs without requiring extensive retraining. This
lightweight, modular modification has the potential to greatly enhance the
practical usability of LLMs, particularly in applications like real-time
language processing in resource-constrained environments.",[{'name': 'Florian Valade'}],2024-07-30T07:58:28Z
http://arxiv.org/abs/2407.20622v1,http://arxiv.org/abs/2407.20622v1,Decoding Linguistic Representations of Human Brain,"Language, as an information medium created by advanced organisms, has always
been a concern of neuroscience regarding how it is represented in the brain.
Decoding linguistic representations in the evoked brain has shown
groundbreaking achievements, thanks to the rapid improvement of neuroimaging,
medical technology, life sciences and artificial intelligence. In this work, we
present a taxonomy of brain-to-language decoding of both textual and speech
formats. This work integrates two types of research: neuroscience focusing on
language understanding and deep learning-based brain decoding. Generating
discernible language information from brain activity could not only help those
with limited articulation, especially amyotrophic lateral sclerosis (ALS)
patients but also open up a new way for the next generation's brain-computer
interface (BCI). This article will help brain scientists and deep-learning
researchers to gain a bird's eye view of fine-grained language perception, and
thus facilitate their further investigation and research of neural process and
language decoding.","[{'name': 'Yu Wang'}, {'name': 'Heyang Liu'}, {'name': 'Yuhao Wang'}, {'name': 'Chuan Xuan'}, {'name': 'Yixuan Hou'}, {'name': 'Sheng Feng'}, {'name': 'Hongcheng Liu'}, {'name': 'Yusheng Liao'}, {'name': 'Yanfeng Wang'}]",2024-07-30T07:55:44Z
http://arxiv.org/abs/2407.20608v1,http://arxiv.org/abs/2407.20608v1,"Questionnaires for Everyone: Streamlining Cross-Cultural Questionnaire
  Adaptation with GPT-Based Translation Quality Evaluation","Adapting questionnaires to new languages is a resource-intensive process
often requiring the hiring of multiple independent translators, which limits
the ability of researchers to conduct cross-cultural research and effectively
creates inequalities in research and society. This work presents a prototype
tool that can expedite the questionnaire translation process. The tool
incorporates forward-backward translation using DeepL alongside GPT-4-generated
translation quality evaluations and improvement suggestions. We conducted two
online studies in which participants translated questionnaires from English to
either German (Study 1; n=10) or Portuguese (Study 2; n=20) using our
prototype. To evaluate the quality of the translations created using the tool,
evaluation scores between conventionally translated and tool-supported versions
were compared. Our results indicate that integrating LLM-generated translation
quality evaluations and suggestions for improvement can help users
independently attain results similar to those provided by conventional,
non-NLP-supported translation methods. This is the first step towards more
equitable questionnaire-based research, powered by AI.","[{'name': 'Otso Haavisto'}, {'name': 'Robin Welsch'}]",2024-07-30T07:34:40Z
http://arxiv.org/abs/2407.20595v1,http://arxiv.org/abs/2407.20595v1,"Harvesting Textual and Structured Data from the HAL Publication
  Repository","HAL (Hyper Articles en Ligne) is the French national publication repository,
used by most higher education and research organizations for their open science
policy. As a digital library, it is a rich repository of scholarly documents,
but its potential for advanced research has been underutilized. We present
HALvest, a unique dataset that bridges the gap between citation networks and
the full text of papers submitted on HAL. We craft our dataset by filtering HAL
for scholarly publications, resulting in approximately 700,000 documents,
spanning 34 languages across 13 identified domains, suitable for language model
training, and yielding approximately 16.5 billion tokens (with 8 billion in
French and 7 billion in English, the most represented languages). We transform
the metadata of each paper into a citation network, producing a directed
heterogeneous graph. This graph includes uniquely identified authors on HAL, as
well as all open submitted papers, and their citations. We provide a baseline
for authorship attribution using the dataset, implement a range of
state-of-the-art models in graph representation learning for link prediction,
and discuss the usefulness of our generated knowledge graph structure.","[{'name': 'Francis Kulumba'}, {'name': 'Wissam Antoun'}, {'name': 'Guillaume Vimont'}, {'name': 'Laurent Romary'}]",2024-07-30T07:14:04Z
http://arxiv.org/abs/2407.20588v1,http://arxiv.org/abs/2407.20588v1,"Enhancing Agricultural Machinery Management through Advanced LLM
  Integration","The integration of artificial intelligence into agricultural practices,
specifically through Consultation on Intelligent Agricultural Machinery
Management (CIAMM), has the potential to revolutionize efficiency and
sustainability in farming. This paper introduces a novel approach that
leverages large language models (LLMs), particularly GPT-4, combined with
multi-round prompt engineering to enhance decision-making processes in
agricultural machinery management. We systematically developed and refined
prompts to guide the LLMs in generating precise and contextually relevant
outputs. Our approach was evaluated using a manually curated dataset from
various online sources, and performance was assessed with accuracy and GPT-4
Scores. Comparative experiments were conducted using LLama-2-70B, ChatGPT, and
GPT-4 models, alongside baseline and state-of-the-art methods such as Chain of
Thought (CoT) and Thought of Thought (ThoT). The results demonstrate that our
method significantly outperforms these approaches, achieving higher accuracy
and relevance in generated responses. This paper highlights the potential of
advanced prompt engineering techniques in improving the robustness and
applicability of AI in agricultural contexts.","[{'name': 'Emily Johnson'}, {'name': 'Noah Wilson'}]",2024-07-30T06:49:55Z
http://arxiv.org/abs/2407.20584v1,http://arxiv.org/abs/2407.20584v1,"Pruning Large Language Models with Semi-Structural Adaptive Sparse
  Training","Transformer-based Large Language Models (LLMs) have demonstrated remarkable
success across various challenging tasks. However, the deployment of LLMs is
hindered by their substantial parameter count and memory consumption. Recently,
numerous studies have attempted to compress LLMs by pruning them using
training-free methods. However, these pruned models often experience
significant performance degradation on complex tasks. To address this issue, we
propose a novel training pipeline for semi-structured sparse models, named
Adaptive Sparse Trainer (AST). By distilling the knowledge stored in its dense
counterpart, we prevent the sparse model from overfitting and ensure a stable
training process. Moreover, AST allows the model to adaptively select better
lottery tickets (e.g., masks) during training. Additionally, we discovered that
adding extra well-initialized parameters can further enhance model performance
with only a small increase in memory footprint. Our method significantly
narrows the performance gap between dense and sparse models while maintaining
limited computational cost. Furthermore, when combined with existing
quantization methods, AST can compress language models by up to 16x compared to
dense FP32 precision models with minimal performance loss. AST outperforms
previous state-of-the-art methods by reducing the zero-shot accuracy gap
between dense and semi-structured sparse models to 1.12% across multiple
zero-shot tasks on Llama2-7B, using less than 0.4% of the pretraining tokens.","[{'name': 'Weiyu Huang'}, {'name': 'Guohao Jian'}, {'name': 'Yuezhou Hu'}, {'name': 'Jun Zhu'}, {'name': 'Jianfei Chen'}]",2024-07-30T06:33:44Z
http://arxiv.org/abs/2407.20581v1,http://arxiv.org/abs/2407.20581v1,Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings,"We present Knesset-DictaBERT, a large Hebrew language model fine-tuned on the
Knesset Corpus, which comprises Israeli parliamentary proceedings. The model is
based on the DictaBERT architecture and demonstrates significant improvements
in understanding parliamentary language according to the MLM task. We provide a
detailed evaluation of the model's performance, showing improvements in
perplexity and accuracy over the baseline DictaBERT model.","[{'name': 'Gili Goldin'}, {'name': 'Shuly Wintner'}]",2024-07-30T06:29:01Z
http://arxiv.org/abs/2407.20578v1,http://arxiv.org/abs/2407.20578v1,"Comparison of Large Language Models for Generating Contextually Relevant
  Questions","This study explores the effectiveness of Large Language Models (LLMs) for
Automatic Question Generation in educational settings. Three LLMs are compared
in their ability to create questions from university slide text without
fine-tuning. Questions were obtained in a two-step pipeline: first, answer
phrases were extracted from slides using Llama 2-Chat 13B; then, the three
models generated questions for each answer. To analyze whether the questions
would be suitable in educational applications for students, a survey was
conducted with 46 students who evaluated a total of 246 questions across five
metrics: clarity, relevance, difficulty, slide relation, and question-answer
alignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan
T5 XXL by a small margin, particularly in terms of clarity and question-answer
alignment. GPT-3.5 especially excels at tailoring questions to match the input
answers. The contribution of this research is the analysis of the capacity of
LLMs for Automatic Question Generation in education.","[{'name': 'Ivo Lodovico Molina'}, {'name': 'Valdemar Švábenský'}, {'name': 'Tsubasa Minematsu'}, {'name': 'Li Chen'}, {'name': 'Fumiya Okubo'}, {'name': 'Atsushi Shimada'}]",2024-07-30T06:23:59Z
http://arxiv.org/abs/2407.20564v1,http://arxiv.org/abs/2407.20564v1,"CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large
  Language Models over Factual Knowledge","While large language models (LLMs) have demonstrated impressive capabilities
across various natural language processing tasks by acquiring rich factual
knowledge from their broad training data, their ability to synthesize and
logically reason with this knowledge in complex ways remains underexplored. In
this work, we present a systematic evaluation of state-of-the-art LLMs' complex
logical reasoning abilities through a novel benchmark of automatically
generated complex reasoning questions over general domain and biomedical
knowledge graphs. Our extensive experiments, employing diverse in-context
learning techniques, reveal that LLMs excel at reasoning over general world
knowledge but face significant challenges with specialized domain-specific
knowledge. We find that prompting with explicit Chain-of-Thought demonstrations
can substantially improve LLM performance on complex logical reasoning tasks
with diverse logical operations. Interestingly, our controlled evaluations
uncover an asymmetry where LLMs display proficiency at set union operations,
but struggle considerably with set intersections - a key building block of
logical reasoning. To foster further work, we will publicly release our
evaluation benchmark and code.","[{'name': 'Tianshi Zheng'}, {'name': 'Jiaxin Bai'}, {'name': 'Yicheng Wang'}, {'name': 'Tianqing Fang'}, {'name': 'Yue Guo'}, {'name': 'Yauwai Yim'}, {'name': 'Yangqiu Song'}]",2024-07-30T05:40:32Z
http://arxiv.org/abs/2407.20556v1,http://arxiv.org/abs/2407.20556v1,Survey of Design Paradigms for Social Robots,"The demand for social robots in fields like healthcare, education, and
entertainment increases due to their emotional adaptation features. These
robots leverage multimodal communication, incorporating speech, facial
expressions, and gestures to enhance user engagement and emotional support. The
understanding of design paradigms of social robots is obstructed by the
complexity of the system and the necessity to tune it to a specific task. This
article provides a structured review of social robot design paradigms,
categorizing them into cognitive architectures, role design models, linguistic
models, communication flow, activity system models, and integrated design
models. By breaking down the articles on social robot design and application
based on these paradigms, we highlight the strengths and areas for improvement
in current approaches. We further propose our original integrated design model
that combines the most important aspects of the design of social robots. Our
approach shows the importance of integrating operational, communicational, and
emotional dimensions to create more adaptive and empathetic interactions
between robots and humans.","[{'name': 'Rita Frieske'}, {'name': 'Xiaoyu Mo'}, {'name': 'Yini Fang'}, {'name': 'Jay Nieles'}, {'name': 'Bertram E. Shi'}]",2024-07-30T05:22:31Z
http://arxiv.org/abs/2407.20524v2,http://arxiv.org/abs/2407.20524v2,Contrastive Feedback Mechanism for Simultaneous Speech Translation,"Recent advances in simultaneous speech translation (SST) focus on the
decision policies that enable the use of offline-trained ST models for
simultaneous inference. These decision policies not only control the
quality-latency trade-off in SST but also mitigate the impact of unstable
predictions on translation quality by delaying translation for more context or
discarding these predictions through stable hypothesis detection. However,
these policies often overlook the potential benefits of utilizing unstable
predictions. We introduce the contrastive feedback mechanism (CFM) for SST, a
novel method that leverages these unstable predictions as feedback to improve
translation quality. CFM guides the system to eliminate undesired model
behaviors from these predictions through a contrastive objective. The
experiments on 3 state-of-the-art decision policies across 8 languages in the
MuST-C v1.0 dataset show that CFM effectively improves the performance of SST.","[{'name': 'Haotian Tan'}, {'name': 'Sakriani Sakti'}]",2024-07-30T03:50:10Z
http://arxiv.org/abs/2407.20516v1,http://arxiv.org/abs/2407.20516v1,Machine Unlearning in Generative AI: A Survey,"Generative AI technologies have been deployed in many places, such as
(multimodal) large language models and vision generative models. Their
remarkable performance should be attributed to massive training data and
emergent reasoning abilities. However, the models would memorize and generate
sensitive, biased, or dangerous information originated from the training data
especially those from web crawl. New machine unlearning (MU) techniques are
being developed to reduce or eliminate undesirable knowledge and its effects
from the models, because those that were designed for traditional
classification tasks could not be applied for Generative AI. We offer a
comprehensive survey on many things about MU in Generative AI, such as a new
problem formulation, evaluation methods, and a structured discussion on the
advantages and limitations of different kinds of MU techniques. It also
presents several critical challenges and promising directions in MU research. A
curated list of readings can be found:
https://github.com/franciscoliu/GenAI-MU-Reading.","[{'name': 'Zheyuan Liu'}, {'name': 'Guangyao Dou'}, {'name': 'Zhaoxuan Tan'}, {'name': 'Yijun Tian'}, {'name': 'Meng Jiang'}]",2024-07-30T03:26:09Z
http://arxiv.org/abs/2407.20513v1,http://arxiv.org/abs/2407.20513v1,"Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural
  Language","This paper presents a conversational pipeline for crafting domain knowledge
for complex neuro-symbolic models through natural language prompts. It
leverages large language models to generate declarative programs in the
DomiKnowS framework. The programs in this framework express concepts and their
relationships as a graph in addition to logical constraints between them. The
graph, later, can be connected to trainable neural models according to those
specifications. Our proposed pipeline utilizes techniques like dynamic
in-context demonstration retrieval, model refinement based on feedback from a
symbolic parser, visualization, and user interaction to generate the tasks'
structure and formal knowledge representation. This approach empowers domain
experts, even those not well-versed in ML/AI, to formally declare their
knowledge to be incorporated in customized neural models in the DomiKnowS
framework.","[{'name': 'Hossein Rajaby Faghihi'}, {'name': 'Aliakbar Nafar'}, {'name': 'Andrzej Uszok'}, {'name': 'Hamid Karimian'}, {'name': 'Parisa Kordjamshidi'}]",2024-07-30T03:10:30Z
http://arxiv.org/abs/2407.20485v2,http://arxiv.org/abs/2407.20485v2,"A2SF: Accumulative Attention Scoring with Forgetting Factor for Token
  Pruning in Transformer Decoder","Recently, large language models (LLM) based on transformers are facing memory
bottleneck issues due to KV cache, especially in long sequence handling.
Previous researches proposed KV cache compression techniques that identify
insignificant tokens based on Accumulative Attention Scores and removes their
items from KV cache, noting that only few tokens play an important role in
attention operations. However, we have observed that the existing Accumulative
Attention Score is not suitable for the transformer decoder structure. In the
decoder model, the number of times the Attention Score accumulates varies
depending on the order of token appearance due to the effect of masking,
causing an uneven comparison between tokens. To solve this, we propose
Accumulative Attention Score with Forgetting Factor (A2SF) technique, which
introduces a Forgetting Factor in the Attention Score accumulation process.
A2SF applies a penalty to the past Attention Score generated from old tokens by
repeatedly multiplying the Forgetting Factor to the Attention Score over time.
Therefore, older tokens receive a larger penalty, providing fairness among
different ages of tokens. Through the fair comparison among tokens, we can more
effectively select important tokens. We have verified the accuracy improvement
through A2SF in the OPT and LLaMA models and A2SF improves the accuracy of
LLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.","[{'name': 'Hyun-rae Jo'}, {'name': 'Dongkun Shin'}]",2024-07-30T01:13:42Z
http://arxiv.org/abs/2407.20454v1,http://arxiv.org/abs/2407.20454v1,"CoMMIT: Coordinated Instruction Tuning for Multimodal Large Language
  Models","Instruction tuning in multimodal large language models (MLLMs) aims to
smoothly integrate a backbone LLM with a pre-trained feature encoder for
downstream tasks. The major challenge is how to efficiently find the synergy
through cooperative learning where LLMs adapt their reasoning abilities in
downstream tasks while feature encoders adjust their encoding to provide more
relevant modal information. In this paper, we analyze the MLLM instruction
tuning from both theoretical and empirical perspectives, where we find
unbalanced learning between the two components, i.e., the feature encoder and
the LLM, can cause diminishing learning gradients that slow the model
convergence and often lead to sub-optimal results due to insufficient learning.
Inspired by our findings, we propose a measurement to quantitatively evaluate
the learning balance, based on which we further design a dynamic learning
scheduler that better coordinates the learning. In addition, we introduce an
auxiliary loss regularization method to promote updating of the generation
distribution of MLLMs considering the learning state of each model component,
which potentially prevents each component from gradient diminishing and enables
a more accurate estimation of the learning balance coefficient. We conduct
experiments with multiple LLM backbones and feature encoders, where our
techniques are model-agnostic and can be generically integrated with various
MLLM backbones. Experiment results on multiple downstream tasks and modalities
in vision and audio, demonstrate the proposed method's better efficiency and
effectiveness in MLLM instruction tuning.","[{'name': 'Junda Wu'}, {'name': 'Xintong Li'}, {'name': 'Tong Yu'}, {'name': 'Yu Wang'}, {'name': 'Xiang Chen'}, {'name': 'Jiuxiang Gu'}, {'name': 'Lina Yao'}, {'name': 'Jingbo Shang'}, {'name': 'Julian McAuley'}]",2024-07-29T23:18:55Z
http://arxiv.org/abs/2407.20413v1,http://arxiv.org/abs/2407.20413v1,"Through the Looking Glass, and what Horn Clause Programs Found There","Dual Horn clauses mirror key properties of Horn clauses. This paper explores
the ``other side of the looking glass'' to reveal some expected and unexpected
symmetries and their practical uses.
  We revisit Dual Horn clauses as enablers of a form of constructive negation
that supports goal-driven forward reasoning and is valid both
intuitionistically and classically. In particular, we explore the ability to
falsify a counterfactual hypothesis in the context of a background theory
expressed as a Dual Horn clause program.
  With Dual Horn clause programs, by contrast to negation as failure, the
variable bindings in their computed answers provide explanations for the
reasons why a statement is successfully falsified. Moreover, in the
propositional case, by contrast to negation as failure as implemented with
stable models semantics in ASP systems, and similarly to Horn clause programs,
Dual Horn clause programs have polynomial complexity.
  After specifying their execution model with a metainterpreter, we devise a
compilation scheme from Dual Horn clause programs to Horn clause programs,
ensuring their execution with no performance penalty and we design the embedded
SymLP language to support combined Horn clause and Dual Horn clause programs.
  As a (motivating) application, we cast LLM reasoning chains into
propositional Horn and Dual Horn clauses that work together to constructively
prove and disprove goals and enhance Generative AI with explainability of
reasoning chains.",[{'name': 'Paul Tarau'}],2024-07-29T20:52:26Z
http://arxiv.org/abs/2407.21077v1,http://arxiv.org/abs/2407.21077v1,"Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions
  for Large Language Models","Large Language Models (LLMs) rely on instruction samples for alignment, but
creating these datasets poses challenges, particularly in expert-dependent
tasks like coding, which can be cost-prohibitive. One approach to mitigate
these challenges is synthesizing data using another LLM. In this paper, we
introduce a scalable method for generating synthetic instructions to enhance
the code generation capability of LLMs. The proposed algorithm,
Genetic-Instruct, mimics evolutionary processes, utilizing self-instruction to
create numerous synthetic samples from a limited number of seeds.
Genetic-Instruct is designed for efficient scaling of the generation process.
Fine-tuning multiple coding LLMs with the synthetic samples demonstrates a
significant improvement in their code generation accuracy compared to the
baselines.","[{'name': 'Somshubra Majumdar'}, {'name': 'Vahid Noroozi'}, {'name': 'Sean Narenthiran'}, {'name': 'Aleksander Ficek'}, {'name': 'Jagadeesh Balam'}, {'name': 'Boris Ginsburg'}]",2024-07-29T20:42:59Z
http://arxiv.org/abs/2407.20382v1,http://arxiv.org/abs/2407.20382v1,"What if Red Can Talk? Dynamic Dialogue Generation Using Large Language
  Models","Role-playing games (RPGs) provide players with a rich, interactive world to
explore. Dialogue serves as the primary means of communication between
developers and players, manifesting in various forms such as guides, NPC
interactions, and storytelling. While most games rely on written scripts to
define the main story and character personalities, player immersion can be
significantly enhanced through casual interactions between characters. With the
advent of large language models (LLMs), we introduce a dialogue filler
framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic
and contextually appropriate character interactions. We test this framework
within the environments of Final Fantasy VII Remake and Pokemon, providing
qualitative and quantitative evidence that demonstrates GPT-4's capability to
act with defined personalities and generate dialogue. However, some flaws
remain, such as GPT-4 being overly positive or more subtle personalities, such
as maturity, tend to be of lower quality compared to more overt traits like
timidity. This study aims to assist developers in crafting more nuanced filler
dialogues, thereby enriching player immersion and enhancing the overall RPG
experience.","[{'name': 'Navapat Nananukul'}, {'name': 'Wichayaporn Wongkamjan'}]",2024-07-29T19:12:18Z
http://arxiv.org/abs/2407.20371v1,http://arxiv.org/abs/2407.20371v1,"Gender, Race, and Intersectional Bias in Resume Screening via Language
  Model Retrieval","Artificial intelligence (AI) hiring tools have revolutionized resume
screening, and large language models (LLMs) have the potential to do the same.
However, given the biases which are embedded within LLMs, it is unclear whether
they can be used in this scenario without disadvantaging groups based on their
protected attributes. In this work, we investigate the possibilities of using
LLMs in a resume screening setting via a document retrieval framework that
simulates job candidate selection. Using that framework, we then perform a
resume audit study to determine whether a selection of Massive Text Embedding
(MTE) models are biased in resume screening scenarios. We simulate this for
nine occupations, using a collection of over 500 publicly available resumes and
500 job descriptions. We find that the MTEs are biased, significantly favoring
White-associated names in 85.1\% of cases and female-associated names in only
11.1\% of cases, with a minority of cases showing no statistically significant
differences. Further analyses show that Black males are disadvantaged in up to
100\% of cases, replicating real-world patterns of bias in employment settings,
and validate three hypotheses of intersectionality. We also find an impact of
document length as well as the corpus frequency of names in the selection of
resumes. These findings have implications for widely used AI tools that are
automating employment, fairness, and tech policy.","[{'name': 'Kyra Wilson'}, {'name': 'Aylin Caliskan'}]",2024-07-29T18:42:39Z
http://arxiv.org/abs/2407.21075v1,http://arxiv.org/abs/2407.21075v1,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence
features, including a ~3 billion parameter model designed to run efficiently on
devices and a large server-based language model designed for Private Cloud
Compute. These models are designed to perform a wide range of tasks
efficiently, accurately, and responsibly. This report describes the model
architecture, the data used to train the model, the training process, how the
models are optimized for inference, and the evaluation results. We highlight
our focus on Responsible AI and how the principles are applied throughout the
model development.","[{'name': 'Tom Gunter'}, {'name': 'Zirui Wang'}, {'name': 'Chong Wang'}, {'name': 'Ruoming Pang'}, {'name': 'Andy Narayanan'}, {'name': 'Aonan Zhang'}, {'name': 'Bowen Zhang'}, {'name': 'Chen Chen'}, {'name': 'Chung-Cheng Chiu'}, {'name': 'David Qiu'}, {'name': 'Deepak Gopinath'}, {'name': 'Dian Ang Yap'}, {'name': 'Dong Yin'}, {'name': 'Feng Nan'}, {'name': 'Floris Weers'}, {'name': 'Guoli Yin'}, {'name': 'Haoshuo Huang'}, {'name': 'Jianyu Wang'}, {'name': 'Jiarui Lu'}, {'name': 'John Peebles'}, {'name': 'Ke Ye'}, {'name': 'Mark Lee'}, {'name': 'Nan Du'}, {'name': 'Qibin Chen'}, {'name': 'Quentin Keunebroek'}, {'name': 'Sam Wiseman'}, {'name': 'Syd Evans'}, {'name': 'Tao Lei'}, {'name': 'Vivek Rathod'}, {'name': 'Xiang Kong'}, {'name': 'Xianzhi Du'}, {'name': 'Yanghao Li'}, {'name': 'Yongqiang Wang'}, {'name': 'Yuan Gao'}, {'name': 'Zaid Ahmed'}, {'name': 'Zhaoyang Xu'}, {'name': 'Zhiyun Lu'}, {'name': 'Al Rashid'}, {'name': 'Albin Madappally Jose'}, {'name': 'Alec Doane'}, {'name': 'Alfredo Bencomo'}, {'name': 'Allison Vanderby'}, {'name': 'Andrew Hansen'}, {'name': 'Ankur Jain'}, {'name': 'Anupama Mann Anupama'}, {'name': 'Areeba Kamal'}, {'name': 'Bugu Wu'}, {'name': 'Carolina Brum'}, {'name': 'Charlie Maalouf'}, {'name': 'Chinguun Erdenebileg'}, {'name': 'Chris Dulhanty'}, {'name': 'Dominik Moritz'}, {'name': 'Doug Kang'}, {'name': 'Eduardo Jimenez'}, {'name': 'Evan Ladd'}, {'name': 'Fangping Shi'}, {'name': 'Felix Bai'}, {'name': 'Frank Chu'}, {'name': 'Fred Hohman'}, {'name': 'Hadas Kotek'}, {'name': 'Hannah Gillis Coleman'}, {'name': 'Jane Li'}, {'name': 'Jeffrey Bigham'}, {'name': 'Jeffery Cao'}, {'name': 'Jeff Lai'}, {'name': 'Jessica Cheung'}, {'name': 'Jiulong Shan'}, {'name': 'Joe Zhou'}, {'name': 'John Li'}, {'name': 'Jun Qin'}, {'name': 'Karanjeet Singh'}, {'name': 'Karla Vega'}, {'name': 'Kelvin Zou'}, {'name': 'Laura Heckman'}, {'name': 'Lauren Gardiner'}, {'name': 'Margit Bowler'}, {'name': 'Maria Cordell'}, {'name': 'Meng Cao'}, {'name': 'Nicole Hay'}, {'name': 'Nilesh Shahdadpuri'}, {'name': 'Otto Godwin'}, {'name': 'Pranay Dighe'}, {'name': 'Pushyami Rachapudi'}, {'name': 'Ramsey Tantawi'}, {'name': 'Roman Frigg'}, {'name': 'Sam Davarnia'}, {'name': 'Sanskruti Shah'}, {'name': 'Saptarshi Guha'}, {'name': 'Sasha Sirovica'}, {'name': 'Shen Ma'}, {'name': 'Shuang Ma'}, {'name': 'Simon Wang'}, {'name': 'Sulgi Kim'}, {'name': 'Suma Jayaram'}, {'name': 'Vaishaal Shankar'}, {'name': 'Varsha Paidi'}, {'name': 'Vivek Kumar'}, {'name': 'Xin Wang'}, {'name': 'Xin Zheng'}, {'name': 'Walker Cheng'}, {'name': 'Yael Shrager'}, {'name': 'Yang Ye'}, {'name': 'Yasu Tanaka'}, {'name': 'Yihao Guo'}, {'name': 'Yunsong Meng'}, {'name': 'Zhao Tang Luo'}, {'name': 'Zhi Ouyang'}, {'name': 'Alp Aygar'}, {'name': 'Alvin Wan'}, {'name': 'Andrew Walkingshaw'}, {'name': 'Andy Narayanan'}, {'name': 'Antonie Lin'}, {'name': 'Arsalan Farooq'}, {'name': 'Brent Ramerth'}, {'name': 'Colorado Reed'}, {'name': 'Chris Bartels'}, {'name': 'Chris Chaney'}, {'name': 'David Riazati'}, {'name': 'Eric Liang Yang'}, {'name': 'Erin Feldman'}, {'name': 'Gabriel Hochstrasser'}, {'name': 'Guillaume Seguin'}, {'name': 'Irina Belousova'}, {'name': 'Joris Pelemans'}, {'name': 'Karen Yang'}, {'name': 'Keivan Alizadeh Vahid'}, {'name': 'Liangliang Cao'}, {'name': 'Mahyar Najibi'}, {'name': 'Marco Zuliani'}, {'name': 'Max Horton'}, {'name': 'Minsik Cho'}, {'name': 'Nikhil Bhendawade'}, {'name': 'Patrick Dong'}, {'name': 'Piotr Maj'}, {'name': 'Pulkit Agrawal'}, {'name': 'Qi Shan'}, {'name': 'Qichen Fu'}, {'name': 'Regan Poston'}, {'name': 'Sam Xu'}, {'name': 'Shuangning Liu'}, {'name': 'Sushma Rao'}, {'name': 'Tashweena Heeramun'}, {'name': 'Thomas Merth'}, {'name': 'Uday Rayala'}, {'name': 'Victor Cui'}, {'name': 'Vivek Rangarajan Sridhar'}, {'name': 'Wencong Zhang'}, {'name': 'Wenqi Zhang'}, {'name': 'Wentao Wu'}, {'name': 'Xingyu Zhou'}, {'name': 'Xinwen Liu'}, {'name': 'Yang Zhao'}, {'name': 'Yin Xia'}, {'name': 'Zhile Ren'}, {'name': 'Zhongzheng Ren'}]",2024-07-29T18:38:49Z
http://arxiv.org/abs/2407.20341v1,http://arxiv.org/abs/2407.20341v1,"BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger
  Visual Cues","Effectively aligning with human judgment when evaluating machine-generated
image captions represents a complex yet intriguing challenge. Existing
evaluation metrics like CIDEr or CLIP-Score fall short in this regard as they
do not take into account the corresponding image or lack the capability of
encoding fine-grained details and penalizing hallucinations. To overcome these
issues, in this paper, we propose BRIDGE, a new learnable and reference-free
image captioning metric that employs a novel module to map visual features into
dense vectors and integrates them into multi-modal pseudo-captions which are
built during the evaluation process. This approach results in a multimodal
metric that properly incorporates information from the input image without
relying on reference captions, bridging the gap between human judgment and
machine-generated image captions. Experiments spanning several datasets
demonstrate that our proposal achieves state-of-the-art results compared to
existing reference-free evaluation scores. Our source code and trained models
are publicly available at: https://github.com/aimagelab/bridge-score.","[{'name': 'Sara Sarto'}, {'name': 'Marcella Cornia'}, {'name': 'Lorenzo Baraldi'}, {'name': 'Rita Cucchiara'}]",2024-07-29T18:00:17Z
http://arxiv.org/abs/2407.20224v2,http://arxiv.org/abs/2407.20224v2,Can Editing LLMs Inject Harm?,"Knowledge editing techniques have been increasingly adopted to efficiently
correct the false or outdated knowledge in Large Language Models (LLMs), due to
the high cost of retraining from scratch. Meanwhile, one critical but
under-explored question is: can knowledge editing be used to inject harm into
LLMs? In this paper, we propose to reformulate knowledge editing as a new type
of safety threat for LLMs, namely Editing Attack, and conduct a systematic
investigation with a newly constructed dataset EditAttack. Specifically, we
focus on two typical safety risks of Editing Attack including Misinformation
Injection and Bias Injection. For the risk of misinformation injection, we
first categorize it into commonsense misinformation injection and long-tail
misinformation injection. Then, we find that editing attacks can inject both
types of misinformation into LLMs, and the effectiveness is particularly high
for commonsense misinformation injection. For the risk of bias injection, we
discover that not only can biased sentences be injected into LLMs with high
effectiveness, but also one single biased sentence injection can cause a bias
increase in general outputs of LLMs, which are even highly irrelevant to the
injected sentence, indicating a catastrophic impact on the overall fairness of
LLMs. Then, we further illustrate the high stealthiness of editing attacks,
measured by their impact on the general knowledge and reasoning capacities of
LLMs, and show the hardness of defending editing attacks with empirical
evidence. Our discoveries demonstrate the emerging misuse risks of knowledge
editing techniques on compromising the safety alignment of LLMs.","[{'name': 'Canyu Chen'}, {'name': 'Baixiang Huang'}, {'name': 'Zekun Li'}, {'name': 'Zhaorun Chen'}, {'name': 'Shiyang Lai'}, {'name': 'Xiongxiao Xu'}, {'name': 'Jia-Chen Gu'}, {'name': 'Jindong Gu'}, {'name': 'Huaxiu Yao'}, {'name': 'Chaowei Xiao'}, {'name': 'Xifeng Yan'}, {'name': 'William Yang Wang'}, {'name': 'Philip Torr'}, {'name': 'Dawn Song'}, {'name': 'Kai Shu'}]",2024-07-29T17:58:06Z
http://arxiv.org/abs/2407.20311v1,http://arxiv.org/abs/2407.20311v1,"Physics of Language Models: Part 2.1, Grade-School Math and the Hidden
  Reasoning Process","Recent advances in language models have demonstrated their capability to
solve mathematical reasoning problems, achieving near-perfect accuracy on
grade-school level math benchmarks like GSM8K. In this paper, we formally study
how language models solve these problems. We design a series of controlled
experiments to address several fundamental questions: (1) Can language models
truly develop reasoning skills, or do they simply memorize templates? (2) What
is the model's hidden (mental) reasoning process? (3) Do models solve math
questions using skills similar to or different from humans? (4) Do models
trained on GSM8K-like datasets develop reasoning skills beyond those necessary
for solving GSM8K problems? (5) What mental process causes models to make
reasoning mistakes? (6) How large or deep must a model be to effectively solve
GSM8K-level math questions?
  Our study uncovers many hidden mechanisms by which language models solve
mathematical questions, providing insights that extend beyond current
understandings of LLMs.","[{'name': 'Tian Ye'}, {'name': 'Zicheng Xu'}, {'name': 'Yuanzhi Li'}, {'name': 'Zeyuan Allen-Zhu'}]",2024-07-29T17:52:40Z
http://arxiv.org/abs/2407.20207v1,http://arxiv.org/abs/2407.20207v1,QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval,"In dense retrieval, embedding long texts into dense vectors can result in
information loss, leading to inaccurate query-text matching. Additionally,
low-quality texts with excessive noise or sparse key information are unlikely
to align well with relevant queries. Recent studies mainly focus on improving
the sentence embedding model or retrieval process. In this work, we introduce a
novel text augmentation framework for dense retrieval. This framework
transforms raw documents into information-dense text formats, which supplement
the original texts to effectively address the aforementioned issues without
modifying embedding or retrieval methodologies. Two text representations are
generated via large language models (LLMs) zero-shot prompting: question-answer
pairs and element-driven events. We term this approach QAEA-DR: unifying
question-answer generation and event extraction in a text augmentation
framework for dense retrieval. To further enhance the quality of generated
texts, a scoring-based evaluation and regeneration mechanism is introduced in
LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,
supported by both theoretical analysis and empirical experiments.","[{'name': 'Hongming Tan'}, {'name': 'Shaoxiong Zhan'}, {'name': 'Hai Lin'}, {'name': 'Hai-Tao Zheng'}, {'name': 'Wai Kin'}, {'name': 'Chan'}]",2024-07-29T17:39:08Z
http://arxiv.org/abs/2407.20189v1,http://arxiv.org/abs/2407.20189v1,"Aligning Query Representation with Rewritten Query and Relevance
  Judgments in Conversational Search","Conversational search supports multi-turn user-system interactions to solve
complex information needs. Different from the traditional single-turn ad-hoc
search, conversational search encounters a more challenging problem of
context-dependent query understanding with the lengthy and long-tail
conversational history context. While conversational query rewriting methods
leverage explicit rewritten queries to train a rewriting model to transform the
context-dependent query into a stand-stone search query, this is usually done
without considering the quality of search results. Conversational dense
retrieval methods use fine-tuning to improve a pre-trained ad-hoc query
encoder, but they are limited by the conversational search data available for
training. In this paper, we leverage both rewritten queries and relevance
judgments in the conversational search data to train a better query
representation model. The key idea is to align the query representation with
those of rewritten queries and relevant documents. The proposed model -- Query
Representation Alignment Conversational Dense Retriever, QRACDR, is tested on
eight datasets, including various settings in conversational search and ad-hoc
search. The results demonstrate the strong performance of QRACDR compared with
state-of-the-art methods, and confirm the effectiveness of representation
alignment.","[{'name': 'Fengran Mo'}, {'name': 'Chen Qu'}, {'name': 'Kelong Mao'}, {'name': 'Yihong Wu'}, {'name': 'Zhan Su'}, {'name': 'Kaiyu Huang'}, {'name': 'Jian-Yun Nie'}]",2024-07-29T17:14:36Z
http://arxiv.org/abs/2407.20183v1,http://arxiv.org/abs/2407.20183v1,MindSearch: Mimicking Human Minds Elicits Deep AI Searcher,"Information seeking and integration is a complex cognitive task that consumes
enormous time and effort. Inspired by the remarkable progress of Large Language
Models, recent works attempt to solve this task by combining LLMs and search
engines. However, these methods still obtain unsatisfying performance due to
three challenges: (1) complex requests often cannot be accurately and
completely retrieved by the search engine once (2) corresponding information to
be integrated is spread over multiple web pages along with massive noise, and
(3) a large number of web pages with long contents may quickly exceed the
maximum context length of LLMs. Inspired by the cognitive process when humans
solve these problems, we introduce MindSearch to mimic the human minds in web
information seeking and integration, which can be instantiated by a simple yet
effective LLM-based multi-agent framework. The WebPlanner models the human mind
of multi-step information seeking as a dynamic graph construction process: it
decomposes the user query into atomic sub-questions as nodes in the graph and
progressively extends the graph based on the search result from WebSearcher.
Tasked with each sub-question, WebSearcher performs hierarchical information
retrieval with search engines and collects valuable information for WebPlanner.
The multi-agent design of MindSearch enables the whole framework to seek and
integrate information parallelly from larger-scale (e.g., more than 300) web
pages in 3 minutes, which is worth 3 hours of human effort. MindSearch
demonstrates significant improvement in the response quality in terms of depth
and breadth, on both close-set and open-set QA problems. Besides, responses
from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web
and Perplexity.ai applications, which implies that MindSearch can already
deliver a competitive solution to the proprietary AI search engine.","[{'name': 'Zehui Chen'}, {'name': 'Kuikun Liu'}, {'name': 'Qiuchen Wang'}, {'name': 'Jiangning Liu'}, {'name': 'Wenwei Zhang'}, {'name': 'Kai Chen'}, {'name': 'Feng Zhao'}]",2024-07-29T17:12:40Z
http://arxiv.org/abs/2407.20177v1,http://arxiv.org/abs/2407.20177v1,"AutoScale: Automatic Prediction of Compute-optimal Data Composition for
  Training LLMs","To ensure performance on a diverse set of downstream tasks, LLMs are
pretrained via data mixtures over different domains. In this work, we
demonstrate that the optimal data composition for a fixed compute budget varies
depending on the scale of the training data, suggesting that the common
practice of empirically determining an optimal composition using small-scale
experiments will not yield the optimal data mixtures when scaling up to the
final model. To address this challenge, we propose *AutoScale*, an automated
tool that finds a compute-optimal data composition for training at any desired
target scale. AutoScale first determines the optimal composition at a small
scale using a novel bilevel optimization framework, Direct Data Optimization
(*DDO*), and then fits a predictor to estimate the optimal composition at
larger scales. The predictor's design is inspired by our theoretical analysis
of scaling laws related to data composition, which could be of independent
interest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2
Large) on RedPajama dataset, AutoScale decreases validation perplexity at least
25% faster than any baseline with up to 38% speed up compared to without
reweighting, achieving the best overall performance across downstream tasks. On
pre-training Encoder-only LMs (BERT) with masked language modeling, DDO is
shown to decrease loss on all domains while visibly improving average task
performance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by
5.9% compared with without reweighting. AutoScale speeds up training by up to
28%. Our codes are open-sourced.","[{'name': 'Feiyang Kang'}, {'name': 'Yifan Sun'}, {'name': 'Bingbing Wen'}, {'name': 'Si Chen'}, {'name': 'Dawn Song'}, {'name': 'Rafid Mahmood'}, {'name': 'Ruoxi Jia'}]",2024-07-29T17:06:30Z
http://arxiv.org/abs/2407.20083v1,http://arxiv.org/abs/2407.20083v1,"An Energy-based Model for Word-level AutoCompletion in Computer-aided
  Translation","Word-level AutoCompletion(WLAC) is a rewarding yet challenging task in
Computer-aided Translation. Existing work addresses this task through a
classification model based on a neural network that maps the hidden vector of
the input context into its corresponding label (i.e., the candidate target word
is treated as a label). Since the context hidden vector itself does not take
the label into account and it is projected to the label through a linear
classifier, the model can not sufficiently leverage valuable information from
the source sentence as verified in our experiments, which eventually hinders
its overall performance. To alleviate this issue, this work proposes an
energy-based model for WLAC, which enables the context hidden vector to capture
crucial information from the source sentence. Unfortunately, training and
inference suffer from efficiency and effectiveness challenges, thereby we
employ three simple yet effective strategies to put our model into practice.
Experiments on four standard benchmarks demonstrate that our reranking-based
approach achieves substantial improvements (about 6.07%) over the previous
state-of-the-art model. Further analyses show that each strategy of our
approach contributes to the final performance.","[{'name': 'Cheng Yang'}, {'name': 'Guoping Huang'}, {'name': 'Mo Yu'}, {'name': 'Zhirui Zhang'}, {'name': 'Siheng Li'}, {'name': 'Mingming Yang'}, {'name': 'Shuming Shi'}, {'name': 'Yujiu Yang'}, {'name': 'Lemao Liu'}]",2024-07-29T15:07:19Z
http://arxiv.org/abs/2407.20076v1,http://arxiv.org/abs/2407.20076v1,"Investigating the Impact of Semi-Supervised Methods with Data
  Augmentation on Offensive Language Detection in Romanian Language","Offensive language detection is a crucial task in today's digital landscape,
where online platforms grapple with maintaining a respectful and inclusive
environment. However, building robust offensive language detection models
requires large amounts of labeled data, which can be expensive and
time-consuming to obtain. Semi-supervised learning offers a feasible solution
by utilizing labeled and unlabeled data to create more accurate and robust
models. In this paper, we explore a few different semi-supervised methods, as
well as data augmentation techniques. Concretely, we implemented eight
semi-supervised methods and ran experiments for them using only the available
data in the RO-Offense dataset and applying five augmentation techniques before
feeding the data to the models. Experimental results demonstrate that some of
them benefit more from augmentations than others.","[{'name': 'Elena-Beatrice Nicola'}, {'name': 'Dumitru-Clementin Cercel'}, {'name': 'Florin Pop'}]",2024-07-29T15:02:51Z
http://arxiv.org/abs/2407.20046v1,http://arxiv.org/abs/2407.20046v1,Exploring Large Language Models to generate Easy to Read content,"Ensuring text accessibility and understandability are essential goals,
particularly for individuals with cognitive impairments and intellectual
disabilities, who encounter challenges in accessing information across various
mediums such as web pages, newspapers, administrative tasks, or health
documents. Initiatives like Easy to Read and Plain Language guidelines aim to
simplify complex texts; however, standardizing these guidelines remains
challenging and often involves manual processes. This work presents an
exploratory investigation into leveraging Artificial Intelligence (AI) and
Natural Language Processing (NLP) approaches to systematically simplify Spanish
texts into Easy to Read formats, with a focus on utilizing Large Language
Models (LLMs) for simplifying texts, especially in generating Easy to Read
content. The study contributes a parallel corpus of Spanish adapted for Easy To
Read format, which serves as a valuable resource for training and testing text
simplification systems. Additionally, several text simplification experiments
using LLMs and the collected corpus are conducted, involving fine-tuning and
testing a Llama2 model to generate Easy to Read content. A qualitative
evaluation, guided by an expert in text adaptation for Easy to Read content, is
carried out to assess the automatically simplified texts. This research
contributes to advancing text accessibility for individuals with cognitive
impairments, highlighting promising strategies for leveraging LLMs while
responsibly managing energy usage.","[{'name': 'Paloma Martínez'}, {'name': 'Lourdes Moreno'}, {'name': 'Alberto Ramos'}]",2024-07-29T14:30:39Z
http://arxiv.org/abs/2407.19998v1,http://arxiv.org/abs/2407.19998v1,Do LLMs Really Adapt to Domains? An Ontology Learning Perspective,"Large Language Models (LLMs) have demonstrated unprecedented prowess across
various natural language processing tasks in various application domains.
Recent studies show that LLMs can be leveraged to perform lexical semantic
tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL).
However, it has not effectively been verified whether their success is due to
their ability to reason over unstructured or semi-structured data, or their
effective learning of linguistic patterns and senses alone. This unresolved
question is particularly crucial when dealing with domain-specific data, where
the lexical senses and their meaning can completely differ from what a LLM has
learned during its training stage. This paper investigates the following
question: Do LLMs really adapt to domains and remain consistent in the
extraction of structured knowledge, or do they only learn lexical senses
instead of reasoning? To answer this question and, we devise a controlled
experiment setup that uses WordNet to synthesize parallel corpora, with English
and gibberish terms. We examine the differences in the outputs of LLMs for each
corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical
results show that, while adapting to the gibberish corpora, off-the-shelf LLMs
do not consistently reason over semantic relationships between concepts, and
instead leverage senses and their frame. However, fine-tuning improves the
performance of LLMs on lexical semantic tasks even when the domain-specific
terms are arbitrary and unseen during pre-training, hinting at the
applicability of pre-trained LLMs for OL.","[{'name': 'Huu Tan Mai'}, {'name': 'Cuong Xuan Chu'}, {'name': 'Heiko Paulheim'}]",2024-07-29T13:29:43Z
http://arxiv.org/abs/2407.19947v1,http://arxiv.org/abs/2407.19947v1,"Inference acceleration for large language models using ""stairs"" assisted
  greedy generation","Large Language Models (LLMs) with billions of parameters are known for their
impressive predicting capabilities but require lots of resources to run. With
their massive rise in popularity, even a small reduction in required resources
could have an impact on environment. On the other hand, smaller models require
fewer resources but may sacrifice accuracy. In this work, we are proposing an
implementation of ``stairs'' assisted greedy generation. It is a modified
assisted generation methodology that makes use of a smaller model's fast
generation, large model's batch prediction, and ""stairs"" validation in order to
achieve a speed up in prediction generation. Results show between 9.58 and
17.24 percent inference time reduction compared to a stand-alone large LLM
prediction in a text generation task without a loss in accuracy.","[{'name': 'Domas Grigaliūnas'}, {'name': 'Mantas Lukoševičius'}]",2024-07-29T12:29:29Z
http://arxiv.org/abs/2407.19914v1,http://arxiv.org/abs/2407.19914v1,"Sentiment Analysis of Lithuanian Online Reviews Using Large Language
  Models","Sentiment analysis is a widely researched area within Natural Language
Processing (NLP), attracting significant interest due to the advent of
automated solutions. Despite this, the task remains challenging because of the
inherent complexity of languages and the subjective nature of sentiments. It is
even more challenging for less-studied and less-resourced languages such as
Lithuanian. Our review of existing Lithuanian NLP research reveals that
traditional machine learning methods and classification algorithms have limited
effectiveness for the task. In this work, we address sentiment analysis of
Lithuanian five-star-based online reviews from multiple domains that we collect
and clean. We apply transformer models to this task for the first time,
exploring the capabilities of pre-trained multilingual Large Language Models
(LLMs), specifically focusing on fine-tuning BERT and T5 models. Given the
inherent difficulty of the task, the fine-tuned models perform quite well,
especially when the sentiments themselves are less ambiguous: 80.74% and 89.61%
testing recognition accuracy of the most popular one- and five-star reviews
respectively. They significantly outperform current commercial state-of-the-art
general-purpose LLM GPT-4. We openly share our fine-tuned LLMs online.","[{'name': 'Brigita Vileikytė'}, {'name': 'Mantas Lukoševičius'}, {'name': 'Lukas Stankevičius'}]",2024-07-29T11:44:21Z
http://arxiv.org/abs/2407.19897v1,http://arxiv.org/abs/2407.19897v1,BEExAI: Benchmark to Evaluate Explainable AI,"Recent research in explainability has given rise to numerous post-hoc
attribution methods aimed at enhancing our comprehension of the outputs of
black-box machine learning models. However, evaluating the quality of
explanations lacks a cohesive approach and a consensus on the methodology for
deriving quantitative metrics that gauge the efficacy of explainability
post-hoc attribution methods. Furthermore, with the development of increasingly
complex deep learning models for diverse data applications, the need for a
reliable way of measuring the quality and correctness of explanations is
becoming critical. We address this by proposing BEExAI, a benchmark tool that
allows large-scale comparison of different post-hoc XAI methods, employing a
set of selected evaluation metrics.","[{'name': 'Samuel Sithakoul'}, {'name': 'Sara Meftah'}, {'name': 'Clément Feutry'}]",2024-07-29T11:21:17Z
http://arxiv.org/abs/2407.19884v1,http://arxiv.org/abs/2407.19884v1,Preliminary WMT24 Ranking of General MT Systems and LLMs,"This is the preliminary ranking of WMT24 General MT systems based on
automatic metrics. The official ranking will be a human evaluation, which is
superior to the automatic ranking and supersedes it. The purpose of this report
is not to interpret any findings but only provide preliminary results to the
participants of the General MT task that may be useful during the writing of
the system submission.","[{'name': 'Tom Kocmi'}, {'name': 'Eleftherios Avramidis'}, {'name': 'Rachel Bawden'}, {'name': 'Ondrej Bojar'}, {'name': 'Anton Dvorkovich'}, {'name': 'Christian Federmann'}, {'name': 'Mark Fishel'}, {'name': 'Markus Freitag'}, {'name': 'Thamme Gowda'}, {'name': 'Roman Grundkiewicz'}, {'name': 'Barry Haddow'}, {'name': 'Marzena Karpinska'}, {'name': 'Philipp Koehn'}, {'name': 'Benjamin Marie'}, {'name': 'Kenton Murray'}, {'name': 'Masaaki Nagata'}, {'name': 'Martin Popel'}, {'name': 'Maja Popovic'}, {'name': 'Mariya Shmatova'}, {'name': 'Steinþór Steingrímsson'}, {'name': 'Vilém Zouhar'}]",2024-07-29T11:01:17Z
http://arxiv.org/abs/2407.19842v1,http://arxiv.org/abs/2407.19842v1,"Detecting and Understanding Vulnerabilities in Language Models via
  Mechanistic Interpretability","Large Language Models (LLMs), characterized by being trained on broad amounts
of data in a self-supervised manner, have shown impressive performance across a
wide range of tasks. Indeed, their generative abilities have aroused interest
on the application of LLMs across a wide range of contexts. However, neural
networks in general, and LLMs in particular, are known to be vulnerable to
adversarial attacks, where an imperceptible change to the input can mislead the
output of the model. This is a serious concern that impedes the use of LLMs on
high-stakes applications, such as healthcare, where a wrong prediction can
imply serious consequences. Even though there are many efforts on making LLMs
more robust to adversarial attacks, there are almost no works that study
\emph{how} and \emph{where} these vulnerabilities that make LLMs prone to
adversarial attacks happen. Motivated by these facts, we explore how to
localize and understand vulnerabilities, and propose a method, based on
Mechanistic Interpretability (MI) techniques, to guide this process.
Specifically, this method enables us to detect vulnerabilities related to a
concrete task by (i) obtaining the subset of the model that is responsible for
that task, (ii) generating adversarial samples for that task, and (iii) using
MI techniques together with the previous samples to discover and understand the
possible vulnerabilities. We showcase our method on a pretrained GPT-2 Small
model carrying out the task of predicting 3-letter acronyms to demonstrate its
effectiveness on locating and understanding concrete vulnerabilities of the
model.","[{'name': 'Jorge García-Carrasco'}, {'name': 'Alejandro Maté'}, {'name': 'Juan Trujillo'}]",2024-07-29T09:55:34Z
http://arxiv.org/abs/2407.19835v1,http://arxiv.org/abs/2407.19835v1,"ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to
  English Translation","Classical Arabic represents a significant era, encompassing the golden age of
Arab culture, philosophy, and scientific literature. With a broad consensus on
the importance of translating these literatures to enrich knowledge
dissemination across communities, the advent of large language models (LLMs)
and translation systems offers promising tools to facilitate this goal.
However, we have identified a scarcity of translation datasets in Classical
Arabic, which are often limited in scope and topics, hindering the development
of high-quality translation systems. In response, we present the ATHAR dataset,
comprising 66,000 high-quality Classical Arabic to English translation samples
that cover a wide array of subjects including science, culture, and philosophy.
Furthermore, we assess the performance of current state-of-the-art LLMs under
various settings, concluding that there is a need for such datasets in current
systems. Our findings highlight how models can benefit from fine-tuning or
incorporating this dataset into their pretraining pipelines. The dataset is
publicly available on the HuggingFace Data Hub at
\url{https://huggingface.co/datasets/mohamed-khalil/ATHAR}.","[{'name': 'Mohammed Khalil'}, {'name': 'Mohammed Sabry'}]",2024-07-29T09:45:34Z
http://arxiv.org/abs/2407.19832v2,http://arxiv.org/abs/2407.19832v2,ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2,"Multimodal Large Language Models (MLLMs) have attracted much attention for
their multifunctionality. However, traditional Transformer architectures incur
significant overhead due to their secondary computational complexity. To
address this issue, we introduce ML-Mamba, a multimodal language model, which
utilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known
for its linear scalability and fast processing of long sequences. We replace
the Transformer-based backbone with a pre-trained Mamba-2 model and explore
methods for integrating 2D visual selective scanning mechanisms into multimodal
learning while also trying various visual encoders and Mamba-2 model variants.
Our extensive experiments in various multimodal benchmark tests demonstrate the
competitive performance of ML-Mamba and highlight the potential of state space
models in multimodal tasks. The experimental results show that: (1) we
empirically explore how to effectively apply the 2D vision selective scan
mechanism for multimodal learning. We propose a novel multimodal connector
called the Mamba-2 Scan Connector (MSC), which enhances representational
capabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art
methods such as TinyLaVA and MobileVLM v2 through its linear sequential
modeling while faster inference speed; (3) Compared to multimodal models
utilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference
performance and effectiveness.","[{'name': 'Wenjun Huang'}, {'name': 'Jianguo Hu'}]",2024-07-29T09:38:15Z
http://arxiv.org/abs/2407.19825v1,http://arxiv.org/abs/2407.19825v1,Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost,"Today's large language models (LLMs) can solve challenging question-answering
tasks, and prompt engineering techniques, such as chain-of-thought (CoT), have
gained attention for enhancing the explanation and correctness of outputs.
Nevertheless, models require significant time to generate answers augmented
with lengthy reasoning details. To address this issue, this paper analyzes the
impact of output lengths on LLM inference pipelines and proposes novel metrics
to evaluate them in terms of \textit{correct conciseness}. It also examines the
impact of controlling output length through a refined prompt engineering
strategy, Constrained-CoT (CCoT), which encourages the model to limit output
length. Experiments on pre-trained LLMs demonstrated the benefit of the
proposed metrics and the effectiveness of CCoT across different models. For
instance, constraining the reasoning of LLaMA2-70b to 100 words improves the
accuracy from 36.01\% (CoT) to 41.07\% (CCoT) on the GSM8K dataset, while
reducing the average output length by 28 words.","[{'name': 'Sania Nayab'}, {'name': 'Giulio Rossolini'}, {'name': 'Giorgio Buttazzo'}, {'name': 'Nicolamaria Manes'}, {'name': 'Fabrizio Giacomelli'}]",2024-07-29T09:21:52Z
http://arxiv.org/abs/2407.19816v1,http://arxiv.org/abs/2407.19816v1,"Comparative Analysis of Encoder-Based NER and Large Language Models for
  Skill Extraction from Russian Job Vacancies","The labor market is undergoing rapid changes, with increasing demands on job
seekers and a surge in job openings. Identifying essential skills and
competencies from job descriptions is challenging due to varying employer
requirements and the omission of key skills. This study addresses these
challenges by comparing traditional Named Entity Recognition (NER) methods
based on encoders with Large Language Models (LLMs) for extracting skills from
Russian job vacancies. Using a labeled dataset of 4,000 job vacancies for
training and 1,472 for testing, the performance of both approaches is
evaluated. Results indicate that traditional NER models, especially DeepPavlov
RuBERT NER tuned, outperform LLMs across various metrics including accuracy,
precision, recall, and inference time. The findings suggest that traditional
NER models provide more effective and efficient solutions for skill extraction,
enhancing job requirement clarity and aiding job seekers in aligning their
qualifications with employer expectations. This research contributes to the
field of natural language processing (NLP) and its application in the labor
market, particularly in non-English contexts.","[{'name': 'Nikita Matkin'}, {'name': 'Aleksei Smirnov'}, {'name': 'Mikhail Usanin'}, {'name': 'Egor Ivanov'}, {'name': 'Kirill Sobyanin'}, {'name': 'Sofiia Paklina'}, {'name': 'Petr Parshakov'}]",2024-07-29T09:08:40Z
http://arxiv.org/abs/2407.21073v1,http://arxiv.org/abs/2407.21073v1,"Enhancing Adversarial Text Attacks on BERT Models with Projected
  Gradient Descent","Adversarial attacks against deep learning models represent a major threat to
the security and reliability of natural language processing (NLP) systems. In
this paper, we propose a modification to the BERT-Attack framework, integrating
Projected Gradient Descent (PGD) to enhance its effectiveness and robustness.
The original BERT-Attack, designed for generating adversarial examples against
BERT-based models, suffers from limitations such as a fixed perturbation budget
and a lack of consideration for semantic similarity. The proposed approach in
this work, PGD-BERT-Attack, addresses these limitations by leveraging PGD to
iteratively generate adversarial examples while ensuring both imperceptibility
and semantic similarity to the original input. Extensive experiments are
conducted to evaluate the performance of PGD-BERT-Attack compared to the
original BERT-Attack and other baseline methods. The results demonstrate that
PGD-BERT-Attack achieves higher success rates in causing misclassification
while maintaining low perceptual changes. Furthermore, PGD-BERT-Attack produces
adversarial instances that exhibit greater semantic resemblance to the initial
input, enhancing their applicability in real-world scenarios. Overall, the
proposed modification offers a more effective and robust approach to
adversarial attacks on BERT-based models, thus contributing to the advancement
of defense against attacks on NLP systems.","[{'name': 'Hetvi Waghela'}, {'name': 'Jaydip Sen'}, {'name': 'Sneha Rakshit'}]",2024-07-29T09:07:29Z
http://arxiv.org/abs/2407.19813v2,http://arxiv.org/abs/2407.19813v2,Improving Retrieval Augmented Language Model with Self-Reasoning,"The Retrieval-Augmented Language Model (RALM) has shown remarkable
performance on knowledge-intensive tasks by incorporating external knowledge
during inference, which mitigates the factual hallucinations inherited in large
language models (LLMs). Despite these advancements, challenges persist in the
implementation of RALMs, particularly concerning their reliability and
traceability. To be specific, the irrelevant document retrieval may result in
unhelpful response generation or even deteriorate the performance of LLMs,
while the lack of proper citations in generated outputs complicates efforts to
verify the trustworthiness of the models. To this end, we propose a novel
self-reasoning framework aimed at improving the reliability and traceability of
RALMs, whose core idea is to leverage reasoning trajectories generated by the
LLM itself. The framework involves constructing self-reason trajectories with
three processes: a relevance-aware process, an evidence-aware selective
process, and a trajectory analysis process. We have evaluated our framework
across four public datasets (two short-form QA datasets, one long-form QA
dataset, and one fact verification dataset) to demonstrate the superiority of
our method, which can outperform existing state-of-art models and can achieve
comparable performance with GPT-4, while only using 2,000 training samples.","[{'name': 'Yuan Xia'}, {'name': 'Jingbo Zhou'}, {'name': 'Zhenhui Shi'}, {'name': 'Jun Chen'}, {'name': 'Haifeng Huang'}]",2024-07-29T09:05:10Z
http://arxiv.org/abs/2407.19808v1,http://arxiv.org/abs/2407.19808v1,Segmentation en phrases : ouvrez les guillemets sans perdre le fil,"This paper presents a graph cascade for sentence segmentation of XML
documents. Our proposal offers sentences inside sentences for cases introduced
by quotation marks and hyphens, and also pays particular attention to
situations involving incises introduced by parentheses and lists introduced by
colons. We present how the tool works and compare the results obtained with
those available in 2019 on the same dataset, together with an evaluation of the
system's performance on a test corpus","[{'name': 'Sandrine Ollinger'}, {'name': 'Denis Maurel'}]",2024-07-29T09:02:38Z
http://arxiv.org/abs/2407.19807v1,http://arxiv.org/abs/2407.19807v1,Cool-Fusion: Fuse Large Language Models without Training,"We focus on the problem of fusing two or more heterogeneous large language
models (LLMs) to facilitate their complementary strengths. One of the
challenges on model fusion is high computational load, i.e. to fine-tune or to
align vocabularies via combinatorial optimization. To this end, we propose
\emph{Cool-Fusion}, a simple yet effective approach that fuses the knowledge of
heterogeneous source LLMs to leverage their complementary strengths.
\emph{Cool-Fusion} is the first method that does not require any type of
training like the ensemble approaches. But unlike ensemble methods, it is
applicable to any set of source LLMs that have different vocabularies. The
basic idea is to have each source LLM individually generate tokens until the
tokens can be decoded into a text segment that ends at word boundaries common
to all source LLMs. Then, the source LLMs jointly rerank the generated text
segment and select the best one, which is the fused text generation in one
step. Extensive experiments are conducted across a variety of benchmark
datasets. On \emph{GSM8K}, \emph{Cool-Fusion} increases accuracy from three
strong source LLMs by a significant 8\%-17.8\%.","[{'name': 'Cong Liu'}, {'name': 'Xiaojun Quan'}, {'name': 'Yan Pan'}, {'name': 'Liang Lin'}, {'name': 'Weigang Wu'}, {'name': 'Xu Chen'}]",2024-07-29T09:02:19Z
http://arxiv.org/abs/2407.19798v1,http://arxiv.org/abs/2407.19798v1,Teaching LLMs at Charles University: Assignments and Activities,"This paper presents teaching materials, particularly assignments and ideas
for classroom activities, from a new course on large language models (LLMs)
taught at Charles University. The assignments include experiments with LLM
inference for weather report generation and machine translation. The classroom
activities include class quizzes, focused research on downstream tasks and
datasets, and an interactive ""best paper"" session aimed at reading and
comprehension of research papers.","[{'name': 'Jindřich Helcl'}, {'name': 'Zdeněk Kasner'}, {'name': 'Ondřej Dušek'}, {'name': 'Tomasz Limisiewicz'}, {'name': 'Dominik Macháček'}, {'name': 'Tomáš Musil'}, {'name': 'Jindřich Libovický'}]",2024-07-29T08:43:48Z
http://arxiv.org/abs/2407.19795v1,http://arxiv.org/abs/2407.19795v1,"VolDoGer: LLM-assisted Datasets for Domain Generalization in
  Vision-Language Tasks","Domain generalizability is a crucial aspect of a deep learning model since it
determines the capability of the model to perform well on data from unseen
domains. However, research on the domain generalizability of deep learning
models for vision-language tasks remains limited, primarily because of the lack
of required datasets. To address these challenges, we propose VolDoGer:
Vision-Language Dataset for Domain Generalization, a dedicated dataset designed
for domain generalization that addresses three vision-language tasks: image
captioning, visual question answering, and visual entailment. We constructed
VolDoGer by extending LLM-based data annotation techniques to vision-language
tasks, thereby alleviating the burden of recruiting human annotators. We
evaluated the domain generalizability of various models, ranging from
fine-tuned models to a recent multimodal large language model, through
VolDoGer.","[{'name': 'Juhwan Choi'}, {'name': 'Junehyoung Kwon'}, {'name': 'JungMin Yun'}, {'name': 'Seunguk Yu'}, {'name': 'YoungBin Kim'}]",2024-07-29T08:38:46Z
http://arxiv.org/abs/2407.19794v1,http://arxiv.org/abs/2407.19794v1,Introducing a new hyper-parameter for RAG: Context Window Utilization,"This paper introduces a new hyper-parameter for Retrieval-Augmented
Generation (RAG) systems called Context Window Utilization. RAG systems enhance
generative models by incorporating relevant information retrieved from external
knowledge bases, improving the factual accuracy and contextual relevance of
generated responses. The size of the text chunks retrieved and processed is a
critical factor influencing RAG performance. This study aims to identify the
optimal chunk size that maximizes answer generation quality. Through systematic
experimentation, we analyze the effects of varying chunk sizes on the
efficiency and effectiveness of RAG frameworks. Our findings reveal that an
optimal chunk size balances the trade-off between providing sufficient context
and minimizing irrelevant information. These insights are crucial for enhancing
the design and implementation of RAG systems, underscoring the importance of
selecting an appropriate chunk size to achieve superior performance.","[{'name': 'Kush Juvekar'}, {'name': 'Anupam Purwar'}]",2024-07-29T08:38:14Z
http://arxiv.org/abs/2407.19779v1,http://arxiv.org/abs/2407.19779v1,"Synthesizing Scientific Summaries: An Extractive and Abstractive
  Approach","The availability of a vast array of research papers in any area of study,
necessitates the need of automated summarisation systems that can present the
key research conducted and their corresponding findings. Scientific paper
summarisation is a challenging task for various reasons including token length
limits in modern transformer models and corresponding memory and compute
requirements for long text. A significant amount of work has been conducted in
this area, with approaches that modify the attention mechanisms of existing
transformer models and others that utilise discourse information to capture
long range dependencies in research papers. In this paper, we propose a hybrid
methodology for research paper summarisation which incorporates an extractive
and abstractive approach. We use the extractive approach to capture the key
findings of research, and pair it with the introduction of the paper which
captures the motivation for research. We use two models based on unsupervised
learning for the extraction stage and two transformer language models,
resulting in four combinations for our hybrid approach. The performances of the
models are evaluated on three metrics and we present our findings in this
paper. We find that using certain combinations of hyper parameters, it is
possible for automated summarisation systems to exceed the abstractiveness of
summaries written by humans. Finally, we state our future scope of research in
extending this methodology to summarisation of generalised long documents.","[{'name': 'Grishma Sharma'}, {'name': 'Aditi Paretkar'}, {'name': 'Deepak Sharma'}]",2024-07-29T08:21:42Z
http://arxiv.org/abs/2407.19775v1,http://arxiv.org/abs/2407.19775v1,Model Agnostic Hybrid Sharding For Heterogeneous Distributed Inference,"The rapid growth of large-scale AI models, particularly large language models
has brought significant challenges in data privacy, computational resources,
and accessibility. Traditional centralized architectures often struggle to meet
required data security and scalability needs which hinders the democratization
of AI systems. Nesa introduces a model-agnostic sharding framework designed for
decentralized AI inference. Our framework uses blockchain-based sequential deep
neural network sharding to distribute computational tasks across a diverse
network of nodes based on a personalised heuristic and routing mechanism. This
enables efficient distributed training and inference for recent large-scale
models even on consumer-grade hardware. We use compression techniques like
dynamic blockwise quantization and mixed matrix decomposition to reduce data
transfer and memory needs. We also integrate robust security measures,
including hardware-based trusted execution environments to ensure data
integrity and confidentiality. Evaluating our system across various natural
language processing and vision tasks shows that these compression strategies do
not compromise model accuracy. Our results highlight the potential to
democratize access to cutting-edge AI technologies by enabling secure and
efficient inference on a decentralized network.","[{'name': 'Claudio Angione'}, {'name': 'Yue Zhao'}, {'name': 'Harry Yang'}, {'name': 'Ahmad Farhan'}, {'name': 'Fielding Johnston'}, {'name': 'James Buban'}, {'name': 'Patrick Colangelo'}]",2024-07-29T08:18:48Z
http://arxiv.org/abs/2407.19760v2,http://arxiv.org/abs/2407.19760v2,"Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional
  Principles in Complex Scenarios","In this paper, we conduct an empirical analysis of how large language models
(LLMs), specifically GPT-4, interpret constitutional principles in complex
decision-making scenarios. We examine rulings from the Italian Constitutional
Court on bioethics issues that involve trade-offs between competing values and
compare model-generated legal arguments on these issues to those presented by
the State, the Court, and the applicants. Our results indicate that GPT-4
consistently aligns more closely with progressive interpretations of the
Constitution, often overlooking competing values and mirroring the applicants'
views rather than the more conservative perspectives of the State or the
Court's moderate positions. Our experiments reveal a distinct tendency of GPT-4
to favor progressive legal interpretations, underscoring the influence of
underlying data biases. We thus underscore the importance of testing alignment
in real-world scenarios and considering the implications of deploying LLMs in
decision-making processes.","[{'name': 'Camilla Bignotti'}, {'name': 'Carolina Camassa'}]",2024-07-29T07:51:43Z
http://arxiv.org/abs/2407.19740v1,http://arxiv.org/abs/2407.19740v1,"KNOWCOMP POKEMON Team at DialAM-2024: A Two-Stage Pipeline for Detecting
  Relations in Dialogical Argument Mining","Dialogical Argument Mining(DialAM) is an important branch of Argument
Mining(AM). DialAM-2024 is a shared task focusing on dialogical argument
mining, which requires us to identify argumentative relations and illocutionary
relations among proposition nodes and locution nodes. To accomplish this, we
propose a two-stage pipeline, which includes the Two-Step S-Node Prediction
Model in Stage 1 and the YA-Node Prediction Model in Stage 2. We also augment
the training data in both stages and introduce context in Stage 2. We
successfully completed the task and achieved good results. Our team Pokemon
ranked 1st in the ARI Focused score and 4th in the Global Focused score.","[{'name': 'Zihao Zheng'}, {'name': 'Zhaowei Wang'}, {'name': 'Qing Zong'}, {'name': 'Yangqiu Song'}]",2024-07-29T07:07:37Z
http://arxiv.org/abs/2407.19726v2,http://arxiv.org/abs/2407.19726v2,Do Text-to-Vis Benchmarks Test Real Use of Visualisations?,"Large language models are able to generate code for visualisations in
response to user requests. This is a useful application, and an appealing one
for NLP research because plots of data provide grounding for language. However,
there are relatively few benchmarks, and it is unknown whether those that exist
are representative of what people do in practice. This paper aims to answer
that question through an empirical study comparing benchmark datasets and code
from public repositories. Our findings reveal a substantial gap in datasets,
with evaluations not testing the same distribution of chart types, attributes,
and the number of actions. The only representative dataset requires
modification to become an end-to-end and practical benchmark. This shows that
new, more benchmarks are needed to support the development of systems that
truly address users' visualisation needs. These observations will guide future
data creation, highlighting which features hold genuine significance for users.","[{'name': 'Hy Nguyen'}, {'name': 'Xuefei He'}, {'name': 'Andrew Reeson'}, {'name': 'Cecile Paris'}, {'name': 'Josiah Poon'}, {'name': 'Jonathan K. Kummerfeld'}]",2024-07-29T06:13:28Z
http://arxiv.org/abs/2407.19705v2,http://arxiv.org/abs/2407.19705v2,"CollectiveSFT: Scaling Large Language Models for Chinese Medical
  Benchmark with Collective Instructions in Healthcare","The rapid progress in Large Language Models (LLMs) has prompted the creation
of numerous benchmarks to evaluate their capabilities.This study focuses on the
Comprehensive Medical Benchmark in Chinese (CMB), showcasing how dataset
diversity and distribution in supervised fine-tuning (SFT) may enhance LLM
performance.Remarkably, We successfully trained a smaller base model to achieve
scores comparable to larger models, indicating that a diverse and
well-distributed dataset can optimize performance regardless of model size.This
study suggests that even smaller models may reach high performance levels with
carefully curated and varied datasets. By integrating a wide range of
instructional content, our approach addresses potential issues such as data
quality inconsistencies. Our results imply that a broader spectrum of training
data may enhance a model's ability to generalize and perform effectively across
different medical scenarios, highlighting the importance of dataset quality and
diversity in fine-tuning processes. We open-source the model for future
research at https://github.com/CAS-SIAT-XinHai/CollectiveSFT","[{'name': 'Jingwei Zhu'}, {'name': 'Minghuan Tan'}, {'name': 'Min Yang'}, {'name': 'Ruixue Li'}, {'name': 'Hamid Alinejad-Rokny'}]",2024-07-29T05:00:48Z
http://arxiv.org/abs/2407.19687v2,http://arxiv.org/abs/2407.19687v2,"Efficiently and Effectively: A Two-stage Approach to Balance Plaintext
  and Encrypted Text for Traffic Classification","Encrypted traffic classification is the task of identifying the application
or service associated with encrypted network traffic. One effective approach
for this task is to use deep learning methods to encode the raw traffic bytes
directly and automatically extract features for classification (byte-based
models). However, current byte-based models input raw traffic bytes, whether
plaintext or encrypted text, for automated feature extraction, neglecting the
distinct impacts of plaintext and encrypted text on downstream tasks.
Additionally, these models primarily focus on improving classification
accuracy, with little emphasis on the efficiency of models. In this paper, for
the first time, we analyze the impact of plaintext and encrypted text on the
model's effectiveness and efficiency. Based on our observations and findings,
we propose a two-phase approach to balance the trade-off between plaintext and
encrypted text in traffic classification. Specifically, Stage one is to
Determine whether the Plain text is enough to be accurately Classified (DPC)
using the proposed DPC Selector. This stage quickly identifies samples that can
be classified using plaintext, leveraging explicit byte features in plaintext
to enhance model's efficiency. Stage two aims to adaptively make a
classification with the result from stage one. This stage incorporates
encrypted text information for samples that cannot be classified using
plaintext alone, ensuring the model's effectiveness on traffic classification
tasks. Experiments on two datasets demonstrate that our proposed model achieves
state-of-the-art results in both effectiveness and efficiency.",[{'name': 'Wei Peng'}],2024-07-29T04:10:13Z
http://arxiv.org/abs/2407.21072v1,http://arxiv.org/abs/2407.21072v1,"Beyond Metrics: A Critical Analysis of the Variability in Large Language
  Model Evaluation Frameworks","As large language models (LLMs) continue to evolve, the need for robust and
standardized evaluation benchmarks becomes paramount. Evaluating the
performance of these models is a complex challenge that requires careful
consideration of various linguistic tasks, model architectures, and
benchmarking methodologies. In recent years, various frameworks have emerged as
noteworthy contributions to the field, offering comprehensive evaluation tests
and benchmarks for assessing the capabilities of LLMs across diverse domains.
This paper provides an exploration and critical analysis of some of these
evaluation methodologies, shedding light on their strengths, limitations, and
impact on advancing the state-of-the-art in natural language processing.","[{'name': 'Marco AF Pimentel'}, {'name': 'Clément Christophe'}, {'name': 'Tathagata Raha'}, {'name': 'Prateek Munjal'}, {'name': 'Praveen K Kanithi'}, {'name': 'Shadab Khan'}]",2024-07-29T03:37:14Z
http://arxiv.org/abs/2407.19672v1,http://arxiv.org/abs/2407.19672v1,"SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models
  for Southeast Asian Languages","Large Language Models (LLMs) have shown remarkable abilities across various
tasks, yet their development has predominantly centered on high-resource
languages like English and Chinese, leaving low-resource languages underserved.
To address this disparity, we present SeaLLMs 3, the latest iteration of the
SeaLLMs model family, tailored for Southeast Asian languages. This region,
characterized by its rich linguistic diversity, has lacked adequate language
technology support. SeaLLMs 3 aims to bridge this gap by covering a
comprehensive range of languages spoken in this region, including English,
Chinese, Indonesian, Vietnamese, Thai, Tagalog, Malay, Burmese, Khmer, Lao,
Tamil, and Javanese. Leveraging efficient language enhancement techniques and a
specially constructed instruction tuning dataset, SeaLLMs 3 significantly
reduces training costs while maintaining high performance and versatility. Our
model excels in tasks such as world knowledge, mathematical reasoning,
translation, and instruction following, achieving state-of-the-art performance
among similarly sized models. Additionally, we prioritized safety and
reliability by addressing both general and culture-specific considerations and
incorporated mechanisms to reduce hallucinations. This work underscores the
importance of inclusive AI, showing that advanced LLM capabilities can benefit
underserved linguistic and cultural communities.","[{'name': 'Wenxuan Zhang'}, {'name': 'Hou Pong Chan'}, {'name': 'Yiran Zhao'}, {'name': 'Mahani Aljunied'}, {'name': 'Jianyu Wang'}, {'name': 'Chaoqun Liu'}, {'name': 'Yue Deng'}, {'name': 'Zhiqiang Hu'}, {'name': 'Weiwen Xu'}, {'name': 'Yew Ken Chia'}, {'name': 'Xin Li'}, {'name': 'Lidong Bing'}]",2024-07-29T03:26:22Z
http://arxiv.org/abs/2407.19670v1,http://arxiv.org/abs/2407.19670v1,"Overview of PerpectiveArg2024: The First Shared Task on Perspective
  Argument Retrieval","Argument retrieval is the task of finding relevant arguments for a given
query. While existing approaches rely solely on the semantic alignment of
queries and arguments, this first shared task on perspective argument retrieval
incorporates perspectives during retrieval, accounting for latent influences in
argumentation. We present a novel multilingual dataset covering demographic and
socio-cultural (socio) variables, such as age, gender, and political attitude,
representing minority and majority groups in society. We distinguish between
three scenarios to explore how retrieval systems consider explicitly (in both
query and corpus) and implicitly (only in query) formulated perspectives. This
paper provides an overview of this shared task and summarizes the results of
the six submitted systems. We find substantial challenges in incorporating
perspectivism, especially when aiming for personalization based solely on the
text of arguments without explicitly providing socio profiles. Moreover,
retrieval systems tend to be biased towards the majority group but partially
mitigate bias for the female gender. While we bootstrap perspective argument
retrieval, further research is essential to optimize retrieval systems to
facilitate personalization and reduce polarization.","[{'name': 'Neele Falk'}, {'name': 'Andreas Waldis'}, {'name': 'Iryna Gurevych'}]",2024-07-29T03:14:57Z
http://arxiv.org/abs/2407.19669v1,http://arxiv.org/abs/2407.19669v1,"mGTE: Generalized Long-Context Text Representation and Reranking Models
  for Multilingual Text Retrieval","We present systematic efforts in building long-context multilingual text
representation model (TRM) and reranker from scratch for text retrieval. We
first introduce a text encoder (base size) enhanced with RoPE and unpadding,
pre-trained in a native 8192-token context (longer than 512 of previous
multilingual encoders). Then we construct a hybrid TRM and a cross-encoder
reranker by contrastive learning. Evaluations show that our text encoder
outperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM
and reranker match the performance of large-sized state-of-the-art BGE-M3
models and achieve better results on long-context retrieval benchmarks. Further
analysis demonstrate that our proposed models exhibit higher efficiency during
both training and inference. We believe their efficiency and effectiveness
could benefit various researches and industrial applications.","[{'name': 'Xin Zhang'}, {'name': 'Yanzhao Zhang'}, {'name': 'Dingkun Long'}, {'name': 'Wen Xie'}, {'name': 'Ziqi Dai'}, {'name': 'Jialong Tang'}, {'name': 'Huan Lin'}, {'name': 'Baosong Yang'}, {'name': 'Pengjun Xie'}, {'name': 'Fei Huang'}, {'name': 'Meishan Zhang'}, {'name': 'Wenjie Li'}, {'name': 'Min Zhang'}]",2024-07-29T03:12:28Z
http://arxiv.org/abs/2407.19638v1,http://arxiv.org/abs/2407.19638v1,"From Pre-training Corpora to Large Language Models: What Factors
  Influence LLM Performance in Causal Discovery Tasks?","Recent advances in artificial intelligence have seen Large Language Models
(LLMs) demonstrate notable proficiency in causal discovery tasks. This study
explores the factors influencing the performance of LLMs in causal discovery
tasks. Utilizing open-source LLMs, we examine how the frequency of causal
relations within their pre-training corpora affects their ability to accurately
respond to causal discovery queries. Our findings reveal that a higher
frequency of causal mentions correlates with better model performance,
suggesting that extensive exposure to causal information during training
enhances the models' causal discovery capabilities. Additionally, we
investigate the impact of context on the validity of causal relations. Our
results indicate that LLMs might exhibit divergent predictions for identical
causal relations when presented in different contexts. This paper provides the
first comprehensive analysis of how different factors contribute to LLM
performance in causal discovery tasks.","[{'name': 'Tao Feng'}, {'name': 'Lizhen Qu'}, {'name': 'Niket Tandon'}, {'name': 'Zhuang Li'}, {'name': 'Xiaoxi Kang'}, {'name': 'Gholamreza Haffari'}]",2024-07-29T01:45:05Z
http://arxiv.org/abs/2407.19625v1,http://arxiv.org/abs/2407.19625v1,"LoginMEA: Local-to-Global Interaction Network for Multi-modal Entity
  Alignment","Multi-modal entity alignment (MMEA) aims to identify equivalent entities
between two multi-modal knowledge graphs (MMKGs), whose entities can be
associated with relational triples and related images. Most previous studies
treat the graph structure as a special modality, and fuse different modality
information with separate uni-modal encoders, neglecting valuable relational
associations in modalities. Other studies refine each uni-modal information
with graph structures, but may introduce unnecessary relations in specific
modalities. To this end, we propose a novel local-to-global interaction network
for MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal
interactions to generate holistic entity semantics and then refine them with
global relational interactions of entity neighbors. In this design, the
uni-modal information is fused adaptively, and can be refined with relations
accordingly. To enrich local interactions of multi-modal entity information, we
device modality weights and low-rank interactive fusion, allowing diverse
impacts and element-level interactions among modalities. To capture global
interactions of graph structures, we adopt relation reflection graph attention
networks, which fully capture relational associations between entities.
Extensive experiments demonstrate superior results of our method over 5
cross-KG or bilingual benchmark datasets, indicating the effectiveness of
capturing local and global interactions.","[{'name': 'Taoyu Su'}, {'name': 'Xinghua Zhang'}, {'name': 'Jiawei Sheng'}, {'name': 'Zhenyu Zhang'}, {'name': 'Tingwen Liu'}]",2024-07-29T01:06:45Z
http://arxiv.org/abs/2407.19616v1,http://arxiv.org/abs/2407.19616v1,"TopicTag: Automatic Annotation of NMF Topic Models Using Chain of
  Thought and Prompt Tuning with LLMs","Topic modeling is a technique for organizing and extracting themes from large
collections of unstructured text. Non-negative matrix factorization (NMF) is a
common unsupervised approach that decomposes a term frequency-inverse document
frequency (TF-IDF) matrix to uncover latent topics and segment the dataset
accordingly. While useful for highlighting patterns and clustering documents,
NMF does not provide explicit topic labels, necessitating subject matter
experts (SMEs) to assign labels manually. We present a methodology for
automating topic labeling in documents clustered via NMF with automatic model
determination (NMFk). By leveraging the output of NMFk and employing prompt
engineering, we utilize large language models (LLMs) to generate accurate topic
labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs
demonstrates the effectiveness of our method in enhancing knowledge management
and document organization.","[{'name': 'Selma Wanna'}, {'name': 'Ryan Barron'}, {'name': 'Nick Solovyev'}, {'name': 'Maksim E. Eren'}, {'name': 'Manish Bhattarai'}, {'name': 'Kim Rasmussen'}, {'name': 'Boian S. Alexandrov'}]",2024-07-29T00:18:17Z
http://arxiv.org/abs/2407.19600v1,http://arxiv.org/abs/2407.19600v1,"You shall know a piece by the company it keeps. Chess plays as a data
  for word2vec models","In this paper, I apply linguistic methods of analysis to non-linguistic data,
chess plays, metaphorically equating one with the other and seeking analogies.
Chess game notations are also a kind of text, and one can consider the records
of moves or positions of pieces as words and statements in a certain language.
In this article I show how word embeddings (word2vec) can work on chess game
texts instead of natural language texts. I don't see how this representation of
chess data can be used productively. It's unlikely that these vector models
will help engines or people choose the best move. But in a purely academic
sense, it's clear that such methods of information representation capture
something important about the very nature of the game, which doesn't
necessarily lead to a win.",[{'name': 'Boris Orekhov'}],2024-07-28T22:12:36Z
http://arxiv.org/abs/2407.19594v2,http://arxiv.org/abs/2407.19594v2,"Meta-Rewarding Language Models: Self-Improving Alignment with
  LLM-as-a-Meta-Judge","Large Language Models (LLMs) are rapidly surpassing human knowledge in many
domains. While improving these models traditionally relies on costly human
data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs
can improve by judging their own responses instead of relying on human
labelers. However, existing methods have primarily focused on improving model
responses rather than judgment capabilities, resulting in rapid saturation
during iterative training. To address this issue, we introduce a novel
Meta-Rewarding step to the self-improvement process, where the model judges its
own judgements and uses that feedback to refine its judgment skills.
Surprisingly, this unsupervised approach improves the model's ability to judge
{\em and} follow instructions, as demonstrated by a win rate improvement of
Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on
Arena-Hard. These results strongly suggest the potential for self-improving
models without human supervision.","[{'name': 'Tianhao Wu'}, {'name': 'Weizhe Yuan'}, {'name': 'Olga Golovneva'}, {'name': 'Jing Xu'}, {'name': 'Yuandong Tian'}, {'name': 'Jiantao Jiao'}, {'name': 'Jason Weston'}, {'name': 'Sainbayar Sukhbaatar'}]",2024-07-28T21:58:28Z
http://arxiv.org/abs/2407.19584v1,http://arxiv.org/abs/2407.19584v1,"SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal
  Domain","In this paper, we introduce SaulLM-54B and SaulLM-141B, two large language
models (LLMs) tailored for the legal sector. These models, which feature
architectures of 54 billion and 141 billion parameters, respectively, are based
on the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is
guided by large-scale domain adaptation, divided into three strategies: (1) the
exploitation of continued pretraining involving a base corpus that includes
over 540 billion of legal tokens, (2) the implementation of a specialized legal
instruction-following protocol, and (3) the alignment of model outputs with
human preferences in legal interpretations. The integration of synthetically
generated data in the second and third steps enhances the models' capabilities
in interpreting and processing legal texts, effectively reaching
state-of-the-art performance and outperforming previous open-source models on
LegalBench-Instruct. This work explores the trade-offs involved in
domain-specific adaptation at this scale, offering insights that may inform
future studies on domain adaptation using strong decoder models. Building upon
SaulLM-7B, this study refines the approach to produce an LLM better equipped
for legal tasks. We are releasing base, instruct, and aligned versions on top
of SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and
collaborative research.","[{'name': 'Pierre Colombo'}, {'name': 'Telmo Pires'}, {'name': 'Malik Boudiaf'}, {'name': 'Rui Melo'}, {'name': 'Dominic Culver'}, {'name': 'Sofia Morgado'}, {'name': 'Etienne Malaboeuf'}, {'name': 'Gabriel Hautreux'}, {'name': 'Johanne Charpentier'}, {'name': 'Michael Desa'}]",2024-07-28T20:50:53Z
http://arxiv.org/abs/2407.19580v1,http://arxiv.org/abs/2407.19580v1,Memory-efficient Training of LLMs with Larger Mini-batches,"Training with larger mini-batches improves the performance and convergence
rate of training machine learning models. However, training with large
mini-batches becomes prohibitive for Large Language Models (LLMs) with billions
of parameters, due to the large GPU memory requirement. To address this
problem, we propose finding small mini-batches that simulate the dynamics of
training with larger mini-batches. Specifically, we formulate selecting smaller
mini-batches of examples that closely capture gradients of large mini-batches
as a submodular maximization problem. Nevertheless, the very large
dimensionality of the gradients makes the problem very challenging to solve. To
address this, we leverage ideas from zeroth-order optimization and neural
network pruning to find lower-dimensional gradient estimates that allow finding
high-quality subsets effectively with a limited amount of memory. We prove the
superior convergence rate of training on the small mini-batches found by our
method and empirically show its effectiveness. Our method can effectively
reduce the memory requirement by 2x and speed up training by 1.3x, as we
confirm for fine-tuning Phi-2 on MathInstruct. Our method can be easily stacked
with LoRA and other memory-efficient methods to further reduce the memory
requirements of training LLMs.","[{'name': 'Dang Nguyen'}, {'name': 'Wenhan Yang'}, {'name': 'Rathul Anand'}, {'name': 'Yu Yang'}, {'name': 'Baharan Mirzasoleiman'}]",2024-07-28T20:39:16Z
http://arxiv.org/abs/2407.19568v1,http://arxiv.org/abs/2407.19568v1,Are LLMs Good Annotators for Discourse-level Event Relation Extraction?,"Large Language Models (LLMs) have demonstrated proficiency in a wide array of
natural language processing tasks. However, its effectiveness over
discourse-level event relation extraction (ERE) tasks remains unexplored. In
this paper, we assess the effectiveness of LLMs in addressing discourse-level
ERE tasks characterized by lengthy documents and intricate relations
encompassing coreference, temporal, causal, and subevent types. Evaluation is
conducted using an commercial model, GPT-3.5, and an open-source model,
LLaMA-2. Our study reveals a notable underperformance of LLMs compared to the
baseline established through supervised learning. Although Supervised
Fine-Tuning (SFT) can improve LLMs performance, it does not scale well compared
to the smaller supervised baseline model. Our quantitative and qualitative
analysis shows that LLMs have several weaknesses when applied for extracting
event relations, including a tendency to fabricate event mentions, and failures
to capture transitivity rules among relations, detect long distance relations,
or comprehend contexts with dense event mentions.","[{'name': 'Kangda Wei'}, {'name': 'Aayush Gautam'}, {'name': 'Ruihong Huang'}]",2024-07-28T19:27:06Z
http://arxiv.org/abs/2407.21070v2,http://arxiv.org/abs/2407.21070v2,Occam's Razor and Bender and Koller's Octopus,"We discuss the teaching of the discussion surrounding Bender and Koller's
prominent ACL 2020 paper, ""Climbing toward NLU: on meaning form, and
understanding in the age of data"" \cite{bender2020climbing}. We present what we
understand to be the main contentions of the paper, and then recommend that the
students engage with the natural counter-arguments to the claims in the paper.
We attach teaching materials that we use to facilitate teaching this topic to
undergraduate students.",[{'name': 'Michael Guerzhoy'}],2024-07-28T18:33:58Z
http://arxiv.org/abs/2407.19528v1,http://arxiv.org/abs/2407.19528v1,"Motamot: A Dataset for Revealing the Supremacy of Large Language Models
  over Transformer Models in Bengali Political Sentiment Analysis","Sentiment analysis is the process of identifying and categorizing people's
emotions or opinions regarding various topics. Analyzing political sentiment is
critical for understanding the complexities of public opinion processes,
especially during election seasons. It gives significant information on voter
preferences, attitudes, and current trends. In this study, we investigate
political sentiment analysis during Bangladeshi elections, specifically
examining how effectively Pre-trained Language Models (PLMs) and Large Language
Models (LLMs) capture complex sentiment characteristics. Our study centers on
the creation of the ""Motamot"" dataset, comprising 7,058 instances annotated
with positive and negative sentiments, sourced from diverse online newspaper
portals, forming a comprehensive resource for political sentiment analysis. We
meticulously evaluate the performance of various PLMs including BanglaBERT,
Bangla BERT Base, XLM-RoBERTa, mBERT, and sahajBERT, alongside LLMs such as
Gemini 1.5 Pro and GPT 3.5 Turbo. Moreover, we explore zero-shot and few-shot
learning strategies to enhance our understanding of political sentiment
analysis methodologies. Our findings underscore BanglaBERT's commendable
accuracy of 88.10% among PLMs. However, the exploration into LLMs reveals even
more promising results. Through the adept application of Few-Shot learning
techniques, Gemini 1.5 Pro achieves an impressive accuracy of 96.33%,
surpassing the remarkable performance of GPT 3.5 Turbo, which stands at 94%.
This underscores Gemini 1.5 Pro's status as the superior performer in this
comparison.","[{'name': 'Fatema Tuj Johora Faria'}, {'name': 'Mukaffi Bin Moin'}, {'name': 'Rabeya Islam Mumu'}, {'name': 'Md Mahabubul Alam Abir'}, {'name': 'Abrar Nawar Alfy'}, {'name': 'Mohammad Shafiul Alam'}]",2024-07-28T16:34:53Z
http://arxiv.org/abs/2407.19527v1,http://arxiv.org/abs/2407.19527v1,"Open Sentence Embeddings for Portuguese with the Serafim PT* encoders
  family","Sentence encoder encode the semantics of their input, enabling key downstream
applications such as classification, clustering, or retrieval. In this paper,
we present Serafim PT*, a family of open-source sentence encoders for
Portuguese with various sizes, suited to different hardware/compute budgets.
Each model exhibits state-of-the-art performance and is made openly available
under a permissive license, allowing its use for both commercial and research
purposes. Besides the sentence encoders, this paper contributes a systematic
study and lessons learned concerning the selection criteria of learning
objectives and parameters that support top-performing encoders.","[{'name': 'Luís Gomes'}, {'name': 'António Branco'}, {'name': 'João Silva'}, {'name': 'João Rodrigues'}, {'name': 'Rodrigo Santos'}]",2024-07-28T16:34:25Z
http://arxiv.org/abs/2407.19526v1,http://arxiv.org/abs/2407.19526v1,Impact of Decoding Methods on Human Alignment of Conversational LLMs,"To be included into chatbot systems, Large language models (LLMs) must be
aligned with human conversational conventions. However, being trained mainly on
web-scraped data gives existing LLMs a voice closer to informational text than
actual human speech. In this paper, we examine the effect of decoding methods
on the alignment between LLM-generated and human conversations, including Beam
Search, Top K Sampling, and Nucleus Sampling. We present new measures of
alignment in substance, style, and psychometric orientation, and experiment
with two conversation datasets. Our results provide subtle insights: better
alignment is attributed to fewer beams in Beam Search and lower values of P in
Nucleus Sampling. We also find that task-oriented and open-ended datasets
perform differently in terms of alignment, indicating the significance of
taking into account the context of the interaction.","[{'name': 'Shaz Furniturewala'}, {'name': 'Kokil Jaidka'}, {'name': 'Yashvardhan Sharma'}]",2024-07-28T16:31:09Z
http://arxiv.org/abs/2408.00016v1,http://arxiv.org/abs/2408.00016v1,Towards a Universal Method for Meaningful Signal Detection,"It is known that human speech and certain animal vocalizations can convey
meaningful content because we can decipher the content that a given utterance
does convey. This paper explores an alternative approach to determining whether
a signal is meaningful, one that analyzes only the signal itself and is
independent of what the conveyed meaning might be. We devise a method that
takes a waveform as input and outputs a score indicating its degree of
`meaningfulness`. We cluster contiguous portions of the input to minimize the
total description length, and then take the length of the code of the assigned
cluster labels as meaningfulness score. We evaluate our method empirically,
against several baselines, and show that it is the only one to give a high
score to human speech in various languages and with various speakers, a
moderate score to animal vocalizations from birds and orcas, and a low score to
ambient noise from various sources.",[{'name': 'Louis Mahon'}],2024-07-28T15:45:08Z
http://arxiv.org/abs/2407.21068v1,http://arxiv.org/abs/2407.21068v1,"Exploring Genre and Success Classification through Song Lyrics using
  DistilBERT: A Fun NLP Venture","This paper presents a natural language processing (NLP) approach to the
problem of thoroughly comprehending song lyrics, with particular attention on
genre classification, view-based success prediction, and approximate release
year. Our tests provide promising results with 65\% accuracy in genre
classification and 79\% accuracy in success prediction, leveraging a DistilBERT
model for genre classification and BERT embeddings for release year prediction.
Support Vector Machines outperformed other models in predicting the release
year, achieving the lowest root mean squared error (RMSE) of 14.18. Our study
offers insights that have the potential to revolutionize our relationship with
music by addressing the shortcomings of current approaches in properly
understanding the emotional intricacies of song lyrics.","[{'name': 'Servando Pizarro Martinez'}, {'name': 'Moritz Zimmermann'}, {'name': 'Miguel Serkan Offermann'}, {'name': 'Florian Reither'}]",2024-07-28T13:35:03Z
http://arxiv.org/abs/2407.19474v1,http://arxiv.org/abs/2407.19474v1,"Visual Riddles: a Commonsense and World Knowledge Challenge for Large
  Vision and Language Models","Imagine observing someone scratching their arm; to understand why, additional
context would be necessary. However, spotting a mosquito nearby would
immediately offer a likely explanation for the person's discomfort, thereby
alleviating the need for further information. This example illustrates how
subtle visual cues can challenge our cognitive skills and demonstrates the
complexity of interpreting visual scenarios. To study these skills, we present
Visual Riddles, a benchmark aimed to test vision and language models on visual
riddles requiring commonsense and world knowledge. The benchmark comprises 400
visual riddles, each featuring a unique image created by a variety of
text-to-image models, question, ground-truth answer, textual hint, and
attribution. Human evaluation reveals that existing models lag significantly
behind human performance, which is at 82\% accuracy, with Gemini-Pro-1.5
leading with 40\% accuracy. Our benchmark comes with automatic evaluation tasks
to make assessment scalable. These findings underscore the potential of Visual
Riddles as a valuable resource for enhancing vision and language models'
capabilities in interpreting complex visual scenarios.","[{'name': 'Nitzan Bitton-Guetta'}, {'name': 'Aviv Slobodkin'}, {'name': 'Aviya Maimon'}, {'name': 'Eliya Habba'}, {'name': 'Royi Rassin'}, {'name': 'Yonatan Bitton'}, {'name': 'Idan Szpektor'}, {'name': 'Amir Globerson'}, {'name': 'Yuval Elovici'}]",2024-07-28T11:56:03Z
http://arxiv.org/abs/2407.19435v1,http://arxiv.org/abs/2407.19435v1,"ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon
  Intention Understanding","Surgical instrument segmentation is crucial in surgical scene understanding,
thereby facilitating surgical safety. Existing algorithms directly detected all
instruments of pre-defined categories in the input image, lacking the
capability to segment specific instruments according to the surgeon's
intention. During different stages of surgery, surgeons exhibit varying
preferences and focus toward different surgical instruments. Therefore, an
instrument segmentation algorithm that adheres to the surgeon's intention can
minimize distractions from irrelevant instruments and assist surgeons to a
great extent. The recent Segment Anything Model (SAM) reveals the capability to
segment objects following prompts, but the manual annotations for prompts are
impractical during the surgery. To address these limitations in operating
rooms, we propose an audio-driven surgical instrument segmentation framework,
named ASI-Seg, to accurately segment the required surgical instruments by
parsing the audio commands of surgeons. Specifically, we propose an
intention-oriented multimodal fusion to interpret the segmentation intention
from audio commands and retrieve relevant instrument details to facilitate
segmentation. Moreover, to guide our ASI-Seg segment of the required surgical
instruments, we devise a contrastive learning prompt encoder to effectively
distinguish the required instruments from the irrelevant ones. Therefore, our
ASI-Seg promotes the workflow in the operating rooms, thereby providing
targeted support and reducing the cognitive load on surgeons. Extensive
experiments are performed to validate the ASI-Seg framework, which reveals
remarkable advantages over classical state-of-the-art and medical SAMs in both
semantic segmentation and intention-oriented segmentation. The source code is
available at https://github.com/Zonmgin-Zhang/ASI-Seg.","[{'name': 'Zhen Chen'}, {'name': 'Zongming Zhang'}, {'name': 'Wenwu Guo'}, {'name': 'Xingjian Luo'}, {'name': 'Long Bai'}, {'name': 'Jinlin Wu'}, {'name': 'Hongliang Ren'}, {'name': 'Hongbin Liu'}]",2024-07-28T09:25:59Z
http://arxiv.org/abs/2407.19409v1,http://arxiv.org/abs/2407.19409v1,LLAVADI: What Matters For Multimodal Large Language Models Distillation,"The recent surge in Multimodal Large Language Models (MLLMs) has showcased
their remarkable potential for achieving generalized intelligence by
integrating visual understanding into Large Language Models.Nevertheless, the
sheer model size of MLLMs leads to substantial memory and computational demands
that hinder their widespread deployment. In this work, we do not propose a new
efficient model structure or train small-scale MLLMs from scratch. Instead, we
focus on what matters for training small-scale MLLMs through knowledge
distillation, which is the first step from the multimodal distillation
perspective. Our extensive studies involve training strategies, model choices,
and distillation algorithms in the knowledge distillation process. These
results show that joint alignment for both tokens and logit alignment plays
critical roles in teacher-student frameworks. In addition, we draw a series of
intriguing observations from this study. By evaluating different benchmarks and
proper strategy, even a 2.7B small-scale model can perform on par with larger
models with 7B or 13B parameters. Our code and models will be publicly
available for further research.","[{'name': 'Shilin Xu'}, {'name': 'Xiangtai Li'}, {'name': 'Haobo Yuan'}, {'name': 'Lu Qi'}, {'name': 'Yunhai Tong'}, {'name': 'Ming-Hsuan Yang'}]",2024-07-28T06:10:47Z
http://arxiv.org/abs/2407.21066v1,http://arxiv.org/abs/2407.21066v1,"ELP-Adapters: Parameter Efficient Adapter Tuning for Various Speech
  Processing Tasks","Self-supervised learning has emerged as a key approach for learning generic
representations from speech data. Despite promising results in downstream tasks
such as speech recognition, speaker verification, and emotion recognition, a
significant number of parameters is required, which makes fine-tuning for each
task memory-inefficient. To address this limitation, we introduce ELP-adapter
tuning, a novel method for parameter-efficient fine-tuning using three types of
adapter, namely encoder adapters (E-adapters), layer adapters (L-adapters), and
a prompt adapter (P-adapter). The E-adapters are integrated into
transformer-based encoder layers and help to learn fine-grained speech
representations that are effective for speech recognition. The L-adapters
create paths from each encoder layer to the downstream head and help to extract
non-linguistic features from lower encoder layers that are effective for
speaker verification and emotion recognition. The P-adapter appends pseudo
features to CNN features to further improve effectiveness and efficiency. With
these adapters, models can be quickly adapted to various speech processing
tasks. Our evaluation across four downstream tasks using five backbone models
demonstrated the effectiveness of the proposed method. With the WavLM backbone,
its performance was comparable to or better than that of full fine-tuning on
all tasks while requiring 90% fewer learnable parameters.","[{'name': 'Nakamasa Inoue'}, {'name': 'Shinta Otake'}, {'name': 'Takumi Hirose'}, {'name': 'Masanari Ohi'}, {'name': 'Rei Kawakami'}]",2024-07-28T05:26:03Z
http://arxiv.org/abs/2407.19400v1,http://arxiv.org/abs/2407.19400v1,"Word Segmentation for Asian Languages: Chinese, Korean, and Japanese","We provide a detailed overview of various approaches to word segmentation of
Asian Languages, specifically Chinese, Korean, and Japanese languages. For each
language, approaches to deal with word segmentation differs. We also include
our analysis about certain advantages and disadvantages to each method. In
addition, there is room for future work in this field.","[{'name': 'Matthew Rho'}, {'name': 'Yexin Tian'}, {'name': 'Qin Chen'}]",2024-07-28T05:06:58Z
http://arxiv.org/abs/2407.19346v1,http://arxiv.org/abs/2407.19346v1,"Polynomial Regression as a Task for Understanding In-context Learning
  Through Finetuning and Alignment","Simple function classes have emerged as toy problems to better understand
in-context-learning in transformer-based architectures used for large language
models. But previously proposed simple function classes like linear regression
or multi-layer-perceptrons lack the structure required to explore things like
prompting and alignment within models capable of in-context-learning. We
propose univariate polynomial regression as a function class that is just rich
enough to study prompting and alignment, while allowing us to visualize and
understand what is going on clearly.","[{'name': 'Max Wilcoxson'}, {'name': 'Morten Svendgård'}, {'name': 'Ria Doshi'}, {'name': 'Dylan Davis'}, {'name': 'Reya Vir'}, {'name': 'Anant Sahai'}]",2024-07-27T22:00:52Z
http://arxiv.org/abs/2407.19345v1,http://arxiv.org/abs/2407.19345v1,Inference-Time Selective Debiasing,"We propose selective debiasing -- an inference-time safety mechanism that
aims to increase the overall quality of models in terms of prediction
performance and fairness in the situation when re-training a model is
prohibitive. The method is inspired by selective prediction, where some
predictions that are considered low quality are discarded at inference time. In
our approach, we identify the potentially biased model predictions and, instead
of discarding them, we debias them using LEACE -- a post-processing debiasing
method. To select problematic predictions, we propose a bias quantification
approach based on KL divergence, which achieves better results than standard UQ
methods. Experiments with text classification datasets demonstrate that
selective debiasing helps to close the performance gap between post-processing
methods and at-training and pre-processing debiasing techniques.","[{'name': 'Gleb Kuzmin'}, {'name': 'Nemeesh Yadav'}, {'name': 'Ivan Smirnov'}, {'name': 'Timothy Baldwin'}, {'name': 'Artem Shelmanov'}]",2024-07-27T21:56:23Z
http://arxiv.org/abs/2407.21065v1,http://arxiv.org/abs/2407.21065v1,LawLLM: Law Large Language Model for the US Legal System,"In the rapidly evolving field of legal analytics, finding relevant cases and
accurately predicting judicial outcomes are challenging because of the
complexity of legal language, which often includes specialized terminology,
complex syntax, and historical context. Moreover, the subtle distinctions
between similar and precedent cases require a deep understanding of legal
knowledge. Researchers often conflate these concepts, making it difficult to
develop specialized techniques to effectively address these nuanced tasks. In
this paper, we introduce the Law Large Language Model (LawLLM), a multi-task
model specifically designed for the US legal domain to address these
challenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case
Recommendation (PCR), and Legal Judgment Prediction (LJP). By clearly
distinguishing between precedent and similar cases, we provide essential
clarity, guiding future research in developing specialized strategies for these
tasks. We propose customized data preprocessing techniques for each task that
transform raw legal data into a trainable format. Furthermore, we also use
techniques such as in-context learning (ICL) and advanced information retrieval
methods in LawLLM. The evaluation results demonstrate that LawLLM consistently
outperforms existing baselines in both zero-shot and few-shot scenarios,
offering unparalleled multi-task capabilities and filling critical gaps in the
legal domain.","[{'name': 'Dong Shu'}, {'name': 'Haoran Zhao'}, {'name': 'Xukun Liu'}, {'name': 'David Demeter'}, {'name': 'Mengnan Du'}, {'name': 'Yongfeng Zhang'}]",2024-07-27T21:51:30Z
http://arxiv.org/abs/2407.19342v1,http://arxiv.org/abs/2407.19342v1,Parameter-Efficient Fine-Tuning via Circular Convolution,"Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large
foundation models, leveraging low-rank matrices $\mathbf{A}$ and $\mathbf{B}$
to represent weight changes (\textit{i.e.,} $\Delta \mathbf{W} = \mathbf{B}
\mathbf{A}$). This method reduces trainable parameters and mitigates heavy
memory consumption associated with full delta matrices by sequentially
multiplying $\mathbf{A}$ and $\mathbf{B}$ with the activation. Despite its
success, the intrinsic low-rank characteristic may limit its performance.
Although several variants have been proposed to address this issue, they often
overlook the crucial computational and memory efficiency brought by LoRA. In
this paper, we propose \underline{C}ir\underline{c}ular \underline{C}onvolution
\underline{A}daptation (C$^3$A), which not only achieves high-rank adaptation
with enhanced performance but also excels in both computational power and
memory utilization. Extensive experiments demonstrate that C$^3$A consistently
outperforms LoRA and its variants across various fine-tuning tasks.","[{'name': 'Aochuan Chen'}, {'name': 'Ziqi Gao'}, {'name': 'Zijing Liu'}, {'name': 'Yu Li'}, {'name': 'Jia Li'}]",2024-07-27T21:12:46Z
http://arxiv.org/abs/2407.19325v1,http://arxiv.org/abs/2407.19325v1,Do Language Models Have a Critical Period for Language Acquisition?,"Humans appear to have a critical period (CP) for language acquisition: Second
language (L2) acquisition becomes harder after early childhood, and ceasing
exposure to a first language (L1) after this period (but not before) typically
does not lead to substantial loss of L1 proficiency. It is unknown whether
these CP effects result from innately determined brain maturation or as a
stabilization of neural connections naturally induced by experience. In this
study, we use language models (LMs) to test the extent to which these phenomena
are peculiar to humans, or shared by a broader class of language learners. We
vary the age of exposure by training LMs on language pairs in various
experimental conditions, and find that LMs, which lack any direct analog to
innate maturational stages, do not show CP effects when trained sequentially on
L1 and L2. Our results contradict the claim that CP effects are an inevitable
result of learning in statistical learners, and they are consistent with an
innate mechanism for CP effects. We show that we can reverse-engineer the CP by
introducing a regularizer partway through training to simulate a maturational
decrease in plasticity. All in all, our results suggest that L1 learning on its
own may not be enough to induce a CP, and additional engineering is necessary
to make language models more cognitively plausible.","[{'name': 'Ionut Constantinescu'}, {'name': 'Tiago Pimentel'}, {'name': 'Ryan Cotterell'}, {'name': 'Alex Warstadt'}]",2024-07-27T19:17:10Z
http://arxiv.org/abs/2407.19302v1,http://arxiv.org/abs/2407.19302v1,"IBMEA: Exploring Variational Information Bottleneck for Multi-modal
  Entity Alignment","Multi-modal entity alignment (MMEA) aims to identify equivalent entities
between multi-modal knowledge graphs (MMKGs), where the entities can be
associated with related images. Most existing studies integrate multi-modal
information heavily relying on the automatically-learned fusion module, rarely
suppressing the redundant information for MMEA explicitly. To this end, we
explore variational information bottleneck for multi-modal entity alignment
(IBMEA), which emphasizes the alignment-relevant information and suppresses the
alignment-irrelevant information in generating entity representations.
Specifically, we devise multi-modal variational encoders to generate
modal-specific entity representations as probability distributions. Then, we
propose four modal-specific information bottleneck regularizers, limiting the
misleading clues in refining modal-specific entity representations. Finally, we
propose a modal-hybrid information contrastive regularizer to integrate all the
refined modal-specific representations, enhancing the entity similarity between
MMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and
three bilingual MMEA datasets. Experimental results demonstrate that our model
consistently outperforms previous state-of-the-art methods, and also shows
promising and robust performance in low-resource and high-noise data scenarios.","[{'name': 'Taoyu Su'}, {'name': 'Jiawei Sheng'}, {'name': 'Shicheng Wang'}, {'name': 'Xinghua Zhang'}, {'name': 'Hongbo Xu'}, {'name': 'Tingwen Liu'}]",2024-07-27T17:12:37Z
http://arxiv.org/abs/2407.19299v1,http://arxiv.org/abs/2407.19299v1,"The Impact of LoRA Adapters for LLMs on Clinical NLP Classification
  Under Data Limitations","Fine-tuning Large Language Models (LLMs) for clinical Natural Language
Processing (NLP) poses significant challenges due to the domain gap and limited
data availability. This study investigates the effectiveness of various adapter
techniques, equivalent to Low-Rank Adaptation (LoRA), for fine-tuning LLMs in a
resource-constrained hospital environment. We experimented with four
structures-Adapter, Lightweight, TinyAttention, and Gated Residual Network
(GRN)-as final layers for clinical notes classification. We fine-tuned
biomedical pre-trained models, including CamemBERT-bio, AliBERT, and DrBERT,
alongside two Transformer-based models. Our extensive experimental results
indicate that i) employing adapter structures does not yield significant
improvements in fine-tuning biomedical pre-trained LLMs, and ii) simpler
Transformer-based models, trained from scratch, perform better under resource
constraints. Among the adapter structures, GRN demonstrated superior
performance with accuracy, precision, recall, and an F1 score of 0.88.
Moreover, the total training time for LLMs exceeded 1000 hours, compared to
under 6 hours for simpler transformer-based models, highlighting that LLMs are
more suitable for environments with extensive computational resources and
larger datasets. Consequently, this study demonstrates that simpler
Transformer-based models can be effectively trained from scratch, providing a
viable solution for clinical NLP tasks in low-resource environments with
limited data availability. By identifying the GRN as the most effective adapter
structure, we offer a practical approach to enhance clinical note
classification without requiring extensive computational resources.","[{'name': 'Thanh-Dung Le'}, {'name': 'Ti Ti Nguyen'}, {'name': 'Vu Nguyen Ha'}]",2024-07-27T16:48:03Z
http://arxiv.org/abs/2407.19262v1,http://arxiv.org/abs/2407.19262v1,"Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and
  Implications","Understanding whether and to what extent large language models (LLMs) have
memorised training data has important implications for the reliability of their
output and the privacy of their training data. In order to cleanly measure and
disentangle memorisation from other phenomena (e.g. in-context learning), we
create an experimental framework that is based on repeatedly exposing LLMs to
random strings. Our framework allows us to better understand the dynamics,
i.e., the behaviour of the model, when repeatedly exposing it to random
strings. Using our framework, we make several striking observations: (a) we
find consistent phases of the dynamics across families of models (Pythia, Phi
and Llama2), (b) we identify factors that make some strings easier to memorise
than others, and (c) we identify the role of local prefixes and global context
in memorisation. We also show that sequential exposition to different random
strings has a significant effect on memorisation. Our results, often
surprising, have significant downstream implications in the study and usage of
LLMs.","[{'name': 'Till Speicher'}, {'name': 'Mohammad Aflah Khan'}, {'name': 'Qinyuan Wu'}, {'name': 'Vedant Nanda'}, {'name': 'Soumi Das'}, {'name': 'Bishwamittra Ghosh'}, {'name': 'Krishna P. Gummadi'}, {'name': 'Evimaria Terzi'}]",2024-07-27T14:00:21Z
http://arxiv.org/abs/2407.19256v1,http://arxiv.org/abs/2407.19256v1,"Stochastic Parrots or ICU Experts? Large Language Models in Critical
  Care Medicine: A Scoping Review","With the rapid development of artificial intelligence (AI), large language
models (LLMs) have shown strong capabilities in natural language understanding,
reasoning, and generation, attracting amounts of research interest in applying
LLMs to health and medicine. Critical care medicine (CCM) provides diagnosis
and treatment for critically ill patients who often require intensive
monitoring and interventions in intensive care units (ICUs). Can LLMs be
applied to CCM? Are LLMs just like stochastic parrots or ICU experts in
assisting clinical decision-making? This scoping review aims to provide a
panoramic portrait of the application of LLMs in CCM. Literature in seven
databases, including PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE
Xplore, and ACM Digital Library, were searched from January 1, 2019, to June
10, 2024. Peer-reviewed journal and conference articles that discussed the
application of LLMs in critical care settings were included. From an initial
619 articles, 24 were selected for final review. This review grouped
applications of LLMs in CCM into three categories: clinical decision support,
medical documentation and reporting, and medical education and doctor-patient
communication. LLMs have advantages in handling unstructured data and do not
require manual feature engineering. Meanwhile, applying LLMs to CCM faces
challenges, including hallucinations, poor interpretability, bias and alignment
challenges, and privacy and ethics issues. Future research should enhance model
reliability and interpretability, integrate up-to-date medical knowledge, and
strengthen privacy and ethical guidelines. As LLMs evolve, they could become
key tools in CCM to help improve patient outcomes and optimize healthcare
delivery. This study is the first review of LLMs in CCM, aiding researchers,
clinicians, and policymakers to understand the current status and future
potentials of LLMs in CCM.","[{'name': 'Tongyue Shi'}, {'name': 'Jun Ma'}, {'name': 'Zihan Yu'}, {'name': 'Haowei Xu'}, {'name': 'Minqi Xiong'}, {'name': 'Meirong Xiao'}, {'name': 'Yilin Li'}, {'name': 'Huiying Zhao'}, {'name': 'Guilan Kong'}]",2024-07-27T13:41:43Z
http://arxiv.org/abs/2407.19200v1,http://arxiv.org/abs/2407.19200v1,"On Behalf of the Stakeholders: Trends in NLP Model Interpretability in
  the Era of LLMs","Recent advancements in NLP systems, particularly with the introduction of
LLMs, have led to widespread adoption of these systems by a broad spectrum of
users across various domains, impacting decision-making, the job market,
society, and scientific research. This surge in usage has led to an explosion
in NLP model interpretability and analysis research, accompanied by numerous
technical surveys. Yet, these surveys often overlook the needs and perspectives
of explanation stakeholders. In this paper, we address three fundamental
questions: Why do we need interpretability, what are we interpreting, and how?
By exploring these questions, we examine existing interpretability paradigms,
their properties, and their relevance to different stakeholders. We further
explore the practical implications of these paradigms by analyzing trends from
the past decade across multiple research fields. To this end, we retrieved
thousands of papers and employed an LLM to characterize them. Our analysis
reveals significant disparities between NLP developers and non-developer users,
as well as between research fields, underscoring the diverse needs of
stakeholders. For example, explanations of internal model components are rarely
used outside the NLP field. We hope this paper informs the future design,
development, and application of methods that align with the objectives and
requirements of various stakeholders.","[{'name': 'Nitay Calderon'}, {'name': 'Roi Reichart'}]",2024-07-27T08:00:27Z
http://arxiv.org/abs/2407.19198v1,http://arxiv.org/abs/2407.19198v1,Towards the Dynamics of a DNN Learning Symbolic Interactions,"This study proves the two-phase dynamics of a deep neural network (DNN)
learning interactions. Despite the long disappointing view of the faithfulness
of post-hoc explanation of a DNN, in recent years, a series of theorems have
been proven to show that given an input sample, a small number of interactions
between input variables can be considered as primitive inference patterns,
which can faithfully represent every detailed inference logic of the DNN on
this sample. Particularly, it has been observed that various DNNs all learn
interactions of different complexities with two-phase dynamics, and this well
explains how a DNN's generalization power changes from under-fitting to
over-fitting. Therefore, in this study, we prove the dynamics of a DNN
gradually encoding interactions of different complexities, which provides a
theoretically grounded mechanism for the over-fitting of a DNN. Experiments
show that our theory well predicts the real learning dynamics of various DNNs
on different tasks.","[{'name': 'Qihan Ren'}, {'name': 'Yang Xu'}, {'name': 'Junpeng Zhang'}, {'name': 'Yue Xin'}, {'name': 'Dongrui Liu'}, {'name': 'Quanshi Zhang'}]",2024-07-27T07:34:49Z
http://arxiv.org/abs/2407.19196v1,http://arxiv.org/abs/2407.19196v1,"Why Misinformation is Created? Detecting them by Integrating Intent
  Features","Various social media platforms, e.g., Twitter and Reddit, allow people to
disseminate a plethora of information more efficiently and conveniently.
However, they are inevitably full of misinformation, causing damage to diverse
aspects of our daily lives. To reduce the negative impact, timely
identification of misinformation, namely Misinformation Detection (MD), has
become an active research topic receiving widespread attention. As a complex
phenomenon, the veracity of an article is influenced by various aspects. In
this paper, we are inspired by the opposition of intents between misinformation
and real information. Accordingly, we propose to reason the intent of articles
and form the corresponding intent features to promote the veracity
discrimination of article features. To achieve this, we build a hierarchy of a
set of intents for both misinformation and real information by referring to the
existing psychological theories, and we apply it to reason the intent of
articles by progressively generating binary answers with an encoder-decoder
structure. We form the corresponding intent features and integrate it with the
token features to achieve more discriminative article features for MD. Upon
these ideas, we suggest a novel MD method, namely Detecting Misinformation by
Integrating Intent featuRes (DM-INTER). To evaluate the performance of
DM-INTER, we conduct extensive experiments on benchmark MD datasets. The
experimental results validate that DM-INTER can outperform the existing
baseline MD methods.","[{'name': 'Bing Wang'}, {'name': 'Ximing Li'}, {'name': 'Changchun Li'}, {'name': 'Bo Fu'}, {'name': 'Songwen Pei'}, {'name': 'Shengsheng Wang'}]",2024-07-27T07:30:47Z
http://arxiv.org/abs/2407.19192v1,http://arxiv.org/abs/2407.19192v1,"Harmfully Manipulated Images Matter in Multimodal Misinformation
  Detection","Nowadays, misinformation is widely spreading over various social media
platforms and causes extremely negative impacts on society. To combat this
issue, automatically identifying misinformation, especially those containing
multimodal content, has attracted growing attention from the academic and
industrial communities, and induced an active research topic named Multimodal
Misinformation Detection (MMD). Typically, existing MMD methods capture the
semantic correlation and inconsistency between multiple modalities, but neglect
some potential clues in multimodal content. Recent studies suggest that
manipulated traces of the images in articles are non-trivial clues for
detecting misinformation. Meanwhile, we find that the underlying intentions
behind the manipulation, e.g., harmful and harmless, also matter in MMD.
Accordingly, in this work, we propose to detect misinformation by learning
manipulation features that indicate whether the image has been manipulated, as
well as intention features regarding the harmful and harmless intentions of the
manipulation. Unfortunately, the manipulation and intention labels that make
these features discriminative are unknown. To overcome the problem, we propose
two weakly supervised signals as alternatives by introducing additional
datasets on image manipulation detection and formulating two classification
tasks as positive and unlabeled learning problems. Based on these ideas, we
propose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD
(HAMI-M3D). Extensive experiments across three benchmark datasets can
demonstrate that HAMI-M3D can consistently improve the performance of any MMD
baselines.","[{'name': 'Bing Wang'}, {'name': 'Shengsheng Wang'}, {'name': 'Changchun Li'}, {'name': 'Renchu Guan'}, {'name': 'Ximing Li'}]",2024-07-27T07:16:07Z
http://arxiv.org/abs/2408.01460v1,http://arxiv.org/abs/2408.01460v1,"LocalValueBench: A Collaboratively Built and Extensible Benchmark for
  Evaluating Localized Value Alignment and Ethical Safety in Large Language
  Models","The proliferation of large language models (LLMs) requires robust evaluation
of their alignment with local values and ethical standards, especially as
existing benchmarks often reflect the cultural, legal, and ideological values
of their creators. \textsc{LocalValueBench}, introduced in this paper, is an
extensible benchmark designed to assess LLMs' adherence to Australian values,
and provides a framework for regulators worldwide to develop their own LLM
benchmarks for local value alignment. Employing a novel typology for ethical
reasoning and an interrogation approach, we curated comprehensive questions and
utilized prompt engineering strategies to probe LLMs' value alignment. Our
evaluation criteria quantified deviations from local values, ensuring a
rigorous assessment process. Comparative analysis of three commercial LLMs by
USA vendors revealed significant insights into their effectiveness and
limitations, demonstrating the critical importance of value alignment. This
study offers valuable tools and methodologies for regulators to create tailored
benchmarks, highlighting avenues for future research to enhance ethical AI
development.","[{'name': 'Gwenyth Isobel Meadows'}, {'name': 'Nicholas Wai Long Lau'}, {'name': 'Eva Adelina Susanto'}, {'name': 'Chi Lok Yu'}, {'name': 'Aditya Paul'}]",2024-07-27T05:55:42Z
http://arxiv.org/abs/2408.01459v1,http://arxiv.org/abs/2408.01459v1,"AgentPeerTalk: Empowering Students through Agentic-AI-Driven Discernment
  of Bullying and Joking in Peer Interactions in Schools","Addressing school bullying effectively and promptly is crucial for the mental
health of students. This study examined the potential of large language models
(LLMs) to empower students by discerning between bullying and joking in school
peer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus,
evaluating their effectiveness through human review. Our results revealed that
not all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the
most promise. We observed variations in LLM outputs, possibly influenced by
political overcorrectness, context window limitations, and pre-existing bias in
their training data. ChatGPT-4 excelled in context-specific accuracy after
implementing the agentic approach, highlighting its potential to provide
continuous, real-time support to vulnerable students. This study underlines the
significant social impact of using agentic AI in educational settings, offering
a new avenue for reducing the negative consequences of bullying and enhancing
student well-being.","[{'name': 'Aditya Paul'}, {'name': 'Chi Lok Yu'}, {'name': 'Eva Adelina Susanto'}, {'name': 'Nicholas Wai Long Lau'}, {'name': 'Gwenyth Isobel Meadows'}]",2024-07-27T05:50:02Z
http://arxiv.org/abs/2407.19173v1,http://arxiv.org/abs/2407.19173v1,"FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity
  Measurement of Persian Social Networks Informal Texts","One fundamental task for NLP is to determine the similarity between two texts
and evaluate the extent of their likeness. The previous methods for the Persian
language have low accuracy and are unable to comprehend the structure and
meaning of texts effectively. Additionally, these methods primarily focus on
formal texts, but in real-world applications of text processing, there is a
need for robust methods that can handle colloquial texts. This requires
algorithms that consider the structure and significance of words based on
context, rather than just the frequency of words. The lack of a proper dataset
for this task in the Persian language makes it important to develop such
algorithms and construct a dataset for Persian text. This paper introduces a
new transformer-based model to measure semantic similarity between Persian
informal short texts from social networks. In addition, a Persian dataset named
FarSSiM has been constructed for this purpose, using real data from social
networks and manually annotated and verified by a linguistic expert team. The
proposed model involves training a large language model using the BERT
architecture from scratch. This model, called FarSSiBERT, is pre-trained on
approximately 104 million Persian informal short texts from social networks,
making it one of a kind in the Persian language. Moreover, a novel specialized
informal language tokenizer is provided that not only performs tokenization on
formal texts well but also accurately identifies tokens that other Persian
tokenizers are unable to recognize. It has been demonstrated that our proposed
model outperforms ParsBERT, laBSE, and multilingual BERT in the Pearson and
Spearman's coefficient criteria. Additionally, the pre-trained large language
model has great potential for use in other NLP tasks on colloquial text and as
a tokenizer for less-known informal words.","[{'name': 'Seyed Mojtaba Sadjadi'}, {'name': 'Zeinab Rajabi'}, {'name': 'Leila Rabiei'}, {'name': 'Mohammad-Shahram Moin'}]",2024-07-27T05:04:49Z
http://arxiv.org/abs/2407.19164v1,http://arxiv.org/abs/2407.19164v1,"Addressing Topic Leakage in Cross-Topic Evaluation for Authorship
  Verification","Authorship verification (AV) aims to identify whether a pair of texts has the
same author. We address the challenge of evaluating AV models' robustness
against topic shifts. The conventional evaluation assumes minimal topic overlap
between training and test data. However, we argue that there can still be topic
leakage in test data, causing misleading model performance and unstable
rankings. To address this, we propose an evaluation method called
Heterogeneity-Informed Topic Sampling (HITS), which creates a smaller dataset
with a heterogeneously distributed topic set. Our experimental results
demonstrate that HITS-sampled datasets yield a more stable ranking of models
across random seeds and evaluation splits. Our contributions include: 1. An
analysis of causes and effects of topic leakage. 2. A demonstration of the HITS
in reducing the effects of topic leakage, and 3. The Robust Authorship
Verification bENchmark (RAVEN) that allows topic shortcut test to uncover AV
models' reliance on topic-specific features.","[{'name': 'Jitkapat Sawatphol'}, {'name': 'Can Udomcharoenchaikit'}, {'name': 'Sarana Nutanong'}]",2024-07-27T04:16:11Z
http://arxiv.org/abs/2407.19089v1,http://arxiv.org/abs/2407.19089v1,Many-Shot In-Context Learning for Molecular Inverse Design,"Large Language Models (LLMs) have demonstrated great performance in few-shot
In-Context Learning (ICL) for a variety of generative and discriminative
chemical design tasks. The newly expanded context windows of LLMs can further
improve ICL capabilities for molecular inverse design and lead optimization. To
take full advantage of these capabilities we developed a new semi-supervised
learning method that overcomes the lack of experimental data available for
many-shot ICL. Our approach involves iterative inclusion of LLM generated
molecules with high predicted performance, along with experimental data. We
further integrated our method in a multi-modal LLM which allows for the
interactive modification of generated molecular structures using text
instructions. As we show, the new method greatly improves upon existing ICL
methods for molecular design while being accessible and easy to use for
scientists.","[{'name': 'Saeed Moayedpour'}, {'name': 'Alejandro Corrochano-Navarro'}, {'name': 'Faryad Sahneh'}, {'name': 'Shahriar Noroozizadeh'}, {'name': 'Alexander Koetter'}, {'name': 'Jiri Vymetal'}, {'name': 'Lorenzo Kogler-Anele'}, {'name': 'Pablo Mas'}, {'name': 'Yasser Jangjou'}, {'name': 'Sizhen Li'}, {'name': 'Michael Bailey'}, {'name': 'Marc Bianciotto'}, {'name': 'Hans Matter'}, {'name': 'Christoph Grebner'}, {'name': 'Gerhard Hessler'}, {'name': 'Ziv Bar-Joseph'}, {'name': 'Sven Jager'}]",2024-07-26T21:10:50Z
http://arxiv.org/abs/2407.19056v1,http://arxiv.org/abs/2407.19056v1,"OfficeBench: Benchmarking Language Agents across Multiple Applications
  for Office Automation","Office automation significantly enhances human productivity by automatically
finishing routine tasks in the workflow. Beyond the basic information
extraction studied in much of the prior document AI literature, the office
automation research should be extended to more realistic office tasks which
require to integrate various information sources in the office system and
produce outputs through a series of decision-making processes. We introduce
OfficeBench, one of the first office automation benchmarks for evaluating
current LLM agents' capability to address office tasks in realistic office
workflows. OfficeBench requires LLM agents to perform feasible long-horizon
planning, proficiently switch between applications in a timely manner, and
accurately ground their actions within a large combined action space, based on
the contextual demands of the workflow. Applying our customized evaluation
methods on each task, we find that GPT-4 Omni achieves the highest pass rate of
47.00%, demonstrating a decent performance in handling office tasks. However,
this is still far below the human performance and accuracy standards required
by real-world office workflows. We further observe that most issues are related
to operation redundancy and hallucinations, as well as limitations in switching
between multiple applications, which may provide valuable insights for
developing effective agent frameworks for office automation.","[{'name': 'Zilong Wang'}, {'name': 'Yuedong Cui'}, {'name': 'Li Zhong'}, {'name': 'Zimin Zhang'}, {'name': 'Da Yin'}, {'name': 'Bill Yuchen Lin'}, {'name': 'Jingbo Shang'}]",2024-07-26T19:27:17Z
http://arxiv.org/abs/2407.19041v1,http://arxiv.org/abs/2407.19041v1,"Optimizing Numerical Estimation and Operational Efficiency in the Legal
  Domain through Large Language Models","The legal landscape encompasses a wide array of lawsuit types, presenting
lawyers with challenges in delivering timely and accurate information to
clients, particularly concerning critical aspects like potential imprisonment
duration or financial repercussions. Compounded by the scarcity of legal
experts, there's an urgent need to enhance the efficiency of traditional legal
workflows. Recent advances in deep learning, especially Large Language Models
(LLMs), offer promising solutions to this challenge. Leveraging LLMs'
mathematical reasoning capabilities, we propose a novel approach integrating
LLM-based methodologies with specially designed prompts to address precision
requirements in legal Artificial Intelligence (LegalAI) applications. The
proposed work seeks to bridge the gap between traditional legal practices and
modern technological advancements, paving the way for a more accessible,
efficient, and equitable legal system. To validate this method, we introduce a
curated dataset tailored to precision-oriented LegalAI tasks, serving as a
benchmark for evaluating LLM-based approaches. Extensive experimentation
confirms the efficacy of our methodology in generating accurate numerical
estimates within the legal domain, emphasizing the role of LLMs in streamlining
legal processes and meeting the evolving demands of LegalAI.","[{'name': 'Jia-Hong Huang'}, {'name': 'Chao-Chun Yang'}, {'name': 'Yixian Shen'}, {'name': 'Alessio M. Pacces'}, {'name': 'Evangelos Kanoulas'}]",2024-07-26T18:46:39Z
http://arxiv.org/abs/2407.18908v1,http://arxiv.org/abs/2407.18908v1,Wolf: Captioning Everything with a World Summarization Framework,"We propose Wolf, a WOrLd summarization Framework for accurate video
captioning. Wolf is an automated captioning framework that adopts a
mixture-of-experts approach, leveraging complementary strengths of Vision
Language Models (VLMs). By utilizing both image and video models, our framework
captures different levels of information and summarizes them efficiently. Our
approach can be applied to enhance video understanding, auto-labeling, and
captioning. To evaluate caption quality, we introduce CapScore, an LLM-based
metric to assess the similarity and quality of generated captions compared to
the ground truth captions. We further build four human-annotated datasets in
three domains: autonomous driving, general scenes, and robotics, to facilitate
comprehensive comparisons. We show that Wolf achieves superior captioning
performance compared to state-of-the-art approaches from the research community
(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For
instance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise
by 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,
we establish a benchmark for video captioning and introduce a leaderboard,
aiming to accelerate advancements in video understanding, captioning, and data
alignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.","[{'name': 'Boyi Li'}, {'name': 'Ligeng Zhu'}, {'name': 'Ran Tian'}, {'name': 'Shuhan Tan'}, {'name': 'Yuxiao Chen'}, {'name': 'Yao Lu'}, {'name': 'Yin Cui'}, {'name': 'Sushant Veer'}, {'name': 'Max Ehrlich'}, {'name': 'Jonah Philion'}, {'name': 'Xinshuo Weng'}, {'name': 'Fuzhao Xue'}, {'name': 'Andrew Tao'}, {'name': 'Ming-Yu Liu'}, {'name': 'Sanja Fidler'}, {'name': 'Boris Ivanovic'}, {'name': 'Trevor Darrell'}, {'name': 'Jitendra Malik'}, {'name': 'Song Han'}, {'name': 'Marco Pavone'}]",2024-07-26T17:59:09Z
http://arxiv.org/abs/2407.18901v1,http://arxiv.org/abs/2407.18901v1,"AppWorld: A Controllable World of Apps and People for Benchmarking
  Interactive Coding Agents","Autonomous agents that address day-to-day digital tasks (e.g., ordering
groceries for a household), must not only operate multiple apps (e.g., notes,
messaging, shopping app) via APIs, but also generate rich code with complex
control flow in an iterative manner based on their interaction with the
environment. However, existing benchmarks for tool use are inadequate, as they
only cover tasks that require a simple sequence of API calls.
  To remedy this gap, we built $\textbf{AppWorld Engine}$, a high-quality
execution environment (60K lines of code) of 9 day-to-day apps operable via 457
APIs and populated with realistic digital activities simulating the lives of
~100 fictitious users. We then created $\textbf{AppWorld Benchmark}$ (40K lines
of code), a suite of 750 natural, diverse, and challenging autonomous agent
tasks requiring rich and interactive code generation. It supports robust
programmatic evaluation with state-based unit tests, allowing for different
ways of completing a task while also checking for unexpected changes, i.e.,
collateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our
'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least
16% fewer. This highlights the benchmark's difficulty and AppWorld's potential
to push the frontiers of interactive coding agents. The project website is
available at https://appworld.dev/.","[{'name': 'Harsh Trivedi'}, {'name': 'Tushar Khot'}, {'name': 'Mareike Hartmann'}, {'name': 'Ruskin Manku'}, {'name': 'Vinty Dong'}, {'name': 'Edward Li'}, {'name': 'Shashank Gupta'}, {'name': 'Ashish Sabharwal'}, {'name': 'Niranjan Balasubramanian'}]",2024-07-26T17:55:45Z
http://arxiv.org/abs/2407.18887v1,http://arxiv.org/abs/2407.18887v1,Embedding And Clustering Your Data Can Improve Contrastive Pretraining,"Recent studies of large-scale contrastive pretraining in the text embedding
domain show that using single-source minibatches, rather than mixed-source
minibatches, can substantially improve overall model accuracy. In this work, we
explore extending training data stratification beyond source granularity by
leveraging a pretrained text embedding model and the classic k-means clustering
algorithm to further split training data apart by the semantic clusters within
each source. Experimentally, we observe a notable increase in NDCG@10 when
pretraining a BERT-based text embedding model on query-passage pairs from the
MSMARCO passage retrieval dataset. Additionally, we conceptually connect our
clustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B
methodology and the nearest-neighbor-based hard-negative mining aspect of the
ANCE methodology and discuss how this unified view motivates future lines of
research on the organization of contrastive pretraining data.",[{'name': 'Luke Merrick'}],2024-07-26T17:36:40Z
http://arxiv.org/abs/2407.18789v1,http://arxiv.org/abs/2407.18789v1,"Granularity is crucial when applying differential privacy to text: An
  investigation for neural machine translation","Applying differential privacy (DP) by means of the DP-SGD algorithm to
protect individual data points during training is becoming increasingly popular
in NLP. However, the choice of granularity at which DP is applied is often
neglected. For example, neural machine translation (NMT) typically operates on
the sentence-level granularity. From the perspective of DP, this setup assumes
that each sentence belongs to a single person and any two sentences in the
training dataset are independent. This assumption is however violated in many
real-world NMT datasets, e.g. those including dialogues. For proper application
of DP we thus must shift from sentences to entire documents. In this paper, we
investigate NMT at both the sentence and document levels, analyzing the
privacy/utility trade-off for both scenarios, and evaluating the risks of not
using the appropriate privacy granularity in terms of leaking personally
identifiable information (PII). Our findings indicate that the document-level
NMT system is more resistant to membership inference attacks, emphasizing the
significance of using the appropriate granularity when working with DP.","[{'name': 'Doan Nam Long Vu'}, {'name': 'Timour Igamberdiev'}, {'name': 'Ivan Habernal'}]",2024-07-26T14:52:37Z
http://arxiv.org/abs/2407.18786v1,http://arxiv.org/abs/2407.18786v1,"The power of Prompts: Evaluating and Mitigating Gender Bias in MT with
  LLMs","This paper studies gender bias in machine translation through the lens of
Large Language Models (LLMs). Four widely-used test sets are employed to
benchmark various base LLMs, comparing their translation quality and gender
bias against state-of-the-art Neural Machine Translation (NMT) models for
English to Catalan (En $\rightarrow$ Ca) and English to Spanish (En
$\rightarrow$ Es) translation directions. Our findings reveal pervasive gender
bias across all models, with base LLMs exhibiting a higher degree of bias
compared to NMT models. To combat this bias, we explore prompting engineering
techniques applied to an instruction-tuned LLM. We identify a prompt structure
that significantly reduces gender bias by up to 12% on the WinoMT evaluation
dataset compared to more straightforward prompts. These results significantly
reduce the gender bias accuracy gap between LLMs and traditional NMT systems.","[{'name': 'Aleix Sant'}, {'name': 'Carlos Escolano'}, {'name': 'Audrey Mash'}, {'name': 'Francesca De Luca Fornaciari'}, {'name': 'Maite Melero'}]",2024-07-26T14:47:31Z
http://arxiv.org/abs/2407.18752v3,http://arxiv.org/abs/2407.18752v3,"Knowledge Graph Structure as Prompt: Improving Small Language Models
  Capabilities for Knowledge-based Causal Discovery","Causal discovery aims to estimate causal structures among variables based on
observational data. Large Language Models (LLMs) offer a fresh perspective to
tackle the causal discovery problem by reasoning on the metadata associated
with variables rather than their actual data values, an approach referred to as
knowledge-based causal discovery. In this paper, we investigate the
capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1
billion parameters) with prompt-based learning for knowledge-based causal
discovery. Specifically, we present KG Structure as Prompt, a novel approach
for integrating structural information from a knowledge graph, such as common
neighbor nodes and metapaths, into prompt-based learning to enhance the
capabilities of SLMs. Experimental results on three types of biomedical and
open-domain datasets under few-shot settings demonstrate the effectiveness of
our approach, surpassing most baselines and even conventional fine-tuning
approaches trained on full datasets. Our findings further highlight the strong
capabilities of SLMs: in combination with knowledge graphs and prompt-based
learning, SLMs demonstrate the potential to surpass LLMs with larger number of
parameters. Our code and datasets are available on GitHub.","[{'name': 'Yuni Susanti'}, {'name': 'Michael Färber'}]",2024-07-26T14:07:00Z
http://arxiv.org/abs/2407.18743v1,http://arxiv.org/abs/2407.18743v1,"Towards Effective and Efficient Continual Pre-training of Large Language
  Models","Continual pre-training (CPT) has been an important approach for adapting
language models to specific domains or tasks. To make the CPT approach more
traceable, this paper presents a technical report for continually pre-training
Llama-3 (8B), which significantly enhances the Chinese language ability and
scientific reasoning ability of the backbone model. To enhance the new
abilities while retaining the original abilities, we design specific data
mixture and curriculum strategies by utilizing existing datasets and
synthesizing high-quality datasets. Specifically, we synthesize
multidisciplinary scientific question and answer (QA) pairs based on related
web pages, and subsequently incorporate these synthetic data to improve the
scientific reasoning ability of Llama-3. We refer to the model after CPT as
Llama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning
experiments with a relatively small model -- TinyLlama, and employ the derived
findings to train the backbone model. Extensive experiments on a number of
evaluation benchmarks show that our approach can largely improve the
performance of the backbone models, including both the general abilities (+8.81
on C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on
MATH and +4.13 on SciEval), without hurting the original capacities. Our model,
data, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.","[{'name': 'Jie Chen'}, {'name': 'Zhipeng Chen'}, {'name': 'Jiapeng Wang'}, {'name': 'Kun Zhou'}, {'name': 'Yutao Zhu'}, {'name': 'Jinhao Jiang'}, {'name': 'Yingqian Min'}, {'name': 'Wayne Xin Zhao'}, {'name': 'Zhicheng Dou'}, {'name': 'Jiaxin Mao'}, {'name': 'Yankai Lin'}, {'name': 'Ruihua Song'}, {'name': 'Jun Xu'}, {'name': 'Xu Chen'}, {'name': 'Rui Yan'}, {'name': 'Zhewei Wei'}, {'name': 'Di Hu'}, {'name': 'Wenbing Huang'}, {'name': 'Ji-Rong Wen'}]",2024-07-26T13:55:21Z
http://arxiv.org/abs/2407.18738v1,http://arxiv.org/abs/2407.18738v1,Towards Generalized Offensive Language Identification,"The prevalence of offensive content on the internet, encompassing hate speech
and cyberbullying, is a pervasive issue worldwide. Consequently, it has
garnered significant attention from the machine learning (ML) and natural
language processing (NLP) communities. As a result, numerous systems have been
developed to automatically identify potentially harmful content and mitigate
its impact. These systems can follow two approaches; (1) Use publicly available
models and application endpoints, including prompting large language models
(LLMs) (2) Annotate datasets and train ML models on them. However, both
approaches lack an understanding of how generalizable they are. Furthermore,
the applicability of these systems is often questioned in off-domain and
practical environments. This paper empirically evaluates the generalizability
of offensive language detection models and datasets across a novel generalized
benchmark. We answer three research questions on generalizability. Our findings
will be useful in creating robust real-world offensive language detection
systems.","[{'name': 'Alphaeus Dmonte'}, {'name': 'Tejas Arya'}, {'name': 'Tharindu Ranasinghe'}, {'name': 'Marcos Zampieri'}]",2024-07-26T13:50:22Z
http://arxiv.org/abs/2407.18730v1,http://arxiv.org/abs/2407.18730v1,"Creating an Aligned Corpus of Sound and Text: The Multimodal Corpus of
  Shakespeare and Milton","In this work we present a corpus of poems by William Shakespeare and John
Milton that have been enriched with readings from the public domain. We have
aligned all the lines with their respective audio segments, at the line, word,
syllable and phone level, and we have included their scansion. We make a basic
visualization platform for these poems and we conclude by conjecturing possible
future directions.",[{'name': 'Manex Agirrezabal'}],2024-07-26T13:30:24Z
http://arxiv.org/abs/2407.18716v1,http://arxiv.org/abs/2407.18716v1,"ChatSchema: A pipeline of extracting structured information with Large
  Multimodal Models based on schema","Objective: This study introduces ChatSchema, an effective method for
extracting and structuring information from unstructured data in medical paper
reports using a combination of Large Multimodal Models (LMMs) and Optical
Character Recognition (OCR) based on the schema. By integrating predefined
schema, we intend to enable LMMs to directly extract and standardize
information according to the schema specifications, facilitating further data
entry. Method: Our approach involves a two-stage process, including
classification and extraction for categorizing report scenarios and structuring
information. We established and annotated a dataset to verify the effectiveness
of ChatSchema, and evaluated key extraction using precision, recall, F1-score,
and accuracy metrics. Based on key extraction, we further assessed value
extraction. We conducted ablation studies on two LMMs to illustrate the
improvement of structured information extraction with different input modals
and methods. Result: We analyzed 100 medical reports from Peking University
First Hospital and established a ground truth dataset with 2,945 key-value
pairs. We evaluated ChatSchema using GPT-4o and Gemini 1.5 Pro and found a
higher overall performance of GPT-4o. The results are as follows: For the
result of key extraction, key-precision was 98.6%, key-recall was 98.5%,
key-F1-score was 98.6%. For the result of value extraction based on correct key
extraction, the overall accuracy was 97.2%, precision was 95.8%, recall was
95.8%, and F1-score was 95.8%. An ablation study demonstrated that ChatSchema
achieved significantly higher overall accuracy and overall F1-score of
key-value extraction, compared to the Baseline, with increases of 26.9% overall
accuracy and 27.4% overall F1-score, respectively.","[{'name': 'Fei Wang'}, {'name': 'Yuewen Zheng'}, {'name': 'Qin Li'}, {'name': 'Jingyi Wu'}, {'name': 'Pengfei Li'}, {'name': 'Luxia Zhang'}]",2024-07-26T13:05:24Z
http://arxiv.org/abs/2407.18712v1,http://arxiv.org/abs/2407.18712v1,Cluster-norm for Unsupervised Probing of Knowledge,"The deployment of language models brings challenges in generating reliable
information, especially when these models are fine-tuned using human
preferences. To extract encoded knowledge without (potentially) biased human
labels, unsupervised probing techniques like Contrast-Consistent Search (CCS)
have been developed (Burns et al., 2022). However, salient but unrelated
features in a given dataset can mislead these probes (Farquhar et al., 2023).
Addressing this, we propose a cluster normalization method to minimize the
impact of such features by clustering and normalizing activations of contrast
pairs before applying unsupervised probing techniques. While this approach does
not address the issue of differentiating between knowledge in general and
simulated knowledge - a major issue in the literature of latent knowledge
elicitation (Christiano et al., 2021) - it significantly improves the ability
of unsupervised probes to identify the intended knowledge amidst distractions.","[{'name': 'Walter Laurito'}, {'name': 'Sharan Maiya'}, {'name': 'Grégoire Dhimoïla'}, {'name': 'Owen'}, {'name': 'Yeung'}, {'name': 'Kaarel Hänni'}]",2024-07-26T12:57:54Z
http://arxiv.org/abs/2407.18698v1,http://arxiv.org/abs/2407.18698v1,"Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended
  Text Generation","Decoding from the output distributions of large language models to produce
high-quality text is a complex challenge in language modeling. Various
approaches, such as beam search, sampling with temperature, $k-$sampling,
nucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive
search, have been proposed to address this problem, aiming to improve
coherence, diversity, as well as resemblance to human-generated text. In this
study, we introduce adaptive contrastive search, a novel decoding strategy
extending contrastive search by incorporating an adaptive degeneration penalty,
guided by the estimated uncertainty of the model at each generation step. This
strategy is designed to enhance both the creativity and diversity of the
language modeling process while at the same time producing coherent and
high-quality generated text output. Our findings indicate performance
enhancement in both aspects, across different model architectures and datasets,
underscoring the effectiveness of our method in text generation tasks. Our code
base, datasets, and models are publicly available.","[{'name': 'Esteban Garces Arias'}, {'name': 'Julian Rodemann'}, {'name': 'Meimingwei Li'}, {'name': 'Christian Heumann'}, {'name': 'Matthias Aßenmacher'}]",2024-07-26T12:23:54Z
http://arxiv.org/abs/2407.18689v1,http://arxiv.org/abs/2407.18689v1,"The BIAS Detection Framework: Bias Detection in Word Embeddings and
  Language Models for European Languages","The project BIAS: Mitigating Diversity Biases of AI in the Labor Market is a
four-year project funded by the European commission and supported by the Swiss
State Secretariat for Education, Research and Innovation (SERI). As part of the
project, novel bias detection methods to identify societal bias in language
models and word embeddings in European languages are developed, with particular
attention to linguistic and geographic particularities. This technical report
describes the overall architecture and components of the BIAS Detection
Framework. The code described in this technical report is available and will be
updated and expanded continuously with upcoming results from the BIAS project.
The details about the datasets for the different languages are described in
corresponding papers at scientific venues.","[{'name': 'Alexandre Puttick'}, {'name': 'Leander Rankwiler'}, {'name': 'Catherine Ikae'}, {'name': 'Mascha Kurpicz-Briki'}]",2024-07-26T12:13:45Z
http://arxiv.org/abs/2407.21061v1,http://arxiv.org/abs/2407.21061v1,"Improving noisy student training for low-resource languages in
  End-to-End ASR using CycleGAN and inter-domain losses","Training a semi-supervised end-to-end speech recognition system using noisy
student training has significantly improved performance. However, this approach
requires a substantial amount of paired speech-text and unlabeled speech, which
is costly for low-resource languages. Therefore, this paper considers a more
extreme case of semi-supervised end-to-end automatic speech recognition where
there are limited paired speech-text, unlabeled speech (less than five hours),
and abundant external text. Firstly, we observe improved performance by
training the model using our previous work on semi-supervised learning
""CycleGAN and inter-domain losses"" solely with external text. Secondly, we
enhance ""CycleGAN and inter-domain losses"" by incorporating automatic
hyperparameter tuning, calling it ""enhanced CycleGAN inter-domain losses.""
Thirdly, we integrate it into the noisy student training approach pipeline for
low-resource scenarios. Our experimental results, conducted on six non-English
languages from Voxforge and Common Voice, show a 20% word error rate reduction
compared to the baseline teacher model and a 10% word error rate reduction
compared to the baseline best student model, highlighting the significant
improvements achieved through our proposed method.","[{'name': 'Chia-Yu Li'}, {'name': 'Ngoc Thang Vu'}]",2024-07-26T10:57:06Z
http://arxiv.org/abs/2407.18626v1,http://arxiv.org/abs/2407.18626v1,"Every Part Matters: Integrity Verification of Scientific Figures Based
  on Multimodal Large Language Models","This paper tackles a key issue in the interpretation of scientific figures:
the fine-grained alignment of text and figures. It advances beyond prior
research that primarily dealt with straightforward, data-driven visualizations
such as bar and pie charts and only offered a basic understanding of diagrams
through captioning and classification. We introduce a novel task, Figure
Integrity Verification, designed to evaluate the precision of technologies in
aligning textual knowledge with visual elements in scientific figures. To
support this, we develop a semi-automated method for constructing a large-scale
dataset, Figure-seg, specifically designed for this task. Additionally, we
propose an innovative framework, Every Part Matters (EPM), which leverages
Multimodal Large Language Models (MLLMs) to not only incrementally improve the
alignment and verification of text-figure integrity but also enhance integrity
through analogical reasoning. Our comprehensive experiments show that these
innovations substantially improve upon existing methods, allowing for more
precise and thorough analysis of complex scientific figures. This progress not
only enhances our understanding of multimodal technologies but also stimulates
further research and practical applications across fields requiring the
accurate interpretation of complex visual data.","[{'name': 'Xiang Shi'}, {'name': 'Jiawei Liu'}, {'name': 'Yinpeng Liu'}, {'name': 'Qikai Cheng'}, {'name': 'Wei Lu'}]",2024-07-26T09:35:36Z
http://arxiv.org/abs/2408.06361v1,http://arxiv.org/abs/2408.06361v1,Large Language Model Agent in Financial Trading: A Survey,"Trading is a highly competitive task that requires a combination of strategy,
knowledge, and psychological fortitude. With the recent success of large
language models(LLMs), it is appealing to apply the emerging intelligence of
LLM agents in this competitive arena and understanding if they can outperform
professional traders. In this survey, we provide a comprehensive review of the
current research on using LLMs as agents in financial trading. We summarize the
common architecture used in the agent, the data inputs, and the performance of
LLM trading agents in backtesting as well as the challenges presented in these
research. This survey aims to provide insights into the current state of
LLM-based financial trading agents and outline future research directions in
this field.","[{'name': 'Han Ding'}, {'name': 'Yinheng Li'}, {'name': 'Junhao Wang'}, {'name': 'Hang Chen'}]",2024-07-26T08:53:05Z
http://arxiv.org/abs/2407.18562v1,http://arxiv.org/abs/2407.18562v1,"Learning Robust Named Entity Recognizers From Noisy Data With Retrieval
  Augmentation","Named entity recognition (NER) models often struggle with noisy inputs, such
as those with spelling mistakes or errors generated by Optical Character
Recognition processes, and learning a robust NER model is challenging. Existing
robust NER models utilize both noisy text and its corresponding gold text for
training, which is infeasible in many real-world applications in which gold
text is not available. In this paper, we consider a more realistic setting in
which only noisy text and its NER labels are available. We propose to retrieve
relevant text of the noisy text from a knowledge corpus and use it to enhance
the representation of the original noisy input. We design three retrieval
methods: sparse retrieval based on lexicon similarity, dense retrieval based on
semantic similarity, and self-retrieval based on task-specific text. After
retrieving relevant text, we concatenate the retrieved text with the original
noisy text and encode them with a transformer network, utilizing self-attention
to enhance the contextual token representations of the noisy text using the
retrieved text. We further employ a multi-view training framework that improves
robust NER without retrieving text during inference. Experiments show that our
retrieval-augmented model achieves significant improvements in various noisy
NER settings.","[{'name': 'Chaoyi Ai'}, {'name': 'Yong Jiang'}, {'name': 'Shen Huang'}, {'name': 'Pengjun Xie'}, {'name': 'Kewei Tu'}]",2024-07-26T07:30:41Z
http://arxiv.org/abs/2407.18552v1,http://arxiv.org/abs/2407.18552v1,"Multimodal Emotion Recognition using Audio-Video Transformer Fusion with
  Cross Attention","Understanding emotions is a fundamental aspect of human communication.
Integrating audio and video signals offers a more comprehensive understanding
of emotional states compared to traditional methods that rely on a single data
source, such as speech or facial expressions. Despite its potential, multimodal
emotion recognition faces significant challenges, particularly in
synchronization, feature extraction, and fusion of diverse data sources. To
address these issues, this paper introduces a novel transformer-based model
named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA
model employs a transformer fusion approach to effectively capture and
synchronize interlinked features from both audio and video inputs, thereby
resolving synchronization problems. Additionally, the Cross Attention mechanism
within AVT-CA selectively extracts and emphasizes critical features while
discarding irrelevant ones from both modalities, addressing feature extraction
and fusion challenges. Extensive experimental analysis conducted on the
CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the
proposed model. The results underscore the importance of AVT-CA in developing
precise and reliable multimodal emotion recognition systems for practical
applications.","[{'name': 'Joe Dhanith P R'}, {'name': 'Shravan Venkatraman'}, {'name': 'Vigya Sharma'}, {'name': 'Santhosh Malarvannan'}]",2024-07-26T07:05:04Z
http://arxiv.org/abs/2407.18525v1,http://arxiv.org/abs/2407.18525v1,"Is larger always better? Evaluating and prompting large language models
  for non-generative medical tasks","The use of Large Language Models (LLMs) in medicine is growing, but their
ability to handle both structured Electronic Health Record (EHR) data and
unstructured clinical notes is not well-studied. This study benchmarks various
models, including GPT-based LLMs, BERT-based models, and traditional clinical
predictive models, for non-generative medical tasks utilizing renowned
datasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7
traditional predictive models using the MIMIC dataset (ICU patient records) and
the TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality
and readmission prediction, disease hierarchy reconstruction, and biomedical
sentence matching, comparing both zero-shot and finetuned performance. Results
indicated that LLMs exhibited robust zero-shot predictive capabilities on
structured EHR data when using well-designed prompting strategies, frequently
surpassing traditional models. However, for unstructured medical texts, LLMs
did not outperform finetuned BERT models, which excelled in both supervised and
unsupervised tasks. Consequently, while LLMs are effective for zero-shot
learning on structured data, finetuned BERT models are more suitable for
unstructured texts, underscoring the importance of selecting models based on
specific task requirements and data characteristics to optimize the application
of NLP technology in healthcare.","[{'name': 'Yinghao Zhu'}, {'name': 'Junyi Gao'}, {'name': 'Zixiang Wang'}, {'name': 'Weibin Liao'}, {'name': 'Xiaochen Zheng'}, {'name': 'Lifang Liang'}, {'name': 'Yasha Wang'}, {'name': 'Chengwei Pan'}, {'name': 'Ewen M. Harrison'}, {'name': 'Liantao Ma'}]",2024-07-26T06:09:10Z
http://arxiv.org/abs/2407.18501v1,http://arxiv.org/abs/2407.18501v1,"The formation of perceptual space in early phonetic acquisition: a
  cross-linguistic modeling approach","This study investigates how learners organize perceptual space in early
phonetic acquisition by advancing previous studies in two key aspects. Firstly,
it examines the shape of the learned hidden representation as well as its
ability to categorize phonetic categories. Secondly, it explores the impact of
training models on context-free acoustic information, without involving
contextual cues, on phonetic acquisition, closely mimicking the early language
learning stage. Using a cross-linguistic modeling approach, autoencoder models
are trained on English and Mandarin and evaluated in both native and non-native
conditions, following experimental conditions used in infant language
perception studies. The results demonstrate that unsupervised bottom-up
training on context-free acoustic information leads to comparable learned
representations of perceptual space between native and non-native conditions
for both English and Mandarin, resembling the early stage of universal
listening in infants. These findings provide insights into the organization of
perceptual space during early phonetic acquisition and contribute to our
understanding of the formation and representation of phonetic categories.","[{'name': 'Frank Lihui Tan'}, {'name': 'Youngah Do'}]",2024-07-26T04:18:36Z
http://arxiv.org/abs/2407.18498v1,http://arxiv.org/abs/2407.18498v1,"A Reliable Common-Sense Reasoning Socialbot Built Using LLMs and
  Goal-Directed ASP","The development of large language models (LLMs), such as GPT, has enabled the
construction of several socialbots, like ChatGPT, that are receiving a lot of
attention for their ability to simulate a human conversation. However, the
conversation is not guided by a goal and is hard to control. In addition,
because LLMs rely more on pattern recognition than deductive reasoning, they
can give confusing answers and have difficulty integrating multiple topics into
a cohesive response. These limitations often lead the LLM to deviate from the
main topic to keep the conversation interesting. We propose AutoCompanion, a
socialbot that uses an LLM model to translate natural language into predicates
(and vice versa) and employs commonsense reasoning based on Answer Set
Programming (ASP) to hold a social conversation with a human. In particular, we
rely on s(CASP), a goal-directed implementation of ASP as the backend. This
paper presents the framework design and how an LLM is used to parse user
messages and generate a response from the s(CASP) engine output. To validate
our proposal, we describe (real) conversations in which the chatbot's goal is
to keep the user entertained by talking about movies and books, and s(CASP)
ensures (i) correctness of answers, (ii) coherence (and precision) during the
conversation, which it dynamically regulates to achieve its specific purpose,
and (iii) no deviation from the main topic.","[{'name': 'Yankai Zeng'}, {'name': 'Abhiramon Rajashekharan'}, {'name': 'Kinjal Basu'}, {'name': 'Huaduo Wang'}, {'name': 'Joaquín Arias'}, {'name': 'Gopal Gupta'}]",2024-07-26T04:13:43Z
http://arxiv.org/abs/2407.18496v1,http://arxiv.org/abs/2407.18496v1,"Towards More Accurate Prediction of Human Empathy and Emotion in Text
  and Multi-turn Conversations by Combining Advanced NLP, Transformers-based
  Networks, and Linguistic Methodologies","Based on the WASSA 2022 Shared Task on Empathy Detection and Emotion
Classification, we predict the level of empathic concern and personal distress
displayed in essays. For the first stage of this project we implemented a
Feed-Forward Neural Network using sentence-level embeddings as features. We
experimented with four different embedding models for generating the inputs to
the neural network. The subsequent stage builds upon the previous work and we
have implemented three types of revisions. The first revision focuses on the
enhancements to the model architecture and the training approach. The second
revision focuses on handling class imbalance using stratified data sampling.
The third revision focuses on leveraging lexical resources, where we apply four
different resources to enrich the features associated with the dataset. During
the final stage of this project, we have created the final end-to-end system
for the primary task using an ensemble of models to revise primary task
performance. Additionally, as part of the final stage, these approaches have
been adapted to the WASSA 2023 Shared Task on Empathy Emotion and Personality
Detection in Interactions, in which the empathic concern, emotion polarity, and
emotion intensity in dyadic text conversations are predicted.","[{'name': 'Manisha Singh'}, {'name': 'Divy Sharma'}, {'name': 'Alonso Ma'}, {'name': 'Nora Goldfine'}]",2024-07-26T04:01:27Z
http://arxiv.org/abs/2407.21059v1,http://arxiv.org/abs/2407.21059v1,"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable
  Frameworks","Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities
of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The
increasing demands of application scenarios have driven the evolution of RAG,
leading to the integration of advanced retrievers, LLMs and other complementary
technologies, which in turn has amplified the intricacy of RAG systems.
However, the rapid advancements are outpacing the foundational RAG paradigm,
with many methods struggling to be unified under the process of
""retrieve-then-generate"". In this context, this paper examines the limitations
of the existing RAG paradigm and introduces the modular RAG framework. By
decomposing complex RAG systems into independent modules and specialized
operators, it facilitates a highly reconfigurable framework. Modular RAG
transcends the traditional linear architecture, embracing a more advanced
design that integrates routing, scheduling, and fusion mechanisms. Drawing on
extensive research, this paper further identifies prevalent RAG
patterns-linear, conditional, branching, and looping-and offers a comprehensive
analysis of their respective implementation nuances. Modular RAG presents
innovative opportunities for the conceptualization and deployment of RAG
systems. Finally, the paper explores the potential emergence of new operators
and paradigms, establishing a solid theoretical foundation and a practical
roadmap for the continued evolution and practical deployment of RAG
technologies.","[{'name': 'Yunfan Gao'}, {'name': 'Yun Xiong'}, {'name': 'Meng Wang'}, {'name': 'Haofen Wang'}]",2024-07-26T03:45:30Z
http://arxiv.org/abs/2407.18483v4,http://arxiv.org/abs/2407.18483v4,"A Role-specific Guided Large Language Model for Ophthalmic Consultation
  Based on Stylistic Differentiation","Ophthalmology consultations are crucial for diagnosing, treating, and
preventing eye diseases. However, the growing demand for consultations exceeds
the availability of ophthalmologists. By leveraging large pre-trained language
models, we can design effective dialogues for specific scenarios, aiding in
consultations. Traditional fine-tuning strategies for question-answering tasks
are impractical due to increasing model size and often ignoring patient-doctor
role function during consultations. In this paper, we propose EyeDoctor, an
ophthalmic medical questioning large language model that enhances accuracy
through doctor-patient role perception guided and an augmented knowledge base
with external disease information. Experimental results show EyeDoctor achieves
higher question-answering precision in ophthalmology consultations. Notably,
EyeDoctor demonstrated a 7.25% improvement in Rouge-1 scores and a 10.16%
improvement in F1 scores on multi-round datasets compared to second best model
ChatGPT, highlighting the importance of doctor-patient role differentiation and
dynamic knowledge base expansion for intelligent medical consultations. EyeDoc
also serves as a free available web based service and souce code is available
at https://github.com/sperfu/EyeDoc.","[{'name': 'Laiyi Fu'}, {'name': 'Binbin Fan'}, {'name': 'Hongkai Du'}, {'name': 'Yanxiang Feng'}, {'name': 'Chunhua Li'}, {'name': 'Huping Song'}]",2024-07-26T03:23:31Z
http://arxiv.org/abs/2407.18479v1,http://arxiv.org/abs/2407.18479v1,Multi-turn Response Selection with Commonsense-enhanced Language Models,"As a branch of advanced artificial intelligence, dialogue systems are
prospering. Multi-turn response selection is a general research problem in
dialogue systems. With the assistance of background information and pre-trained
language models, the performance of state-of-the-art methods on this problem
gains impressive improvement. However, existing studies neglect the importance
of external commonsense knowledge. Hence, we design a Siamese network where a
pre-trained Language model merges with a Graph neural network (SinLG). SinLG
takes advantage of Pre-trained Language Models (PLMs) to catch the word
correlations in the context and response candidates and utilizes a Graph Neural
Network (GNN) to reason helpful common sense from an external knowledge graph.
The GNN aims to assist the PLM in fine-tuning, and arousing its related
memories to attain better performance. Specifically, we first extract related
concepts as nodes from an external knowledge graph to construct a subgraph with
the context response pair as a super node for each sample. Next, we learn two
representations for the context response pair via both the PLM and GNN. A
similarity loss between the two representations is utilized to transfer the
commonsense knowledge from the GNN to the PLM. Then only the PLM is used to
infer online so that efficiency can be guaranteed. Finally, we conduct
extensive experiments on two variants of the PERSONA-CHAT dataset, which proves
that our solution can not only improve the performance of the PLM but also
achieve an efficient inference.","[{'name': 'Yuandong Wang'}, {'name': 'Xuhui Ren'}, {'name': 'Tong Chen'}, {'name': 'Yuxiao Dong'}, {'name': 'Nguyen Quoc Viet Hung'}, {'name': 'Jie Tang'}]",2024-07-26T03:13:47Z
http://arxiv.org/abs/2407.18471v1,http://arxiv.org/abs/2407.18471v1,Constructing the CORD-19 Vaccine Dataset,"We introduce new dataset 'CORD-19-Vaccination' to cater to scientists
specifically looking into COVID-19 vaccine-related research. This dataset is
extracted from CORD-19 dataset [Wang et al., 2020] and augmented with new
columns for language detail, author demography, keywords, and topic per paper.
Facebook's fastText model is used to identify languages [Joulin et al., 2016].
To establish author demography (author affiliation, lab/institution location,
and lab/institution country columns) we processed the JSON file for each paper
and then further enhanced using Google's search API to determine country
values. 'Yake' was used to extract keywords from the title, abstract, and body
of each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to
add topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset,
we demonstrate a question-answering task like the one used in the CORD-19
Kaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential
sentence classification was performed on each paper's abstract using the model
from Dernoncourt et al. [2016]. We partially hand annotated the training
dataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination'
contains 30k research papers and can be immensely valuable for NLP research
such as text mining, information extraction, and question answering, specific
to the domain of COVID-19 vaccine research.","[{'name': 'Manisha Singh'}, {'name': 'Divy Sharma'}, {'name': 'Alonso Ma'}, {'name': 'Bridget Tyree'}, {'name': 'Margaret Mitchell'}]",2024-07-26T02:44:55Z
http://arxiv.org/abs/2407.18461v1,http://arxiv.org/abs/2407.18461v1,"Enhancing Dysarthric Speech Recognition for Unseen Speakers via
  Prototype-Based Adaptation","Dysarthric speech recognition (DSR) presents a formidable challenge due to
inherent inter-speaker variability, leading to severe performance degradation
when applying DSR models to new dysarthric speakers. Traditional speaker
adaptation methodologies typically involve fine-tuning models for each speaker,
but this strategy is cost-prohibitive and inconvenient for disabled users,
requiring substantial data collection. To address this issue, we introduce a
prototype-based approach that markedly improves DSR performance for unseen
dysarthric speakers without additional fine-tuning. Our method employs a
feature extractor trained with HuBERT to produce per-word prototypes that
encapsulate the characteristics of previously unseen speakers. These prototypes
serve as the basis for classification. Additionally, we incorporate supervised
contrastive learning to refine feature extraction. By enhancing representation
quality, we further improve DSR performance, enabling effective personalized
DSR. We release our code at https://github.com/NKU-HLT/PB-DSR.","[{'name': 'Shiyao Wang'}, {'name': 'Shiwan Zhao'}, {'name': 'Jiaming Zhou'}, {'name': 'Aobo Kong'}, {'name': 'Yong Qin'}]",2024-07-26T02:03:23Z
http://arxiv.org/abs/2407.18454v1,http://arxiv.org/abs/2407.18454v1,Fairness Definitions in Language Models Explained,"Language Models (LMs) have demonstrated exceptional performance across
various Natural Language Processing (NLP) tasks. Despite these advancements,
LMs can inherit and amplify societal biases related to sensitive attributes
such as gender and race, limiting their adoption in real-world applications.
Therefore, fairness has been extensively explored in LMs, leading to the
proposal of various fairness notions. However, the lack of clear agreement on
which fairness definition to apply in specific contexts (\textit{e.g.,}
medium-sized LMs versus large-sized LMs) and the complexity of understanding
the distinctions between these definitions can create confusion and impede
further progress. To this end, this paper proposes a systematic survey that
clarifies the definitions of fairness as they apply to LMs. Specifically, we
begin with a brief introduction to LMs and fairness in LMs, followed by a
comprehensive, up-to-date overview of existing fairness notions in LMs and the
introduction of a novel taxonomy that categorizes these concepts based on their
foundational principles and operational distinctions. We further illustrate
each definition through experiments, showcasing their practical implications
and outcomes. Finally, we discuss current research challenges and open
questions, aiming to foster innovative ideas and advance the field. The
implementation and additional resources are publicly available at
https://github.com/LavinWong/Fairness-in-Large-Language-Models/tree/main/definitions.","[{'name': 'Thang Viet Doan'}, {'name': 'Zhibo Chu'}, {'name': 'Zichong Wang'}, {'name': 'Wenbin Zhang'}]",2024-07-26T01:21:25Z
http://arxiv.org/abs/2407.18442v1,http://arxiv.org/abs/2407.18442v1,"Guidance-Based Prompt Data Augmentation in Specialized Domains for Named
  Entity Recognition","While the abundance of rich and vast datasets across numerous fields has
facilitated the advancement of natural language processing, sectors in need of
specialized data types continue to struggle with the challenge of finding
quality data. Our study introduces a novel guidance data augmentation technique
utilizing abstracted context and sentence structures to produce varied
sentences while maintaining context-entity relationships, addressing data
scarcity challenges. By fostering a closer relationship between context,
sentence structure, and role of entities, our method enhances data
augmentation's effectiveness. Consequently, by showcasing diversification in
both entity-related vocabulary and overall sentence structure, and
simultaneously improving the training performance of named entity recognition
task.","[{'name': 'Hyeonseok Kang'}, {'name': 'Hyein Seo'}, {'name': 'Jeesu Jung'}, {'name': 'Sangkeun Jung'}, {'name': 'Du-Seong Chang'}, {'name': 'Riwoo Chung'}]",2024-07-26T00:48:28Z
http://arxiv.org/abs/2407.21058v1,http://arxiv.org/abs/2407.21058v1,"Understanding the Interplay of Scale, Data, and Bias in Language Models:
  A Case Study with BERT","In the current landscape of language model research, larger models, larger
datasets and more compute seems to be the only way to advance towards
intelligence. While there have been extensive studies of scaling laws and
models' scaling behaviors, the effect of scale on a model's social biases and
stereotyping tendencies has received less attention. In this study, we explore
the influence of model scale and pre-training data on its learnt social biases.
We focus on BERT -- an extremely popular language model -- and investigate
biases as they show up during language modeling (upstream), as well as during
classification applications after fine-tuning (downstream). Our experiments on
four architecture sizes of BERT demonstrate that pre-training data
substantially influences how upstream biases evolve with model scale. With
increasing scale, models pre-trained on large internet scrapes like Common
Crawl exhibit higher toxicity, whereas models pre-trained on moderated data
sources like Wikipedia show greater gender stereotypes. However, downstream
biases generally decrease with increasing model scale, irrespective of the
pre-training data. Our results highlight the qualitative role of pre-training
data in the biased behavior of language models, an often overlooked aspect in
the study of scale. Through a detailed case study of BERT, we shed light on the
complex interplay of data and model scale, and investigate how it translates to
concrete biases.","[{'name': 'Muhammad Ali'}, {'name': 'Swetasudha Panda'}, {'name': 'Qinlan Shen'}, {'name': 'Michael Wick'}, {'name': 'Ari Kobren'}]",2024-07-25T23:09:33Z
http://arxiv.org/abs/2407.18421v1,http://arxiv.org/abs/2407.18421v1,Self-Directed Synthetic Dialogues and Revisions Technical Report,"Synthetic data has become an important tool in the fine-tuning of language
models to follow instructions and solve complex problems. Nevertheless, the
majority of open data to date is often lacking multi-turn data and collected on
closed models, limiting progress on advancing open fine-tuning methods. We
introduce Self Directed Synthetic Dialogues (SDSD), an experimental dataset
consisting of guided conversations of language models talking to themselves.
The dataset consists of multi-turn conversations generated with DBRX, Llama 2
70B, and Mistral Large, all instructed to follow a conversation plan generated
prior to the conversation. We also explore including principles from
Constitutional AI and other related works to create synthetic preference data
via revisions to the final conversation turn. We hope this work encourages
further exploration in multi-turn data and the use of open models for expanding
the impact of synthetic data.","[{'name': 'Nathan Lambert'}, {'name': 'Hailey Schoelkopf'}, {'name': 'Aaron Gokaslan'}, {'name': 'Luca Soldaini'}, {'name': 'Valentina Pyatkin'}, {'name': 'Louis Castricato'}]",2024-07-25T22:42:36Z
http://arxiv.org/abs/2407.18418v2,http://arxiv.org/abs/2407.18418v2,Know Your Limits: A Survey of Abstention in Large Language Models,"Abstention, the refusal of large language models (LLMs) to provide an answer,
is increasingly recognized for its potential to mitigate hallucinations and
enhance safety in LLM systems. In this survey, we introduce a framework to
examine abstention from three perspectives: the query, the model, and human
values. We organize the literature on abstention methods, benchmarks, and
evaluation metrics using this framework, and discuss merits and limitations of
prior work. We further identify and motivate areas for future work, centered
around whether abstention can be achieved as a meta-capability that transcends
specific tasks or domains, while still providing opportunities to optimize
abstention abilities based on context.","[{'name': 'Bingbing Wen'}, {'name': 'Jihan Yao'}, {'name': 'Shangbin Feng'}, {'name': 'Chenjun Xu'}, {'name': 'Yulia Tsvetkov'}, {'name': 'Bill Howe'}, {'name': 'Lucy Lu Wang'}]",2024-07-25T22:31:50Z
http://arxiv.org/abs/2407.18416v2,http://arxiv.org/abs/2407.18416v2,PersonaGym: Evaluating Persona Agents and LLMs,"Persona agents, which are LLM agents that act according to an assigned
persona, have demonstrated impressive contextual response capabilities across
various applications. These persona agents offer significant enhancements
across diverse sectors, such as education, healthcare, and entertainment, where
model developers can align agent responses to different user requirements
thereby broadening the scope of agent applications. However, evaluating persona
agent performance is incredibly challenging due to the complexity of assessing
persona adherence in free-form interactions across various environments that
are relevant to each persona agent. We introduce PersonaGym, the first dynamic
evaluation framework for assessing persona agents, and PersonaScore, the first
automated human-aligned metric grounded in decision theory for comprehensive
large-scale evaluation of persona agents. Our evaluation of 6 open and
closed-source LLMs, using a benchmark encompassing 200 personas and 10,000
questions, reveals significant opportunities for advancement in persona agent
capabilities across state-of-the-art models. For example, Claude 3.5 Sonnet
only has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite
being a much more advanced model. Importantly, we find that increased model
size and complexity do not necessarily imply enhanced persona agent
capabilities thereby highlighting the pressing need for algorithmic and
architectural invention towards faithful and performant persona agents.","[{'name': 'Vinay Samuel'}, {'name': 'Henry Peng Zou'}, {'name': 'Yue Zhou'}, {'name': 'Shreyas Chaudhari'}, {'name': 'Ashwin Kalyan'}, {'name': 'Tanmay Rajpurohit'}, {'name': 'Ameet Deshpande'}, {'name': 'Karthik Narasimhan'}, {'name': 'Vishvak Murahari'}]",2024-07-25T22:24:45Z
http://arxiv.org/abs/2407.18376v1,http://arxiv.org/abs/2407.18376v1,"Exploring Bengali Religious Dialect Biases in Large Language Models with
  Evaluation Perspectives","While Large Language Models (LLM) have created a massive technological impact
in the past decade, allowing for human-enabled applications, they can produce
output that contains stereotypes and biases, especially when using low-resource
languages. This can be of great ethical concern when dealing with sensitive
topics such as religion. As a means toward making LLMS more fair, we explore
bias from a religious perspective in Bengali, focusing specifically on two main
religious dialects: Hindu and Muslim-majority dialects. Here, we perform
different experiments and audit showing the comparative analysis of different
sentences using three commonly used LLMs: ChatGPT, Gemini, and Microsoft
Copilot, pertaining to the Hindu and Muslim dialects of specific words and
showcasing which ones catch the social biases and which do not. Furthermore, we
analyze our findings and relate them to potential reasons and evaluation
perspectives, considering their global impact with over 300 million speakers
worldwide. With this work, we hope to establish the rigor for creating more
fairness in LLMs, as these are widely used as creative writing agents.","[{'name': 'Azmine Toushik Wasi'}, {'name': 'Raima Islam'}, {'name': 'Mst Rafia Islam'}, {'name': 'Taki Hasan Rafi'}, {'name': 'Dong-Kyu Chae'}]",2024-07-25T20:19:29Z
http://arxiv.org/abs/2407.18370v1,http://arxiv.org/abs/2407.18370v1,"Trust or Escalate: LLM Judges with Provable Guarantees for Human
  Agreement","We present a principled approach to provide LLM-based evaluation with a
rigorous guarantee of human agreement. We first propose that a reliable
evaluation method should not uncritically rely on model preferences for
pairwise evaluation, but rather assess the confidence of judge models and
selectively decide when to trust its judgement. We then show that under this
selective evaluation framework, human agreement can be provably guaranteed --
such that the model evaluation aligns with that of humans to a user-specified
agreement level. As part of our framework, we also introduce Simulated
Annotators, a novel confidence estimation method that significantly improves
judge calibration and thus enables high coverage of evaluated instances.
Finally, we propose Cascaded Selective Evaluation, where we use cheaper models
as initial judges and escalate to stronger models only when necessary -- again,
while still providing a provable guarantee of human agreement. Experimental
results show that Cascaded Selective Evaluation guarantees strong alignment
with humans, far beyond what LLM judges could achieve without selective
evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never
achieves 80% human agreement, our method, even while employing substantially
cost-effective models such as Mistral-7B, guarantees over 80% human agreement
with almost 80% test coverage.","[{'name': 'Jaehun Jung'}, {'name': 'Faeze Brahman'}, {'name': 'Yejin Choi'}]",2024-07-25T20:04:59Z
http://arxiv.org/abs/2407.18367v1,http://arxiv.org/abs/2407.18367v1,Robust Claim Verification Through Fact Detection,"Claim verification can be a challenging task. In this paper, we present a
method to enhance the robustness and reasoning capabilities of automated claim
verification through the extraction of short facts from evidence. Our novel
approach, FactDetect, leverages Large Language Models (LLMs) to generate
concise factual statements from evidence and label these facts based on their
semantic relevance to the claim and evidence. The generated facts are then
combined with the claim and evidence. To train a lightweight supervised model,
we incorporate a fact-detection task into the claim verification process as a
multitasking approach to improve both performance and explainability. We also
show that augmenting FactDetect in the claim verification prompt enhances
performance in zero-shot claim verification using LLMs. Our method demonstrates
competitive results in the supervised claim verification model by 15% on the F1
score when evaluated for challenging scientific claim verification datasets. We
also demonstrate that FactDetect can be augmented with claim and evidence for
zero-shot prompting (AugFactDetect) in LLMs for verdict prediction. We show
that AugFactDetect outperforms the baseline with statistical significance on
three challenging scientific claim verification datasets with an average of
17.3% performance gain compared to the best performing baselines.","[{'name': 'Nazanin Jafari'}, {'name': 'James Allan'}]",2024-07-25T20:03:43Z
http://arxiv.org/abs/2407.18248v1,http://arxiv.org/abs/2407.18248v1,"Self-Training with Direct Preference Optimization Improves
  Chain-of-Thought Reasoning","Effective training of language models (LMs) for mathematical reasoning tasks
demands high-quality supervised fine-tuning data. Besides obtaining annotations
from human experts, a common alternative is sampling from larger and more
powerful LMs. However, this knowledge distillation approach can be costly and
unstable, particularly when relying on closed-source, proprietary LMs like
GPT-4, whose behaviors are often unpredictable. In this work, we demonstrate
that the reasoning abilities of small-scale LMs can be enhanced through
self-training, a process where models learn from their own outputs. We also
show that the conventional self-training can be further augmented by a
preference learning algorithm called Direct Preference Optimization (DPO). By
integrating DPO into self-training, we leverage preference data to guide LMs
towards more accurate and diverse chain-of-thought reasoning. We evaluate our
method across various mathematical reasoning tasks using different base models.
Our experiments show that this approach not only improves LMs' reasoning
performance but also offers a more cost-effective and scalable solution
compared to relying on large proprietary LMs.","[{'name': 'Tianduo Wang'}, {'name': 'Shichen Li'}, {'name': 'Wei Lu'}]",2024-07-25T17:59:16Z
http://arxiv.org/abs/2407.18242v1,http://arxiv.org/abs/2407.18242v1,LoRA-Pro: Are Low-Rank Adapters Properly Optimized?,"Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method
for parameter-efficient fine-tuning foundation models by re-parameterizing the
original matrix into the product of two low-rank matrices. Despite its
efficiency, LoRA often yields inferior performance compared to full
fine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.
Firstly, we delve into the optimization processes in LoRA and full fine-tuning.
We reveal that while LoRA employs low-rank approximation, it neglects to
approximate the optimization process of full fine-tuning. To address this, we
introduce a novel concept called the ""equivalent gradient."" This virtual
gradient makes the optimization process on the re-parameterized matrix
equivalent to LoRA, which can be used to quantify the differences between LoRA
and full fine-tuning. The equivalent gradient is derived from the gradients of
matrices $A$ and $B$. To narrow the performance gap, our approach minimizes the
differences between the equivalent gradient and the gradient obtained from full
fine-tuning during the optimization process. By solving this objective, we
derive optimal closed-form solutions for updating matrices $A$ and $B$. Our
method constrains the optimization process, shrinking the performance gap
between LoRA and full fine-tuning. Extensive experiments on natural language
processing tasks validate the effectiveness of our method.","[{'name': 'Zhengbo Wang'}, {'name': 'Jian Liang'}]",2024-07-25T17:57:12Z
http://arxiv.org/abs/2407.18219v2,http://arxiv.org/abs/2407.18219v2,"Recursive Introspection: Teaching Language Model Agents How to
  Self-Improve","A central piece in enabling intelligent agentic behavior in foundation models
is to make them capable of introspecting upon their behavior, reasoning, and
correcting their mistakes as more computation or interaction is available. Even
the strongest proprietary large language models (LLMs) do not quite exhibit the
ability of continually improving their responses sequentially, even in
scenarios where they are explicitly told that they are making a mistake. In
this paper, we develop RISE: Recursive IntroSpEction, an approach for
fine-tuning LLMs to introduce this capability, despite prior work hypothesizing
that this capability may not be possible to attain. Our approach prescribes an
iterative fine-tuning procedure, which attempts to teach the model how to alter
its response after having executed previously unsuccessful attempts to solve a
hard test-time problem, with optionally additional environment feedback. RISE
poses fine-tuning for a single-turn prompt as solving a multi-turn Markov
decision process (MDP), where the initial state is the prompt. Inspired by
principles in online imitation learning and reinforcement learning, we propose
strategies for multi-turn data collection and training so as to imbue an LLM
with the capability to recursively detect and correct its previous mistakes in
subsequent iterations. Our experiments show that RISE enables Llama2, Llama3,
and Mistral models to improve themselves with more turns on math reasoning
tasks, outperforming several single-turn strategies given an equal amount of
inference-time computation. We also find that RISE scales well, often attaining
larger benefits with more capable models. Our analysis shows that RISE makes
meaningful improvements to responses to arrive at the correct solution for
challenging prompts, without disrupting one-turn abilities as a result of
expressing more complex distributions.","[{'name': 'Yuxiao Qu'}, {'name': 'Tianjun Zhang'}, {'name': 'Naman Garg'}, {'name': 'Aviral Kumar'}]",2024-07-25T17:35:59Z
http://arxiv.org/abs/2407.18213v2,http://arxiv.org/abs/2407.18213v2,Exploring Scaling Trends in LLM Robustness,"Language model capabilities predictably improve from scaling a model's size
and training data. Motivated by this, increasingly large language models have
been trained, yielding an array of impressive capabilities. Yet these models
are vulnerable to adversarial prompts, such as ""jailbreaks"" that hijack models
to perform undesired behaviors, posing a significant risk of misuse. Prior work
indicates that computer vision models become more robust with model and data
scaling, raising the question: does language model robustness also improve with
scale? We study this question empirically, finding that larger models respond
substantially better to adversarial training, but there is little to no benefit
from model scale in the absence of explicit defenses.","[{'name': 'Nikolaus Howe'}, {'name': 'Michał Zajac'}, {'name': 'Ian McKenzie'}, {'name': 'Oskar Hollinsworth'}, {'name': 'Tom Tseng'}, {'name': 'Pierre-Luc Bacon'}, {'name': 'Adam Gleave'}]",2024-07-25T17:26:41Z
http://arxiv.org/abs/2407.18147v1,http://arxiv.org/abs/2407.18147v1,The FIGNEWS Shared Task on News Media Narratives,"We present an overview of the FIGNEWS shared task, organized as part of the
ArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses
bias and propaganda annotation in multilingual news posts. We focus on the
early days of the Israel War on Gaza as a case study. The task aims to foster
collaboration in developing annotation guidelines for subjective tasks by
creating frameworks for analyzing diverse narratives highlighting potential
bias and propaganda. In a spirit of fostering and encouraging diversity, we
address the problem from a multilingual perspective, namely within five
languages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams
participated in two annotation subtasks: bias (16 teams) and propaganda (6
teams). The teams competed in four evaluation tracks: guidelines development,
annotation quality, annotation quantity, and consistency. Collectively, the
teams produced 129,800 data points. Key findings and implications for the field
are discussed.","[{'name': 'Wajdi Zaghouani'}, {'name': 'Mustafa Jarrar'}, {'name': 'Nizar Habash'}, {'name': 'Houda Bouamor'}, {'name': 'Imed Zitouni'}, {'name': 'Mona Diab'}, {'name': 'Samhaa R. El-Beltagy'}, {'name': 'Muhammed AbuOdeh'}]",2024-07-25T15:58:19Z
http://arxiv.org/abs/2407.18129v2,http://arxiv.org/abs/2407.18129v2,Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic,"Recent advancements have significantly enhanced the capabilities of
Multimodal Large Language Models (MLLMs) in generating and understanding
image-to-text content. Despite these successes, progress is predominantly
limited to English due to the scarcity of high quality multimodal resources in
other languages. This limitation impedes the development of competitive models
in languages such as Arabic. To alleviate this situation, we introduce an
efficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced
language model based on LLaMA-2 to facilitate multimodal interactions. Dallah
demonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning
six Arabic dialects, Dallah showcases its capability to handle complex
dialectal interactions incorporating both textual and visual elements. The
model excels in two benchmark tests: one evaluating its performance on Modern
Standard Arabic (MSA) and another specifically designed to assess dialectal
responses. Beyond its robust performance in multimodal interaction tasks,
Dallah has the potential to pave the way for further development of
dialect-aware Arabic MLLMs.","[{'name': 'Fakhraddin Alwajih'}, {'name': 'Gagan Bhatia'}, {'name': 'Muhammad Abdul-Mageed'}]",2024-07-25T15:36:48Z
http://arxiv.org/abs/2407.18119v1,http://arxiv.org/abs/2407.18119v1,"Tracking linguistic information in transformer-based sentence embeddings
  through targeted sparsification","Analyses of transformer-based models have shown that they encode a variety of
linguistic information from their textual input. While these analyses have shed
a light on the relation between linguistic information on one side, and
internal architecture and parameters on the other, a question remains
unanswered: how is this linguistic information reflected in sentence
embeddings? Using datasets consisting of sentences with known structure, we
test to what degree information about chunks (in particular noun, verb or
prepositional phrases), such as grammatical number, or semantic role, can be
localized in sentence embeddings. Our results show that such information is not
distributed over the entire sentence embedding, but rather it is encoded in
specific regions. Understanding how the information from an input text is
compressed into sentence embeddings helps understand current transformer models
and help build future explainable neural models.","[{'name': 'Vivi Nastase'}, {'name': 'Paola Merlo'}]",2024-07-25T15:27:08Z
http://arxiv.org/abs/2407.18078v1,http://arxiv.org/abs/2407.18078v1,PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization,"The recent emergence of Large Language Models (LLMs) has heralded a new era
of human-AI interaction. These sophisticated models, exemplified by Chat-GPT
and its successors, have exhibited remarkable capabilities in language
understanding. However, as these LLMs have undergone exponential growth, a
crucial dimension that remains understudied is the personalization of these
models. Large foundation models such as GPT-3 etc. focus on creating a
universal model that serves a broad range of tasks and users. This approach
emphasizes the model's generalization capabilities, treating users as a
collective rather than as distinct individuals. While practical for many common
applications, this one-size-fits-all approach often fails to address the rich
tapestry of human diversity and individual needs. To explore this issue we
introduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP
models for user personalization. \datasetname{} consists of a series of
user-centered tasks containing diverse and individualized expressions where the
preferences of users can potentially differ for the same input. Using PEFT-U,
we explore the challenge of efficiently personalizing LLMs to accommodate
user-specific preferences in the context of diverse user-centered tasks.","[{'name': 'Christopher Clarke'}, {'name': 'Yuzhao Heng'}, {'name': 'Lingjia Tang'}, {'name': 'Jason Mars'}]",2024-07-25T14:36:18Z
http://arxiv.org/abs/2407.18061v1,http://arxiv.org/abs/2407.18061v1,Difficulty Estimation and Simplification of French Text Using LLMs,"We leverage generative large language models for language learning
applications, focusing on estimating the difficulty of foreign language texts
and simplifying them to lower difficulty levels. We frame both tasks as
prediction problems and develop a difficulty classification model using labeled
examples, transfer learning, and large language models, demonstrating superior
accuracy compared to previous approaches. For simplification, we evaluate the
trade-off between simplification quality and meaning preservation, comparing
zero-shot and fine-tuned performances of large language models. We show that
meaningful text simplifications can be obtained with limited fine-tuning. Our
experiments are conducted on French texts, but our methods are
language-agnostic and directly applicable to other foreign languages.","[{'name': 'Henri Jamet'}, {'name': 'Yash Raj Shrestha'}, {'name': 'Michalis Vlachos'}]",2024-07-25T14:16:08Z
http://arxiv.org/abs/2407.18058v1,http://arxiv.org/abs/2407.18058v1,"I can listen but cannot read: An evaluation of two-tower multimodal
  systems for instrument recognition","Music two-tower multimodal systems integrate audio and text modalities into a
joint audio-text space, enabling direct comparison between songs and their
corresponding labels. These systems enable new approaches for classification
and retrieval, leveraging both modalities. Despite the promising results they
have shown for zero-shot classification and retrieval tasks, closer inspection
of the embeddings is needed. This paper evaluates the inherent zero-shot
properties of joint audio-text spaces for the case-study of instrument
recognition. We present an evaluation and analysis of two-tower systems for
zero-shot instrument recognition and a detailed analysis of the properties of
the pre-joint and joint embeddings spaces. Our findings suggest that audio
encoders alone demonstrate good quality, while challenges remain within the
text encoder or joint space projection. Specifically, two-tower systems exhibit
sensitivity towards specific words, favoring generic prompts over musically
informed ones. Despite the large size of textual encoders, they do not yet
leverage additional textual context or infer instruments accurately from their
descriptions. Lastly, a novel approach for quantifying the semantic
meaningfulness of the textual space leveraging an instrument ontology is
proposed. This method reveals deficiencies in the systems' understanding of
instruments and provides evidence of the need for fine-tuning text encoders on
musical data.","[{'name': 'Yannis Vasilakis'}, {'name': 'Rachel Bittner'}, {'name': 'Johan Pauwels'}]",2024-07-25T14:15:05Z
http://arxiv.org/abs/2407.18035v1,http://arxiv.org/abs/2407.18035v1,"RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large
  Language Models","Natural images captured by mobile devices often suffer from multiple types of
degradation, such as noise, blur, and low light. Traditional image restoration
methods require manual selection of specific tasks, algorithms, and execution
sequences, which is time-consuming and may yield suboptimal results. All-in-one
models, though capable of handling multiple tasks, typically support only a
limited range and often produce overly smooth, low-fidelity outcomes due to
their broad data distribution fitting. To address these challenges, we first
define a new pipeline for restoring images with multiple degradations, and then
introduce RestoreAgent, an intelligent image restoration system leveraging
multimodal large language models. RestoreAgent autonomously assesses the type
and extent of degradation in input images and performs restoration through (1)
determining the appropriate restoration tasks, (2) optimizing the task
sequence, (3) selecting the most suitable models, and (4) executing the
restoration. Experimental results demonstrate the superior performance of
RestoreAgent in handling complex degradation, surpassing human experts.
Furthermore, the system modular design facilitates the fast integration of new
tasks and models, enhancing its flexibility and scalability for various
applications.","[{'name': 'Haoyu Chen'}, {'name': 'Wenbo Li'}, {'name': 'Jinjin Gu'}, {'name': 'Jingjing Ren'}, {'name': 'Sixiang Chen'}, {'name': 'Tian Ye'}, {'name': 'Renjing Pei'}, {'name': 'Kaiwen Zhou'}, {'name': 'Fenglong Song'}, {'name': 'Lei Zhu'}]",2024-07-25T13:29:37Z
http://arxiv.org/abs/2407.18008v1,http://arxiv.org/abs/2407.18008v1,"GermanPartiesQA: Benchmarking Commercial Large Language Models for
  Political Bias and Sycophancy","LLMs are changing the way humans create and interact with content,
potentially affecting citizens' political opinions and voting decisions. As
LLMs increasingly shape our digital information ecosystems, auditing to
evaluate biases, sycophancy, or steerability has emerged as an active field of
research. In this paper, we evaluate and compare the alignment of six LLMs by
OpenAI, Anthropic, and Cohere with German party positions and evaluate
sycophancy based on a prompt experiment. We contribute to evaluating political
bias and sycophancy in multi-party systems across major commercial LLMs. First,
we develop the benchmark dataset GermanPartiesQA based on the Voting Advice
Application Wahl-o-Mat covering 10 state and 1 national elections between 2021
and 2023. In our study, we find a left-green tendency across all examined LLMs.
We then conduct our prompt experiment for which we use the benchmark and
sociodemographic data of leading German parliamentarians to evaluate changes in
LLMs responses. To differentiate between sycophancy and steerabilty, we use 'I
am [politician X], ...' and 'You are [politician X], ...' prompts. Against our
expectations, we do not observe notable differences between prompting 'I am'
and 'You are'. While our findings underscore that LLM responses can be
ideologically steered with political personas, they suggest that observed
changes in LLM outputs could be better described as personalization to the
given context rather than sycophancy.","[{'name': 'Jan Batzner'}, {'name': 'Volker Stocker'}, {'name': 'Stefan Schmid'}, {'name': 'Gjergji Kasneci'}]",2024-07-25T13:04:25Z
http://arxiv.org/abs/2407.18003v3,http://arxiv.org/abs/2407.18003v3,"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache
  Consumption","Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,
have revolutionized various industries with their advanced language
comprehension. However, their efficiency is challenged by the Transformer
architecture' s struggle with handling long texts. KV-Cache has emerged as a
pivotal solution to this issue, converting the time complexity of token
generation from quadratic to linear, albeit with increased GPU memory overhead
proportional to conversation length. With the development of the LLM community
and academia, various KV-Cache compression methods have been proposed. In this
review, we dissect the various properties of KV-Cache and elaborate on various
methods currently used to optimize the KV-Cache space usage of LLMs. These
methods span the pre-training phase, deployment phase, and inference phase, and
we summarize the commonalities and differences among these methods.
Additionally, we list some metrics for evaluating the long-text capabilities of
large language models, from both efficiency and capability perspectives. Our
review thus sheds light on the evolving landscape of LLM optimization, offering
insights into future advancements in this dynamic field.","[{'name': 'Luohe Shi'}, {'name': 'Hongyi Zhang'}, {'name': 'Yao Yao'}, {'name': 'Zuchao Li'}, {'name': 'Hai Zhao'}]",2024-07-25T12:56:22Z
http://arxiv.org/abs/2407.17997v1,http://arxiv.org/abs/2407.17997v1,"On the Effect of Purely Synthetic Training Data for Different Automatic
  Speech Recognition Architectures","In this work we evaluate the utility of synthetic data for training automatic
speech recognition (ASR). We use the ASR training data to train a
text-to-speech (TTS) system similar to FastSpeech-2. With this TTS we reproduce
the original training data, training ASR systems solely on synthetic data. For
ASR, we use three different architectures, attention-based encoder-decoder,
hybrid deep neural network hidden Markov model and a Gaussian mixture hidden
Markov model, showing the different sensitivity of the models to synthetic data
generation. In order to extend previous work, we present a number of ablation
studies on the effectiveness of synthetic vs. real training data for ASR. In
particular we focus on how the gap between training on synthetic and real data
changes by varying the speaker embedding or by scaling the model size. For the
latter we show that the TTS models generalize well, even when training scores
indicate overfitting.","[{'name': 'Nick Rossenbach'}, {'name': 'Benedikt Hilmes'}, {'name': 'Ralf Schlüter'}]",2024-07-25T12:44:45Z
http://arxiv.org/abs/2407.17974v1,http://arxiv.org/abs/2407.17974v1,"What does Kiki look like? Cross-modal associations between speech sounds
  and visual shapes in vision-and-language models","Humans have clear cross-modal preferences when matching certain novel words
to visual shapes. Evidence suggests that these preferences play a prominent
role in our linguistic processing, language learning, and the origins of
signal-meaning mappings. With the rise of multimodal models in AI, such as
vision- and-language (VLM) models, it becomes increasingly important to uncover
the kinds of visio-linguistic associations these models encode and whether they
align with human representations. Informed by experiments with humans, we probe
and compare four VLMs for a well-known human cross-modal preference, the
bouba-kiki effect. We do not find conclusive evidence for this effect but
suggest that results may depend on features of the models, such as architecture
design, model size, and training details. Our findings inform discussions on
the origins of the bouba-kiki effect in human cognition and future developments
of VLMs that align well with human cross-modal associations.","[{'name': 'Tessa Verhoef'}, {'name': 'Kiana Shahrasbi'}, {'name': 'Tom Kouwenhoven'}]",2024-07-25T12:09:41Z
http://arxiv.org/abs/2407.18990v2,http://arxiv.org/abs/2407.18990v2,"Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM
  Tuning in Real-World Applications","Fine-tuning Large Language Models (LLMs) is an effective method to enhance
their performance on downstream tasks. However, choosing the appropriate
setting of tuning hyperparameters (HPs) is a labor-intensive and
computationally expensive process. Here, we provide recommended HP
configurations for practical use-cases that represent a better starting point
for practitioners, when considering two SOTA LLMs and two commonly used tuning
methods. We describe Coverage-based Search (CBS), a process for ranking HP
configurations based on an offline extensive grid search, such that the top
ranked configurations collectively provide a practical robust recommendation
for a wide range of datasets and domains. We focus our experiments on
Llama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a
total of > 10,000 tuning experiments. Our results suggest that, in general,
Llama-3-8B and LoRA should be preferred, when possible. Moreover, we show that
for both models and tuning methods, exploring only a few HP configurations, as
recommended by our analysis, can provide excellent results in practice, making
this work a valuable resource for practitioners.","[{'name': 'Alon Halfon'}, {'name': 'Shai Gretz'}, {'name': 'Ofir Arviv'}, {'name': 'Artem Spector'}, {'name': 'Orith Toledo-Ronen'}, {'name': 'Yoav Katz'}, {'name': 'Liat Ein-Dor'}, {'name': 'Michal Shmueli-Scheuer'}, {'name': 'Noam Slonim'}]",2024-07-25T12:07:55Z
http://arxiv.org/abs/2407.17960v1,http://arxiv.org/abs/2407.17960v1,"The Curious Case of Representational Alignment: Unravelling
  Visio-Linguistic Tasks in Emergent Communication","Natural language has the universal properties of being compositional and
grounded in reality. The emergence of linguistic properties is often
investigated through simulations of emergent communication in referential
games. However, these experiments have yielded mixed results compared to
similar experiments addressing linguistic properties of human language. Here we
address representational alignment as a potential contributing factor to these
results. Specifically, we assess the representational alignment between agent
image representations and between agent representations and input images. Doing
so, we confirm that the emergent language does not appear to encode human-like
conceptual visual features, since agent image representations drift away from
inputs whilst inter-agent alignment increases. We moreover identify a strong
relationship between inter-agent alignment and topographic similarity, a common
metric for compositionality, and address its consequences. To address these
issues, we introduce an alignment penalty that prevents representational drift
but interestingly does not improve performance on a compositional
discrimination task. Together, our findings emphasise the key role
representational alignment plays in simulations of language emergence.","[{'name': 'Tom Kouwenhoven'}, {'name': 'Max Peeperkorn'}, {'name': 'Bram van Dijk'}, {'name': 'Tessa Verhoef'}]",2024-07-25T11:29:27Z
http://arxiv.org/abs/2407.17940v2,http://arxiv.org/abs/2407.17940v2,Positive Text Reframing under Multi-strategy Optimization,"Differing from sentiment transfer, positive reframing seeks to substitute
negative perspectives with positive expressions while preserving the original
meaning. With the emergence of pre-trained language models (PLMs), it is
possible to achieve acceptable results by fine-tuning PLMs. Nevertheless,
generating fluent, diverse and task-constrained reframing text remains a
significant challenge. To tackle this issue, a \textbf{m}ulti-\textbf{s}trategy
\textbf{o}ptimization \textbf{f}ramework (MSOF) is proposed in this paper.
Starting from the objective of positive reframing, we first design positive
sentiment reward and content preservation reward to encourage the model to
transform the negative expressions of the original text while ensuring the
integrity and consistency of the semantics. Then, different decoding
optimization approaches are introduced to improve the quality of text
generation. Finally, based on the modeling formula of positive reframing, we
propose a multi-dimensional re-ranking method that further selects candidate
sentences from three dimensions: strategy consistency, text similarity and
fluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate
our framework achieves significant improvements on unconstrained and controlled
positive reframing tasks.","[{'name': 'Shutong Jia'}, {'name': 'Biwei Cao'}, {'name': 'Qingqing Gao'}, {'name': 'Jiuxin Cao'}, {'name': 'Bo Liu'}]",2024-07-25T10:58:42Z
http://arxiv.org/abs/2407.20274v1,http://arxiv.org/abs/2407.20274v1,"Exploring the Plausibility of Hate and Counter Speech Detectors with
  Explainable AI","In this paper we investigate the explainability of transformer models and
their plausibility for hate speech and counter speech detection. We compare
representatives of four different explainability approaches, i.e.,
gradient-based, perturbation-based, attention-based, and prototype-based
approaches, and analyze them quantitatively with an ablation study and
qualitatively in a user study. Results show that perturbation-based
explainability performs best, followed by gradient-based and attention-based
explainability. Prototypebased experiments did not yield useful results.
Overall, we observe that explainability strongly supports the users in better
understanding the model predictions.","[{'name': 'Adrian Jaques Böck'}, {'name': 'Djordje Slijepčević'}, {'name': 'Matthias Zeppelzauer'}]",2024-07-25T10:17:04Z
http://arxiv.org/abs/2407.17914v1,http://arxiv.org/abs/2407.17914v1,"Modelling Multimodal Integration in Human Concept Processing with
  Vision-and-Language Models","Representations from deep neural networks (DNNs) have proven remarkably
predictive of neural activity involved in both visual and linguistic
processing. Despite these successes, most studies to date concern unimodal
DNNs, encoding either visual or textual input but not both. Yet, there is
growing evidence that human meaning representations integrate linguistic and
sensory-motor information. Here we investigate whether the integration of
multimodal information operated by current vision-and-language DNN models
(VLMs) leads to representations that are more aligned with human brain activity
than those obtained by language-only and vision-only DNNs. We focus on fMRI
responses recorded while participants read concept words in the context of
either a full sentence or an accompanying picture. Our results reveal that VLM
representations correlate more strongly than language- and vision-only DNNs
with activations in brain areas functionally related to language processing. A
comparison between different types of visuo-linguistic architectures shows that
recent generative VLMs tend to be less brain-aligned than previous
architectures with lower performance on downstream applications. Moreover,
through an additional analysis comparing brain vs. behavioural alignment across
multiple VLMs, we show that -- with one remarkable exception -- representations
that strongly align with behavioural judgments do not correlate highly with
brain responses. This indicates that brain similarity does not go hand in hand
with behavioural similarity, and vice versa.","[{'name': 'Anna Bavaresco'}, {'name': 'Marianne de Heer Kloots'}, {'name': 'Sandro Pezzelle'}, {'name': 'Raquel Fernández'}]",2024-07-25T10:08:37Z
http://arxiv.org/abs/2407.17900v4,http://arxiv.org/abs/2407.17900v4,"The Power of Combining Data and Knowledge: GPT-4o is an Effective
  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of
  Lung Cancer","Lymph node metastasis (LNM) is a crucial factor in determining the initial
treatment for patients with lung cancer, yet accurate preoperative diagnosis of
LNM remains challenging. Recently, large language models (LLMs) have garnered
significant attention due to their remarkable text generation capabilities.
Leveraging the extensive medical knowledge learned from vast corpora, LLMs can
estimate probabilities for clinical problems, though their performance has
historically been inferior to data-driven machine learning models. In this
paper, we propose a novel ensemble method that combines the medical knowledge
acquired by LLMs with the latent patterns identified by machine learning models
to enhance LNM prediction performance. Initially, we developed machine learning
models using patient data. We then designed a prompt template to integrate the
patient data with the predicted probability from the machine learning model.
Subsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,
to estimate the likelihood of LNM based on patient data and then adjust the
estimate using the machine learning output. Finally, we collected three outputs
from the GPT-4o using the same prompt and ensembled these results as the final
prediction. Using the proposed method, our models achieved an AUC value of
0.778 and an AP value of 0.426 for LNM prediction, significantly improving
predictive performance compared to baseline machine learning models. The
experimental results indicate that GPT-4o can effectively leverage its medical
knowledge and the probabilities predicted by machine learning models to achieve
more accurate LNM predictions. These findings demonstrate that LLMs can perform
well in clinical risk prediction tasks, offering a new paradigm for integrating
medical knowledge and patient data in clinical predictions.","[{'name': 'Danqing Hu'}, {'name': 'Bing Liu'}, {'name': 'Xiaofeng Zhu'}, {'name': 'Nan Wu'}]",2024-07-25T09:42:24Z
http://arxiv.org/abs/2407.17876v1,http://arxiv.org/abs/2407.17876v1,"A Large-Scale Sensitivity Analysis on Latent Embeddings and
  Dimensionality Reductions for Text Spatializations","The semantic similarity between documents of a text corpus can be visualized
using map-like metaphors based on two-dimensional scatterplot layouts. These
layouts result from a dimensionality reduction on the document-term matrix or a
representation within a latent embedding, including topic models. Thereby, the
resulting layout depends on the input data and hyperparameters of the
dimensionality reduction and is therefore affected by changes in them.
Furthermore, the resulting layout is affected by changes in the input data and
hyperparameters of the dimensionality reduction. However, such changes to the
layout require additional cognitive efforts from the user. In this work, we
present a sensitivity study that analyzes the stability of these layouts
concerning (1) changes in the text corpora, (2) changes in the hyperparameter,
and (3) randomness in the initialization. Our approach has two stages: data
measurement and data analysis. First, we derived layouts for the combination of
three text corpora and six text embeddings and a grid-search-inspired
hyperparameter selection of the dimensionality reductions. Afterward, we
quantified the similarity of the layouts through ten metrics, concerning local
and global structures and class separation. Second, we analyzed the resulting
42817 tabular data points in a descriptive statistical analysis. From this, we
derived guidelines for informed decisions on the layout algorithm and highlight
specific hyperparameter settings. We provide our implementation as a Git
repository at
https://github.com/hpicgs/Topic-Models-and-Dimensionality-Reduction-Sensitivity-Study
and results as Zenodo archive at https://doi.org/10.5281/zenodo.12772898.","[{'name': 'Daniel Atzberger'}, {'name': 'Tim Cech'}, {'name': 'Willy Scheibel'}, {'name': 'Jürgen Döllner'}, {'name': 'Michael Behrisch'}, {'name': 'Tobias Schreck'}]",2024-07-25T08:46:49Z
http://arxiv.org/abs/2407.17874v1,http://arxiv.org/abs/2407.17874v1,Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions,"End-to-end automatic speech recognition (E2E ASR) systems have significantly
improved speech recognition through training on extensive datasets. Despite
these advancements, they still struggle to accurately recognize domain specific
words, such as proper nouns and technical terminologies. To address this
problem, we propose a method to utilize the state-of-the-art Whisper without
modifying its architecture, preserving its generalization performance while
enabling it to leverage descriptions effectively. Moreover, we propose two
additional training techniques to improve the domain specific ASR: decoder
fine-tuning, and context perturbation. We also propose a method to use a Large
Language Model (LLM) to generate descriptions with simple metadata, when
descriptions are unavailable. Our experiments demonstrate that proposed methods
notably enhance domain-specific ASR accuracy on real-life datasets, with
LLM-generated descriptions outperforming human-crafted ones in effectiveness.","[{'name': 'Jiwon Suh'}, {'name': 'Injae Na'}, {'name': 'Woohwan Jung'}]",2024-07-25T08:44:04Z
http://arxiv.org/abs/2407.17870v1,http://arxiv.org/abs/2407.17870v1,"Is the Digital Forensics and Incident Response Pipeline Ready for
  Text-Based Threats in LLM Era?","In the era of generative AI, the widespread adoption of Neural Text
Generators (NTGs) presents new cybersecurity challenges, particularly within
the realms of Digital Forensics and Incident Response (DFIR). These challenges
primarily involve the detection and attribution of sources behind advanced
attacks like spearphishing and disinformation campaigns. As NTGs evolve, the
task of distinguishing between human and NTG-authored texts becomes critically
complex. This paper rigorously evaluates the DFIR pipeline tailored for
text-based security systems, specifically focusing on the challenges of
detecting and attributing authorship of NTG-authored texts. By introducing a
novel human-NTG co-authorship text attack, termed CS-ACT, our study uncovers
significant vulnerabilities in traditional DFIR methodologies, highlighting
discrepancies between ideal scenarios and real-world conditions. Utilizing 14
diverse datasets and 43 unique NTGs, up to the latest GPT-4, our research
identifies substantial vulnerabilities in the forensic profiling phase,
particularly in attributing authorship to NTGs. Our comprehensive evaluation
points to factors such as model sophistication and the lack of distinctive
style within NTGs as significant contributors for these vulnerabilities. Our
findings underscore the necessity for more sophisticated and adaptable
strategies, such as incorporating adversarial learning, stylizing NTGs, and
implementing hierarchical attribution through the mapping of NTG lineages to
enhance source attribution. This sets the stage for future research and the
development of more resilient text-based security systems.","[{'name': 'Avanti Bhandarkar'}, {'name': 'Ronald Wilson'}, {'name': 'Anushka Swarup'}, {'name': 'Mengdi Zhu'}, {'name': 'Damon Woodard'}]",2024-07-25T08:42:53Z
http://arxiv.org/abs/2407.17866v1,http://arxiv.org/abs/2407.17866v1,Financial Statement Analysis with Large Language Models,"We investigate whether an LLM can successfully perform financial statement
analysis in a way similar to a professional human analyst. We provide
standardized and anonymous financial statements to GPT4 and instruct the model
to analyze them to determine the direction of future earnings. Even without any
narrative or industry-specific information, the LLM outperforms financial
analysts in its ability to predict earnings changes. The LLM exhibits a
relative advantage over human analysts in situations when the analysts tend to
struggle. Furthermore, we find that the prediction accuracy of the LLM is on
par with the performance of a narrowly trained state-of-the-art ML model. LLM
prediction does not stem from its training memory. Instead, we find that the
LLM generates useful narrative insights about a company's future performance.
Lastly, our trading strategies based on GPT's predictions yield a higher Sharpe
ratio and alphas than strategies based on other models. Taken together, our
results suggest that LLMs may take a central role in decision-making.","[{'name': 'Alex Kim'}, {'name': 'Maximilian Muhn'}, {'name': 'Valeri Nikolaev'}]",2024-07-25T08:36:58Z
http://arxiv.org/abs/2407.17863v1,http://arxiv.org/abs/2407.17863v1,factgenie: A Framework for Span-based Evaluation of Generated Texts,"We present factgenie: a framework for annotating and visualizing word spans
in textual model outputs. Annotations can capture various span-based phenomena
such as semantic inaccuracies or irrelevant text. With factgenie, the
annotations can be collected both from human crowdworkers and large language
models. Our framework consists of a web interface for data visualization and
gathering text annotations, powered by an easily extensible codebase.","[{'name': 'Zdeněk Kasner'}, {'name': 'Ondřej Plátek'}, {'name': 'Patrícia Schmidtová'}, {'name': 'Simone Balloccu'}, {'name': 'Ondřej Dušek'}]",2024-07-25T08:33:23Z
http://arxiv.org/abs/2407.17862v1,http://arxiv.org/abs/2407.17862v1,Exploring Description-Augmented Dataless Intent Classification,"In this work, we introduce several schemes to leverage description-augmented
embedding similarity for dataless intent classification using current
state-of-the-art (SOTA) text embedding models. We report results of our methods
on four commonly used intent classification datasets and compare against
previous works of a similar nature. Our work shows promising results for
dataless classification scaling to a large number of unseen intents. We show
competitive results and significant improvements (+6.12\% Avg.) over strong
zero-shot baselines, all without training on labelled or task-specific data.
Furthermore, we provide qualitative error analysis of the shortfalls of this
methodology to help guide future research in this area.","[{'name': 'Ruoyu Hu'}, {'name': 'Foaad Khosmood'}, {'name': 'Abbas Edalat'}]",2024-07-25T08:31:57Z
http://arxiv.org/abs/2407.17854v1,http://arxiv.org/abs/2407.17854v1,"Shapley Value-based Contrastive Alignment for Multimodal Information
  Extraction","The rise of social media and the exponential growth of multimodal
communication necessitates advanced techniques for Multimodal Information
Extraction (MIE). However, existing methodologies primarily rely on direct
Image-Text interactions, a paradigm that often faces significant challenges due
to semantic and modality gaps between images and text. In this paper, we
introduce a new paradigm of Image-Context-Text interaction, where large
multimodal models (LMMs) are utilized to generate descriptive textual context
to bridge these gaps. In line with this paradigm, we propose a novel Shapley
Value-based Contrastive Alignment (Shap-CA) method, which aligns both
context-text and context-image pairs. Shap-CA initially applies the Shapley
value concept from cooperative game theory to assess the individual
contribution of each element in the set of contexts, texts and images towards
total semantic and modality overlaps. Following this quantitative evaluation, a
contrastive learning strategy is employed to enhance the interactive
contribution within context-text/image pairs, while minimizing the influence
across these pairs. Furthermore, we design an adaptive fusion module for
selective cross-modal fusion. Extensive experiments across four MIE datasets
demonstrate that our method significantly outperforms existing state-of-the-art
methods.","[{'name': 'Wen Luo'}, {'name': 'Yu Xia'}, {'name': 'Shen Tianshu'}, {'name': 'Sujian Li'}]",2024-07-25T08:15:43Z
http://arxiv.org/abs/2407.17852v1,http://arxiv.org/abs/2407.17852v1,Scaling A Simple Approach to Zero-Shot Speech Recognition,"Despite rapid progress in increasing the language coverage of automatic
speech recognition, the field is still far from covering all languages with a
known writing script. Recent work showed promising results with a zero-shot
approach requiring only a small amount of text data, however, accuracy heavily
depends on the quality of the used phonemizer which is often weak for unseen
languages. In this paper, we present MMS Zero-shot a conceptually simpler
approach based on romanization and an acoustic model trained on data in 1,078
different languages or three orders of magnitude more than prior art. MMS
Zero-shot reduces the average character error rate by a relative 46% over 100
unseen languages compared to the best previous work. Moreover, the error rate
of our approach is only 2.5x higher compared to in-domain supervised baselines,
while our approach uses no labeled data for the evaluation languages at all.","[{'name': 'Jinming Zhao'}, {'name': 'Vineel Pratap'}, {'name': 'Michael Auli'}]",2024-07-25T08:08:55Z
http://arxiv.org/abs/2407.17844v1,http://arxiv.org/abs/2407.17844v1,"Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease
  Classification: A Systematic Review","Parkinson's disease (PD), the second most prevalent neurodegenerative
disorder worldwide, frequently presents with early-stage speech impairments.
Recent advancements in Artificial Intelligence (AI), particularly deep learning
(DL), have significantly enhanced PD diagnosis through the analysis of speech
data. Nevertheless, the progress of research is restricted by the limited
availability of publicly accessible speech-based PD datasets, primarily due to
privacy and ethical concerns. This review covers the latest DL-based AI
approaches for speech-based PD classification, focusing on performance,
available resources and associated challenges of 33 scientific works published
between 2020 and March 2024. These DL approaches are categorized into
end-to-end (E2E) learning, transfer learning (TL) and deep acoustic features
(DAF) extraction. Among E2E approaches, Convolutional Neural Networks (CNNs)
are prevalent, though Transformers are increasingly popular. E2E approaches
face challenges such as limited data and computational resources, especially
with Transformers. TL addresses these issues by providing more robust PD
diagnosis and better generalizability across languages. DAF extraction aims to
improve the explainability and interpretability of results by examining the
specific effects of deep features on both other DL approaches and more
traditional machine learning (ML) methods. However, it often underperforms
compared to E2E and TL approaches. This review also discusses unresolved issues
related to bias, explainability and privacy, highlighting the need for future
research.","[{'name': 'Lisanne van Gelderen'}, {'name': 'Cristian Tejedor-García'}]",2024-07-25T07:58:19Z
http://arxiv.org/abs/2407.20272v1,http://arxiv.org/abs/2407.20272v1,An Efficient Inference Framework for Early-exit Large Language Models,"Building efficient inference framework has gained increasing interests for
research community. Early-exit models, a variant of LLMs, improves the
inference efficiency of LLMs by skipping rest layers and directly generate
output tokens when they are confident enough. However, there is no work of LLM
inference framework that takes early-exit models into consideration. This is
non-trivial as prior art on LLM inference cannot be directly applied to
early-exit models. In this work, we solves two key challenges in building
efficient inference framework for early-exit models: (1) batch inference at
iteration-level granularity; and (2) KV cache management. For the former, we
propose to process the batch until all sequences surpass the early-exit
confidence threshold. For the latter, we propose to fill the KV cache of rest
layers before the iteration terminates. Our evaluation shows that, compared
with the original vLLM operating at full layers, our solution achieves up to
1.25x speed up.","[{'name': 'Ruijie Miao'}, {'name': 'Yihan Yan'}, {'name': 'Xinshuo Yao'}, {'name': 'Tong Yang'}]",2024-07-25T07:50:17Z
http://arxiv.org/abs/2407.17827v1,http://arxiv.org/abs/2407.17827v1,"Unified Lexical Representation for Interpretable Visual-Language
  Alignment","Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's
groundbreaking work. Although CLIP performs well, the typical direct latent
feature alignment lacks clarity in its representation and similarity scores. On
the other hand, lexical representation, a vector whose element represents the
similarity between the sample and a word from the vocabulary, is a natural
sparse representation and interpretable, providing exact matches for individual
words. However, lexical representations is difficult to learn due to no
ground-truth supervision and false-discovery issues, and thus requires complex
design to train effectively. In this paper, we introduce LexVLA, a more
interpretable VLA framework by learning a unified lexical representation for
both modalities without complex design. We use DINOv2 as our visual model for
its local-inclined features and Llama 2, a generative language model, to
leverage its in-context lexical prediction ability. To avoid the false
discovery, we propose an overuse penalty to refrain the lexical representation
from falsely frequently activating meaningless words. We demonstrate that these
two pre-trained uni-modal models can be well-aligned by fine-tuning on modest
multi-modal dataset and avoid intricate training configurations. On cross-modal
retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,
outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those
trained from scratch on even bigger datasets (e.g., 1.1B data, including
CC-12M). We conduct extensive experiments to analyze LexVLA.","[{'name': 'Yifan Li'}, {'name': 'Yikai Wang'}, {'name': 'Yanwei Fu'}, {'name': 'Dongyu Ru'}, {'name': 'Zheng Zhang'}, {'name': 'Tong He'}]",2024-07-25T07:35:27Z
http://arxiv.org/abs/2408.00588v1,http://arxiv.org/abs/2408.00588v1,"Closing the gap between open-source and commercial large language models
  for medical evidence summarization","Large language models (LLMs) hold great promise in summarizing medical
evidence. Most recent studies focus on the application of proprietary LLMs.
Using proprietary LLMs introduces multiple risk factors, including a lack of
transparency and vendor dependency. While open-source LLMs allow better
transparency and customization, their performance falls short compared to
proprietary ones. In this study, we investigated to what extent fine-tuning
open-source LLMs can further improve their performance in summarizing medical
evidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs
of systematic reviews and summaries, we fine-tuned three broadly-used,
open-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned
LLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:
8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and
15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of
fine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,
smaller fine-tuned models sometimes even demonstrated superior performance
compared to larger zero-shot models. The above trends of improvement were also
manifested in both human and GPT4-simulated evaluations. Our results can be
applied to guide model selection for tasks demanding particular domain
knowledge, such as medical evidence summarization.","[{'name': 'Gongbo Zhang'}, {'name': 'Qiao Jin'}, {'name': 'Yiliang Zhou'}, {'name': 'Song Wang'}, {'name': 'Betina R. Idnay'}, {'name': 'Yiming Luo'}, {'name': 'Elizabeth Park'}, {'name': 'Jordan G. Nestor'}, {'name': 'Matthew E. Spotnitz'}, {'name': 'Ali Soroush'}, {'name': 'Thomas Campion'}, {'name': 'Zhiyong Lu'}, {'name': 'Chunhua Weng'}, {'name': 'Yifan Peng'}]",2024-07-25T05:03:01Z
http://arxiv.org/abs/2407.17773v1,http://arxiv.org/abs/2407.17773v1,KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models,"This paper investigates visual analogical reasoning in large multimodal
models (LMMs) compared to human adults and children. A ""visual analogy"" is an
abstract rule inferred from one image and applied to another. While benchmarks
exist for testing visual reasoning in LMMs, they require advanced skills and
omit basic visual analogies that even young children can make. Inspired by
developmental psychology, we propose a new benchmark of 1,400 visual
transformations of everyday objects to test LMMs on visual analogical reasoning
and compare them to children and adults. We structure the evaluation into three
stages: identifying what changed (e.g., color, number, etc.), how it changed
(e.g., added one object), and applying the rule to new scenarios. Our findings
show that while models like GPT-4V, LLaVA-1.5, and MANTIS identify the ""what""
effectively, they struggle with quantifying the ""how"" and extrapolating this
rule to new objects. In contrast, children and adults exhibit much stronger
analogical reasoning at all three stages. Additionally, the strongest tested
model, GPT-4V, performs better in tasks involving simple visual attributes like
color and size, correlating with quicker human adult response times.
Conversely, more complex tasks such as number, rotation, and reflection, which
necessitate extensive cognitive processing and understanding of the 3D physical
world, present more significant challenges. Altogether, these findings
highlight the limitations of training models on data that primarily consists of
2D images and text.","[{'name': 'Eunice Yiu'}, {'name': 'Maan Qraitem'}, {'name': 'Charlie Wong'}, {'name': 'Anisa Noor Majhi'}, {'name': 'Yutong Bai'}, {'name': 'Shiry Ginosar'}, {'name': 'Alison Gopnik'}, {'name': 'Kate Saenko'}]",2024-07-25T05:02:39Z
http://arxiv.org/abs/2407.17772v1,http://arxiv.org/abs/2407.17772v1,"ERIT Lightweight Multimodal Dataset for Elderly Emotion Recognition and
  Multimodal Fusion Evaluation","ERIT is a novel multimodal dataset designed to facilitate research in a
lightweight multimodal fusion. It contains text and image data collected from
videos of elderly individuals reacting to various situations, as well as seven
emotion labels for each data sample. Because of the use of labeled images of
elderly users reacting emotionally, it is also facilitating research on emotion
recognition in an underrepresented age group in machine learning visual emotion
recognition. The dataset is validated through comprehensive experiments
indicating its importance in neural multimodal fusion research.","[{'name': 'Rita Frieske'}, {'name': 'Bertram E. Shi'}]",2024-07-25T05:02:27Z
http://arxiv.org/abs/2407.17771v1,http://arxiv.org/abs/2407.17771v1,Banyan: Improved Representation Learning with Explicit Structure,"We present Banyan, an improved model to learn semantic representations by
inducing explicit structure over data. In contrast to prior approaches using
structure spanning single sentences, Banyan learns by resolving multiple
constituent structures into a shared one explicitly incorporating global
context. Combined with an improved message-passing scheme inspired by Griffin,
Banyan learns significantly better representations, avoids spurious false
negatives with contrastive learning, and drastically improves memory efficiency
in such explicit-structured models. Using the Self-StrAE framework, we show
that Banyan (a) outperforms baselines using sentential structure across various
settings (b) matches or outperforms unstructured baselines like GloVe
(+augmentations) and a RoBERTa medium (+simcse) pre-trained on 100M tokens,
despite having just a handful of (non-embedding) parameters, and (c) also
learns effective representations across several low resource (Asian and
African) languages as measured on SemRel tasks.","[{'name': 'Mattia Opper'}, {'name': 'N. Siddharth'}]",2024-07-25T04:58:08Z
http://arxiv.org/abs/2407.17770v1,http://arxiv.org/abs/2407.17770v1,BotEval: Facilitating Interactive Human Evaluation,"Following the rapid progress in natural language processing (NLP) models,
language models are applied to increasingly more complex interactive tasks such
as negotiations and conversation moderations. Having human evaluators directly
interact with these NLP models is essential for adequately evaluating the
performance on such interactive tasks. We develop BotEval, an easily
customizable, open-source, evaluation toolkit that focuses on enabling
human-bot interactions as part of the evaluation process, as opposed to human
evaluators making judgements for a static input. BotEval balances flexibility
for customization and user-friendliness by providing templates for common use
cases that span various degrees of complexity and built-in compatibility with
popular crowdsourcing platforms. We showcase the numerous useful features of
BotEval through a study that evaluates the performance of various chatbots on
their effectiveness for conversational moderation and discuss how BotEval
differs from other annotation tools.","[{'name': 'Hyundong Cho'}, {'name': 'Thamme Gowda'}, {'name': 'Yuyang Huang'}, {'name': 'Zixun Lu'}, {'name': 'Tianli Tong'}, {'name': 'Jonathan May'}]",2024-07-25T04:57:31Z
http://arxiv.org/abs/2407.17745v1,http://arxiv.org/abs/2407.17745v1,"Beyond Entity Alignment: Towards Complete Knowledge Graph Alignment via
  Entity-Relation Synergy","Knowledge Graph Alignment (KGA) aims to integrate knowledge from multiple
sources to address the limitations of individual Knowledge Graphs (KGs) in
terms of coverage and depth. However, current KGA models fall short in
achieving a ``complete'' knowledge graph alignment. Existing models primarily
emphasize the linkage of cross-graph entities but overlook aligning relations
across KGs, thereby providing only a partial solution to KGA. The semantic
correlations embedded in relations are largely overlooked, potentially
restricting a comprehensive understanding of cross-KG signals. In this paper,
we propose to conceptualize relation alignment as an independent task and
conduct KGA by decomposing it into two distinct but highly correlated
sub-tasks: entity alignment and relation alignment. To capture the mutually
reinforcing correlations between these objectives, we propose a novel
Expectation-Maximization-based model, EREM, which iteratively optimizes both
sub-tasks. Experimental results on real-world datasets demonstrate that EREM
consistently outperforms state-of-the-art models in both entity alignment and
relation alignment tasks.","[{'name': 'Xiaohan Fang'}, {'name': 'Chaozhuo Li'}, {'name': 'Yi Zhao'}, {'name': 'Qian Zang'}, {'name': 'Litian Zhang'}, {'name': 'Jiquan Peng'}, {'name': 'Xi Zhang'}, {'name': 'Jibing Gong'}]",2024-07-25T03:40:09Z
http://arxiv.org/abs/2407.17734v1,http://arxiv.org/abs/2407.17734v1,"Cost-effective Instruction Learning for Pathology Vision and Language
  Analysis","The advent of vision-language models fosters the interactive conversations
between AI-enabled models and humans. Yet applying these models into clinics
must deal with daunting challenges around large-scale training data, financial,
and computational resources. Here we propose a cost-effective instruction
learning framework for conversational pathology named as CLOVER. CLOVER only
trains a lightweight module and uses instruction tuning while freezing the
parameters of the large language model. Instead of using costly GPT-4, we
propose well-designed prompts on GPT-3.5 for building generation-based
instructions, emphasizing the utility of pathological knowledge derived from
the Internet source. To augment the use of instructions, we construct a
high-quality set of template-based instructions in the context of digital
pathology. From two benchmark datasets, our findings reveal the strength of
hybrid-form instructions in the visual question-answer in pathology. Extensive
results show the cost-effectiveness of CLOVER in answering both open-ended and
closed-ended questions, where CLOVER outperforms strong baselines that possess
37 times more training parameters and use instruction data generated from
GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot
learning in the external clinical dataset. These findings demonstrate that
cost-effective modeling of CLOVER could accelerate the adoption of rapid
conversational applications in the landscape of digital pathology.","[{'name': 'Kaitao Chen'}, {'name': 'Mianxin Liu'}, {'name': 'Fang Yan'}, {'name': 'Lei Ma'}, {'name': 'Xiaoming Shi'}, {'name': 'Lilong Wang'}, {'name': 'Xiaosong Wang'}, {'name': 'Lifeng Zhu'}, {'name': 'Zhe Wang'}, {'name': 'Mu Zhou'}, {'name': 'Shaoting Zhang'}]",2024-07-25T03:12:57Z
http://arxiv.org/abs/2407.17730v1,http://arxiv.org/abs/2407.17730v1,"Are Large Language Models Possible to Conduct Cognitive Behavioral
  Therapy?","In contemporary society, the issue of psychological health has become
increasingly prominent, characterized by the diversification, complexity, and
universality of mental disorders. Cognitive Behavioral Therapy (CBT), currently
the most influential and clinically effective psychological treatment method
with no side effects, has limited coverage and poor quality in most countries.
In recent years, researches on the recognition and intervention of emotional
disorders using large language models (LLMs) have been validated, providing new
possibilities for psychological assistance therapy. However, are LLMs truly
possible to conduct cognitive behavioral therapy? Many concerns have been
raised by mental health experts regarding the use of LLMs for therapy. Seeking
to answer this question, we collected real CBT corpus from online video
websites, designed and conducted a targeted automatic evaluation framework
involving the evaluation of emotion tendency of generated text, structured
dialogue pattern and proactive inquiry ability. For emotion tendency, we
calculate the emotion tendency score of the CBT dialogue text generated by each
model. For structured dialogue pattern, we use a diverse range of automatic
evaluation metrics to compare speaking style, the ability to maintain
consistency of topic and the use of technology in CBT between different models
. As for inquiring to guide the patient, we utilize PQA (Proactive Questioning
Ability) metric. We also evaluated the CBT ability of the LLM after integrating
a CBT knowledge base to explore the help of introducing additional knowledge to
enhance the model's CBT counseling ability. Four LLM variants with excellent
performance on natural language processing are evaluated, and the experimental
result shows the great potential of LLMs in psychological counseling realm,
especially after combining with other technological means.","[{'name': 'Hao Shen'}, {'name': 'Zihan Li'}, {'name': 'Minqiang Yang'}, {'name': 'Minghui Ni'}, {'name': 'Yongfeng Tao'}, {'name': 'Zhengyang Yu'}, {'name': 'Weihao Zheng'}, {'name': 'Chen Xu'}, {'name': 'Bin Hu'}]",2024-07-25T03:01:47Z
http://arxiv.org/abs/2407.21057v1,http://arxiv.org/abs/2407.21057v1,Multi-group Uncertainty Quantification for Long-form Text Generation,"While large language models are rapidly moving towards consumer-facing
applications, they are often still prone to factual errors and hallucinations.
In order to reduce the potential harms that may come from these errors, it is
important for users to know to what extent they can trust an LLM when it makes
a factual claim. To this end, we study the problem of uncertainty
quantification of factual correctness in long-form natural language generation.
Given some output from a large language model, we study both uncertainty at the
level of individual claims contained within the output (via calibration) and
uncertainty across the entire output itself (via conformal prediction).
Moreover, we invoke multicalibration and multivalid conformal prediction to
ensure that such uncertainty guarantees are valid both marginally and across
distinct groups of prompts. Using the task of biography generation, we
demonstrate empirically that having access to and making use of additional
group attributes for each prompt improves both overall and group-wise
performance. As the problems of calibration, conformal prediction, and their
multi-group counterparts have not been extensively explored previously in the
context of long-form text generation, we consider these empirical results to
form a benchmark for this setting.","[{'name': 'Terrance Liu'}, {'name': 'Zhiwei Steven Wu'}]",2024-07-25T02:59:52Z
http://arxiv.org/abs/2407.17716v1,http://arxiv.org/abs/2407.17716v1,"Describe Where You Are: Improving Noise-Robustness for Speech Emotion
  Recognition with Text Description of the Environment","Speech emotion recognition (SER) systems often struggle in real-world
environments, where ambient noise severely degrades their performance. This
paper explores a novel approach that exploits prior knowledge of testing
environments to maximize SER performance under noisy conditions. To address
this task, we propose a text-guided, environment-aware training where an SER
model is trained with contaminated speech samples and their paired noise
description. We use a pre-trained text encoder to extract the text-based
environment embedding and then fuse it to a transformer-based SER model during
training and inference. We demonstrate the effectiveness of our approach
through our experiment with the MSP-Podcast corpus and real-world additive
noise samples collected from the Freesound repository. Our experiment indicates
that the text-based environment descriptions processed by a large language
model (LLM) produce representations that improve the noise-robustness of the
SER system. In addition, our proposed approach with an LLM yields better
performance than our environment-agnostic baselines, especially in low
signal-to-noise ratio (SNR) conditions. When testing at -5dB SNR level, our
proposed method shows better performance than our best baseline model by 31.8 %
(arousal), 23.5% (dominance), and 9.5% (valence).","[{'name': 'Seong-Gyun Leem'}, {'name': 'Daniel Fulford'}, {'name': 'Jukka-Pekka Onnela'}, {'name': 'David Gard'}, {'name': 'Carlos Busso'}]",2024-07-25T02:30:40Z
http://arxiv.org/abs/2407.17695v1,http://arxiv.org/abs/2407.17695v1,Enhancing Agent Learning through World Dynamics Modeling,"While large language models (LLMs) have been increasingly deployed across
tasks in language understanding and interactive decision-making, their
impressive performance is largely due to the comprehensive and in-depth domain
knowledge embedded within them. However, the extent of this knowledge can vary
across different domains. Existing methods often assume that LLMs already
possess such comprehensive and in-depth knowledge of their environment,
overlooking potential gaps in their understanding of actual world dynamics. To
address this gap, we introduce Discover, Verify, and Evolve (DiVE), a framework
that discovers world dynamics from a small number of demonstrations, verifies
the correctness of these dynamics, and evolves new, advanced dynamics tailored
to the current situation. Through extensive evaluations, we analyze the impact
of each component on performance and compare the automatically generated
dynamics from DiVE with human-annotated world dynamics. Our results demonstrate
that LLMs guided by DiVE can make better decisions, achieving rewards
comparable to human players in the Crafter environment.","[{'name': 'Zhiyuan Sun'}, {'name': 'Haochen Shi'}, {'name': 'Marc-Alexandre Côté'}, {'name': 'Glen Berseth'}, {'name': 'Xingdi Yuan'}, {'name': 'Bang Liu'}]",2024-07-25T01:32:41Z
http://arxiv.org/abs/2407.17688v2,http://arxiv.org/abs/2407.17688v2,"Examining the Influence of Political Bias on Large Language Model
  Performance in Stance Classification","Large Language Models (LLMs) have demonstrated remarkable capabilities in
executing tasks based on natural language queries. However, these models,
trained on curated datasets, inherently embody biases ranging from racial to
national and gender biases. It remains uncertain whether these biases impact
the performance of LLMs for certain tasks. In this study, we investigate the
political biases of LLMs within the stance classification task, specifically
examining whether these models exhibit a tendency to more accurately classify
politically-charged stances. Utilizing three datasets, seven LLMs, and four
distinct prompting schemes, we analyze the performance of LLMs on politically
oriented statements and targets. Our findings reveal a statistically
significant difference in the performance of LLMs across various politically
oriented stance classification tasks. Furthermore, we observe that this
difference primarily manifests at the dataset level, with models and prompting
schemes showing statistically similar performances across different stance
classification datasets. Lastly, we observe that when there is greater
ambiguity in the target the statement is directed towards, LLMs have poorer
stance classification accuracy.
  Code & Dataset: http://doi.org/10.5281/zenodo.12938478","[{'name': 'Lynnette Hui Xian Ng'}, {'name': 'Iain Cruickshank'}, {'name': 'Roy Ka-Wei Lee'}]",2024-07-25T01:11:38Z
http://arxiv.org/abs/2407.17686v1,http://arxiv.org/abs/2407.17686v1,Transformers on Markov Data: Constant Depth Suffices,"Attention-based transformers have been remarkably successful at modeling
generative processes across various domains and modalities. In this paper, we
study the behavior of transformers on data drawn from \kth Markov processes,
where the conditional distribution of the next symbol in a sequence depends on
the previous $k$ symbols observed. We observe a surprising phenomenon
empirically which contradicts previous findings: when trained for sufficiently
long, a transformer with a fixed depth and $1$ head per layer is able to
achieve low test loss on sequences drawn from \kth Markov sources, even as $k$
grows. Furthermore, this low test loss is achieved by the transformer's ability
to represent and learn the in-context conditional empirical distribution. On
the theoretical side, our main result is that a transformer with a single head
and three layers can represent the in-context conditional empirical
distribution for \kth Markov sources, concurring with our empirical
observations. Along the way, we prove that \textit{attention-only} transformers
with $O(\log_2(k))$ layers can represent the in-context conditional empirical
distribution by composing induction heads to track the previous $k$ symbols in
the sequence. These results provide more insight into our current understanding
of the mechanisms by which transformers learn to capture context, by
understanding their behavior on Markov sources.","[{'name': 'Nived Rajaraman'}, {'name': 'Marco Bondaschi'}, {'name': 'Kannan Ramchandran'}, {'name': 'Michael Gastpar'}, {'name': 'Ashok Vardhan Makkuva'}]",2024-07-25T01:07:09Z
http://arxiv.org/abs/2407.17678v1,http://arxiv.org/abs/2407.17678v1,"Efficient LLM Training and Serving with Heterogeneous Context Sharding
  among Attention Heads","Existing LLM training and inference frameworks struggle in boosting
efficiency with sparsity while maintaining the integrity of context and model
architecture. Inspired by the sharding concept in database and the fact that
attention parallelizes over heads on accelerators, we propose Sparsely-Sharded
(S2) Attention, an attention algorithm that allocates heterogeneous context
partitions for different attention heads to divide and conquer. S2-Attention
enforces each attention head to only attend to a partition of contexts
following a strided sparsity pattern, while the full context is preserved as
the union of all the shards. As attention heads are processed in separate
thread blocks, the context reduction for each head can thus produce end-to-end
speed-up and memory reduction. At inference, LLMs trained with S2-Attention can
then take the KV cache reduction as free meals with guaranteed model quality
preserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X
wall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction
in end-to-end training time and 10X inference latency, (2) on-par model
training quality compared to default attention, (3)perfect needle retrieval
accuracy over 32K context window. On top of the algorithm, we build DKernel, an
LLM training and inference kernel library that allows users to customize
sparsity patterns for their own models. We open-sourced DKerneland make it
compatible with Megatron, Pytorch, and vLLM.","[{'name': 'Xihui Lin'}, {'name': 'Yunan Zhang'}, {'name': 'Suyu Ge'}, {'name': 'Barun Patra'}, {'name': 'Vishrav Chaudhary'}, {'name': 'Xia Song'}]",2024-07-25T00:27:07Z
http://arxiv.org/abs/2407.17638v2,http://arxiv.org/abs/2407.17638v2,Time Matters: Examine Temporal Effects on Biomedical Language Models,"Time roots in applying language models for biomedical applications: models
are trained on historical data and will be deployed for new or future data,
which may vary from training data. While increasing biomedical tasks have
employed state-of-the-art language models, there are very few studies have
examined temporal effects on biomedical models when data usually shifts across
development and deployment. This study fills the gap by statistically probing
relations between language model performance and data shifts across three
biomedical tasks. We deploy diverse metrics to evaluate model performance,
distance methods to measure data drifts, and statistical methods to quantify
temporal effects on biomedical language models. Our study shows that time
matters for deploying biomedical language models, while the degree of
performance degradation varies by biomedical tasks and statistical
quantification approaches. We believe this study can establish a solid
benchmark to evaluate and assess temporal effects on deploying biomedical
language models.","[{'name': 'Weisi Liu'}, {'name': 'Zhe He'}, {'name': 'Xiaolei Huang'}]",2024-07-24T21:06:40Z
http://arxiv.org/abs/2407.17636v1,http://arxiv.org/abs/2407.17636v1,"IgnitionInnovators at ""Discharge Me!"": Chain-of-Thought Instruction
  Finetuning Large Language Models for Discharge Summaries","This paper presents our proposed approach to the Discharge Me! shared task,
collocated with the 23th Workshop on Biomedical Natural Language Processing
(BioNLP). In this work, we develop an LLM-based framework for solving the
Discharge Summary Documentation (DSD) task, i.e., generating the two critical
target sections `Brief Hospital Course' and `Discharge Instructions' in the
discharge summary. By streamlining the recent instruction-finetuning process on
LLMs, we explore several prompting strategies for optimally adapting LLMs to
specific generation task of DSD. Experimental results show that providing a
clear output structure, complimented by a set of comprehensive
Chain-of-Thoughts (CoT) questions, effectively improves the model's reasoning
capability, and thereby, enhancing the structural correctness and faithfulness
of clinical information in the generated text. Source code is available at:
https://github.com/antangrocket1312/Discharge_LLM","[{'name': 'An Quang Tang'}, {'name': 'Xiuzhen Zhang'}, {'name': 'Minh Ngoc Dinh'}]",2024-07-24T21:02:53Z
http://arxiv.org/abs/2407.17629v2,http://arxiv.org/abs/2407.17629v2,"Papilusion at DAGPap24: Paper or Illusion? Detecting AI-generated
  Scientific Papers","This paper presents Papilusion, an AI-generated scientific text detector
developed within the DAGPap24 shared task on detecting automatically generated
scientific papers. We propose an ensemble-based approach and conduct ablation
studies to analyze the effect of the detector configurations on the
performance. Papilusion is ranked 6th on the leaderboard, and we improve our
performance after the competition ended, achieving 99.46 (+9.63) of the
F1-score on the official test set.","[{'name': 'Nikita Andreev'}, {'name': 'Alexander Shirnin'}, {'name': 'Vladislav Mikhailov'}, {'name': 'Ekaterina Artemova'}]",2024-07-24T20:38:13Z
http://arxiv.org/abs/2407.17624v1,http://arxiv.org/abs/2407.17624v1,"Traditional Methods Outperform Generative LLMs at Forecasting Credit
  Ratings","Large Language Models (LLMs) have been shown to perform well for many
downstream tasks. Transfer learning can enable LLMs to acquire skills that were
not targeted during pre-training. In financial contexts, LLMs can sometimes
beat well-established benchmarks. This paper investigates how well LLMs perform
in the task of forecasting corporate credit ratings. We show that while LLMs
are very good at encoding textual information, traditional methods are still
very competitive when it comes to encoding numeric and multimodal data. For our
task, current LLMs perform worse than a more traditional XGBoost architecture
that combines fundamental and macroeconomic data with high-density text-based
embedding features.","[{'name': 'Felix Drinkall'}, {'name': 'Janet B. Pierrehumbert'}, {'name': 'Stefan Zohren'}]",2024-07-24T20:30:55Z
http://arxiv.org/abs/2407.17605v1,http://arxiv.org/abs/2407.17605v1,Coupling Speech Encoders with Downstream Text Models,"We present a modular approach to building cascade speech translation (AST)
models that guarantees that the resulting model performs no worse than the
1-best cascade baseline while preserving state-of-the-art speech recognition
(ASR) and text translation (MT) performance for a given task. Our novel
contribution is the use of an ``exporter'' layer that is trained under L2-loss
to ensure a strong match between ASR embeddings and the MT token embeddings for
the 1-best sequence. The ``exporter'' output embeddings are fed directly to the
MT model in lieu of 1-best token embeddings, thus guaranteeing that the
resulting model performs no worse than the 1-best cascade baseline, while
allowing back-propagation gradient to flow from the MT model into the ASR
components. The matched-embeddings cascade architecture provide a significant
improvement over its 1-best counterpart in scenarios where incremental training
of the MT model is not an option and yet we seek to improve quality by
leveraging (speech, transcription, translated transcription) data provided with
the AST task. The gain disappears when the MT model is incrementally trained on
the parallel text data available with the AST task. The approach holds promise
for other scenarios that seek to couple ASR encoders and immutable text models,
such at large language models (LLM).","[{'name': 'Ciprian Chelba'}, {'name': 'Johan Schalkwyk'}]",2024-07-24T19:29:13Z
http://arxiv.org/abs/2407.17469v1,http://arxiv.org/abs/2407.17469v1,I Could've Asked That: Reformulating Unanswerable Questions,"When seeking information from unfamiliar documents, users frequently pose
questions that cannot be answered by the documents. While existing large
language models (LLMs) identify these unanswerable questions, they do not
assist users in reformulating their questions, thereby reducing their overall
utility. We curate CouldAsk, an evaluation benchmark composed of existing and
new datasets for document-grounded question answering, specifically designed to
study reformulating unanswerable questions. We evaluate state-of-the-art
open-source and proprietary LLMs on CouldAsk. The results demonstrate the
limited capabilities of these models in reformulating questions. Specifically,
GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the
time, respectively. Error analysis shows that 62% of the unsuccessful
reformulations stem from the models merely rephrasing the questions or even
generating identical questions. We publicly release the benchmark and the code
to reproduce the experiments.","[{'name': 'Wenting Zhao'}, {'name': 'Ge Gao'}, {'name': 'Claire Cardie'}, {'name': 'Alexander M. Rush'}]",2024-07-24T17:59:07Z
http://arxiv.org/abs/2407.17468v1,http://arxiv.org/abs/2407.17468v1,"WildHallucinations: Evaluating Long-form Factuality in LLMs with
  Real-World Entity Queries","While hallucinations of large language models (LLMs) prevail as a major
challenge, existing evaluation benchmarks on factuality do not cover the
diverse domains of knowledge that the real-world users of LLMs seek information
about. To bridge this gap, we introduce WildHallucinations, a benchmark that
evaluates factuality. It does so by prompting LLMs to generate information
about entities mined from user-chatbot conversations in the wild. These
generations are then automatically fact-checked against a systematically
curated knowledge source collected from web search. Notably, half of these
real-world entities do not have associated Wikipedia pages. We evaluate 118,785
generations from 15 LLMs on 7,919 entities. We find that LLMs consistently
hallucinate more on entities without Wikipedia pages and exhibit varying
hallucination rates across different domains. Finally, given the same base
models, adding a retrieval component only slightly reduces hallucinations but
does not eliminate hallucinations.","[{'name': 'Wenting Zhao'}, {'name': 'Tanya Goyal'}, {'name': 'Yu Ying Chiu'}, {'name': 'Liwei Jiang'}, {'name': 'Benjamin Newman'}, {'name': 'Abhilasha Ravichander'}, {'name': 'Khyathi Chandu'}, {'name': 'Ronan Le Bras'}, {'name': 'Claire Cardie'}, {'name': 'Yuntian Deng'}, {'name': 'Yejin Choi'}]",2024-07-24T17:59:05Z
http://arxiv.org/abs/2407.17467v1,http://arxiv.org/abs/2407.17467v1,"CMR Scaling Law: Predicting Critical Mixture Ratios for Continual
  Pre-training of Language Models","Large Language Models (LLMs) excel in diverse tasks but often underperform in
specialized fields due to limited domain-specific or proprietary corpus.
Continual pre-training (CPT) enhances LLM capabilities by imbuing new
domain-specific or proprietary knowledge while replaying general corpus to
prevent catastrophic forgetting. The data mixture ratio of general corpus and
domain-specific corpus, however, has been chosen heuristically, leading to
sub-optimal training efficiency in practice. In this context, we attempt to
re-visit the scaling behavior of LLMs under the hood of CPT, and discover a
power-law relationship between loss, mixture ratio, and training tokens scale.
We formalize the trade-off between general and domain-specific capabilities,
leading to a well-defined Critical Mixture Ratio (CMR) of general and domain
data. By striking the balance, CMR maintains the model's general ability and
achieves the desired domain transfer, ensuring the highest utilization of
available resources. Therefore, if we value the balance between efficiency and
effectiveness, CMR can be consider as the optimal mixture ratio.Through
extensive experiments, we ascertain the predictability of CMR, and propose CMR
scaling law and have substantiated its generalization. These findings offer
practical guidelines for optimizing LLM training in specialized domains,
ensuring both general and domain-specific performance while efficiently
managing training resources.","[{'name': 'Jiawei Gu'}, {'name': 'Zacc Yang'}, {'name': 'Chuanghao Ding'}, {'name': 'Rui Zhao'}, {'name': 'Fei Tan'}]",2024-07-24T17:59:02Z
http://arxiv.org/abs/2407.17546v1,http://arxiv.org/abs/2407.17546v1,"Exploring Domain Robust Lightweight Reward Models based on Router
  Mechanism","Recent advancements in large language models have heavily relied on the large
reward model from reinforcement learning from human feedback for fine-tuning.
However, the use of a single reward model across various domains may not always
be optimal, often requiring retraining from scratch when new domain data is
introduced. To address these challenges, we explore the utilization of small
language models operating in a domain-specific manner based on router
mechanisms. Our three approaches are: 1) utilize mixture of experts to form a
single reward model by modularizing an internal router and experts, 2)
employing external router to select the appropriate reward model from multiple
domain-specific models, and 3) the framework reduces parameter size by loading
reward models and router adapters onto a single small language model using
adapters. Experimental validation underscores the effectiveness of our
approach, demonstrating performance comparable to baseline methods while also
reducing the total parameter size.","[{'name': 'Hyuk Namgoong'}, {'name': 'Jeesu Jung'}, {'name': 'Sangkeun Jung'}, {'name': 'Yoonhyung Roh'}]",2024-07-24T17:25:12Z
http://arxiv.org/abs/2407.17447v1,http://arxiv.org/abs/2407.17447v1,Fluent Student-Teacher Redteaming,"Many publicly available language models have been safety tuned to reduce the
likelihood of toxic or liability-inducing text. Users or security analysts
attempt to jailbreak or redteam these models with adversarial prompts which
cause compliance with requests. One attack method is to apply discrete
optimization techniques to the prompt. However, the resulting attack strings
are often gibberish text, easily filtered by defenders due to high measured
perplexity, and may fail for unseen tasks and/or well-tuned models. In this
work, we improve existing algorithms (primarily GCG and BEAST) to develop
powerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our
technique centers around a new distillation-based approach that encourages the
victim model to emulate a toxified finetune, either in terms of output
probabilities or internal activations. To encourage human-fluent attacks, we
add a multi-model perplexity penalty and a repetition penalty to the objective.
We also enhance optimizer strength by allowing token insertions, token swaps,
and token deletions and by using longer attack sequences. The resulting process
is able to reliably jailbreak the most difficult target models with prompts
that appear similar to human-written prompts. On Advbench we achieve attack
success rates $>93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while
maintaining model-measured perplexity $<33$; we achieve $95$% attack success
for Phi-3, though with higher perplexity. We also find a universally-optimized
single fluent prompt that induces $>88$% compliance on previously unseen tasks
across Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box
models.","[{'name': 'T. Ben Thompson'}, {'name': 'Michael Sklar'}]",2024-07-24T17:23:18Z
http://arxiv.org/abs/2407.17406v1,http://arxiv.org/abs/2407.17406v1,"Dependency Transformer Grammars: Integrating Dependency Structures into
  Transformer Language Models","Syntactic Transformer language models aim to achieve better generalization
through simultaneously modeling syntax trees and sentences. While prior work
has been focusing on adding constituency-based structures to Transformers, we
introduce Dependency Transformer Grammars (DTGs), a new class of Transformer
language model with explicit dependency-based inductive bias. DTGs simulate
dependency transition systems with constrained attention patterns by modifying
attention masks, incorporate the stack information through relative positional
encoding, and augment dependency arc representation with a combination of token
embeddings and operation embeddings. When trained on a dataset of sentences
annotated with dependency trees, DTGs achieve better generalization while
maintaining comparable perplexity with Transformer language model baselines.
DTGs also outperform recent constituency-based models, showing that dependency
can better guide Transformer language models. Our code is released at
https://github.com/zhaoyd1/Dep_Transformer_Grammars.","[{'name': 'Yida Zhao'}, {'name': 'Chao Lou'}, {'name': 'Kewei Tu'}]",2024-07-24T16:38:38Z
http://arxiv.org/abs/2407.17545v1,http://arxiv.org/abs/2407.17545v1,"Large Language Models for Anomaly Detection in Computational Workflows:
  from Supervised Fine-Tuning to In-Context Learning","Anomaly detection in computational workflows is critical for ensuring system
reliability and security. However, traditional rule-based methods struggle to
detect novel anomalies. This paper leverages large language models (LLMs) for
workflow anomaly detection by exploiting their ability to learn complex data
patterns. Two approaches are investigated: 1) supervised fine-tuning (SFT),
where pre-trained LLMs are fine-tuned on labeled data for sentence
classification to identify anomalies, and 2) in-context learning (ICL) where
prompts containing task descriptions and examples guide LLMs in few-shot
anomaly detection without fine-tuning. The paper evaluates the performance,
efficiency, generalization of SFT models, and explores zero-shot and few-shot
ICL prompts and interpretability enhancement via chain-of-thought prompting.
Experiments across multiple workflow datasets demonstrate the promising
potential of LLMs for effective anomaly detection in complex executions.","[{'name': 'Hongwei Jin'}, {'name': 'George Papadimitriou'}, {'name': 'Krishnan Raghavan'}, {'name': 'Pawel Zuk'}, {'name': 'Prasanna Balaprakash'}, {'name': 'Cong Wang'}, {'name': 'Anirban Mandal'}, {'name': 'Ewa Deelman'}]",2024-07-24T16:33:04Z
http://arxiv.org/abs/2408.01453v1,http://arxiv.org/abs/2408.01453v1,"Reporting and Analysing the Environmental Impact of Language Models on
  the Example of Commonsense Question Answering with External Knowledge","Human-produced emissions are growing at an alarming rate, causing already
observable changes in the climate and environment in general. Each year global
carbon dioxide emissions hit a new record, and it is reported that 0.5% of
total US greenhouse gas emissions are attributed to data centres as of 2021.
The release of ChatGPT in late 2022 sparked social interest in Large Language
Models (LLMs), the new generation of Language Models with a large number of
parameters and trained on massive amounts of data. Currently, numerous
companies are releasing products featuring various LLMs, with many more models
in development and awaiting release. Deep Learning research is a competitive
field, with only models that reach top performance attracting attention and
being utilized. Hence, achieving better accuracy and results is often the first
priority, while the model's efficiency and the environmental impact of the
study are neglected. However, LLMs demand substantial computational resources
and are very costly to train, both financially and environmentally. It becomes
essential to raise awareness and promote conscious decisions about algorithmic
and hardware choices. Providing information on training time, the approximate
carbon dioxide emissions and power consumption would assist future studies in
making necessary adjustments and determining the compatibility of available
computational resources with model requirements. In this study, we infused T5
LLM with external knowledge and fine-tuned the model for Question-Answering
task. Furthermore, we calculated and reported the approximate environmental
impact for both steps. The findings demonstrate that the smaller models may not
always be sustainable options, and increased training does not always imply
better performance. The most optimal outcome is achieved by carefully
considering both performance and efficiency factors.","[{'name': 'Aida Usmanova'}, {'name': 'Junbo Huang'}, {'name': 'Debayan Banerjee'}, {'name': 'Ricardo Usbeck'}]",2024-07-24T16:16:16Z
http://arxiv.org/abs/2407.17390v1,http://arxiv.org/abs/2407.17390v1,CovScore: Evaluation of Multi-Document Abstractive Title Set Generation,"This paper introduces CovScore, an automatic reference-less methodology for
evaluating thematic title sets, extracted from a corpus of documents. While
such extraction methods are widely used, evaluating their effectiveness remains
an open question. Moreover, some existing practices heavily rely on slow and
laborious human annotation procedures. Inspired by recently introduced
LLM-based judge methods, we propose a novel methodology that decomposes quality
into five main metrics along different aspects of evaluation. This framing
simplifies and expedites the manual evaluation process and enables automatic
and independent LLM-based evaluation. As a test case, we apply our approach to
a corpus of Holocaust survivor testimonies, motivated both by its relevance to
title set extraction and by the moral significance of this pursuit. We validate
the methodology by experimenting with naturalistic and synthetic title set
generation systems and compare their performance with the methodology.","[{'name': 'Itamar Trainin'}, {'name': 'Omri Abend'}]",2024-07-24T16:14:15Z
http://arxiv.org/abs/2407.17387v1,http://arxiv.org/abs/2407.17387v1,PERSONA: A Reproducible Testbed for Pluralistic Alignment,"The rapid advancement of language models (LMs) necessitates robust alignment
with diverse user values. However, current preference optimization approaches
often fail to capture the plurality of user opinions, instead reinforcing
majority viewpoints and marginalizing minority perspectives. We introduce
PERSONA, a reproducible test bed designed to evaluate and improve pluralistic
alignment of LMs. We procedurally generate diverse user profiles from US census
data, resulting in 1,586 synthetic personas with varied demographic and
idiosyncratic attributes. We then generate a large-scale evaluation dataset
containing 3,868 prompts and 317,200 feedback pairs obtained from our synthetic
personas. Leveraging this dataset, we systematically evaluate LM capabilities
in role-playing diverse users, verified through human judges, and the
establishment of both a benchmark, PERSONA Bench, for pluralistic alignment
approaches as well as an extensive dataset to create new and future benchmarks.
The full dataset and benchmarks are available here:
https://www.synthlabs.ai/research/persona.","[{'name': 'Louis Castricato'}, {'name': 'Nathan Lile'}, {'name': 'Rafael Rafailov'}, {'name': 'Jan-Philipp Fränken'}, {'name': 'Chelsea Finn'}]",2024-07-24T16:11:39Z
http://arxiv.org/abs/2407.17383v1,http://arxiv.org/abs/2407.17383v1,"A Comprehensive Approach to Misspelling Correction with BERT and
  Levenshtein Distance","Writing, as an omnipresent form of human communication, permeates nearly
every aspect of contemporary life. Consequently, inaccuracies or errors in
written communication can lead to profound consequences, ranging from financial
losses to potentially life-threatening situations. Spelling mistakes, among the
most prevalent writing errors, are frequently encountered due to various
factors. This research aims to identify and rectify diverse spelling errors in
text using neural networks, specifically leveraging the Bidirectional Encoder
Representations from Transformers (BERT) masked language model. To achieve this
goal, we compiled a comprehensive dataset encompassing both non-real-word and
real-word errors after categorizing different types of spelling mistakes.
Subsequently, multiple pre-trained BERT models were employed. To ensure optimal
performance in correcting misspelling errors, we propose a combined approach
utilizing the BERT masked language model and Levenshtein distance. The results
from our evaluation data demonstrate that the system presented herein exhibits
remarkable capabilities in identifying and rectifying spelling mistakes, often
surpassing existing systems tailored for the Persian language.","[{'name': 'Amirreza Naziri'}, {'name': 'Hossein Zeinali'}]",2024-07-24T16:07:11Z
http://arxiv.org/abs/2407.17379v2,http://arxiv.org/abs/2407.17379v2,"MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image
  Relational Association Capabilities in Large Visual Language Models","Given the remarkable success that large visual language models (LVLMs) have
achieved in image perception tasks, the endeavor to make LVLMs perceive the
world like humans is drawing increasing attention. Current multi-modal
benchmarks primarily focus on facts or specific topic-related knowledge
contained within individual images. However, they often overlook the
associative relations between multiple images, which require the identification
and analysis of similarities among entities or content present in different
images. Therefore, we propose the multi-image relation association task and a
meticulously curated Multi-granularity Multi-image Relational Association
(MMRA) benchmark, comprising 1,024 samples. In order to systematically and
comprehensively evaluate current LVLMs, we establish an associational relation
system among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)
at two granularity levels (i.e., image and entity) according to the relations
in ConceptNet. Our experiments reveal that on the MMRA benchmark, current
multi-image LVLMs exhibit distinct advantages and disadvantages across various
subtasks. Notably, fine-grained, entity-level multi-image perception tasks pose
a greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs
perform poorly on spatial-related tasks, indicating that LVLMs still have
limited spatial awareness. Additionally, our findings indicate that while LVLMs
demonstrate a strong capability to perceive image details, enhancing their
ability to associate information across multiple images hinges on improving the
reasoning capabilities of their language model component. Moreover, we explored
the ability of LVLMs to perceive image sequences within the context of our
multi-image association task. Our experiments show that the majority of current
LVLMs do not adequately model image sequences during the pre-training process.","[{'name': 'Siwei Wu'}, {'name': 'Kang Zhu'}, {'name': 'Yu Bai'}, {'name': 'Yiming Liang'}, {'name': 'Yizhi Li'}, {'name': 'Haoning Wu'}, {'name': 'J. H. Liu'}, {'name': 'Ruibo Liu'}, {'name': 'Xingwei Qu'}, {'name': 'Xuxin Cheng'}, {'name': 'Ge Zhang'}, {'name': 'Wenhao Huang'}, {'name': 'Chenghua Lin'}]",2024-07-24T15:59:01Z
http://arxiv.org/abs/2407.17349v1,http://arxiv.org/abs/2407.17349v1,"Boosting Large Language Models with Socratic Method for Conversational
  Mathematics Teaching","With the introduction of large language models (LLMs), automatic math
reasoning has seen tremendous success. However, current methods primarily focus
on providing solutions or using techniques like Chain-of-Thought to enhance
problem-solving accuracy. In this paper, we focus on improving the capability
of mathematics teaching via a Socratic teaching-based LLM
(\texttt{SocraticLLM}), which guides learners toward profound thinking with
clarity and self-discovery via conversation. We collect and release a
high-quality mathematical teaching dataset, named \texttt{SocraticMATH}, which
provides Socratic-style conversations of problems with extra knowledge. Also,
we propose a knowledge-enhanced LLM as a strong baseline to generate reliable
responses with review, guidance/heuristic, rectification, and summarization.
Experimental results show the great advantages of \texttt{SocraticLLM} by
comparing it with several strong generative models. The codes and datasets are
available on \url{https://github.com/ECNU-ICALK/SocraticMath}.","[{'name': 'Yuyang Ding'}, {'name': 'Hanglei Hu'}, {'name': 'Jie Zhou'}, {'name': 'Qin Chen'}, {'name': 'Bo Jiang'}, {'name': 'Liang He'}]",2024-07-24T15:18:17Z
http://arxiv.org/abs/2407.17344v1,http://arxiv.org/abs/2407.17344v1,"Label Alignment and Reassignment with Generalist Large Language Model
  for Enhanced Cross-Domain Named Entity Recognition","Named entity recognition on the in-domain supervised and few-shot settings
have been extensively discussed in the NLP community and made significant
progress. However, cross-domain NER, a more common task in practical scenarios,
still poses a challenge for most NER methods. Previous research efforts in that
area primarily focus on knowledge transfer such as correlate label information
from source to target domains but few works pay attention to the problem of
label conflict. In this study, we introduce a label alignment and reassignment
approach, namely LAR, to address this issue for enhanced cross-domain named
entity recognition, which includes two core procedures: label alignment between
source and target domains and label reassignment for type inference. The
process of label reassignment can significantly be enhanced by integrating with
an advanced large-scale language model such as ChatGPT. We conduct an extensive
range of experiments on NER datasets involving both supervised and zero-shot
scenarios. Empirical experimental results demonstrate the validation of our
method with remarkable performance under the supervised and zero-shot
out-of-domain settings compared to SOTA methods.","[{'name': 'Ke Bao'}, {'name': 'Chonghuan Yang'}]",2024-07-24T15:13:12Z
http://arxiv.org/abs/2407.17291v1,http://arxiv.org/abs/2407.17291v1,How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?,"In this study, we address the growing issue of misleading charts, a prevalent
problem that undermines the integrity of information dissemination. Misleading
charts can distort the viewer's perception of data, leading to
misinterpretations and decisions based on false information. The development of
effective automatic detection methods for misleading charts is an urgent field
of research. The recent advancement of multimodal Large Language Models (LLMs)
has introduced a promising direction for addressing this challenge. We explored
the capabilities of these models in analyzing complex charts and assessing the
impact of different prompting strategies on the models' analyses. We utilized a
dataset of misleading charts collected from the internet by prior research and
crafted nine distinct prompts, ranging from simple to complex, to test the
ability of four different multimodal LLMs in detecting over 21 different chart
issues. Through three experiments--from initial exploration to detailed
analysis--we progressively gained insights into how to effectively prompt LLMs
to identify misleading charts and developed strategies to address the
scalability challenges encountered as we expanded our detection range from the
initial five issues to 21 issues in the final experiment. Our findings reveal
that multimodal LLMs possess a strong capability for chart comprehension and
critical thinking in data interpretation. There is significant potential in
employing multimodal LLMs to counter misleading information by supporting
critical thinking and enhancing visualization literacy. This study demonstrates
the applicability of LLMs in addressing the pressing concern of misleading
charts.","[{'name': 'Leo Yu-Ho Lo'}, {'name': 'Huamin Qu'}]",2024-07-24T14:02:20Z
http://arxiv.org/abs/2407.21056v1,http://arxiv.org/abs/2407.21056v1,"What Matters in Explanations: Towards Explainable Fake Review Detection
  Focusing on Transformers","Customers' reviews and feedback play crucial role on electronic
commerce~(E-commerce) platforms like Amazon, Zalando, and eBay in influencing
other customers' purchasing decisions. However, there is a prevailing concern
that sellers often post fake or spam reviews to deceive potential customers and
manipulate their opinions about a product. Over the past decade, there has been
considerable interest in using machine learning (ML) and deep learning (DL)
models to identify such fraudulent reviews. Unfortunately, the decisions made
by complex ML and DL models - which often function as \emph{black-boxes} - can
be surprising and difficult for general users to comprehend. In this paper, we
propose an explainable framework for detecting fake reviews with high precision
in identifying fraudulent content with explanations and investigate what
information matters most for explaining particular decisions by conducting
empirical user evaluation. Initially, we develop fake review detection models
using DL and transformer models including XLNet and DistilBERT. We then
introduce layer-wise relevance propagation (LRP) technique for generating
explanations that can map the contributions of words toward the predicted
class. The experimental results on two benchmark fake review detection datasets
demonstrate that our predictive models achieve state-of-the-art performance and
outperform several existing methods. Furthermore, the empirical user evaluation
of the generated explanations concludes which important information needs to be
considered in generating explanations in the context of fake review
identification.","[{'name': 'Md Shajalal'}, {'name': 'Md Atabuzzaman'}, {'name': 'Alexander Boden'}, {'name': 'Gunnar Stevens'}, {'name': 'Delong Du'}]",2024-07-24T13:26:02Z
http://arxiv.org/abs/2407.17230v1,http://arxiv.org/abs/2407.17230v1,"Improving ICD coding using Chapter based Named Entities and Attentional
  Models","Recent advancements in natural language processing (NLP) have led to
automation in various domains. However, clinical NLP often relies on benchmark
datasets that may not reflect real-world scenarios accurately. Automatic ICD
coding, a vital NLP task, typically uses outdated and imbalanced datasets like
MIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4
and 0.7 due to many false positives. Our research introduces an enhanced
approach to ICD coding that improves F1 scores by using chapter-based named
entities and attentional models. This method categorizes discharge summaries
into ICD-9 Chapters and develops attentional models with chapter-specific data,
eliminating the need to consider external data for code identification. For
categorization, we use Chapter-IV to de-bias and influence key entities and
weights without neural networks, creating accurate thresholds and providing
interpretability for human validation. Post-validation, we develop attentional
models for three frequent and three non-frequent codes from Chapter-IV using
Bidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with
Multi-head Attention architectures. The average Micro-F1 scores of 0.79 and
0.81 from these models demonstrate significant performance improvements in ICD
coding.","[{'name': 'Abhijith R. Beeravolu'}, {'name': 'Mirjam Jonkman'}, {'name': 'Sami Azam'}, {'name': 'Friso De Boer'}]",2024-07-24T12:34:23Z
http://arxiv.org/abs/2407.17227v1,http://arxiv.org/abs/2407.17227v1,"LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN
  prover","Recently, large language models have presented promising results in aiding
formal mathematical reasoning. However, their performance is restricted due to
the scarcity of formal theorem-proving data, which requires additional effort
to be extracted from raw formal language corpora. Meanwhile, a significant
amount of human-written formal language corpora remains underutilized. To
address this issue, we propose LEAN-GitHub, a dataset consisting of large-scale
formal data extracted from almost all Lean 4 repositories on GitHub. After
fine-tuning InternLM-math-plus on this dataset, our model achieved accuracies
of 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F
test, surpassing state-of-the-art method at 52%. And it also achieves
state-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting
different fields/levels of math. These results demonstrate that our proposed
dataset is beneficial for formal reasoning on a wide range of math topics. We
open-source our model at https://GitHub. com/InternLM/InternLM-Math and our
data at https://huggingface.co/ datasets/InternLM/Lean-GitHub","[{'name': 'Zijian Wu'}, {'name': 'Jiayu Wang'}, {'name': 'Dahua Lin'}, {'name': 'Kai Chen'}]",2024-07-24T12:28:03Z
http://arxiv.org/abs/2407.21055v1,http://arxiv.org/abs/2407.21055v1,"Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework
  for Medical Applications","Large Language Models (LLMs) have exhibited remarkable proficiency in natural
language understanding, prompting extensive exploration of their potential
applications across diverse domains. In the medical domain, open-source LLMs
have demonstrated moderate efficacy following domain-specific fine-tuning;
however, they remain substantially inferior to proprietary models such as GPT-4
and GPT-3.5. These open-source models encounter limitations in the
comprehensiveness of domain-specific knowledge and exhibit a propensity for
'hallucinations' during text generation. To mitigate these issues, researchers
have implemented the Retrieval-Augmented Generation (RAG) approach, which
augments LLMs with background information from external knowledge bases while
preserving the model's internal parameters. However, document noise can
adversely affect performance, and the application of RAG in the medical field
remains in its nascent stages. This study presents the Bailicai framework: a
novel integration of retrieval-augmented generation with large language models
optimized for the medical domain. The Bailicai framework augments the
performance of LLMs in medicine through the implementation of four sub-modules.
Experimental results demonstrate that the Bailicai approach surpasses existing
medical domain LLMs across multiple medical benchmarks and exceeds the
performance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates
the prevalent issue of hallucinations in medical applications of LLMs and
ameliorates the noise-related challenges associated with traditional RAG
techniques when processing irrelevant or pseudo-relevant documents.","[{'name': 'Cui Long'}, {'name': 'Yongbin Liu'}, {'name': 'Chunping Ouyang'}, {'name': 'Ying Yu'}]",2024-07-24T12:27:33Z
http://arxiv.org/abs/2407.21054v1,http://arxiv.org/abs/2407.21054v1,Sentiment Reasoning for Healthcare,"Transparency in AI decision-making is crucial in healthcare due to the severe
consequences of errors, and this is important for building trust among AI and
users in sentiment analysis task. Incorporating reasoning capabilities helps
Large Language Models (LLMs) understand human emotions within broader contexts,
handle nuanced and ambiguous language, and infer underlying sentiments that may
not be explicitly stated. In this work, we introduce a new task - Sentiment
Reasoning - for both speech and text modalities, along with our proposed
multimodal multitask framework and dataset. Our study showed that
rationale-augmented training enhances model performance in sentiment
classification across both human transcript and ASR settings. Also, we found
that the generated rationales typically exhibit different vocabularies compared
to human-generated rationales, but maintain similar semantics. All code, data
(English-translated and Vietnamese) and models are published online:
https://github.com/leduckhai/MultiMed","[{'name': 'Khai Le-Duc'}, {'name': 'Khai-Nguyen Nguyen'}, {'name': 'Bach Phan Tat'}, {'name': 'Duy Le'}, {'name': 'Jerry Ngo'}, {'name': 'Long Vo-Dang'}, {'name': 'Anh Totti Nguyen'}, {'name': 'Truong-Son Hy'}]",2024-07-24T12:07:54Z
http://arxiv.org/abs/2407.17174v1,http://arxiv.org/abs/2407.17174v1,"NarrationDep: Narratives on Social Media For Automatic Depression
  Detection","Social media posts provide valuable insight into the narrative of users and
their intentions, including providing an opportunity to automatically model
whether a social media user is depressed or not. The challenge lies in
faithfully modelling user narratives from their online social media posts,
which could potentially be useful in several different applications. We have
developed a novel and effective model called \texttt{NarrationDep}, which
focuses on detecting narratives associated with depression. By analyzing a
user's tweets, \texttt{NarrationDep} accurately identifies crucial narratives.
\texttt{NarrationDep} is a deep learning framework that jointly models
individual user tweet representations and clusters of users' tweets. As a
result, \texttt{NarrationDep} is characterized by a novel two-layer deep
learning model: the first layer models using social media text posts, and the
second layer learns semantic representations of tweets associated with a
cluster. To faithfully model these cluster representations, the second layer
incorporates a novel component that hierarchically learns from users' posts.
The results demonstrate that our framework outperforms other comparative models
including recently developed models on a variety of datasets.","[{'name': 'Hamad Zogan'}, {'name': 'Imran Razzak'}, {'name': 'Shoaib Jameel'}, {'name': 'Guandong Xu'}]",2024-07-24T11:24:25Z
http://arxiv.org/abs/2407.17172v1,http://arxiv.org/abs/2407.17172v1,Speech Editing -- a Summary,"With the rise of video production and social media, speech editing has become
crucial for creators to address issues like mispronunciations, missing words,
or stuttering in audio recordings. This paper explores text-based speech
editing methods that modify audio via text transcripts without manual waveform
editing. These approaches ensure edited audio is indistinguishable from the
original by altering the mel-spectrogram. Recent advancements, such as
context-aware prosody correction and advanced attention mechanisms, have
improved speech editing quality. This paper reviews state-of-the-art methods,
compares key metrics, and examines widely used datasets. The aim is to
highlight ongoing issues and inspire further research and innovation in speech
editing.","[{'name': 'Tobias Kässmann'}, {'name': 'Yining Liu'}, {'name': 'Danni Liu'}]",2024-07-24T11:22:57Z
http://arxiv.org/abs/2407.17167v1,http://arxiv.org/abs/2407.17167v1,"Zero-Shot vs. Few-Shot Multi-Speaker TTS Using Pre-trained Czech
  SpeechT5 Model","In this paper, we experimented with the SpeechT5 model pre-trained on
large-scale datasets. We pre-trained the foundation model from scratch and
fine-tuned it on a large-scale robust multi-speaker text-to-speech (TTS) task.
We tested the model capabilities in a zero- and few-shot scenario. Based on two
listening tests, we evaluated the synthetic audio quality and the similarity of
how synthetic voices resemble real voices. Our results showed that the SpeechT5
model can generate a synthetic voice for any speaker using only one minute of
the target speaker's data. We successfully demonstrated the high quality and
similarity of our synthetic voices on publicly known Czech politicians and
celebrities.","[{'name': 'Jan Lehečka'}, {'name': 'Zdeněk Hanzlíček'}, {'name': 'Jindřich Matoušek'}, {'name': 'Daniel Tihelka'}]",2024-07-24T11:14:06Z
http://arxiv.org/abs/2407.17160v1,http://arxiv.org/abs/2407.17160v1,"A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for
  Automatic Speech Recognition in Multilingual Oral History Archives","In this paper, we are comparing monolingual Wav2Vec 2.0 models with various
multilingual models to see whether we could improve speech recognition
performance on a unique oral history archive containing a lot of mixed-language
sentences. Our main goal is to push forward research on this unique dataset,
which is an extremely valuable part of our cultural heritage. Our results
suggest that monolingual speech recognition models are, in most cases, superior
to multilingual models, even when processing the oral history archive full of
mixed-language sentences from non-native speakers. We also performed the same
experiments on the public CommonVoice dataset to verify our results. We are
contributing to the research community by releasing our pre-trained models to
the public.","[{'name': 'Jan Lehečka'}, {'name': 'Josef V. Psutka'}, {'name': 'Luboš Šmídl'}, {'name': 'Pavel Ircing'}, {'name': 'Josef Psutka'}]",2024-07-24T11:03:47Z
http://arxiv.org/abs/2407.17150v2,http://arxiv.org/abs/2407.17150v2,SimCT: A Simple Consistency Test Protocol in LLMs Development Lifecycle,"In this work, we report our efforts to advance the standard operation
procedure of developing Large Language Models (LLMs) or LLMs-based systems or
services in industry. We introduce the concept of Large Language Model
Development Lifecycle (LDLC) and then highlight the importance of consistency
test in ensuring the delivery quality. The principled solution of consistency
test, however, is usually overlooked by industrial practitioners and not urgent
in academia, and current practical solutions are insufficiently rigours and
labor-intensive. We thus propose a simple yet effective consistency test
protocol, named SimCT. SimCT is mainly to proactively check the consistency
across different development stages of ""bare metal"" LLMs or associated services
without accessing the model artifacts, in an attempt to expedite the delivery
by reducing the back-and-forth alignment communications among multiple teams
involved in different development stages.
  Specifically, SimCT encompasses response-wise and model-wise tests. We
implement the protocol with LightGBM and Student's t-test for two components
respectively, and perform extensive experiments to substantiate the
effectiveness of SimCT and the involved components.","[{'name': 'Fufangchen Zhao'}, {'name': 'Guoqiang Jin'}, {'name': 'Rui Zhao'}, {'name': 'Jiangheng Huang'}, {'name': 'Fei Tan'}]",2024-07-24T10:49:19Z
http://arxiv.org/abs/2407.17126v1,http://arxiv.org/abs/2407.17126v1,"SDoH-GPT: Using Large Language Models to Extract Social Determinants of
  Health (SDoH)","Extracting social determinants of health (SDoH) from unstructured medical
notes depends heavily on labor-intensive annotations, which are typically
task-specific, hampering reusability and limiting sharing. In this study we
introduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)
method leveraging contrastive examples and concise instructions to extract SDoH
without relying on extensive medical annotations or costly human intervention.
It achieved tenfold and twentyfold reductions in time and cost respectively,
and superior consistency with human annotators measured by Cohen's kappa of up
to 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the
strengths of both, ensuring high accuracy and computational efficiency while
consistently maintaining 0.90+ AUROC scores. Testing across three distinct
datasets has confirmed its robustness and accuracy. This study highlights the
potential of leveraging LLMs to revolutionize medical note classification,
demonstrating their capability to achieve highly accurate classifications with
significantly reduced time and cost.","[{'name': 'Bernardo Consoli'}, {'name': 'Xizhi Wu'}, {'name': 'Song Wang'}, {'name': 'Xinyu Zhao'}, {'name': 'Yanshan Wang'}, {'name': 'Justin Rousseau'}, {'name': 'Tom Hartvigsen'}, {'name': 'Li Shen'}, {'name': 'Huanmei Wu'}, {'name': 'Yifan Peng'}, {'name': 'Qi Long'}, {'name': 'Tianlong Chen'}, {'name': 'Ying Ding'}]",2024-07-24T09:57:51Z
http://arxiv.org/abs/2407.17125v2,http://arxiv.org/abs/2407.17125v2,"Behavioral Testing: Can Large Language Models Implicitly Resolve
  Ambiguous Entities?","One of the major aspects contributing to the striking performance of large
language models (LLMs) is the vast amount of factual knowledge accumulated
during pre-training. Yet, many LLMs suffer from self-inconsistency, which
raises doubts about their trustworthiness and reliability. In this paper, we
focus on entity type ambiguity and analyze current state-of-the-art LLMs for
their proficiency and consistency in applying their factual knowledge when
prompted for entities under ambiguity. To do so, we propose an evaluation
protocol that disentangles knowing from applying knowledge, and test
state-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs perform
poorly with ambiguous prompts, achieving only 80% accuracy. Our results further
demonstrate systematic discrepancies in LLM behavior and their failure to
consistently apply information, indicating that the models can exhibit
knowledge without being able to utilize it, significant biases for preferred
readings, as well as self inconsistencies. Our study highlights the importance
of handling entity ambiguity in future for more trustworthy LLMs","[{'name': 'Anastasiia Sedova'}, {'name': 'Robert Litschko'}, {'name': 'Diego Frassinelli'}, {'name': 'Benjamin Roth'}, {'name': 'Barbara Plank'}]",2024-07-24T09:48:48Z
http://arxiv.org/abs/2407.17081v1,http://arxiv.org/abs/2407.17081v1,"A Survey Forest Diagram : Gain a Divergent Insight View on a Specific
  Research Topic","With the exponential growth in the number of papers and the trend of AI
research, the use of Generative AI for information retrieval and
question-answering has become popular for conducting research surveys. However,
novice researchers unfamiliar with a particular field may not significantly
improve their efficiency in interacting with Generative AI because they have
not developed divergent thinking in that field. This study aims to develop an
in-depth Survey Forest Diagram that guides novice researchers in divergent
thinking about the research topic by indicating the citation clues among
multiple papers, to help expand the survey perspective for novice researchers.","[{'name': 'Jinghong Li'}, {'name': 'Wen Gu'}, {'name': 'Koichi Ota'}, {'name': 'Shinobu Hasegawa'}]",2024-07-24T08:17:37Z
http://arxiv.org/abs/2407.17075v3,http://arxiv.org/abs/2407.17075v3,SAFETY-J: Evaluating Safety with Critique,"The deployment of Large Language Models (LLMs) in content generation raises
significant safety concerns, particularly regarding the transparency and
interpretability of content evaluations. Current methods, primarily focused on
binary safety classifications, lack mechanisms for detailed critique, limiting
their utility for model improvement and user trust. To address these
limitations, we introduce SAFETY-J, a bilingual generative safety evaluator for
English and Chinese with critique-based judgment. SAFETY-J utilizes a robust
training dataset that includes diverse dialogues and augmented query-response
pairs to assess safety across various scenarios comprehensively. We establish
an automated meta-evaluation benchmark that objectively assesses the quality of
critiques with minimal human intervention, facilitating scalable and continuous
improvement. Additionally, SAFETY-J employs an iterative preference learning
technique to dynamically refine safety assessments based on meta-evaluations
and critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced
and accurate safety evaluations, thereby enhancing both critique quality and
predictive reliability in complex content scenarios. To facilitate further
research and application, we open-source SAFETY-J's training protocols,
datasets, and code at https://github.com/GAIR-NLP/Safety-J.","[{'name': 'Yixiu Liu'}, {'name': 'Yuxiang Zheng'}, {'name': 'Shijie Xia'}, {'name': 'Jiajun Li'}, {'name': 'Yi Tu'}, {'name': 'Chaoling Song'}, {'name': 'Pengfei Liu'}]",2024-07-24T08:04:00Z
http://arxiv.org/abs/2407.17060v1,http://arxiv.org/abs/2407.17060v1,High Efficiency Image Compression for Large Visual-Language Models,"In recent years, large visual language models (LVLMs) have shown impressive
performance and promising generalization capability in multi-modal tasks, thus
replacing humans as receivers of visual information in various application
scenarios. In this paper, we pioneer to propose a variable bitrate image
compression framework consisting of a pre-editing module and an end-to-end
codec to achieve promising rate-accuracy performance for different LVLMs. In
particular, instead of optimizing an adaptive pre-editing network towards a
particular task or several representative tasks, we propose a new optimization
strategy tailored for LVLMs, which is designed based on the representation and
discrimination capability with token-level distortion and rank. The pre-editing
module and the variable bitrate end-to-end image codec are jointly trained by
the losses based on semantic tokens of the large model, which introduce
enhanced generalization capability for various data and tasks. {Experimental
results demonstrate that the proposed framework could efficiently achieve much
better rate-accuracy performance compared to the state-of-the-art coding
standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal
tasks have revealed the robustness and generalization capability of the
proposed framework.","[{'name': 'Binzhe Li'}, {'name': 'Shurun Wang'}, {'name': 'Shiqi Wang'}, {'name': 'Yan Ye'}]",2024-07-24T07:37:12Z
http://arxiv.org/abs/2407.17023v1,http://arxiv.org/abs/2407.17023v1,From Internal Conflict to Contextual Adaptation of Language Models,"Knowledge-intensive language understanding tasks require Language Models
(LMs) to integrate relevant context, mitigating their inherent weaknesses, such
as incomplete or outdated knowledge. Nevertheless, studies indicate that LMs
often ignore the provided context as it can conflict with the pre-existing LM's
memory learned during pre-training. Moreover, conflicting knowledge can already
be present in the LM's parameters, termed intra-memory conflict. Existing works
have studied the two types of knowledge conflicts only in isolation. We
conjecture that the (degree of) intra-memory conflicts can in turn affect LM's
handling of context-memory conflicts. To study this, we introduce the DYNAMICQA
dataset, which includes facts with a temporal dynamic nature where a fact can
change with a varying time frequency and disputable dynamic facts, which can
change depending on the viewpoint. DYNAMICQA is the first to include real-world
knowledge conflicts and provide context to study the link between the different
types of knowledge conflicts. With the proposed dataset, we assess the use of
uncertainty for measuring the intra-memory conflict and introduce a novel
Coherent Persuasion (CP) score to evaluate the context's ability to sway LM's
semantic output. Our extensive experiments reveal that static facts, which are
unlikely to change, are more easily updated with additional context, relative
to temporal and disputable facts.","[{'name': 'Sara Vera Marjanović'}, {'name': 'Haeun Yu'}, {'name': 'Pepa Atanasova'}, {'name': 'Maria Maistro'}, {'name': 'Christina Lioma'}, {'name': 'Isabelle Augenstein'}]",2024-07-24T06:06:07Z
http://arxiv.org/abs/2407.17022v1,http://arxiv.org/abs/2407.17022v1,"Can Language Models Evaluate Human Written Text? Case Study on Korean
  Student Writing for Education","Large language model (LLM)-based evaluation pipelines have demonstrated their
capability to robustly evaluate machine-generated text. Extending this
methodology to assess human-written text could significantly benefit
educational settings by providing direct feedback to enhance writing skills,
although this application is not straightforward. In this paper, we investigate
whether LLMs can effectively assess human-written text for educational
purposes. We collected 100 texts from 32 Korean students across 15 types of
writing and employed GPT-4-Turbo to evaluate them using grammaticality,
fluency, coherence, consistency, and relevance as criteria. Our analyses
indicate that LLM evaluators can reliably assess grammaticality and fluency, as
well as more objective types of writing, though they struggle with other
criteria and types of writing. We publicly release our dataset and feedback.","[{'name': 'Seungyoon Kim'}, {'name': 'Seungone Kim'}]",2024-07-24T06:02:57Z
http://arxiv.org/abs/2407.17011v1,http://arxiv.org/abs/2407.17011v1,"Unveiling In-Context Learning: A Coordinate System to Understand Its
  Working Mechanism","Large language models (LLMs) exhibit remarkable in-context learning (ICL)
capabilities. However, the underlying working mechanism of ICL remains poorly
understood. Recent research presents two conflicting views on ICL: One
attributes it to LLMs' inherent ability of task recognition, deeming label
correctness and shot numbers of demonstrations as not crucial; the other
emphasizes the impact of similar examples in the demonstrations, stressing the
need for label correctness and more shots. In this work, we provide a
Two-Dimensional Coordinate System that unifies both views into a systematic
framework. The framework explains the behavior of ICL through two orthogonal
variables: whether LLMs can recognize the task and whether similar examples are
presented in the demonstrations. We propose the peak inverse rank metric to
detect the task recognition ability of LLMs and study LLMs' reactions to
different definitions of similarity. Based on these, we conduct extensive
experiments to elucidate how ICL functions across each quadrant on multiple
representative classification tasks. Finally, we extend our analyses to
generation tasks, showing that our coordinate system can also be used to
interpret ICL for generation tasks effectively.","[{'name': 'Anhao Zhao'}, {'name': 'Fanghua Ye'}, {'name': 'Jinlan Fu'}, {'name': 'Xiaoyu Shen'}]",2024-07-24T05:26:52Z
http://arxiv.org/abs/2407.16997v1,http://arxiv.org/abs/2407.16997v1,"Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal
  Intervention Perspective","This paper investigates Who's Harry Potter (WHP), a pioneering yet
insufficiently understood method for LLM unlearning. We explore it in two
steps. First, we introduce a new task of LLM targeted unlearning, where given
an unlearning target (e.g., a person) and some unlearning documents, we aim to
unlearn only the information about the target, rather than everything in the
unlearning documents. We further argue that a successful unlearning should
satisfy criteria such as not outputting gibberish, not fabricating facts about
the unlearning target, and not releasing factual information under jailbreak
attacks. Second, we construct a causal intervention framework for targeted
unlearning, where the knowledge of the unlearning target is modeled as a
confounder between LLM input and output, and the unlearning process as a
deconfounding process. This framework justifies and extends WHP, deriving a
simple unlearning algorithm that includes WHP as a special case. Experiments on
existing and new datasets show that our approach, without explicitly optimizing
for the aforementioned criteria, achieves competitive performance in all of
them. Our code is available at
https://github.com/UCSB-NLP-Chang/causal_unlearn.git.","[{'name': 'Yujian Liu'}, {'name': 'Yang Zhang'}, {'name': 'Tommi Jaakkola'}, {'name': 'Shiyu Chang'}]",2024-07-24T04:39:24Z
http://arxiv.org/abs/2407.16994v1,http://arxiv.org/abs/2407.16994v1,"A Voter-Based Stochastic Rejection-Method Framework for Asymptotically
  Safe Language Model Outputs","This paper proposes a new method for preventing unsafe or otherwise low
quality large language model (LLM) outputs, by leveraging the stochasticity of
LLMs. We propose a system whereby LLM checkers vote on the acceptability of a
generated output, regenerating it if a threshold of disapproval is reached,
until sufficient checkers approve. We further propose estimators for cost and
failure rate, and based on those estimators and experimental data tailored to
the application, we propose an algorithm that achieves a desired failure rate
at the least possible cost. We demonstrate that, under these models, failure
rate decreases exponentially as a function of cost when voter count and
threshold are chosen according to the algorithm, and that the models reasonably
estimate the actual performance of such a system in action, even with limited
data.","[{'name': 'Jake R. Watts'}, {'name': 'Joel Sokol'}]",2024-07-24T04:27:55Z
http://arxiv.org/abs/2407.17532v1,http://arxiv.org/abs/2407.17532v1,"Generative artificial intelligence in dentistry: Current approaches and
  future challenges","Artificial intelligence (AI) has become a commodity for people because of the
advent of generative AI (GenAI) models that bridge the usability gap of AI by
providing a natural language interface to interact with complex models. These
GenAI models range from text generation - such as two-way chat systems - to the
generation of image or video from textual descriptions input by a user. These
advancements in AI have impacted Dentistry in multiple aspects. In dental
education, the student now has the opportunity to solve a plethora of questions
by only prompting a GenAI model and have the answer in a matter of seconds.
GenAI models can help us deliver better patient healthcare by helping
practitioners gather knowledge quickly and efficiently. Finally, GenAI can also
be used in dental research, where the applications range from new drug
discovery to assistance in academic writing. In this review, we first define
GenAI models and describe their multiple generation modalities; then, we
explain and discuss their current and potential applications in Dentistry; and
finally, we describe the challenges these new technologies impose in our area.","[{'name': 'Fabián Villena'}, {'name': 'Claudia Véliz'}, {'name': 'Rosario García-Huidobro'}, {'name': 'Sebastián Aguayo'}]",2024-07-24T03:33:47Z
http://arxiv.org/abs/2407.16970v1,http://arxiv.org/abs/2407.16970v1,Towards Aligning Language Models with Textual Feedback,"We present ALT (ALignment with Textual feedback), an approach that aligns
language models with user preferences expressed in text. We argue that text
offers greater expressiveness, enabling users to provide richer feedback than
simple comparative preferences and this richer feedback can lead to more
efficient and effective alignment. ALT aligns the model by conditioning its
generation on the textual feedback. Our method relies solely on language
modeling techniques and requires minimal hyper-parameter tuning, though it
still presents the main benefits of RL-based alignment algorithms and can
effectively learn from textual feedback. We explore the efficacy and efficiency
of textual feedback across different tasks such as toxicity reduction,
summarization, and dialog response generation. We find that ALT outperforms PPO
for the task of toxicity reduction while being able to match its performance on
summarization with only 20% of the samples. We also explore how ALT can be used
with feedback provided by an existing LLM where we explore an LLM providing
constrained and unconstrained textual feedback. We also outline future
directions to align models with natural language feedback.","[{'name': 'Saüc Abadal Lloret'}, {'name': 'Shehzaad Dhuliawala'}, {'name': 'Keerthiram Murugesan'}, {'name': 'Mrinmaya Sachan'}]",2024-07-24T03:32:05Z
http://arxiv.org/abs/2407.16951v1,http://arxiv.org/abs/2407.16951v1,"Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias
  Mitigation","Large language models (LLMs) often inherit biases from vast amounts of
training corpora. Traditional debiasing methods, while effective to some
extent, do not completely eliminate memorized biases and toxicity in LLMs. In
this paper, we study an unlearning-based approach to debiasing in LLMs by
performing gradient ascent on hate speech against minority groups, i.e.,
minimizing the likelihood of biased or toxic content. Specifically, we propose
a mask language modeling unlearning technique, which unlearns the harmful part
of the text. This method enables LLMs to selectively forget and disassociate
from biased and harmful content. Experimental results demonstrate the
effectiveness of our approach in diminishing bias while maintaining the
language modeling abilities. Surprisingly, the results also unveil an
unexpected potential for cross-domain transfer unlearning: debiasing in one
bias form (e.g. gender) may contribute to mitigating others (e.g. race and
religion).","[{'name': 'Huimin Lu'}, {'name': 'Masaru Isonuma'}, {'name': 'Junichiro Mori'}, {'name': 'Ichiro Sakata'}]",2024-07-24T02:37:42Z
http://arxiv.org/abs/2407.16939v1,http://arxiv.org/abs/2407.16939v1,"Early screening of potential breakthrough technologies with enhanced
  interpretability: A patent-specific hierarchical attention network model","Despite the usefulness of machine learning approaches for the early screening
of potential breakthrough technologies, their practicality is often hindered by
opaque models. To address this, we propose an interpretable machine learning
approach to predicting future citation counts from patent texts using a
patent-specific hierarchical attention network (PatentHAN) model. Central to
this approach are (1) a patent-specific pre-trained language model, capturing
the meanings of technical words in patent claims, (2) a hierarchical network
structure, enabling detailed analysis at the claim level, and (3) a claim-wise
self-attention mechanism, revealing pivotal claims during the screening
process. A case study of 35,376 pharmaceutical patents demonstrates the
effectiveness of our approach in early screening of potential breakthrough
technologies while ensuring interpretability. Furthermore, we conduct
additional analyses using different language models and claim types to examine
the robustness of the approach. It is expected that the proposed approach will
enhance expert-machine collaboration in identifying breakthrough technologies,
providing new insight derived from text mining into technological value.","[{'name': 'Jaewoong Choi'}, {'name': 'Janghyeok Yoon'}, {'name': 'Changyong Lee'}]",2024-07-24T02:17:10Z
http://arxiv.org/abs/2407.16931v1,http://arxiv.org/abs/2407.16931v1,"ScholarChemQA: Unveiling the Power of Language Models in Chemical
  Research Question Answering","Question Answering (QA) effectively evaluates language models' reasoning and
knowledge depth. While QA datasets are plentiful in areas like general domain
and biomedicine, academic chemistry is less explored. Chemical QA plays a
crucial role in both education and research by effectively translating complex
chemical information into readily understandable format. Addressing this gap,
we introduce ScholarChemQA, a large-scale QA dataset constructed from chemical
papers. This dataset reflects typical real-world challenges, including an
imbalanced data distribution and a substantial amount of unlabeled data that
can be potentially useful. Correspondingly, we introduce a QAMatch model,
specifically designed to effectively answer chemical questions by fully
leveraging our collected data. We first address the issue of imbalanced label
distribution by re-weighting the instance-wise loss based on the inverse
frequency of each class, ensuring minority classes are not dominated by
majority ones during optimization. Next, we utilize the unlabeled data to
enrich the learning process, generating a variety of augmentations based on a
SoftMix operation and ensuring their predictions align with the same target,
i.e., pseudo-labels. To ensure the quality of the pseudo-labels, we propose a
calibration procedure aimed at closely aligning the pseudo-label estimates of
individual samples with a desired ground truth distribution. Experiments show
that our QAMatch significantly outperforms the recent similar-scale baselines
and Large Language Models (LLMs) not only on our ScholarChemQA dataset but also
on four benchmark datasets. We hope our benchmark and model can facilitate and
promote more research on chemical QA.","[{'name': 'Xiuying Chen'}, {'name': 'Tairan Wang'}, {'name': 'Taicheng Guo'}, {'name': 'Kehan Guo'}, {'name': 'Juexiao Zhou'}, {'name': 'Haoyang Li'}, {'name': 'Mingchen Zhuge'}, {'name': 'Jürgen Schmidhuber'}, {'name': 'Xin Gao'}, {'name': 'Xiangliang Zhang'}]",2024-07-24T01:46:55Z
http://arxiv.org/abs/2407.16920v1,http://arxiv.org/abs/2407.16920v1,"Train-Attention: Meta-Learning Where to Focus in Continual Knowledge
  Learning","Previous studies on continual knowledge learning (CKL) in large language
models (LLMs) have predominantly focused on approaches such as regularization,
architectural modifications, and rehearsal techniques to mitigate catastrophic
forgetting. However, these methods naively inherit the inefficiencies of
standard training procedures, indiscriminately applying uniform weight across
all tokens, which can lead to unnecessary parameter updates and increased
forgetting. To address these shortcomings, we propose a novel CKL approach
termed Train-Attention-Augmented Language Model (TAALM), which enhances
learning efficiency by dynamically predicting and applying weights to tokens
based on their usefulness. This method employs a meta-learning framework that
optimizes token importance predictions, facilitating targeted knowledge updates
and minimizing forgetting. Also, we observe that existing benchmarks do not
clearly exhibit the trade-off between learning and retaining, therefore we
propose a new benchmark, \textsc{LAMA-ckl}, to address this issue. Through
experiments conducted on both newly introduced and established CKL benchmarks,
TAALM proves the state-of-the-art performance upon the baselines, and also
shows synergistic compatibility when integrated with previous CKL approaches.","[{'name': 'Yeongbin Seo'}, {'name': 'Dongha Lee'}, {'name': 'Jinyoung Yeo'}]",2024-07-24T01:04:34Z
http://arxiv.org/abs/2407.16908v1,http://arxiv.org/abs/2407.16908v1,Generation Constraint Scaling Can Mitigate Hallucination,"Addressing the issue of hallucinations in large language models (LLMs) is a
critical challenge. As the cognitive mechanisms of hallucination have been
related to memory, here we explore hallucination for LLM that is enabled with
explicit memory mechanisms. We empirically demonstrate that by simply scaling
the readout vector that constrains generation in a memory-augmented LLM
decoder, hallucination mitigation can be achieved in a training-free manner.
Our method is geometry-inspired and outperforms a state-of-the-art LLM editing
method on the task of generation of Wikipedia-like biography entries both in
terms of generation quality and runtime complexity.","[{'name': 'Georgios Kollias'}, {'name': 'Payel Das'}, {'name': 'Subhajit Chaudhury'}]",2024-07-23T23:58:19Z
http://arxiv.org/abs/2407.16860v1,http://arxiv.org/abs/2407.16860v1,"$\textit{BenchIE}^{FL}$ : A Manually Re-Annotated Fact-Based Open
  Information Extraction Benchmark","Open Information Extraction (OIE) is a field of natural language processing
that aims to present textual information in a format that allows it to be
organized, analyzed and reflected upon. Numerous OIE systems are developed,
claiming ever-increasing performance, marking the need for objective
benchmarks. BenchIE is the latest reference we know of. Despite being very well
thought out, we noticed a number of issues we believe are limiting. Therefore,
we propose $\textit{BenchIE}^{FL}$, a new OIE benchmark which fully enforces
the principles of BenchIE while containing fewer errors, omissions and
shortcomings when candidate facts are matched towards reference ones.
$\textit{BenchIE}^{FL}$ allows insightful conclusions to be drawn on the actual
performance of OIE extractors.","[{'name': 'Fabrice Lamarche'}, {'name': 'Philippe Langlais'}]",2024-07-23T22:04:04Z
http://arxiv.org/abs/2407.16837v1,http://arxiv.org/abs/2407.16837v1,CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs,"The ability to compare objects, scenes, or situations is crucial for
effective decision-making and problem-solving in everyday life. For instance,
comparing the freshness of apples enables better choices during grocery
shopping, while comparing sofa designs helps optimize the aesthetics of our
living space. Despite its significance, the comparative capability is largely
unexplored in artificial general intelligence (AGI). In this paper, we
introduce CompBench, a benchmark designed to evaluate the comparative reasoning
capability of multimodal large language models (MLLMs). CompBench mines and
pairs images through visually oriented questions covering eight dimensions of
relative comparison: visual attribute, existence, state, emotion, temporality,
spatiality, quantity, and quality. We curate a collection of around 40K image
pairs using metadata from diverse vision datasets and CLIP similarity scores.
These image pairs span a broad array of visual domains, including animals,
fashion, sports, and both outdoor and indoor scenes. The questions are
carefully crafted to discern relative characteristics between two images and
are labeled by human annotators for accuracy and relevance. We use CompBench to
evaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our
results reveal notable shortcomings in their comparative abilities. We believe
CompBench not only sheds light on these limitations but also establishes a
solid foundation for future enhancements in the comparative capability of
MLLMs.","[{'name': 'Jihyung Kil'}, {'name': 'Zheda Mai'}, {'name': 'Justin Lee'}, {'name': 'Zihe Wang'}, {'name': 'Kerrie Cheng'}, {'name': 'Lemeng Wang'}, {'name': 'Ye Liu'}, {'name': 'Arpita Chowdhury'}, {'name': 'Wei-Lun Chao'}]",2024-07-23T21:02:38Z
http://arxiv.org/abs/2407.16833v1,http://arxiv.org/abs/2407.16833v1,"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
  Study and Hybrid Approach","Retrieval Augmented Generation (RAG) has been a powerful tool for Large
Language Models (LLMs) to efficiently process overly lengthy contexts. However,
recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to
understand long contexts directly. We conduct a comprehensive comparison
between RAG and long-context (LC) LLMs, aiming to leverage the strengths of
both. We benchmark RAG and LC across various public datasets using three latest
LLMs. Results reveal that when resourced sufficiently, LC consistently
outperforms RAG in terms of average performance. However, RAG's significantly
lower cost remains a distinct advantage. Based on this observation, we propose
Self-Route, a simple yet effective method that routes queries to RAG or LC
based on model self-reflection. Self-Route significantly reduces the
computation cost while maintaining a comparable performance to LC. Our findings
provide a guideline for long-context applications of LLMs using RAG and LC.","[{'name': 'Zhuowan Li'}, {'name': 'Cheng Li'}, {'name': 'Mingyang Zhang'}, {'name': 'Qiaozhu Mei'}, {'name': 'Michael Bendersky'}]",2024-07-23T20:51:52Z
http://arxiv.org/abs/2407.16772v1,http://arxiv.org/abs/2407.16772v1,VisMin: Visual Minimal-Change Understanding,"Fine-grained understanding of objects, attributes, and relationships between
objects is crucial for visual-language models (VLMs). Existing benchmarks
primarily focus on evaluating VLMs' capability to distinguish between two very
similar \textit{captions} given an image. In this paper, we introduce a new,
challenging benchmark termed \textbf{Vis}ual \textbf{Min}imal-Change
Understanding (VisMin), which requires models to predict the correct
image-caption match given two images and two captions. The image pair and
caption pair contain minimal changes, i.e., only one aspect changes at a time
from among the following: \textit{object}, \textit{attribute}, \textit{count},
and \textit{spatial relation}. These changes test the models' understanding of
objects, attributes (such as color, material, shape), counts, and spatial
relationships between objects. We built an automatic framework using large
language models and diffusion models, followed by a rigorous 4-step
verification process by human annotators. Empirical experiments reveal that
current VLMs exhibit notable deficiencies in understanding spatial
relationships and counting abilities. We also generate a large-scale training
dataset to finetune CLIP and Idefics2, showing significant improvements in
fine-grained understanding across benchmarks and in CLIP's general image-text
alignment. We release all resources, including the benchmark, training data,
and finetuned model checkpoints, at \url{https://vismin.net/}.","[{'name': 'Rabiul Awal'}, {'name': 'Saba Ahmadi'}, {'name': 'Le Zhang'}, {'name': 'Aishwarya Agrawal'}]",2024-07-23T18:10:43Z
http://arxiv.org/abs/2407.16695v1,http://arxiv.org/abs/2407.16695v1,"Stress-Testing Long-Context Language Models with Lifelong ICL and Task
  Haystack","We introduce Lifelong ICL, a problem setting that challenges long-context
language models (LMs) to learn from a sequence of language tasks through
in-context learning (ICL). We further introduce Task Haystack, an evaluation
suite dedicated to assessing and diagnosing how long-context LMs utilizes
contexts in Lifelong ICL. When given a task instruction and test inputs,
long-context LMs are expected to leverage the relevant demonstrations in the
Lifelong ICL prompt, avoid distraction and interference from other tasks, and
achieve test accuracies that are not significantly worse than the Single-task
ICL baseline.
  Task Haystack draws inspiration from the widely-adopted
""needle-in-a-haystack"" (NIAH) evaluation, but presents new and unique
challenges. It demands that models (1) utilize the contexts with deeper
understanding, rather than resorting to simple copying and pasting; (2)
navigate through long streams of evolving topics and tasks, which closely
approximates the complexities of real-world usage of long-context LMs.
Additionally, Task Haystack inherits the controllability aspect of NIAH,
providing model developers with tools and visualizations to identify model
vulnerabilities effectively.
  We benchmark 12 long-context LMs using Task Haystack. We find that
state-of-the-art closed models such as GPT-4o still struggle in this setting,
failing 15% of the cases on average, while all open-weight models we evaluate
further lack behind by a large margin, failing up to 61% of the cases. In our
controlled analysis, we identify factors such as distraction and recency bias
as contributors to these failure cases. Further, we observe declines in
performance when task instructions are paraphrased at test time or when ICL
demonstrations are repeated excessively, raising concerns about the robustness,
instruction understanding, and true context utilization of current long-context
LMs.","[{'name': 'Xiaoyue Xu'}, {'name': 'Qinyuan Ye'}, {'name': 'Xiang Ren'}]",2024-07-23T17:57:41Z
http://arxiv.org/abs/2407.16693v1,http://arxiv.org/abs/2407.16693v1,Explanation Regularisation through the Lens of Attributions,"Explanation regularisation (ER) has been introduced as a way to guide models
to make their predictions in a manner more akin to humans, i.e., making their
attributions ""plausible"". This is achieved by introducing an auxiliary
explanation loss, that measures how well the output of an input attribution
technique for the model agrees with relevant human-annotated rationales. One
positive outcome of using ER appears to be improved performance in
out-of-domain (OOD) settings, presumably due to an increased reliance on
""plausible"" tokens. However, previous work has under-explored the impact of the
ER objective on model attributions, in particular when obtained with techniques
other than the one used to train ER. In this work, we contribute a study of
ER's effectiveness at informing classification decisions on plausible tokens,
and the relationship between increased plausibility and robustness to OOD
conditions. Through a series of analyses, we find that the connection between
ER and the ability of a classifier to rely on plausible features has been
overstated and that a stronger reliance on plausible tokens does not seem to be
the cause for any perceived OOD improvements.","[{'name': 'Pedro Ferreira'}, {'name': 'Wilker Aziz'}, {'name': 'Ivan Titov'}]",2024-07-23T17:56:32Z
http://arxiv.org/abs/2407.16686v1,http://arxiv.org/abs/2407.16686v1,Can Large Language Models Automatically Jailbreak GPT-4V?,"GPT-4V has attracted considerable attention due to its extraordinary capacity
for integrating and processing multimodal information. At the same time, its
ability of face recognition raises new safety concerns of privacy leakage.
Despite researchers' efforts in safety alignment through RLHF or preprocessing
filters, vulnerabilities might still be exploited. In our study, we introduce
AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt
optimization. We leverage Large Language Models (LLMs) for red-teaming to
refine the jailbreak prompt and employ weak-to-strong in-context learning
prompts to boost efficiency. Furthermore, we present an effective search method
that incorporates early stopping to minimize optimization time and token
expenditure. Our experiments demonstrate that AutoJailbreak significantly
surpasses conventional methods, achieving an Attack Success Rate (ASR)
exceeding 95.3\%. This research sheds light on strengthening GPT-4V security,
underscoring the potential for LLMs to be exploited in compromising GPT-4V
integrity.","[{'name': 'Yuanwei Wu'}, {'name': 'Yue Huang'}, {'name': 'Yixin Liu'}, {'name': 'Xiang Li'}, {'name': 'Pan Zhou'}, {'name': 'Lichao Sun'}]",2024-07-23T17:50:45Z
http://arxiv.org/abs/2407.16741v1,http://arxiv.org/abs/2407.16741v1,"OpenDevin: An Open Platform for AI Software Developers as Generalist
  Agents","Software is one of the most powerful tools that we humans have at our
disposal; it allows a skilled programmer to interact with the world in complex
and profound ways. At the same time, thanks to improvements in large language
models (LLMs), there has also been a rapid development in AI agents that
interact with and affect change in their surrounding environments. In this
paper, we introduce OpenDevin, a platform for the development of powerful and
flexible AI agents that interact with the world in similar ways to those of a
human developer: by writing code, interacting with a command line, and browsing
the web. We describe how the platform allows for the implementation of new
agents, safe interaction with sandboxed environments for code execution,
coordination between multiple agents, and incorporation of evaluation
benchmarks. Based on our currently incorporated benchmarks, we perform an
evaluation of agents over 15 challenging tasks, including software engineering
(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released
under the permissive MIT license, OpenDevin is a community project spanning
academia and industry with more than 1.3K contributions from over 160
contributors and will improve going forward.","[{'name': 'Xingyao Wang'}, {'name': 'Boxuan Li'}, {'name': 'Yufan Song'}, {'name': 'Frank F. Xu'}, {'name': 'Xiangru Tang'}, {'name': 'Mingchen Zhuge'}, {'name': 'Jiayi Pan'}, {'name': 'Yueqi Song'}, {'name': 'Bowen Li'}, {'name': 'Jaskirat Singh'}, {'name': 'Hoang H. Tran'}, {'name': 'Fuqiang Li'}, {'name': 'Ren Ma'}, {'name': 'Mingzhang Zheng'}, {'name': 'Bill Qian'}, {'name': 'Yanjun Shao'}, {'name': 'Niklas Muennighoff'}, {'name': 'Yizhe Zhang'}, {'name': 'Binyuan Hui'}, {'name': 'Junyang Lin'}, {'name': 'Robert Brennan'}, {'name': 'Hao Peng'}, {'name': 'Heng Ji'}, {'name': 'Graham Neubig'}]",2024-07-23T17:50:43Z
http://arxiv.org/abs/2407.16667v1,http://arxiv.org/abs/2407.16667v1,"RedAgent: Red Teaming Large Language Models with Context-aware
  Autonomous Language Agent","Recently, advanced Large Language Models (LLMs) such as GPT-4 have been
integrated into many real-world applications like Code Copilot. These
applications have significantly expanded the attack surface of LLMs, exposing
them to a variety of threats. Among them, jailbreak attacks that induce toxic
responses through jailbreak prompts have raised critical safety concerns. To
identify these threats, a growing number of red teaming approaches simulate
potential adversarial scenarios by crafting jailbreak prompts to test the
target LLM. However, existing red teaming methods do not consider the unique
vulnerabilities of LLM in different scenarios, making it difficult to adjust
the jailbreak prompts to find context-specific vulnerabilities. Meanwhile,
these methods are limited to refining jailbreak templates using a few mutation
operations, lacking the automation and scalability to adapt to different
scenarios. To enable context-aware and efficient red teaming, we abstract and
model existing attacks into a coherent concept called ""jailbreak strategy"" and
propose a multi-agent LLM system named RedAgent that leverages these strategies
to generate context-aware jailbreak prompts. By self-reflecting on contextual
feedback in an additional memory buffer, RedAgent continuously learns how to
leverage these strategies to achieve effective jailbreaks in specific contexts.
Extensive experiments demonstrate that our system can jailbreak most black-box
LLMs in just five queries, improving the efficiency of existing red teaming
methods by two times. Additionally, RedAgent can jailbreak customized LLM
applications more efficiently. By generating context-aware jailbreak prompts
towards applications on GPTs, we discover 60 severe vulnerabilities of these
real-world applications with only two queries per vulnerability. We have
reported all found issues and communicated with OpenAI and Meta for bug fixes.","[{'name': 'Huiyu Xu'}, {'name': 'Wenhui Zhang'}, {'name': 'Zhibo Wang'}, {'name': 'Feng Xiao'}, {'name': 'Rui Zheng'}, {'name': 'Yunhe Feng'}, {'name': 'Zhongjie Ba'}, {'name': 'Kui Ren'}]",2024-07-23T17:34:36Z
http://arxiv.org/abs/2407.16664v1,http://arxiv.org/abs/2407.16664v1,Towards scalable efficient on-device ASR with transfer learning,"Multilingual pretraining for transfer learning significantly boosts the
robustness of low-resource monolingual ASR models. This study systematically
investigates three main aspects: (a) the impact of transfer learning on model
performance during initial training or fine-tuning, (b) the influence of
transfer learning across dataset domains and languages, and (c) the effect on
rare-word recognition compared to non-rare words. Our finding suggests that
RNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word
Error Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across
languages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8%
compared to monolingual baselines for MLS and in-house datasets. Out-of-domain
pretraining leads to 28% higher WERR than in-domain pretraining. Both rare and
non-rare words benefit, with rare words showing greater improvements with
out-of-domain pretraining, and non-rare words with in-domain pretraining.","[{'name': 'Laxmi Pandey'}, {'name': 'Ke Li'}, {'name': 'Jinxi Guo'}, {'name': 'Debjyoti Paul'}, {'name': 'Arthur Guo'}, {'name': 'Jay Mahadeokar'}, {'name': 'Xuedong Zhang'}]",2024-07-23T17:29:02Z
http://arxiv.org/abs/2407.16737v1,http://arxiv.org/abs/2407.16737v1,A Survey of Text Style Transfer: Applications and Ethical Implications,"Text style transfer (TST) is an important task in controllable text
generation, which aims to control selected attributes of language use, such as
politeness, formality, or sentiment, without altering the style-independent
content of the text. The field has received considerable research attention in
recent years and has already been covered in several reviews, but the focus has
mostly been on the development of new algorithms and learning from different
types of data (supervised, unsupervised, out-of-domain, etc.) and not so much
on the application side. However, TST-related technologies are gradually
reaching a production- and deployment-ready level, and therefore, the inclusion
of the application perspective in TST research becomes crucial. Similarly, the
often overlooked ethical considerations of TST technology have become a
pressing issue. This paper presents a comprehensive review of TST applications
that have been researched over the years, using both traditional linguistic
approaches and more recent deep learning methods. We discuss current
challenges, future research directions, and ethical implications of TST
applications in text generation. By providing a holistic overview of the
landscape of TST applications, we hope to stimulate further research and
contribute to a better understanding of the potential as well as ethical
considerations associated with TST.","[{'name': 'Sourabrata Mukherjee'}, {'name': 'Mateusz Lango'}, {'name': 'Zdenek Kasner'}, {'name': 'Ondrej Dušek'}]",2024-07-23T17:15:23Z
http://arxiv.org/abs/2407.16637v1,http://arxiv.org/abs/2407.16637v1,Course-Correction: Safety Alignment Using Synthetic Preferences,"The risk of harmful content generated by large language models (LLMs) becomes
a critical concern. This paper presents a systematic study on assessing and
improving LLMs' capability to perform the task of \textbf{course-correction},
\ie, the model can steer away from generating harmful content autonomously. To
start with, we introduce the \textsc{C$^2$-Eval} benchmark for quantitative
assessment and analyze 10 popular LLMs, revealing varying proficiency of
current safety-tuned LLMs in course-correction. To improve, we propose
fine-tuning LLMs with preference learning, emphasizing the preference for
timely course-correction. Using an automated pipeline, we create
\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to
teach models the concept of timely course-correction through data-driven
preference learning. Experiments on 2 LLMs, \textsc{Llama2-Chat 7B} and
\textsc{Qwen2 7B}, show that our method effectively enhances course-correction
skills without affecting general performance. Additionally, it effectively
improves LLMs' safety, particularly in resisting jailbreak attacks.","[{'name': 'Rongwu Xu'}, {'name': 'Yishuo Cai'}, {'name': 'Zhenhong Zhou'}, {'name': 'Renjie Gu'}, {'name': 'Haiqin Weng'}, {'name': 'Yan Liu'}, {'name': 'Tianwei Zhang'}, {'name': 'Wei Xu'}, {'name': 'Han Qiu'}]",2024-07-23T16:54:28Z
http://arxiv.org/abs/2407.16624v1,http://arxiv.org/abs/2407.16624v1,Semantic Change Characterization with LLMs using Rhetorics,"Languages continually evolve in response to societal events, resulting in new
terms and shifts in meanings. These changes have significant implications for
computer applications, including automatic translation and chatbots, making it
essential to characterize them accurately. The recent development of LLMs has
notably advanced natural language understanding, particularly in sense
inference and reasoning. In this paper, we investigate the potential of LLMs in
characterizing three types of semantic change: dimension, relation, and
orientation. We achieve this by combining LLMs' Chain-of-Thought with
rhetorical devices and conducting an experimental assessment of our approach
using newly created datasets. Our results highlight the effectiveness of LLMs
in capturing and analyzing semantic changes, providing valuable insights to
improve computational linguistic applications.","[{'name': 'Jader Martins Camboim de Sá'}, {'name': 'Marcos Da Silveira'}, {'name': 'Cédric Pruski'}]",2024-07-23T16:32:49Z
http://arxiv.org/abs/2407.16615v1,http://arxiv.org/abs/2407.16615v1,Lawma: The Power of Specialization for Legal Tasks,"Annotation and classification of legal text are central components of
empirical legal research. Traditionally, these tasks are often delegated to
trained research assistants. Motivated by the advances in language modeling,
empirical legal scholars are increasingly turning to prompting commercial
models, hoping that it will alleviate the significant cost of human annotation.
Despite growing use, our understanding of how to best utilize large language
models for legal tasks remains limited. We conduct a comprehensive study of 260
legal text classification tasks, nearly all new to the machine learning
community. Starting from GPT-4 as a baseline, we show that it has non-trivial
but highly varied zero-shot accuracy, often exhibiting performance that may be
insufficient for legal work. We then demonstrate that a lightly fine-tuned
Llama 3 model vastly outperforms GPT-4 on almost all tasks, typically by
double-digit percentage points. We find that larger models respond better to
fine-tuning than smaller models. A few tens to hundreds of examples suffice to
achieve high classification accuracy. Notably, we can fine-tune a single model
on all 260 tasks simultaneously at a small loss in accuracy relative to having
a separate model for each task. Our work points to a viable alternative to the
predominant practice of prompting commercial models. For concrete legal tasks
with some available labeled data, researchers are better off using a fine-tuned
open-source model.","[{'name': 'Ricardo Dominguez-Olmedo'}, {'name': 'Vedant Nanda'}, {'name': 'Rediet Abebe'}, {'name': 'Stefan Bechtold'}, {'name': 'Christoph Engel'}, {'name': 'Jens Frankenreiter'}, {'name': 'Krishna Gummadi'}, {'name': 'Moritz Hardt'}, {'name': 'Michael Livermore'}]",2024-07-23T16:23:04Z
http://arxiv.org/abs/2407.16607v2,http://arxiv.org/abs/2407.16607v2,"Data Mixture Inference: What do BPE Tokenizers Reveal about their
  Training Data?","The pretraining data of today's strongest language models is opaque; in
particular, little is known about the proportions of various domains or
languages represented. In this work, we tackle a task which we call data
mixture inference, which aims to uncover the distributional make-up of training
data. We introduce a novel attack based on a previously overlooked source of
information -- byte-pair encoding (BPE) tokenizers, used by the vast majority
of modern language models. Our key insight is that the ordered list of merge
rules learned by a BPE tokenizer naturally reveals information about the token
frequencies in its training data: the first merge is the most common byte pair,
the second is the most common pair after merging the first token, and so on.
Given a tokenizer's merge list along with data samples for each category of
interest, we formulate a linear program that solves for the proportion of each
category in the tokenizer's training set. Importantly, to the extent to which
tokenizer training data is representative of the pretraining data, we
indirectly learn about pretraining data. In controlled experiments, we show
that our attack recovers mixture ratios with high precision for tokenizers
trained on known mixtures of natural languages, programming languages, and data
sources. We then apply our approach to off-the-shelf tokenizers released with
recent LMs. We confirm much publicly disclosed information about these models,
and also make several new inferences: GPT-4o's tokenizer is much more
multilingual than its predecessors, training on 39% non-English data; Llama3
extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and
Claude's tokenizers are trained on predominantly code (~60%). We hope our work
sheds light on current design practices for pretraining data, and inspires
continued research into data mixture inference for LMs.","[{'name': 'Jonathan Hayase'}, {'name': 'Alisa Liu'}, {'name': 'Yejin Choi'}, {'name': 'Sewoong Oh'}, {'name': 'Noah A. Smith'}]",2024-07-23T16:13:22Z
http://arxiv.org/abs/2407.16604v1,http://arxiv.org/abs/2407.16604v1,Shared Imagination: LLMs Hallucinate Alike,"Despite the recent proliferation of large language models (LLMs), their
training recipes -- model architecture, pre-training data and optimization
algorithm -- are often very similar. This naturally raises the question of the
similarity among the resulting models. In this paper, we propose a novel
setting, imaginary question answering (IQA), to better understand model
similarity. In IQA, we ask one model to generate purely imaginary questions
(e.g., on completely made-up concepts in physics) and prompt another model to
answer. Surprisingly, despite the total fictionality of these questions, all
models can answer each other's questions with remarkable success, suggesting a
""shared imagination space"" in which these models operate during such
hallucinations. We conduct a series of investigations into this phenomenon and
discuss implications on model homogeneity, hallucination, and computational
creativity.","[{'name': 'Yilun Zhou'}, {'name': 'Caiming Xiong'}, {'name': 'Silvio Savarese'}, {'name': 'Chien-Sheng Wu'}]",2024-07-23T16:06:22Z
http://arxiv.org/abs/2407.16593v1,http://arxiv.org/abs/2407.16593v1,"A Comparative Study on Patient Language across Therapeutic Domains for
  Effective Patient Voice Classification in Online Health Discussions","There exists an invisible barrier between healthcare professionals'
perception of a patient's clinical experience and the reality. This barrier may
be induced by the environment that hinders patients from sharing their
experiences openly with healthcare professionals. As patients are observed to
discuss and exchange knowledge more candidly on social media, valuable insights
can be leveraged from these platforms. However, the abundance of non-patient
posts on social media necessitates filtering out such irrelevant content to
distinguish the genuine voices of patients, a task we refer to as patient voice
classification. In this study, we analyse the importance of linguistic
characteristics in accurately classifying patient voices. Our findings
underscore the essential role of linguistic and statistical text similarity
analysis in identifying common patterns among patient groups. These results
allude to even starker differences in the way patients express themselves at a
disease level and across various therapeutic domains. Additionally, we
fine-tuned a pre-trained Language Model on the combined datasets with similar
linguistic patterns, resulting in a highly accurate automatic patient voice
classification. Being the pioneering study on the topic, our focus on
extracting authentic patient experiences from social media stands as a crucial
step towards advancing healthcare standards and fostering a patient-centric
approach.","[{'name': 'Giorgos Lysandrou'}, {'name': 'Roma English Owen'}, {'name': 'Vanja Popovic'}, {'name': 'Grant Le Brun'}, {'name': 'Aryo Pradipta Gema'}, {'name': 'Beatrice Alex'}, {'name': 'Elizabeth A. L. Fairley'}]",2024-07-23T15:51:46Z
http://arxiv.org/abs/2407.16574v1,http://arxiv.org/abs/2407.16574v1,"TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement
  Learning from Human Feedback","Reinforcement Learning from Human Feedback (RLHF) leverages human preference
data to train language models to align more closely with human essence. These
human preference data, however, are labeled at the sequence level, creating a
mismatch between sequence-level preference labels and tokens, which are
autoregressively generated from the language model. Although several recent
approaches have tried to provide token-level (i.e., dense) rewards for each
individual token, these typically rely on predefined discrete reward values
(e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying
degrees of preference inherent to each token. To address this limitation, we
introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a
discriminator trained to distinguish positive and negative tokens, and the
confidence of the discriminator is used to assign continuous rewards to each
token considering the context. Extensive experiments show that our proposed
TLCR leads to consistent performance improvements over previous sequence-level
or token-level discrete rewards on open-ended generation benchmarks.","[{'name': 'Eunseop Yoon'}, {'name': 'Hee Suk Yoon'}, {'name': 'SooHwan Eom'}, {'name': 'Gunsoo Han'}, {'name': 'Daniel Wontae Nam'}, {'name': 'Daejin Jo'}, {'name': 'Kyoung-Woon On'}, {'name': 'Mark A. Hasegawa-Johnson'}, {'name': 'Sungwoong Kim'}, {'name': 'Chang D. Yoo'}]",2024-07-23T15:27:37Z
http://arxiv.org/abs/2407.16565v1,http://arxiv.org/abs/2407.16565v1,"Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases
  Generation with Small Language Models","Recent surge in the accessibility of large language models (LLMs) to the
general population can lead to untrackable use of such models for
medical-related recommendations. Language generation via LLMs models has two
key problems: firstly, they are prone to hallucination and therefore, for any
medical purpose they require scientific and factual grounding; secondly, LLMs
pose tremendous challenge to computational resources due to their gigantic
model size. In this work, we introduce pRAGe, a pipeline for Retrieval
Augmented Generation and evaluation of medical paraphrases generation using
Small Language Models (SLM). We study the effectiveness of SLMs and the impact
of external knowledge base for medical paraphrase generation in French.","[{'name': 'Ioana Buhnila'}, {'name': 'Aman Sinha'}, {'name': 'Mathieu Constant'}]",2024-07-23T15:17:11Z
http://arxiv.org/abs/2407.16537v1,http://arxiv.org/abs/2407.16537v1,"Quantifying the Role of Textual Predictability in Automatic Speech
  Recognition","A long-standing question in automatic speech recognition research is how to
attribute errors to the ability of a model to model the acoustics, versus its
ability to leverage higher-order context (lexicon, morphology, syntax,
semantics). We validate a novel approach which models error rates as a function
of relative textual predictability, and yields a single number, $k$, which
measures the effect of textual predictability on the recognizer. We use this
method to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use
of textual context than a hybrid ASR model, in spite of not using an explicit
language model, and also use it to shed light on recent results demonstrating
poor performance of standard ASR systems on African-American English. We
demonstrate that these mostly represent failures of acoustic--phonetic
modelling. We show how this approach can be used straightforwardly in
diagnosing and improving ASR.","[{'name': 'Sean Robertson'}, {'name': 'Gerald Penn'}, {'name': 'Ewan Dunbar'}]",2024-07-23T14:47:25Z
http://arxiv.org/abs/2407.16526v1,http://arxiv.org/abs/2407.16526v1,"Imperfect Vision Encoders: Efficient and Robust Tuning for
  Vision-Language Models","Vision language models (VLMs) demonstrate impressive capabilities in visual
question answering and image captioning, acting as a crucial link between
visual and language models. However, existing open-source VLMs heavily rely on
pretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness
across diverse domains, it still exhibits non-negligible image understanding
errors. These errors propagate to the VLM responses, resulting in sub-optimal
performance. In our work, we propose an efficient and robust method for
updating vision encoders within VLMs. Our approach selectively and locally
updates encoders, leading to substantial performance improvements on data where
previous mistakes occurred, while maintaining overall robustness. Furthermore,
we demonstrate the effectiveness of our method during continual few-shot
updates. Theoretical grounding, generality, and computational efficiency
characterize our approach.","[{'name': 'Aristeidis Panos'}, {'name': 'Rahaf Aljundi'}, {'name': 'Daniel Olmeda Reino'}, {'name': 'Richard E Turner'}]",2024-07-23T14:39:40Z
http://arxiv.org/abs/2407.16521v2,http://arxiv.org/abs/2407.16521v2,"AMONGAGENTS: Evaluating Large Language Models in the Interactive
  Text-Based Social Deduction Game","Strategic social deduction games serve as valuable testbeds for evaluating
the understanding and inference skills of language models, offering crucial
insights into social science, artificial intelligence, and strategic gaming.
This paper focuses on creating proxies of human behavior in simulated
environments, with Among Us utilized as a tool for studying simulated human
behavior. The study introduces a text-based game environment, named
AmongAgents, that mirrors the dynamics of Among Us. Players act as crew members
aboard a spaceship, tasked with identifying impostors who are sabotaging the
ship and eliminating the crew. Within this environment, the behavior of
simulated language agents is analyzed. The experiments involve diverse game
sequences featuring different configurations of Crewmates and Impostor
personality archetypes. Our work demonstrates that state-of-the-art large
language models (LLMs) can effectively grasp the game rules and make decisions
based on the current context. This work aims to promote further exploration of
LLMs in goal-oriented games with incomplete information and complex action
spaces, as these settings offer valuable opportunities to assess language model
performance in socially driven scenarios.","[{'name': 'Yizhou Chi'}, {'name': 'Lingjun Mao'}, {'name': 'Zineng Tang'}]",2024-07-23T14:34:38Z
http://arxiv.org/abs/2407.16516v1,http://arxiv.org/abs/2407.16516v1,"Assessing In-context Learning and Fine-tuning for Topic Classification
  of German Web Data","Researchers in the political and social sciences often rely on classification
models to analyze trends in information consumption by examining browsing
histories of millions of webpages. Automated scalable methods are necessary due
to the impracticality of manual labeling. In this paper, we model the detection
of topic-related content as a binary classification task and compare the
accuracy of fine-tuned pre-trained encoder models against in-context learning
strategies. Using only a few hundred annotated data points per topic, we detect
content related to three German policies in a database of scraped webpages. We
compare multilingual and monolingual models, as well as zero and few-shot
approaches, and investigate the impact of negative sampling strategies and the
combination of URL & content-based features. Our results show that a small
sample of annotated data is sufficient to train an effective classifier.
Fine-tuning encoder-based models yields better results than in-context
learning. Classifiers using both URL & content-based features perform best,
while using URLs alone provides adequate results when content is unavailable.","[{'name': 'Julian Schelb'}, {'name': 'Roberto Ulloa'}, {'name': 'Andreas Spitz'}]",2024-07-23T14:31:59Z
http://arxiv.org/abs/2407.16470v2,http://arxiv.org/abs/2407.16470v2,"Machine Translation Hallucination Detection for Low and High Resource
  Languages using Large Language Models","Recent advancements in massively multilingual machine translation systems
have significantly enhanced translation accuracy; however, even the best
performing systems still generate hallucinations, severely impacting user
trust. Detecting hallucinations in Machine Translation (MT) remains a critical
challenge, particularly since existing methods excel with High-Resource
Languages (HRLs) but exhibit substantial limitations when applied to
Low-Resource Languages (LRLs). This paper evaluates hallucination detection
approaches using Large Language Models (LLMs) and semantic similarity within
massively multilingual embeddings. Our study spans 16 language directions,
covering HRLs, LRLs, with diverse scripts. We find that the choice of model is
essential for performance. On average, for HRLs, Llama3-70B outperforms the
previous state of the art by as much as 0.16 MCC (Matthews Correlation
Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other
LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can
achieve performance comparable or even better than previously proposed models,
despite not being explicitly trained for any machine translation task. However,
their advantage is less significant for LRLs.","[{'name': 'Kenza Benkirane'}, {'name': 'Laura Gongas'}, {'name': 'Shahar Pelles'}, {'name': 'Naomi Fuchs'}, {'name': 'Joshua Darmon'}, {'name': 'Pontus Stenetorp'}, {'name': 'David Ifeoluwa Adelani'}, {'name': 'Eduardo Sánchez'}]",2024-07-23T13:40:54Z
http://arxiv.org/abs/2407.16444v1,http://arxiv.org/abs/2407.16444v1,"Psychomatics -- A Multidisciplinary Framework for Understanding
  Artificial Minds","Although LLMs and other artificial intelligence systems demonstrate cognitive
skills similar to humans, like concept learning and language acquisition, the
way they process information fundamentally differs from biological cognition.
To better understand these differences this paper introduces Psychomatics, a
multidisciplinary framework bridging cognitive science, linguistics, and
computer science. It aims to better understand the high-level functioning of
LLMs, focusing specifically on how LLMs acquire, learn, remember, and use
information to produce their outputs. To achieve this goal, Psychomatics will
rely on a comparative methodology, starting from a theory-driven research
question - is the process of language development and use different in humans
and LLMs? - drawing parallels between LLMs and biological systems. Our analysis
shows how LLMs can map and manipulate complex linguistic patterns in their
training data. Moreover, LLMs can follow Grice's Cooperative Principle to
provide relevant and informative responses. However, human cognition draws from
multiple sources of meaning, including experiential, emotional, and imaginative
facets, which transcend mere language processing and are rooted in our social
and developmental trajectories. Moreover, current LLMs lack physical
embodiment, reducing their ability to make sense of the intricate interplay
between perception, action, and cognition that shapes human understanding and
expression. Ultimately, Psychomatics holds the potential to yield
transformative insights into the nature of language, cognition, and
intelligence, both artificial and biological. Moreover, by drawing parallels
between LLMs and human cognitive processes, Psychomatics can inform the
development of more robust and human-like AI systems.","[{'name': 'Giuseppe Riva'}, {'name': 'Fabrizia Mantovani'}, {'name': 'Brenda K. Wiederhold'}, {'name': 'Antonella Marchetti'}, {'name': 'Andrea Gaggioli'}]",2024-07-23T12:53:41Z
http://arxiv.org/abs/2407.16724v1,http://arxiv.org/abs/2407.16724v1,"Educating LLMs like Human Students: Structure-aware Injection of Domain
  Knowledge","This paper presents a pioneering methodology, termed StructTuning, to
efficiently transform foundation Large Language Models (LLMs) into domain
specialists. It significantly minimizes the training corpus requirement to a
mere 0.3% while achieving an impressive 50% of traditional knowledge injection
performance. Our method is inspired by the educational processes for human
students, particularly how structured domain knowledge from textbooks is
absorbed and then applied to tackle real-world challenges through specific
exercises. Based on this, we propose a novel two-stage knowledge injection
strategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware
Supervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data
into an auto-generated taxonomy of domain knowledge, enabling LLMs to
effectively memorize textual segments linked to specific expertise within the
taxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt
models to reveal the underlying knowledge structure in their outputs,
leveraging this structured domain insight to address practical problems
adeptly. Our ultimate method has undergone extensive evaluations across model
architectures and scales, using closed-book question-answering tasks on
LongBench and MMedBench datasets. Remarkably, our method matches 50% of the
improvement displayed by the state-of-the-art MMedLM2 on MMedBench, but with
only 0.3% quantity of the training corpus. This breakthrough showcases the
potential to scale up our StructTuning for stronger domain-specific LLMs. Code
will be made public soon.","[{'name': 'Kai Liu'}, {'name': 'Ze Chen'}, {'name': 'Zhihang Fu'}, {'name': 'Rongxin Jiang'}, {'name': 'Fan Zhou'}, {'name': 'Yaowu Chen'}, {'name': 'Yue Wu'}, {'name': 'Jieping Ye'}]",2024-07-23T12:38:48Z
http://arxiv.org/abs/2407.16434v1,http://arxiv.org/abs/2407.16434v1,Enhancing LLM's Cognition via Structurization,"When reading long-form text, human cognition is complex and structurized.
While large language models (LLMs) process input contexts through a causal and
sequential perspective, this approach can potentially limit their ability to
handle intricate and complex inputs effectively. To enhance LLM's cognition
capability, this paper presents a novel concept of context structurization.
Specifically, we transform the plain, unordered contextual sentences into
well-ordered and hierarchically structurized elements. By doing so, LLMs can
better grasp intricate and extended contexts through precise attention and
information-seeking along the organized structures. Extensive evaluations are
conducted across various model architectures and sizes (including several 7B-
to 72B-size auto-regressive LLMs as well as BERT-like masking models) on a
diverse set of NLP tasks (e.g., context-based question-answering, exhaustive
hallucination evaluation, and passage-level dense retrieval). Empirical results
show consistent and significant performance gains afforded by a single-round
structurization. In particular, we boost a 72B-parameter open-source model to
achieve comparable performance against GPT-3.5-Turbo as the hallucination
evaluator. Besides, we show the feasibility of distilling advanced LLMs'
language processing abilities to a smaller yet effective StruXGPT-7B to execute
structurization, addressing the practicality of our approach. Code will be made
public soon.","[{'name': 'Kai Liu'}, {'name': 'Zhihang Fu'}, {'name': 'Chao Chen'}, {'name': 'Wei Zhang'}, {'name': 'Rongxin Jiang'}, {'name': 'Fan Zhou'}, {'name': 'Yaowu Chen'}, {'name': 'Yue Wu'}, {'name': 'Jieping Ye'}]",2024-07-23T12:33:58Z
http://arxiv.org/abs/2407.16431v1,http://arxiv.org/abs/2407.16431v1,"FairFlow: An Automated Approach to Model-based Counterfactual Data
  Augmentation For NLP","Despite the evolution of language models, they continue to portray harmful
societal biases and stereotypes inadvertently learned from training data. These
inherent biases often result in detrimental effects in various applications.
Counterfactual Data Augmentation (CDA), which seeks to balance demographic
attributes in training data, has been a widely adopted approach to mitigate
bias in natural language processing. However, many existing CDA approaches rely
on word substitution techniques using manually compiled word-pair dictionaries.
These techniques often lead to out-of-context substitutions, resulting in
potential quality issues. The advancement of model-based techniques, on the
other hand, has been challenged by the need for parallel training data. Works
in this area resort to manually generated parallel data that are expensive to
collect and are consequently limited in scale. This paper proposes FairFlow, an
automated approach to generating parallel data for training counterfactual text
generator models that limits the need for human intervention. Furthermore, we
show that FairFlow significantly overcomes the limitations of dictionary-based
word-substitution approaches whilst maintaining good performance.","[{'name': 'Ewoenam Kwaku Tokpo'}, {'name': 'Toon Calders'}]",2024-07-23T12:29:37Z
http://arxiv.org/abs/2407.21053v1,http://arxiv.org/abs/2407.21053v1,"Knowledge Models for Cancer Clinical Practice Guidelines : Construction,
  Management and Usage in Question Answering","An automated knowledge modeling algorithm for Cancer Clinical Practice
Guidelines (CPGs) extracts the knowledge contained in the CPG documents and
transforms it into a programmatically interactable, easy-to-update structured
model with minimal human intervention. The existing automated algorithms have
minimal scope and cannot handle the varying complexity of the knowledge content
in the CPGs for different cancer types. This work proposes an improved
automated knowledge modeling algorithm to create knowledge models from the
National Comprehensive Cancer Network (NCCN) CPGs in Oncology for different
cancer types. The proposed algorithm has been evaluated with NCCN CPGs for four
different cancer types. We also proposed an algorithm to compare the knowledge
models for different versions of a guideline to discover the specific changes
introduced in the treatment protocol of a new version. We created a
question-answering (Q&A) framework with the guideline knowledge models as the
augmented knowledge base to study our ability to query the knowledge models. We
compiled a set of 32 question-answer pairs derived from two reliable data
sources for the treatment of Non-Small Cell Lung Cancer (NSCLC) to evaluate the
Q&A framework. The framework was evaluated against the question-answer pairs
from one data source, and it can generate the answers with 54.5% accuracy from
the treatment algorithm and 81.8% accuracy from the discussion part of the NCCN
NSCLC guideline knowledge model.","[{'name': 'Pralaypati Ta'}, {'name': 'Bhumika Gupta'}, {'name': 'Arihant Jain'}, {'name': 'Sneha Sree C'}, {'name': 'Keerthi Ram'}, {'name': 'Mohanasankar Sivaprakasam'}]",2024-07-23T11:26:40Z
http://arxiv.org/abs/2407.16382v1,http://arxiv.org/abs/2407.16382v1,TookaBERT: A Step Forward for Persian NLU,"The field of natural language processing (NLP) has seen remarkable
advancements, thanks to the power of deep learning and foundation models.
Language models, and specifically BERT, have been key players in this progress.
In this study, we trained and introduced two new BERT models using Persian
data. We put our models to the test, comparing them to seven existing models
across 14 diverse Persian natural language understanding (NLU) tasks. The
results speak for themselves: our larger model outperforms the competition,
showing an average improvement of at least +2.8 points. This highlights the
effectiveness and potential of our new BERT models for Persian NLU tasks.","[{'name': 'MohammadAli SadraeiJavaheri'}, {'name': 'Ali Moghaddaszadeh'}, {'name': 'Milad Molazadeh'}, {'name': 'Fariba Naeiji'}, {'name': 'Farnaz Aghababaloo'}, {'name': 'Hamideh Rafiee'}, {'name': 'Zahra Amirmahani'}, {'name': 'Tohid Abedini'}, {'name': 'Fatemeh Zahra Sheikhi'}, {'name': 'Amirmohammad Salehoof'}]",2024-07-23T11:12:47Z
http://arxiv.org/abs/2407.16370v1,http://arxiv.org/abs/2407.16370v1,Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction,"Building upon the strength of modern large language models (LLMs), generative
error correction (GEC) has emerged as a promising paradigm that can elevate the
performance of modern automatic speech recognition (ASR) systems. One
representative approach is to leverage in-context learning to prompt LLMs so
that a better hypothesis can be generated by the LLMs based on a
carefully-designed prompt and an $N$-best list of hypotheses produced by ASR
systems. However, it is yet unknown whether the existing prompts are the most
effective ones for the task of post-ASR error correction. In this context, this
paper first explores alternative prompts to identify an initial set of
effective prompts, and then proposes to employ an evolutionary prompt
optimization algorithm to refine the initial prompts. Evaluations results on
the CHiME-4 subset of the Task $1$ of the SLT $2024$ GenSEC challenge show the
effectiveness and potential of the proposed algorithms.","[{'name': 'Rithik Sachdev'}, {'name': 'Zhong-Qiu Wang'}, {'name': 'Chao-Han Huck Yang'}]",2024-07-23T10:38:49Z
http://arxiv.org/abs/2407.16347v1,http://arxiv.org/abs/2407.16347v1,FACTTRACK: Time-Aware World State Tracking in Story Outlines,"While accurately detecting and correcting factual contradictions in language
model outputs has become increasingly important as their capabilities improve,
doing so is highly challenging. We propose a novel method, FACTTRACK, for
tracking atomic facts and addressing factual contradictions. Crucially,
FACTTRACK also maintains time-aware validity intervals for each fact, allowing
for change over time. At a high level, FACTTRACK consists of a four-step
pipeline to update a world state data structure for each new event: (1)
decompose the event into directional atomic facts; (2) determine the validity
interval of each atomic fact using the world state; (3) detect contradictions
with existing facts in the world state; and finally (4) add new facts to the
world state and update existing atomic facts. When we apply FACTTRACK to
contradiction detection on structured story outlines, we find that FACTTRACK
using LLaMA2-7B-Chat substantially outperforms a fair baseline using
LLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline.
Moreover, when using GPT4, FACTTRACK significantly outperforms the GPT4
baseline.","[{'name': 'Zhiheng Lyu'}, {'name': 'Kevin Yang'}, {'name': 'Lingpeng Kong'}, {'name': 'Daniel Klein'}]",2024-07-23T09:50:14Z
http://arxiv.org/abs/2407.16318v1,http://arxiv.org/abs/2407.16318v1,PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing,"Deploying language models (LMs) necessitates outputs to be both high-quality
and compliant with safety guidelines. Although Inference-Time Guardrails (ITG)
offer solutions that shift model output distributions towards compliance, we
find that current methods struggle in balancing safety with helpfulness. ITG
Methods that safely address non-compliant queries exhibit lower helpfulness
while those that prioritize helpfulness compromise on safety. We refer to this
trade-off as the guardrail tax, analogous to the alignment tax. To address
this, we propose PrimeGuard, a novel ITG method that utilizes structured
control flow.
  PrimeGuard routes requests to different self-instantiations of the LM with
varying instructions, leveraging its inherent instruction-following
capabilities and in-context learning. Our tuning-free approach dynamically
compiles system-designer guidelines for each query. We construct and release
safe-eval, a diverse red-team safety benchmark. Extensive evaluations
demonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail tax
by (1) significantly increasing resistance to iterative jailbreak attacks and
(2) achieving state-of-the-art results in safety guardrailing while (3)
matching helpfulness scores of alignment-tuned models. Extensive evaluations
demonstrate that PrimeGuard, without fine-tuning, outperforms all competing
baselines and overcomes the guardrail tax by improving the fraction of safe
responses from 61% to 97% and increasing average helpfulness scores from 4.17
to 4.29 on the largest models, while reducing attack success rate from 100% to
8%.
  PrimeGuard implementation is available at
https://github.com/dynamofl/PrimeGuard and safe-eval dataset is available at
https://huggingface.co/datasets/dynamoai/safe_eval.","[{'name': 'Blazej Manczak'}, {'name': 'Eliott Zemour'}, {'name': 'Eric Lin'}, {'name': 'Vaikkunth Mugunthan'}]",2024-07-23T09:14:27Z
http://arxiv.org/abs/2407.21052v1,http://arxiv.org/abs/2407.21052v1,"Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet
  Extraction","Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract
fine-grained sentiment elements from target domain sentences by leveraging the
knowledge acquired from the source domain. Due to the absence of labeled data
in the target domain, recent studies tend to rely on pre-trained language
models to generate large amounts of synthetic data for training purposes.
However, these approaches entail additional computational costs associated with
the generation process. Different from them, we discover a striking resemblance
between table-filling methods in ASTE and two-stage Object Detection (OD) in
computer vision, which inspires us to revisit the cross-domain ASTE task and
approach it from an OD standpoint. This allows the model to benefit from the OD
extraction paradigm and region-level alignment. Building upon this premise, we
propose a novel method named \textbf{T}able-\textbf{F}illing via \textbf{M}ean
\textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the
sentence into a 2D table to detect word relations, while TFMT treats the table
as a feature map and utilizes a region consistency to enhance the quality of
those generated pseudo labels. Additionally, considering the existence of the
domain gap, a cross-domain consistency based on Maximum Mean Discrepancy is
designed to alleviate domain shift problems. Our method achieves
state-of-the-art performance with minimal parameters and computational costs,
making it a strong baseline for cross-domain ASTE.","[{'name': 'Kun Peng'}, {'name': 'Lei Jiang'}, {'name': 'Qian Li'}, {'name': 'Haoran Li'}, {'name': 'Xiaoyan Yu'}, {'name': 'Li Sun'}, {'name': 'Shuo Sun'}, {'name': 'Yanxian Bi'}, {'name': 'Hao Peng'}]",2024-07-23T09:04:08Z
http://arxiv.org/abs/2408.06345v1,http://arxiv.org/abs/2408.06345v1,"Deep Learning based Key Information Extraction from Business Documents:
  Systematic Literature Review","Extracting key information from documents represents a large portion of
business workloads and therefore offers a high potential for efficiency
improvements and process automation. With recent advances in deep learning, a
plethora of deep learning-based approaches for Key Information Extraction have
been proposed under the umbrella term Document Understanding that enable the
processing of complex business documents. The goal of this systematic
literature review is an in-depth analysis of existing approaches in this domain
and the identification of opportunities for further research. To this end, 96
approaches published between 2017 and 2023 are analyzed in this study.","[{'name': 'Alexander Rombach'}, {'name': 'Peter Fettke'}]",2024-07-23T08:15:55Z
http://arxiv.org/abs/2407.16266v1,http://arxiv.org/abs/2407.16266v1,"Beyond Binary Gender: Evaluating Gender-Inclusive Machine Translation
  with Ambiguous Attitude Words","Gender bias has been a focal point in the study of bias in machine
translation and language models. Existing machine translation gender bias
evaluations are primarily focused on male and female genders, limiting the
scope of the evaluation. To assess gender bias accurately, these studies often
rely on calculating the accuracy of gender pronouns or the masculine and
feminine attributes of grammatical gender via the stereotypes triggered by
occupations or sentiment words ({\em i.e.}, clear positive or negative
attitude), which cannot extend to non-binary groups. This study presents a
benchmark AmbGIMT (Gender-Inclusive Machine Translation with Ambiguous attitude
words), which assesses gender bias beyond binary gender. Meanwhile, we propose
a novel process to evaluate gender bias based on the Emotional Attitude Score
(EAS), which is used to quantify ambiguous attitude words. In evaluating three
recent and effective open-source LLMs and one powerful multilingual
translation-specific model, our main observations are: (1) The translation
performance within non-binary gender contexts is markedly inferior in terms of
translation quality and exhibits more negative attitudes than binary-gender
contexts. (2) The analysis experiments indicate that incorporating constraint
context in prompts for gender identity terms can substantially reduce
translation bias, while the bias remains evident despite the presence of the
constraints. The code is publicly available at
\url{https://github.com/pppa2019/ambGIMT}.","[{'name': 'Yijie Chen'}, {'name': 'Yijin Liu'}, {'name': 'Fandong Meng'}, {'name': 'Jinan Xu'}, {'name': 'Yufeng Chen'}, {'name': 'Jie Zhou'}]",2024-07-23T08:13:51Z
http://arxiv.org/abs/2407.16252v2,http://arxiv.org/abs/2407.16252v2,LawLuo: A Chinese Law Firm Co-run by LLM Agents,"Large Language Models (LLMs) demonstrate substantial potential in delivering
legal consultation services to users without a legal background, attributed to
their superior text comprehension and generation capabilities. Nonetheless,
existing Chinese legal LLMs limit interaction to a single model-user dialogue,
unlike the collaborative consultations typical of law firms, where multiple
staff members contribute to a single consultation. This limitation prevents an
authentic consultation experience. Additionally, extant Chinese legal LLMs
suffer from critical limitations: (1) insufficient control over the quality of
instruction fine-tuning data; (2) increased model hallucination resulting from
users' ambiguous queries; and (3) a reduction in the model's ability to follow
instructions over multiple dialogue turns. In response to these challenges, we
propose a novel legal dialogue framework that leverages the collaborative
capabilities of multiple LLM agents, termed LawLuo. This framework encompasses
four agents: a receptionist, a lawyer, a secretary, and a boss, each
responsible for different functionalities, collaboratively providing a
comprehensive legal consultation to users. Additionally, we constructed two
high-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned
ChatGLM-3-6b using these datasets. We propose a legal query clarification
algorithm called ToLC. Experimental results demonstrate that LawLuo outperforms
baseline LLMs, including GPT-4, across three dimensions: lawyer-like language
style, the usefulness of legal advice, and the accuracy of legal knowledge. Our
code and datasets are available at https://github.com/NEFUJing/LawLuo.","[{'name': 'Jingyun Sun'}, {'name': 'Chengxiao Dai'}, {'name': 'Zhongze Luo'}, {'name': 'Yangbo Chang'}, {'name': 'Yang Li'}]",2024-07-23T07:40:41Z
http://arxiv.org/abs/2407.16245v1,http://arxiv.org/abs/2407.16245v1,"Exploring the Effectiveness and Consistency of Task Selection in
  Intermediate-Task Transfer Learning","Identifying beneficial tasks to transfer from is a critical step toward
successful intermediate-task transfer learning. In this work, we experiment
with 130 source-target task combinations and demonstrate that the transfer
performance exhibits severe variance across different source tasks and training
seeds, highlighting the crucial role of intermediate-task selection in a
broader context. We compare four representative task selection methods in a
unified setup, focusing on their effectiveness and consistency. Compared to
embedding-free methods and text embeddings, task embeddings constructed from
fine-tuned weights can better estimate task transferability by improving task
prediction scores from 2.59% to 3.96%. Despite their strong performance, we
observe that the task embeddings do not consistently demonstrate superiority
for tasks requiring reasoning abilities. Furthermore, we introduce a novel
method that measures pairwise token similarity using maximum inner product
search, leading to the highest performance in task prediction. Our findings
suggest that token-wise similarity is better predictive for predicting
transferability compared to averaging weights.","[{'name': 'Pin-Jie Lin'}, {'name': 'Miaoran Zhang'}, {'name': 'Marius Mosbach'}, {'name': 'Dietrich Klakow'}]",2024-07-23T07:31:43Z
http://arxiv.org/abs/2407.16234v1,http://arxiv.org/abs/2407.16234v1,"A Multi-view Mask Contrastive Learning Graph Convolutional Neural
  Network for Age Estimation","The age estimation task aims to use facial features to predict the age of
people and is widely used in public security, marketing, identification, and
other fields. However, the features are mainly concentrated in facial
keypoints, and existing CNN and Transformer-based methods have inflexibility
and redundancy for modeling complex irregular structures. Therefore, this paper
proposes a Multi-view Mask Contrastive Learning Graph Convolutional Neural
Network (MMCL-GCN) for age estimation. Specifically, the overall structure of
the MMCL-GCN network contains a feature extraction stage and an age estimation
stage. In the feature extraction stage, we introduce a graph structure to
construct face images as input and then design a Multi-view Mask Contrastive
Learning (MMCL) mechanism to learn complex structural and semantic information
about face images. The learning mechanism employs an asymmetric siamese network
architecture, which utilizes an online encoder-decoder structure to reconstruct
the missing information from the original graph and utilizes the target encoder
to learn latent representations for contrastive learning. Furthermore, to
promote the two learning mechanisms better compatible and complementary, we
adopt two augmentation strategies and optimize the joint losses. In the age
estimation stage, we design a Multi-layer Extreme Learning Machine (ML-IELM)
with identity mapping to fully use the features extracted by the online
encoder. Then, a classifier and a regressor were constructed based on ML-IELM,
which were used to identify the age grouping interval and accurately estimate
the final age. Extensive experiments show that MMCL-GCN can effectively reduce
the error of age estimation on benchmark datasets such as Adience, MORPH-II,
and LAP-2016.","[{'name': 'Yiping Zhang'}, {'name': 'Yuntao Shou'}, {'name': 'Tao Meng'}, {'name': 'Wei Ai'}, {'name': 'Keqin Li'}]",2024-07-23T07:17:46Z
http://arxiv.org/abs/2407.16222v1,http://arxiv.org/abs/2407.16222v1,"PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of
  Multilingual Alignment","Large language models demonstrate reasonable multilingual abilities, despite
predominantly English-centric pretraining. However, the spontaneous
multilingual alignment in these models is shown to be weak, leading to
unsatisfactory cross-lingual transfer and knowledge sharing. Previous works
attempt to address this issue by explicitly injecting multilingual alignment
information during or after pretraining. Thus for the early stage in
pretraining, the alignment is weak for sharing information or knowledge across
languages. In this paper, we propose PreAlign, a framework that establishes
multilingual alignment prior to language model pretraining. PreAlign injects
multilingual alignment by initializing the model to generate similar
representations of aligned words and preserves this alignment using a
code-switching strategy during pretraining. Extensive experiments in a
synthetic English to English-Clone setting demonstrate that PreAlign
significantly outperforms standard multilingual joint training in language
modeling, zero-shot cross-lingual transfer, and cross-lingual knowledge
application. Further experiments in real-world scenarios further validate
PreAlign's effectiveness across various model sizes.","[{'name': 'Jiahuan Li'}, {'name': 'Shujian Huang'}, {'name': 'Xinyu Dai'}, {'name': 'Jiajun Chen'}]",2024-07-23T06:59:53Z
http://arxiv.org/abs/2407.16221v1,http://arxiv.org/abs/2407.16221v1,"Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of
  Large Language Models","As Large Language Models (LLMs) achieve remarkable performance across various
NLP tasks, their reliability becomes essential for widespread adoption. This
paper focuses on Abstention Ability (AA), a critical yet under explored aspect
of reliability - the ability of LLMs to refrain from answering questions when
they are uncertain or when definitive answer is not possible, while maintaining
question-answering (QA) task performance. While previous works have focused on
understanding the recollection abilities of LLMs or their ability to identify
imponderable/unanswerable questions, we believe there is a need for an
effective AA evaluation method. Therefore, we propose a black-box evaluation
methodology to examine and understand the AA of LLMs across a variety of
multiple-choice QA tasks. We measure AA by rewarding models for abstaining from
answering when their predictions are incorrect or when the questions are
inherently unanswerable. We investigate three strategies, Strict Prompting,
Verbal Confidence Thresholding, and Chain-of-Thought (CoT), to understand their
impact on abstention across different LLMs. Our findings reveal that while even
state-of-the-art LLMs like GPT-4 struggle with abstention, strategic prompting
such as CoT, can significantly enhance this ability. Furthermore, we
demonstrate that improving AA also leads to better overall QA task performance,
underscoring the importance of evaluating AA in LLMs.","[{'name': 'Nishanth Madhusudhan'}, {'name': 'Sathwik Tejaswi Madhusudhan'}, {'name': 'Vikas Yadav'}, {'name': 'Masoud Hashemi'}]",2024-07-23T06:56:54Z
http://arxiv.org/abs/2407.16216v1,http://arxiv.org/abs/2407.16216v1,"A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO,
  DPO and More","With advancements in self-supervised learning, the availability of trillions
tokens in a pre-training corpus, instruction fine-tuning, and the development
of large Transformers with billions of parameters, large language models (LLMs)
are now capable of generating factual and coherent responses to human queries.
However, the mixed quality of training data can lead to the generation of
undesired responses, presenting a significant challenge. Over the past two
years, various methods have been proposed from different perspectives to
enhance LLMs, particularly in aligning them with human expectation. Despite
these efforts, there has not been a comprehensive survey paper that categorizes
and details these approaches. In this work, we aim to address this gap by
categorizing these papers into distinct topics and providing detailed
explanations of each alignment method, thereby helping readers gain a thorough
understanding of the current state of the field.","[{'name': 'Zhichao Wang'}, {'name': 'Bin Bi'}, {'name': 'Shiva Kumar Pentyala'}, {'name': 'Kiran Ramnath'}, {'name': 'Sougata Chaudhuri'}, {'name': 'Shubham Mehrotra'}, {'name': 'Zixu'}, {'name': 'Zhu'}, {'name': 'Xiang-Bo Mao'}, {'name': 'Sitaram Asur'}, {'name': 'Na'}, {'name': 'Cheng'}]",2024-07-23T06:45:52Z
http://arxiv.org/abs/2407.16207v1,http://arxiv.org/abs/2407.16207v1,Graph-Structured Speculative Decoding,"Speculative decoding has emerged as a promising technique to accelerate the
inference of Large Language Models (LLMs) by employing a small language model
to draft a hypothesis sequence, which is then validated by the LLM. The
effectiveness of this approach heavily relies on the balance between
performance and efficiency of the draft model. In our research, we focus on
enhancing the proportion of draft tokens that are accepted to the final output
by generating multiple hypotheses instead of just one. This allows the LLM more
options to choose from and select the longest sequence that meets its
standards. Our analysis reveals that hypotheses produced by the draft model
share many common token sequences, suggesting a potential for optimizing
computation. Leveraging this observation, we introduce an innovative approach
utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This
structure enables us to efficiently predict and merge recurring token
sequences, vastly reducing the computational demands of the draft model. We
term this approach Graph-structured Speculative Decoding (GSD). We apply GSD
across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and
observe a remarkable speedup of 1.73$\times$ to 1.96$\times$, significantly
surpassing standard speculative decoding.","[{'name': 'Zhuocheng Gong'}, {'name': 'Jiahao Liu'}, {'name': 'Ziyue Wang'}, {'name': 'Pengfei Wu'}, {'name': 'Jingang Wang'}, {'name': 'Xunliang Cai'}, {'name': 'Dongyan Zhao'}, {'name': 'Rui Yan'}]",2024-07-23T06:21:24Z
http://arxiv.org/abs/2407.16205v3,http://arxiv.org/abs/2407.16205v3,Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models,"The rapid development of Large Language Models (LLMs) has brought remarkable
generative capabilities across diverse tasks. However, despite the impressive
achievements, these LLMs still have numerous inherent vulnerabilities,
particularly when faced with jailbreak attacks. By investigating jailbreak
attacks, we can uncover hidden weaknesses in LLMs and inform the development of
more robust defense mechanisms to fortify their security. In this paper, we
further explore the boundary of jailbreak attacks on LLMs and propose
Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes
advantage of LLMs' growing analyzing and reasoning capability and reveals their
underlying vulnerabilities when facing analyzing-based tasks. We conduct a
detailed evaluation of ABJ across various open-source and closed-source LLMs,
which achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)
on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and
efficiency. Our research highlights the importance of prioritizing and
enhancing the safety of LLMs to mitigate the risks of misuse. The code is
publicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This
paper contains examples of LLMs that might be offensive or harmful.","[{'name': 'Shi Lin'}, {'name': 'Rongchang Li'}, {'name': 'Xun Wang'}, {'name': 'Changting Lin'}, {'name': 'Wenpeng Xing'}, {'name': 'Meng Han'}]",2024-07-23T06:14:41Z
http://arxiv.org/abs/2407.16192v1,http://arxiv.org/abs/2407.16192v1,"How to Leverage Personal Textual Knowledge for Personalized
  Conversational Information Retrieval","Personalized conversational information retrieval (CIR) combines
conversational and personalizable elements to satisfy various users' complex
information needs through multi-turn interaction based on their backgrounds.
The key promise is that the personal textual knowledge base (PTKB) can improve
the CIR effectiveness because the retrieval results can be more related to the
user's background. However, PTKB is noisy: not every piece of knowledge in PTKB
is relevant to the specific query at hand. In this paper, we explore and test
several ways to select knowledge from PTKB and use it for query reformulation
by using a large language model (LLM). The experimental results show the PTKB
might not always improve the search results when used alone, but LLM can help
generate a more appropriate personalized query when high-quality guidance is
provided.","[{'name': 'Fengran Mo'}, {'name': 'Longxiang Zhao'}, {'name': 'Kaiyu Huang'}, {'name': 'Yue Dong'}, {'name': 'Degen Huang'}, {'name': 'Jian-Yun Nie'}]",2024-07-23T05:34:41Z
http://arxiv.org/abs/2407.16190v2,http://arxiv.org/abs/2407.16190v2,Artificial Agency and Large Language Models,"The arrival of Large Language Models (LLMs) has stirred up philosophical
debates about the possibility of realizing agency in an artificial manner. In
this work we contribute to the debate by presenting a theoretical model that
can be used as a threshold conception for artificial agents. The model defines
agents as systems whose actions and goals are always influenced by a dynamic
framework of factors that consists of the agent's accessible history, its
adaptive repertoire and its external environment. This framework, in turn, is
influenced by the actions that the agent takes and the goals that it forms. We
show with the help of the model that state-of-the-art LLMs are not agents yet,
but that there are elements to them that suggest a way forward. The paper
argues that a combination of the agent architecture presented in Park et al.
(2023) together with the use of modules like the Coscientist in Boiko et al.
(2023) could potentially be a way to realize agency in an artificial manner. We
end the paper by reflecting on the obstacles one might face in building such an
artificial agent and by presenting possible directions for future research.","[{'name': 'Maud van Lier'}, {'name': 'Gorka Muñoz-Gil'}]",2024-07-23T05:32:00Z
http://arxiv.org/abs/2407.21051v1,http://arxiv.org/abs/2407.21051v1,"An Active Inference Strategy for Prompting Reliable Responses from Large
  Language Models in Medical Practice","Continuing advances in Large Language Models (LLMs) in artificial
intelligence offer important capacities in intuitively accessing and using
medical knowledge in many contexts, including education and training as well as
assessment and treatment. Most of the initial literature on LLMs in medicine
has emphasized that LLMs are unsuitable for medical use because they are
non-deterministic, may provide incorrect or harmful responses, and cannot be
regulated to assure quality control. If these issues could be corrected,
optimizing LLM technology could benefit patients and physicians by providing
affordable, point-of-care medical knowledge. Our proposed framework refines LLM
responses by restricting their primary knowledge base to domain-specific
datasets containing validated medical information. Additionally, we introduce
an actor-critic LLM prompting protocol based on active inference principles of
human cognition, where a Therapist agent initially responds to patient queries,
and a Supervisor agent evaluates and adjusts responses to ensure accuracy and
reliability. We conducted a validation study where expert cognitive behaviour
therapy for insomnia (CBT-I) therapists evaluated responses from the LLM in a
blind format. Experienced human CBT-I therapists assessed responses to 100
patient queries, comparing LLM-generated responses with appropriate and
inappropriate responses crafted by experienced CBT-I therapists. Results showed
that LLM responses received high ratings from the CBT-I therapists, often
exceeding those of therapist-generated appropriate responses. This structured
approach aims to integrate advanced LLM technology into medical applications,
meeting regulatory requirements for establishing the safe and effective use of
special purpose validated LLMs in medicine.","[{'name': 'Roma Shusterman'}, {'name': 'Allison C. Waters'}, {'name': 'Shannon O`Neill'}, {'name': 'Phan Luu'}, {'name': 'Don M. Tucker'}]",2024-07-23T05:00:18Z
http://arxiv.org/abs/2407.16181v1,http://arxiv.org/abs/2407.16181v1,"Structural Optimization Ambiguity and Simplicity Bias in Unsupervised
  Neural Grammar Induction","Neural parameterization has significantly advanced unsupervised grammar
induction. However, training these models with a traditional likelihood loss
for all possible parses exacerbates two issues: 1) $\textit{structural
optimization ambiguity}$ that arbitrarily selects one among structurally
ambiguous optimal grammars despite the specific preference of gold parses, and
2) $\textit{structural simplicity bias}$ that leads a model to underutilize
rules to compose parse trees. These challenges subject unsupervised neural
grammar induction (UNGI) to inevitable prediction errors, high variance, and
the necessity for extensive grammars to achieve accurate predictions. This
paper tackles these issues, offering a comprehensive analysis of their origins.
As a solution, we introduce $\textit{sentence-wise parse-focusing}$ to reduce
the parse pool per sentence for loss evaluation, using the structural bias from
pre-trained parsers on the same dataset. In unsupervised parsing benchmark
tests, our method significantly improves performance while effectively reducing
variance and bias toward overly simplistic parses. Our research promotes
learning more compact, accurate, and consistent explicit grammars, facilitating
better interpretability.","[{'name': 'Jinwook Park'}, {'name': 'Kangil Kim'}]",2024-07-23T04:57:03Z
http://arxiv.org/abs/2407.16168v1,http://arxiv.org/abs/2407.16168v1,Progressively Modality Freezing for Multi-Modal Entity Alignment,"Multi-Modal Entity Alignment aims to discover identical entities across
heterogeneous knowledge graphs. While recent studies have delved into fusion
paradigms to represent entities holistically, the elimination of features
irrelevant to alignment and modal inconsistencies is overlooked, which are
caused by inherent differences in multi-modal features. To address these
challenges, we propose a novel strategy of progressive modality freezing,
called PMF, that focuses on alignmentrelevant features and enhances multi-modal
feature fusion. Notably, our approach introduces a pioneering cross-modal
association loss to foster modal consistency. Empirical evaluations across nine
datasets confirm PMF's superiority, demonstrating stateof-the-art performance
and the rationale for freezing modalities. Our code is available at
https://github.com/ninibymilk/PMF-MMEA.","[{'name': 'Yani Huang'}, {'name': 'Xuefeng Zhang'}, {'name': 'Richong Zhang'}, {'name': 'Junfan Chen'}, {'name': 'Jaein Kim'}]",2024-07-23T04:22:30Z
http://arxiv.org/abs/2407.16166v1,http://arxiv.org/abs/2407.16166v1,"Robust Privacy Amidst Innovation with Large Language Models Through a
  Critical Assessment of the Risks","This study examines integrating EHRs and NLP with large language models
(LLMs) to improve healthcare data management and patient care. It focuses on
using advanced models to create secure, HIPAA-compliant synthetic patient notes
for biomedical research. The study used de-identified and re-identified MIMIC
III datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.
Text generation employed templates and keyword extraction for contextually
relevant notes, with one-shot generation for comparison. Privacy assessment
checked PHI occurrence, while text utility was tested using an ICD-9 coding
task. Text quality was evaluated with ROUGE and cosine similarity metrics to
measure semantic similarity with source notes. Analysis of PHI occurrence and
text utility via the ICD-9 coding task showed that the keyword-based method had
low risk and good performance. One-shot generation showed the highest PHI
exposure and PHI co-occurrence, especially in geographic location and date
categories. The Normalized One-shot method achieved the highest classification
accuracy. Privacy analysis revealed a critical balance between data utility and
privacy protection, influencing future data use and sharing. Re-identified data
consistently outperformed de-identified data. This study demonstrates the
effectiveness of keyword-based methods in generating privacy-protecting
synthetic clinical notes that retain data usability, potentially transforming
clinical data-sharing practices. The superior performance of re-identified over
de-identified data suggests a shift towards methods that enhance utility and
privacy by using dummy PHIs to perplex privacy attacks.","[{'name': 'Yao-Shun Chuang'}, {'name': 'Atiquer Rahman Sarkar'}, {'name': 'Noman Mohammed'}, {'name': 'Xiaoqian Jiang'}]",2024-07-23T04:20:14Z
http://arxiv.org/abs/2407.21050v2,http://arxiv.org/abs/2407.21050v2,"Artificial Intelligence in Extracting Diagnostic Data from Dental
  Records","This research addresses the issue of missing structured data in dental
records by extracting diagnostic information from unstructured text. The
updated periodontology classification system's complexity has increased
incomplete or missing structured diagnoses. To tackle this, we use advanced AI
and NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a
RoBERTa model. This significantly enhances the model's ability to understand
medical and dental language. We evaluated the model using 120 randomly selected
clinical notes from two datasets, demonstrating its improved diagnostic
extraction accuracy. The results showed high accuracy in diagnosing periodontal
status, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In
the subtype category, Site 2 achieved perfect scores, outperforming Site 1.
This method enhances extraction accuracy and broadens its use across dental
contexts. The study underscores AI and NLP's transformative impact on
healthcare delivery and management. Integrating AI and NLP technologies
enhances documentation and simplifies administrative tasks by precisely
extracting complex clinical information. This approach effectively addresses
challenges in dental diagnostics. Using synthetic training data from LLMs
optimizes the training process, improving accuracy and efficiency in
identifying periodontal diagnoses from clinical notes. This innovative method
holds promise for broader healthcare applications, potentially improving
patient care quality.","[{'name': 'Yao-Shun Chuang'}, {'name': 'Chun-Teh Lee'}, {'name': 'Oluwabunmi Tokede'}, {'name': 'Guo-Hao Lin'}, {'name': 'Ryan Brandon'}, {'name': 'Trung Duong Tran'}, {'name': 'Xiaoqian Jiang'}, {'name': 'Muhammad F. Walji'}]",2024-07-23T04:05:48Z
http://arxiv.org/abs/2407.16160v1,http://arxiv.org/abs/2407.16160v1,"UniMEL: A Unified Framework for Multimodal Entity Linking with Large
  Language Models","Multimodal Entity Linking (MEL) is a crucial task that aims at linking
ambiguous mentions within multimodal contexts to the referent entities in a
multimodal knowledge base, such as Wikipedia. Existing methods focus heavily on
using complex mechanisms and extensive model tuning methods to model the
multimodal interaction on specific datasets. However, these methods
overcomplicate the MEL task and overlook the visual semantic information, which
makes them costly and hard to scale. Moreover, these methods can not solve the
issues like textual ambiguity, redundancy, and noisy images, which severely
degrade their performance. Fortunately, the advent of Large Language Models
(LLMs) with robust capabilities in text understanding and reasoning,
particularly Multimodal Large Language Models (MLLMs) that can process
multimodal inputs, provides new insights into addressing this challenge.
However, how to design a universally applicable LLMs-based MEL approach remains
a pressing challenge. To this end, we propose UniMEL, a unified framework which
establishes a new paradigm to process multimodal entity linking tasks using
LLMs. In this framework, we employ LLMs to augment the representation of
mentions and entities individually by integrating textual and visual
information and refining textual information. Subsequently, we employ the
embedding-based method for retrieving and re-ranking candidate entities. Then,
with only ~0.26% of the model parameters fine-tuned, LLMs can make the final
selection from the candidate entities. Extensive experiments on three public
benchmark datasets demonstrate that our solution achieves state-of-the-art
performance, and ablation studies verify the effectiveness of all modules. Our
code is available at https://anonymous.4open.science/r/UniMEL/.","[{'name': 'Liu Qi'}, {'name': 'He Yongyi'}, {'name': 'Lian Defu'}, {'name': 'Zheng Zhi'}, {'name': 'Xu Tong'}, {'name': 'Liu Che'}, {'name': 'Chen Enhong'}]",2024-07-23T03:58:08Z
http://arxiv.org/abs/2407.16154v1,http://arxiv.org/abs/2407.16154v1,DDK: Distilling Domain Knowledge for Efficient Large Language Models,"Despite the advanced intelligence abilities of large language models (LLMs)
in various applications, they still face significant computational and storage
demands. Knowledge Distillation (KD) has emerged as an effective strategy to
improve the performance of a smaller LLM (i.e., the student model) by
transferring knowledge from a high-performing LLM (i.e., the teacher model).
Prevailing techniques in LLM distillation typically use a black-box model API
to generate high-quality pretrained and aligned datasets, or utilize white-box
distillation by altering the loss function to better transfer knowledge from
the teacher LLM. However, these methods ignore the knowledge differences
between the student and teacher LLMs across domains. This results in excessive
focus on domains with minimal performance gaps and insufficient attention to
domains with large gaps, reducing overall performance. In this paper, we
introduce a new LLM distillation framework called DDK, which dynamically
adjusts the composition of the distillation dataset in a smooth manner
according to the domain performance differences between the teacher and student
models, making the distillation process more stable and effective. Extensive
evaluations show that DDK significantly improves the performance of student
models, outperforming both continuously pretrained baselines and existing
knowledge distillation methods by a large margin.","[{'name': 'Jiaheng Liu'}, {'name': 'Chenchen Zhang'}, {'name': 'Jinyang Guo'}, {'name': 'Yuanxing Zhang'}, {'name': 'Haoran Que'}, {'name': 'Ken Deng'}, {'name': 'Zhiqi Bai'}, {'name': 'Jie Liu'}, {'name': 'Ge Zhang'}, {'name': 'Jiakai Wang'}, {'name': 'Yanan Wu'}, {'name': 'Congnan Liu'}, {'name': 'Wenbo Su'}, {'name': 'Jiamang Wang'}, {'name': 'Lin Qu'}, {'name': 'Bo Zheng'}]",2024-07-23T03:47:28Z
http://arxiv.org/abs/2407.16148v1,http://arxiv.org/abs/2407.16148v1,"CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for
  Literature Review Support","Literature review requires researchers to synthesize a large amount of
information and is increasingly challenging as the scientific literature
expands. In this work, we investigate the potential of LLMs for producing
hierarchical organizations of scientific studies to assist researchers with
literature review. We define hierarchical organizations as tree structures
where nodes refer to topical categories and every node is linked to the studies
assigned to that category. Our naive LLM-based pipeline for hierarchy
generation from a set of studies produces promising yet imperfect hierarchies,
motivating us to collect CHIME, an expert-curated dataset for this task focused
on biomedicine. Given the challenging and time-consuming nature of building
hierarchies from scratch, we use a human-in-the-loop process in which experts
correct errors (both links between categories and study assignment) in
LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies
covering 472 topics, and expert-corrected hierarchies for a subset of 100
topics. Expert corrections allow us to quantify LLM performance, and we find
that while they are quite good at generating and organizing categories, their
assignment of studies to categories could be improved. We attempt to train a
corrector model with human feedback which improves study assignment by 12.6 F1
points. We release our dataset and models to encourage research on developing
better assistive tools for literature review.","[{'name': 'Chao-Chun Hsu'}, {'name': 'Erin Bransom'}, {'name': 'Jenna Sparks'}, {'name': 'Bailey Kuehl'}, {'name': 'Chenhao Tan'}, {'name': 'David Wadden'}, {'name': 'Lucy Lu Wang'}, {'name': 'Aakanksha Naik'}]",2024-07-23T03:18:00Z
http://arxiv.org/abs/2407.21049v1,http://arxiv.org/abs/2407.21049v1,"Evaluating Long Range Dependency Handling in Code Generation Models
  using Multi-Step Key Retrieval","As language models support larger and larger context sizes, evaluating their
ability to make effective use of that context becomes increasingly important.
We analyze the ability of several code generation models to handle long range
dependencies using a suite of multi-step key retrieval tasks in context windows
up to 8k tokens in length. The tasks progressively increase in difficulty and
allow more nuanced evaluation of model capabilities than tests like the popular
needle-in-the-haystack test. We find that performance degrades significantly
(up to 2x) when a function references another function that is defined later in
the prompt. We also observe that models that use sliding window attention
mechanisms have difficulty handling references further than the size of a
single window. We perform simple prompt modifications using call graph
information to improve multi-step retrieval performance up to 3x. Our analysis
highlights different facets of long-context performance and is suggestive of
prompt construction strategies for code completion tools","[{'name': 'Yannick Assogba'}, {'name': 'Donghao Ren'}]",2024-07-23T02:45:22Z
http://arxiv.org/abs/2407.16127v1,http://arxiv.org/abs/2407.16127v1,"Finetuning Generative Large Language Models with Discrimination
  Instructions for Knowledge Graph Completion","Traditional knowledge graph (KG) completion models learn embeddings to
predict missing facts. Recent works attempt to complete KGs in a
text-generation manner with large language models (LLMs). However, they need to
ground the output of LLMs to KG entities, which inevitably brings errors. In
this paper, we present a finetuning framework, DIFT, aiming to unleash the KG
completion ability of LLMs and avoid grounding errors. Given an incomplete
fact, DIFT employs a lightweight model to obtain candidate entities and
finetunes an LLM with discrimination instructions to select the correct one
from the given candidates. To improve performance while reducing instruction
data, DIFT uses a truncated sampling method to select useful facts for
finetuning and injects KG embeddings into the LLM. Extensive experiments on
benchmark datasets demonstrate the effectiveness of our proposed framework.","[{'name': 'Yang Liu'}, {'name': 'Xiaobin Tian'}, {'name': 'Zequn Sun'}, {'name': 'Wei Hu'}]",2024-07-23T02:25:01Z
http://arxiv.org/abs/2407.21048v1,http://arxiv.org/abs/2407.21048v1,"APTNESS: Incorporating Appraisal Theory and Emotion Support Strategies
  for Empathetic Response Generation","Empathetic response generation is designed to comprehend the emotions of
others and select the most appropriate strategies to assist them in resolving
emotional challenges. Empathy can be categorized into cognitive empathy and
affective empathy. The former pertains to the ability to understand and discern
the emotional issues and situations of others, while the latter involves the
capacity to provide comfort. To enhance one's empathetic abilities, it is
essential to develop both these aspects. Therefore, we develop an innovative
framework that combines retrieval augmentation and emotional support strategy
integration. Our framework starts with the introduction of a comprehensive
emotional palette for empathy. We then apply appraisal theory to decompose this
palette and create a database of empathetic responses. This database serves as
an external resource and enhances the LLM's empathy by integrating semantic
retrieval mechanisms. Moreover, our framework places a strong emphasis on the
proper articulation of response strategies. By incorporating emotional support
strategies, we aim to enrich the model's capabilities in both cognitive and
affective empathy, leading to a more nuanced and comprehensive empathetic
response. Finally, we extract datasets ED and ET from the empathetic dialogue
dataset \textsc{EmpatheticDialogues} and ExTES based on dialogue length.
Experiments demonstrate that our framework can enhance the empathy ability of
LLMs from both cognitive and affective empathy perspectives. Our code is
released at https://github.com/CAS-SIAT-XinHai/APTNESS.","[{'name': 'Yuxuan Hu'}, {'name': 'Minghuan Tan'}, {'name': 'Chenwei Zhang'}, {'name': 'Zixuan Li'}, {'name': 'Xiaodan Liang'}, {'name': 'Min Yang'}, {'name': 'Chengming Li'}, {'name': 'Xiping Hu'}]",2024-07-23T02:23:37Z
http://arxiv.org/abs/2407.16047v1,http://arxiv.org/abs/2407.16047v1,"Leveraging Large Language Models to Geolocate Linguistic Variations in
  Social Media Posts","Geolocalization of social media content is the task of determining the
geographical location of a user based on textual data, that may show linguistic
variations and informal language. In this project, we address the GeoLingIt
challenge of geolocalizing tweets written in Italian by leveraging large
language models (LLMs). GeoLingIt requires the prediction of both the region
and the precise coordinates of the tweet. Our approach involves fine-tuning
pre-trained LLMs to simultaneously predict these geolocalization aspects. By
integrating innovative methodologies, we enhance the models' ability to
understand the nuances of Italian social media text to improve the
state-of-the-art in this domain. This work is conducted as part of the Large
Language Models course at the Bertinoro International Spring School 2024. We
make our code publicly available on GitHub
https://github.com/dawoz/geolingit-biss2024.","[{'name': 'Davide Savarro'}, {'name': 'Davide Zago'}, {'name': 'Stefano Zoia'}]",2024-07-22T20:54:35Z
http://arxiv.org/abs/2408.00802v1,http://arxiv.org/abs/2408.00802v1,Leveraging LLM Reasoning Enhances Personalized Recommender Systems,"Recent advancements have showcased the potential of Large Language Models
(LLMs) in executing reasoning tasks, particularly facilitated by
Chain-of-Thought (CoT) prompting. While tasks like arithmetic reasoning involve
clear, definitive answers and logical chains of thought, the application of LLM
reasoning in recommendation systems (RecSys) presents a distinct challenge.
RecSys tasks revolve around subjectivity and personalized preferences, an
under-explored domain in utilizing LLMs' reasoning capabilities. Our study
explores several aspects to better understand reasoning for RecSys and
demonstrate how task quality improves by utilizing LLM reasoning in both
zero-shot and finetuning settings. Additionally, we propose RecSAVER
(Recommender Systems Automatic Verification and Evaluation of Reasoning) to
automatically assess the quality of LLM reasoning responses without the
requirement of curated gold references or human raters. We show that our
framework aligns with real human judgment on the coherence and faithfulness of
reasoning responses. Overall, our work shows that incorporating reasoning into
RecSys can improve personalized tasks, paving the way for further advancements
in recommender system methodologies.","[{'name': 'Alicia Y. Tsai'}, {'name': 'Adam Kraft'}, {'name': 'Long Jin'}, {'name': 'Chenwei Cai'}, {'name': 'Anahita Hosseini'}, {'name': 'Taibai Xu'}, {'name': 'Zemin Zhang'}, {'name': 'Lichan Hong'}, {'name': 'Ed H. Chi'}, {'name': 'Xinyang Yi'}]",2024-07-22T20:18:50Z
http://arxiv.org/abs/2407.16030v1,http://arxiv.org/abs/2407.16030v1,Enhancing Temporal Understanding in LLMs for Semi-structured Tables,"Temporal reasoning over tabular data presents substantial challenges for
large language models (LLMs), as evidenced by recent research. In this study,
we conduct a comprehensive analysis of temporal datasets to pinpoint the
specific limitations of LLMs. Our investigation leads to enhancements in
TempTabQA, a dataset specifically designed for tabular temporal question
answering. We provide critical insights for improving LLM performance in
temporal reasoning tasks with tabular data. Furthermore, we introduce a novel
approach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings
demonstrate that our method significantly improves evidence-based reasoning
across various models. Additionally, our experimental results reveal that
indirect supervision with auxiliary data substantially boosts model performance
in these tasks. This work contributes to a deeper understanding of LLMs'
temporal reasoning abilities over tabular data and promotes advancements in
their application across diverse fields.","[{'name': 'Irwin Deng'}, {'name': 'Kushagra Dixit'}, {'name': 'Vivek Gupta'}, {'name': 'Dan Roth'}]",2024-07-22T20:13:10Z
http://arxiv.org/abs/2407.16008v1,http://arxiv.org/abs/2407.16008v1,"Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic
  Data Generation","Reward models (RMs) are crucial for aligning large language models (LLMs)
with human preferences. They are trained using preference datasets where each
example consists of one input prompt, two responses, and a preference label. As
curating a high-quality human labeled preference dataset is both time-consuming
and expensive, people often rely on existing powerful LLMs for preference label
generation. This can potentially introduce noise and impede RM training. In
this work, we present RMBoost, a novel synthetic preference data generation
paradigm to boost reward model quality. Unlike traditional methods, which
generate two responses before obtaining the preference label, RMBoost first
generates one response and selects a preference label, followed by generating
the second more (or less) preferred response conditioned on the pre-selected
preference label and the first response. This approach offers two main
advantages. First, RMBoost reduces labeling noise since preference pairs are
constructed intentionally. Second, RMBoost facilitates the creation of more
diverse responses by incorporating various quality aspects (e.g., helpfulness,
relevance, completeness) into the prompts. We conduct extensive experiments
across three diverse datasets and demonstrate that RMBoost outperforms other
synthetic preference data generation techniques and significantly boosts the
performance of four distinct reward models.","[{'name': 'Jiaming Shen'}, {'name': 'Ran Xu'}, {'name': 'Yennie Jun'}, {'name': 'Zhen Qin'}, {'name': 'Tianqi Liu'}, {'name': 'Carl Yang'}, {'name': 'Yi Liang'}, {'name': 'Simon Baumgartner'}, {'name': 'Michael Bendersky'}]",2024-07-22T19:21:55Z
http://arxiv.org/abs/2407.16007v1,http://arxiv.org/abs/2407.16007v1,"SocialQuotes: Learning Contextual Roles of Social Media Quotes on the
  Web","Web authors frequently embed social media to support and enrich their
content, creating the potential to derive web-based, cross-platform social
media representations that can enable more effective social media retrieval
systems and richer scientific analyses. As step toward such capabilities, we
introduce a novel language modeling framework that enables automatic annotation
of roles that social media entities play in their embedded web context. Using
related communication theory, we liken social media embeddings to quotes,
formalize the page context as structured natural language signals, and identify
a taxonomy of roles for quotes within the page context. We release
SocialQuotes, a new data set built from the Common Crawl of over 32 million
social quotes, 8.3k of them with crowdsourced quote annotations. Using
SocialQuotes and the accompanying annotations, we provide a role classification
case study, showing reasonable performance with modern-day LLMs, and exposing
explainable aspects of our framework via page content ablations. We also
classify a large batch of un-annotated quotes, revealing interesting
cross-domain, cross-platform role distributions on the web.","[{'name': 'John Palowitch'}, {'name': 'Hamidreza Alvari'}, {'name': 'Mehran Kazemi'}, {'name': 'Tanvir Amin'}, {'name': 'Filip Radlinski'}]",2024-07-22T19:21:01Z
http://arxiv.org/abs/2407.15992v1,http://arxiv.org/abs/2407.15992v1,Multimodal Input Aids a Bayesian Model of Phonetic Learning,"One of the many tasks facing the typically-developing child language learner
is learning to discriminate between the distinctive sounds that make up words
in their native language. Here we investigate whether multimodal
information--specifically adult speech coupled with video frames of speakers'
faces--benefits a computational model of phonetic learning. We introduce a
method for creating high-quality synthetic videos of speakers' faces for an
existing audio corpus. Our learning model, when both trained and tested on
audiovisual inputs, achieves up to a 8.1% relative improvement on a phoneme
discrimination battery compared to a model trained and tested on audio-only
input. It also outperforms the audio model by up to 3.9% when both are tested
on audio-only data, suggesting that visual information facilitates the
acquisition of acoustic distinctions. Visual information is especially
beneficial in noisy audio environments, where an audiovisual model closes 67%
of the loss in discrimination performance of the audio model in noise relative
to a non-noisy environment. These results demonstrate that visual information
benefits an ideal learner and illustrate some of the ways that children might
be able to leverage visual cues when learning to discriminate speech sounds.","[{'name': 'Sophia Zhi'}, {'name': 'Roger P. Levy'}, {'name': 'Stephan C. Meylan'}]",2024-07-22T19:00:11Z
http://arxiv.org/abs/2407.15975v1,http://arxiv.org/abs/2407.15975v1,Multilingual Fine-Grained News Headline Hallucination Detection,"The popularity of automated news headline generation has surged with
advancements in pre-trained language models. However, these models often suffer
from the ``hallucination'' problem, where the generated headline is not fully
supported by its source article. Efforts to address this issue have
predominantly focused on English, using over-simplistic classification schemes
that overlook nuanced hallucination types. In this study, we introduce the
first multilingual, fine-grained news headline hallucination detection dataset
that contains over 11 thousand pairs in 5 languages, each annotated with
detailed hallucination types by experts. We conduct extensive experiments on
this dataset under two settings. First, we implement several supervised
fine-tuning approaches as preparatory solutions and demonstrate this dataset's
challenges and utilities. Second, we test various large language models'
in-context learning abilities and propose two novel techniques,
language-dependent demonstration selection and coarse-to-fine prompting, to
boost the few-shot hallucination detection performance in terms of the
example-F1 metric. We release this dataset to foster further research in
multilingual, fine-grained headline hallucination detection.","[{'name': 'Jiaming Shen'}, {'name': 'Tianqi Liu'}, {'name': 'Jialu Liu'}, {'name': 'Zhen Qin'}, {'name': 'Jay Pavagadhi'}, {'name': 'Simon Baumgartner'}, {'name': 'Michael Bendersky'}]",2024-07-22T18:37:53Z
http://arxiv.org/abs/2407.21046v1,http://arxiv.org/abs/2407.21046v1,"Promises and Pitfalls of Generative Masked Language Modeling:
  Theoretical Framework and Practical Guidelines","Autoregressive language models are the currently dominant paradigm for text
generation, but they have some fundamental limitations that cannot be remedied
by scale-for example inherently sequential and unidirectional generation. While
alternate classes of models have been explored, we have limited mathematical
understanding of their fundamental power and limitations. In this paper we
focus on Generative Masked Language Models (GMLMs), a non-autoregressive
paradigm in which we train a model to fit conditional probabilities of the data
distribution via masking, which are subsequently used as inputs to a Markov
Chain to draw samples from the model, These models empirically strike a
promising speed-quality trade-off as each step can be typically parallelized by
decoding the entire sequence in parallel. We develop a mathematical framework
for analyzing and improving such models which sheds light on questions of
sample complexity and inference speed and quality. Empirically, we adapt the T5
model for iteratively-refined parallel decoding, achieving 2-3x speedup in
machine translation with minimal sacrifice in quality compared with
autoregressive models. We run careful ablation experiments to give
recommendations on key design choices, and make fine-grained observations on
the common error modes in connection with our theory. Our mathematical analyses
and empirical observations characterize both potentials and limitations of this
approach, and can be applied to future works on improving understanding and
performance of GMLMs. Our codes are released at
https://github.com/google-research/google-research/tree/master/padir","[{'name': 'Yuchen Li'}, {'name': 'Alexandre Kirchmeyer'}, {'name': 'Aashay Mehta'}, {'name': 'Yilong Qin'}, {'name': 'Boris Dadachev'}, {'name': 'Kishore Papineni'}, {'name': 'Sanjiv Kumar'}, {'name': 'Andrej Risteski'}]",2024-07-22T18:00:00Z
http://arxiv.org/abs/2407.16711v2,http://arxiv.org/abs/2407.16711v2,Benchmarks as Microscopes: A Call for Model Metrology,"Modern language models (LMs) pose a new challenge in capability assessment.
Static benchmarks inevitably saturate without providing confidence in the
deployment tolerances of LM-based systems, but developers nonetheless claim
that their models have generalized traits such as reasoning or open-domain
language understanding based on these flawed metrics. The science and practice
of LMs requires a new approach to benchmarking which measures specific
capabilities with dynamic assessments. To be confident in our metrics, we need
a new discipline of model metrology -- one which focuses on how to generate
benchmarks that predict performance under deployment. Motivated by our
evaluation criteria, we outline how building a community of model metrology
practitioners -- one focused on building tools and studying how to measure
system capabilities -- is the best way to meet these needs to and add clarity
to the AI discussion.","[{'name': 'Michael Saxon'}, {'name': 'Ari Holtzman'}, {'name': 'Peter West'}, {'name': 'William Yang Wang'}, {'name': 'Naomi Saphra'}]",2024-07-22T17:52:12Z
http://arxiv.org/abs/2407.15835v1,http://arxiv.org/abs/2407.15835v1,dMel: Speech Tokenization made Simple,"Large language models have revolutionized natural language processing by
leveraging self-supervised pretraining on vast textual data. Inspired by this
success, researchers have investigated complicated speech tokenization methods
to discretize continuous speech signals so that language modeling techniques
can be applied to speech data. However, existing approaches either model
semantic tokens, potentially losing acoustic information, or model acoustic
tokens, risking the loss of semantic information. Having multiple token types
also complicates the architecture and requires additional pretraining. Here we
show that discretizing mel-filterbank channels into discrete intensity bins
produces a simple representation (dMel), that performs better than other
existing speech tokenization methods. Using a transformer decoder-only
architecture for speech-text modeling, we comprehensively evaluate different
speech tokenization methods on speech recognition (ASR), speech synthesis
(TTS). Our results demonstrate the effectiveness of dMel in achieving high
performance on both tasks within a unified framework, paving the way for
efficient and effective joint modeling of speech and text.","[{'name': 'He Bai'}, {'name': 'Tatiana Likhomanenko'}, {'name': 'Ruixiang Zhang'}, {'name': 'Zijin Gu'}, {'name': 'Zakaria Aldeneh'}, {'name': 'Navdeep Jaitly'}]",2024-07-22T17:51:53Z
http://arxiv.org/abs/2407.15828v1,http://arxiv.org/abs/2407.15828v1,"J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue
  Language Modeling","Spoken dialogue plays a crucial role in human-AI interactions, necessitating
dialogue-oriented spoken language models (SLMs). To develop versatile SLMs,
large-scale and diverse speech datasets are essential. Additionally, to ensure
hiqh-quality speech generation, the data must be spontaneous like in-wild data
and must be acoustically clean with noise removed. Despite the critical need,
no open-source corpus meeting all these criteria has been available. This study
addresses this gap by constructing and releasing a large-scale spoken dialogue
corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly
accessible. Furthermore, this paper presents a language-independent method for
corpus construction and describes experiments on dialogue generation using SLMs
trained on J-CHAT. Experimental results indicate that the collected data from
multiple domains by our method improve the naturalness and meaningfulness of
dialogue generation.","[{'name': 'Wataru Nakata'}, {'name': 'Kentaro Seki'}, {'name': 'Hitomi Yanaka'}, {'name': 'Yuki Saito'}, {'name': 'Shinnosuke Takamichi'}, {'name': 'Hiroshi Saruwatari'}]",2024-07-22T17:46:50Z
http://arxiv.org/abs/2407.15814v1,http://arxiv.org/abs/2407.15814v1,Perceptions of Linguistic Uncertainty by Language Models and Humans,"Uncertainty expressions such as ``probably'' or ``highly unlikely'' are
pervasive in human language. While prior work has established that there is
population-level agreement in terms of how humans interpret these expressions,
there has been little inquiry into the abilities of language models to
interpret such expressions. In this paper, we investigate how language models
map linguistic expressions of uncertainty to numerical responses. Our approach
assesses whether language models can employ theory of mind in this setting:
understanding the uncertainty of another agent about a particular statement,
independently of the model's own certainty about that statement. We evaluate
both humans and 10 popular language models on a task created to assess these
abilities. Unexpectedly, we find that 8 out of 10 models are able to map
uncertainty expressions to probabilistic responses in a human-like manner.
However, we observe systematically different behavior depending on whether a
statement is actually true or false. This sensitivity indicates that language
models are substantially more susceptible to bias based on their prior
knowledge (as compared to humans). These findings raise important questions and
have broad implications for human-AI alignment and AI-AI communication.","[{'name': 'Catarina G Belem'}, {'name': 'Markelle Kelly'}, {'name': 'Mark Steyvers'}, {'name': 'Sameer Singh'}, {'name': 'Padhraic Smyth'}]",2024-07-22T17:26:12Z
http://arxiv.org/abs/2407.15762v1,http://arxiv.org/abs/2407.15762v1,"Conditioned Language Policy: A General Framework for Steerable
  Multi-Objective Finetuning","Reward-based finetuning is crucial for aligning language policies with
intended behaviors (e.g., creativity and safety). A key challenge here is to
develop steerable language models that trade-off multiple (conflicting)
objectives in a flexible and efficient manner. This paper presents Conditioned
Language Policy (CLP), a general framework for finetuning language models on
multiple objectives. Building on techniques from multi-task training and
parameter-efficient finetuning, CLP can learn steerable models that effectively
trade-off conflicting objectives at inference time. Notably, this does not
require training or maintaining multiple models to achieve different trade-offs
between the objectives. Through an extensive set of experiments and ablations,
we show that the CLP framework learns steerable models that outperform and
Pareto-dominate the current state-of-the-art approaches for multi-objective
finetuning.","[{'name': 'Kaiwen Wang'}, {'name': 'Rahul Kidambi'}, {'name': 'Ryan Sullivan'}, {'name': 'Alekh Agarwal'}, {'name': 'Christoph Dann'}, {'name': 'Andrea Michi'}, {'name': 'Marco Gelmi'}, {'name': 'Yunxuan Li'}, {'name': 'Raghav Gupta'}, {'name': 'Avinava Dubey'}, {'name': 'Alexandre Ramé'}, {'name': 'Johan Ferret'}, {'name': 'Geoffrey Cideron'}, {'name': 'Le Hou'}, {'name': 'Hongkun Yu'}, {'name': 'Amr Ahmed'}, {'name': 'Aranyak Mehta'}, {'name': 'Léonard Hussenot'}, {'name': 'Olivier Bachem'}, {'name': 'Edouard Leurent'}]",2024-07-22T16:13:38Z
http://arxiv.org/abs/2407.15754v1,http://arxiv.org/abs/2407.15754v1,"LongVideoBench: A Benchmark for Long-context Interleaved Video-Language
  Understanding","Large multimodal models (LMMs) are processing increasingly longer and richer
inputs. Albeit the progress, few public benchmark is available to measure such
development. To mitigate this gap, we introduce LongVideoBench, a
question-answering benchmark that features video-language interleaved inputs up
to an hour long. Our benchmark includes 3,763 varying-length web-collected
videos with their subtitles across diverse themes, designed to comprehensively
evaluate LMMs on long-term multimodal understanding. To achieve this, we
interpret the primary challenge as to accurately retrieve and reason over
detailed multimodal information from long inputs. As such, we formulate a novel
video question-answering task termed referring reasoning. Specifically, as part
of the question, it contains a referring query that references related video
contexts, called referred context. The model is then required to reason over
relevant video details from the referred context. Following the paradigm of
referring reasoning, we curate 6,678 human-annotated multiple-choice questions
in 17 fine-grained categories, establishing one of the most comprehensive
benchmarks for long-form video understanding. Evaluations suggest that the
LongVideoBench presents significant challenges even for the most advanced
proprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their
open-source counterparts show an even larger performance gap. In addition, our
results indicate that model performance on the benchmark improves only when
they are capable of processing more frames, positioning LongVideoBench as a
valuable benchmark for evaluating future-generation long-context LMMs.","[{'name': 'Haoning Wu'}, {'name': 'Dongxu Li'}, {'name': 'Bei Chen'}, {'name': 'Junnan Li'}]",2024-07-22T16:00:55Z
http://arxiv.org/abs/2407.15736v1,http://arxiv.org/abs/2407.15736v1,"OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a
  German Migration Context","When immigrating to a new country, it is easy to feel overwhelmed by the need
to obtain information on financial support, housing, schooling, language
courses, and other issues. If relocation is rushed or even forced, the
necessity for high-quality answers to such questions is all the more urgent.
Official immigration counselors are usually overbooked, and online systems
could guide newcomers to the requested information or a suitable counseling
service.
  To this end, we present OMoS-QA, a dataset of German and English questions
paired with relevant trustworthy documents and manually annotated answers,
specifically tailored to this scenario. Questions are automatically generated
with an open-source large language model (LLM) and answer sentences are
selected by crowd workers with high agreement. With our data, we conduct a
comparison of 5 pretrained LLMs on the task of extractive question answering
(QA) in German and English. Across all models and both languages, we find high
precision and low-to-mid recall in selecting answer sentences, which is a
favorable trade-off to avoid misleading users. This performance even holds up
when the question language does not match the document language. When it comes
to identifying unanswerable questions given a context, there are larger
differences between the two languages.","[{'name': 'Steffen Kleinle'}, {'name': 'Jakob Prange'}, {'name': 'Annemarie Friedrich'}]",2024-07-22T15:40:17Z
http://arxiv.org/abs/2407.15723v1,http://arxiv.org/abs/2407.15723v1,"DStruct2Design: Data and Benchmarks for Data Structure Driven Generative
  Floor Plan Design","Text conditioned generative models for images have yielded impressive
results. Text conditioned floorplan generation as a special type of raster
image generation task also received particular attention. However there are
many use cases in floorpla generation where numerical properties of the
generated result are more important than the aesthetics. For instance, one
might want to specify sizes for certain rooms in a floorplan and compare the
generated floorplan with given specifications Current approaches, datasets and
commonly used evaluations do not support these kinds of constraints. As such,
an attractive strategy is to generate an intermediate data structure that
contains numerical properties of a floorplan which can be used to generate the
final floorplan image. To explore this setting we (1) construct a new dataset
for this data-structure to data-structure formulation of floorplan generation
using two popular image based floorplan datasets RPLAN and ProcTHOR-10k, and
provide the tools to convert further procedurally generated ProcTHOR floorplan
data into our format. (2) We explore the task of floorplan generation given a
partial or complete set of constraints and we design a series of metrics and
benchmarks to enable evaluating how well samples generated from models respect
the constraints. (3) We create multiple baselines by finetuning a large
language model (LLM), Llama3, and demonstrate the feasibility of using
floorplan data structure conditioned LLMs for the problem of floorplan
generation respecting numerical constraints. We hope that our new datasets and
benchmarks will encourage further research on different ways to improve the
performance of LLMs and other generative modelling techniques for generating
designs where quantitative constraints are only partially specified, but must
be respected.","[{'name': 'Zhi Hao Luo'}, {'name': 'Luis Lara'}, {'name': 'Ge Ya Luo'}, {'name': 'Florian Golemo'}, {'name': 'Christopher Beckham'}, {'name': 'Christopher Pal'}]",2024-07-22T15:27:55Z
http://arxiv.org/abs/2407.15720v2,http://arxiv.org/abs/2407.15720v2,"Do Large Language Models Have Compositional Ability? An Investigation
  into Limitations and Scalability","Large language models (LLMs) have emerged as powerful tools for many AI
problems and exhibit remarkable in-context learning (ICL) capabilities.
Compositional ability, solving unseen complex tasks that combine two or more
simple tasks, is an essential reasoning ability for Artificial General
Intelligence. Despite the tremendous success of LLMs, how they approach
composite tasks, especially those not encountered during the pretraining phase,
remains an open and largely underexplored question. In this study, we delve
into the ICL capabilities of LLMs on composite tasks, with only simple tasks as
in-context examples. We develop a test suite of composite tasks including
linguistic and logical challenges and perform empirical studies across
different LLM families. We observe that models exhibit divergent behaviors: (1)
For simpler composite tasks that apply distinct mapping mechanisms to different
input segments, the models demonstrate decent compositional ability, while
scaling up the model enhances this ability; (2) for more complex composite
tasks involving reasoning multiple steps, where each step represents one task,
models typically underperform, and scaling up generally provides no
improvements. We offer theoretical analysis in a simplified setting, explaining
that models exhibit compositional capability when the task handles different
input parts separately. We believe our work sheds new light on the capabilities
of LLMs in solving composite tasks regarding the nature of the tasks and model
scale. Our dataset and code are available at
{\url{https://github.com/OliverXUZY/LLM_Compose}}.","[{'name': 'Zhuoyan Xu'}, {'name': 'Zhenmei Shi'}, {'name': 'Yingyu Liang'}]",2024-07-22T15:22:34Z
http://arxiv.org/abs/2407.15711v1,http://arxiv.org/abs/2407.15711v1,AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?,"Language agents, built on top of language models (LMs), are systems that can
interact with complex environments, such as the open web. In this work, we
examine whether such agents can perform realistic and time-consuming tasks on
the web, e.g., monitoring real-estate markets or locating relevant nearby
businesses. We introduce AssistantBench, a challenging new benchmark consisting
of 214 realistic tasks that can be automatically evaluated, covering different
scenarios and domains. We find that AssistantBench exposes the limitations of
current systems, including language models and retrieval-augmented language
models, as no model reaches an accuracy of more than 25 points. While
closed-book LMs perform well, they exhibit low precision since they tend to
hallucinate facts. State-of-the-art web agents reach a score of near zero.
Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly
outperforms previous agents, and an ensemble of SPA and closed-book models
reaches the best overall performance. Moreover, we analyze failures of current
systems and highlight that web navigation remains a major challenge.","[{'name': 'Ori Yoran'}, {'name': 'Samuel Joseph Amouyal'}, {'name': 'Chaitanya Malaviya'}, {'name': 'Ben Bogin'}, {'name': 'Ofir Press'}, {'name': 'Jonathan Berant'}]",2024-07-22T15:18:45Z
http://arxiv.org/abs/2407.15695v1,http://arxiv.org/abs/2407.15695v1,Supporting the Digital Autonomy of Elders Through LLM Assistance,"The internet offers tremendous access to services, social connections, and
needed products. However, to those without sufficient experience, engaging with
businesses and friends across the internet can be daunting due to the ever
present danger of scammers and thieves, to say nothing of the myriad of
potential computer viruses. Like a forest rich with both edible and poisonous
plants, those familiar with the norms inhabit it safely with ease while
newcomers need a guide. However, reliance on a human digital guide can be
taxing and often impractical. We propose and pilot a simple but unexplored
idea: could an LLM provide the necessary support to help the elderly who are
separated by the digital divide safely achieve digital autonomy?","[{'name': 'Jesse Roberts'}, {'name': 'Lindsey Roberts'}, {'name': 'Alice Reed'}]",2024-07-22T15:01:45Z
http://arxiv.org/abs/2407.15694v1,http://arxiv.org/abs/2407.15694v1,"Counter Turing Test ($CT^2$): Investigating AI-Generated Text Detection
  for Hindi -- Ranking LLMs based on Hindi AI Detectability Index ($ADI_{hi}$)","The widespread adoption of large language models (LLMs) and awareness around
multilingual LLMs have raised concerns regarding the potential risks and
repercussions linked to the misapplication of AI-generated text, necessitating
increased vigilance. While these models are primarily trained for English,
their extensive training on vast datasets covering almost the entire web,
equips them with capabilities to perform well in numerous other languages.
AI-Generated Text Detection (AGTD) has emerged as a topic that has already
received immediate attention in research, with some initial methods having been
proposed, soon followed by the emergence of techniques to bypass detection. In
this paper, we report our investigation on AGTD for an indic language Hindi.
Our major contributions are in four folds: i) examined 26 LLMs to evaluate
their proficiency in generating Hindi text, ii) introducing the AI-generated
news article in Hindi ($AG_{hi}$) dataset, iii) evaluated the effectiveness of
five recently proposed AGTD techniques: ConDA, J-Guard, RADAR, RAIDAR and
Intrinsic Dimension Estimation for detecting AI-generated Hindi text, iv)
proposed Hindi AI Detectability Index ($ADI_{hi}$) which shows a spectrum to
understand the evolving landscape of eloquence of AI-generated text in Hindi.
We will make the codes and datasets available to encourage further research.","[{'name': 'Ishan Kavathekar'}, {'name': 'Anku Rani'}, {'name': 'Ashmit Chamoli'}, {'name': 'Ponnurangam Kumaraguru'}, {'name': 'Amit Sheth'}, {'name': 'Amitava Das'}]",2024-07-22T15:00:23Z
http://arxiv.org/abs/2407.15645v1,http://arxiv.org/abs/2407.15645v1,"Psychometric Alignment: Capturing Human Knowledge Distributions via
  Language Models","Language models (LMs) are increasingly used to simulate human-like responses
in scenarios where accurately mimicking a population's behavior can guide
decision-making, such as in developing educational materials and designing
public policies. The objective of these simulations is for LMs to capture the
variations in human responses, rather than merely providing the expected
correct answers. Prior work has shown that LMs often generate unrealistically
accurate responses, but there are no established metrics to quantify how
closely the knowledge distribution of LMs aligns with that of humans. To
address this, we introduce ""psychometric alignment,"" a metric that measures the
extent to which LMs reflect human knowledge distribution. Assessing this
alignment involves collecting responses from both LMs and humans to the same
set of test items and using Item Response Theory to analyze the differences in
item functioning between the groups. We demonstrate that our metric can capture
important variations in populations that traditional metrics, like differences
in accuracy, fail to capture. We apply this metric to assess existing LMs for
their alignment with human knowledge distributions across three real-world
domains. We find significant misalignment between LMs and human populations,
though using persona-based prompts can improve alignment. Interestingly,
smaller LMs tend to achieve greater psychometric alignment than larger LMs.
Further, training LMs on human response data from the target distribution
enhances their psychometric alignment on unseen test items, but the
effectiveness of such training varies across domains.","[{'name': 'Joy He-Yueya'}, {'name': 'Wanjing Anya Ma'}, {'name': 'Kanishk Gandhi'}, {'name': 'Benjamin W. Domingue'}, {'name': 'Emma Brunskill'}, {'name': 'Noah D. Goodman'}]",2024-07-22T14:02:59Z
http://arxiv.org/abs/2407.15621v1,http://arxiv.org/abs/2407.15621v1,"RadioRAG: Factual Large Language Models for Enhanced Diagnostics in
  Radiology Using Dynamic Retrieval Augmented Generation","Large language models (LLMs) have advanced the field of artificial
intelligence (AI) in medicine. However LLMs often generate outdated or
inaccurate information based on static training datasets. Retrieval augmented
generation (RAG) mitigates this by integrating outside data sources. While
previous RAG systems used pre-assembled, fixed databases with limited
flexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end
framework that retrieves data from authoritative radiologic online sources in
real-time. RadioRAG is evaluated using a dedicated radiologic
question-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of
various LLMs when answering radiology-specific questions with and without
access to additional online information via RAG. Using 80 questions from RSNA
Case Collection across radiologic subspecialties and 24 additional
expert-curated questions, for which the correct gold-standard answers were
available, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B
and 70B]) were prompted with and without RadioRAG. RadioRAG retrieved
context-specific information from www.radiopaedia.org in real-time and
incorporated them into its reply. RadioRAG consistently improved diagnostic
accuracy across all LLMs, with relative improvements ranging from 2% to 54%. It
matched or exceeded question answering without RAG across radiologic
subspecialties, particularly in breast imaging and emergency radiology.
However, degree of improvement varied among models; GPT-3.5-turbo and
Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2
showed no improvement, highlighting variability in its effectiveness. LLMs
benefit when provided access to domain-specific data beyond their training
data. For radiology, RadioRAG establishes a robust framework that substantially
improves diagnostic accuracy and factuality in radiological question answering.","[{'name': 'Soroosh Tayebi Arasteh'}, {'name': 'Mahshad Lotfinia'}, {'name': 'Keno Bressem'}, {'name': 'Robert Siepmann'}, {'name': 'Dyke Ferber'}, {'name': 'Christiane Kuhl'}, {'name': 'Jakob Nikolas Kather'}, {'name': 'Sven Nebelung'}, {'name': 'Daniel Truhn'}]",2024-07-22T13:29:56Z
http://arxiv.org/abs/2407.15612v2,http://arxiv.org/abs/2407.15612v2,Can GPT-4 learn to analyze moves in research article abstracts?,"One of the most powerful and enduring ideas in written discourse analysis is
that genres can be described in terms of the moves which structure a writer's
purpose. Considerable research has sought to identify these distinct
communicative acts, but analyses have been beset by problems of subjectivity,
reliability and the time-consuming need for multiple coders to confirm
analyses. In this paper we employ the affordances of GPT-4 to automate the
annotation process by using natural language prompts. Focusing on abstracts
from articles in four applied linguistics journals, we devise prompts which
enable the model to identify moves effectively. The annotated outputs of these
prompts were evaluated by two assessors with a third addressing disagreements.
The results show that an 8-shot prompt was more effective than one using two,
confirming that the inclusion of examples illustrating areas of variability can
enhance GPT-4's ability to recognize multiple moves in a single sentence and
reduce bias related to textual position. We suggest that GPT-4 offers
considerable potential in automating this annotation process, when human actors
with domain specific linguistic expertise inform the prompting process.","[{'name': 'Danni Yu'}, {'name': 'Marina Bondi'}, {'name': 'Ken Hyland'}]",2024-07-22T13:14:27Z
http://arxiv.org/abs/2407.15608v1,http://arxiv.org/abs/2407.15608v1,"StylusAI: Stylistic Adaptation for Robust German Handwritten Text
  Generation","In this study, we introduce StylusAI, a novel architecture leveraging
diffusion models in the domain of handwriting style generation. StylusAI is
specifically designed to adapt and integrate the stylistic nuances of one
language's handwriting into another, particularly focusing on blending English
handwriting styles into the context of the German writing system. This approach
enables the generation of German text in English handwriting styles and German
handwriting styles into English, enriching machine-generated handwriting
diversity while ensuring that the generated text remains legible across both
languages. To support the development and evaluation of StylusAI, we present
the \lq{Deutscher Handschriften-Datensatz}\rq~(DHSD), a comprehensive dataset
encompassing 37 distinct handwriting styles within the German language. This
dataset provides a fundamental resource for training and benchmarking in the
realm of handwritten text generation. Our results demonstrate that StylusAI not
only introduces a new method for style adaptation in handwritten text
generation but also surpasses existing models in generating handwriting samples
that improve both text quality and stylistic fidelity, evidenced by its
performance on the IAM database and our newly proposed DHSD. Thus, StylusAI
represents a significant advancement in the field of handwriting style
generation, offering promising avenues for future research and applications in
cross-linguistic style adaptation for languages with similar scripts.","[{'name': 'Nauman Riaz'}, {'name': 'Saifullah Saifullah'}, {'name': 'Stefan Agne'}, {'name': 'Andreas Dengel'}, {'name': 'Sheraz Ahmed'}]",2024-07-22T13:08:30Z
http://arxiv.org/abs/2407.21045v1,http://arxiv.org/abs/2407.21045v1,"Unlocking the Potential: Benchmarking Large Language Models in Water
  Engineering and Research","Recent advancements in Large Language Models (LLMs) have sparked interest in
their potential applications across various fields. This paper embarked on a
pivotal inquiry: Can existing LLMs effectively serve as ""water expert models""
for water engineering and research tasks? This study was the first to evaluate
LLMs' contributions across various water engineering and research tasks by
establishing a domain-specific benchmark suite, namely, WaterER. Herein, we
prepared 983 tasks related to water engineering and research, categorized into
""wastewater treatment"", ""environmental restoration"", ""drinking water treatment
and distribution"", ""sanitation"", ""anaerobic digestion"" and ""contaminants
assessment"". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5,
Gemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the
strengths of GPT-4 in handling diverse and complex tasks of water engineering
and water research, the specialized capabilities of Gemini in academic
contexts, Llama3's strongest capacity to answer Chinese water engineering
questions and the competitive performance of Chinese-oriented models like
GLM-4, ERNIE and QWEN in some water engineering tasks. More specifically,
current LLMs excelled particularly in generating precise research gaps for
papers on ""contaminants and related water quality monitoring and assessment"".
Additionally, they were more adept at creating appropriate titles for research
papers on ""treatment processes for wastewaters"", ""environmental restoration"",
and ""drinking water treatment"". Overall, this study pioneered evaluating LLMs
in water engineering and research by introducing the WaterER benchmark to
assess the trustworthiness of their predictions. This standardized evaluation
framework would also drive future advancements in LLM technology by using
targeting datasets, propelling these models towards becoming true ""water
expert"".","[{'name': 'Boyan Xu'}, {'name': 'Liang Wen'}, {'name': 'Zihao Li'}, {'name': 'Yuxing Yang'}, {'name': 'Guanlan Wu'}, {'name': 'Xiongpeng Tang'}, {'name': 'Yu Li'}, {'name': 'Zihao Wu'}, {'name': 'Qingxian Su'}, {'name': 'Xueqing Shi'}, {'name': 'Yue Yang'}, {'name': 'Rui Tong'}, {'name': 'How Yong Ng'}]",2024-07-22T12:32:22Z
http://arxiv.org/abs/2407.15588v1,http://arxiv.org/abs/2407.15588v1,"Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of
  Entity and Relation Texts","Cross-lingual entity alignment (EA) enables the integration of multiple
knowledge graphs (KGs) across different languages, providing users with
seamless access to diverse and comprehensive knowledge.Existing methods, mostly
supervised, face challenges in obtaining labeled entity pairs. To address this,
recent studies have shifted towards a self-supervised and unsupervised
frameworks. Despite their effectiveness, these approaches have limitations: (1)
they mainly focus on entity features, neglecting the semantic information of
relations, (2) they assume isomorphism between source and target graphs,
leading to noise and reduced alignment accuracy, and (3) they are susceptible
to noise in the textual features, especially when encountering inconsistent
translations or Out-Of-Vocabulary (OOV) problems.
  In this paper, we propose ERAlign, an unsupervised and robust cross-lingual
EA framework that jointly performs Entity-level and Relation-level Alignment
using semantic textual features of relations and entities. Its refinement
process iteratively enhances results by fusing entity-level and relation-level
alignments based on neighbor triple matching. The additional verification
process examines the entities' neighbor triples as the linearized text. This
\textit{Align-and-Verify} pipeline that rigorously assesses alignment results,
achieving near-perfect alignment even in the presence of noisy textual features
of entities. Our extensive experiments demonstrate that robustness and general
applicability of \proposed improved the accuracy and effectiveness of EA tasks,
contributing significantly to knowledge-oriented applications.","[{'name': 'Soojin Yoon'}, {'name': 'Sungho Ko'}, {'name': 'Tongyoung Kim'}, {'name': 'SeongKu Kang'}, {'name': 'Jinyoung Yeo'}, {'name': 'Dongha Lee'}]",2024-07-22T12:25:48Z
http://arxiv.org/abs/2407.15569v1,http://arxiv.org/abs/2407.15569v1,"An Empirical Study of Retrieval Augmented Generation with
  Chain-of-Thought","Since the launch of ChatGPT at the end of 2022, generative dialogue models
represented by ChatGPT have quickly become essential tools in daily life. As
user expectations increase, enhancing the capability of generative dialogue
models to solve complex problems has become a focal point of current research.
This paper delves into the effectiveness of the RAFT (Retrieval Augmented
Fine-Tuning) method in improving the performance of Generative dialogue models.
RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and
retrieval augmented generation (RAG), which significantly enhanced the model's
information extraction and logical reasoning abilities. We evaluated the RAFT
method across multiple datasets and analysed its performance in various
reasoning tasks, including long-form QA and short-form QA tasks, tasks in both
Chinese and English, and supportive and comparison reasoning tasks. Notably, it
addresses the gaps in previous research regarding long-form QA tasks and
Chinese datasets. Moreover, we also evaluate the benefit of the
chain-of-thought (CoT) in the RAFT method. This work offers valuable insights
for studies focused on enhancing the performance of generative dialogue models.","[{'name': 'Yuetong Zhao'}, {'name': 'Hongyu Cao'}, {'name': 'Xianyu Zhao'}, {'name': 'Zhijian Ou'}]",2024-07-22T11:55:14Z
http://arxiv.org/abs/2407.15556v1,http://arxiv.org/abs/2407.15556v1,"SETTP: Style Extraction and Tunable Inference via Dual-level
  Transferable Prompt Learning","Text style transfer, an important research direction in natural language
processing, aims to adapt the text to various preferences but often faces
challenges with limited resources. In this work, we introduce a novel method
termed Style Extraction and Tunable Inference via Dual-level Transferable
Prompt Learning (SETTP) for effective style transfer in low-resource scenarios.
First, SETTP learns source style-level prompts containing fundamental style
characteristics from high-resource style transfer. During training, the source
style-level prompts are transferred through an attention module to derive a
target style-level prompt for beneficial knowledge provision in low-resource
style transfer. Additionally, we propose instance-level prompts obtained by
clustering the target resources based on the semantic content to reduce
semantic bias. We also propose an automated evaluation approach of style
similarity based on alignment with human evaluations using ChatGPT-4. Our
experiments across three resourceful styles show that SETTP requires only
1/20th of the data volume to achieve performance comparable to state-of-the-art
methods. In tasks involving scarce data like writing style and role style,
SETTP outperforms previous methods by 16.24\%.","[{'name': 'Chunzhen Jin'}, {'name': 'Yongfeng Huang'}, {'name': 'Yaqi Wang'}, {'name': 'Peng Cao'}, {'name': 'Osmar Zaiane'}]",2024-07-22T11:34:48Z
http://arxiv.org/abs/2407.15549v1,http://arxiv.org/abs/2407.15549v1,"Targeted Latent Adversarial Training Improves Robustness to Persistent
  Harmful Behaviors in LLMs","Large language models (LLMs) can often be made to behave in undesirable ways
that they are explicitly fine-tuned not to. For example, the LLM red-teaming
literature has produced a wide variety of `jailbreaking' techniques to elicit
harmful text from models that were fine-tuned to be harmless. Recent work on
red-teaming, model editing, and interpretability suggests that this challenge
stems from how (adversarial) fine-tuning largely serves to suppress rather than
remove undesirable capabilities from LLMs. Prior work has introduced latent
adversarial training (LAT) as a way to improve robustness to broad classes of
failures. These prior works have considered untargeted latent space attacks
where the adversary perturbs latent activations to maximize loss on examples of
desirable behavior. Untargeted LAT can provide a generic type of robustness but
does not leverage information about specific failure modes. Here, we experiment
with targeted LAT where the adversary seeks to minimize loss on a specific
competing task. We find that it can augment a wide variety of state-of-the-art
methods. First, we use targeted LAT to improve robustness to jailbreaks,
outperforming a strong R2D2 baseline with orders of magnitude less compute.
Second, we use it to more effectively remove backdoors with no knowledge of the
trigger. Finally, we use it to more effectively unlearn knowledge for specific
undesirable tasks in a way that is also more robust to re-learning. Overall,
our results suggest that targeted LAT can be an effective tool for defending
against harmful behaviors from LLMs.","[{'name': 'Abhay Sheshadri'}, {'name': 'Aidan Ewart'}, {'name': 'Phillip Guo'}, {'name': 'Aengus Lynch'}, {'name': 'Cindy Wu'}, {'name': 'Vivek Hebbar'}, {'name': 'Henry Sleight'}, {'name': 'Asa Cooper Stickland'}, {'name': 'Ethan Perez'}, {'name': 'Dylan Hadfield-Menell'}, {'name': 'Stephen Casper'}]",2024-07-22T11:19:14Z
http://arxiv.org/abs/2407.15516v1,http://arxiv.org/abs/2407.15516v1,"Attention Is All You Need But You Don't Need All Of It For Inference of
  Large Language Models","The inference demand for LLMs has skyrocketed in recent months, and serving
models with low latencies remains challenging due to the quadratic input length
complexity of the attention layers. In this work, we investigate the effect of
dropping MLP and attention layers at inference time on the performance of
Llama-v2 models. We find that dropping dreeper attention layers only marginally
decreases performance but leads to the best speedups alongside dropping entire
layers. For example, removing 33\% of attention layers in a 13B Llama2 model
results in a 1.8\% drop in average performance over the OpenLLM benchmark. We
also observe that skipping layers except the latter layers reduces performances
for more layers skipped, except for skipping the attention layers.","[{'name': 'Georgy Tyukin'}, {'name': 'Gbetondji J-S Dovonon'}, {'name': 'Jean Kaddour'}, {'name': 'Pasquale Minervini'}]",2024-07-22T10:09:05Z
http://arxiv.org/abs/2407.15508v1,http://arxiv.org/abs/2407.15508v1,"Compensate Quantization Errors+: Quantized Models Are Inquisitive
  Learners","Large Language Models (LLMs) showcase remarkable performance and robust
deductive capabilities, yet their expansive size complicates deployment and
raises environmental concerns due to substantial resource consumption. The
recent development of a quantization technique known as Learnable
Singular-value Increment (LSI) has addressed some of these quantization
challenges. Leveraging insights from LSI and our extensive research, we have
developed innovative methods that enhance the performance of quantized LLMs,
particularly in low-bit settings. Our methods consistently deliver
state-of-the-art results across various quantization scenarios and offer deep
theoretical insights into the quantization process, elucidating the potential
of quantized models for widespread application.","[{'name': 'Yifei Gao'}, {'name': 'Jie Ou'}, {'name': 'Lei Wang'}, {'name': 'Fanhua Shang'}, {'name': 'Jaji Wu'}, {'name': 'Jun Cheng'}]",2024-07-22T09:45:16Z
http://arxiv.org/abs/2407.15504v1,http://arxiv.org/abs/2407.15504v1,"Fundamental Limits of Prompt Compression: A Rate-Distortion Framework
  for Black-Box Language Models","We formalize the problem of prompt compression for large language models
(LLMs) and present a framework to unify token-level prompt compression methods
which create hard prompts for black-box models. We derive the distortion-rate
function for this setup as a linear program, and provide an efficient algorithm
to compute this fundamental limit via the dual of the linear program. Using the
distortion-rate function as the baseline, we study the performance of existing
compression schemes on a synthetic dataset consisting of prompts generated from
a Markov chain, natural language queries, and their respective answers. Our
empirical analysis demonstrates the criticality of query-aware prompt
compression, where the compressor has knowledge of the downstream task/query
for the black-box LLM. We show that there is a large gap between the
performance of current prompt compression methods and the optimal strategy, and
propose a query-aware, variable-rate adaptation of a prior work to close the
gap. We extend our experiments to a small natural language dataset to further
confirm our findings on our synthetic dataset.","[{'name': 'Adway Girish'}, {'name': 'Alliot Nagle'}, {'name': 'Marco Bondaschi'}, {'name': 'Michael Gastpar'}, {'name': 'Ashok Vardhan Makkuva'}, {'name': 'Hyeji Kim'}]",2024-07-22T09:40:13Z
http://arxiv.org/abs/2407.15459v1,http://arxiv.org/abs/2407.15459v1,"Text-to-Battery Recipe: A language modeling-based protocol for automatic
  battery recipe extraction and retrieval","Recent studies have increasingly applied natural language processing (NLP) to
automatically extract experimental research data from the extensive battery
materials literature. Despite the complex process involved in battery
manufacturing -- from material synthesis to cell assembly -- there has been no
comprehensive study systematically organizing this information. In response, we
propose a language modeling-based protocol, Text-to-Battery Recipe (T2BR), for
the automatic extraction of end-to-end battery recipes, validated using a case
study on batteries containing LiFePO4 cathode material. We report machine
learning-based paper filtering models, screening 2,174 relevant papers from the
keyword-based search results, and unsupervised topic models to identify 2,876
paragraphs related to cathode synthesis and 2,958 paragraphs related to cell
assembly. Then, focusing on the two topics, two deep learning-based named
entity recognition models are developed to extract a total of 30 entities --
including precursors, active materials, and synthesis methods -- achieving F1
scores of 88.18% and 94.61%. The accurate extraction of entities enables the
systematic generation of 165 end-toend recipes of LiFePO4 batteries. Our
protocol and results offer valuable insights into specific trends, such as
associations between precursor materials and synthesis methods, or combinations
between different precursor materials. We anticipate that our findings will
serve as a foundational knowledge base for facilitating battery-recipe
information retrieval. The proposed protocol will significantly accelerate the
review of battery material literature and catalyze innovations in battery
design and development.","[{'name': 'Daeun Lee'}, {'name': 'Jaewoong Choi'}, {'name': 'Hiroshi Mizuseki'}, {'name': 'Byungju Lee'}]",2024-07-22T08:15:02Z
http://arxiv.org/abs/2407.15441v1,http://arxiv.org/abs/2407.15441v1,"Developing a Reliable, General-Purpose Hallucination Detection and
  Mitigation Service: Insights and Lessons Learned","Hallucination, a phenomenon where large language models (LLMs) produce output
that is factually incorrect or unrelated to the input, is a major challenge for
LLM applications that require accuracy and dependability. In this paper, we
introduce a reliable and high-speed production system aimed at detecting and
rectifying the hallucination issue within LLMs. Our system encompasses named
entity recognition (NER), natural language inference (NLI), span-based
detection (SBD), and an intricate decision tree-based process to reliably
detect a wide range of hallucinations in LLM responses. Furthermore, our team
has crafted a rewriting mechanism that maintains an optimal mix of precision,
response time, and cost-effectiveness. We detail the core elements of our
framework and underscore the paramount challenges tied to response time,
availability, and performance metrics, which are crucial for real-world
deployment of these technologies. Our extensive evaluation, utilizing offline
data and live production traffic, confirms the efficacy of our proposed
framework and service.","[{'name': 'Song Wang'}, {'name': 'Xun Wang'}, {'name': 'Jie Mei'}, {'name': 'Yujia Xie'}, {'name': 'Sean Muarray'}, {'name': 'Zhang Li'}, {'name': 'Lingfeng Wu'}, {'name': 'Si-Qing Chen'}, {'name': 'Wayne Xiong'}]",2024-07-22T07:48:30Z
http://arxiv.org/abs/2407.15425v2,http://arxiv.org/abs/2407.15425v2,Empirical Capacity Model for Self-Attention Neural Networks,"Large pretrained self-attention neural networks, or transformers, have been
very successful in various tasks recently. The performance of a model on a
given task depends on its ability to memorize and generalize the training data.
Large transformer models, which may have billions of parameters, in theory have
a huge capacity to memorize content. However, the current algorithms for the
optimization fall short of the theoretical capacity, and the capacity is also
highly dependent on the content. In this paper, we focus on the memory capacity
of these models obtained using common training algorithms and synthetic
training data. Based on the results, we derive an empirical capacity model
(ECM) for a generic transformer. The ECM can be used to design task-specific
transformer models with an optimal number of parameters in cases where the
target memorization capability of the task can be defined.","[{'name': 'Aki Härmä'}, {'name': 'Marcin Pietrasik'}, {'name': 'Anna Wilbik'}]",2024-07-22T07:02:15Z
http://arxiv.org/abs/2407.15415v1,http://arxiv.org/abs/2407.15415v1,"LLaST: Improved End-to-end Speech Translation System Leveraged by Large
  Language Models","We introduces LLaST, a framework for building high-performance Large Language
model based Speech-to-text Translation systems. We address the limitations of
end-to-end speech translation(E2E ST) models by exploring model architecture
design and optimization techniques tailored for LLMs. Our approach includes
LLM-based speech translation architecture design, ASR-augmented training,
multilingual data augmentation, and dual-LoRA optimization. Our approach
demonstrates superior performance on the CoVoST-2 benchmark and showcases
exceptional scaling capabilities powered by LLMs. We believe this effective
method will serve as a strong baseline for speech translation and provide
insights for future improvements of the LLM-based speech translation framework.
We release the data, code and models in https://github.com/openaudiolab/LLaST.","[{'name': 'Xi Chen'}, {'name': 'Songyang Zhang'}, {'name': 'Qibing Bai'}, {'name': 'Kai Chen'}, {'name': 'Satoshi Nakamura'}]",2024-07-22T06:42:00Z
http://arxiv.org/abs/2407.15399v1,http://arxiv.org/abs/2407.15399v1,"Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned
  Large Language Models","With the development of large language models (LLMs) like ChatGPT, both their
vast applications and potential vulnerabilities have come to the forefront.
While developers have integrated multiple safety mechanisms to mitigate their
misuse, a risk remains, particularly when models encounter adversarial inputs.
This study unveils an attack mechanism that capitalizes on human conversation
strategies to extract harmful information from LLMs. We delineate three pivotal
strategies: (i) decomposing malicious questions into seemingly innocent
sub-questions; (ii) rewriting overtly malicious questions into more covert,
benign-sounding ones; (iii) enhancing the harmfulness of responses by prompting
models for illustrative examples. Unlike conventional methods that target
explicit malicious responses, our approach delves deeper into the nature of the
information provided in responses. Through our experiments conducted on
GPT-3.5-turbo, GPT-4, and Llama2, our method has demonstrated a marked efficacy
compared to conventional attack methods. In summary, this work introduces a
novel attack method that outperforms previous approaches, raising an important
question: How to discern whether the ultimate intent in a dialogue is
malicious?","[{'name': 'Xiao Liu'}, {'name': 'Liangzhi Li'}, {'name': 'Tong Xiang'}, {'name': 'Fuying Ye'}, {'name': 'Lu Wei'}, {'name': 'Wangyue Li'}, {'name': 'Noa Garcia'}]",2024-07-22T06:04:29Z
http://arxiv.org/abs/2407.15390v1,http://arxiv.org/abs/2407.15390v1,ALLaM: Large Language Models for Arabic and English,"We present ALLaM: Arabic Large Language Model, a series of large language
models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is
carefully trained considering the values of language alignment and knowledge
transfer at scale. Our autoregressive decoder-only architecture models
demonstrate how second-language acquisition via vocabulary expansion and
pretraining on a mixture of Arabic and English text can steer a model towards a
new language (Arabic) without any catastrophic forgetting in the original
language (English). Furthermore, we highlight the effectiveness of using
parallel/translated data to aid the process of knowledge alignment between
languages. Finally, we show that extensive alignment with human preferences can
significantly enhance the performance of a language model compared to models of
a larger scale with lower quality alignment. ALLaM achieves state-of-the-art
performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and
Arabic Exams. Our aligned models improve both in Arabic and English from their
base aligned models.","[{'name': 'M Saiful Bari'}, {'name': 'Yazeed Alnumay'}, {'name': 'Norah A. Alzahrani'}, {'name': 'Nouf M. Alotaibi'}, {'name': 'Hisham A. Alyahya'}, {'name': 'Sultan AlRashed'}, {'name': 'Faisal A. Mirza'}, {'name': 'Shaykhah Z. Alsubaie'}, {'name': 'Hassan A. Alahmed'}, {'name': 'Ghadah Alabduljabbar'}, {'name': 'Raghad Alkhathran'}, {'name': 'Yousef Almushayqih'}, {'name': 'Raneem Alnajim'}, {'name': 'Salman Alsubaihi'}, {'name': 'Maryam Al Mansour'}, {'name': 'Majed Alrubaian'}, {'name': 'Ali Alammari'}, {'name': 'Zaki Alawami'}, {'name': 'Abdulmohsen Al-Thubaity'}, {'name': 'Ahmed Abdelali'}, {'name': 'Jeril Kuriakose'}, {'name': 'Abdalghani Abujabal'}, {'name': 'Nora Al-Twairesh'}, {'name': 'Areeb Alowisheq'}, {'name': 'Haidar Khan'}]",2024-07-22T05:35:17Z
http://arxiv.org/abs/2407.15375v1,http://arxiv.org/abs/2407.15375v1,"The Development of a Comprehensive Spanish Dictionary for Phonetic and
  Lexical Tagging in Socio-phonetic Research (ESPADA)","Pronunciation dictionaries are an important component in the process of
speech forced alignment. The accuracy of these dictionaries has a strong effect
on the aligned speech data since they help the mapping between orthographic
transcriptions and acoustic signals. In this paper, I present the creation of a
comprehensive pronunciation dictionary in Spanish (ESPADA) that can be used in
most of the dialect variants of Spanish data. Current dictionaries focus on
specific regional variants, but with the flexible nature of our tool, it can be
readily applied to capture the most common phonetic differences across major
dialectal variants. We propose improvements to current pronunciation
dictionaries as well as mapping other relevant annotations such as
morphological and lexical information. In terms of size, it is currently the
most complete dictionary with more than 628,000 entries, representing words
from 16 countries. All entries come with their corresponding pronunciations,
morphological and lexical tagging, and other relevant information for phonetic
analysis: stress patterns, phonotactics, IPA transcriptions, and more. This
aims to equip socio-phonetic researchers with a complete open-source tool that
enhances dialectal research within socio-phonetic frameworks in the Spanish
language.",[{'name': 'Simon Gonzalez'}],2024-07-22T04:51:33Z
http://arxiv.org/abs/2407.15374v1,http://arxiv.org/abs/2407.15374v1,"ILiAD: An Interactive Corpus for Linguistic Annotated Data from Twitter
  Posts","Social Media platforms have offered invaluable opportunities for linguistic
research. The availability of up-to-date data, coming from any part in the
world, and coming from natural contexts, has allowed researchers to study
language in real time. One of the fields that has made great use of social
media platforms is Corpus Linguistics. There is currently a wide range of
projects which have been able to successfully create corpora from social media.
In this paper, we present the development and deployment of a linguistic corpus
from Twitter posts in English, coming from 26 news agencies and 27 individuals.
The main goal was to create a fully annotated English corpus for linguistic
analysis. We include information on morphology and syntax, as well as NLP
features such as tokenization, lemmas, and n- grams. The information is
presented through a range of powerful visualisations for users to explore
linguistic patterns in the corpus. With this tool, we aim to contribute to the
area of language technologies applied to linguistic research.",[{'name': 'Simon Gonzalez'}],2024-07-22T04:48:04Z
http://arxiv.org/abs/2407.15370v1,http://arxiv.org/abs/2407.15370v1,A Network Analysis Approach to Conlang Research Literature,"The field of conlang has evidenced an important growth in the last decades.
This has been the product of a wide interest in the use and study of conlangs
for artistic purposes. However, one important question is what it is happening
with conlang in the academic world. This paper aims to have an overall
understanding of the literature on conlang research. With this we aim to give a
realistic picture of the field in present days. We have implemented a
computational linguistic approach, combining bibliometrics and network analysis
to examine all publications available in the Scopus database. Analysing over
2300 academic publications since 1927 until 2022, we have found that Esperanto
is by far the most documented conlang. Three main authors have contributed to
this: Garv\'ia R., Fiedler S., and Blanke D. The 1970s and 1980s have been the
decades where the foundations of current research have been built. In terms of
methodologies, language learning and experimental linguistics are the ones
contributing to most to the preferred approaches of study in the field. We
present the results and discuss our limitations and future work.",[{'name': 'Simon Gonzalez'}],2024-07-22T04:40:45Z
http://arxiv.org/abs/2407.15366v1,http://arxiv.org/abs/2407.15366v1,"Walking in Others' Shoes: How Perspective-Taking Guides Large Language
  Models in Reducing Toxicity and Bias","The common toxicity and societal bias in contents generated by large language
models (LLMs) necessitate strategies to reduce harm. Present solutions often
demand white-box access to the model or substantial training, which is
impractical for cutting-edge commercial LLMs. Moreover, prevailing prompting
methods depend on external tool feedback and fail to simultaneously lessen
toxicity and bias. Motivated by social psychology principles, we propose a
novel strategy named \textbf{perspective-taking prompting (\textsc{PeT})} that
inspires LLMs to integrate diverse human perspectives and self-regulate their
responses. This self-correction mechanism can significantly diminish toxicity
(up to $89\%$) and bias (up to $73\%$) in LLMs' responses. Rigorous evaluations
and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and
three open-source LLMs, revealing \textsc{PeT}'s superiority in producing less
harmful responses, outperforming five strong baselines.","[{'name': 'Rongwu Xu'}, {'name': ""Zi'an Zhou""}, {'name': 'Tianwei Zhang'}, {'name': 'Zehan Qi'}, {'name': 'Su Yao'}, {'name': 'Ke Xu'}, {'name': 'Wei Xu'}, {'name': 'Han Qiu'}]",2024-07-22T04:25:01Z
http://arxiv.org/abs/2407.15360v1,http://arxiv.org/abs/2407.15360v1,Dissecting Multiplication in Transformers: Insights into LLMs,"Transformer-based large language models have achieved remarkable performance
across various natural language processing tasks. However, they often struggle
with seemingly easy tasks like arithmetic despite their vast capabilities. This
stark disparity raise human's concerns about their safe and ethical use, hinder
their widespread adoption.In this paper, we focus on a typical arithmetic task,
integer multiplication, to explore and explain the imperfection of transformers
in this domain. We provide comprehensive analysis of a vanilla transformer
trained to perform n-digit integer multiplication. Our observations indicate
that the model decomposes multiplication task into multiple parallel subtasks,
sequentially optimizing each subtask for each digit to complete the final
multiplication. Based on observation and analysis, we infer the reasons of
transformers deficiencies in multiplication tasks lies in their difficulty in
calculating successive carryovers and caching intermediate results, and
confirmed this inference through experiments. Guided by these findings, we
propose improvements to enhance transformers performance on multiplication
tasks. These enhancements are validated through rigorous testing and
mathematical modeling, not only enhance transformer's interpretability, but
also improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit
integer multiplication with a tiny transformer, outperform LLMs GPT-4. Our
method contributes to the broader fields of model understanding and
interpretability, paving the way for analyzing more complex tasks and
Transformer models. This work underscores the importance of explainable AI,
helping to build trust in large language models and promoting their adoption in
critical applications.","[{'name': 'Luyu Qiu'}, {'name': 'Jianing Li'}, {'name': 'Chi Su'}, {'name': 'Chen Jason Zhang'}, {'name': 'Lei Chen'}]",2024-07-22T04:07:26Z
http://arxiv.org/abs/2407.21043v2,http://arxiv.org/abs/2407.21043v2,"CP-Prompt: Composition-Based Cross-modal Prompting for
  Domain-Incremental Continual Learning","The key challenge of cross-modal domain-incremental learning (DIL) is to
enable the learning model to continuously learn from novel data with different
feature distributions under the same task without forgetting old ones. However,
existing top-performing methods still cause high forgetting rates, by lacking
intra-domain knowledge extraction and inter-domain common prompting strategy.
In this paper, we propose a simple yet effective framework, CP-Prompt, by
training limited parameters to instruct a pre-trained model to learn new
domains and avoid forgetting existing feature distributions. CP-Prompt captures
intra-domain knowledge by compositionally inserting personalized prompts on
multi-head self-attention layers and then learns the inter-domain knowledge
with a common prompting strategy. CP-Prompt shows superiority compared with
state-of-the-art baselines among three widely evaluated DIL tasks. The source
code is available at https://github.com/dannis97500/CP_Prompt.","[{'name': 'Yu Feng'}, {'name': 'Zhen Tian'}, {'name': 'Yifan Zhu'}, {'name': 'Zongfu Han'}, {'name': 'Haoran Luo'}, {'name': 'Guangwei Zhang'}, {'name': 'Meina Song'}]",2024-07-22T04:07:12Z
http://arxiv.org/abs/2407.15352v1,http://arxiv.org/abs/2407.15352v1,MAVEN-Fact: A Large-scale Event Factuality Detection Dataset,"Event Factuality Detection (EFD) task determines the factuality of textual
events, i.e., classifying whether an event is a fact, possibility, or
impossibility, which is essential for faithfully understanding and utilizing
event knowledge. However, due to the lack of high-quality large-scale data,
event factuality detection is under-explored in event understanding research,
which limits the development of EFD community. To address these issues and
provide faithful event understanding, we introduce MAVEN-Fact, a large-scale
and high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes
factuality annotations of 112,276 events, making it the largest EFD dataset.
Extensive experiments demonstrate that MAVEN-Fact is challenging for both
conventional fine-tuned models and large language models (LLMs). Thanks to the
comprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact
also supports some further analyses and we find that adopting event arguments
and relations helps in event factuality detection for fine-tuned models but
does not benefit LLMs. Furthermore, we preliminarily study an application case
of event factuality detection and find it helps in mitigating event-related
hallucination in LLMs. Our dataset and codes can be obtained from
\url{https://github.com/lcy2723/MAVEN-FACT}","[{'name': 'Chunyang Li'}, {'name': 'Hao Peng'}, {'name': 'Xiaozhi Wang'}, {'name': 'Yunjia Qi'}, {'name': 'Lei Hou'}, {'name': 'Bin Xu'}, {'name': 'Juanzi Li'}]",2024-07-22T03:43:46Z
http://arxiv.org/abs/2407.15351v2,http://arxiv.org/abs/2407.15351v2,"LLMExplainer: Large Language Model based Bayesian Inference for Graph
  Explanation Generation","Recent studies seek to provide Graph Neural Network (GNN) interpretability
via multiple unsupervised learning models. Due to the scarcity of datasets,
current methods easily suffer from learning bias. To solve this problem, we
embed a Large Language Model (LLM) as knowledge into the GNN explanation
network to avoid the learning bias problem. We inject LLM as a Bayesian
Inference (BI) module to mitigate learning bias. The efficacy of the BI module
has been proven both theoretically and experimentally. We conduct experiments
on both synthetic and real-world datasets. The innovation of our work lies in
two parts: 1. We provide a novel view of the possibility of an LLM functioning
as a Bayesian inference to improve the performance of existing algorithms; 2.
We are the first to discuss the learning bias issues in the GNN explanation
problem.","[{'name': 'Jiaxing Zhang'}, {'name': 'Jiayi Liu'}, {'name': 'Dongsheng Luo'}, {'name': 'Jennifer Neville'}, {'name': 'Hua Wei'}]",2024-07-22T03:36:38Z
http://arxiv.org/abs/2407.15346v1,http://arxiv.org/abs/2407.15346v1,"Knowledge Acquisition Disentanglement for Knowledge-based Visual
  Question Answering with Large Language Models","Knowledge-based Visual Question Answering (KVQA) requires both image and
world knowledge to answer questions. Current methods first retrieve knowledge
from the image and external knowledge base with the original complex question,
then generate answers with Large Language Models (LLMs). However, since the
original question contains complex elements that require knowledge from
different sources, acquiring different kinds of knowledge in a coupled manner
may confuse models and hinder them from retrieving precise knowledge.
Furthermore, the ``forward-only'' answering process fails to explicitly capture
the knowledge needs of LLMs, which can further hurt answering quality. To cope
with the above limitations, we propose DKA: Disentangled Knowledge Acquisition
from LLM feedback, a training-free framework that disentangles knowledge
acquisition to avoid confusion and uses LLM's feedback to specify the required
knowledge. Specifically, DKA requires LLMs to specify what knowledge they need
to answer the question and decompose the original complex question into two
simple sub-questions: Image-based sub-question and Knowledge-based
sub-question. Then we use the two sub-questions to retrieve knowledge from the
image and knowledge base, respectively. In this way, two knowledge acquisition
models can focus on the content that corresponds to them and avoid disturbance
of irrelevant elements in the original complex question, which can help to
provide more precise knowledge and better align the knowledge needs of LLMs to
yield correct answers. Experiments on benchmark datasets show that DKA
significantly outperforms SOTA models. To facilitate future research, our data
and code are available at \url{https://github.com/Lackel/DKA}.","[{'name': 'Wenbin An'}, {'name': 'Feng Tian'}, {'name': 'Jiahao Nie'}, {'name': 'Wenkai Shi'}, {'name': 'Haonan Lin'}, {'name': 'Yan Chen'}, {'name': 'QianYing Wang'}, {'name': 'Yaqiang Wu'}, {'name': 'Guang Dai'}, {'name': 'Ping Chen'}]",2024-07-22T03:05:32Z
http://arxiv.org/abs/2407.15343v1,http://arxiv.org/abs/2407.15343v1,Improving Minimum Bayes Risk Decoding with Multi-Prompt,"While instruction fine-tuned LLMs are effective text generators, sensitivity
to prompt construction makes performance unstable and sub-optimal in practice.
Relying on a single ""best"" prompt cannot capture all differing approaches to a
generation problem. Using this observation, we propose multi-prompt decoding,
where many candidate generations are decoded from a prompt bank at
inference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR)
decoding, which selects a final output using a trained value metric. We show
multi-prompt improves MBR across a comprehensive set of conditional generation
tasks, and show this is a result of estimating a more diverse and higher
quality candidate space than that of a single prompt. Further experiments
confirm multi-prompt improves generation across tasks, models and metrics.","[{'name': 'David Heineman'}, {'name': 'Yao Dou'}, {'name': 'Wei Xu'}]",2024-07-22T02:57:10Z
http://arxiv.org/abs/2407.15341v1,http://arxiv.org/abs/2407.15341v1,"ZZU-NLP at SIGHAN-2024 dimABSA Task: Aspect-Based Sentiment Analysis
  with Coarse-to-Fine In-context Learning","The DimABSA task requires fine-grained sentiment intensity prediction for
restaurant reviews, including scores for Valence and Arousal dimensions for
each Aspect Term. In this study, we propose a Coarse-to-Fine In-context
Learning(CFICL) method based on the Baichuan2-7B model for the DimABSA task in
the SIGHAN 2024 workshop. Our method improves prediction accuracy through a
two-stage optimization process. In the first stage, we use fixed in-context
examples and prompt templates to enhance the model's sentiment recognition
capability and provide initial predictions for the test data. In the second
stage, we encode the Opinion field using BERT and select the most similar
training data as new in-context examples based on similarity. These examples
include the Opinion field and its scores, as well as related opinion words and
their average scores. By filtering for sentiment polarity, we ensure that the
examples are consistent with the test data. Our method significantly improves
prediction accuracy and consistency by effectively utilizing training data and
optimizing in-context examples, as validated by experimental results.","[{'name': 'Senbin Zhu'}, {'name': 'Hanjie Zhao'}, {'name': 'Xingren Wang'}, {'name': 'Shanhong Liu'}, {'name': 'Yuxiang Jia'}, {'name': 'Hongying Zan'}]",2024-07-22T02:54:46Z
http://arxiv.org/abs/2407.15339v1,http://arxiv.org/abs/2407.15339v1,Deep Learning for Economists,"Deep learning provides powerful methods to impute structured information from
large-scale, unstructured text and image datasets. For example, economists
might wish to detect the presence of economic activity in satellite images, or
to measure the topics or entities mentioned in social media, the congressional
record, or firm filings. This review introduces deep neural networks, covering
methods such as classifiers, regression models, generative AI, and embedding
models. Applications include classification, document digitization, record
linkage, and methods for data exploration in massive scale text and image
corpora. When suitable methods are used, deep learning models can be cheap to
tune and can scale affordably to problems involving millions or billions of
data points.. The review is accompanied by a companion website, EconDL, with
user-friendly demo notebooks, software resources, and a knowledge base that
provides technical details and additional applications.",[{'name': 'Melissa Dell'}],2024-07-22T02:53:18Z
http://arxiv.org/abs/2407.15891v1,http://arxiv.org/abs/2407.15891v1,RazorAttention: Efficient KV Cache Compression Through Retrieval Heads,"The memory and computational demands of Key-Value (KV) cache present
significant challenges for deploying long-context language models. Previous
approaches attempt to mitigate this issue by selectively dropping tokens, which
irreversibly erases critical information that might be needed for future
queries. In this paper, we propose a novel compression technique for KV cache
that preserves all token information. Our investigation reveals that: i) Most
attention heads primarily focus on the local context; ii) Only a few heads,
denoted as retrieval heads, can essentially pay attention to all input tokens.
These key observations motivate us to use separate caching strategy for
attention heads. Therefore, we propose RazorAttention, a training-free KV cache
compression algorithm, which maintains a full cache for these crucial retrieval
heads and discards the remote tokens in non-retrieval heads. Furthermore, we
introduce a novel mechanism involving a ""compensation token"" to further recover
the information in the dropped tokens. Extensive evaluations across a diverse
set of large language models (LLMs) demonstrate that RazorAttention achieves a
reduction in KV cache size by over 70% without noticeable impacts on
performance. Additionally, RazorAttention is compatible with FlashAttention,
rendering it an efficient and plug-and-play solution that enhances LLM
inference efficiency without overhead or retraining of the original model.","[{'name': 'Hanlin Tang'}, {'name': 'Yang Lin'}, {'name': 'Jing Lin'}, {'name': 'Qingsen Han'}, {'name': 'Shikuan Hong'}, {'name': 'Yiwu Yao'}, {'name': 'Gongyi Wang'}]",2024-07-22T01:12:23Z
http://arxiv.org/abs/2407.15296v1,http://arxiv.org/abs/2407.15296v1,"Weak-to-Strong Compositional Learning from Generative Models for
  Language-based Object Detection","Vision-language (VL) models often exhibit a limited understanding of complex
expressions of visual objects (e.g., attributes, shapes, and their relations),
given complex and diverse language queries. Traditional approaches attempt to
improve VL models using hard negative synthetic text, but their effectiveness
is limited. In this paper, we harness the exceptional compositional
understanding capabilities of generative foundational models. We introduce a
novel method for structured synthetic data generation aimed at enhancing the
compositional understanding of VL models in language-based object detection.
Our framework generates densely paired positive and negative triplets (image,
text descriptions, and bounding boxes) in both image and text domains. By
leveraging these synthetic triplets, we transform 'weaker' VL models into
'stronger' models in terms of compositional understanding, a process we call
""Weak-to-Strong Compositional Learning"" (WSCL). To achieve this, we propose a
new compositional contrastive learning formulation that discovers semantics and
structures in complex descriptions from synthetic triplets. As a result, VL
models trained with our synthetic data generation exhibit a significant
performance boost in the Omnilabel benchmark by up to +5AP and the D3 benchmark
by +6.9AP upon existing baselines.","[{'name': 'Kwanyong Park'}, {'name': 'Kuniaki Saito'}, {'name': 'Donghyun Kim'}]",2024-07-21T23:43:24Z
http://arxiv.org/abs/2407.15286v2,http://arxiv.org/abs/2407.15286v2,"Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal
  Mechanisms and the Superficial Hypothesis","Large Language Models (LLMs) are capable of producing content that
perpetuates stereotypes, discrimination, and toxicity. The recently proposed
moral self-correction is a computationally efficient method for reducing
harmful content in the responses of LLMs. However, the process of how injecting
self-correction instructions can modify the behavior of LLMs remains
under-explored. In this paper, we explore the effectiveness of moral
self-correction by answering three research questions: (1) In what scenarios
does moral self-correction work? (2) What are the internal mechanisms of LLMs,
e.g., hidden states, that are influenced by moral self-correction instructions?
(3) Is intrinsic moral self-correction actually superficial? We argue that
self-correction can help LLMs find a shortcut to more morally correct output,
rather than truly reducing the immorality stored in hidden states. Through
empirical investigation with tasks of language generation and multi-choice
question answering, we conclude: (i) LLMs exhibit good performance across both
tasks, and self-correction instructions are particularly beneficial when the
correct answer is already top-ranked; (ii) The morality levels in intermediate
hidden states are strong indicators as to whether one instruction would be more
effective than another; (iii) Based on our analysis of intermediate hidden
states and task case studies of self-correction behaviors, we are first to
propose the hypothesis that intrinsic moral self-correction is in fact
superficial.","[{'name': 'Guangliang Liu'}, {'name': 'Haitao Mao'}, {'name': 'Jiliang Tang'}, {'name': 'Kristen Marie Johnson'}]",2024-07-21T22:50:11Z
http://arxiv.org/abs/2407.15281v1,http://arxiv.org/abs/2407.15281v1,"SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense
  Persona Knowledge Linking","Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.",[{'name': 'Kuan-Yen Lin'}],2024-07-21T22:07:14Z
http://arxiv.org/abs/2407.15268v1,http://arxiv.org/abs/2407.15268v1,"Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical
  Radiology Report Generation","Multimodal foundation models hold significant potential for automating
radiology report generation, thereby assisting clinicians in diagnosing cardiac
diseases. However, generated reports often suffer from serious factual
inaccuracy. In this paper, we introduce a fact-aware multimodal
retrieval-augmented pipeline in generating accurate radiology reports
(FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then
integrate factual knowledge to train a universal multimodal retriever. Given a
radiology image, our retriever can identify high-quality reference reports to
augment multimodal foundation models, thus enhancing the factual completeness
and correctness of report generation. Experiments on two benchmark datasets
show that our multimodal retriever outperforms state-of-the-art retrievers on
both language generation and radiology-specific metrics, up to 6.5% and 2%
score in F1CheXbert and F1RadGraph. Further analysis indicates that employing
our factually-informed training strategy imposes an effective supervision
signal, without relying on explicit diagnostic label guidance, and successfully
propagates fact-aware capabilities from the multimodal retriever to the
multimodal foundation model in radiology report generation.","[{'name': 'Liwen Sun'}, {'name': 'James Zhao'}, {'name': 'Megan Han'}, {'name': 'Chenyan Xiong'}]",2024-07-21T21:04:28Z
http://arxiv.org/abs/2407.21041v1,http://arxiv.org/abs/2407.21041v1,"They Look Like Each Other: Case-based Reasoning for Explainable
  Depression Detection on Twitter using Large Language Models","Depression is a common mental health issue that requires prompt diagnosis and
treatment. Despite the promise of social media data for depression detection,
the opacity of employed deep learning models hinders interpretability and
raises bias concerns. We address this challenge by introducing ProtoDep, a
novel, explainable framework for Twitter-based depression detection. ProtoDep
leverages prototype learning and the generative power of Large Language Models
to provide transparent explanations at three levels: (i) symptom-level
explanations for each tweet and user, (ii) case-based explanations comparing
the user to similar individuals, and (iii) transparent decision-making through
classification weights. Evaluated on five benchmark datasets, ProtoDep achieves
near state-of-the-art performance while learning meaningful prototypes. This
multi-faceted approach offers significant potential to enhance the reliability
and transparency of depression detection on social media, ultimately aiding
mental health professionals in delivering more informed care.","[{'name': 'Mohammad Saeid Mahdavinejad'}, {'name': 'Peyman Adibi'}, {'name': 'Amirhassan Monadjemi'}, {'name': 'Pascal Hitzler'}]",2024-07-21T20:13:50Z
http://arxiv.org/abs/2407.15248v1,http://arxiv.org/abs/2407.15248v1,"XAI meets LLMs: A Survey of the Relation between Explainable AI and
  Large Language Models","In this survey, we address the key challenges in Large Language Models (LLM)
research, focusing on the importance of interpretability. Driven by increasing
interest from AI and business sectors, we highlight the need for transparency
in LLMs. We examine the dual paths in current LLM research and eXplainable
Artificial Intelligence (XAI): enhancing performance through XAI and the
emerging focus on model interpretability. Our paper advocates for a balanced
approach that values interpretability equally with functional advancements.
Recognizing the rapid development in LLM research, our survey includes both
peer-reviewed and preprint (arXiv) papers, offering a comprehensive overview of
XAI's role in LLM research. We conclude by urging the research community to
advance both LLM and XAI fields together.","[{'name': 'Erik Cambria'}, {'name': 'Lorenzo Malandri'}, {'name': 'Fabio Mercorio'}, {'name': 'Navid Nobani'}, {'name': 'Andrea Seveso'}]",2024-07-21T19:23:45Z
http://arxiv.org/abs/2407.15237v1,http://arxiv.org/abs/2407.15237v1,"Two eyes, Two views, and finally, One summary! Towards Multi-modal
  Multi-tasking Knowledge-Infused Medical Dialogue Summarization","We often summarize a multi-party conversation in two stages: chunking with
homogeneous units and summarizing the chunks. Thus, we hypothesize that there
exists a correlation between homogeneous speaker chunking and overall
summarization tasks. In this work, we investigate the effectiveness of a
multi-faceted approach that simultaneously produces summaries of medical
concerns, doctor impressions, and an overall view. We introduce a multi-modal,
multi-tasking, knowledge-infused medical dialogue summary generation
(MMK-Summation) model, which is incorporated with adapter-based fine-tuning
through a gated mechanism for multi-modal information integration. The model,
MMK-Summation, takes dialogues as input, extracts pertinent external knowledge
based on the context, integrates the knowledge and visual cues from the
dialogues into the textual content, and ultimately generates concise summaries
encompassing medical concerns, doctor impressions, and a comprehensive
overview. The introduced model surpasses multiple baselines and traditional
summarization models across all evaluation metrics (including human
evaluation), which firmly demonstrates the efficacy of the knowledge-guided
multi-tasking, multimodal medical conversation summarization. The code is
available at https://github.com/NLP-RL/MMK-Summation.","[{'name': 'Anisha Saha'}, {'name': 'Abhisek Tiwari'}, {'name': 'Sai Ruthvik'}, {'name': 'Sriparna Saha'}]",2024-07-21T18:00:10Z
http://arxiv.org/abs/2407.15235v1,http://arxiv.org/abs/2407.15235v1,"TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for
  Instruction Tuning Data","Instruction tuning has achieved unprecedented success in NLP, turning large
language models into versatile chatbots. However, the increasing variety and
volume of instruction datasets demand significant computational resources. To
address this, it is essential to extract a small and highly informative subset
(i.e., Coreset) that achieves comparable performance to the full dataset.
Achieving this goal poses non-trivial challenges: 1) data selection requires
accurate data representations that reflect the training samples' quality, 2)
considering the diverse nature of instruction datasets, and 3) ensuring the
efficiency of the coreset selection algorithm for large models. To address
these challenges, we propose Task-Agnostic Gradient Clustered COreset Selection
(TAGCOS). Specifically, we leverage sample gradients as the data
representations, perform clustering to group similar data, and apply an
efficient greedy algorithm for coreset selection. Experimental results show
that our algorithm, selecting only 5% of the data, surpasses other unsupervised
methods and achieves performance close to that of the full dataset.","[{'name': 'Jipeng Zhang'}, {'name': 'Yaxuan Qin'}, {'name': 'Renjie Pi'}, {'name': 'Weizhong Zhang'}, {'name': 'Rui Pan'}, {'name': 'Tong Zhang'}]",2024-07-21T17:59:20Z
http://arxiv.org/abs/2407.15229v1,http://arxiv.org/abs/2407.15229v1,The Hitchhiker's Guide to Human Alignment with *PO,"With the growing utilization of large language models (LLMs) across domains,
alignment towards human preferences has become one of the most critical aspects
of training models. At the forefront of state-of-the-art human alignment
methods are preference optimization methods (*PO). However, prior research has
often concentrated on identifying the best-performing method, typically
involving a grid search over hyperparameters, which can be impractical for
general practitioners. In this paper, we aim to identify the algorithm that,
while being performant, is simultaneously more robust to varying
hyperparameters, thereby increasing the likelihood of achieving better results.
We focus on a realistic out-of-distribution (OOD) scenario that mirrors
real-world applications of human alignment, offering practical insights into
the strengths and weaknesses of these methods. Furthermore, to better
understand the shortcomings of generations from the different methods, we
analyze the model generations through the lens of KL divergence of the SFT
model and the response length statistics. Our analysis reveals that the widely
adopted DPO method consistently produces lengthy responses of inferior quality
that are very close to the SFT responses. Motivated by these findings, we
propose an embarrassingly simple extension to the DPO algorithm, LN-DPO,
resulting in more concise responses without sacrificing quality compared to the
policy obtained by vanilla DPO.","[{'name': 'Kian Ahrabian'}, {'name': 'Xihui Lin'}, {'name': 'Barun Patra'}, {'name': 'Vishrav Chaudhary'}, {'name': 'Alon Benhaim'}, {'name': 'Jay Pujara'}, {'name': 'Xia Song'}]",2024-07-21T17:35:20Z
http://arxiv.org/abs/2407.15227v1,http://arxiv.org/abs/2407.15227v1,"A Community-Centric Perspective for Characterizing and Detecting
  Anti-Asian Violence-Provoking Speech","Violence-provoking speech -- speech that implicitly or explicitly promotes
violence against the members of the targeted community, contributed to a
massive surge in anti-Asian crimes during the pandemic. While previous works
have characterized and built tools for detecting other forms of harmful speech,
like fear speech and hate speech, our work takes a community-centric approach
to studying anti-Asian violence-provoking speech. Using data from ~420k Twitter
posts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we
develop a codebook to characterize anti-Asian violence-provoking speech and
collect a community-crowdsourced dataset to facilitate its large-scale
detection using state-of-the-art classifiers. We contrast the capabilities of
natural language processing classifiers, ranging from BERT-based to LLM-based
classifiers, in detecting violence-provoking speech with their capabilities to
detect anti-Asian hateful speech. In contrast to prior work that has
demonstrated the effectiveness of such classifiers in detecting hateful speech
($F_1 = 0.89$), our work shows that accurate and reliable detection of
violence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the
implications of our findings, particularly the need for proactive interventions
to support Asian communities during public health crises. The resources related
to the study are available at
https://claws-lab.github.io/violence-provoking-speech/.","[{'name': 'Gaurav Verma'}, {'name': 'Rynaa Grover'}, {'name': 'Jiawei Zhou'}, {'name': 'Binny Mathew'}, {'name': 'Jordan Kraemer'}, {'name': 'Munmun De Choudhury'}, {'name': 'Srijan Kumar'}]",2024-07-21T17:27:17Z
http://arxiv.org/abs/2407.15211v1,http://arxiv.org/abs/2407.15211v1,"When Do Universal Image Jailbreaks Transfer Between Vision-Language
  Models?","The integration of new modalities into frontier AI systems offers exciting
capabilities, but also increases the possibility such systems can be
adversarially manipulated in undesirable ways. In this work, we focus on a
popular class of vision-language models (VLMs) that generate text outputs
conditioned on visual and textual inputs. We conducted a large-scale empirical
study to assess the transferability of gradient-based universal image
""jailbreaks"" using a diverse set of over 40 open-parameter VLMs, including 18
new VLMs that we publicly release. Overall, we find that transferable
gradient-based image jailbreaks are extremely difficult to obtain. When an
image jailbreak is optimized against a single VLM or against an ensemble of
VLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits
little-to-no transfer to any other VLMs; transfer is not affected by whether
the attacked and target VLMs possess matching vision backbones or language
models, whether the language model underwent instruction-following and/or
safety-alignment training, or many other factors. Only two settings display
partially successful transfer: between identically-pretrained and
identically-initialized VLMs with slightly different VLM training data, and
between different training checkpoints of a single VLM. Leveraging these
results, we then demonstrate that transfer can be significantly improved
against a specific target VLM by attacking larger ensembles of ""highly-similar""
VLMs. These results stand in stark contrast to existing evidence of universal
and transferable text jailbreaks against language models and transferable
adversarial attacks against image classifiers, suggesting that VLMs may be more
robust to gradient-based transfer attacks.","[{'name': 'Rylan Schaeffer'}, {'name': 'Dan Valentine'}, {'name': 'Luke Bailey'}, {'name': 'James Chua'}, {'name': 'Cristóbal Eyzaguirre'}, {'name': 'Zane Durante'}, {'name': 'Joe Benton'}, {'name': 'Brando Miranda'}, {'name': 'Henry Sleight'}, {'name': 'John Hughes'}, {'name': 'Rajashree Agrawal'}, {'name': 'Mrinank Sharma'}, {'name': 'Scott Emmons'}, {'name': 'Sanmi Koyejo'}, {'name': 'Ethan Perez'}]",2024-07-21T16:27:24Z
http://arxiv.org/abs/2407.15186v2,http://arxiv.org/abs/2407.15186v2,A Survey on Employing Large Language Models for Text-to-SQL Tasks,"The increasing volume of data stored in relational databases has led to the
need for efficient querying and utilization of this data in various sectors.
However, writing SQL queries requires specialized knowledge, which poses a
challenge for non-professional users trying to access and query databases.
Text-to-SQL parsing solves this issue by converting natural language queries
into SQL queries, thus making database access more accessible for non-expert
users. To take advantage of the recent developments in Large Language Models
(LLMs), a range of new methods have emerged, with a primary focus on prompt
engineering and fine-tuning. This survey provides a comprehensive overview of
LLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,
fine-tuning methods, and future research directions. We hope this review will
enable readers to gain a broader understanding of the recent advances in this
field and offer some insights into its future trajectory.","[{'name': 'Liang Shi'}, {'name': 'Zhengju Tang'}, {'name': 'Nan Zhang'}, {'name': 'Xiaotong Zhang'}, {'name': 'Zhi Yang'}]",2024-07-21T14:48:23Z
http://arxiv.org/abs/2407.15184v1,http://arxiv.org/abs/2407.15184v1,"Decoding Multilingual Moral Preferences: Unveiling LLM's Biases Through
  the Moral Machine Experiment","Large language models (LLMs) increasingly find their way into the most
diverse areas of our everyday lives. They indirectly influence people's
decisions or opinions through their daily use. Therefore, understanding how and
which moral judgements these LLMs make is crucial. However, morality is not
universal and depends on the cultural background. This raises the question of
whether these cultural preferences are also reflected in LLMs when prompted in
different languages or whether moral decision-making is consistent across
different languages. So far, most research has focused on investigating the
inherent values of LLMs in English. While a few works conduct multilingual
analyses of moral bias in LLMs in a multilingual setting, these analyses do not
go beyond atomic actions. To the best of our knowledge, a multilingual analysis
of moral bias in dilemmas has not yet been conducted.
  To address this, our paper builds on the moral machine experiment (MME) to
investigate the moral preferences of five LLMs, Falcon, Gemini, Llama, GPT, and
MPT, in a multilingual setting and compares them with the preferences collected
from humans belonging to different cultures. To accomplish this, we generate
6500 scenarios of the MME and prompt the models in ten languages on which
action to take. Our analysis reveals that all LLMs inhibit different moral
biases to some degree and that they not only differ from the human preferences
but also across multiple languages within the models themselves. Moreover, we
find that almost all models, particularly Llama 3, divert greatly from human
values and, for instance, prefer saving fewer people over saving more.","[{'name': 'Karina Vida'}, {'name': 'Fabian Damken'}, {'name': 'Anne Lauscher'}]",2024-07-21T14:48:13Z
http://arxiv.org/abs/2407.15176v1,http://arxiv.org/abs/2407.15176v1,"Farewell to Length Extrapolation, a Training-Free Infinite Context with
  Finite Attention Scope","The maximum supported context length is a critical bottleneck limiting the
practical application of the Large Language Model (LLM). Although existing
length extrapolation methods can extend the context of LLMs to millions of
tokens, these methods all have an explicit upper bound. In this work, we
propose LongCache, a training-free approach that enables LLM to support an
infinite context with finite context scope, through full-context cache
selection and training-free integration. This effectively frees LLMs from the
length extrapolation issue. We validate LongCache on the LongBench and L-Eval
and demonstrate its performance is on par with traditional full-attention
mechanisms. Furthermore, we have applied LongCache on mainstream LLMs,
including LLaMA3 and Mistral-v0.3, enabling them to support context lengths of
at least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of
LongCache by GPU-aware optimization soon.","[{'name': 'Xiaoran Liu'}, {'name': 'Qipeng Guo'}, {'name': 'Yuerong Song'}, {'name': 'Zhigeng Liu'}, {'name': 'Kai Lv'}, {'name': 'Hang Yan'}, {'name': 'Linlin Li'}, {'name': 'Qun Liu'}, {'name': 'Xipeng Qiu'}]",2024-07-21T14:23:37Z
http://arxiv.org/abs/2407.15160v1,http://arxiv.org/abs/2407.15160v1,When Can Transformers Count to n?,"Large language models based on the transformer architectures can solve highly
complex tasks. But are there simple tasks that such models cannot solve? Here
we focus on very simple counting tasks, that involve counting how many times a
token in the vocabulary have appeared in a string. We show that if the
dimension of the transformer state is linear in the context length, this task
can be solved. However, the solution we propose does not scale beyond this
limit, and we provide theoretical arguments for why it is likely impossible for
a size limited transformer to implement this task. Our empirical results
demonstrate the same phase-transition in performance, as anticipated by the
theoretical argument. Our results demonstrate the importance of understanding
how transformers can solve simple tasks.","[{'name': 'Gilad Yehudai'}, {'name': 'Haim Kaplan'}, {'name': 'Asma Ghandeharioun'}, {'name': 'Mor Geva'}, {'name': 'Amir Globerson'}]",2024-07-21T13:31:02Z
http://arxiv.org/abs/2407.15154v1,http://arxiv.org/abs/2407.15154v1,"Fine-grained Gender Control in Machine Translation with Large Language
  Models","In machine translation, the problem of ambiguously gendered input has been
pointed out, where the gender of an entity is not available in the source
sentence. To address this ambiguity issue, the task of controlled translation
that takes the gender of the ambiguous entity as additional input have been
proposed. However, most existing works have only considered a simplified setup
of one target gender for input. In this paper, we tackle controlled translation
in a more realistic setting of inputs with multiple entities and propose
Gender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs
the model with fine-grained entity-level gender information to translate with
correct gender inflections. By utilizing four evaluation benchmarks, we
investigate the controlled translation capability of LLMs in multiple
dimensions and find that LLMs reach state-of-the-art performance in controlled
translation. Furthermore, we discover an emergence of gender interference
phenomenon when controlling the gender of multiple entities. Finally, we
address the limitations of existing gender accuracy evaluation metrics and
propose leveraging LLMs as an evaluator for gender inflection in machine
translation.","[{'name': 'Minwoo Lee'}, {'name': 'Hyukhun Koh'}, {'name': 'Minsung Kim'}, {'name': 'Kyomin Jung'}]",2024-07-21T13:15:00Z
http://arxiv.org/abs/2407.15136v1,http://arxiv.org/abs/2407.15136v1,"A multi-level multi-label text classification dataset of 19th century
  Ottoman and Russian literary and critical texts","This paper introduces a multi-level, multi-label text classification dataset
comprising over 3000 documents. The dataset features literary and critical
texts from 19th-century Ottoman Turkish and Russian. It is the first study to
apply large language models (LLMs) to this dataset, sourced from prominent
literary periodicals of the era. The texts have been meticulously organized and
labeled. This was done according to a taxonomic framework that takes into
account both their structural and semantic attributes. Articles are categorized
and tagged with bibliometric metadata by human experts. We present baseline
classification results using a classical bag-of-words (BoW) naive Bayes model
and three modern LLMs: multilingual BERT, Falcon, and Llama-v2. We found that
in certain cases, Bag of Words (BoW) outperforms Large Language Models (LLMs),
emphasizing the need for additional research, especially in low-resource
language settings. This dataset is expected to be a valuable resource for
researchers in natural language processing and machine learning, especially for
historical and low-resource languages. The dataset is publicly available^1.","[{'name': 'Gokcen Gokceoglu'}, {'name': 'Devrim Cavusoglu'}, {'name': 'Emre Akbas'}, {'name': 'Özen Nergis Dolcerocca'}]",2024-07-21T12:14:45Z
http://arxiv.org/abs/2407.15073v1,http://arxiv.org/abs/2407.15073v1,Multi-Agent Causal Discovery Using Large Language Models,"Large Language Models (LLMs) have demonstrated significant potential in
causal discovery tasks by utilizing their vast expert knowledge from extensive
text corpora. However, the multi-agent capabilities of LLMs in causal discovery
remain underexplored. This paper introduces a general framework to investigate
this potential. The first is the Meta Agents Model, which relies exclusively on
reasoning and discussions among LLM agents to conduct causal discovery. The
second is the Coding Agents Model, which leverages the agents' ability to plan,
write, and execute code, utilizing advanced statistical libraries for causal
discovery. The third is the Hybrid Model, which integrates both the Meta Agents
Model and CodingAgents Model approaches, combining the statistical analysis and
reasoning skills of multiple agents. Our proposed framework shows promising
results by effectively utilizing LLMs expert knowledge, reasoning capabilities,
multi-agent cooperation, and statistical causal methods. By exploring the
multi-agent potential of LLMs, we aim to establish a foundation for further
research in utilizing LLMs multi-agent for solving causal-related problems.","[{'name': 'Hao Duong Le'}, {'name': 'Xin Xia'}, {'name': 'Zhang Chen'}]",2024-07-21T06:21:47Z
http://arxiv.org/abs/2407.15071v1,http://arxiv.org/abs/2407.15071v1,Relational Database Augmented Large Language Model,"Large language models (LLMs) excel in many natural language processing (NLP)
tasks. However, since LLMs can only incorporate new knowledge through training
or supervised fine-tuning processes, they are unsuitable for applications that
demand precise, up-to-date, and private information not available in the
training corpora. This precise, up-to-date, and private information is
typically stored in relational databases. Thus, a promising solution is to
augment LLMs with the inclusion of relational databases as external memory.
This can ensure the timeliness, correctness, and consistency of data, and
assist LLMs in performing complex arithmetic operations beyond their inherent
capabilities. However, bridging the gap between LLMs and relational databases
is challenging. It requires the awareness of databases and data values stored
in databases to select correct databases and issue correct SQL queries.
Besides, it is necessary for the external memory to be independent of the LLM
to meet the needs of real-world applications. We introduce a novel LLM-agnostic
memory architecture comprising a database selection memory, a data value
memory, and relational databases. And we design an elegant pipeline to retrieve
information from it. Besides, we carefully design the prompts to instruct the
LLM to maximize the framework's potential. To evaluate our method, we compose a
new dataset with various types of questions. Experimental results show that our
framework enables LLMs to effectively answer database-related questions, which
is beyond their direct ability.","[{'name': 'Zongyue Qin'}, {'name': 'Chen Luo'}, {'name': 'Zhengyang Wang'}, {'name': 'Haoming Jiang'}, {'name': 'Yizhou Sun'}]",2024-07-21T06:19:10Z
http://arxiv.org/abs/2407.15055v1,http://arxiv.org/abs/2407.15055v1,Natural Language Task-Oriented Dialog System 2.0,"Task-oriented dialog (TOD) systems play a crucial role in facilitating
efficient interactions between users and machines by focusing on achieving
specific goals through natural language communication. These systems
traditionally rely on manually annotated metadata, such as dialog states and
policy annotations, which is labor-intensive, expensive, inconsistent, and
prone to errors, thereby limiting the potential to leverage the vast amounts of
available conversational data. A critical aspect of TOD systems involves
accessing and integrating information from external sources to effectively
engage users. The process of determining when and how to query external
resources represents a fundamental challenge in system design, however existing
approaches expect this information to provided in the context. In this paper,
we introduce Natural Language Task Oriented Dialog System (NL-ToD), a novel
model that removes the dependency on manually annotated turn-wise data by
utilizing dialog history and domain schemas to create a Zero Shot Generalizable
TOD system. We also incorporate query generation as a core task of the system,
where the output of the system could be a response to the user or an API query
to communicate with an external resource. To achieve a more granular analysis
of the system output, we classify the output into multiple categories: slot
filling, retrieval, and query generation. Our analysis reveals that slot
filling is the most challenging TOD task for all models. Experimental results
on three popular TOD datasets (SGD, KETOD and BiToD) shows the effectiveness of
our approach as NL-ToD outperforms state-of-the-art approaches, particularly
with a \textbf{31.4\%} and \textbf{82.1\%} improvement in the BLEU-4 score on
the SGD and KETOD dataset.","[{'name': 'Adib Mosharrof'}, {'name': 'A. B. Siddique'}]",2024-07-21T04:52:38Z
http://arxiv.org/abs/2407.15047v2,http://arxiv.org/abs/2407.15047v2,"End-to-End Video Question Answering with Frame Scoring Mechanisms and
  Adaptive Sampling","Video Question Answering (VideoQA) has emerged as a challenging frontier in
the field of multimedia processing, requiring intricate interactions between
visual and textual modalities. Simply uniformly sampling frames or
indiscriminately aggregating frame-level visual features often falls short in
capturing the nuanced and relevant contexts of videos to well perform VideoQA.
To mitigate these issues, we propose VidF4, a novel VideoQA framework equipped
with tailored frame selection strategy for effective and efficient VideoQA. We
propose three frame-scoring mechanisms that consider both question relevance
and inter-frame similarity to evaluate the importance of each frame for a given
question on the video. Furthermore, we design a differentiable adaptive frame
sampling mechanism to facilitate end-to-end training for the frame selector and
answer generator. The experimental results across three widely adopted
benchmarks demonstrate that our model consistently outperforms existing VideoQA
methods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA
(+1.0%). Furthermore, through both quantitative and qualitative analyses, we
validate the effectiveness of each design choice.","[{'name': 'Jianxin Liang'}, {'name': 'Xiaojun Meng'}, {'name': 'Yueqian Wang'}, {'name': 'Chang Liu'}, {'name': 'Qun Liu'}, {'name': 'Dongyan Zhao'}]",2024-07-21T04:09:37Z
http://arxiv.org/abs/2407.15046v1,http://arxiv.org/abs/2407.15046v1,Audio-visual training for improved grounding in video-text LLMs,"Recent advances in multimodal LLMs, have led to several video-text models
being proposed for critical video-related tasks. However, most of the previous
works support visual input only, essentially muting the audio signal in the
video. Few models that support both audio and visual input, are not explicitly
trained on audio data. Hence, the effect of audio towards video understanding
is largely unexplored. To this end, we propose a model architecture that
handles audio-visual inputs explicitly. We train our model with both audio and
visual data from a video instruction-tuning dataset. Comparison with
vision-only baselines, and other audio-visual models showcase that training on
audio data indeed leads to improved grounding of responses. For better
evaluation of audio-visual models, we also release a human-annotated benchmark
dataset, with audio-aware question-answer pairs.","[{'name': 'Shivprasad Sagare'}, {'name': 'Hemachandran S'}, {'name': 'Kinshuk Sarabhai'}, {'name': 'Prashant Ullegaddi'}, {'name': 'Rajeshkumar SA'}]",2024-07-21T03:59:14Z
http://arxiv.org/abs/2407.15021v1,http://arxiv.org/abs/2407.15021v1,Enhancing Incremental Summarization with Structured Representations,"Large language models (LLMs) often struggle with processing extensive input
contexts, which can lead to redundant, inaccurate, or incoherent summaries.
Recent methods have used unstructured memory to incrementally process these
contexts, but they still suffer from information overload due to the volume of
unstructured data handled. In our study, we introduce structured knowledge
representations ($GU_{json}$), which significantly improve summarization
performance by 40% and 14% across two public datasets. Most notably, we propose
the Chain-of-Key strategy ($CoK_{json}$) that dynamically updates or augments
these representations with new information, rather than recreating the
structured memory for each new source. This method further enhances performance
by 7% and 4% on the datasets.","[{'name': 'EunJeong Hwang'}, {'name': 'Yichao Zhou'}, {'name': 'James Bradley Wendt'}, {'name': 'Beliz Gunel'}, {'name': 'Nguyen Vo'}, {'name': 'Jing Xie'}, {'name': 'Sandeep Tata'}]",2024-07-21T00:23:33Z
http://arxiv.org/abs/2407.15018v1,http://arxiv.org/abs/2407.15018v1,"Answer, Assemble, Ace: Understanding How Transformers Answer Multiple
  Choice Questions","Multiple-choice question answering (MCQA) is a key competence of performant
transformer language models that is tested by mainstream benchmarks. However,
recent evidence shows that models can have quite a range of performance,
particularly when the task format is diversified slightly (such as by shuffling
answer choice order). In this work we ask: how do successful models perform
formatted MCQA? We employ vocabulary projection and activation patching methods
to localize key hidden states that encode relevant information for predicting
the correct answer. We find that prediction of a specific answer symbol is
causally attributed to a single middle layer, and specifically its multi-head
self-attention mechanism. We show that subsequent layers increase the
probability of the predicted answer symbol in vocabulary space, and that this
probability increase is associated with a sparse set of attention heads with
unique roles. We additionally uncover differences in how different models
adjust to alternative symbols. Finally, we demonstrate that a synthetic task
can disentangle sources of model error to pinpoint when a model has learned
formatted MCQA, and show that an inability to separate answer symbol tokens in
vocabulary space is a property of models unable to perform formatted MCQA
tasks.","[{'name': 'Sarah Wiegreffe'}, {'name': 'Oyvind Tafjord'}, {'name': 'Yonatan Belinkov'}, {'name': 'Hannaneh Hajishirzi'}, {'name': 'Ashish Sabharwal'}]",2024-07-21T00:10:23Z
http://arxiv.org/abs/2407.14997v1,http://arxiv.org/abs/2407.14997v1,"Improving Citation Text Generation: Overcoming Limitations in Length
  Control","A key challenge in citation text generation is that the length of generated
text often differs from the length of the target, lowering the quality of the
generation. While prior works have investigated length-controlled generation,
their effectiveness depends on knowing the appropriate generation length. In
this work, we present an in-depth study of the limitations of predicting
scientific citation text length and explore the use of heuristic estimates of
desired length.","[{'name': 'Biswadip Mandal'}, {'name': 'Xiangci Li'}, {'name': 'Jessica Ouyang'}]",2024-07-20T22:10:37Z
http://arxiv.org/abs/2407.14985v1,http://arxiv.org/abs/2407.14985v1,"Generalization v.s. Memorization: Tracing Language Models' Capabilities
  Back to Pretraining Data","Despite the proven utility of large language models (LLMs) in real-world
applications, there remains a lack of understanding regarding how they leverage
their large-scale pretraining text corpora to achieve such capabilities. In
this work, we investigate the interplay between generalization and memorization
in pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their
training data. Our experiments focus on three general task types: translation,
question-answering, and multiple-choice reasoning. With various sizes of
open-source LLMs and their pretraining corpora, we observe that as the model
size increases, the task-relevant $n$-gram pair data becomes increasingly
important, leading to improved task performance, decreased memorization,
stronger generalization, and emergent abilities. Our results support the
hypothesis that LLMs' capabilities emerge from a delicate balance of
memorization and generalization with sufficient task-related pretraining data,
and point the way to larger-scale analyses that could further improve our
understanding of these models.","[{'name': 'Antonis Antoniades'}, {'name': 'Xinyi Wang'}, {'name': 'Yanai Elazar'}, {'name': 'Alfonso Amayuelas'}, {'name': 'Alon Albalak'}, {'name': 'Kexun Zhang'}, {'name': 'William Yang Wang'}]",2024-07-20T21:24:40Z
http://arxiv.org/abs/2407.14971v1,http://arxiv.org/abs/2407.14971v1,"Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and
  Semantically-Rich Vision-Language Models","Vision-language models (VLMs) have achieved significant strides in recent
times specially in multimodal tasks, yet they remain susceptible to adversarial
attacks on their vision components. To address this, we propose Sim-CLIP, an
unsupervised adversarial fine-tuning method that enhances the robustness of the
widely-used CLIP vision encoder against such attacks while maintaining semantic
richness and specificity. By employing a Siamese architecture with cosine
similarity loss, Sim-CLIP learns semantically meaningful and attack-resilient
visual representations without requiring large batch sizes or momentum
encoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned
CLIP encoder exhibit significantly enhanced robustness against adversarial
attacks, while preserving semantic meaning of the perturbed images. Notably,
Sim-CLIP does not require additional training or fine-tuning of the VLM itself;
replacing the original vision encoder with our fine-tuned Sim-CLIP suffices to
provide robustness. This work underscores the significance of reinforcing
foundational models like CLIP to safeguard the reliability of downstream VLM
applications, paving the way for more secure and effective multimodal systems.","[{'name': 'Md Zarif Hossain'}, {'name': 'Ahmed Imteaj'}]",2024-07-20T19:53:52Z
http://arxiv.org/abs/2407.14962v4,http://arxiv.org/abs/2407.14962v4,"Recent Advances in Generative AI and Large Language Models: Current
  Status, Challenges, and Perspectives","The emergence of Generative Artificial Intelligence (AI) and Large Language
Models (LLMs) has marked a new era of Natural Language Processing (NLP),
introducing unprecedented capabilities that are revolutionizing various
domains. This paper explores the current state of these cutting-edge
technologies, demonstrating their remarkable advancements and wide-ranging
applications. Our paper contributes to providing a holistic perspective on the
technical foundations, practical applications, and emerging challenges within
the evolving landscape of Generative AI and LLMs. We believe that understanding
the generative capabilities of AI systems and the specific context of LLMs is
crucial for researchers, practitioners, and policymakers to collaboratively
shape the responsible and ethical integration of these technologies into
various domains. Furthermore, we identify and address main research gaps,
providing valuable insights to guide future research endeavors within the AI
research community.","[{'name': 'Desta Haileselassie Hagos'}, {'name': 'Rick Battle'}, {'name': 'Danda B. Rawat'}]",2024-07-20T18:48:35Z
http://arxiv.org/abs/2407.17522v1,http://arxiv.org/abs/2407.17522v1,"Mapping the Technological Future: A Topic, Sentiment, and Emotion
  Analysis in Social Media Discourse","People worldwide are currently confronted with a number of technological
challenges, which act as a potent source of uncertainty. The uncertainty
arising from the volatility and unpredictability of technology (such as AI) and
its potential consequences is widely discussed on social media. This study uses
BERTopic modelling along with sentiment and emotion analysis on 1.5 million
tweets from 2021 to 2023 to identify anticipated tech-driven futures and
capture the emotions communicated by 400 key opinion leaders (KOLs). Findings
indicate positive sentiment significantly outweighs negative, with a prevailing
dominance of positive anticipatory emotions. Specifically, the 'Hope' score is
approximately 10.33\% higher than the median 'Anxiety' score. KOLs emphasize
'Optimism' and benefits over 'Pessimism' and challenges. The study emphasizes
the important role KOLs play in shaping future visions through anticipatory
discourse and emotional tone during times of technological uncertainty.","[{'name': 'Alina Landowska'}, {'name': 'Maciej Skorski'}, {'name': 'Krzysztof Rajda'}]",2024-07-20T18:15:30Z
http://arxiv.org/abs/2407.14940v1,http://arxiv.org/abs/2407.14940v1,"Conversational Rubert for Detecting Competitive Interruptions in
  ASR-Transcribed Dialogues","Interruption in a dialogue occurs when the listener begins their speech
before the current speaker finishes speaking. Interruptions can be broadly
divided into two groups: cooperative (when the listener wants to support the
speaker), and competitive (when the listener tries to take control of the
conversation against the speaker's will). A system that automatically
classifies interruptions can be used in call centers, specifically in the tasks
of customer satisfaction monitoring and agent monitoring. In this study, we
developed a text-based interruption classification model by preparing an
in-house dataset consisting of ASR-transcribed customer support telephone
dialogues in Russian. We fine-tuned Conversational RuBERT on our dataset and
optimized hyperparameters, and the model performed well. With further
improvements, the proposed model can be applied to automatic monitoring
systems.","[{'name': 'Dmitrii Galimzianov'}, {'name': 'Viacheslav Vyshegorodtsev'}]",2024-07-20T17:25:53Z
http://arxiv.org/abs/2407.14937v1,http://arxiv.org/abs/2407.14937v1,"Operationalizing a Threat Model for Red-Teaming Large Language Models
  (LLMs)","Creating secure and resilient applications with large language models (LLM)
requires anticipating, adjusting to, and countering unforeseen threats.
Red-teaming has emerged as a critical technique for identifying vulnerabilities
in real-world LLM implementations. This paper presents a detailed threat model
and provides a systematization of knowledge (SoK) of red-teaming attacks on
LLMs. We develop a taxonomy of attacks based on the stages of the LLM
development and deployment process and extract various insights from previous
research. In addition, we compile methods for defense and practical red-teaming
strategies for practitioners. By delineating prominent attack motifs and
shedding light on various entry points, this paper provides a framework for
improving the security and robustness of LLM-based systems.","[{'name': 'Apurv Verma'}, {'name': 'Satyapriya Krishna'}, {'name': 'Sebastian Gehrmann'}, {'name': 'Madhavan Seshadri'}, {'name': 'Anu Pradhan'}, {'name': 'Tom Ault'}, {'name': 'Leslie Barrett'}, {'name': 'David Rabinowitz'}, {'name': 'John Doucette'}, {'name': 'NhatHai Phan'}]",2024-07-20T17:05:04Z
http://arxiv.org/abs/2407.14933v2,http://arxiv.org/abs/2407.14933v2,Consent in Crisis: The Rapid Decline of the AI Data Commons,"General-purpose artificial intelligence (AI) systems are built on massive
swathes of public web data, assembled into corpora such as C4, RefinedWeb, and
Dolma. To our knowledge, we conduct the first, large-scale, longitudinal audit
of the consent protocols for the web domains underlying AI training corpora.
Our audit of 14,000 web domains provides an expansive view of crawlable web
data and how codified data use preferences are changing over time. We observe a
proliferation of AI-specific clauses to limit use, acute differences in
restrictions on AI developers, as well as general inconsistencies between
websites' expressed intentions in their Terms of Service and their robots.txt.
We diagnose these as symptoms of ineffective web protocols, not designed to
cope with the widespread re-purposing of the internet for AI. Our longitudinal
analyses show that in a single year (2023-2024) there has been a rapid
crescendo of data restrictions from web sources, rendering ~5%+ of all tokens
in C4, or 28%+ of the most actively maintained, critical sources in C4, fully
restricted from use. For Terms of Service crawling restrictions, a full 45% of
C4 is now restricted. If respected or enforced, these restrictions are rapidly
biasing the diversity, freshness, and scaling laws for general-purpose AI
systems. We hope to illustrate the emerging crises in data consent, for both
developers and creators. The foreclosure of much of the open web will impact
not only commercial AI, but also non-commercial AI and academic research.","[{'name': 'Shayne Longpre'}, {'name': 'Robert Mahari'}, {'name': 'Ariel Lee'}, {'name': 'Campbell Lund'}, {'name': 'Hamidah Oderinwale'}, {'name': 'William Brannon'}, {'name': 'Nayan Saxena'}, {'name': 'Naana Obeng-Marnu'}, {'name': 'Tobin South'}, {'name': 'Cole Hunter'}, {'name': 'Kevin Klyman'}, {'name': 'Christopher Klamm'}, {'name': 'Hailey Schoelkopf'}, {'name': 'Nikhil Singh'}, {'name': 'Manuel Cherep'}, {'name': 'Ahmad Anis'}, {'name': 'An Dinh'}, {'name': 'Caroline Chitongo'}, {'name': 'Da Yin'}, {'name': 'Damien Sileo'}, {'name': 'Deividas Mataciunas'}, {'name': 'Diganta Misra'}, {'name': 'Emad Alghamdi'}, {'name': 'Enrico Shippole'}, {'name': 'Jianguo Zhang'}, {'name': 'Joanna Materzynska'}, {'name': 'Kun Qian'}, {'name': 'Kush Tiwary'}, {'name': 'Lester Miranda'}, {'name': 'Manan Dey'}, {'name': 'Minnie Liang'}, {'name': 'Mohammed Hamdy'}, {'name': 'Niklas Muennighoff'}, {'name': 'Seonghyeon Ye'}, {'name': 'Seungone Kim'}, {'name': 'Shrestha Mohanty'}, {'name': 'Vipul Gupta'}, {'name': 'Vivek Sharma'}, {'name': 'Vu Minh Chien'}, {'name': 'Xuhui Zhou'}, {'name': 'Yizhi Li'}, {'name': 'Caiming Xiong'}, {'name': 'Luis Villa'}, {'name': 'Stella Biderman'}, {'name': 'Hanlin Li'}, {'name': 'Daphne Ippolito'}, {'name': 'Sara Hooker'}, {'name': 'Jad Kabbara'}, {'name': 'Sandy Pentland'}]",2024-07-20T16:50:18Z
http://arxiv.org/abs/2407.14916v1,http://arxiv.org/abs/2407.14916v1,Improving Context-Aware Preference Modeling for Language Models,"While finetuning language models from pairwise preferences has proven
remarkably effective, the underspecified nature of natural language presents
critical challenges. Direct preference feedback is uninterpretable, difficult
to provide where multidimensional criteria may apply, and often inconsistent,
either because it is based on incomplete instructions or provided by diverse
principals. To address these challenges, we consider the two-step preference
modeling procedure that first resolves the under-specification by selecting a
context, and then evaluates preference with respect to the chosen context. We
decompose reward modeling error according to these two steps, which suggests
that supervising context in addition to context-specific preference may be a
viable approach to aligning models with diverse human preferences. For this to
work, the ability of models to evaluate context-specific preference is
critical. To this end, we contribute context-conditioned preference datasets
and accompanying experiments that investigate the ability of language models to
evaluate context-specific preference. We use our datasets to (1) show that
existing preference models benefit from, but fail to fully consider, added
context, (2) finetune a context-aware reward model with context-specific
performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)
investigate the value of context-aware preference modeling.","[{'name': 'Silviu Pitis'}, {'name': 'Ziang Xiao'}, {'name': 'Nicolas Le Roux'}, {'name': 'Alessandro Sordoni'}]",2024-07-20T16:05:17Z
http://arxiv.org/abs/2407.14904v1,http://arxiv.org/abs/2407.14904v1,"Large-vocabulary forensic pathological analyses via prototypical
  cross-modal contrastive learning","Forensic pathology is critical in determining the cause and manner of death
through post-mortem examinations, both macroscopic and microscopic. The field,
however, grapples with issues such as outcome variability, laborious processes,
and a scarcity of trained professionals. This paper presents SongCi, an
innovative visual-language model (VLM) designed specifically for forensic
pathology. SongCi utilizes advanced prototypical cross-modal self-supervised
contrastive learning to enhance the accuracy, efficiency, and generalizability
of forensic analyses. It was pre-trained and evaluated on a comprehensive
multi-center dataset, which includes over 16 million high-resolution image
patches, 2,228 vision-language pairs of post-mortem whole slide images (WSIs),
and corresponding gross key findings, along with 471 distinct diagnostic
outcomes. Our findings indicate that SongCi surpasses existing multi-modal AI
models in many forensic pathology tasks, performs comparably to experienced
forensic pathologists and significantly better than less experienced ones, and
provides detailed multi-modal explainability, offering critical assistance in
forensic investigations. To the best of our knowledge, SongCi is the first VLM
specifically developed for forensic pathological analysis and the first
large-vocabulary computational pathology (CPath) model that directly processes
gigapixel WSIs in forensic science.","[{'name': 'Chen Shen'}, {'name': 'Chunfeng Lian'}, {'name': 'Wanqing Zhang'}, {'name': 'Fan Wang'}, {'name': 'Jianhua Zhang'}, {'name': 'Shuanliang Fan'}, {'name': 'Xin Wei'}, {'name': 'Gongji Wang'}, {'name': 'Kehan Li'}, {'name': 'Hongshu Mu'}, {'name': 'Hao Wu'}, {'name': 'Xinggong Liang'}, {'name': 'Jianhua Ma'}, {'name': 'Zhenyuan Wang'}]",2024-07-20T15:34:52Z
http://arxiv.org/abs/2407.21039v1,http://arxiv.org/abs/2407.21039v1,"Mapping Patient Trajectories: Understanding and Visualizing Sepsis
  Prognostic Pathways from Patients Clinical Narratives","In recent years, healthcare professionals are increasingly emphasizing on
personalized and evidence-based patient care through the exploration of
prognostic pathways. To study this, structured clinical variables from
Electronic Health Records (EHRs) data have traditionally been employed by many
researchers. Presently, Natural Language Processing models have received great
attention in clinical research which expanded the possibilities of using
clinical narratives. In this paper, we propose a systematic methodology for
developing sepsis prognostic pathways derived from clinical notes, focusing on
diverse patient subgroups identified by exploring comorbidities associated with
sepsis and generating explanations of these subgroups using SHAP. The extracted
prognostic pathways of these subgroups provide valuable insights into the
dynamic trajectories of sepsis severity over time. Visualizing these pathways
sheds light on the likelihood and direction of disease progression across
various contexts and reveals patterns and pivotal factors or biomarkers
influencing the transition between sepsis stages, whether toward deterioration
or improvement. This empowers healthcare providers to implement more
personalized and effective healthcare strategies for individual patients.","[{'name': 'Sudeshna Jana'}, {'name': 'Tirthankar Dasgupta'}, {'name': 'Lipika Dey'}]",2024-07-20T14:45:55Z
http://arxiv.org/abs/2407.14885v1,http://arxiv.org/abs/2407.14885v1,Falcon2-11B Technical Report,"We introduce Falcon2-11B, a foundation model trained on over five trillion
tokens, and its multimodal counterpart, Falcon2-11B-vlm, which is a
vision-to-text model. We report our findings during the training of the
Falcon2-11B which follows a multi-stage approach where the early stages are
distinguished by their context length and a final stage where we use a curated,
high-quality dataset. Additionally, we report the effect of doubling the batch
size mid-training and how training loss spikes are affected by the learning
rate. The downstream performance of the foundation model is evaluated on
established benchmarks, including multilingual and code datasets. The
foundation model shows strong generalization across all the tasks which makes
it suitable for downstream finetuning use cases. For the vision language model,
we report the performance on several benchmarks and show that our model
achieves a higher average score compared to open-source models of similar size.
The model weights and code of both Falcon2-11B and Falcon2-11B-vlm are made
available under a permissive license.","[{'name': 'Quentin Malartic'}, {'name': 'Nilabhra Roy Chowdhury'}, {'name': 'Ruxandra Cojocaru'}, {'name': 'Mugariya Farooq'}, {'name': 'Giulia Campesan'}, {'name': 'Yasser Abdelaziz Dahou Djilali'}, {'name': 'Sanath Narayan'}, {'name': 'Ankit Singh'}, {'name': 'Maksim Velikanov'}, {'name': 'Basma El Amel Boussaha'}, {'name': 'Mohammed Al-Yafeai'}, {'name': 'Hamza Alobeidli'}, {'name': 'Leen Al Qadi'}, {'name': 'Mohamed El Amine Seddik'}, {'name': 'Kirill Fedyanin'}, {'name': 'Reda Alami'}, {'name': 'Hakim Hacid'}]",2024-07-20T14:23:15Z
http://arxiv.org/abs/2407.14878v1,http://arxiv.org/abs/2407.14878v1,"Modular Sentence Encoders: Separating Language Specialization from
  Cross-Lingual Alignment","Multilingual sentence encoders are commonly obtained by training multilingual
language models to map sentences from different languages into a shared
semantic space. As such, they are subject to curse of multilinguality, a loss
of monolingual representational accuracy due to parameter sharing. Another
limitation of multilingual sentence encoders is the trade-off between
monolingual and cross-lingual performance. Training for cross-lingual alignment
of sentence embeddings distorts the optimal monolingual structure of semantic
spaces of individual languages, harming the utility of sentence embeddings in
monolingual tasks. In this work, we address both issues by modular training of
sentence encoders, i.e., by separating monolingual specialization from
cross-lingual alignment. We first efficiently train language-specific sentence
encoders to avoid negative interference between languages (i.e., the curse). We
then align all non-English monolingual encoders to the English encoder by
training a cross-lingual alignment adapter on top of each, preventing
interference with monolingual specialization from the first step. In both
steps, we resort to contrastive learning on machine-translated paraphrase data.
Monolingual and cross-lingual evaluations on semantic text
similarity/relatedness and multiple-choice QA render our modular solution more
effective than multilingual sentence encoders, especially benefiting
low-resource languages.","[{'name': 'Yongxin Huang'}, {'name': 'Kexin Wang'}, {'name': 'Goran Glavaš'}, {'name': 'Iryna Gurevych'}]",2024-07-20T13:56:39Z
http://arxiv.org/abs/2407.14875v1,http://arxiv.org/abs/2407.14875v1,Seal: Advancing Speech Language Models to be Few-Shot Learners,"Existing auto-regressive language models have demonstrated a remarkable
capability to perform a new task with just a few examples in prompt, without
requiring any additional training. In order to extend this capability to a
multi-modal setting (i.e. speech and language), this paper introduces the Seal
model, an abbreviation for speech language model. It incorporates a novel
alignment method, in which Kullback-Leibler divergence loss is performed to
train a projector that bridges a frozen speech encoder with a frozen language
model decoder. The resulting Seal model exhibits robust performance as a
few-shot learner on two speech understanding tasks. Additionally, consistency
experiments are conducted to validate its robustness on different pre-trained
language models.","[{'name': 'Shuyu Lei'}, {'name': 'Lingen Liu'}, {'name': 'Jiaolong Yang'}, {'name': 'Yasen Jiao'}, {'name': 'Yuxiang Yang'}, {'name': 'Yushu Yang'}, {'name': 'Xiang Guo'}]",2024-07-20T13:28:12Z
http://arxiv.org/abs/2407.14845v1,http://arxiv.org/abs/2407.14845v1,"Understanding the Relationship between Prompts and Response Uncertainty
  in Large Language Models","Large language models (LLMs) are widely used in decision-making, but their
reliability, especially in critical tasks like healthcare, is not
well-established. Therefore, understanding how LLMs reason and make decisions
is crucial for their safe deployment. This paper investigates how the
uncertainty of responses generated by LLMs relates to the information provided
in the input prompt. Leveraging the insight that LLMs learn to infer latent
concepts during pretraining, we propose a prompt-response concept model that
explains how LLMs generate responses and helps understand the relationship
between prompts and response uncertainty. We show that the uncertainty
decreases as the prompt's informativeness increases, similar to epistemic
uncertainty. Our detailed experimental results on real datasets validate our
proposed model.","[{'name': 'Ze Yu Zhang'}, {'name': 'Arun Verma'}, {'name': 'Finale Doshi-Velez'}, {'name': 'Bryan Kian Hsiang Low'}]",2024-07-20T11:19:58Z
http://arxiv.org/abs/2407.14829v2,http://arxiv.org/abs/2407.14829v2,Overview of AI-Debater 2023: The Challenges of Argument Generation Tasks,"In this paper we present the results of the AI-Debater 2023 Challenge held by
the Chinese Conference on Affect Computing (CCAC 2023), and introduce the
related datasets. We organize two tracks to handle the argumentative generation
tasks in different scenarios, namely, Counter-Argument Generation (Track 1) and
Claim-based Argument Generation (Track 2). Each track is equipped with its
distinct dataset and baseline model respectively. In total, 32 competing teams
register for the challenge, from which we received 11 successful submissions.
In this paper, we will present the results of the challenge and a summary of
the systems, highlighting commonalities and innovations among participating
systems. Datasets and baseline models of the AI-Debater 2023 Challenge have
been already released and can be accessed through the official website of the
challenge.","[{'name': 'Jiayu Lin'}, {'name': 'Guanrong Chen'}, {'name': 'Bojun Jin'}, {'name': 'Chenyang Li'}, {'name': 'Shutong Jia'}, {'name': 'Wancong Lin'}, {'name': 'Yang Sun'}, {'name': 'Yuhang He'}, {'name': 'Caihua Yang'}, {'name': 'Jianzhu Bao'}, {'name': 'Jipeng Wu'}, {'name': 'Wen Su'}, {'name': 'Jinglu Chen'}, {'name': 'Xinyi Li'}, {'name': 'Tianyu Chen'}, {'name': 'Mingjie Han'}, {'name': 'Shuaiwen Du'}, {'name': 'Zijian Wang'}, {'name': 'Jiyin Li'}, {'name': 'Fuzhong Suo'}, {'name': 'Hao Wang'}, {'name': 'Nuanchen Lin'}, {'name': 'Xuanjing Huang'}, {'name': 'Changjian Jiang'}, {'name': 'RuiFeng Xu'}, {'name': 'Long Zhang'}, {'name': 'Jiuxin Cao'}, {'name': 'Ting Jin'}, {'name': 'Zhongyu Wei'}]",2024-07-20T10:13:54Z
http://arxiv.org/abs/2407.14822v1,http://arxiv.org/abs/2407.14822v1,Text Style Transfer: An Introductory Overview,"Text Style Transfer (TST) is a pivotal task in natural language generation to
manipulate text style attributes while preserving style-independent content.
The attributes targeted in TST can vary widely, including politeness,
authorship, mitigation of offensive language, modification of feelings, and
adjustment of text formality. TST has become a widely researched topic with
substantial advancements in recent years. This paper provides an introductory
overview of TST, addressing its challenges, existing approaches, datasets,
evaluation measures, subtasks, and applications. This fundamental overview
improves understanding of the background and fundamentals of text style
transfer.","[{'name': 'Sourabrata Mukherjee'}, {'name': 'Ondrej Dušek'}]",2024-07-20T09:54:55Z
http://arxiv.org/abs/2407.14795v1,http://arxiv.org/abs/2407.14795v1,Automatic Real-word Error Correction in Persian Text,"Automatic spelling correction stands as a pivotal challenge within the ambit
of natural language processing (NLP), demanding nuanced solutions. Traditional
spelling correction techniques are typically only capable of detecting and
correcting non-word errors, such as typos and misspellings. However,
context-sensitive errors, also known as real-word errors, are more challenging
to detect because they are valid words that are used incorrectly in a given
context. The Persian language, characterized by its rich morphology and complex
syntax, presents formidable challenges to automatic spelling correction
systems. Furthermore, the limited availability of Persian language resources
makes it difficult to train effective spelling correction models. This paper
introduces a cutting-edge approach for precise and efficient real-word error
correction in Persian text. Our methodology adopts a structured, multi-tiered
approach, employing semantic analysis, feature selection, and advanced
classifiers to enhance error detection and correction efficacy. The innovative
architecture discovers and stores semantic similarities between words and
phrases in Persian text. The classifiers accurately identify real-word errors,
while the semantic ranking algorithm determines the most probable corrections
for real-word errors, taking into account specific spelling correction and
context properties such as context, semantic similarity, and edit-distance
measures. Evaluations have demonstrated that our proposed method surpasses
previous Persian real-word error correction models. Our method achieves an
impressive F-measure of 96.6% in the detection phase and an accuracy of 99.1%
in the correction phase. These results clearly indicate that our approach is a
highly promising solution for automatic real-word error correction in Persian
text.","[{'name': 'Seyed Mohammad Sadegh Dashti'}, {'name': 'Amid Khatibi Bardsiri'}, {'name': 'Mehdi Jafari Shahbazzadeh'}]",2024-07-20T07:50:52Z
http://arxiv.org/abs/2407.14790v1,http://arxiv.org/abs/2407.14790v1,Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?,"Solving grid puzzles involves a significant amount of logical reasoning.
Hence, it is a good domain to evaluate the reasoning capability of a model
which can then guide us to improve the reasoning ability of models. However,
most existing works evaluate only the final predicted answer of a puzzle,
without delving into an in-depth analysis of the LLMs' reasoning chains (such
as where they falter) or providing any finer metrics to evaluate them. Since
LLMs may rely on simple heuristics or artifacts to predict the final answer, it
is crucial to evaluate the generated reasoning chain beyond overall correctness
measures, for accurately evaluating the reasoning abilities of LLMs. To this
end, we first develop GridPuzzle, an evaluation dataset comprising 274
grid-based puzzles with different complexities. Second, we propose a new error
taxonomy derived from manual analysis of reasoning chains from LLMs including
GPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop an LLM-based
framework for large-scale subjective evaluation (i.e., identifying errors) and
an objective metric, PuzzleEval, to evaluate the correctness of reasoning
chains. Evaluating reasoning chains from LLMs leads to several interesting
findings. We further show that existing prompting methods used for enhancing
models' reasoning abilities do not improve performance on GridPuzzle. This
highlights the importance of understanding fine-grained errors and presents a
challenge for future research to enhance LLMs' puzzle-solving abilities by
developing methods that address these errors. Data and source code are
available at https://github.com/Mihir3009/GridPuzzle.","[{'name': 'Nemika Tyagi'}, {'name': 'Mihir Parmar'}, {'name': 'Mohith Kulkarni'}, {'name': 'Aswin RRV'}, {'name': 'Nisarg Patel'}, {'name': 'Mutsumi Nakamura'}, {'name': 'Arindam Mitra'}, {'name': 'Chitta Baral'}]",2024-07-20T07:43:07Z
http://arxiv.org/abs/2407.14789v1,http://arxiv.org/abs/2407.14789v1,"PERCORE: A Deep Learning-Based Framework for Persian Spelling Correction
  with Phonetic Analysis","This research introduces a state-of-the-art Persian spelling correction
system that seamlessly integrates deep learning techniques with phonetic
analysis, significantly enhancing the accuracy and efficiency of natural
language processing (NLP) for Persian. Utilizing a fine-tuned language
representation model, our methodology effectively combines deep contextual
analysis with phonetic insights, adeptly correcting both non-word and real-word
spelling errors. This strategy proves particularly effective in tackling the
unique complexities of Persian spelling, including its elaborate morphology and
the challenge of homophony. A thorough evaluation on a wide-ranging dataset
confirms our system's superior performance compared to existing methods, with
impressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for
correcting them. Additionally, the system demonstrates a strong capability in
non-word error correction, achieving an F1-Score of 0.891. These results
illustrate the significant benefits of incorporating phonetic insights into
deep learning models for spelling correction. Our contributions not only
advance Persian language processing by providing a versatile solution for a
variety of NLP applications but also pave the way for future research in the
field, emphasizing the critical role of phonetic analysis in developing
effective spelling correction system.","[{'name': 'Seyed Mohammad Sadegh Dashti'}, {'name': 'Amid Khatibi Bardsiri'}, {'name': 'Mehdi Jafari Shahbazzadeh'}]",2024-07-20T07:41:04Z
http://arxiv.org/abs/2407.14788v1,http://arxiv.org/abs/2407.14788v1,On the Design and Analysis of LLM-Based Algorithms,"We initiate a formal investigation into the design and analysis of LLM-based
algorithms, i.e. algorithms that contain one or multiple calls of large
language models (LLMs) as sub-routines and critically rely on the capabilities
of LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt
engineering to complicated LLM-powered agent systems and compound AI systems,
have achieved remarkable empirical success, the design and optimization of them
have mostly relied on heuristics and trial-and-errors, which is largely due to
a lack of formal and analytical study for these algorithms. To fill this gap,
we start by identifying the computational-graph representation of LLM-based
algorithms, the design principle of task decomposition, and some key
abstractions, which then facilitate our formal analysis for the accuracy and
efficiency of LLM-based algorithms, despite the black-box nature of LLMs. We
further consider parallel decomposition for a case study, providing extensive
analytical and empirical study for four concrete examples of this pattern. Our
proposed framework holds promise for advancing LLM-based algorithms, by
revealing the reasons behind curious empirical phenomena, guiding the choices
of hyperparameters, predicting the empirical performance of algorithms, and
inspiring new algorithm design. To promote further study of LLM-based
algorithms, we release our source code at
https://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.","[{'name': 'Yanxi Chen'}, {'name': 'Yaliang Li'}, {'name': 'Bolin Ding'}, {'name': 'Jingren Zhou'}]",2024-07-20T07:39:07Z
http://arxiv.org/abs/2407.14767v1,http://arxiv.org/abs/2407.14767v1,"I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case
  Study on Text-to-SQL Generation","In this study, we explore the proactive ability of LLMs to seek user support,
using text-to-SQL generation as a case study. We propose metrics to evaluate
the trade-off between performance improvements and user burden, and investigate
whether LLMs can determine when to request help and examine their performance
with varying levels of information availability. Our experiments reveal that
without external feedback, many LLMs struggle to recognize their need for
additional support. Our findings highlight the importance of external signals
and provide insights for future research on improving support-seeking
strategies.","[{'name': 'Cheng-Kuang Wu'}, {'name': 'Zhi Rui Tam'}, {'name': 'Chao-Chung Wu'}, {'name': 'Chieh-Yen Lin'}, {'name': 'Hung-yi Lee'}, {'name': 'Yun-Nung Chen'}]",2024-07-20T06:12:29Z
http://arxiv.org/abs/2408.00798v1,http://arxiv.org/abs/2408.00798v1,"Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation
  for Industrial Knowledge Base","This paper introduces Golden-Retriever, designed to efficiently navigate vast
industrial knowledge bases, overcoming challenges in traditional LLM
fine-tuning and RAG frameworks with domain-specific jargon and context
interpretation. Golden-Retriever incorporates a reflection-based question
augmentation step before document retrieval, which involves identifying jargon,
clarifying its meaning based on context, and augmenting the question
accordingly. Specifically, our method extracts and lists all jargon and
abbreviations in the input question, determines the context against a
pre-defined list, and queries a jargon dictionary for extended definitions and
descriptions. This comprehensive augmentation ensures the RAG framework
retrieves the most relevant documents by providing clear context and resolving
ambiguities, significantly improving retrieval accuracy. Evaluations using
three open-source LLMs on a domain-specific question-answer dataset demonstrate
Golden-Retriever's superior performance, providing a robust solution for
efficiently integrating and querying industrial knowledge bases.","[{'name': 'Zhiyu An'}, {'name': 'Xianzhong Ding'}, {'name': 'Yen-Chun Fu'}, {'name': 'Cheng-Chung Chu'}, {'name': 'Yan Li'}, {'name': 'Wan Du'}]",2024-07-20T06:10:46Z
http://arxiv.org/abs/2407.14733v1,http://arxiv.org/abs/2407.14733v1,"Hard Prompts Made Interpretable: Sparse Entropy Regularization for
  Prompt Tuning with RL","With the advent of foundation models, prompt tuning has positioned itself as
an important technique for directing model behaviors and eliciting desired
responses. Prompt tuning regards selecting appropriate keywords included into
the input, thereby adapting to the downstream task without adjusting or
fine-tuning the model parameters. There is a wide range of work in prompt
tuning, from approaches that directly harness the backpropagated gradient
signals from the model, to those employing black-box optimization such as
reinforcement learning (RL) methods. Our primary focus is on RLPrompt, which
aims to find optimal prompt tokens leveraging soft Q-learning. While the
results show promise, we have observed that the prompts frequently appear
unnatural, which impedes their interpretability. We address this limitation by
using sparse Tsallis entropy regularization, a principled approach to filtering
out unlikely tokens from consideration. We extensively evaluate our approach
across various tasks, including few-shot text classification, unsupervised text
style transfer, and textual inversion from images. The results indicate a
notable improvement over baselines, highlighting the efficacy of our approach
in addressing the challenges of prompt tuning. Moreover, we show that the
prompts discovered using our method are more natural and interpretable compared
to those from other baselines.","[{'name': 'Yunseon Choi'}, {'name': 'Sangmin Bae'}, {'name': 'Seonghyun Ban'}, {'name': 'Minchan Jeong'}, {'name': 'Chuheng Zhang'}, {'name': 'Lei Song'}, {'name': 'Li Zhao'}, {'name': 'Jiang Bian'}, {'name': 'Kee-Eung Kim'}]",2024-07-20T03:10:19Z
http://arxiv.org/abs/2407.14727v1,http://arxiv.org/abs/2407.14727v1,"Economy Watchers Survey provides Datasets and Tasks for Japanese
  Financial Domain","Many natural language processing (NLP) tasks in English or general domains
are widely available and are often used to evaluate pre-trained language
models. In contrast, there are fewer tasks available for languages other than
English and for the financial domain. In particular, tasks in Japanese and the
financial domain are limited. We construct two large datasets using materials
published by a Japanese central government agency. The datasets provide three
Japanese financial NLP tasks, which include a 3-class and 12-class
classification for categorizing sentences, as well as a 5-class classification
task for sentiment analysis. Our datasets are designed to be comprehensive and
up-to-date, leveraging an automatic update framework that ensures the latest
task datasets are publicly available anytime.","[{'name': 'Masahiro Suzuki'}, {'name': 'Hiroki Sakaji'}]",2024-07-20T02:35:14Z
http://arxiv.org/abs/2407.14701v1,http://arxiv.org/abs/2407.14701v1,"Contextual modulation of language comprehension in a dynamic neural
  model of lexical meaning","We propose and computationally implement a dynamic neural model of lexical
meaning, and experimentally test its behavioral predictions. We demonstrate the
architecture and behavior of the model using as a test case the English lexical
item 'have', focusing on its polysemous use. In the model, 'have' maps to a
semantic space defined by two continuous conceptual dimensions, connectedness
and control asymmetry, previously proposed to parameterize the conceptual
system for language. The mapping is modeled as coupling between a neural node
representing the lexical item and neural fields representing the conceptual
dimensions. While lexical knowledge is modeled as a stable coupling pattern,
real-time lexical meaning retrieval is modeled as the motion of neural
activation patterns between metastable states corresponding to semantic
interpretations or readings. Model simulations capture two previously reported
empirical observations: (1) contextual modulation of lexical semantic
interpretation, and (2) individual variation in the magnitude of this
modulation. Simulations also generate a novel prediction that the by-trial
relationship between sentence reading time and acceptability should be
contextually modulated. An experiment combining self-paced reading and
acceptability judgments replicates previous results and confirms the new model
prediction. Altogether, results support a novel perspective on lexical
polysemy: that the many related meanings of a word are metastable neural
activation states that arise from the nonlinear dynamics of neural populations
governing interpretation on continuous semantic dimensions.","[{'name': 'Michael C. Stern'}, {'name': 'Maria M. Piñango'}]",2024-07-19T23:28:55Z
http://arxiv.org/abs/2407.14679v1,http://arxiv.org/abs/2407.14679v1,Compact Language Models via Pruning and Knowledge Distillation,"Large language models (LLMs) targeting different deployment scales and sizes
are currently produced by training each variant from scratch; this is extremely
compute-intensive. In this paper, we investigate if pruning an existing LLM and
then re-training it with a fraction (<3%) of the original training data can be
a suitable alternative to repeated, full retraining. To this end, we develop a
set of practical and effective compression best practices for LLMs that combine
depth, width, attention and MLP pruning with knowledge distillation-based
retraining; we arrive at these best practices through a detailed empirical
exploration of pruning strategies for each axis, methods to combine axes,
distillation strategies, and search techniques for arriving at optimal
compressed architectures. We use this guide to compress the Nemotron-4 family
of LLMs by a factor of 2-4x, and compare their performance to similarly-sized
models on a variety of language modeling tasks. Deriving 8B and 4B models from
an already pretrained 15B model using our approach requires up to 40x fewer
training tokens per model compared to training from scratch; this results in
compute cost savings of 1.8x for training the full model family (15B, 8B, and
4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to
training from scratch, perform comparably to other community models such as
Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art
compression techniques from the literature. We have open-sourced Minitron model
weights on Huggingface, with corresponding supplementary material including
example code available on GitHub.","[{'name': 'Saurav Muralidharan'}, {'name': 'Sharath Turuvekere Sreenivas'}, {'name': 'Raviraj Joshi'}, {'name': 'Marcin Chochowski'}, {'name': 'Mostofa Patwary'}, {'name': 'Mohammad Shoeybi'}, {'name': 'Bryan Catanzaro'}, {'name': 'Jan Kautz'}, {'name': 'Pavlo Molchanov'}]",2024-07-19T21:47:57Z
http://arxiv.org/abs/2407.21038v1,http://arxiv.org/abs/2407.21038v1,"Advancing Chart Question Answering with Robust Chart Component
  Recognition","Chart comprehension presents significant challenges for machine learning
models due to the diverse and intricate shapes of charts. Existing multimodal
methods often overlook these visual features or fail to integrate them
effectively for chart question answering (ChartQA). To address this, we
introduce Chartformer, a unified framework that enhances chart component
recognition by accurately identifying and classifying components such as bars,
lines, pies, titles, legends, and axes. Additionally, we propose a novel
Question-guided Deformable Co-Attention (QDCAt) mechanism, which fuses chart
features encoded by Chartformer with the given question, leveraging the
question's guidance to ground the correct answer. Extensive experiments
demonstrate that the proposed approaches significantly outperform baseline
models in chart component recognition and ChartQA tasks, achieving improvements
of 3.2% in mAP and 15.4% in accuracy, respectively. These results underscore
the robustness of our solution for detailed visual data interpretation across
various applications.","[{'name': 'Hanwen Zheng'}, {'name': 'Sijia Wang'}, {'name': 'Chris Thomas'}, {'name': 'Lifu Huang'}]",2024-07-19T20:55:06Z
http://arxiv.org/abs/2407.14644v2,http://arxiv.org/abs/2407.14644v2,"Human-Interpretable Adversarial Prompt Attack on Large Language Models
  with Situational Context","Previous research on testing the vulnerabilities in Large Language Models
(LLMs) using adversarial attacks has primarily focused on nonsensical prompt
injections, which are easily detected upon manual or automated review (e.g.,
via byte entropy). However, the exploration of innocuous human-understandable
malicious prompts augmented with adversarial injections remains limited. In
this research, we explore converting a nonsensical suffix attack into a
sensible prompt via a situation-driven contextual re-writing. This allows us to
show suffix conversion without any gradients, using only LLMs to perform the
attacks, and thus better understand the scope of possible risks. We combine an
independent, meaningful adversarial insertion and situations derived from
movies to check if this can trick an LLM. The situations are extracted from the
IMDB dataset, and prompts are defined following a few-shot chain-of-thought
prompting. Our approach demonstrates that a successful situation-driven attack
can be executed on both open-source and proprietary LLMs. We find that across
many LLMs, as few as 1 attempt produces an attack and that these attacks
transfer between LLMs.","[{'name': 'Nilanjana Das'}, {'name': 'Edward Raff'}, {'name': 'Manas Gaur'}]",2024-07-19T19:47:26Z
http://arxiv.org/abs/2407.14640v1,http://arxiv.org/abs/2407.14640v1,"CVE-LLM : Automatic vulnerability evaluation in medical device industry
  using large language models","The healthcare industry is currently experiencing an unprecedented wave of
cybersecurity attacks, impacting millions of individuals. With the discovery of
thousands of vulnerabilities each month, there is a pressing need to drive the
automation of vulnerability assessment processes for medical devices,
facilitating rapid mitigation efforts. Generative AI systems have
revolutionized various industries, offering unparalleled opportunities for
automation and increased efficiency. This paper presents a solution leveraging
Large Language Models (LLMs) to learn from historical evaluations of
vulnerabilities for the automatic assessment of vulnerabilities in the medical
devices industry. This approach is applied within the portfolio of a single
manufacturer, taking into account device characteristics, including existing
security posture and controls. The primary contributions of this paper are
threefold. Firstly, it provides a detailed examination of the best practices
for training a vulnerability Language Model (LM) in an industrial context.
Secondly, it presents a comprehensive comparison and insightful analysis of the
effectiveness of Language Models in vulnerability assessment. Finally, it
proposes a new human-in-the-loop framework to expedite vulnerability evaluation
processes.","[{'name': 'Rikhiya Ghosh'}, {'name': 'Oladimeji Farri'}, {'name': 'Hans-Martin von Stockhausen'}, {'name': 'Martin Schmitt'}, {'name': 'George Marica Vasile'}]",2024-07-19T19:34:17Z
http://arxiv.org/abs/2407.14622v1,http://arxiv.org/abs/2407.14622v1,BOND: Aligning LLMs with Best-of-N Distillation,"Reinforcement learning from human feedback (RLHF) is a key driver of quality
and safety in state-of-the-art large language models. Yet, a surprisingly
simple and strong inference-time strategy is Best-of-N sampling that selects
the best generation among N candidates. In this paper, we propose Best-of-N
Distillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but
without its significant computational overhead at inference time. Specifically,
BOND is a distribution matching algorithm that forces the distribution of
generations from the policy to get closer to the Best-of-N distribution. We use
the Jeffreys divergence (a linear combination of forward and backward KL) to
balance between mode-covering and mode-seeking behavior, and derive an
iterative formulation that utilizes a moving anchor for efficiency. We
demonstrate the effectiveness of our approach and several design choices
through experiments on abstractive summarization and Gemma models. Aligning
Gemma policies with BOND outperforms other RLHF algorithms by improving results
on several benchmarks.","[{'name': 'Pier Giuseppe Sessa'}, {'name': 'Robert Dadashi'}, {'name': 'Léonard Hussenot'}, {'name': 'Johan Ferret'}, {'name': 'Nino Vieillard'}, {'name': 'Alexandre Ramé'}, {'name': 'Bobak Shariari'}, {'name': 'Sarah Perrin'}, {'name': 'Abe Friesen'}, {'name': 'Geoffrey Cideron'}, {'name': 'Sertan Girgin'}, {'name': 'Piotr Stanczyk'}, {'name': 'Andrea Michi'}, {'name': 'Danila Sinopalnikov'}, {'name': 'Sabela Ramos'}, {'name': 'Amélie Héliou'}, {'name': 'Aliaksei Severyn'}, {'name': 'Matt Hoffman'}, {'name': 'Nikola Momchev'}, {'name': 'Olivier Bachem'}]",2024-07-19T18:38:25Z
http://arxiv.org/abs/2407.14507v1,http://arxiv.org/abs/2407.14507v1,"Internal Consistency and Self-Feedback in Large Language Models: A
  Survey","Large language models (LLMs) are expected to respond accurately but often
exhibit deficient reasoning or generate hallucinatory content. To address
these, studies prefixed with ``Self-'' such as Self-Consistency, Self-Improve,
and Self-Refine have been initiated. They share a commonality: involving LLMs
evaluating and updating itself to mitigate the issues. Nonetheless, these
efforts lack a unified perspective on summarization, as existing surveys
predominantly focus on categorization without examining the motivations behind
these works.
  In this paper, we summarize a theoretical framework, termed Internal
Consistency, which offers unified explanations for phenomena such as the lack
of reasoning and the presence of hallucinations. Internal Consistency assesses
the coherence among LLMs' latent layer, decoding layer, and response layer
based on sampling methodologies. Expanding upon the Internal Consistency
framework, we introduce a streamlined yet effective theoretical framework
capable of mining Internal Consistency, named Self-Feedback. The Self-Feedback
framework consists of two modules: Self-Evaluation and Self-Update. This
framework has been employed in numerous studies.
  We systematically classify these studies by tasks and lines of work;
summarize relevant evaluation methods and benchmarks; and delve into the
concern, ``Does Self-Feedback Really Work?'' We propose several critical
viewpoints, including the ``Hourglass Evolution of Internal Consistency'',
``Consistency Is (Almost) Correctness'' hypothesis, and ``The Paradox of Latent
and Explicit Reasoning''. Furthermore, we outline promising directions for
future research. We have open-sourced the experimental code, reference list,
and statistical data, available at
\url{https://github.com/IAAR-Shanghai/ICSFSurvey}.","[{'name': 'Xun Liang'}, {'name': 'Shichao Song'}, {'name': 'Zifan Zheng'}, {'name': 'Hanyu Wang'}, {'name': 'Qingchen Yu'}, {'name': 'Xunkai Li'}, {'name': 'Rong-Hua Li'}, {'name': 'Feiyu Xiong'}, {'name': 'Zhiyu Li'}]",2024-07-19T17:59:03Z
http://arxiv.org/abs/2407.14506v2,http://arxiv.org/abs/2407.14506v2,"On Pre-training of Multimodal Language Models Customized for Chart
  Understanding","Recent studies customizing Multimodal Large Language Models (MLLMs) for
domain-specific tasks have yielded promising results, especially in the field
of scientific chart comprehension. These studies generally utilize visual
instruction tuning with specialized datasets to enhance question and answer
(QA) accuracy within the chart domain. However, they often neglect the
fundamental discrepancy between natural image-caption pre-training data and
digital chart image-QA data, particularly in the models' capacity to extract
underlying numeric values from charts. This paper tackles this oversight by
exploring the training processes necessary to improve MLLMs' comprehension of
charts. We present three key findings: (1) Incorporating raw data values in
alignment pre-training markedly improves comprehension of chart data. (2)
Replacing images with their textual representation randomly during end-to-end
fine-tuning transfer the language reasoning capability to chart interpretation
skills. (3) Requiring the model to first extract the underlying chart data and
then answer the question in the fine-tuning can further improve the accuracy.
Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart
comprehension. CHOPINLLM effectively interprets various types of charts,
including unannotated ones, while maintaining robust reasoning abilities.
Furthermore, we establish a new benchmark to evaluate MLLMs' understanding of
different chart types across various comprehension levels. Experimental results
show that CHOPINLLM exhibits strong performance in understanding both annotated
and unannotated charts across a wide range of types.","[{'name': 'Wan-Cyuan Fan'}, {'name': 'Yen-Chun Chen'}, {'name': 'Mengchen Liu'}, {'name': 'Lu Yuan'}, {'name': 'Leonid Sigal'}]",2024-07-19T17:58:36Z
http://arxiv.org/abs/2407.14487v1,http://arxiv.org/abs/2407.14487v1,Evaluating the Reliability of Self-Explanations in Large Language Models,"This paper investigates the reliability of explanations generated by large
language models (LLMs) when prompted to explain their previous output. We
evaluate two kinds of such self-explanations - extractive and counterfactual -
using three state-of-the-art LLMs (2B to 8B parameters) on two different
classification tasks (objective and subjective). Our findings reveal, that,
while these self-explanations can correlate with human judgement, they do not
fully and accurately follow the model's decision process, indicating a gap
between perceived and actual model reasoning. We show that this gap can be
bridged because prompting LLMs for counterfactual explanations can produce
faithful, informative, and easy-to-verify results. These counterfactuals offer
a promising alternative to traditional explainability methods (e.g. SHAP,
LIME), provided that prompts are tailored to specific tasks and checked for
validity.","[{'name': 'Korbinian Randl'}, {'name': 'John Pavlopoulos'}, {'name': 'Aron Henriksson'}, {'name': 'Tony Lindgren'}]",2024-07-19T17:41:08Z
http://arxiv.org/abs/2407.14482v1,http://arxiv.org/abs/2407.14482v1,"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG
  Capabilities","In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge
the gap between open-access LLMs and leading proprietary models (e.g.,
GPT-4-Turbo) in long-context understanding and retrieval-augmented generation
(RAG) capabilities. These two capabilities are essential for LLMs to process
large volumes of information that cannot fit into a single prompt and are
complementary to each other, depending on the downstream tasks and
computational budgets. We present a detailed continued training recipe to
extend the context window of Llama3-70B-base from 8K to 128K tokens, along with
a three-stage instruction tuning process to enhance the model's
instruction-following, RAG performance, and long-context understanding
capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model
achieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context
understanding tasks and surpasses it on the RAG benchmark. Interestingly, we
find that the state-of-the-art long-context retriever can alleviate the top-k
context fragmentation issue in RAG, further improving RAG-based results for
long-context understanding tasks. We also provide extensive comparisons between
RAG and long-context solutions using state-of-the-art long-context LLMs.","[{'name': 'Peng Xu'}, {'name': 'Wei Ping'}, {'name': 'Xianchao Wu'}, {'name': 'Zihan Liu'}, {'name': 'Mohammad Shoeybi'}, {'name': 'Bryan Catanzaro'}]",2024-07-19T17:35:47Z
http://arxiv.org/abs/2407.14467v1,http://arxiv.org/abs/2407.14467v1,Check-Eval: A Checklist-based Approach for Evaluating Text Quality,"Evaluating the quality of text generated by large language models (LLMs)
remains a significant challenge. Traditional metrics often fail to align well
with human judgments, particularly in tasks requiring creativity and nuance. In
this paper, we propose Check-Eval, a novel evaluation framework leveraging LLMs
to assess the quality of generated text through a checklist-based approach.
Check-Eval can be employed as both a reference-free and reference-dependent
evaluation method, providing a structured and interpretable assessment of text
quality. The framework consists of two main stages: checklist generation and
checklist evaluation. We validate Check-Eval on two benchmark datasets:
Portuguese Legal Semantic Textual Similarity and SummEval. Our results
demonstrate that Check-Eval achieves higher correlations with human judgments
compared to existing metrics, such as G-Eval and GPTScore, underscoring its
potential as a more reliable and effective evaluation framework for natural
language generation tasks. The code for our experiments is available at
https://anonymous.4open.science/r/check-eval-0DB4.","[{'name': 'Jayr Pereira'}, {'name': 'Roberto Lotufo'}]",2024-07-19T17:14:16Z
http://arxiv.org/abs/2407.14458v1,http://arxiv.org/abs/2407.14458v1,"AudioInsight: Detecting Social Contexts Relevant to Social Anxiety from
  Speech","During social interactions, understanding the intricacies of the context can
be vital, particularly for socially anxious individuals. While previous
research has found that the presence of a social interaction can be detected
from ambient audio, the nuances within social contexts, which influence how
anxiety provoking interactions are, remain largely unexplored. As an
alternative to traditional, burdensome methods like self-report, this study
presents a novel approach that harnesses ambient audio segments to detect
social threat contexts. We focus on two key dimensions: number of interaction
partners (dyadic vs. group) and degree of evaluative threat (explicitly
evaluative vs. not explicitly evaluative). Building on data from a Zoom-based
social interaction study (N=52 college students, of whom the majority N=45 are
socially anxious), we employ deep learning methods to achieve strong detection
performance. Under sample-wide 5-fold Cross Validation (CV), our model
distinguished dyadic from group interactions with 90\% accuracy and detected
evaluative threat at 83\%. Using a leave-one-group-out CV, accuracies were 82\%
and 77\%, respectively. While our data are based on virtual interactions due to
pandemic constraints, our method has the potential to extend to diverse
real-world settings. This research underscores the potential of passive sensing
and AI to differentiate intricate social contexts, and may ultimately advance
the ability of context-aware digital interventions to offer personalized mental
health support.","[{'name': 'Varun Reddy'}, {'name': 'Zhiyuan Wang'}, {'name': 'Emma Toner'}, {'name': 'Max Larrazabal'}, {'name': 'Mehdi Boukhechba'}, {'name': 'Bethany A. Teachman'}, {'name': 'Laura E. Barnes'}]",2024-07-19T17:01:12Z
http://arxiv.org/abs/2407.14414v1,http://arxiv.org/abs/2407.14414v1,"System-1.x: Learning to Balance Fast and Slow Planning with Language
  Models","Language models can be used to solve long-horizon planning problems in two
distinct modes: a fast 'System-1' mode, directly generating plans without any
explicit search or backtracking, and a slow 'System-2' mode, planning
step-by-step by explicitly searching over possible actions. While System-2 is
typically more effective, it is also more computationally expensive, making it
infeasible for long plans or large action spaces. Moreover, isolated System-1
or 2 ignores the user's end goals, failing to provide ways to control the
model's behavior. To this end, we propose the System-1.x Planner, a
controllable planning framework with LLMs that is capable of generating hybrid
plans and balancing between the two planning modes based on the difficulty of
the problem at hand. System-1.x consists of (i) a controller, (ii) a System-1
Planner, and (iii) a System-2 Planner. Based on a user-specified hybridization
factor (x) governing the mixture between System-1 and 2, the controller
decomposes a problem into sub-goals, and classifies them as easy or hard to be
solved by either System-1 or 2, respectively. We fine-tune all three components
on top of a single base LLM, requiring only search traces as supervision.
Experiments with two diverse planning tasks -- Maze Navigation and Blocksworld
-- show that our System-1.x Planner outperforms a System-1 Planner, a System-2
Planner trained to approximate A* search, and also a symbolic planner (A*). We
demonstrate the following key properties of our planner: (1) controllability:
increasing the hybridization factor (e.g., System-1.75 vs 1.5) performs more
search, improving performance, (2) flexibility: by building a neuro-symbolic
variant with a neural System-1 and a symbolic System-2, we can use existing
symbolic methods, and (3) generalizability: by being able to learn from
different search algorithms, our method is robust to the choice of search
algorithm.","[{'name': 'Swarnadeep Saha'}, {'name': 'Archiki Prasad'}, {'name': 'Justin Chih-Yao Chen'}, {'name': 'Peter Hase'}, {'name': 'Elias Stengel-Eskin'}, {'name': 'Mohit Bansal'}]",2024-07-19T15:40:59Z
http://arxiv.org/abs/2407.14402v1,http://arxiv.org/abs/2407.14402v1,The Vision of Autonomic Computing: Can LLMs Make It a Reality?,"The Vision of Autonomic Computing (ACV), proposed over two decades ago,
envisions computing systems that self-manage akin to biological organisms,
adapting seamlessly to changing environments. Despite decades of research,
achieving ACV remains challenging due to the dynamic and complex nature of
modern computing systems. Recent advancements in Large Language Models (LLMs)
offer promising solutions to these challenges by leveraging their extensive
knowledge, language understanding, and task automation capabilities. This paper
explores the feasibility of realizing ACV through an LLM-based multi-agent
framework for microservice management. We introduce a five-level taxonomy for
autonomous service maintenance and present an online evaluation benchmark based
on the Sock Shop microservice demo project to assess our framework's
performance. Our findings demonstrate significant progress towards achieving
Level 3 autonomy, highlighting the effectiveness of LLMs in detecting and
resolving issues within microservice architectures. This study contributes to
advancing autonomic computing by pioneering the integration of LLMs into
microservice management frameworks, paving the way for more adaptive and
self-managing computing systems. The code will be made available at
https://aka.ms/ACV-LLM.","[{'name': 'Zhiyang Zhang'}, {'name': 'Fangkai Yang'}, {'name': 'Xiaoting Qin'}, {'name': 'Jue Zhang'}, {'name': 'Qingwei Lin'}, {'name': 'Gong Cheng'}, {'name': 'Dongmei Zhang'}, {'name': 'Saravan Rajmohan'}, {'name': 'Qi Zhang'}]",2024-07-19T15:30:32Z
http://arxiv.org/abs/2407.14371v1,http://arxiv.org/abs/2407.14371v1,Open Artificial Knowledge,"The tremendous success of chat-based AI systems like ChatGPT, Claude, and
Gemini stems from Large Language Models (LLMs) trained on vast amount of
datasets. However, acquiring high-quality, diverse, and ethically sourced
training data remains a significant challenge. We introduce the Open Artificial
Knowledge (OAK) dataset, a large-scale resource of over 500 million tokens (at
the moment of writing) designed to address this issue. OAK leverages an
ensemble of state-of-the-art LLMs, including GPT4o, LLaMa3-70B, LLaMa3-8B,
Mixtral-8x7B, Gemma-7B, and Gemma-2-9B , to generate high-quality text across
diverse domains, guided by Wikipedia's main categories. Our methodology ensures
broad knowledge coverage while maintaining coherence and factual accuracy. The
OAK dataset aims to foster the development of more capable and aligned language
models while addressing critical issues of data scarcity and privacy in LLM
training, and it is freely available on www.oakdataset.org.","[{'name': 'Vadim Borisov'}, {'name': 'Richard H. Schreiber'}]",2024-07-19T15:01:24Z
http://arxiv.org/abs/2407.14346v1,http://arxiv.org/abs/2407.14346v1,"Improving Retrieval in Sponsored Search by Leveraging Query Context
  Signals","Accurately retrieving relevant bid keywords for user queries is critical in
Sponsored Search but remains challenging, particularly for short, ambiguous
queries. Existing dense and generative retrieval models often fail to capture
nuanced user intent in these cases. To address this, we propose an approach to
enhance query understanding by augmenting queries with rich contextual signals
derived from web search results and large language models, stored in an online
cache. Specifically, we use web search titles and snippets to ground queries in
real-world information and utilize GPT-4 to generate query rewrites and
explanations that clarify user intent. These signals are efficiently integrated
through a Fusion-in-Decoder based Unity architecture, enabling both dense and
generative retrieval with serving costs on par with traditional context-free
models. To address scenarios where context is unavailable in the cache, we
introduce context glancing, a curriculum learning strategy that improves model
robustness and performance even without contextual signals during inference.
Extensive offline experiments demonstrate that our context-aware approach
substantially outperforms context-free models. Furthermore, online A/B testing
on a prominent search engine across 160+ countries shows significant
improvements in user engagement and revenue.","[{'name': 'Akash Kumar Mohankumar'}, {'name': 'Gururaj K'}, {'name': 'Gagan Madan'}, {'name': 'Amit Singh'}]",2024-07-19T14:28:53Z
http://arxiv.org/abs/2407.14344v1,http://arxiv.org/abs/2407.14344v1,"LLMs left, right, and center: Assessing GPT's capabilities to label
  political bias from web domains","This research investigates whether OpenAI's GPT-4, a state-of-the-art large
language model, can accurately classify the political bias of news sources
based solely on their URLs. Given the subjective nature of political labels,
third-party bias ratings like those from Ad Fontes Media, AllSides, and Media
Bias/Fact Check (MBFC) are often used in research to analyze news source
diversity. This study aims to determine if GPT-4 can replicate these human
ratings on a seven-degree scale (""far-left"" to ""far-right""). The analysis
compares GPT-4's classifications against MBFC's, and controls for website
popularity using Open PageRank scores. Findings reveal a high correlation
($\text{Spearman's } \rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and
MBFC's ratings, indicating the model's potential reliability. However, GPT-4
abstained from classifying approximately $\frac{2}{3}$ of the dataset,
particularly less popular and less biased sources. The study also identifies a
slight leftward skew in GPT-4's classifications compared to MBFC's. The
analysis suggests that while GPT-4 can be a scalable, cost-effective tool for
political bias classification of news websites, but its use should complement
human judgment to mitigate biases. Further research is recommended to explore
the model's performance across different settings, languages, and additional
datasets.",[{'name': 'Raphael Hernandes'}],2024-07-19T14:28:07Z
http://arxiv.org/abs/2407.14321v1,http://arxiv.org/abs/2407.14321v1,Multimodal Misinformation Detection using Large Vision-Language Models,"The increasing proliferation of misinformation and its alarming impact have
motivated both industry and academia to develop approaches for misinformation
detection and fact checking. Recent advances on large language models (LLMs)
have shown remarkable performance in various tasks, but whether and how LLMs
could help with misinformation detection remains relatively underexplored. Most
of existing state-of-the-art approaches either do not consider evidence and
solely focus on claim related features or assume the evidence to be provided.
Few approaches consider evidence retrieval as part of the misinformation
detection but rely on fine-tuning models. In this paper, we investigate the
potential of LLMs for misinformation detection in a zero-shot setting. We
incorporate an evidence retrieval component into the process as it is crucial
to gather pertinent information from various sources to detect the veracity of
claims. To this end, we propose a novel re-ranking approach for multimodal
evidence retrieval using both LLMs and large vision-language models (LVLM). The
retrieved evidence samples (images and texts) serve as the input for an
LVLM-based approach for multimodal fact verification (LVLM4FV). To enable a
fair evaluation, we address the issue of incomplete ground truth for evidence
samples in an existing evidence retrieval dataset by annotating a more complete
set of evidence samples for both image and text retrieval. Our experimental
results on two datasets demonstrate the superiority of the proposed approach in
both evidence retrieval and fact verification tasks and also better
generalization capability across dataset compared to the supervised baseline.","[{'name': 'Sahar Tahmasebi'}, {'name': 'Eric Müller-Budack'}, {'name': 'Ralph Ewerth'}]",2024-07-19T13:57:11Z
http://arxiv.org/abs/2407.14309v2,http://arxiv.org/abs/2407.14309v2,"How to Engage Your Readers? Generating Guiding Questions to Promote
  Active Reading","Using questions in written text is an effective strategy to enhance
readability. However, what makes an active reading question good, what the
linguistic role of these questions is, and what is their impact on human
reading remains understudied. We introduce GuidingQ, a dataset of 10K in-text
questions from textbooks and scientific articles. By analyzing the dataset, we
present a comprehensive understanding of the use, distribution, and linguistic
characteristics of these questions. Then, we explore various approaches to
generate such questions using language models. Our results highlight the
importance of capturing inter-question relationships and the challenge of
question position identification in generating these questions. Finally, we
conduct a human study to understand the implication of such questions on
reading comprehension. We find that the generated questions are of high quality
and are almost as effective as human-written questions in terms of improving
readers' memorization and comprehension.","[{'name': 'Peng Cui'}, {'name': 'Vilém Zouhar'}, {'name': 'Xiaoyu Zhang'}, {'name': 'Mrinmaya Sachan'}]",2024-07-19T13:42:56Z
http://arxiv.org/abs/2407.14296v2,http://arxiv.org/abs/2407.14296v2,Foundation Models for Autonomous Robots in Unstructured Environments,"Automating activities through robots in unstructured environments, such as
construction sites, has been a long-standing desire. However, the high degree
of unpredictable events in these settings has resulted in far less adoption
compared to more structured settings, such as manufacturing, where robots can
be hard-coded or trained on narrowly defined datasets. Recently, pretrained
foundation models, such as Large Language Models (LLMs), have demonstrated
superior generalization capabilities by providing zero-shot solutions for
problems do not present in the training data, proposing them as a potential
solution for introducing robots to unstructured environments. To this end, this
study investigates potential opportunities and challenges of pretrained
foundation models from a multi-dimensional perspective. The study
systematically reviews application of foundation models in two field of robotic
and unstructured environment and then synthesized them with deliberative acting
theory. Findings showed that linguistic capabilities of LLMs have been utilized
more than other features for improving perception in human-robot interactions.
On the other hand, findings showed that the use of LLMs demonstrated more
applications in project management and safety in construction, and natural
hazard detection in disaster management. Synthesizing these findings, we
located the current state-of-the-art in this field on a five-level scale of
automation, placing them at conditional automation. This assessment was then
used to envision future scenarios, challenges, and solutions toward autonomous
safe unstructured environments. Our study can be seen as a benchmark to track
our progress toward that future.","[{'name': 'Hossein Naderi'}, {'name': 'Alireza Shojaei'}, {'name': 'Lifu Huang'}]",2024-07-19T13:26:52Z
http://arxiv.org/abs/2407.14295v1,http://arxiv.org/abs/2407.14295v1,"CoVoSwitch: Machine Translation of Synthetic Code-Switched Text Based on
  Intonation Units","Multilingual code-switching research is often hindered by the lack and
linguistically biased status of available datasets. To expand language
representation, we synthesize code-switching data by replacing intonation units
detected through PSST, a speech segmentation model fine-tuned from OpenAI's
Whisper, using a speech-to-text translation dataset, CoVoST 2. With our
dataset, CoVoSwitch, spanning 13 languages, we evaluate the code-switching
translation performance of two multilingual translation models, M2M-100 418M
and NLLB-200 600M. We reveal that the inclusion of code-switching units results
in higher translation performance than monolingual settings and that models are
better at code-switching translation into English than non-English. Further,
low-resource languages gain most from integration of code-switched units when
translating into English but much less when translating into non-English.
Translations into low-resource languages also perform worse than even raw
code-switched inputs. We find that systems excel at copying English tokens but
struggle with non-English tokens, that the off-target problem in monolingual
settings is also relevant in code-switching settings, and that models
hallucinate in code-switching translation by introducing words absent in both
of the original source sentences. CoVoSwitch and code are available at
https://github.com/sophiayk20/covoswitch.",[{'name': 'Yeeun Kang'}],2024-07-19T13:26:35Z
http://arxiv.org/abs/2407.14259v1,http://arxiv.org/abs/2407.14259v1,Voices in a Crowd: Searching for Clusters of Unique Perspectives,"Language models have been shown to reproduce underlying biases existing in
their training data, which is the majority perspective by default. Proposed
solutions aim to capture minority perspectives by either modelling annotator
disagreements or grouping annotators based on shared metadata, both of which
face significant challenges. We propose a framework that trains models without
encoding annotator metadata, extracts latent embeddings informed by annotator
behaviour, and creates clusters of similar opinions, that we refer to as
voices. Resulting clusters are validated post-hoc via internal and external
quantitative metrics, as well a qualitative analysis to identify the type of
voice that each cluster represents. Our results demonstrate the strong
generalisation capability of our framework, indicated by resulting clusters
being adequately robust, while also capturing minority perspectives based on
different demographic factors throughout two distinct datasets.","[{'name': 'Nikolas Vitsakis'}, {'name': 'Amit Parekh'}, {'name': 'Ioannis Konstas'}]",2024-07-19T12:37:15Z
http://arxiv.org/abs/2407.14246v2,http://arxiv.org/abs/2407.14246v2,Unipa-GPT: Large Language Models for university-oriented QA in Italian,"This paper illustrates the architecture and training of Unipa-GPT, a chatbot
relying on a Large Language Model, developed for assisting students in choosing
a bachelor/master degree course at the University of Palermo. Unipa-GPT relies
on gpt-3.5-turbo, it was presented in the context of the European Researchers'
Night (SHARPER night). In our experiments we adopted both the Retrieval
Augmented Generation (RAG) approach and fine-tuning to develop the system. The
whole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned
systems are compared, and a brief discussion on their performance is reported.
Further comparison with other Large Language Models and the experimental
results during the SHARPER night are illustrated.","[{'name': 'Irene Siragusa'}, {'name': 'Roberto Pirrone'}]",2024-07-19T12:28:22Z
http://arxiv.org/abs/2407.14224v1,http://arxiv.org/abs/2407.14224v1,"Hierarchical Windowed Graph Attention Network and a Large Scale Dataset
  for Isolated Indian Sign Language Recognition","Automatic Sign Language (SL) recognition is an important task in the computer
vision community. To build a robust SL recognition system, we need a
considerable amount of data which is lacking particularly in Indian sign
language (ISL). In this paper, we propose a large-scale isolated ISL dataset
and a novel SL recognition model based on skeleton graph structure. The dataset
covers 2,002 daily used common words in the deaf community recorded by 20 (10
male and 10 female) deaf adult signers (contains 40033 videos). We propose a SL
recognition model namely Hierarchical Windowed Graph Attention Network (HWGAT)
by utilizing the human upper body skeleton graph structure. The HWGAT tries to
capture distinctive motions by giving attention to different body parts induced
by the human skeleton graph structure. The utility of the proposed dataset and
the usefulness of our model are evaluated through extensive experiments. We
pre-trained the proposed model on the proposed dataset and fine-tuned it across
different sign language datasets further boosting the performance of 1.10,
0.46, 0.78, and 6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL
respectively compared to the existing state-of-the-art skeleton-based models.","[{'name': 'Suvajit Patra'}, {'name': 'Arkadip Maitra'}, {'name': 'Megha Tiwari'}, {'name': 'K. Kumaran'}, {'name': 'Swathy Prabhu'}, {'name': 'Swami Punyeshwarananda'}, {'name': 'Soumitra Samanta'}]",2024-07-19T11:48:36Z
http://arxiv.org/abs/2407.14212v1,http://arxiv.org/abs/2407.14212v1,"Braille-to-Speech Generator: Audio Generation Based on Joint Fine-Tuning
  of CLIP and Fastspeech2","An increasing number of Chinese people are troubled by different degrees of
visual impairment, which has made the modal conversion between a single image
or video frame in the visual field and the audio expressing the same
information a research hotspot. Deep learning technologies such as OCR+Vocoder
and Im2Wav enable English audio synthesis or image-to-sound matching in a
self-supervised manner. However, the audio data used for training is limited
and English is not universal for visually impaired people with different
educational levels. Therefore, for the sake of solving the problems of data
volume and language applicability to improve the reading efficiency of visually
impaired people, a set of image-to-speech framework CLIP-KNN-Fastspeech2 based
on the Chinese context was constructed. The framework integrates multiple basic
models and adopts the strategy of independent pre-training and joint
fine-tuning. First, the Chinese CLIP and Fastspeech2 text-to-speech models were
pre-trained on two public datasets, MUGE and Baker, respectively, and their
convergence was verified. Subsequently, joint fine-tuning was performed using a
self-built Braille image dataset. Experimental results on multiple public
datasets such as VGGSound, Flickr8k, ImageHear, and the self-built Braille
dataset BIT-DP show that the model has improved objective indicators such as
BLEU4,FAD(Fr\'echet Audio Distance), WER(Word Error Ratio), and even inference
speed. This verifies that the constructed model still has the ability to
synthesize high-quality speech under limited data, and also proves the
effectiveness of the joint training strategy that integrates multiple basic
models.","[{'name': 'Chun Xu'}, {'name': 'En-Wei Sun'}]",2024-07-19T11:18:44Z
http://arxiv.org/abs/2407.14192v1,http://arxiv.org/abs/2407.14192v1,LeKUBE: A Legal Knowledge Update BEnchmark,"Recent advances in Large Language Models (LLMs) have significantly shaped the
applications of AI in multiple fields, including the studies of legal
intelligence. Trained on extensive legal texts, including statutes and legal
documents, the legal LLMs can capture important legal knowledge/concepts
effectively and provide important support for downstream legal applications
such as legal consultancy. Yet, the dynamic nature of legal statutes and
interpretations also poses new challenges to the use of LLMs in legal
applications. Particularly, how to update the legal knowledge of LLMs
effectively and efficiently has become an important research problem in
practice. Existing benchmarks for evaluating knowledge update methods are
mostly designed for the open domain and cannot address the specific challenges
of the legal domain, such as the nuanced application of new legal knowledge,
the complexity and lengthiness of legal regulations, and the intricate nature
of legal reasoning. To address this gap, we introduce the Legal Knowledge
Update BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for
legal LLMs across five dimensions. Specifically, we categorize the needs of
knowledge updates in the legal domain with the help of legal professionals, and
then hire annotators from law schools to create synthetic updates to the
Chinese Criminal and Civil Code as well as sets of questions of which the
answers would change after the updates. Through a comprehensive evaluation of
state-of-the-art knowledge update methods, we reveal a notable gap between
existing knowledge update methods and the unique needs of the legal domain,
emphasizing the need for further research and development of knowledge update
mechanisms tailored for legal LLMs.","[{'name': 'Changyue Wang'}, {'name': 'Weihang Su'}, {'name': 'Hu Yiran'}, {'name': 'Qingyao Ai'}, {'name': 'Yueyue Wu'}, {'name': 'Cheng Luo'}, {'name': 'Yiqun Liu'}, {'name': 'Min Zhang'}, {'name': 'Shaoping Ma'}]",2024-07-19T10:40:10Z
http://arxiv.org/abs/2407.14145v1,http://arxiv.org/abs/2407.14145v1,PassTSL: Modeling Human-Created Passwords through Two-Stage Learning,"Textual passwords are still the most widely used user authentication
mechanism. Due to the close connections between textual passwords and natural
languages, advanced technologies in natural language processing (NLP) and
machine learning (ML) could be used to model passwords for different purposes
such as studying human password-creation behaviors and developing more advanced
password cracking methods for informing better defence mechanisms. In this
paper, we propose PassTSL (modeling human-created Passwords through Two-Stage
Learning), inspired by the popular pretraining-finetuning framework in NLP and
deep learning (DL). We report how different pretraining settings affected
PassTSL and proved its effectiveness by applying it to six large leaked
password databases. Experimental results showed that it outperforms five
state-of-the-art (SOTA) password cracking methods on password guessing by a
significant margin ranging from 4.11% to 64.69% at the maximum point. Based on
PassTSL, we also implemented a password strength meter (PSM), and our
experiments showed that it was able to estimate password strength more
accurately, causing fewer unsafe errors (overestimating the password strength)
than two other SOTA PSMs when they produce the same rate of safe errors
(underestimating the password strength): a neural-network based method and
zxcvbn. Furthermore, we explored multiple finetuning settings, and our
evaluations showed that, even a small amount of additional training data, e.g.,
only 0.1% of the pretrained data, can lead to over 3% improvement in password
guessing on average. We also proposed a heuristic approach to selecting
finetuning passwords based on JS (Jensen-Shannon) divergence and experimental
results validated its usefulness. In summary, our contributions demonstrate the
potential and feasibility of applying advanced NLP and ML methods to password
modeling and cracking.","[{'name': 'Yangde Wang'}, {'name': 'Haozhang Li'}, {'name': 'Weidong Qiu'}, {'name': 'Shujun Li'}, {'name': 'Peng Tang'}]",2024-07-19T09:23:30Z
http://arxiv.org/abs/2407.14133v1,http://arxiv.org/abs/2407.14133v1,"I Know About ""Up""! Enhancing Spatial Reasoning in Visual Language Models
  Through 3D Reconstruction","Visual Language Models (VLMs) are essential for various tasks, particularly
visual reasoning tasks, due to their robust multi-modal information
integration, visual reasoning capabilities, and contextual awareness. However,
existing \VLMs{}' visual spatial reasoning capabilities are often inadequate,
struggling even with basic tasks such as distinguishing left from right. To
address this, we propose the \ours{} model, designed to enhance the visual
spatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D
reconstruction model for obtaining different views of the input images and
incorporates a prompting mechanism to further improve visual spatial reasoning.
Experimental results on four visual spatial reasoning datasets show that our
\ours{} achieves up to 19.48% accuracy improvement, which indicates the
effectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.","[{'name': 'Zaiqiao Meng'}, {'name': 'Hao Zhou'}, {'name': 'Yifang Chen'}]",2024-07-19T09:03:30Z
http://arxiv.org/abs/2407.14088v1,http://arxiv.org/abs/2407.14088v1,"Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
  Generation: A State-of-the-Art Investigation","Data-to-text (D2T) generation aims to generate human-readable text from
semi-structured data, such as tables and graphs. The recent success of D2T is
largely attributed to advancements in LLMs. Despite the success of LLMs, no
research has been conducted to illustrate the impact of model size on the
performance of fine-tuned LLMs for D2T tasks. D2T model performance is
typically assessed based on three key qualities: \textit{readability}
(indicates fluency and coherence), \textit{informativeness} (measures content
similarity), and \textit{faithfulness} (assesses consistency of factual
information). It is currently uncertain whether increasing the size of LLMs
effectively improves performance in D2T tasks across these three qualities. The
objective of this study is to investigate the performance of fine-tuned LLMs in
D2T tasks in terms of model size. Through extensive comparative analysis, we
aim to elucidate both the advantages and limitations of scaling model sizes
across five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and
WebNLG) and twelve state-of-the-art LLMs with varying sizes from five different
LLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all
the three essential qualities of D2T models, we incorporate six widely
recognized automatic metrics -- \textsc{BLEU}, \textsc{METEOR},
\textsc{BERTScore}, \textsc{MoverScore}, \textsc{Parent}, and
\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance
concerning model size in the presence of source-reference divergence, a
critical aspect of D2T tasks. Our investigation reveals that increasing LLM
size enhances \textit{readability} and \textit{informativeness} in D2T tasks,
but larger (in terms of size) LLMs may sacrifice \textit{faithfulness}.
Moreover, small-sized LLMs show more resilience than larger ones when
source-reference divergence is present.","[{'name': 'Joy Mahapatra'}, {'name': 'Utpal Garain'}]",2024-07-19T07:54:30Z
http://arxiv.org/abs/2407.14085v1,http://arxiv.org/abs/2407.14085v1,"An Improved Method for Class-specific Keyword Extraction: A Case Study
  in the German Business Registry","The task of $\textit{keyword extraction}$ is often an important initial step
in unsupervised information extraction, forming the basis for tasks such as
topic modeling or document classification. While recent methods have proven to
be quite effective in the extraction of keywords, the identification of
$\textit{class-specific}$ keywords, or only those pertaining to a predefined
class, remains challenging. In this work, we propose an improved method for
class-specific keyword extraction, which builds upon the popular
$\textbf{KeyBERT}$ library to identify only keywords related to a class
described by $\textit{seed keywords}$. We test this method using a dataset of
German business registry entries, where the goal is to classify each business
according to an economic sector. Our results reveal that our method greatly
improves upon previous approaches, setting a new standard for
$\textit{class-specific}$ keyword extraction.","[{'name': 'Stephen Meisenbacher'}, {'name': 'Tim Schopf'}, {'name': 'Weixin Yan'}, {'name': 'Patrick Holl'}, {'name': 'Florian Matthes'}]",2024-07-19T07:42:48Z
http://arxiv.org/abs/2407.14076v2,http://arxiv.org/abs/2407.14076v2,"Domain-Specific Pretraining of Language Models: A Comparative Study in
  the Medical Field","There are many cases where LLMs are used for specific tasks in a single
domain. These usually require less general, but more domain-specific knowledge.
Highly capable, general-purpose state-of-the-art language models like GPT-4 or
Claude-3-opus can often be used for such tasks, but they are very large and
cannot be run locally, even if they were not proprietary. This can be a problem
when working with sensitive data. This paper focuses on domain-specific and
mixed-domain pretraining as potentially more efficient methods than general
pretraining for specialized language models. We will take a look at work
related to domain-specific pretraining, specifically in the medical area, and
compare benchmark results of specialized language models to general-purpose
language models.",[{'name': 'Tobias Kerner'}],2024-07-19T07:12:43Z
http://arxiv.org/abs/2407.14057v1,http://arxiv.org/abs/2407.14057v1,LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference,"The inference of transformer-based large language models consists of two
sequential stages: 1) a prefilling stage to compute the KV cache of prompts and
generate the first token, and 2) a decoding stage to generate subsequent
tokens. For long prompts, the KV cache must be computed for all tokens during
the prefilling stage, which can significantly increase the time needed to
generate the first token. Consequently, the prefilling stage may become a
bottleneck in the generation process. An open question remains whether all
prompt tokens are essential for generating the first token. To answer this, we
introduce a novel method, LazyLLM, that selectively computes the KV for tokens
important for the next token prediction in both the prefilling and decoding
stages. Contrary to static pruning approaches that prune the prompt at once,
LazyLLM allows language models to dynamically select different subsets of
tokens from the context in different generation steps, even though they might
be pruned in previous steps. Extensive experiments on standard datasets across
various tasks demonstrate that LazyLLM is a generic method that can be
seamlessly integrated with existing language models to significantly accelerate
the generation without fine-tuning. For instance, in the multi-document
question-answering task, LazyLLM accelerates the prefilling stage of the LLama
2 7B model by 2.34x while maintaining accuracy.","[{'name': 'Qichen Fu'}, {'name': 'Minsik Cho'}, {'name': 'Thomas Merth'}, {'name': 'Sachin Mehta'}, {'name': 'Mohammad Rastegari'}, {'name': 'Mahyar Najibi'}]",2024-07-19T06:34:45Z
http://arxiv.org/abs/2407.14056v1,http://arxiv.org/abs/2407.14056v1,"Rasa: Building Expressive Speech Synthesis Systems for Indian Languages
  in Low-resource Settings","We release Rasa, the first multilingual expressive TTS dataset for any Indian
language, which contains 10 hours of neutral speech and 1-3 hours of expressive
speech for each of the 6 Ekman emotions covering 3 languages: Assamese,
Bengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and
30 minutes of expressive data can yield a Fair system as indicated by MUSHRA
scores. Increasing neutral data to 10 hours, with minimal expressive data,
significantly enhances expressiveness. This offers a practical recipe for
resource-constrained languages, prioritizing easily obtainable neutral data
alongside smaller amounts of expressive data. We show the importance of
syllabically balanced data and pooling emotions to enhance expressiveness. We
also highlight challenges in generating specific emotions, e.g., fear and
surprise.","[{'name': 'Praveen Srinivasa Varadhan'}, {'name': 'Ashwin Sankar'}, {'name': 'Giri Raju'}, {'name': 'Mitesh M. Khapra'}]",2024-07-19T06:33:10Z
http://arxiv.org/abs/2407.14049v1,http://arxiv.org/abs/2407.14049v1,Prompted Aspect Key Point Analysis for Quantitative Review Summarization,"Key Point Analysis (KPA) aims for quantitative summarization that provides
key points (KPs) as succinct textual summaries and quantities measuring their
prevalence. KPA studies for arguments and reviews have been reported in the
literature. A majority of KPA studies for reviews adopt supervised learning to
extract short sentences as KPs before matching KPs to review comments for
quantification of KP prevalence. Recent abstractive approaches still generate
KPs based on sentences, often leading to KPs with overlapping and hallucinated
opinions, and inaccurate quantification. In this paper, we propose Prompted
Aspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA
employs aspect sentiment analysis and prompted in-context learning with Large
Language Models (LLMs) to generate and quantify KPs grounded in aspects for
business entities, which achieves faithful KPs with accurate quantification,
and removes the need for large amounts of annotated data for supervised
training. Experiments on the popular review dataset Yelp and the
aspect-oriented review summarization dataset SPACE show that our framework
achieves state-of-the-art performance. Source code and data are available at:
https://github.com/antangrocket1312/PAKPA","[{'name': 'An Quang Tang'}, {'name': 'Xiuzhen Zhang'}, {'name': 'Minh Ngoc Dinh'}, {'name': 'Erik Cambria'}]",2024-07-19T06:07:32Z
http://arxiv.org/abs/2407.14568v1,http://arxiv.org/abs/2407.14568v1,"SQLfuse: Enhancing Text-to-SQL Performance through Comprehensive LLM
  Synergy","Text-to-SQL conversion is a critical innovation, simplifying the transition
from complex SQL to intuitive natural language queries, especially significant
given SQL's prevalence in the job market across various roles. The rise of
Large Language Models (LLMs) like GPT-3.5 and GPT-4 has greatly advanced this
field, offering improved natural language understanding and the ability to
generate nuanced SQL statements. However, the potential of open-source LLMs in
Text-to-SQL applications remains underexplored, with many frameworks failing to
leverage their full capabilities, particularly in handling complex database
queries and incorporating feedback for iterative refinement. Addressing these
limitations, this paper introduces SQLfuse, a robust system integrating
open-source LLMs with a suite of tools to enhance Text-to-SQL translation's
accuracy and usability. SQLfuse features four modules: schema mining, schema
linking, SQL generation, and a SQL critic module, to not only generate but also
continuously enhance SQL query quality. Demonstrated by its leading performance
on the Spider Leaderboard and deployment by Ant Group, SQLfuse showcases the
practical merits of open-source LLMs in diverse business contexts.","[{'name': 'Tingkai Zhang'}, {'name': 'Chaoyu Chen'}, {'name': 'Cong Liao'}, {'name': 'Jun Wang'}, {'name': 'Xudong Zhao'}, {'name': 'Hang Yu'}, {'name': 'Jianchao Wang'}, {'name': 'Jianguo Li'}, {'name': 'Wenhui Shi'}]",2024-07-19T06:01:57Z
http://arxiv.org/abs/2407.14044v1,http://arxiv.org/abs/2407.14044v1,"ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing
  Functional Correctness?","Although large language models (LLMs) have been largely successful in
generating functionally correct programs, conditioning models to produce
efficient solutions while ensuring correctness remains a challenge. Further,
unreliability in benchmarking code efficiency is a hurdle across varying
hardware specifications for popular interpreted languages such as Python. In
this paper, we present ECCO, a reproducible benchmark for evaluating program
efficiency via two paradigms: natural language (NL) based code generation and
history-based code editing. On ECCO, we adapt and thoroughly investigate the
three most promising existing LLM-based approaches: in-context learning,
iterative refinement with execution or NL feedback, and fine-tuning conditioned
on execution and editing history. While most methods degrade functional
correctness and moderately increase program efficiency, we find that adding
execution information often helps maintain functional correctness, and NL
feedback enhances more on efficiency. We release our benchmark to support
future work on LLM-based generation of efficient code.","[{'name': 'Siddhant Waghjale'}, {'name': 'Vishruth Veerendranath'}, {'name': 'Zora Zhiruo Wang'}, {'name': 'Daniel Fried'}]",2024-07-19T05:47:40Z
http://arxiv.org/abs/2407.14039v1,http://arxiv.org/abs/2407.14039v1,BERTer: The Efficient One,"We explore advanced fine-tuning techniques to boost BERT's performance in
sentiment analysis, paraphrase detection, and semantic textual similarity. Our
approach leverages SMART regularization to combat overfitting, improves
hyperparameter choices, employs a cross-embedding Siamese architecture for
improved sentence embeddings, and introduces innovative early exiting methods.
Our fine-tuning findings currently reveal substantial improvements in model
efficiency and effectiveness when combining multiple fine-tuning architectures,
achieving a state-of-the-art performance score of on the test set, surpassing
current benchmarks and highlighting BERT's adaptability in multifaceted
linguistic tasks.","[{'name': 'Pradyumna Saligram'}, {'name': 'Andrew Lanpouthakoun'}]",2024-07-19T05:33:09Z
http://arxiv.org/abs/2407.14030v1,http://arxiv.org/abs/2407.14030v1,"HeCiX: Integrating Knowledge Graphs and Large Language Models for
  Biomedical Research","Despite advancements in drug development strategies, 90% of clinical trials
fail. This suggests overlooked aspects in target validation and drug
optimization. In order to address this, we introduce HeCiX-KG,
Hetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from
ClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines
data on previously conducted clinical trials from ClinicalTrials.gov, and
domain expertise on diseases and genes from Hetionet. This offers a thorough
resource for clinical researchers. Further, we introduce HeCiX, a system that
uses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability.
HeCiX shows high performance during evaluation against a range of clinically
relevant issues, proving this model to be promising for enhancing the
effectiveness of clinical research. Thus, this approach provides a more
holistic view of clinical trials and existing biological data.","[{'name': 'Prerana Sanjay Kulkarni'}, {'name': 'Muskaan Jain'}, {'name': 'Disha Sheshanarayana'}, {'name': 'Srinivasan Parthiban'}]",2024-07-19T05:04:24Z
http://arxiv.org/abs/2407.14000v1,http://arxiv.org/abs/2407.14000v1,"Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by
  Direct Preference Optimization","Extractive question answering over clinical text is a crucial need to help
deal with the deluge of clinical text generated in hospitals. While encoder
models (e.g., BERT) have been popular for this reading comprehension task,
recently encoder-decoder models (e.g., T5) are on the rise. There is also the
emergence of preference optimization techniques to align decoder-only LLMs with
human preferences. In this paper, we combine encoder-decoder models with the
direct preference optimization (DPO) method to improve over prior state of the
art for the RadQA radiology question answering task by 12-15 F1 points. To the
best of our knowledge, this effort is the first to show that DPO method also
works for reading comprehension via novel heuristics to generate preference
data without human inputs.","[{'name': 'Md Sultan Al Nahian'}, {'name': 'Ramakanth Kavuluru'}]",2024-07-19T03:12:10Z
http://arxiv.org/abs/2407.13999v1,http://arxiv.org/abs/2407.13999v1,"NeLLCom-X: A Comprehensive Neural-Agent Framework to Simulate Language
  Learning and Group Communication","Recent advances in computational linguistics include simulating the emergence
of human-like languages with interacting neural network agents, starting from
sets of random symbols. The recently introduced NeLLCom framework (Lian et al.,
2023) allows agents to first learn an artificial language and then use it to
communicate, with the aim of studying the emergence of specific linguistics
properties. We extend this framework (NeLLCom-X) by introducing more realistic
role-alternating agents and group communication in order to investigate the
interplay between language learnability, communication pressures, and group
size effects. We validate NeLLCom-X by replicating key findings from prior
research simulating the emergence of a word-order/case-marking trade-off. Next,
we investigate how interaction affects linguistic convergence and emergence of
the trade-off. The novel framework facilitates future simulations of diverse
linguistic aspects, emphasizing the importance of interaction and group
dynamics in language evolution.","[{'name': 'Yuchen Lian'}, {'name': 'Tessa Verhoef'}, {'name': 'Arianna Bisazza'}]",2024-07-19T03:03:21Z
http://arxiv.org/abs/2407.13998v1,http://arxiv.org/abs/2407.13998v1,"RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval
  Augmented Question Answering","Question answering based on retrieval augmented generation (RAG-QA) is an
important research topic in NLP and has a wide range of real-world
applications. However, most existing datasets for this task are either
constructed using a single source corpus or consist of short extractive
answers, which fall short of evaluating large language model (LLM) based RAG-QA
systems on cross-domain generalization. To address these limitations, we create
Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form
answers that integrate short extractive answers from multiple documents into a
single, coherent narrative, covering 26K queries and large corpora across seven
different domains. We further propose RAG-QA Arena by directly comparing
model-generated answers against LFRQA's answers using LLMs as evaluators. We
show via extensive experiments that RAG-QA Arena and human judgments on answer
quality are highly correlated. Moreover, only 41.3% of the most competitive
LLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a
challenging evaluation platform for future research.","[{'name': 'Rujun Han'}, {'name': 'Yuhao Zhang'}, {'name': 'Peng Qi'}, {'name': 'Yumo Xu'}, {'name': 'Jenyuan Wang'}, {'name': 'Lan Liu'}, {'name': 'William Yang Wang'}, {'name': 'Bonan Min'}, {'name': 'Vittorio Castelli'}]",2024-07-19T03:02:51Z
http://arxiv.org/abs/2407.13982v1,http://arxiv.org/abs/2407.13982v1,"Reexamining Racial Disparities in Automatic Speech Recognition
  Performance: The Role of Confounding by Provenance","Automatic speech recognition (ASR) models trained on large amounts of audio
data are now widely used to convert speech to written text in a variety of
applications from video captioning to automated assistants used in healthcare
and other domains. As such, it is important that ASR models and their use is
fair and equitable. Prior work examining the performance of commercial ASR
systems on the Corpus of Regional African American Language (CORAAL)
demonstrated significantly worse ASR performance on African American English
(AAE). The current study seeks to understand the factors underlying this
disparity by examining the performance of the current state-of-the-art neural
network based ASR system (Whisper, OpenAI) on the CORAAL dataset. Two key
findings have been identified as a result of the current study. The first
confirms prior findings of significant dialectal variation even across
neighboring communities, and worse ASR performance on AAE that can be improved
to some extent with fine-tuning of ASR models. The second is a novel finding
not discussed in prior work on CORAAL: differences in audio recording practices
within the dataset have a significant impact on ASR accuracy resulting in a
``confounding by provenance'' effect in which both language use and recording
quality differ by study location. These findings highlight the need for further
systematic investigation to disentangle the effects of recording quality and
inherent linguistic diversity when examining the fairness and bias present in
neural ASR models, as any bias in ASR accuracy may have negative downstream
effects on disparities in various domains of life in which ASR technology is
used.","[{'name': 'Changye Li'}, {'name': 'Trevor Cohen'}, {'name': 'Serguei Pakhomov'}]",2024-07-19T02:14:17Z
http://arxiv.org/abs/2407.13945v1,http://arxiv.org/abs/2407.13945v1,"FANTAstic SEquences and Where to Find Them: Faithful and Efficient API
  Call Generation through State-tracked Constrained Decoding and Reranking","API call generation is the cornerstone of large language models' tool-using
ability that provides access to the larger world. However, existing supervised
and in-context learning approaches suffer from high training costs, poor data
efficiency, and generated API calls that can be unfaithful to the API
documentation and the user's request. To address these limitations, we propose
an output-side optimization approach called FANTASE. Two of the unique
contributions of FANTASE are its State-Tracked Constrained Decoding (SCD) and
Reranking components. SCD dynamically incorporates appropriate API constraints
in the form of Token Search Trie for efficient and guaranteed generation
faithfulness with respect to the API documentation. The Reranking component
efficiently brings in the supervised signal by leveraging a lightweight model
as the discriminator to rerank the beam-searched candidate generations of the
large language model. We demonstrate the superior performance of FANTASE in API
call generation accuracy, inference efficiency, and context efficiency with
DSTC8 and API Bank datasets.","[{'name': 'Zhuoer Wang'}, {'name': 'Leonardo F. R. Ribeiro'}, {'name': 'Alexandros Papangelis'}, {'name': 'Rohan Mukherjee'}, {'name': 'Tzu-Yen Wang'}, {'name': 'Xinyan Zhao'}, {'name': 'Arijit Biswas'}, {'name': 'James Caverlee'}, {'name': 'Angeliki Metallinou'}]",2024-07-18T23:44:02Z
http://arxiv.org/abs/2407.13943v1,http://arxiv.org/abs/2407.13943v1,Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction,"This paper introduces Werewolf Arena, a novel framework for evaluating large
language models (LLMs) through the lens of the classic social deduction game,
Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the
game's complex dynamics of deception, deduction, and persuasion. The framework
introduces a dynamic turn-taking system based on bidding, mirroring real-world
discussions where individuals strategically choose when to speak. We
demonstrate the framework's utility through an arena-style tournament featuring
Gemini and GPT models. Our results reveal distinct strengths and weaknesses in
the models' strategic reasoning and communication. These findings highlight
Werewolf Arena's potential as a challenging and scalable LLM benchmark.","[{'name': 'Suma Bailis'}, {'name': 'Jane Friedhoff'}, {'name': 'Feiyang Chen'}]",2024-07-18T23:41:05Z
http://arxiv.org/abs/2407.13928v1,http://arxiv.org/abs/2407.13928v1,"BiasDPO: Mitigating Bias in Language Models through Direct Preference
  Optimization","Large Language Models (LLMs) have become pivotal in advancing natural
language processing, yet their potential to perpetuate biases poses significant
concerns. This paper introduces a new framework employing Direct Preference
Optimization (DPO) to mitigate gender, racial, and religious biases in
LLM-generated English text. By developing a loss function that favors less
biased over biased completions, our approach cultivates a preference for
respectful and non-discriminatory language in LLMs. We also contribute a
manually designed dataset for training LLMs to recognize and correct biases.
This dataset encompasses a diverse range of prompts paired with both biased and
unbiased completions. Implementing this approach on the Microsoft Phi-2 model,
we demonstrate substantial reductions in biased outputs as our model
outperforms the baseline model on almost all bias benchmarks. Our model also
achieves better performance compared to other open-source models on most
benchmarks. By reducing biases in the language generated by the model, our
study marks a significant step towards developing more ethical and socially
responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.",[{'name': 'Ahmed Allam'}],2024-07-18T22:32:20Z
http://arxiv.org/abs/2407.13906v1,http://arxiv.org/abs/2407.13906v1,Crafting Efficient Fine-Tuning Strategies for Large Language Models,"This paper addresses the challenges of efficiently fine-tuning large language
models (LLMs) by exploring data efficiency and hyperparameter optimization. We
investigate the minimum data required for effective fine-tuning and propose a
novel hyperparameter optimization method that leverages early-stage model
performance. Our experiments demonstrate that fine-tuning with as few as 200
samples can improve model accuracy from 70\% to 88\% in a product attribute
extraction task. We identify a saturation point of approximately 6,500 samples,
beyond which additional data yields diminishing returns. Our proposed bayesian
hyperparameter optimization method, which evaluates models at 20\% of total
training time, correlates strongly with final model performance, with 4 out of
5 top early-stage models remaining in the top 5 at completion. This approach
led to a 2\% improvement in accuracy over baseline models when evaluated on an
independent test set. These findings offer actionable insights for
practitioners, potentially reducing computational load and dependency on
extensive datasets while enhancing overall performance of fine-tuned LLMs.","[{'name': 'Michael Oliver'}, {'name': 'Guan Wang'}]",2024-07-18T21:36:00Z
http://arxiv.org/abs/2408.00005v1,http://arxiv.org/abs/2408.00005v1,"Framework for Curating Speech Datasets and Evaluating ASR Systems: A
  Case Study for Polish","Speech datasets available in the public domain are often underutilized
because of challenges in discoverability and interoperability. A comprehensive
framework has been designed to survey, catalog, and curate available speech
datasets, which allows replicable evaluation of automatic speech recognition
(ASR) systems. A case study focused on the Polish language was conducted; the
framework was applied to curate more than 24 datasets and evaluate 25
combinations of ASR systems and models. This research constitutes the most
extensive comparison to date of both commercial and free ASR systems for the
Polish language. It draws insights from 600 system-model-test set evaluations,
marking a significant advancement in both scale and comprehensiveness. The
results of surveys and performance comparisons are available as interactive
dashboards (https://huggingface.co/spaces/amu-cai/pl-asr-leaderboard) along
with curated datasets (https://huggingface.co/datasets/amu-cai/pl-asr-bigos-v2,
https://huggingface.co/datasets/pelcra/pl-asr-pelcra-for-bigos) and the open
challenge call (https://poleval.pl/tasks/task3). Tools used for evaluation are
open-sourced (https://github.com/goodmike31/pl-asr-bigos-tools), facilitating
replication and adaptation for other languages, as well as continuous expansion
with new datasets and systems.",[{'name': 'Michał Junczyk'}],2024-07-18T21:32:12Z
http://arxiv.org/abs/2407.13891v1,http://arxiv.org/abs/2407.13891v1,"Uncovering Political Bias in Emotion Inference Models: Implications for
  sentiment analysis in social science research","This paper investigates the presence of political bias in emotion inference
models used for sentiment analysis (SA) in social science research. Machine
learning models often reflect biases in their training data, impacting the
validity of their outcomes. While previous research has highlighted gender and
race biases, our study focuses on political bias - an underexplored yet
pervasive issue that can skew the interpretation of text data across a wide
array of studies. We conducted a bias audit on a Polish sentiment analysis
model developed in our lab. By analyzing valence predictions for names and
sentences involving Polish politicians, we uncovered systematic differences
influenced by political affiliations. Our findings indicate that annotations by
human raters propagate political biases into the model's predictions. To
mitigate this, we pruned the training dataset of texts mentioning these
politicians and observed a reduction in bias, though not its complete
elimination. Given the significant implications of political bias in SA, our
study emphasizes caution in employing these models for social science research.
We recommend a critical examination of SA results and propose using
lexicon-based systems as a more ideologically neutral alternative. This paper
underscores the necessity for ongoing scrutiny and methodological adjustments
to ensure the reliability and impartiality of the use of machine learning in
academic and applied contexts.","[{'name': 'Hubert Plisiecki'}, {'name': 'Paweł Lenartowicz'}, {'name': 'Maria Flakus'}, {'name': 'Artur Pokropek'}]",2024-07-18T20:31:07Z
http://arxiv.org/abs/2407.13887v1,http://arxiv.org/abs/2407.13887v1,Learning Goal-Conditioned Representations for Language Reward Models,"Techniques that learn improved representations via offline data or
self-supervised objectives have shown impressive results in traditional
reinforcement learning (RL). Nevertheless, it is unclear how improved
representation learning can benefit reinforcement learning from human feedback
(RLHF) on language models (LMs). In this work, we propose training reward
models (RMs) in a contrastive, $\textit{goal-conditioned}$ fashion by
increasing the representation similarity of future states along sampled
preferred trajectories and decreasing the similarity along randomly sampled
dispreferred trajectories. This objective significantly improves RM performance
by up to 0.09 AUROC across challenging benchmarks, such as MATH and GSM8k.
These findings extend to general alignment as well -- on the Helpful-Harmless
dataset, we observe $2.3\%$ increase in accuracy. Beyond improving reward model
performance, we show this way of training RM representations enables improved
$\textit{steerability}$ because it allows us to evaluate the likelihood of an
action achieving a particular goal-state (e.g., whether a solution is correct
or helpful). Leveraging this insight, we find that we can filter up to $55\%$
of generated tokens during majority voting by discarding trajectories likely to
end up in an ""incorrect"" state, which leads to significant cost savings. We
additionally find that these representations can perform fine-grained control
by conditioning on desired future goal-states. For example, we show that
steering a Llama 3 model towards helpful generations with our approach improves
helpfulness by $9.6\%$ over a supervised-fine-tuning trained baseline.
Similarly, steering the model towards complex generations improves complexity
by $21.6\%$ over the baseline. Overall, we find that training RMs in this
contrastive, goal-conditioned fashion significantly improves performance and
enables model steerability.","[{'name': 'Vaskar Nath'}, {'name': 'Dylan Slack'}, {'name': 'Jeff Da'}, {'name': 'Yuntao Ma'}, {'name': 'Hugh Zhang'}, {'name': 'Spencer Whitehead'}, {'name': 'Sean Hendryx'}]",2024-07-18T20:23:11Z
http://arxiv.org/abs/2407.14562v2,http://arxiv.org/abs/2407.14562v2,"Thought-Like-Pro: Enhancing Reasoning of Large Language Models through
  Self-Driven Prolog-based Chain-of-Thought","Large language models (LLMs) have shown exceptional performance as
general-purpose assistants, excelling across a variety of reasoning tasks. This
achievement represents a significant step toward achieving artificial general
intelligence (AGI). Despite these advancements, the effectiveness of LLMs often
hinges on the specific prompting strategies employed, and there remains a lack
of a robust framework to facilitate learning and generalization across diverse
reasoning tasks. To address these challenges, we introduce a novel learning
framework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to
imitate the Chain-of-Thought (CoT) process which is verified and translated
from reasoning trajectories generated by a symbolic Prolog logic engine. This
framework proceeds in a self-driven manner, that enables LLMs to formulate
rules and statements from given instructions and leverage the symbolic Prolog
engine to derive results. Subsequently, LLMs convert Prolog-derived successive
reasoning trajectories into natural language CoT for imitation learning. Our
empirical findings indicate that our proposed approach substantially enhances
the reasoning abilities of LLMs and demonstrates robust generalization across
out-of-distribution reasoning tasks.","[{'name': 'Xiaoyu Tan'}, {'name': 'Yongxin Deng'}, {'name': 'Xihe Qiu'}, {'name': 'Weidi Xu'}, {'name': 'Chao Qu'}, {'name': 'Wei Chu'}, {'name': 'Yinghui Xu'}, {'name': 'Yuan Qi'}]",2024-07-18T18:52:10Z
http://arxiv.org/abs/2407.13833v1,http://arxiv.org/abs/2407.13833v1,"Phi-3 Safety Post-Training: Aligning Language Models with a ""Break-Fix""
  Cycle","Recent innovations in language model training have demonstrated that it is
possible to create highly performant models that are small enough to run on a
smartphone. As these models are deployed in an increasing number of domains, it
is critical to ensure that they are aligned with human preferences and safety
considerations. In this report, we present our methodology for safety aligning
the Phi-3 series of language models. We utilized a ""break-fix"" cycle,
performing multiple rounds of dataset curation, safety post-training,
benchmarking, red teaming, and vulnerability identification to cover a variety
of harm areas in both single and multi-turn scenarios. Our results indicate
that this approach iteratively improved the performance of the Phi-3 models
across a wide range of responsible AI benchmarks.","[{'name': 'Emman Haider'}, {'name': 'Daniel Perez-Becker'}, {'name': 'Thomas Portet'}, {'name': 'Piyush Madan'}, {'name': 'Amit Garg'}, {'name': 'David Majercak'}, {'name': 'Wen Wen'}, {'name': 'Dongwoo Kim'}, {'name': 'Ziyi Yang'}, {'name': 'Jianwen Zhang'}, {'name': 'Hiteshi Sharma'}, {'name': 'Blake Bullwinkel'}, {'name': 'Martin Pouliot'}, {'name': 'Amanda Minnich'}, {'name': 'Shiven Chawla'}, {'name': 'Solianna Herrera'}, {'name': 'Shahed Warreth'}, {'name': 'Maggie Engler'}, {'name': 'Gary Lopez'}, {'name': 'Nina Chikanov'}, {'name': 'Raja Sekhar Rao Dheekonda'}, {'name': 'Bolor-Erdene Jagdagdorj'}, {'name': 'Roman Lutz'}, {'name': 'Richard Lundeen'}, {'name': 'Tori Westerhoff'}, {'name': 'Pete Bryan'}, {'name': 'Christian Seifert'}, {'name': 'Ram Shankar Siva Kumar'}, {'name': 'Andrew Berkley'}, {'name': 'Alex Kessler'}]",2024-07-18T18:06:59Z
http://arxiv.org/abs/2407.13765v2,http://arxiv.org/abs/2407.13765v2,"Latent Causal Probing: A Formal Perspective on Probing with Causal
  Models of Data","As language models (LMs) deliver increasing performance on a range of NLP
tasks, probing classifiers have become an indispensable technique in the effort
to better understand their inner workings. A typical setup involves (1)
defining an auxiliary task consisting of a dataset of text annotated with
labels, then (2) supervising small classifiers to predict the labels from the
representations of a pretrained LM as it processed the dataset. A high probing
accuracy is interpreted as evidence that the LM has learned to perform the
auxiliary task as an unsupervised byproduct of its original pretraining
objective. Despite the widespread usage of probes, however, the robust design
and analysis of probing experiments remains a challenge. We develop a formal
perspective on probing using structural causal models (SCM). Specifically,
given an SCM which explains the distribution of tokens observed during
training, we frame the central hypothesis as whether the LM has learned to
represent the latent variables of the SCM. Empirically, we extend a recent
study of LMs in the context of a synthetic grid-world navigation task, where
having an exact model of the underlying causal structure allows us to draw
strong inferences from the result of probing experiments. Our techniques
provide robust empirical evidence for the ability of LMs to induce the latent
concepts underlying text.","[{'name': 'Charles Jin'}, {'name': 'Martin Rinard'}]",2024-07-18T17:59:27Z
http://arxiv.org/abs/2407.13757v1,http://arxiv.org/abs/2407.13757v1,"Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation
  of Large Language Models","Retrieval-Augmented Generation (RAG) is applied to solve hallucination
problems and real-time constraints of large language models, but it also
induces vulnerabilities against retrieval corruption attacks. Existing research
mainly explores the unreliability of RAG in white-box and closed-domain QA
tasks. In this paper, we aim to reveal the vulnerabilities of
Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks
for opinion manipulation. We explore the impact of such attacks on user
cognition and decision-making, providing new insight to enhance the reliability
and security of RAG models. We manipulate the ranking results of the retrieval
model in RAG with instruction and use these results as data to train a
surrogate model. By employing adversarial retrieval attack methods to the
surrogate model, black-box transfer attacks on RAG are further realized.
Experiments conducted on opinion datasets across multiple topics show that the
proposed attack strategy can significantly alter the opinion polarity of the
content generated by RAG. This demonstrates the model's vulnerability and, more
importantly, reveals the potential negative impact on user cognition and
decision-making, making it easier to mislead users into accepting incorrect or
biased information.","[{'name': 'Zhuo Chen'}, {'name': 'Jiawei Liu'}, {'name': 'Haotan Liu'}, {'name': 'Qikai Cheng'}, {'name': 'Fan Zhang'}, {'name': 'Wei Lu'}, {'name': 'Xiaozhong Liu'}]",2024-07-18T17:55:55Z
http://arxiv.org/abs/2407.13744v1,http://arxiv.org/abs/2407.13744v1,"LLMs as Function Approximators: Terminology, Taxonomy, and Questions for
  Evaluation","Natural Language Processing has moved rather quickly from modelling specific
tasks to taking more general pre-trained models and fine-tuning them for
specific tasks, to a point where we now have what appear to be inherently
generalist models. This paper argues that the resultant loss of clarity on what
these models model leads to metaphors like ""artificial general intelligences""
that are not helpful for evaluating their strengths and weaknesses. The
proposal is to see their generality, and their potential value, in their
ability to approximate specialist function, based on a natural language
specification. This framing brings to the fore questions of the quality of the
approximation, but beyond that, also questions of discoverability, stability,
and protectability of these functions. As the paper will show, this framing
hence brings together in one conceptual framework various aspects of
evaluation, both from a practical and a theoretical perspective, as well as
questions often relegated to a secondary status (such as ""prompt injection"" and
""jailbreaking"").",[{'name': 'David Schlangen'}],2024-07-18T17:49:56Z
http://arxiv.org/abs/2407.13702v1,http://arxiv.org/abs/2407.13702v1,"ANHALTEN: Cross-Lingual Transfer for German Token-Level Reference-Free
  Hallucination Detection","Research on token-level reference-free hallucination detection has
predominantly focused on English, primarily due to the scarcity of robust
datasets in other languages. This has hindered systematic investigations into
the effectiveness of cross-lingual transfer for this important NLP application.
To address this gap, we introduce ANHALTEN, a new evaluation dataset that
extends the English hallucination detection dataset to German. To the best of
our knowledge, this is the first work that explores cross-lingual transfer for
token-level reference-free hallucination detection. ANHALTEN contains gold
annotations in German that are parallel (i.e., directly comparable to the
original English instances). We benchmark several prominent cross-lingual
transfer approaches, demonstrating that larger context length leads to better
hallucination detection in German, even without succeeding context.
Importantly, we show that the sample-efficient few-shot transfer is the most
effective approach in most setups. This highlights the practical benefits of
minimal annotation effort in the target language for reference-free
hallucination detection. Aiming to catalyze future research on cross-lingual
token-level reference-free hallucination detection, we make ANHALTEN publicly
available: https://github.com/janekh24/anhalten","[{'name': 'Janek Herrlein'}, {'name': 'Chia-Chien Hung'}, {'name': 'Goran Glavaš'}]",2024-07-18T17:01:38Z
http://arxiv.org/abs/2407.13696v1,http://arxiv.org/abs/2407.13696v1,"Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark
  Evaluation","Recent advancements in Language Models (LMs) have catalyzed the creation of
multiple benchmarks, designed to assess these models' general capabilities. A
crucial task, however, is assessing the validity of the benchmarks themselves.
This is most commonly done via Benchmark Agreement Testing (BAT), where new
benchmarks are validated against established ones using some agreement metric
(e.g., rank correlation). Despite the crucial role of BAT for benchmark
builders and consumers, there are no standardized procedures for such agreement
testing. This deficiency can lead to invalid conclusions, fostering mistrust in
benchmarks and upending the ability to properly choose the appropriate
benchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how
some overlooked methodological choices can significantly influence BAT results,
potentially undermining the validity of conclusions. To address these
inconsistencies, we propose a set of best practices for BAT and demonstrate how
utilizing these methodologies greatly improves BAT robustness and validity. To
foster adoption and facilitate future research,, we introduce BenchBench, a
python package for BAT, and release the BenchBench-leaderboard, a
meta-benchmark designed to evaluate benchmarks using their peers. Our findings
underscore the necessity for standardized BAT, ensuring the robustness and
validity of benchmark evaluations in the evolving landscape of language model
research.
  BenchBench Package: https://github.com/IBM/BenchBench
  Leaderboard: https://huggingface.co/spaces/per/BenchBench","[{'name': 'Yotam Perlitz'}, {'name': 'Ariel Gera'}, {'name': 'Ofir Arviv'}, {'name': 'Asaf Yehudai'}, {'name': 'Elron Bandel'}, {'name': 'Eyal Shnarch'}, {'name': 'Michal Shmueli-Scheuer'}, {'name': 'Leshem Choshen'}]",2024-07-18T17:00:23Z
http://arxiv.org/abs/2407.13647v1,http://arxiv.org/abs/2407.13647v1,Weak-to-Strong Reasoning,"When large language models (LLMs) exceed human-level capabilities, it becomes
increasingly challenging to provide full-scale and accurate supervisions for
these models. Weak-to-strong learning, which leverages a less capable model to
unlock the latent abilities of a stronger model, proves valuable in this
context. Yet, the efficacy of this approach for complex reasoning tasks is
still untested. Furthermore, tackling reasoning tasks under the weak-to-strong
setting currently lacks efficient methods to avoid blindly imitating the weak
supervisor including its errors. In this paper, we introduce a progressive
learning framework that enables the strong model to autonomously refine its
training data, without requiring input from either a more advanced model or
human-annotated data. This framework begins with supervised fine-tuning on a
selective small but high-quality dataset, followed by preference optimization
on contrastive samples identified by the strong model itself. Extensive
experiments on the GSM8K and MATH datasets demonstrate that our method
significantly enhances the reasoning capabilities of Llama2-70b using three
separate weak models. This method is further validated in a forward-looking
experimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b
on the highly challenging OlympicArena dataset. This work paves the way for a
more scalable and sophisticated strategy to enhance AI reasoning powers. All
relevant code and resources are available in
\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.","[{'name': 'Yuqing Yang'}, {'name': 'Yan Ma'}, {'name': 'Pengfei Liu'}]",2024-07-18T16:25:17Z
http://arxiv.org/abs/2407.13638v1,http://arxiv.org/abs/2407.13638v1,"A Comparative Study on Automatic Coding of Medical Letters with
  Explainability","This study aims to explore the implementation of Natural Language Processing
(NLP) and machine learning (ML) techniques to automate the coding of medical
letters with visualised explainability and light-weighted local computer
settings. Currently in clinical settings, coding is a manual process that
involves assigning codes to each condition, procedure, and medication in a
patient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There
are preliminary research on automatic coding in this field using
state-of-the-art ML models; however, due to the complexity and size of the
models, the real-world deployment is not achieved. To further facilitate the
possibility of automatic coding practice, we explore some solutions in a local
computer setting; in addition, we explore the function of explainability for
transparency of AI models. We used the publicly available MIMIC-III database
and the HAN/HLAN network models for ICD code prediction purposes. We also
experimented with the mapping between ICD and SNOMED CT knowledge bases. In our
experiments, the models provided useful information for 97.98\% of codes. The
result of this investigation can shed some light on implementing automatic
clinical coding in practice, such as in hospital settings, on the local
computers used by clinicians , project page
\url{https://github.com/Glenj01/Medical-Coding}.","[{'name': 'Jamie Glen'}, {'name': 'Lifeng Han'}, {'name': 'Paul Rayson'}, {'name': 'Goran Nenadic'}]",2024-07-18T16:12:47Z
http://arxiv.org/abs/2407.13623v2,http://arxiv.org/abs/2407.13623v2,Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies,"Research on scaling large language models (LLMs) has primarily focused on
model parameters and training data size, overlooking the role of vocabulary
size. We investigate how vocabulary size impacts LLM scaling laws by training
models ranging from 33M to 3B parameters on up to 500B characters with various
vocabulary configurations. We propose three complementary approaches for
predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative
estimation, and parametric fit of the loss function. Our approaches converge on
the same result that the optimal vocabulary size depends on the available
compute budget and that larger models deserve larger vocabularies. However,
most LLMs use too small vocabulary sizes. For example, we predict that the
optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times
larger than its vocabulary of 32K. We validate our predictions empirically by
training models with 3B parameters across different FLOPs budgets. Adopting our
predicted optimal vocabulary size consistently improves downstream performance
over commonly used vocabulary sizes. By increasing the vocabulary size from the
conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to
32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly
considering model parameters and vocabulary size for efficient scaling.","[{'name': 'Chaofan Tao'}, {'name': 'Qian Liu'}, {'name': 'Longxu Dou'}, {'name': 'Niklas Muennighoff'}, {'name': 'Zhongwei Wan'}, {'name': 'Ping Luo'}, {'name': 'Min Lin'}, {'name': 'Ngai Wong'}]",2024-07-18T15:58:54Z
http://arxiv.org/abs/2407.13608v1,http://arxiv.org/abs/2407.13608v1,"dzNLP at NADI 2024 Shared Task: Multi-Classifier Ensemble with Weighted
  Voting and TF-IDF Features","This paper presents the contribution of our dzNLP team to the NADI 2024
shared task, specifically in Subtask 1 - Multi-label Country-level Dialect
Identification (MLDID) (Closed Track). We explored various configurations to
address the challenge: in Experiment 1, we utilized a union of n-gram analyzers
(word, character, character with word boundaries) with different n-gram values;
in Experiment 2, we combined a weighted union of Term Frequency-Inverse
Document Frequency (TF-IDF) features with various weights; and in Experiment 3,
we implemented a weighted major voting scheme using three classifiers: Linear
Support Vector Classifier (LSVC), Random Forest (RF), and K-Nearest Neighbors
(KNN).
  Our approach, despite its simplicity and reliance on traditional machine
learning techniques, demonstrated competitive performance in terms of F1-score
and precision. Notably, we achieved the highest precision score of 63.22% among
the participating teams. However, our overall F1 score was approximately 21%,
significantly impacted by a low recall rate of 12.87%. This indicates that
while our models were highly precise, they struggled to recall a broad range of
dialect labels, highlighting a critical area for improvement in handling
diverse dialectal variations.","[{'name': 'Mohamed Lichouri'}, {'name': 'Khaled Lounnas'}, {'name': 'Boualem Nadjib Zahaf'}, {'name': 'Mehdi Ayoub Rabiai'}]",2024-07-18T15:47:42Z
http://arxiv.org/abs/2407.13603v1,http://arxiv.org/abs/2407.13603v1,"dzStance at StanceEval2024: Arabic Stance Detection based on Sentence
  Transformers","This study compares Term Frequency-Inverse Document Frequency (TF-IDF)
features with Sentence Transformers for detecting writers' stances--favorable,
opposing, or neutral--towards three significant topics: COVID-19 vaccine,
digital transformation, and women empowerment. Through empirical evaluation, we
demonstrate that Sentence Transformers outperform TF-IDF features across
various experimental setups. Our team, dzStance, participated in a stance
detection competition, achieving the 13th position (74.91%) among 15 teams in
Women Empowerment, 10th (73.43%) in COVID Vaccine, and 12th (66.97%) in Digital
Transformation. Overall, our team's performance ranked 13th (71.77%) among all
participants. Notably, our approach achieved promising F1-scores, highlighting
its effectiveness in identifying writers' stances on diverse topics. These
results underscore the potential of Sentence Transformers to enhance stance
detection models for addressing critical societal issues.","[{'name': 'Mohamed Lichouri'}, {'name': 'Khaled Lounnas'}, {'name': 'Khelil Rafik Ouaras'}, {'name': 'Mohamed Abi'}, {'name': 'Anis Guechtouli'}]",2024-07-18T15:43:27Z
http://arxiv.org/abs/2407.13597v1,http://arxiv.org/abs/2407.13597v1,"PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like
  (PL) Tasks","Text summarization is a well-studied problem that deals with deriving
insights from unstructured text consumed by humans, and it has found extensive
business applications. However, many real-life tasks involve generating a
series of actions to achieve specific goals, such as workflows, recipes,
dialogs, and travel plans. We refer to them as planning-like (PL) tasks noting
that the main commonality they share is control flow information. which may be
partially specified. Their structure presents an opportunity to create more
practical summaries to help users make quick decisions. We investigate this
observation by introducing a novel plan summarization problem, presenting a
dataset, and providing a baseline method for generating PL summaries. Using
quantitative metrics and qualitative user studies to establish baselines, we
evaluate the plan summaries from our method and large language models. We
believe the novel problem and dataset can reinvigorate research in
summarization, which some consider as a solved problem.","[{'name': 'Vishal Pallagani'}, {'name': 'Biplav Srivastava'}, {'name': 'Nitin Gupta'}]",2024-07-18T15:36:02Z
http://arxiv.org/abs/2407.13579v1,http://arxiv.org/abs/2407.13579v1,Towards Zero-Shot Multimodal Machine Translation,"Current multimodal machine translation (MMT) systems rely on fully supervised
data (i.e models are trained on sentences with their translations and
accompanying images). However, this type of data is costly to collect, limiting
the extension of MMT to other language pairs for which such data does not
exist. In this work, we propose a method to bypass the need for fully
supervised data to train MMT systems, using multimodal English data only. Our
method, called ZeroMMT, consists in adapting a strong text-only machine
translation (MT) model by training it on a mixture of two objectives: visually
conditioned masked language modelling and the Kullback-Leibler divergence
between the original and new MMT outputs. We evaluate on standard MMT
benchmarks and the recently released CoMMuTE, a contrastive benchmark aiming to
evaluate how well models use images to disambiguate English sentences. We
obtain disambiguation performance close to state-of-the-art MMT models trained
additionally on fully supervised examples. To prove that our method generalizes
to languages with no fully supervised training data available, we extend the
CoMMuTE evaluation dataset to three new languages: Arabic, Russian and Chinese.
We further show that we can control the trade-off between disambiguation
capabilities and translation fidelity at inference time using classifier-free
guidance and without any additional data. Our code, data and trained models are
publicly accessible.","[{'name': 'Matthieu Futeral'}, {'name': 'Cordelia Schmid'}, {'name': 'Benoît Sagot'}, {'name': 'Rachel Bawden'}]",2024-07-18T15:20:31Z
http://arxiv.org/abs/2407.13578v1,http://arxiv.org/abs/2407.13578v1,Large Language Models as Reliable Knowledge Bases?,"The NLP community has recently shown a growing interest in leveraging Large
Language Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential
knowledge bases (KBs). However, the reliability and extent to which LLMs can
function as KBs remain underexplored. While previous studies suggest LLMs can
encode knowledge within their parameters, the amount of parametric knowledge
alone is not sufficient to evaluate their effectiveness as KBs. This study
defines criteria that a reliable LLM-as-KB should meet, focusing on factuality
and consistency, and covering both seen and unseen knowledge. We develop
several metrics based on these criteria and use them to evaluate 26 popular
LLMs, while providing a comprehensive analysis of the effects of model size,
instruction tuning, and in-context learning (ICL). Our results paint a worrying
picture. Even a high-performant model like GPT-3.5-turbo is not factual or
consistent, and strategies like ICL and fine-tuning are unsuccessful at making
LLMs better KBs.","[{'name': 'Danna Zheng'}, {'name': 'Mirella Lapata'}, {'name': 'Jeff Z. Pan'}]",2024-07-18T15:20:18Z
http://arxiv.org/abs/2407.13571v1,http://arxiv.org/abs/2407.13571v1,New Capability to Look Up an ASL Sign from a Video Example,"Looking up an unknown sign in an ASL dictionary can be difficult. Most ASL
dictionaries are organized based on English glosses, despite the fact that (1)
there is no convention for assigning English-based glosses to ASL signs; and
(2) there is no 1-1 correspondence between ASL signs and English words.
Furthermore, what if the user does not know either the meaning of the target
sign or its possible English translation(s)? Some ASL dictionaries enable
searching through specification of articulatory properties, such as handshapes,
locations, movement properties, etc. However, this is a cumbersome process and
does not always result in successful lookup. Here we describe a new system,
publicly shared on the Web, to enable lookup of a video of an ASL sign (e.g., a
webcam recording or a clip from a continuous signing video). The user submits a
video for analysis and is presented with the five most likely sign matches, in
decreasing order of likelihood, so that the user can confirm the selection and
then be taken to our ASLLRP Sign Bank entry for that sign. Furthermore, this
video lookup is also integrated into our newest version of SignStream(R)
software to facilitate linguistic annotation of ASL video data, enabling the
user to directly look up a sign in the video being annotated, and, upon
confirmation of the match, to directly enter into the annotation the gloss and
features of that sign, greatly increasing the efficiency and consistency of
linguistic annotations of ASL video data.","[{'name': 'Carol Neidle'}, {'name': 'Augustine Opoku'}, {'name': 'Carey Ballard'}, {'name': 'Yang Zhou'}, {'name': 'Xiaoxiao He'}, {'name': 'Gregory Dimitriadis'}, {'name': 'Dimitris Metaxas'}]",2024-07-18T15:14:35Z
http://arxiv.org/abs/2407.13565v1,http://arxiv.org/abs/2407.13565v1,"dzFinNlp at AraFinNLP: Improving Intent Detection in Financial
  Conversational Agents","In this paper, we present our dzFinNlp team's contribution for intent
detection in financial conversational agents, as part of the AraFinNLP shared
task. We experimented with various models and feature configurations, including
traditional machine learning methods like LinearSVC with TF-IDF, as well as
deep learning models like Long Short-Term Memory (LSTM). Additionally, we
explored the use of transformer-based models for this task. Our experiments
show promising results, with our best model achieving a micro F1-score of
93.02% and 67.21% on the ArBanking77 dataset, in the development and test sets,
respectively.","[{'name': 'Mohamed Lichouri'}, {'name': 'Khaled Lounnas'}, {'name': 'Mohamed Zakaria Amziane'}]",2024-07-18T14:37:20Z
http://arxiv.org/abs/2407.13561v1,http://arxiv.org/abs/2407.13561v1,"Research on Tibetan Tourism Viewpoints information generation system
  based on LLM","Tibet, ensconced within China's territorial expanse, is distinguished by its
labyrinthine and heterogeneous topography, a testament to its profound
historical heritage, and the cradle of a unique religious ethos. The very
essence of these attributes, however, has impeded the advancement of Tibet's
tourism service infrastructure, rendering existing smart tourism services
inadequate for the region's visitors. This study delves into the ramifications
of informational disparities at tourist sites on Tibetan tourism and addresses
the challenge of establishing the Large Language Model (LLM) evaluation
criteria. It introduces an innovative approach, the DualGen Bridge AI system,
employing supervised fine-tuning techniques to bolster model functionality and
enhance optimization processes. Furthermore, it pioneers a multi-structured
generative results assessment framework. Empirical validation confirms the
efficacy of this framework. The study also explores the application of the
supervised fine-tuning method within the proprietary DualGen Bridge AI, aimed
at refining the generation of tourist site information. The study's findings
offer valuable insights for optimizing system performance and provide support
and inspiration for the application of LLM technology in Tibet's tourism
services and beyond, potentially revolutionizing the smart tourism industry
with advanced, tailored information generation capabilities.","[{'name': 'Jinhu Qi'}, {'name': 'Shuai Yan'}, {'name': 'Wentao Zhang'}, {'name': 'Yibo Zhang'}, {'name': 'Zirui Liu'}, {'name': 'Ke Wang'}]",2024-07-18T14:31:53Z
http://arxiv.org/abs/2407.13559v1,http://arxiv.org/abs/2407.13559v1,"Qalam : A Multimodal LLM for Arabic Optical Character and Handwriting
  Recognition","Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)
pose unique challenges due to the cursive and context-sensitive nature of the
Arabic script. This study introduces Qalam, a novel foundation model designed
for Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder
architecture. Our model significantly outperforms existing methods, achieving a
Word Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We
train Qalam on a diverse dataset, including over 4.5 million images from Arabic
manuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,
Qalam demonstrates exceptional handling of Arabic diacritics, a critical
feature in Arabic scripts. Furthermore, it shows a remarkable ability to
process high-resolution inputs, addressing a common limitation in current OCR
systems. These advancements underscore Qalam's potential as a leading solution
for Arabic script recognition, offering a significant leap in accuracy and
efficiency.","[{'name': 'Gagan Bhatia'}, {'name': 'El Moatez Billah Nagoudi'}, {'name': 'Fakhraddin Alwajih'}, {'name': 'Muhammad Abdul-Mageed'}]",2024-07-18T14:31:09Z
http://arxiv.org/abs/2407.13511v1,http://arxiv.org/abs/2407.13511v1,"Can Open-Source LLMs Compete with Commercial Models? Exploring the
  Few-Shot Performance of Current GPT Models in Biomedical Tasks","Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT
and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)
benchmarks across different domains. New competing Open-Source alternatives
like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while
often offering higher throughput and being less costly to use. Open-Source LLMs
can also be self-hosted, which makes them interesting for enterprise and
clinical use cases where sensitive data should not be processed by third
parties. We participated in the 12th BioASQ challenge, which is a retrieval
augmented generation (RAG) setting, and explored the performance of current GPT
models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning
(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional
relevant knowledge from Wikipedia added to the context-window of the LLM might
improve their performance. Mixtral 8x7b was competitive in the 10-shot setting,
both with and without fine-tuning, but failed to produce usable results in the
zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to
measurable performance gains. Our results indicate that the performance gap
between commercial and open-source models in RAG setups exists mainly in the
zero-shot setting and can be closed by simply collecting few-shot examples for
domain-specific use cases. The code needed to rerun these experiments is
available through GitHub.","[{'name': 'Samy Ateia'}, {'name': 'Udo Kruschwitz'}]",2024-07-18T13:43:01Z
http://arxiv.org/abs/2407.13509v1,http://arxiv.org/abs/2407.13509v1,"Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous
  Behaviors Based on Language Models","Spontaneous style speech synthesis, which aims to generate human-like speech,
often encounters challenges due to the scarcity of high-quality data and
limitations in model capabilities. Recent language model-based TTS systems can
be trained on large, diverse, and low-quality speech datasets, resulting in
highly natural synthesized speech. However, they are limited by the difficulty
of simulating various spontaneous behaviors and capturing prosody variations in
spontaneous speech. In this paper, we propose a novel spontaneous speech
synthesis system based on language models. We systematically categorize and
uniformly model diverse spontaneous behaviors. Moreover, fine-grained prosody
modeling is introduced to enhance the model's ability to capture subtle prosody
variations in spontaneous speech.Experimental results show that our proposed
method significantly outperforms the baseline methods in terms of prosody
naturalness and spontaneous behavior naturalness.","[{'name': 'Weiqin Li'}, {'name': 'Peiji Yang'}, {'name': 'Yicheng Zhong'}, {'name': 'Yixuan Zhou'}, {'name': 'Zhisheng Wang'}, {'name': 'Zhiyong Wu'}, {'name': 'Xixin Wu'}, {'name': 'Helen Meng'}]",2024-07-18T13:42:38Z
http://arxiv.org/abs/2407.13492v1,http://arxiv.org/abs/2407.13492v1,"Enhancing Biomedical Knowledge Discovery for Diseases: An End-To-End
  Open-Source Framework","The ever-growing volume of biomedical publications creates a critical need
for efficient knowledge discovery. In this context, we introduce an open-source
end-to-end framework designed to construct knowledge around specific diseases
directly from raw text. To facilitate research in disease-related knowledge
discovery, we create two annotated datasets focused on Rett syndrome and
Alzheimer's disease, enabling the identification of semantic relations between
biomedical entities. Extensive benchmarking explores various ways to represent
relations and entity representations, offering insights into optimal modeling
strategies for semantic relation detection and highlighting language models'
competence in knowledge discovery. We also conduct probing experiments using
different layer representations and attention scores to explore transformers'
ability to capture semantic relations.","[{'name': 'Christos Theodoropoulos'}, {'name': 'Andrei Catalin Coman'}, {'name': 'James Henderson'}, {'name': 'Marie-Francine Moens'}]",2024-07-18T13:20:53Z
http://arxiv.org/abs/2407.13490v1,http://arxiv.org/abs/2407.13490v1,"Combining Constraint Programming Reasoning with Large Language Model
  Predictions","Constraint Programming (CP) and Machine Learning (ML) face challenges in text
generation due to CP's struggle with implementing ""meaning'' and ML's
difficulty with structural constraints. This paper proposes a solution by
combining both approaches and embedding a Large Language Model (LLM) in CP. The
LLM handles word generation and meaning, while CP manages structural
constraints. This approach builds on GenCP, an improved version of On-the-fly
Constraint Programming Search (OTFS) using LLM-generated domains. Compared to
Beam Search (BS), a standard NLP method, this combined approach (GenCP with
LLM) is faster and produces better results, ensuring all constraints are
satisfied. This fusion of CP and ML presents new possibilities for enhancing
text generation under constraints.","[{'name': 'Florian Régin'}, {'name': 'Elisabetta De Maria'}, {'name': 'Alexandre Bonlarron'}]",2024-07-18T13:15:55Z
http://arxiv.org/abs/2407.13481v1,http://arxiv.org/abs/2407.13481v1,"Attention Overflow: Language Model Input Blur during Long-Context
  Missing Items Recommendation","Large language models (LLMs) can suggest missing elements from items listed
in a prompt, which can be used for list completion or recommendations based on
users' history. However, their performance degrades when presented with too
many items, as they start to suggest items already included in the input list.
This occurs at around 100 items for mid-2024 flagship LLMs. We evaluate this
phenomenon on both synthetic problems (e.g., finding missing numbers in a given
range of shuffled integers) and realistic movie recommendation scenarios. We
refer to this issue as \textit{attention overflow}, as preventing repetition
requires attending to all items simultaneously. Although iterative loops can
mitigate this problem, their costs increase with the repetition rate, affecting
the language models' ability to derive novelty from lengthy inputs.",[{'name': 'Damien Sileo'}],2024-07-18T13:00:30Z
http://arxiv.org/abs/2407.13469v1,http://arxiv.org/abs/2407.13469v1,"Fixed and Adaptive Simultaneous Machine Translation Strategies Using
  Adapters","Simultaneous machine translation aims at solving the task of real-time
translation by starting to translate before consuming the full input, which
poses challenges in terms of balancing quality and latency of the translation.
The wait-$k$ policy offers a solution by starting to translate after consuming
$k$ words, where the choice of the number $k$ directly affects the latency and
quality. In applications where we seek to keep the choice over latency and
quality at inference, the wait-$k$ policy obliges us to train more than one
model. In this paper, we address the challenge of building one model that can
fulfil multiple latency levels and we achieve this by introducing lightweight
adapter modules into the decoder. The adapters are trained to be specialized
for different wait-$k$ values and compared to other techniques they offer more
flexibility to allow for reaping the benefits of parameter sharing and
minimizing interference. Additionally, we show that by combining with an
adaptive strategy, we can further improve the results. Experiments on two
language directions show that our method outperforms or competes with other
strong baselines on most latency values.","[{'name': 'Abderrahmane Issam'}, {'name': 'Yusuf Can Semerci'}, {'name': 'Jan Scholtes'}, {'name': 'Gerasimos Spanakis'}]",2024-07-18T12:42:45Z
http://arxiv.org/abs/2407.13463v1,http://arxiv.org/abs/2407.13463v1,End-To-End Clinical Trial Matching with Large Language Models,"Matching cancer patients to clinical trials is essential for advancing
treatment and patient care. However, the inconsistent format of medical free
text documents and complex trial eligibility criteria make this process
extremely challenging and time-consuming for physicians. We investigated
whether the entire trial matching process - from identifying relevant trials
among 105,600 oncology-related clinical trials on clinicaltrials.gov to
generating criterion-level eligibility matches - could be automated using Large
Language Models (LLMs). Using GPT-4o and a set of 51 synthetic Electronic
Health Records (EHRs), we demonstrate that our approach identifies relevant
candidate trials in 93.3% of cases and achieves a preliminary accuracy of 88.0%
when matching patient-level information at the criterion level against a
baseline defined by human experts. Utilizing LLM feedback reveals that 39.3%
criteria that were initially considered incorrect are either ambiguous or
inaccurately annotated, leading to a total model accuracy of 92.7% after
refining our human baseline. In summary, we present an end-to-end pipeline for
clinical trial matching using LLMs, demonstrating high precision in screening
and matching trials to individual patients, even outperforming the performance
of qualified medical doctors. Our fully end-to-end pipeline can operate
autonomously or with human supervision and is not restricted to oncology,
offering a scalable solution for enhancing patient-trial matching in real-world
settings.","[{'name': 'Dyke Ferber'}, {'name': 'Lars Hilgers'}, {'name': 'Isabella C. Wiest'}, {'name': 'Marie-Elisabeth Leßmann'}, {'name': 'Jan Clusmann'}, {'name': 'Peter Neidlinger'}, {'name': 'Jiefu Zhu'}, {'name': 'Georg Wölflein'}, {'name': 'Jacqueline Lammert'}, {'name': 'Maximilian Tschochohei'}, {'name': 'Heiko Böhme'}, {'name': 'Dirk Jäger'}, {'name': 'Mihaela Aldea'}, {'name': 'Daniel Truhn'}, {'name': 'Christiane Höper'}, {'name': 'Jakob Nikolas Kather'}]",2024-07-18T12:36:26Z
http://arxiv.org/abs/2407.13442v1,http://arxiv.org/abs/2407.13442v1,"BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in
  Vision-language Models","Vision language models (VLMs) perceive the world through a combination of a
visual encoder and a large language model (LLM). The visual encoder,
pre-trained on large-scale vision-text datasets, provides zero-shot
generalization to visual data, and the LLM endows its high reasoning ability to
VLMs. It leads VLMs to achieve high performance on wide benchmarks without
fine-tuning, exhibiting zero or few-shot capability. However, recent studies
show that VLMs are vulnerable to hallucination. This undesirable behavior
degrades reliability and credibility, thereby making users unable to fully
trust the output from VLMs. To enhance trustworthiness and better tackle the
hallucination of VLMs, we curate a new evaluation dataset, called the
BEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True
Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID).
Unlike prior works that focus only on constructing questions and answers, the
key idea of our benchmark is to manipulate visual scene information by image
editing models and to design the metrics based on scene changes. This allows us
to clearly assess whether VLMs correctly understand a given scene by observing
the ability to perceive changes. We also visualize image-wise object
relationship by virtue of our two-axis view: vision and text. Upon evaluating
VLMs with our dataset, we observed that our metrics reveal different aspects of
VLM hallucination that have not been reported before. Project page:
\url{https://beafbench.github.io/}","[{'name': 'Moon Ye-Bin'}, {'name': 'Nam Hyeon-Woo'}, {'name': 'Wonseok Choi'}, {'name': 'Tae-Hyun Oh'}]",2024-07-18T12:11:12Z
http://arxiv.org/abs/2407.13435v1,http://arxiv.org/abs/2407.13435v1,"Enhancing Out-of-Vocabulary Performance of Indian TTS Systems for
  Practical Applications through Low-Effort Data Strategies","Publicly available TTS datasets for low-resource languages like Hindi and
Tamil typically contain 10-20 hours of data, leading to poor vocabulary
coverage. This limitation becomes evident in downstream applications where
domain-specific vocabulary coupled with frequent code-mixing with English,
results in many OOV words. To highlight this problem, we create a benchmark
containing OOV words from several real-world applications. Indeed,
state-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV
benchmark, as indicated by intelligibility tests. To improve the model's OOV
performance, we propose a low-effort and economically viable strategy to obtain
more training data. Specifically, we propose using volunteers as opposed to
high quality voice artists to record words containing character bigrams unseen
in the training data. We show that using such inexpensive data, the model's
performance improves on OOV words, while not affecting voice quality and
in-domain performance.","[{'name': 'Srija Anand'}, {'name': 'Praveen Srinivasa Varadhan'}, {'name': 'Ashwin Sankar'}, {'name': 'Giri Raju'}, {'name': 'Mitesh M. Khapra'}]",2024-07-18T12:03:14Z
http://arxiv.org/abs/2407.13419v1,http://arxiv.org/abs/2407.13419v1,From Words to Worlds: Compositionality for Cognitive Architectures,"Large language models (LLMs) are very performant connectionist systems, but
do they exhibit more compositionality? More importantly, is that part of why
they perform so well? We present empirical analyses across four LLM families
(12 models) and three task categories, including a novel task introduced below.
Our findings reveal a nuanced relationship in learning of compositional
strategies by LLMs -- while scaling enhances compositional abilities,
instruction tuning often has a reverse effect. Such disparity brings forth some
open issues regarding the development and improvement of large language models
in alignment with human cognitive capacities.","[{'name': 'Ruchira Dhar'}, {'name': 'Anders Søgaard'}]",2024-07-18T11:42:13Z
http://arxiv.org/abs/2407.13399v2,http://arxiv.org/abs/2407.13399v2,"Correcting the Mythos of KL-Regularization: Direct Alignment without
  Overoptimization via Chi-Squared Preference Optimization","Language model alignment methods, such as reinforcement learning from human
feedback (RLHF), have led to impressive advances in language model
capabilities, but existing techniques are limited by a widely observed
phenomenon known as overoptimization, where the quality of the language model
plateaus or degrades over the course of the alignment process. Overoptimization
is often attributed to overfitting to an inaccurate reward model, and while it
can be mitigated through online data collection, this is infeasible in many
settings. This raises a fundamental question: Do existing offline alignment
algorithms make the most of the data they have, or can their sample-efficiency
be improved further?
  We address this question with a new algorithm for offline alignment,
$\chi^2$-Preference Optimization ($\chi$PO). $\chi$PO is a one-line change to
Direct Preference Optimization (DPO; Rafailov et al., 2023), which only
involves modifying the logarithmic link function in the DPO objective. Despite
this minimal change, $\chi$PO implicitly implements the principle of pessimism
in the face of uncertainty via regularization with the $\chi^2$-divergence --
which quantifies uncertainty more effectively than KL-regularization -- and
provably alleviates overoptimization, achieving sample-complexity guarantees
based on single-policy concentrability -- the gold standard in offline
reinforcement learning. $\chi$PO's simplicity and strong guarantees make it the
first practical and general-purpose offline alignment algorithm that is
provably robust to overoptimization.","[{'name': 'Audrey Huang'}, {'name': 'Wenhao Zhan'}, {'name': 'Tengyang Xie'}, {'name': 'Jason D. Lee'}, {'name': 'Wen Sun'}, {'name': 'Akshay Krishnamurthy'}, {'name': 'Dylan J. Foster'}]",2024-07-18T11:08:40Z
http://arxiv.org/abs/2407.13377v1,http://arxiv.org/abs/2407.13377v1,Linear-Complexity Self-Supervised Learning for Speech Processing,"Self-supervised learning (SSL) models usually require weeks of pre-training
with dozens of high-end GPUs. These models typically have a multi-headed
self-attention (MHSA) context encoder. However, MHSA takes quadratic time and
space in the input length, contributing to the high pre-training cost.
Linear-complexity alternatives to MHSA have been proposed. For instance, in
supervised training, the SummaryMixing model is the first to outperform MHSA
across multiple speech processing tasks. However, these cheaper alternatives
have not been explored for SSL yet. This paper studies a linear-complexity
context encoder for SSL for the first time. With better or equivalent
performance for the downstream tasks of the MP3S benchmark, SummaryMixing
reduces the pre-training time and peak VRAM of wav2vec 2.0 model by 18% and by
23%, respectively, leading to the pre-training of a 155M wav2vec 2.0 model
finished within one week with 4 Tesla A100 GPUs. Code is available at
https://github.com/SamsungLabs/SummaryMixing.","[{'name': 'Shucong Zhang'}, {'name': 'Titouan Parcollet'}, {'name': 'Rogier van Dalen'}, {'name': 'Sourav Bhattacharya'}]",2024-07-18T10:34:33Z
http://arxiv.org/abs/2407.13358v1,http://arxiv.org/abs/2407.13358v1,Capturing Style in Author and Document Representation,"A wide range of Deep Natural Language Processing (NLP) models integrates
continuous and low dimensional representations of words and documents.
Surprisingly, very few models study representation learning for authors. These
representations can be used for many NLP tasks, such as author identification
and classification, or in recommendation systems. A strong limitation of
existing works is that they do not explicitly capture writing style, making
them hardly applicable to literary data. We therefore propose a new
architecture based on Variational Information Bottleneck (VIB) that learns
embeddings for both authors and documents with a stylistic constraint. Our
model fine-tunes a pre-trained document encoder. We stimulate the detection of
writing style by adding predefined stylistic features making the representation
axis interpretable with respect to writing style indicators. We evaluate our
method on three datasets: a literary corpus extracted from the Gutenberg
Project, the Blog Authorship Corpus and IMDb62, for which we show that it
matches or outperforms strong/recent baselines in authorship attribution while
capturing much more accurately the authors stylistic aspects.","[{'name': 'Enzo Terreau'}, {'name': 'Antoine Gourru'}, {'name': 'Julien Velcin'}]",2024-07-18T10:01:09Z
http://arxiv.org/abs/2407.14556v1,http://arxiv.org/abs/2407.14556v1,Mechanical Self-replication,"This study presents a theoretical model for a self-replicating mechanical
system inspired by biological processes within living cells and supported by
computer simulations. The model decomposes self-replication into core
components, each of which is executed by a single machine constructed from a
set of basic block types. Key functionalities such as sorting, copying, and
building, are demonstrated. The model provides valuable insights into the
constraints of self-replicating systems. The discussion also addresses the
spatial and timing behavior of the system, as well as its efficiency and
complexity. This work provides a foundational framework for future studies on
self-replicating mechanisms and their information-processing applications.",[{'name': 'Ralph P. Lano'}],2024-07-18T09:49:50Z
http://arxiv.org/abs/2408.00004v1,http://arxiv.org/abs/2408.00004v1,Handling Numeric Expressions in Automatic Speech Recognition,"This paper addresses the problem of correctly formatting numeric expressions
in automatic speech recognition (ASR) transcripts. This is challenging since
the expected transcript format depends on the context, e.g., 1945 (year) vs.
19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize
and format numeric expression, such as years, timestamps, currency amounts, and
quantities. For the end-to-end approach we employed a data generation strategy
using a large language model (LLM) together with a text to speech (TTS) model
to generate adaptation data. The results on our test dataset show that while
approaches based on LLMs perform well on recognizing formatted numeric
expressions, adapted end-to-end models offer competitive performance with the
advantage of lower latency and inference cost.","[{'name': 'Christian Huber'}, {'name': 'Alexander Waibel'}]",2024-07-18T09:46:19Z
http://arxiv.org/abs/2407.13343v1,http://arxiv.org/abs/2407.13343v1,Learning-From-Mistakes Prompting for Indigenous Language Translation,"Using large language models, this paper presents techniques to improve
extremely low-resourced indigenous language translations. Our approaches are
grounded in the use of (1) the presence of a datastore consisting of a limited
number of parallel translation examples, (2) the inherent capabilities of LLMs
like GPT-3.5, and (3) a word-level translation dictionary. We harness the
potential of LLMs and in-context learning techniques in such a setting for
using LLMs as universal translators for extremely low-resourced languages. Our
methodology hinges on utilizing LLMs as language compilers for selected
language pairs, hypothesizing that they could internalize syntactic structures
to facilitate accurate translation. We introduce three techniques: KNNPrompting
with Retrieved Prompting Context, Chain-of-Thought Prompting and
Learningfrom-Mistakes Prompting, with the last method addressing past errors.
The evaluation results suggest that, even with limited corpora, LLMs can
effectively translate extremely low-resource languages when paired with proper
prompting.","[{'name': 'You-Cheng Liao'}, {'name': 'Chen-Jui Yu'}, {'name': 'Chi-Yi Lin'}, {'name': 'He-Feng Yun'}, {'name': 'Yen-Hsiang Wang'}, {'name': 'Hsiao-Min Li'}, {'name': 'Yao-Chung Fan'}]",2024-07-18T09:41:20Z
http://arxiv.org/abs/2407.13329v1,http://arxiv.org/abs/2407.13329v1,"Why do you cite? An investigation on citation intents and
  decision-making classification processes","Identifying the reason for which an author cites another work is essential to
understand the nature of scientific contributions and to assess their impact.
Citations are one of the pillars of scholarly communication and most metrics
employed to analyze these conceptual links are based on quantitative
observations. Behind the act of referencing another scholarly work there is a
whole world of meanings that needs to be proficiently and effectively revealed.
This study emphasizes the importance of trustfully classifying citation intents
to provide more comprehensive and insightful analyses in research assessment.
We address this task by presenting a study utilizing advanced Ensemble
Strategies for Citation Intent Classification (CIC) incorporating Language
Models (LMs) and employing Explainable AI (XAI) techniques to enhance the
interpretability and trustworthiness of models' predictions. Our approach
involves two ensemble classifiers that utilize fine-tuned SciBERT and XLNet LMs
as baselines. We further demonstrate the critical role of section titles as a
feature in improving models' performances. The study also introduces a web
application developed with Flask and currently available at
http://137.204.64.4:81/cic/classifier, aimed at classifying citation intents.
One of our models sets as a new state-of-the-art (SOTA) with an 89.46% Macro-F1
score on the SciCite benchmark. The integration of XAI techniques provides
insights into the decision-making processes, highlighting the contributions of
individual words for level-0 classifications, and of individual models for the
metaclassification. The findings suggest that the inclusion of section titles
significantly enhances classification performances in the CIC task. Our
contributions provide useful insights for developing more robust datasets and
methodologies, thus fostering a deeper understanding of scholarly
communication.","[{'name': 'Lorenzo Paolini'}, {'name': 'Sahar Vahdati'}, {'name': 'Angelo Di Iorio'}, {'name': 'Robert Wardenga'}, {'name': 'Ivan Heibi'}, {'name': 'Silvio Peroni'}]",2024-07-18T09:29:33Z
http://arxiv.org/abs/2407.13301v1,http://arxiv.org/abs/2407.13301v1,"CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis","The field of medical diagnosis has undergone a significant transformation
with the advent of large language models (LLMs), yet the challenges of
interpretability within these models remain largely unaddressed. This study
introduces Chain-of-Diagnosis (CoD) to enhance the interpretability of
LLM-based medical diagnostics. CoD transforms the diagnostic process into a
diagnostic chain that mirrors a physician's thought process, providing a
transparent reasoning pathway. Additionally, CoD outputs the disease confidence
distribution to ensure transparency in decision-making. This interpretability
makes model diagnostics controllable and aids in identifying critical symptoms
for inquiry through the entropy reduction of confidences. With CoD, we
developed DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental
results demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic
benchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring
controllability in diagnostic rigor.","[{'name': 'Junying Chen'}, {'name': 'Chi Gui'}, {'name': 'Anningzhe Gao'}, {'name': 'Ke Ji'}, {'name': 'Xidong Wang'}, {'name': 'Xiang Wan'}, {'name': 'Benyou Wang'}]",2024-07-18T09:06:27Z
http://arxiv.org/abs/2407.13292v1,http://arxiv.org/abs/2407.13292v1,"Low-Resourced Speech Recognition for Iu Mien Language via
  Weakly-Supervised Phoneme-based Multilingual Pre-training","The mainstream automatic speech recognition (ASR) technology usually requires
hundreds to thousands of hours of annotated speech data. Three approaches to
low-resourced ASR are phoneme or subword based supervised pre-training, and
self-supervised pre-training over multilingual data. The Iu Mien language is
the main ethnic language of the Yao ethnic group in China and is low-resourced
in the sense that the annotated speech is very limited. With less than 10 hours
of transcribed Iu Mien language, this paper investigates and compares the three
approaches for Iu Mien speech recognition. Our experiments are based on the
recently released, three backbone models pretrained over the 10 languages from
the CommonVoice dataset (CV-Lang10), which correspond to the three approaches
for low-resourced ASR. It is found that phoneme supervision can achieve better
results compared to subword supervision and self-supervision, thereby providing
higher data-efficiency. Particularly, the Whistle models, i.e., obtained by the
weakly-supervised phoneme-based multilingual pre-training, obtain the most
competitive results.","[{'name': 'Lukuan Dong'}, {'name': 'Donghong Qin'}, {'name': 'Fengbo Bai'}, {'name': 'Fanhua Song'}, {'name': 'Yan Liu'}, {'name': 'Chen Xu'}, {'name': 'Zhijian Ou'}]",2024-07-18T08:46:47Z
http://arxiv.org/abs/2407.13248v1,http://arxiv.org/abs/2407.13248v1,Are Large Language Models Capable of Generating Human-Level Narratives?,"This paper investigates the capability of LLMs in storytelling, focusing on
narrative development and plot progression. We introduce a novel computational
framework to analyze narratives through three discourse-level aspects: i) story
arcs, ii) turning points, and iii) affective dimensions, including arousal and
valence. By leveraging expert and automatic annotations, we uncover significant
discrepancies between the LLM- and human- written stories. While human-written
stories are suspenseful, arousing, and diverse in narrative structures, LLM
stories are homogeneously positive and lack tension. Next, we measure narrative
reasoning skills as a precursor to generative capacities, concluding that most
LLMs fall short of human abilities in discourse understanding. Finally, we show
that explicit integration of aforementioned discourse features can enhance
storytelling, as is demonstrated by over 40% improvement in neural storytelling
in terms of diversity, suspense, and arousal.","[{'name': 'Yufei Tian'}, {'name': 'Tenghao Huang'}, {'name': 'Miri Liu'}, {'name': 'Derek Jiang'}, {'name': 'Alexander Spangher'}, {'name': 'Muhao Chen'}, {'name': 'Jonathan May'}, {'name': 'Nanyun Peng'}]",2024-07-18T08:02:49Z
http://arxiv.org/abs/2407.13244v1,http://arxiv.org/abs/2407.13244v1,"PM-LLM-Benchmark: Evaluating Large Language Models on Process Mining
  Tasks","Large Language Models (LLMs) have the potential to semi-automate some process
mining (PM) analyses. While commercial models are already adequate for many
analytics tasks, the competitive level of open-source LLMs in PM tasks is
unknown. In this paper, we propose PM-LLM-Benchmark, the first comprehensive
benchmark for PM focusing on domain knowledge (process-mining-specific and
process-specific) and on different implementation strategies. We focus also on
the challenges in creating such a benchmark, related to the public availability
of the data and on evaluation biases by the LLMs. Overall, we observe that most
of the considered LLMs can perform some process mining tasks at a satisfactory
level, but tiny models that would run on edge devices are still inadequate. We
also conclude that while the proposed benchmark is useful for identifying LLMs
that are adequate for process mining tasks, further research is needed to
overcome the evaluation biases and perform a more thorough ranking of the
competitive LLMs.","[{'name': 'Alessandro Berti'}, {'name': 'Humam Kourani'}, {'name': 'Wil M. P. van der Aalst'}]",2024-07-18T07:57:31Z
http://arxiv.org/abs/2407.13228v1,http://arxiv.org/abs/2407.13228v1,"Evaluating Large Language Models for Anxiety and Depression
  Classification using Counseling and Psychotherapy Transcripts","We aim to evaluate the efficacy of traditional machine learning and large
language models (LLMs) in classifying anxiety and depression from long
conversational transcripts. We fine-tune both established transformer models
(BERT, RoBERTa, Longformer) and more recent large models (Mistral-7B), trained
a Support Vector Machine with feature engineering, and assessed GPT models
through prompting. We observe that state-of-the-art models fail to enhance
classification outcomes compared to traditional machine learning methods.","[{'name': 'Junwei Sun'}, {'name': 'Siqi Ma'}, {'name': 'Yiran Fan'}, {'name': 'Peter Washington'}]",2024-07-18T07:26:09Z
http://arxiv.org/abs/2407.13205v1,http://arxiv.org/abs/2407.13205v1,Transformer-based Single-Cell Language Model: A Survey,"The transformers have achieved significant accomplishments in the natural
language processing as its outstanding parallel processing capabilities and
highly flexible attention mechanism. In addition, increasing studies based on
transformers have been proposed to model single-cell data. In this review, we
attempt to systematically summarize the single-cell language models and
applications based on transformers. First, we provide a detailed introduction
about the structure and principles of transformers. Then, we review the
single-cell language models and large language models for single-cell data
analysis. Moreover, we explore the datasets and applications of single-cell
language models in downstream tasks such as batch correction, cell clustering,
cell type annotation, gene regulatory network inference and perturbation
response. Further, we discuss the challenges of single-cell language models and
provide promising research directions. We hope this review will serve as an
up-to-date reference for researchers interested in the direction of single-cell
language models.","[{'name': 'Wei Lan'}, {'name': 'Guohang He'}, {'name': 'Mingyang Liu'}, {'name': 'Qingfeng Chen'}, {'name': 'Junyue Cao'}, {'name': 'Wei Peng'}]",2024-07-18T06:43:12Z
http://arxiv.org/abs/2407.13193v2,http://arxiv.org/abs/2407.13193v2,Retrieval-Augmented Generation for Natural Language Processing: A Survey,"Large language models (LLMs) have demonstrated great success in various
fields, benefiting from their huge amount of parameters that store knowledge.
However, LLMs still suffer from several key issues, such as hallucination
problems, knowledge update issues, and lacking domain-specific expertise. The
appearance of retrieval-augmented generation (RAG), which leverages an external
knowledge database to augment LLMs, makes up those drawbacks of LLMs. This
paper reviews all significant techniques of RAG, especially in the retriever
and the retrieval fusions. Besides, tutorial codes are provided for
implementing the representative techniques in RAG. This paper further discusses
the RAG training, including RAG with/without datastore update. Then, we
introduce the application of RAG in representative natural language processing
tasks and industrial scenarios. Finally, this paper discusses the future
directions and challenges of RAG for promoting its development.","[{'name': 'Shangyu Wu'}, {'name': 'Ying Xiong'}, {'name': 'Yufei Cui'}, {'name': 'Haolun Wu'}, {'name': 'Can Chen'}, {'name': 'Ye Yuan'}, {'name': 'Lianming Huang'}, {'name': 'Xue Liu'}, {'name': 'Tei-Wei Kuo'}, {'name': 'Nan Guan'}, {'name': 'Chun Jason Xue'}]",2024-07-18T06:06:53Z
http://arxiv.org/abs/2407.13168v1,http://arxiv.org/abs/2407.13168v1,SciCode: A Research Coding Benchmark Curated by Scientists,"Since language models (LMs) now outperform average humans on many challenging
tasks, it has become increasingly difficult to develop challenging,
high-quality, and realistic evaluations. We address this issue by examining
LMs' capabilities to generate code for solving real scientific research
problems. Incorporating input from scientists and AI researchers in 16 diverse
natural science sub-fields, including mathematics, physics, chemistry, biology,
and materials science, we created a scientist-curated coding benchmark,
SciCode. The problems in SciCode naturally factorize into multiple subproblems,
each involving knowledge recall, reasoning, and code synthesis. In total,
SciCode contains 338 subproblems decomposed from 80 challenging main problems.
It offers optional descriptions specifying useful scientific background
information and scientist-annotated gold-standard solutions and test cases for
evaluation. Claude3.5-Sonnet, the best-performing model among those tested, can
solve only 4.6% of the problems in the most realistic setting. We believe that
SciCode demonstrates both contemporary LMs' progress towards becoming helpful
scientific assistants and sheds light on the development and evaluation of
scientific AI in the future.","[{'name': 'Minyang Tian'}, {'name': 'Luyu Gao'}, {'name': 'Shizhuo Dylan Zhang'}, {'name': 'Xinan Chen'}, {'name': 'Cunwei Fan'}, {'name': 'Xuefei Guo'}, {'name': 'Roland Haas'}, {'name': 'Pan Ji'}, {'name': 'Kittithat Krongchon'}, {'name': 'Yao Li'}, {'name': 'Shengyan Liu'}, {'name': 'Di Luo'}, {'name': 'Yutao Ma'}, {'name': 'Hao Tong'}, {'name': 'Kha Trinh'}, {'name': 'Chenyu Tian'}, {'name': 'Zihan Wang'}, {'name': 'Bohao Wu'}, {'name': 'Yanyu Xiong'}, {'name': 'Shengzhu Yin'}, {'name': 'Minhui Zhu'}, {'name': 'Kilian Lieret'}, {'name': 'Yanxin Lu'}, {'name': 'Genglin Liu'}, {'name': 'Yufeng Du'}, {'name': 'Tianhua Tao'}, {'name': 'Ofir Press'}, {'name': 'Jamie Callan'}, {'name': 'Eliu Huerta'}, {'name': 'Hao Peng'}]",2024-07-18T05:15:24Z
http://arxiv.org/abs/2407.13164v1,http://arxiv.org/abs/2407.13164v1,"Translate-and-Revise: Boosting Large Language Models for Constrained
  Translation","Imposing constraints on machine translation systems presents a challenging
issue because these systems are not trained to make use of constraints in
generating adequate, fluent translations. In this paper, we leverage the
capabilities of large language models (LLMs) for constrained translation, given
that LLMs can easily adapt to this task by taking translation instructions and
constraints as prompts. However, LLMs cannot always guarantee the adequacy of
translation, and, in some cases, ignore the given constraints. This is in part
because LLMs might be overly confident in their predictions, overriding the
influence of the constraints. To overcome this overiding behaviour, we propose
to add a revision process that encourages LLMs to correct the outputs by
prompting them about the constraints that have not yet been met. We evaluate
our approach on four constrained translation tasks, encompassing both lexical
and structural constraints in multiple constraint domains. Experiments show
15\% improvement in constraint-based translation accuracy over standard LLMs
and the approach also significantly outperforms neural machine translation
(NMT) state-of-the-art methods.","[{'name': 'Pengcheng Huang'}, {'name': 'Yongyu Mu'}, {'name': 'Yuzhang Wu'}, {'name': 'Bei Li'}, {'name': 'Chunyang Xiao'}, {'name': 'Tong Xiao'}, {'name': 'Jingbo Zhu'}]",2024-07-18T05:08:09Z
http://arxiv.org/abs/2407.13153v1,http://arxiv.org/abs/2407.13153v1,"Preset-Voice Matching for Privacy Regulated Speech-to-Speech Translation
  Systems","In recent years, there has been increased demand for speech-to-speech
translation (S2ST) systems in industry settings. Although successfully
commercialized, cloning-based S2ST systems expose their distributors to
liabilities when misused by individuals and can infringe on personality rights
when exploited by media organizations. This work proposes a regulated S2ST
framework called Preset-Voice Matching (PVM). PVM removes cross-lingual voice
cloning in S2ST by first matching the input voice to a similar prior consenting
speaker voice in the target-language. With this separation, PVM avoids cloning
the input speaker, ensuring PVM systems comply with regulations and reduce risk
of misuse. Our results demonstrate PVM can significantly improve S2ST system
run-time in multi-speaker settings and the naturalness of S2ST synthesized
speech. To our knowledge, PVM is the first explicitly regulated S2ST framework
leveraging similarly-matched preset-voices for dynamic S2ST tasks.","[{'name': 'Daniel Platnick'}, {'name': 'Bishoy Abdelnour'}, {'name': 'Eamon Earl'}, {'name': 'Rahul Kumar'}, {'name': 'Zahra Rezaei'}, {'name': 'Thomas Tsangaris'}, {'name': 'Faraj Lagum'}]",2024-07-18T04:42:01Z
http://arxiv.org/abs/2407.13142v1,http://arxiv.org/abs/2407.13142v1,"A light-weight and efficient punctuation and word casing prediction
  model for on-device streaming ASR","Punctuation and word casing prediction are necessary for automatic speech
recognition (ASR). With the popularity of on-device end-to-end streaming ASR
systems, the on-device punctuation and word casing prediction become a
necessity while we found little discussion on this. With the emergence of
Transformer, Transformer based models have been explored for this scenario.
However, Transformer based models are too large for on-device ASR systems. In
this paper, we propose a light-weight and efficient model that jointly predicts
punctuation and word casing in real time. The model is based on Convolutional
Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM).
Experimental results on the IWSLT2011 test set show that the proposed model
obtains 9% relative improvement compared to the best of non-Transformer models
on overall F1-score. Compared to the representative of Transformer based
models, the proposed model achieves comparable results to the representative
model while being only one-fortieth its size and 2.5 times faster in terms of
inference time. It is suitable for on-device streaming ASR systems. Our code is
publicly available.","[{'name': 'Jian You'}, {'name': 'Xiangfeng Li'}]",2024-07-18T04:01:12Z
http://arxiv.org/abs/2407.13115v1,http://arxiv.org/abs/2407.13115v1,"TrialEnroll: Predicting Clinical Trial Enrollment Success with Deep &
  Cross Network and Large Language Models","Clinical trials need to recruit a sufficient number of volunteer patients to
demonstrate the statistical power of the treatment (e.g., a new drug) in curing
a certain disease. Clinical trial recruitment has a significant impact on trial
success. Forecasting whether the recruitment process would be successful before
we run the trial would save many resources and time. This paper develops a
novel deep & cross network with large language model (LLM)-augmented text
feature that learns semantic information from trial eligibility criteria and
predicts enrollment success. The proposed method enables interpretability by
understanding which sentence/word in eligibility criteria contributes heavily
to prediction. We also demonstrate the empirical superiority of the proposed
method (0.7002 PR-AUC) over a bunch of well-established machine learning
methods. The code and curated dataset are publicly available at
https://anonymous.4open.science/r/TrialEnroll-7E12.","[{'name': 'Ling Yue'}, {'name': 'Sixue Xing'}, {'name': 'Jintai Chen'}, {'name': 'Tianfan Fu'}]",2024-07-18T02:50:40Z
http://arxiv.org/abs/2407.13101v1,http://arxiv.org/abs/2407.13101v1,"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with
  an Iterative Approach","Multi-hop question answering is a challenging task with distinct industrial
relevance, and Retrieval-Augmented Generation (RAG) methods based on large
language models (LLMs) have become a popular approach to tackle this task.
Owing to the potential inability to retrieve all necessary information in a
single iteration, a series of iterative RAG methods has been recently
developed, showing significant performance improvements. However, existing
methods still face two critical challenges: context overload resulting from
multiple rounds of retrieval, and over-planning and repetitive planning due to
the lack of a recorded retrieval trajectory. In this paper, we propose a novel
iterative RAG method called ReSP, equipped with a dual-function summarizer.
This summarizer compresses information from retrieved documents, targeting both
the overarching question and the current sub-question concurrently.
Experimental results on the multi-hop question-answering datasets HotpotQA and
2WikiMultihopQA demonstrate that our method significantly outperforms the
state-of-the-art, and exhibits excellent robustness concerning context length.","[{'name': 'Zhouyu Jiang'}, {'name': 'Mengshu Sun'}, {'name': 'Lei Liang'}, {'name': 'Zhiqiang Zhang'}]",2024-07-18T02:19:00Z
http://arxiv.org/abs/2407.13097v1,http://arxiv.org/abs/2407.13097v1,AlcLaM: Arabic Dialectal Language Model,"Pre-trained Language Models (PLMs) are integral to many modern natural
language processing (NLP) systems. Although multilingual models cover a wide
range of languages, they often grapple with challenges like high inference
costs and a lack of diverse non-English training data. Arabic-specific PLMs are
trained predominantly on modern standard Arabic, which compromises their
performance on regional dialects. To tackle this, we construct an Arabic
dialectal corpus comprising 3.4M sentences gathered from social media
platforms. We utilize this corpus to expand the vocabulary and retrain a
BERT-based model from scratch. Named AlcLaM, our model was trained using only
13 GB of text, which represents a fraction of the data used by existing models
such as CAMeL, MARBERT, and ArBERT, compared to 7.8%, 10.2%, and 21.3%,
respectively. Remarkably, AlcLaM demonstrates superior performance on a variety
of Arabic NLP tasks despite the limited training data. AlcLaM is available at
GitHub https://github.com/amurtadha/Alclam and HuggingFace
https://huggingface.co/rahbi.","[{'name': 'Murtadha Ahmed'}, {'name': 'Saghir Alfasly'}, {'name': 'Bo Wen'}, {'name': 'Jamaal Qasem'}, {'name': 'Mohammed Ahmed'}, {'name': 'Yunfeng Liu'}]",2024-07-18T02:13:50Z
http://arxiv.org/abs/2407.13089v1,http://arxiv.org/abs/2407.13089v1,"MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for
  Fact-Checking","Fact-checking real-world claims often requires reviewing multiple multimodal
documents to assess a claim's truthfulness, which is a highly laborious and
time-consuming task. In this paper, we present a summarization model designed
to generate claim-specific summaries useful for fact-checking from multimodal,
multi-document datasets. The model takes inputs in the form of documents,
images, and a claim, with the objective of assisting in fact-checking tasks. We
introduce a dynamic perceiver-based model that can handle inputs from multiple
modalities of arbitrary lengths. To train our model, we leverage a novel
reinforcement learning-based entailment objective to generate summaries that
provide evidence distinguishing between different truthfulness labels. To
assess the efficacy of our approach, we conduct experiments on both an existing
benchmark and a new dataset of multi-document claims that we contribute. Our
approach outperforms the SOTA approach by 4.6% in the claim verification task
on the MOCHEG dataset and demonstrates strong performance on our new
Multi-News-Fact-Checking dataset.","[{'name': 'Ting-Chih Chen'}, {'name': 'Chia-Wei Tang'}, {'name': 'Chris Thomas'}]",2024-07-18T01:33:20Z
http://arxiv.org/abs/2407.13069v1,http://arxiv.org/abs/2407.13069v1,"Dynamic Sentiment Analysis with Local Large Language Models using
  Majority Voting: A Study on Factors Affecting Restaurant Evaluation","User-generated contents (UGCs) on online platforms allow marketing
researchers to understand consumer preferences for products and services. With
the advance of large language models (LLMs), some studies utilized the models
for annotation and sentiment analysis. However, the relationship between the
accuracy and the hyper-parameters of LLMs is yet to be thoroughly examined. In
addition, the issues of variability and reproducibility of results from each
trial of LLMs have rarely been considered in existing literature. Since actual
human annotation uses majority voting to resolve disagreements among
annotators, this study introduces a majority voting mechanism to a sentiment
analysis model using local LLMs. By a series of three analyses of online
reviews on restaurant evaluations, we demonstrate that majority voting with
multiple attempts using a medium-sized model produces more robust results than
using a large model with a single attempt. Furthermore, we conducted further
analysis to investigate the effect of each aspect on the overall evaluation.",[{'name': 'Junichiro Niimi'}],2024-07-18T00:28:04Z
http://arxiv.org/abs/2407.13048v1,http://arxiv.org/abs/2407.13048v1,Establishing Knowledge Preference in Language Models,"Language models are known to encode a great amount of factual knowledge
through pretraining. However, such knowledge might be insufficient to cater to
user requests, requiring the model to integrate external knowledge sources and
adhere to user-provided specifications. When answering questions about ongoing
events, the model should use recent news articles to update its response; when
asked to provide recommendations, the model should prioritize user
specifications over retrieved product reviews; when some facts are edited in
the model, the updated facts should override all prior knowledge learned by the
model even if they are conflicting. In all of the cases above, the model faces
a decision between its own parametric knowledge, (retrieved) contextual
knowledge, and user instruction knowledge. In this paper, we (1) unify such
settings into the problem of knowledge preference and define a three-level
preference hierarchy over these knowledge sources; (2) compile a collection of
existing datasets IfQA, MQuAKE, and MRQA covering a combination of settings
(with/without user specifications, with/without context documents) to
systematically evaluate how well models obey the intended knowledge preference;
and (3) propose a dataset synthesis method that composes diverse
question-answer pairs with user assumptions and related context to directly
fine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a
7B model, fine-tuned on only a few thousand examples automatically generated by
our proposed method, effectively achieves superior performance (more than 18%
improvement across all evaluation benchmarks) in adhering to the desired
knowledge preference hierarchy.","[{'name': 'Sizhe Zhou'}, {'name': 'Sha Li'}, {'name': 'Yu Meng'}, {'name': 'Yizhu Jiao'}, {'name': 'Heng Ji'}, {'name': 'Jiawei Han'}]",2024-07-17T23:16:11Z
http://arxiv.org/abs/2407.20244v1,http://arxiv.org/abs/2407.20244v1,"Steamroller Problems: An Evaluation of LLM Reasoning Capability with
  Automated Theorem Prover Strategies","This study presents the first examination of the ability of Large Language
Models (LLMs) to follow reasoning strategies that are used to guide Automated
Theorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and
Google's recent Gemini model on problems from a steamroller domain. In addition
to determining accuracy we make use of the Natural Language Processing library
spaCy to explore new methods of investigating LLM's reasoning capabilities.
This led to one alarming result, the low correlation between correct reasoning
and correct answers for any of the tested models. We found that the models'
performance when using the ATP reasoning strategies was comparable to one-shot
chain of thought and observe that attention to uncertainty in the accuracy
results is critical when drawing conclusions about model performance.
Consistent with previous speculation we confirm that LLMs have a preference
for, and are best able to follow, bottom up reasoning processes. However, the
reasoning strategies can still be beneficial for deriving small and relevant
sets of formulas for external processing by a trusted inference engine.","[{'name': 'Lachlan McGinness'}, {'name': 'Peter Baumgartner'}]",2024-07-17T22:49:23Z
http://arxiv.org/abs/2407.13040v1,http://arxiv.org/abs/2407.13040v1,Turkish Delights: a Dataset on Turkish Euphemisms,"Euphemisms are a form of figurative language relatively understudied in
natural language processing. This research extends the current computational
work on potentially euphemistic terms (PETs) to Turkish. We introduce the
Turkish PET dataset, the first available of its kind in the field. By creating
a list of euphemisms in Turkish, collecting example contexts, and annotating
them, we provide both euphemistic and non-euphemistic examples of PETs in
Turkish. We describe the dataset and methodologies, and also experiment with
transformer-based models on Turkish euphemism detection by using our dataset
for binary classification. We compare performances across models using F1,
accuracy, and precision as evaluation metrics.","[{'name': 'Hasan Can Biyik'}, {'name': 'Patrick Lee'}, {'name': 'Anna Feldman'}]",2024-07-17T22:13:42Z
http://arxiv.org/abs/2407.13035v1,http://arxiv.org/abs/2407.13035v1,"Pre-Trained Foundation Model representations to uncover Breathing
  patterns in Speech","The process of human speech production involves coordinated respiratory
action to elicit acoustic speech signals. Typically, speech is produced when
air is forced from the lungs and is modulated by the vocal tract, where such
actions are interspersed by moments of breathing in air (inhalation) to refill
the lungs again. Respiratory rate (RR) is a vital metric that is used to assess
the overall health, fitness, and general well-being of an individual. Existing
approaches to measure RR (number of breaths one takes in a minute) are
performed using specialized equipment or training. Studies have demonstrated
that machine learning algorithms can be used to estimate RR using bio-sensor
signals as input. Speech-based estimation of RR can offer an effective approach
to measure the vital metric without requiring any specialized equipment or
sensors. This work investigates a machine learning based approach to estimate
RR from speech segments obtained from subjects speaking to a close-talking
microphone device. Data were collected from N=26 individuals, where the
groundtruth RR was obtained through commercial grade chest-belts and then
manually corrected for any errors. A convolutional long-short term memory
network (Conv-LSTM) is proposed to estimate respiration time-series data from
the speech signal. We demonstrate that the use of pre-trained representations
obtained from a foundation model, such as Wav2Vec2, can be used to estimate
respiration-time-series with low root-mean-squared error and high correlation
coefficient, when compared with the baseline. The model-driven time series can
be used to estimate $RR$ with a low mean absolute error (MAE) ~ 1.6
breaths/min.","[{'name': 'Vikramjit Mitra'}, {'name': 'Anirban Chatterjee'}, {'name': 'Ke Zhai'}, {'name': 'Helen Weng'}, {'name': 'Ayuko Hill'}, {'name': 'Nicole Hay'}, {'name': 'Christopher Webb'}, {'name': 'Jamie Cheng'}, {'name': 'Erdrin Azemi'}]",2024-07-17T21:57:18Z
http://arxiv.org/abs/2407.12994v2,http://arxiv.org/abs/2407.12994v2,"A Survey of Prompt Engineering Methods in Large Language Models for
  Different NLP Tasks","Large language models (LLMs) have shown remarkable performance on many
different Natural Language Processing (NLP) tasks. Prompt engineering plays a
key role in adding more to the already existing abilities of LLMs to achieve
significant performance gains on various NLP tasks. Prompt engineering requires
composing natural language instructions called prompts to elicit knowledge from
LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models,
prompt engineering does not require extensive parameter re-training or
fine-tuning based on the given NLP task and thus solely operates on the
embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently
extract LLMs' knowledge through a basic natural language conversational
exchange or prompt engineering, allowing more and more people even without deep
mathematical machine learning background to experiment with LLMs. With prompt
engineering gaining popularity in the last two years, researchers have come up
with numerous engineering techniques around designing prompts to improve
accuracy of information extraction from the LLMs. In this paper, we summarize
different prompting techniques and club them together based on different NLP
tasks that they have been used for. We further granularly highlight the
performance of these prompting strategies on various datasets belonging to that
NLP task, talk about the corresponding LLMs used, present a taxonomy diagram
and discuss the possible SoTA for specific datasets. In total, we read and
present a survey of 44 research papers which talk about 39 different prompting
methods on 29 different NLP tasks of which most of them have been published in
the last two years.","[{'name': 'Shubham Vatsal'}, {'name': 'Harsh Dubey'}]",2024-07-17T20:23:19Z
http://arxiv.org/abs/2407.12982v1,http://arxiv.org/abs/2407.12982v1,Retrieval-Enhanced Machine Learning: Synthesis and Opportunities,"In the field of language modeling, models augmented with retrieval components
have emerged as a promising solution to address several challenges faced in the
natural language processing (NLP) field, including knowledge grounding,
interpretability, and scalability. Despite the primary focus on NLP, we posit
that the paradigm of retrieval-enhancement can be extended to a broader
spectrum of machine learning (ML) such as computer vision, time series
prediction, and computational biology. Therefore, this work introduces a formal
framework of this paradigm, Retrieval-Enhanced Machine Learning (REML), by
synthesizing the literature in various domains in ML with consistent notations
which is missing from the current literature. Also, we found that while a
number of studies employ retrieval components to augment their models, there is
a lack of integration with foundational Information Retrieval (IR) research. We
bridge this gap between the seminal IR research and contemporary REML studies
by investigating each component that comprises the REML framework. Ultimately,
the goal of this work is to equip researchers across various disciplines with a
comprehensive, formally structured framework of retrieval-enhanced models,
thereby fostering interdisciplinary future research.","[{'name': 'To Eun Kim'}, {'name': 'Alireza Salemi'}, {'name': 'Andrew Drozdov'}, {'name': 'Fernando Diaz'}, {'name': 'Hamed Zamani'}]",2024-07-17T20:01:21Z
http://arxiv.org/abs/2407.13803v1,http://arxiv.org/abs/2407.13803v1,Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality,"With the widespread adoption of Large Language Models (LLMs), concerns about
potential misuse have emerged. To this end, watermarking has been adapted to
LLM, enabling a simple and effective way to detect and monitor generated text.
However, while the existing methods can differentiate between watermarked and
unwatermarked text with high accuracy, they often face a trade-off between the
quality of the generated text and the effectiveness of the watermarking
process. In this work, we present a novel type of LLM watermark, Sparse
Watermark, which aims to mitigate this trade-off by applying watermarks to a
small subset of generated tokens distributed across the text. The key strategy
involves anchoring watermarked tokens to words that have specific
Part-of-Speech (POS) tags. Our experimental results demonstrate that the
proposed watermarking scheme achieves high detectability while generating text
that outperforms previous LLM watermarking methods in quality across various
tasks","[{'name': 'Duy C. Hoang'}, {'name': 'Hung T. Q. Le'}, {'name': 'Rui Chu'}, {'name': 'Ping Li'}, {'name': 'Weijie Zhao'}, {'name': 'Yingjie Lao'}, {'name': 'Khoa D. Doan'}]",2024-07-17T18:52:12Z
http://arxiv.org/abs/2407.12943v1,http://arxiv.org/abs/2407.12943v1,Halu-J: Critique-Based Hallucination Judge,"Large language models (LLMs) frequently generate non-factual content, known
as hallucinations. Existing retrieval-augmented-based hallucination detection
approaches typically address this by framing it as a classification task,
evaluating hallucinations based on their consistency with retrieved evidence.
However, this approach usually lacks detailed explanations for these
evaluations and does not assess the reliability of these explanations.
Furthermore, deficiencies in retrieval systems can lead to irrelevant or
partially relevant evidence retrieval, impairing the detection process.
Moreover, while real-world hallucination detection requires analyzing multiple
pieces of evidence, current systems usually treat all evidence uniformly
without considering its relevance to the content. To address these challenges,
we introduce Halu-J, a critique-based hallucination judge with 7 billion
parameters. Halu-J enhances hallucination detection by selecting pertinent
evidence and providing detailed critiques. Our experiments indicate that Halu-J
outperforms GPT-4o in multiple-evidence hallucination detection and matches its
capability in critique generation and evidence selection. We also introduce
ME-FEVER, a new dataset designed for multiple-evidence hallucination detection.
Our code and dataset can be found in https://github.com/GAIR-NLP/factool .","[{'name': 'Binjie Wang'}, {'name': 'Steffi Chern'}, {'name': 'Ethan Chern'}, {'name': 'Pengfei Liu'}]",2024-07-17T18:21:01Z
http://arxiv.org/abs/2407.20243v1,http://arxiv.org/abs/2407.20243v1,"Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller
  Embedding Dimensions","Embeddings from Large Language Models (LLMs) have emerged as critical
components in various applications, particularly for information retrieval.
While high-dimensional embeddings generally demonstrate superior performance as
they contain more salient information, their practical application is
frequently hindered by elevated computational latency and the associated higher
cost. To address these challenges, we propose Matryoshka-Adaptor, a novel
tuning framework designed for the customization of LLM embeddings.
Matryoshka-Adaptor facilitates substantial dimensionality reduction while
maintaining comparable performance levels, thereby achieving a significant
enhancement in computational efficiency and cost-effectiveness. Our framework
directly modifies the embeddings from pre-trained LLMs which is designed to be
seamlessly integrated with any LLM architecture, encompassing those accessible
exclusively through black-box APIs. Also, it exhibits efficacy in both
unsupervised and supervised learning settings. A rigorous evaluation conducted
across a diverse corpus of English, multilingual, and multimodal datasets
consistently reveals substantial gains with Matryoshka-Adaptor. Notably, with
Google and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in
dimensionality ranging from two- to twelve-fold without compromising
performance across multiple BEIR datasets.","[{'name': 'Jinsung Yoon'}, {'name': 'Raj Sinha'}, {'name': 'Sercan O Arik'}, {'name': 'Tomas Pfister'}]",2024-07-17T18:03:29Z
http://arxiv.org/abs/2407.12772v1,http://arxiv.org/abs/2407.12772v1,LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models,"The advances of large foundation models necessitate wide-coverage, low-cost,
and zero-contamination benchmarks. Despite continuous exploration of language
model evaluations, comprehensive studies on the evaluation of Large Multi-modal
Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified
and standardized multimodal benchmark framework with over 50 tasks and more
than 10 models to promote transparent and reproducible evaluations. Although
LMMS-EVAL offers comprehensive coverage, we find it still falls short in
achieving low cost and zero contamination. To approach this evaluation
trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that
emphasizes both coverage and efficiency. Additionally, we present Multimodal
LIVEBENCH that utilizes continuously updating news and online forums to assess
models' generalization abilities in the wild, featuring a low-cost and
zero-contamination evaluation approach. In summary, our work highlights the
importance of considering the evaluation trilemma and provides practical
solutions to navigate the trade-offs in evaluating large multi-modal models,
paving the way for more effective and reliable benchmarking of LMMs. We
opensource our codebase and maintain leaderboard of LIVEBENCH at
https://github.com/EvolvingLMMs-Lab/lmms-eval and
https://huggingface.co/spaces/lmms-lab/LiveBench.","[{'name': 'Kaichen Zhang'}, {'name': 'Bo Li'}, {'name': 'Peiyuan Zhang'}, {'name': 'Fanyi Pu'}, {'name': 'Joshua Adrian Cahyono'}, {'name': 'Kairui Hu'}, {'name': 'Shuai Liu'}, {'name': 'Yuanhan Zhang'}, {'name': 'Jingkang Yang'}, {'name': 'Chunyuan Li'}, {'name': 'Ziwei Liu'}]",2024-07-17T17:51:53Z
http://arxiv.org/abs/2407.12771v1,http://arxiv.org/abs/2407.12771v1,The Role of Network and Identity in the Diffusion of Hashtags,"Although the spread of behaviors is influenced by many social factors,
existing literature tends to study the effects of single factors -- most often,
properties of the social network -- on the final cascade. In order to move
towards a more integrated view of cascades, this paper offers the first
comprehensive investigation into the role of two social factors in the
diffusion of 1,337 popular hashtags representing the production of novel
culture on Twitter: 1) the topology of the Twitter social network and 2)
performance of each user's probable demographic identity. Here, we show that
cascades are best modeled using a combination of network and identity, rather
than either factor alone. This combined model best reproduces a composite index
of ten cascade properties across all 1,337 hashtags. However, there is
important heterogeneity in what social factors are required to reproduce
different properties of hashtag cascades. For instance, while a combined
network+identity model best predicts the popularity of cascades, a network-only
model has better performance in predicting cascade growth and an identity-only
model in adopter composition. We are able to predict what type of hashtag is
best modeled by each combination of features and use this to further improve
performance. Additionally, consistent with prior literature on the combined
network+identity model most outperforms the single-factor counterfactuals among
hashtags used for expressing racial or regional identity, stance-taking,
talking about sports, or variants of existing cultural trends with very slow-
or fast-growing communicative need. In sum, our results imply the utility of
multi-factor models in predicting cascades, in order to account for the varied
ways in which network, identity, and other social factors play a role in the
diffusion of hashtags on Twitter.","[{'name': 'Aparna Ananthasubramaniam'}, {'name': 'Yufei Zhu'}, {'name': 'David Jurgens'}, {'name': 'Daniel Romero'}]",2024-07-17T17:51:49Z
http://arxiv.org/abs/2407.12749v1,http://arxiv.org/abs/2407.12749v1,HDLCopilot: Hardware Design Library Querying with Natural Language,"Hardware design engineers routinely work with multiple Process Design Kits
(PDKs) from various fabrication labs, each containing several standard cell
libraries, optimized for specific metric such as speed, power, or density.
These libraries include multiple views such as liberty files for timing
information, LEF files for abstract layout details, and technology LEF for
process design rules. Navigating this complex landscape to retrieve specific
information about gates or design rules is often time-consuming and
error-prone. To address this, we present HDLCopilot, an LLM-powered PDK query
system that allows engineers to streamline interactions with PDKs in natural
language format, making information retrieval accurate and more efficient.
HDLCopilot achieves an accuracy of 94.23\% on an evaluation set comprised of
diverse and complex natural language queries. HDLCopilot positions itself as a
powerful assistant in the hardware design process, enhancing productivity and
reducing potential human errors.","[{'name': 'Manar Abdelatty'}, {'name': 'Sherief Reda'}]",2024-07-17T17:11:13Z
http://arxiv.org/abs/2407.12734v1,http://arxiv.org/abs/2407.12734v1,A LLM Benchmark based on the Minecraft Builder Dialog Agent Task,"In this work we proposing adapting the Minecraft builder task into an LLM
benchmark suitable for evaluating LLM ability in spatially orientated tasks,
and informing builder agent design. Previous works have proposed corpora with
varying complex structures, and human written instructions. We instead attempt
to provide a comprehensive synthetic benchmark for testing builder agents over
a series of distinct tasks that comprise of common building operations. We
believe this approach allows us to probe specific strengths and weaknesses of
different agents, and test the ability of LLMs in the challenging area of
spatial reasoning and vector based math.","[{'name': 'Chris Madge'}, {'name': 'Massimo Poesio'}]",2024-07-17T16:52:23Z
http://arxiv.org/abs/2407.12725v1,http://arxiv.org/abs/2407.12725v1,"Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language
  Models?","Elaborating a series of intermediate reasoning steps significantly improves
the ability of large language models (LLMs) to solve complex problems, as such
steps would evoke LLMs to think sequentially. However, human sarcasm
understanding is often considered an intuitive and holistic cognitive process,
in which various linguistic, contextual, and emotional cues are integrated to
form a comprehensive understanding of the speaker's true intention, which is
argued not be limited to a step-by-step reasoning process. To verify this
argument, we introduce a new prompting framework called SarcasmCue, which
contains four prompting strategies, $viz.$ chain of contradiction (CoC), graph
of cues (GoC), bagging of cues (BoC) and tensor of cues (ToC), which elicits
LLMs to detect human sarcasm by considering sequential and non-sequential
prompting methods. Through a comprehensive empirical comparison on four
benchmarking datasets, we show that the proposed four prompting methods
outperforms standard IO prompting, CoT and ToT with a considerable margin, and
non-sequential prompting generally outperforms sequential prompting.","[{'name': 'Ben Yao'}, {'name': 'Yazhou Zhang'}, {'name': 'Qiuchi Li'}, {'name': 'Jing Qin'}]",2024-07-17T16:42:03Z
http://arxiv.org/abs/2407.12707v2,http://arxiv.org/abs/2407.12707v2,TTSDS -- Text-to-Speech Distribution Score,"Many recently published Text-to-Speech (TTS) systems produce audio close to
real speech. However, TTS evaluation needs to be revisited to make sense of the
results obtained with the new architectures, approaches and datasets. We
propose evaluating the quality of synthetic speech as a combination of multiple
factors such as prosody, speaker identity, and intelligibility. Our approach
assesses how well synthetic speech mirrors real speech by obtaining correlates
of each factor and measuring their distance from both real speech datasets and
noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and
show that our score computed as an unweighted average of factors strongly
correlates with the human evaluations from each time period.","[{'name': 'Christoph Minixhofer'}, {'name': 'Ondřej Klejch'}, {'name': 'Peter Bell'}]",2024-07-17T16:30:27Z
http://arxiv.org/abs/2407.12703v3,http://arxiv.org/abs/2407.12703v3,"Subgraph-Aware Training of Text-based Methods for Knowledge Graph
  Completion","Fine-tuning pre-trained language models (PLMs) has recently shown a potential
to improve knowledge graph completion (KGC). However, most PLM-based methods
encode only textual information, neglecting various topological structures of
knowledge graphs (KGs). In this paper, we empirically validate the significant
relations between the structural properties of KGs and the performance of the
PLM-based methods. To leverage the structural knowledge, we propose a
Subgraph-Aware Training framework for KGC (SATKGC) that combines (i)
subgraph-aware mini-batching to encourage hard negative sampling, and (ii) a
new contrastive learning method to focus more on harder entities and harder
negative triples in terms of the structural properties. To the best of our
knowledge, this is the first study to comprehensively incorporate the
structural inductive bias of the subgraphs into fine-tuning PLMs. Extensive
experiments on four KGC benchmarks demonstrate the superiority of SATKGC. Our
code is available.","[{'name': 'Youmin Ko'}, {'name': 'Hyemin Yang'}, {'name': 'Taeuk Kim'}, {'name': 'Hyunjoon Kim'}]",2024-07-17T16:25:37Z
http://arxiv.org/abs/2407.12665v1,http://arxiv.org/abs/2407.12665v1,Patch-Level Training for Large Language Models,"As Large Language Models (LLMs) achieve remarkable progress in language
understanding and generation, their training efficiency has become a critical
concern. Traditionally, LLMs are trained to predict the next token in a
sequence. Despite the success of token-level training, it suffers from
considerable computational costs due to the need to process an extensive number
of tokens. To mitigate this issue, this paper introduces patch-level training
for LLMs, which reduces the sequence length by compressing multiple tokens into
a single patch. During patch-level training, we feed the language model shorter
sequences of patches and train it to predict the next patch, thereby processing
the majority of the training data at a significantly reduced computational
cost. Following this, the model continues token-level training on the remaining
training data to align with the inference mode. Experiments on a diverse range
of models (370M-2.7B parameters) demonstrate that patch-level training can
reduce overall computational costs to 0.5$\times$, without compromising the
model performance compared to token-level training. Source code:
\url{https://github.com/shaochenze/PatchTrain}.","[{'name': 'Chenze Shao'}, {'name': 'Fandong Meng'}, {'name': 'Jie Zhou'}]",2024-07-17T15:48:39Z
http://arxiv.org/abs/2407.12626v1,http://arxiv.org/abs/2407.12626v1,"Domain-specific or Uncertainty-aware models: Does it really make a
  difference for biomedical text classification?","The success of pretrained language models (PLMs) across a spate of use-cases
has led to significant investment from the NLP community towards building
domain-specific foundational models. On the other hand, in mission critical
settings such as biomedical applications, other aspects also factor in-chief of
which is a model's ability to produce reasonable estimates of its own
uncertainty. In the present study, we discuss these two desiderata through the
lens of how they shape the entropy of a model's output probability
distribution. We find that domain specificity and uncertainty awareness can
often be successfully combined, but the exact task at hand weighs in much more
strongly.","[{'name': 'Aman Sinha'}, {'name': 'Timothee Mickus'}, {'name': 'Marianne Clausel'}, {'name': 'Mathieu Constant'}, {'name': 'Xavier Coubez'}]",2024-07-17T14:52:46Z
http://arxiv.org/abs/2407.12620v2,http://arxiv.org/abs/2407.12620v2,"Harnessing the Power of Artificial Intelligence to Vitalize Endangered
  Indigenous Languages: Technologies and Experiences","Since 2022 we have been exploring application areas and technologies in which
Artificial Intelligence (AI) and modern Natural Language Processing (NLP), such
as Large Language Models (LLMs), can be employed to foster the usage and
facilitate the documentation of Indigenous languages which are in danger of
disappearing. We start by discussing the decreasing diversity of languages in
the world and how working with Indigenous languages poses unique ethical
challenges for AI and NLP. To address those challenges, we propose an
alternative development AI cycle based on community engagement and usage. Then,
we report encouraging results in the development of high-quality machine
learning translators for Indigenous languages by fine-tuning state-of-the-art
(SOTA) translators with tiny amounts of data and discuss how to avoid some
common pitfalls in the process. We also present prototypes we have built in
projects done in 2023 and 2024 with Indigenous communities in Brazil, aimed at
facilitating writing, and discuss the development of Indigenous Language Models
(ILMs) as a replicable and scalable way to create spell-checkers, next-word
predictors, and similar tools. Finally, we discuss how we envision a future for
language documentation where dying languages are preserved as interactive
language models.","[{'name': 'Claudio Pinhanez'}, {'name': 'Paulo Cavalin'}, {'name': 'Luciana Storto'}, {'name': 'Thomas Finbow'}, {'name': 'Alexander Cobbinah'}, {'name': 'Julio Nogima'}, {'name': 'Marisa Vasconcelos'}, {'name': 'Pedro Domingues'}, {'name': 'Priscila de Souza Mizukami'}, {'name': 'Nicole Grell'}, {'name': 'Majoí Gongora'}, {'name': 'Isabel Gonçalves'}]",2024-07-17T14:46:37Z
http://arxiv.org/abs/2407.12613v1,http://arxiv.org/abs/2407.12613v1,"AudienceView: AI-Assisted Interpretation of Audience Feedback in
  Journalism","Understanding and making use of audience feedback is important but difficult
for journalists, who now face an impractically large volume of audience
comments online. We introduce AudienceView, an online tool to help journalists
categorize and interpret this feedback by leveraging large language models
(LLMs). AudienceView identifies themes and topics, connects them back to
specific comments, provides ways to visualize the sentiment and distribution of
the comments, and helps users develop ideas for subsequent reporting projects.
We consider how such tools can be useful in a journalist's workflow, and
emphasize the importance of contextual awareness and human judgment.","[{'name': 'William Brannon'}, {'name': 'Doug Beeferman'}, {'name': 'Hang Jiang'}, {'name': 'Andrew Heyward'}, {'name': 'Deb Roy'}]",2024-07-17T14:41:35Z
http://arxiv.org/abs/2407.12580v1,http://arxiv.org/abs/2407.12580v1,E5-V: Universal Embeddings with Multimodal Large Language Models,"Multimodal large language models (MLLMs) have shown promising advancements in
general visual and language understanding. However, the representation of
multimodal information using MLLMs remains largely unexplored. In this work, we
introduce a new framework, E5-V, designed to adapt MLLMs for achieving
universal multimodal embeddings. Our findings highlight the significant
potential of MLLMs in representing multimodal inputs compared to previous
approaches. By leveraging MLLMs with prompts, E5-V effectively bridges the
modality gap between different types of inputs, demonstrating strong
performance in multimodal embeddings even without fine-tuning. We propose a
single modality training approach for E5-V, where the model is trained
exclusively on text pairs. This method demonstrates significant improvements
over traditional multimodal training on image-text pairs, while reducing
training costs by approximately 95%. Additionally, this approach eliminates the
need for costly multimodal training data collection. Extensive experiments
across four types of tasks demonstrate the effectiveness of E5-V. As a
universal multimodal model, E5-V not only achieves but often surpasses
state-of-the-art performance in each task, despite being trained on a single
modality.","[{'name': 'Ting Jiang'}, {'name': 'Minghui Song'}, {'name': 'Zihan Zhang'}, {'name': 'Haizhen Huang'}, {'name': 'Weiwei Deng'}, {'name': 'Feng Sun'}, {'name': 'Qi Zhang'}, {'name': 'Deqing Wang'}, {'name': 'Fuzhen Zhuang'}]",2024-07-17T14:04:12Z
http://arxiv.org/abs/2407.12543v1,http://arxiv.org/abs/2407.12543v1,"Abstraction Alignment: Comparing Model and Human Conceptual
  Relationships","Abstraction -- the process of generalizing specific examples into broad
reusable patterns -- is central to how people efficiently process and store
information and apply their knowledge to new data. Promisingly, research has
shown that ML models learn representations that span levels of abstraction,
from specific concepts like ""bolo tie"" and ""car tire"" to more general concepts
like ""CEO"" and ""model"". However, existing techniques analyze these
representations in isolation, treating learned concepts as independent
artifacts rather than an interconnected web of abstraction. As a result,
although we can identify the concepts a model uses to produce its output, it is
difficult to assess if it has learned a human-aligned abstraction of the
concepts that will generalize to new data. To address this gap, we introduce
abstraction alignment, a methodology to measure the agreement between a model's
learned abstraction and the expected human abstraction. We quantify abstraction
alignment by comparing model outputs against a human abstraction graph, such as
linguistic relationships or medical disease hierarchies. In evaluation tasks
interpreting image models, benchmarking language models, and analyzing medical
datasets, abstraction alignment provides a deeper understanding of model
behavior and dataset content, differentiating errors based on their agreement
with human knowledge, expanding the verbosity of current model quality metrics,
and revealing ways to improve existing human abstractions.","[{'name': 'Angie Boggust'}, {'name': 'Hyemin Bang'}, {'name': 'Hendrik Strobelt'}, {'name': 'Arvind Satyanarayan'}]",2024-07-17T13:27:26Z
http://arxiv.org/abs/2407.12532v1,http://arxiv.org/abs/2407.12532v1,"Towards Collaborative Intelligence: Propagating Intentions and Reasoning
  for Multi-Agent Coordination with Large Language Models","Effective collaboration in multi-agent systems requires communicating goals
and intentions between agents. Current agent frameworks often suffer from
dependencies on single-agent execution and lack robust inter-module
communication, frequently leading to suboptimal multi-agent reinforcement
learning (MARL) policies and inadequate task coordination. To address these
challenges, we present a framework for training large language models (LLMs) as
collaborative agents to enable coordinated behaviors in cooperative MARL. Each
agent maintains a private intention consisting of its current goal and
associated sub-tasks. Agents broadcast their intentions periodically, allowing
other agents to infer coordination tasks. A propagation network transforms
broadcast intentions into teammate-specific communication messages, sharing
relevant goals with designated teammates. The architecture of our framework is
structured into planning, grounding, and execution modules. During execution,
multiple agents interact in a downstream environment and communicate intentions
to enable coordinated behaviors. The grounding module dynamically adapts
comprehension strategies based on emerging coordination patterns, while
feedback from execution agents influnces the planning module, enabling the
dynamic re-planning of sub-tasks. Results in collaborative environment
simulation demonstrate intention propagation reduces miscoordination errors by
aligning sub-task dependencies between agents. Agents learn when to communicate
intentions and which teammates require task details, resulting in emergent
coordinated behaviors. This demonstrates the efficacy of intention sharing for
cooperative multi-agent RL based on LLMs.","[{'name': 'Xihe Qiu'}, {'name': 'Haoyu Wang'}, {'name': 'Xiaoyu Tan'}, {'name': 'Chao Qu'}, {'name': 'Yujie Xiong'}, {'name': 'Yuan Cheng'}, {'name': 'Yinghui Xu'}, {'name': 'Wei Chu'}, {'name': 'Yuan Qi'}]",2024-07-17T13:14:00Z
http://arxiv.org/abs/2407.12529v1,http://arxiv.org/abs/2407.12529v1,Crafting the Path: Robust Query Rewriting for Information Retrieval,"Query rewriting aims to generate a new query that can complement the original
query to improve the information retrieval system. Recent studies on query
rewriting, such as query2doc (Q2D), query2expand (Q2E) and querey2cot (Q2C),
rely on the internal knowledge of Large Language Models (LLMs) to generate a
relevant passage to add information to the query. Nevertheless, the efficacy of
these methodologies may markedly decline in instances where the requisite
knowledge is not encapsulated within the model's intrinsic parameters. In this
paper, we propose a novel structured query rewriting method called Crafting the
Path tailored for retrieval systems. Crafting the Path involves a three-step
process that crafts query-related information necessary for finding the
passages to be searched in each step. Specifically, the Crafting the Path
begins with Query Concept Comprehension, proceeds to Query Type Identification,
and finally conducts Expected Answer Extraction. Experimental results show that
our method outperforms previous rewriting methods, especially in less familiar
domains for LLMs. We demonstrate that our method is less dependent on the
internal parameter knowledge of the model and generates queries with fewer
factual inaccuracies. Furthermore, we observe that Crafting the Path has less
latency compared to the baselines.","[{'name': 'Ingeol Baek'}, {'name': 'Jimin Lee'}, {'name': 'Joonho Yang'}, {'name': 'Hwanhee Lee'}]",2024-07-17T13:11:28Z
http://arxiv.org/abs/2407.12522v1,http://arxiv.org/abs/2407.12522v1,Struct-X: Enhancing Large Language Models Reasoning with Structured Data,"Structured data, rich in logical and relational information, has the
potential to enhance the reasoning abilities of large language models (LLMs).
Still, its integration poses a challenge due to the risk of overwhelming LLMs
with excessive tokens and irrelevant context information. To address this, we
propose Struct-X, a novel framework that operates through five key phases:
``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize
structured data. It begins by encoding structured data into a topological space
using graph embeddings, followed by filling in missing entity information with
knowledge retrieval modules, and filtering out irrelevant tokens via a
self-supervised module. The final phase involves constructing a topological
network with selected tokens to further reduce the total token length for more
effective LLM inference. Additionally, Struct-X includes an Auxiliary Module
trained to generate prompts, aiding LLMs in analyzing structured data.
Extensive experiments on benchmarks, including the knowledge graph
question-answer task and the long document reading comprehension task, show
that Struct-X notably improves LLM reasoning, demonstrating the effectiveness
of structured data augmentation in improving LLM inference with complex input
context.","[{'name': 'Xiaoyu Tan'}, {'name': 'Haoyu Wang'}, {'name': 'Xihe Qiu'}, {'name': 'Yuan Cheng'}, {'name': 'Yinghui Xu'}, {'name': 'Wei Chu'}, {'name': 'Yuan Qi'}]",2024-07-17T13:06:25Z
http://arxiv.org/abs/2407.12514v1,http://arxiv.org/abs/2407.12514v1,On Initializing Transformers with Pre-trained Embeddings,"It has become common practice now to use random initialization schemes,
rather than the pre-trained embeddings, when training transformer based models
from scratch. Indeed, we find that pre-trained word embeddings from GloVe, and
some sub-word embeddings extracted from language models such as T5 and mT5 fare
much worse compared to random initialization. This is counter-intuitive given
the well-known representational and transfer-learning advantages of
pre-training. Interestingly, we also find that BERT and mBERT embeddings fare
better than random initialization, showing the advantages of pre-trained
representations. In this work, we posit two potential factors that contribute
to these mixed results: the model sensitivity to parameter distribution and the
embedding interactions with position encodings. We observe that pre-trained
GloVe, T5, and mT5 embeddings have a wider distribution of values. As argued in
the initialization studies, such large value initializations can lead to poor
training because of saturated outputs. Further, the larger embedding values
can, in effect, absorb the smaller position encoding values when added
together, thus losing position information. Standardizing the pre-trained
embeddings to a narrow range (e.g. as prescribed by Xavier) leads to
substantial gains for Glove, T5, and mT5 embeddings. On the other hand, BERT
pre-trained embeddings, while larger, are still relatively closer to Xavier
initialization range which may allow it to effectively transfer the pre-trained
knowledge.","[{'name': 'Ha Young Kim'}, {'name': 'Niranjan Balasubramanian'}, {'name': 'Byungkon Kang'}]",2024-07-17T11:57:10Z
http://arxiv.org/abs/2407.12512v1,http://arxiv.org/abs/2407.12512v1,"$\textit{GeoHard}$: Towards Measuring Class-wise Hardness through
  Modelling Class Semantics","Recent advances in measuring hardness-wise properties of data guide language
models in sample selection within low-resource scenarios. However,
class-specific properties are overlooked for task setup and learning. How will
these properties influence model learning and is it generalizable across
datasets? To answer this question, this work formally initiates the concept of
$\textit{class-wise hardness}$. Experiments across eight natural language
understanding (NLU) datasets demonstrate a consistent hardness distribution
across learning paradigms, models, and human judgment. Subsequent experiments
unveil a notable challenge in measuring such class-wise hardness with
instance-level metrics in previous works. To address this, we propose
$\textit{GeoHard}$ for class-wise hardness measurement by modeling class
geometry in the semantic embedding space. $\textit{GeoHard}$ surpasses
instance-level metrics by over 59 percent on $\textit{Pearson}$'s correlation
on measuring class-wise hardness. Our analysis theoretically and empirically
underscores the generality of $\textit{GeoHard}$ as a fresh perspective on data
diagnosis. Additionally, we showcase how understanding class-wise hardness can
practically aid in improving task learning.","[{'name': 'Fengyu Cai'}, {'name': 'Xinran Zhao'}, {'name': 'Hongming Zhang'}, {'name': 'Iryna Gurevych'}, {'name': 'Heinz Koeppl'}]",2024-07-17T11:53:39Z
http://arxiv.org/abs/2407.12508v1,http://arxiv.org/abs/2407.12508v1,"MERLIN: Multimodal Embedding Refinement via LLM-based Iterative
  Navigation for Text-Video Retrieval-Rerank Pipeline","The rapid expansion of multimedia content has made accurately retrieving
relevant videos from large collections increasingly challenging. Recent
advancements in text-video retrieval have focused on cross-modal interactions,
large-scale foundation model training, and probabilistic modeling, yet often
neglect the crucial user perspective, leading to discrepancies between user
queries and the content retrieved. To address this, we introduce MERLIN
(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,
training-free pipeline that leverages Large Language Models (LLMs) for
iterative feedback learning. MERLIN refines query embeddings from a user
perspective, enhancing alignment between queries and video content through a
dynamic question answering process. Experimental results on datasets like
MSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves
Recall@1, outperforming existing systems and confirming the benefits of
integrating LLMs into multimodal retrieval systems for more responsive and
context-aware multimedia retrieval.","[{'name': 'Donghoon Han'}, {'name': 'Eunhwan Park'}, {'name': 'Gisang Lee'}, {'name': 'Adam Lee'}, {'name': 'Nojun Kwak'}]",2024-07-17T11:45:02Z
http://arxiv.org/abs/2407.12504v1,http://arxiv.org/abs/2407.12504v1,Case2Code: Learning Inductive Reasoning with Synthetic Data,"Complex reasoning is an impressive ability shown by large language models
(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought
prompting or iterative tool-using to solve challenging tasks step-by-step. In
this paper, we hope to focus on evaluating and teaching LLMs to conduct
inductive reasoning, that is, LLMs are supposed to infer underlying rules by
observing examples or sequential transformations. However, collecting
large-scale and diverse human-generated inductive data is challenging. We focus
on data synthesis in the code domain and propose a \textbf{Case2Code} task by
exploiting the expressiveness and correctness of programs. Specifically, we
collect a diverse set of executable programs, synthesize input-output
transformations for each program, and force LLMs to infer the underlying code
implementations based on the synthetic I/O cases. We first evaluate
representative LLMs on the synthesized Case2Code task and demonstrate that the
Case-to-code induction is challenging for LLMs. Then, we synthesize large-scale
Case2Code training samples to train LLMs to perform inductive reasoning.
Experimental results show that such induction training benefits not only in
distribution Case2Code performance but also enhances various coding abilities
of trained LLMs, demonstrating the great potential of learning inductive
reasoning via synthetic data.","[{'name': 'Yunfan Shao'}, {'name': 'Linyang Li'}, {'name': 'Yichuan Ma'}, {'name': 'Peiji Li'}, {'name': 'Demin Song'}, {'name': 'Qinyuan Cheng'}, {'name': 'Shimin Li'}, {'name': 'Xiaonan Li'}, {'name': 'Pengyu Wang'}, {'name': 'Qipeng Guo'}, {'name': 'Hang Yan'}, {'name': 'Xipeng Qiu'}, {'name': 'Xuanjing Huang'}, {'name': 'Dahua Lin'}]",2024-07-17T11:35:00Z
http://arxiv.org/abs/2407.12500v2,http://arxiv.org/abs/2407.12500v2,"Automate or Assist? The Role of Computational Models in Identifying
  Gendered Discourse in US Capital Trial Transcripts","The language used by US courtroom actors in criminal trials has long been
studied for biases. However, systematic studies for bias in high-stakes court
trials have been difficult, due to the nuanced nature of bias and the legal
expertise required. Large language models offer the possibility to automate
annotation. But validating the computational approach requires both an
understanding of how automated methods fit in existing annotation workflows and
what they really offer. We present a case study of adding a computational model
to a complex and high-stakes problem: identifying gender-biased language in US
capital trials for women defendants. Our team of experienced death-penalty
lawyers and NLP technologists pursue a three-phase study: first annotating
manually, then training and evaluating computational models, and finally
comparing expert annotations to model predictions. Unlike many typical NLP
tasks, annotating for gender bias in months-long capital trials is complicated,
with many individual judgment calls. Contrary to standard arguments for
automation that are based on efficiency and scalability, legal experts find the
computational models most useful in providing opportunities to reflect on their
own bias in annotation and to build consensus on annotation rules. This
experience suggests that seeking to replace experts with computational models
for complex annotation is both unrealistic and undesirable. Rather,
computational models offer valuable opportunities to assist the legal experts
in annotation-based studies.","[{'name': 'Andrea W Wen-Yi'}, {'name': 'Kathryn Adamson'}, {'name': 'Nathalie Greenfield'}, {'name': 'Rachel Goldberg'}, {'name': 'Sandra Babcock'}, {'name': 'David Mimno'}, {'name': 'Allison Koenecke'}]",2024-07-17T11:30:04Z
http://arxiv.org/abs/2407.12498v1,http://arxiv.org/abs/2407.12498v1,"Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of
  Few-Shot Learning","The linguistic capabilities of Multimodal Large Language Models (MLLMs) are
critical for their effective application across diverse tasks. This study aims
to evaluate the performance of MLLMs on the VALSE benchmark, focusing on the
efficacy of few-shot In-Context Learning (ICL), and Chain-of-Thought (CoT)
prompting. We conducted a comprehensive assessment of state-of-the-art MLLMs,
varying in model size and pretraining datasets. The experimental results reveal
that ICL and CoT prompting significantly boost model performance, particularly
in tasks requiring complex reasoning and contextual understanding. Models
pretrained on captioning datasets show superior zero-shot performance, while
those trained on interleaved image-text data benefit from few-shot learning.
Our findings provide valuable insights into optimizing MLLMs for better
grounding of language in visual contexts, highlighting the importance of the
composition of pretraining data and the potential of few-shot learning
strategies to improve the reasoning abilities of MLLMs.","[{'name': 'Mustafa Dogan'}, {'name': 'Ilker Kesen'}, {'name': 'Iacer Calixto'}, {'name': 'Aykut Erdem'}, {'name': 'Erkut Erdem'}]",2024-07-17T11:26:47Z
http://arxiv.org/abs/2407.12481v1,http://arxiv.org/abs/2407.12481v1,Pretraining Data and Tokenizer for Indic LLM,"We present a novel approach to data preparation for developing multilingual
Indic large language model. Our meticulous data acquisition spans open-source
and proprietary sources, including Common Crawl, Indic books, news articles,
and Wikipedia, ensuring a diverse and rich linguistic representation. For each
Indic language, we design a custom preprocessing pipeline to effectively
eliminate redundant and low-quality text content. Additionally, we perform
deduplication on Common Crawl data to address the redundancy present in 70% of
the crawled web pages. This study focuses on developing high-quality data,
optimizing tokenization for our multilingual dataset for Indic large language
models with 3B and 7B parameters, engineered for superior performance in Indic
languages. We introduce a novel multilingual tokenizer training strategy,
demonstrating our custom-trained Indic tokenizer outperforms the
state-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word
ratio for Indic languages.","[{'name': 'Rahul Kumar'}, {'name': 'Shubham Kakde'}, {'name': 'Divyansh Rajput'}, {'name': 'Daud Ibrahim'}, {'name': 'Rishabh Nahata'}, {'name': 'Pidathala Sowjanya'}, {'name': 'Deepak Kumar'}]",2024-07-17T11:06:27Z
http://arxiv.org/abs/2407.12473v1,http://arxiv.org/abs/2407.12473v1,A Novel Dependency Framework for Enhancing Discourse Data Analysis,"The development of different theories of discourse structure has led to the
establishment of discourse corpora based on these theories. However, the
existence of discourse corpora established on different theoretical bases
creates challenges when it comes to exploring them in a consistent and cohesive
way. This study has as its primary focus the conversion of PDTB annotations
into dependency structures. It employs refined BERT-based discourse parsers to
test the validity of the dependency data derived from the PDTB-style corpora in
English, Chinese, and several other languages. By converting both PDTB and RST
annotations for the same texts into dependencies, this study also applies
``dependency distance'' metrics to examine the correlation between RST
dependencies and PDTB dependencies in English. The results show that the PDTB
dependency data is valid and that there is a strong correlation between the two
types of dependency distance. This study presents a comprehensive approach for
analyzing and evaluating discourse corpora by employing discourse dependencies
to achieve unified analysis. By applying dependency representations, we can
extract data from PDTB, RST, and SDRT corpora in a coherent and unified manner.
Moreover, the cross-linguistic validation establishes the framework's
generalizability beyond English. The establishment of this comprehensive
dependency framework overcomes limitations of existing discourse corpora,
supporting a diverse range of algorithms and facilitating further studies in
computational discourse analysis and language sciences.","[{'name': 'Kun Sun'}, {'name': 'Rong Wang'}]",2024-07-17T10:55:00Z
http://arxiv.org/abs/2407.12471v1,http://arxiv.org/abs/2407.12471v1,"Characterization of Political Polarized Users Attacked by Language
  Toxicity on Twitter","Understanding the dynamics of language toxicity on social media is important
for us to investigate the propagation of misinformation and the development of
echo chambers for political scenarios such as U.S. presidential elections.
Recent research has used large-scale data to investigate the dynamics across
social media platforms. However, research on the toxicity dynamics is not
enough. This study aims to provide a first exploration of the potential
language toxicity flow among Left, Right and Center users. Specifically, we aim
to examine whether Left users were easier to be attacked by language toxicity.
In this study, more than 500M Twitter posts were examined. It was discovered
that Left users received much more toxic replies than Right and Center users.",[{'name': 'Wentao Xu'}],2024-07-17T10:49:47Z
http://arxiv.org/abs/2407.12470v1,http://arxiv.org/abs/2407.12470v1,Continual Learning for Temporal-Sensitive Question Answering,"In this study, we explore an emerging research area of Continual Learning for
Temporal Sensitive Question Answering (CLTSQA). Previous research has primarily
focused on Temporal Sensitive Question Answering (TSQA), often overlooking the
unpredictable nature of future events. In real-world applications, it's crucial
for models to continually acquire knowledge over time, rather than relying on a
static, complete dataset. Our paper investigates strategies that enable models
to adapt to the ever-evolving information landscape, thereby addressing the
challenges inherent in CLTSQA. To support our research, we first create a novel
dataset, divided into five subsets, designed specifically for various stages of
continual learning. We then propose a training framework for CLTSQA that
integrates temporal memory replay and temporal contrastive learning. Our
experimental results highlight two significant insights: First, the CLTSQA task
introduces unique challenges for existing models. Second, our proposed
framework effectively navigates these challenges, resulting in improved
performance.","[{'name': 'Wanqi Yang'}, {'name': 'Yunqiu Xu'}, {'name': 'Yanda Li'}, {'name': 'Kunze Wang'}, {'name': 'Binbin Huang'}, {'name': 'Ling Chen'}]",2024-07-17T10:47:43Z
http://arxiv.org/abs/2407.12451v2,http://arxiv.org/abs/2407.12451v2,"Across Platforms and Languages: Dutch Influencers and Legal Disclosures
  on Instagram, YouTube and TikTok","Content monetization on social media fuels a growing influencer economy.
Influencer marketing remains largely undisclosed or inappropriately disclosed
on social media. Non-disclosure issues have become a priority for national and
supranational authorities worldwide, who are starting to impose increasingly
harsher sanctions on them. This paper proposes a transparent methodology for
measuring whether and how influencers comply with disclosures based on legal
standards. We introduce a novel distinction between disclosures that are
legally sufficient (green) and legally insufficient (yellow). We apply this
methodology to an original dataset reflecting the content of 150 Dutch
influencers publicly registered with the Dutch Media Authority based on
recently introduced registration obligations. The dataset consists of 292,315
posts and is multi-language (English and Dutch) and cross-platform (Instagram,
YouTube and TikTok). We find that influencer marketing remains generally
underdisclosed on social media, and that bigger influencers are not necessarily
more compliant with disclosure standards.","[{'name': 'Haoyang Gui'}, {'name': 'Thales Bertaglia'}, {'name': 'Catalina Goanta'}, {'name': 'Sybe de Vries'}, {'name': 'Gerasimos Spanakis'}]",2024-07-17T09:59:52Z
http://arxiv.org/abs/2407.12426v1,http://arxiv.org/abs/2407.12426v1,"Sharif-STR at SemEval-2024 Task 1: Transformer as a Regression Model for
  Fine-Grained Scoring of Textual Semantic Relations","Semantic Textual Relatedness holds significant relevance in Natural Language
Processing, finding applications across various domains. Traditionally,
approaches to STR have relied on knowledge-based and statistical methods.
However, with the emergence of Large Language Models, there has been a paradigm
shift, ushering in new methodologies. In this paper, we delve into the
investigation of sentence-level STR within Track A (Supervised) by leveraging
fine-tuning techniques on the RoBERTa transformer. Our study focuses on
assessing the efficacy of this approach across different languages. Notably,
our findings indicate promising advancements in STR performance, particularly
in Latin languages. Specifically, our results demonstrate notable improvements
in English, achieving a correlation of 0.82 and securing a commendable 19th
rank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the
15th position. However, our approach encounters challenges in languages like
Arabic, where we observed a correlation of only 0.38, resulting in a 20th rank.","[{'name': 'Seyedeh Fatemeh Ebrahimi'}, {'name': 'Karim Akhavan Azari'}, {'name': 'Amirmasoud Iravani'}, {'name': 'Hadi Alizadeh'}, {'name': 'Zeinab Sadat Taghavi'}, {'name': 'Hossein Sameti'}]",2024-07-17T09:25:18Z
http://arxiv.org/abs/2407.12425v1,http://arxiv.org/abs/2407.12425v1,"Navigating the Noisy Crowd: Finding Key Information for Claim
  Verification","Claim verification is a task that involves assessing the truthfulness of a
given claim based on multiple evidence pieces. Using large language models
(LLMs) for claim verification is a promising way. However, simply feeding all
the evidence pieces to an LLM and asking if the claim is factual does not yield
good results. The challenge lies in the noisy nature of both the evidence and
the claim: evidence passages typically contain irrelevant information, with the
key facts hidden within the context, while claims often convey multiple aspects
simultaneously. To navigate this ""noisy crowd"" of information, we propose EACon
(Evidence Abstraction and Claim Deconstruction), a framework designed to find
key information within evidence and verify each aspect of a claim separately.
EACon first finds keywords from the claim and employs fuzzy matching to select
relevant keywords for each raw evidence piece. These keywords serve as a guide
to extract and summarize critical information into abstracted evidence.
Subsequently, EACon deconstructs the original claim into subclaims, which are
then verified against both abstracted and raw evidence individually. We
evaluate EACon using two open-source LLMs on two challenging datasets. Results
demonstrate that EACon consistently and substantially improve LLMs' performance
in claim verification.","[{'name': 'Haisong Gong'}, {'name': 'Huanhuan Ma'}, {'name': 'Qiang Liu'}, {'name': 'Shu Wu'}, {'name': 'Liang Wang'}]",2024-07-17T09:24:10Z
http://arxiv.org/abs/2407.12402v1,http://arxiv.org/abs/2407.12402v1,"TurkishMMLU: Measuring Massive Multitask Language Understanding in
  Turkish","Multiple choice question answering tasks evaluate the reasoning,
comprehension, and mathematical abilities of Large Language Models (LLMs).
While existing benchmarks employ automatic translation for multilingual
evaluation, this approach is error-prone and potentially introduces culturally
biased questions, especially in social sciences. We introduce the first
multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'
understanding of the Turkish language. TurkishMMLU includes over 10,000
questions, covering 9 different subjects from Turkish high-school education
curricula. These questions are written by curriculum experts, suitable for the
high-school curricula in Turkey, covering subjects ranging from natural
sciences and math questions to more culturally representative topics such as
Turkish Literature and the history of the Turkish Republic. We evaluate over 20
LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),
closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)
models. We provide an extensive evaluation, including zero-shot and few-shot
evaluation of LLMs, chain-of-thought reasoning, and question difficulty
analysis along with model performance. We provide an in-depth analysis of the
Turkish capabilities and limitations of current LLMs to provide insights for
future LLMs for the Turkish language. We publicly release our code for the
dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.","[{'name': 'Arda Yüksel'}, {'name': 'Abdullatif Köksal'}, {'name': 'Lütfi Kerem Şenel'}, {'name': 'Anna Korhonen'}, {'name': 'Hinrich Schütze'}]",2024-07-17T08:28:55Z
http://arxiv.org/abs/2407.12393v4,http://arxiv.org/abs/2407.12393v4,PersLLM: A Personified Training Approach for Large Language Models,"Large language models exhibit aspects of human-level intelligence that
catalyze their application as human-like agents in domains such as social
simulations, human-machine interactions, and collaborative multi-agent systems.
However, the absence of distinct personalities, such as displaying ingratiating
behaviors, inconsistent opinions, and uniform response patterns, diminish LLMs
utility in practical applications. Addressing this, the development of
personality traits in LLMs emerges as a crucial area of research to unlock
their latent potential. Existing methods to personify LLMs generally involve
strategies like employing stylized training data for instruction tuning or
using prompt engineering to simulate different personalities. These methods
only capture superficial linguistic styles instead of the core of personalities
and are therefore not stable. In this study, we propose PersLLM, integrating
psychology-grounded principles of personality: social practice, consistency,
and dynamic development, into a comprehensive training methodology. We
incorporate personality traits directly into the model parameters, enhancing
the model's resistance to induction, promoting consistency, and supporting the
dynamic evolution of personality. Single-agent evaluation validates our
method's superiority, as it produces responses more aligned with reference
personalities compared to other approaches. Case studies for multi-agent
communication highlight its benefits in enhancing opinion consistency within
individual agents and fostering collaborative creativity among multiple agents
in dialogue contexts, potentially benefiting human simulation and multi-agent
cooperation. Additionally, human-agent interaction evaluations indicate that
our personified models significantly enhance interactive experiences,
underscoring the practical implications of our research.","[{'name': 'Zheni Zeng'}, {'name': 'Jiayi Chen'}, {'name': 'Huimin Chen'}, {'name': 'Yukun Yan'}, {'name': 'Yuxuan Chen'}, {'name': 'Zhenghao Liu'}, {'name': 'Zhiyuan Liu'}, {'name': 'Maosong Sun'}]",2024-07-17T08:13:22Z
http://arxiv.org/abs/2407.12389v1,http://arxiv.org/abs/2407.12389v1,Morphosyntactic Analysis for CHILDES,"Language development researchers are interested in comparing the process of
language learning across languages. Unfortunately, it has been difficult to
construct a consistent quantitative framework for such comparisons. However,
recent advances in AI (Artificial Intelligence) and ML (Machine Learning) are
providing new methods for ASR (automatic speech recognition) and NLP (natural
language processing) that can be brought to bear on this problem. Using the
Batchalign2 program (Liu et al., 2023), we have been transcribing and linking
data for the CHILDES database and have applied the UD (Universal Dependencies)
framework to provide a consistent and comparable morphosyntactic analysis for
27 languages. These new resources open possibilities for deeper crosslinguistic
study of language learning.","[{'name': 'Houjun Liu'}, {'name': 'Brian MacWhinney'}]",2024-07-17T08:11:24Z
http://arxiv.org/abs/2407.12376v1,http://arxiv.org/abs/2407.12376v1,Deep Learning-based Sentiment Analysis of Olympics Tweets,"Sentiment analysis (SA), is an approach of natural language processing (NLP)
for determining a text's emotional tone by analyzing subjective information
such as views, feelings, and attitudes toward specific topics, products,
services, events, or experiences. This study attempts to develop an advanced
deep learning (DL) model for SA to understand global audience emotions through
tweets in the context of the Olympic Games. The findings represent global
attitudes around the Olympics and contribute to advancing the SA models. We
have used NLP for tweet pre-processing and sophisticated DL models for arguing
with SA, this research enhances the reliability and accuracy of sentiment
classification. The study focuses on data selection, preprocessing,
visualization, feature extraction, and model building, featuring a baseline
Na\""ive Bayes (NB) model and three advanced DL models: Convolutional Neural
Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Bidirectional
Encoder Representations from Transformers (BERT). The results of the
experiments show that the BERT model can efficiently classify sentiments
related to the Olympics, achieving the highest accuracy of 99.23%.","[{'name': 'Indranil Bandyopadhyay'}, {'name': 'Rahul Karmakar'}]",2024-07-17T07:55:04Z
http://arxiv.org/abs/2407.12366v1,http://arxiv.org/abs/2407.12366v1,"NavGPT-2: Unleashing Navigational Reasoning Capability for Large
  Vision-Language Models","Capitalizing on the remarkable advancements in Large Language Models (LLMs),
there is a burgeoning initiative to harness LLMs for instruction following
robotic navigation. Such a trend underscores the potential of LLMs to
generalize navigational reasoning and diverse language understanding. However,
a significant discrepancy in agent performance is observed when integrating
LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous
downstream specialist models. Furthermore, the inherent capacity of language to
interpret and facilitate communication in agent interactions is often
underutilized in these integrations. In this work, we strive to bridge the
divide between VLN-specialized models and LLM-based navigation paradigms, while
maintaining the interpretative prowess of LLMs in generating linguistic
navigational reasoning. By aligning visual content in a frozen LLM, we
encompass visual observation comprehension for LLMs and exploit a way to
incorporate LLMs and navigation policy networks for effective action
predictions and navigational reasoning. We demonstrate the data efficiency of
the proposed methods and eliminate the gap between LM-based agents and
state-of-the-art VLN specialists.","[{'name': 'Gengze Zhou'}, {'name': 'Yicong Hong'}, {'name': 'Zun Wang'}, {'name': 'Xin Eric Wang'}, {'name': 'Qi Wu'}]",2024-07-17T07:44:26Z
http://arxiv.org/abs/2407.12888v1,http://arxiv.org/abs/2407.12888v1,"Explainable Biomedical Hypothesis Generation via Retrieval Augmented
  Generation enabled Large Language Models","The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.","[{'name': 'Alexander R. Pelletier'}, {'name': 'Joseph Ramirez'}, {'name': 'Irsyad Adam'}, {'name': 'Simha Sankar'}, {'name': 'Yu Yan'}, {'name': 'Ding Wang'}, {'name': 'Dylan Steinecke'}, {'name': 'Wei Wang'}, {'name': 'Peipei Ping'}]",2024-07-17T07:44:18Z
http://arxiv.org/abs/2407.12363v1,http://arxiv.org/abs/2407.12363v1,"Conversational Query Reformulation with the Guidance of Retrieved
  Documents","Conversational search seeks to retrieve relevant passages for the given
questions in Conversational QA (ConvQA). Questions in ConvQA face challenges
such as omissions and coreferences, making it difficult to obtain desired
search results. Conversational Query Reformulation (CQR) transforms these
current queries into de-contextualized forms to resolve these issues. However,
existing CQR methods focus on rewriting human-friendly queries, which may not
always yield optimal search results for the retriever. To overcome this
challenge, we introduce GuideCQR, a framework that utilizes guided documents to
refine queries, ensuring that they are optimal for retrievers. Specifically, we
augment keywords, generate expected answers from the re-ranked documents, and
unify them with the filtering process. Experimental results show that queries
enhanced by guided documents outperform previous CQR methods. Especially,
GuideCQR surpasses the performance of Large Language Model (LLM) prompt-powered
approaches and demonstrates the importance of the guided documents in
formulating retriever-friendly queries across diverse setups.","[{'name': 'Jeonghyun Park'}, {'name': 'Hwanhee Lee'}]",2024-07-17T07:39:16Z
http://arxiv.org/abs/2407.12358v1,http://arxiv.org/abs/2407.12358v1,"ProcTag: Process Tagging for Assessing the Efficacy of Document
  Instruction Data","Recently, large language models (LLMs) and multimodal large language models
(MLLMs) have demonstrated promising results on document visual question
answering (VQA) task, particularly after training on document instruction
datasets. An effective evaluation method for document instruction data is
crucial in constructing instruction data with high efficacy, which, in turn,
facilitates the training of LLMs and MLLMs for document VQA. However, most
existing evaluation methods for instruction data are limited to the textual
content of the instructions themselves, thereby hindering the effective
assessment of document instruction datasets and constraining their
construction. In this paper, we propose ProcTag, a data-oriented method that
assesses the efficacy of document instruction data. ProcTag innovatively
performs tagging on the execution process of instructions rather than the
instruction text itself. By leveraging the diversity and complexity of these
tags to assess the efficacy of the given dataset, ProcTag enables selective
sampling or filtering of document instructions. Furthermore, DocLayPrompt, a
novel semi-structured layout-aware document prompting strategy, is proposed for
effectively representing documents. Experiments demonstrate that sampling
existing open-sourced and generated document VQA/instruction datasets with
ProcTag significantly outperforms current methods for evaluating instruction
data. Impressively, with ProcTag-based sampling in the generated document
datasets, only 30.5\% of the document instructions are required to achieve
100\% efficacy compared to the complete dataset. The code is publicly available
at
https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/ProcTag.","[{'name': 'Yufan Shen'}, {'name': 'Chuwei Luo'}, {'name': 'Zhaoqing Zhu'}, {'name': 'Yang Chen'}, {'name': 'Qi Zheng'}, {'name': 'Zhi Yu'}, {'name': 'Jiajun Bu'}, {'name': 'Cong Yao'}]",2024-07-17T07:29:59Z
http://arxiv.org/abs/2407.12344v1,http://arxiv.org/abs/2407.12344v1,"The Better Angels of Machine Personality: How Personality Relates to LLM
  Safety","Personality psychologists have analyzed the relationship between personality
and safety behaviors in human society. Although Large Language Models (LLMs)
demonstrate personality traits, the relationship between personality traits and
safety abilities in LLMs still remains a mystery. In this paper, we discover
that LLMs' personality traits are closely related to their safety abilities,
i.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.
Meanwhile, the safety alignment generally increases various LLMs' Extraversion,
Sensing, and Judging traits. According to such findings, we can edit LLMs'
personality traits and improve their safety performance, e.g., inducing
personality from ISTJ to ISTP resulted in a relative improvement of
approximately 43% and 10% in privacy and fairness performance, respectively.
Additionally, we find that LLMs with different personality traits are
differentially susceptible to jailbreak. This study pioneers the investigation
of LLM safety from a personality perspective, providing new insights into LLM
safety enhancement.","[{'name': 'Jie Zhang'}, {'name': 'Dongrui Liu'}, {'name': 'Chen Qian'}, {'name': 'Ziyue Gan'}, {'name': 'Yong Liu'}, {'name': 'Yu Qiao'}, {'name': 'Jing Shao'}]",2024-07-17T06:36:29Z
http://arxiv.org/abs/2407.12342v1,http://arxiv.org/abs/2407.12342v1,"Word Embedding Dimension Reduction via Weakly-Supervised Feature
  Selection","As a fundamental task in natural language processing, word embedding converts
each word into a representation in a vector space. A challenge with word
embedding is that as the vocabulary grows, the vector space's dimension
increases and it can lead to a vast model size. Storing and processing word
vectors are resource-demanding, especially for mobile edge-devices
applications. This paper explores word embedding dimension reduction. To
balance computational costs and performance, we propose an efficient and
effective weakly-supervised feature selection method, named WordFS. It has two
variants, each utilizing novel criteria for feature selection. Experiments
conducted on various tasks (e.g., word and sentence similarity and binary and
multi-class classification) indicate that the proposed WordFS model outperforms
other dimension reduction methods at lower computational costs.","[{'name': 'Jintang Xue'}, {'name': 'Yun-Cheng Wang'}, {'name': 'Chengwei Wei'}, {'name': 'C. -C. Jay Kuo'}]",2024-07-17T06:36:09Z
http://arxiv.org/abs/2407.12336v1,http://arxiv.org/abs/2407.12336v1,M2DS: Multilingual Dataset for Multi-document Summarisation,"In the rapidly evolving digital era, there is an increasing demand for
concise information as individuals seek to distil key insights from various
sources. Recent attention from researchers on Multi-document Summarisation
(MDS) has resulted in diverse datasets covering customer reviews, academic
papers, medical and legal documents, and news articles. However, the
English-centric nature of these datasets has created a conspicuous void for
multilingual datasets in today's globalised digital landscape, where linguistic
diversity is celebrated. Media platforms such as British Broadcasting
Corporation (BBC) have disseminated news in 20+ languages for decades. With
only 380 million people speaking English natively as their first language,
accounting for less than 5% of the global population, the vast majority
primarily relies on other languages. These facts underscore the need for
inclusivity in MDS research, utilising resources from diverse languages.
Recognising this gap, we present the Multilingual Dataset for Multi-document
Summarisation (M2DS), which, to the best of our knowledge, is the first dataset
of its kind. It includes document-summary pairs in five languages from BBC
articles published during the 2010-2023 period. This paper introduces M2DS,
emphasising its unique multilingual aspect, and includes baseline scores from
state-of-the-art MDS models evaluated on our dataset.","[{'name': 'Kushan Hewapathirana'}, {'name': 'Nisansa de Silva'}, {'name': 'C. D. Athuraliya'}]",2024-07-17T06:25:51Z
http://arxiv.org/abs/2407.12327v1,http://arxiv.org/abs/2407.12327v1,"Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language
  Models","Post-training quantization is the leading method for addressing
memory-related bottlenecks in LLM inference, but unfortunately, it suffers from
significant performance degradation below 4-bit precision. An alternative
approach involves training compressed models directly at a low bitwidth (e.g.,
binary or ternary models). However, the performance, training dynamics, and
scaling trends of such models are not yet well understood. To address this
issue, we train and openly release the Spectra LLM suite consisting of 54
language models ranging from 99M to 3.9B parameters, trained on 300B tokens.
Spectra includes FloatLMs, post-training quantized QuantLMs (3, 4, 6, and 8
bits), and ternary LLMs (TriLMs) - our improved architecture for ternary
language modeling, which significantly outperforms previously proposed ternary
models of a given size (in bits), matching half-precision models at scale. For
example, TriLM 3.9B is (bit-wise) smaller than the half-precision FloatLM 830M,
but matches half-precision FloatLM 3.9B in commonsense reasoning and knowledge
benchmarks. However, TriLM 3.9B is also as toxic and stereotyping as FloatLM
3.9B, a model six times larger in size. Additionally, TriLM 3.9B lags behind
FloatLM in perplexity on validation splits and web-based corpora but performs
better on less noisy datasets like Lambada and PennTreeBank.
  To enhance understanding of low-bitwidth models, we are releasing 500+
intermediate checkpoints of the Spectra suite at
\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.","[{'name': 'Ayush Kaushal'}, {'name': 'Tejas Pandey'}, {'name': 'Tejas Vaidhya'}, {'name': 'Aaryan Bhagat'}, {'name': 'Irina Rish'}]",2024-07-17T05:53:20Z
http://arxiv.org/abs/2407.21033v1,http://arxiv.org/abs/2407.21033v1,"Multi-Grained Query-Guided Set Prediction Network for Grounded
  Multimodal Named Entity Recognition","Grounded Multimodal Named Entity Recognition (GMNER) is an emerging
information extraction (IE) task, aiming to simultaneously extract entity
spans, types, and entity-matched bounding box groundings in images from given
sentence-image pairs data. Recent unified methods employing machine reading
comprehension (MRC-based) frameworks or sequence generation-based models face
challenges in understanding the relationships of multimodal entities. MRC-based
frameworks, utilizing human-designed queries, struggle to model intra-entity
connections. Meanwhile, sequence generation-based outputs excessively rely on
inter-entity dependencies due to pre-defined decoding order. To tackle these,
we propose a novel unified framework named Multi-grained Query-guided Set
Prediction Network (MQSPN) to learn appropriate relationships at intra-entity
and inter-entity levels. Specifically, MQSPN consists of a Multi-grained Query
Set (MQS) and a Multimodal Set Prediction Network (MSP). MQS combines specific
type-grained and learnable entity-grained queries to adaptively strengthen
intra-entity connections by explicitly aligning visual regions with textual
spans. Based on solid intra-entity modeling, MSP reformulates GMNER as a set
prediction, enabling the parallel prediction of multimodal entities in a
non-autoregressive manner, eliminating redundant dependencies from preceding
sequences, and guiding models to establish appropriate inter-entity
relationships from a global matching perspective. Additionally, to boost better
alignment of two-level relationships, we also incorporate a Query-guided Fusion
Net (QFNet) to work as a glue network between MQS and MSP. Extensive
experiments demonstrate that our approach achieves state-of-the-art
performances in widely used benchmarks. Notably, our method improves 2.83% F1
in the difficult fine-grained GMNER benchmark.","[{'name': 'Jielong Tang'}, {'name': 'Zhenxing Wang'}, {'name': 'Ziyang Gong'}, {'name': 'Jianxing Yu'}, {'name': 'Shuang Wang'}, {'name': 'Jian Yin'}]",2024-07-17T05:42:43Z
http://arxiv.org/abs/2407.12309v1,http://arxiv.org/abs/2407.12309v1,"MEDFuse: Multimodal EHR Data Fusion with Masked Lab-Test Modeling and
  Large Language Models","Electronic health records (EHRs) are multimodal by nature, consisting of
structured tabular features like lab tests and unstructured clinical notes. In
real-life clinical practice, doctors use complementary multimodal EHR data
sources to get a clearer picture of patients' health and support clinical
decision-making. However, most EHR predictive models do not reflect these
procedures, as they either focus on a single modality or overlook the
inter-modality interactions/redundancy. In this work, we propose MEDFuse, a
Multimodal EHR Data Fusion framework that incorporates masked lab-test modeling
and large language models (LLMs) to effectively integrate structured and
unstructured medical data. MEDFuse leverages multimodal embeddings extracted
from two sources: LLMs fine-tuned on free clinical text and masked tabular
transformers trained on structured lab test results. We design a disentangled
transformer module, optimized by a mutual information loss to 1) decouple
modality-specific and modality-shared information and 2) extract useful joint
representation from the noise and redundancy present in clinical notes. Through
comprehensive validation on the public MIMIC-III dataset and the in-house FEMH
dataset, MEDFuse demonstrates great potential in advancing clinical
predictions, achieving over 90% F1 score in the 10-disease multi-label
classification task.","[{'name': 'Thao Minh Nguyen Phan'}, {'name': 'Cong-Tinh Dao'}, {'name': 'Chenwei Wu'}, {'name': 'Jian-Zhe Wang'}, {'name': 'Shun Liu'}, {'name': 'Jun-En Ding'}, {'name': 'David Restrepo'}, {'name': 'Feng Liu'}, {'name': 'Fang-Ming Hung'}, {'name': 'Wen-Chih Peng'}]",2024-07-17T04:17:09Z
http://arxiv.org/abs/2407.12277v1,http://arxiv.org/abs/2407.12277v1,Multimodal Reranking for Knowledge-Intensive Visual Question Answering,"Knowledge-intensive visual question answering requires models to effectively
use external knowledge to help answer visual questions. A typical pipeline
includes a knowledge retriever and an answer generator. However, a retriever
that utilizes local information, such as an image patch, may not provide
reliable question-candidate relevance scores. Besides, the two-tower
architecture also limits the relevance score modeling of a retriever to select
top candidates for answer generator reasoning. In this paper, we introduce an
additional module, a multi-modal reranker, to improve the ranking quality of
knowledge candidates for answer generation. Our reranking module takes
multi-modal information from both candidates and questions and performs
cross-item interaction for better relevance score modeling. Experiments on
OK-VQA and A-OKVQA show that multi-modal reranker from distant supervision
provides consistent improvements. We also find a training-testing discrepancy
with reranking in answer generation, where performance improves if training
knowledge candidates are similar to or noisier than those used in testing.","[{'name': 'Haoyang Wen'}, {'name': 'Honglei Zhuang'}, {'name': 'Hamed Zamani'}, {'name': 'Alexander Hauptmann'}, {'name': 'Michael Bendersky'}]",2024-07-17T02:58:52Z
http://arxiv.org/abs/2407.12259v1,http://arxiv.org/abs/2407.12259v1,In-Context Probing Approximates Influence Function for Data Valuation,"Data valuation quantifies the value of training data, and is used for data
attribution (i.e., determining the contribution of training data towards model
predictions), and data selection; both of which are important for curating
high-quality datasets to train large language models. In our paper, we show
that data valuation through in-context probing (i.e., prompting a LLM)
approximates influence functions for selecting training data. We provide a
theoretical sketch on this connection based on transformer models performing
""implicit"" gradient descent on its in-context inputs. Our empirical findings
show that in-context probing and gradient-based influence frameworks are
similar in how they rank training data. Furthermore, fine-tuning experiments on
data selected by either method reveal similar model performance.","[{'name': 'Cathy Jiao'}, {'name': 'Gary Gao'}, {'name': 'Chenyan Xiong'}]",2024-07-17T02:06:56Z
http://arxiv.org/abs/2407.12247v1,http://arxiv.org/abs/2407.12247v1,"Lacuna Language Learning: Leveraging RNNs for Ranked Text Completion in
  Digitized Coptic Manuscripts","Ancient manuscripts are frequently damaged, containing gaps in the text known
as lacunae. In this paper, we present a bidirectional RNN model for character
prediction of Coptic characters in manuscript lacunae. Our best model performs
with 72% accuracy on single character reconstruction, but falls to 37% when
reconstructing lacunae of various lengths. While not suitable for definitive
manuscript reconstruction, we argue that our RNN model can help scholars rank
the likelihood of textual reconstructions. As evidence, we use our RNN model to
rank reconstructions in two early Coptic manuscripts. Our investigation shows
that neural models can augment traditional methods of textual restoration,
providing scholars with an additional tool to assess lacunae in Coptic
manuscripts.","[{'name': 'Lauren Levine'}, {'name': 'Cindy Tung Li'}, {'name': 'Lydia Bremer-McCollum'}, {'name': 'Nicholas Wagner'}, {'name': 'Amir Zeldes'}]",2024-07-17T01:28:12Z
http://arxiv.org/abs/2407.12220v1,http://arxiv.org/abs/2407.12220v1,Questionable practices in machine learning,"Evaluating modern ML models is hard. The strong incentive for researchers and
companies to report a state-of-the-art result on some metric often leads to
questionable research practices (QRPs): bad practices which fall short of
outright research fraud. We describe 43 such practices which can undermine
reported results, giving examples where possible. Our list emphasises the
evaluation of large language models (LLMs) on public benchmarks. We also
discuss ""irreproducible research practices"", i.e. decisions that make it
difficult or impossible for other researchers to reproduce, build on or audit
previous research.","[{'name': 'Gavin Leech'}, {'name': 'Juan J. Vazquez'}, {'name': 'Misha Yagudin'}, {'name': 'Niclas Kupper'}, {'name': 'Laurence Aitchison'}]",2024-07-17T00:06:30Z
http://arxiv.org/abs/2407.12886v1,http://arxiv.org/abs/2407.12886v1,Whitening Not Recommended for Classification Tasks in LLMs,"Sentence embedding is a cornerstone in NLP. Whitening has been claimed to be
an effective operation to improve embedding quality obtained from Large
Language Models (LLMs). However, we find that the efficacy of whitening is
model-dependent and task-dependent. In particular, whitening degenerates
embeddings for classification tasks. The conclusion is supported by extensive
experiments. We also explored a variety of whitening operations, including PCA,
ZCA, PCA-Cor, ZCA-Cor and Cholesky whitenings. A by-product of our research is
embedding evaluation platform for LLMs called SentEval+.","[{'name': 'Ali Forooghi'}, {'name': 'Shaghayegh Sadeghi'}, {'name': 'Jianguo Lu'}]",2024-07-16T22:48:30Z
http://arxiv.org/abs/2407.12206v1,http://arxiv.org/abs/2407.12206v1,A Language Modeling Approach to Diacritic-Free Hebrew TTS,"We tackle the task of text-to-speech (TTS) in Hebrew. Traditional Hebrew
contains Diacritics, which dictate the way individuals should pronounce given
words, however, modern Hebrew rarely uses them. The lack of diacritics in
modern Hebrew results in readers expected to conclude the correct pronunciation
and understand which phonemes to use based on the context. This imposes a
fundamental challenge on TTS systems to accurately map between text-to-speech.
In this work, we propose to adopt a language modeling Diacritics-Free approach,
for the task of Hebrew TTS. The model operates on discrete speech
representations and is conditioned on a word-piece tokenizer. We optimize the
proposed method using in-the-wild weakly supervised data and compare it to
several diacritic-based TTS systems. Results suggest the proposed method is
superior to the evaluated baselines considering both content preservation and
naturalness of the generated speech. Samples can be found under the following
link: pages.cs.huji.ac.il/adiyoss-lab/HebTTS/","[{'name': 'Amit Roth'}, {'name': 'Arnon Turetzky'}, {'name': 'Yossi Adi'}]",2024-07-16T22:43:49Z
http://arxiv.org/abs/2407.12196v1,http://arxiv.org/abs/2407.12196v1,MASIVE: Open-Ended Affective State Identification in English and Spanish,"In the field of emotion analysis, much NLP research focuses on identifying a
limited number of discrete emotion categories, often applied across languages.
These basic sets, however, are rarely designed with textual data in mind, and
culture, language, and dialect can influence how particular emotions are
interpreted. In this work, we broaden our scope to a practically unbounded set
of \textit{affective states}, which includes any terms that humans use to
describe their experiences of feeling. We collect and publish MASIVE, a dataset
of Reddit posts in English and Spanish containing over 1,000 unique affective
states each. We then define the new problem of \textit{affective state
identification} for language generation models framed as a masked span
prediction task. On this task, we find that smaller finetuned multilingual
models outperform much larger LLMs, even on region-specific Spanish affective
states. Additionally, we show that pretraining on MASIVE improves model
performance on existing emotion benchmarks. Finally, through machine
translation experiments, we find that native speaker-written data is vital to
good performance on this task.","[{'name': 'Nicholas Deas'}, {'name': 'Elsbeth Turcan'}, {'name': 'Iván Pérez Mejía'}, {'name': 'Kathleen McKeown'}]",2024-07-16T21:43:47Z
http://arxiv.org/abs/2407.12176v1,http://arxiv.org/abs/2407.12176v1,GPT-4V Cannot Generate Radiology Reports Yet,"GPT-4V's purported strong multimodal abilities raise interests in using it to
automate radiology report writing, but there lacks thorough evaluations. In
this work, we perform a systematic evaluation of GPT-4V in generating radiology
reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt
to directly generate reports using GPT-4V through different prompting
strategies and find that it fails terribly in both lexical metrics and clinical
efficacy metrics. To understand the low performance, we decompose the task into
two steps: 1) the medical image reasoning step of predicting medical condition
labels from images; and 2) the report synthesis step of generating reports from
(groundtruth) conditions. We show that GPT-4V's performance in image reasoning
is consistently low across different prompts. In fact, the distributions of
model-predicted labels remain constant regardless of which groundtruth
conditions are present on the image, suggesting that the model is not
interpreting chest X-rays meaningfully. Even when given groundtruth conditions
in report synthesis, its generated reports are less correct and less
natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt
on the viability of using GPT-4V in a radiology workflow.","[{'name': 'Yuyang Jiang'}, {'name': 'Chacha Chen'}, {'name': 'Dang Nguyen'}, {'name': 'Benjamin M. Mervak'}, {'name': 'Chenhao Tan'}]",2024-07-16T21:03:14Z
http://arxiv.org/abs/2407.13796v1,http://arxiv.org/abs/2407.13796v1,"Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large
  Language Models","Security concerns for large language models (LLMs) have recently escalated,
focusing on thwarting jailbreaking attempts in discrete prompts. However, the
exploration of jailbreak vulnerabilities arising from continuous embeddings has
been limited, as prior approaches primarily involved appending discrete or
continuous suffixes to inputs. Our study presents a novel channel for
conducting direct attacks on LLM inputs, eliminating the need for suffix
addition or specific questions provided that the desired output is predefined.
We additionally observe that extensive iterations often lead to overfitting,
characterized by repetition in the output. To counteract this, we propose a
simple yet effective strategy named CLIP. Our experiments show that for an
input length of 40 at iteration 1000, applying CLIP improves the ASR from 62%
to 83%","[{'name': 'Zihao Xu'}, {'name': 'Yi Liu'}, {'name': 'Gelei Deng'}, {'name': 'Kailong Wang'}, {'name': 'Yuekang Li'}, {'name': 'Ling Shi'}, {'name': 'Stjepan Picek'}]",2024-07-16T20:53:00Z
http://arxiv.org/abs/2407.12141v1,http://arxiv.org/abs/2407.12141v1,"Predicting Emotion Intensity in Polish Political Texts: Comparing
  Supervised Models and Large Language Models in a Resource-Poor Language","This study explores the use of large language models (LLMs) to predict
emotion intensity in Polish political texts, a resource-poor language context.
The research compares the performance of several LLMs against a supervised
model trained on an annotated corpus of 10,000 social media texts, evaluated
for the intensity of emotions by expert judges. The findings indicate that
while the supervised model generally outperforms LLMs, offering higher accuracy
and lower variance, LLMs present a viable alternative, especially given the
high costs associated with data annotation. The study highlights the potential
of LLMs in low-resource language settings and underscores the need for further
research on emotion intensity prediction and its application across different
languages and continuous features. The implications suggest a nuanced
decision-making process to choose the right approach to emotion prediction for
researchers and practitioners based on resource availability and the specific
requirements of their tasks.","[{'name': 'Hubert Plisiecki'}, {'name': 'Piotr Koc'}, {'name': 'Maria Flakus'}, {'name': 'Artur Pokropek'}]",2024-07-16T19:53:14Z
http://arxiv.org/abs/2407.12126v2,http://arxiv.org/abs/2407.12126v2,"LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text
  Translation","Machine translation is indispensable in healthcare for enabling the global
dissemination of medical knowledge across languages. However, complex medical
terminology poses unique challenges to achieving adequate translation quality
and accuracy. This study introduces a novel ""LLMs-in-the-loop"" approach to
develop supervised neural machine translation models optimized specifically for
medical texts. While large language models (LLMs) have demonstrated powerful
capabilities, this research shows that small, specialized models trained on
high-quality in-domain (mostly synthetic) data can outperform even vastly
larger LLMs.
  Custom parallel corpora in six languages were compiled from scientific
articles, synthetically generated clinical documents, and medical texts. Our
LLMs-in-the-loop methodology employs synthetic data generation, rigorous
evaluation, and agent orchestration to enhance performance. We developed small
medical translation models using the MarianMT base model. We introduce a new
medical translation test dataset to standardize evaluation in this domain.
Assessed using BLEU, METEOR, ROUGE, and BERT scores on this test set, our
MarianMT-based models outperform Google Translate, DeepL, and GPT-4-Turbo.
  Results demonstrate that our LLMs-in-the-loop approach, combined with
fine-tuning high-quality, domain-specific data, enables specialized models to
outperform general-purpose and some larger systems. This research, part of a
broader series on expert small models, paves the way for future
healthcare-related AI developments, including deidentification and bio-medical
entity extraction models. Our study underscores the potential of tailored
neural translation models and the LLMs-in-the-loop methodology to advance the
field through improved data generation, evaluation, agent, and modeling
techniques.","[{'name': 'Bunyamin Keles'}, {'name': 'Murat Gunay'}, {'name': 'Serdar I. Caglar'}]",2024-07-16T19:32:23Z
http://arxiv.org/abs/2407.12108v1,http://arxiv.org/abs/2407.12108v1,Private prediction for large-scale synthetic text generation,"We present an approach for generating differentially private synthetic text
using large language models (LLMs), via private prediction. In the private
prediction framework, we only require the output synthetic data to satisfy
differential privacy guarantees. This is in contrast to approaches that train a
generative model on potentially sensitive user-supplied source data and seek to
ensure the model itself is safe to release.
  We prompt a pretrained LLM with source data, but ensure that next-token
predictions are made with differential privacy guarantees. Previous work in
this paradigm reported generating a small number of examples (<10) at
reasonable privacy levels, an amount of data that is useful only for downstream
in-context learning or prompting. In contrast, we make changes that allow us to
generate thousands of high-quality synthetic data points, greatly expanding the
set of potential applications. Our improvements come from an improved privacy
analysis and a better private selection mechanism, which makes use of the
equivalence between the softmax layer for sampling tokens in LLMs and the
exponential mechanism. Furthermore, we introduce a novel use of public
predictions via the sparse vector technique, in which we do not pay privacy
costs for tokens that are predictable without sensitive data; we find this to
be particularly effective for structured data.","[{'name': 'Kareem Amin'}, {'name': 'Alex Bie'}, {'name': 'Weiwei Kong'}, {'name': 'Alexey Kurakin'}, {'name': 'Natalia Ponomareva'}, {'name': 'Umar Syed'}, {'name': 'Andreas Terzis'}, {'name': 'Sergei Vassilvitskii'}]",2024-07-16T18:28:40Z
http://arxiv.org/abs/2407.12101v1,http://arxiv.org/abs/2407.12101v1,Better RAG using Relevant Information Gain,"A common way to extend the memory of large language models (LLMs) is by
retrieval augmented generation (RAG), which inserts text retrieved from a
larger memory into an LLM's context window. However, the context window is
typically limited to several thousand tokens, which limits the number of
retrieved passages that can inform a model's response. For this reason, it's
important to avoid occupying context window space with redundant information by
ensuring a degree of diversity among retrieved passages. At the same time, the
information should also be relevant to the current task. Most prior methods
that encourage diversity among retrieved results, such as Maximal Marginal
Relevance (MMR), do so by incorporating an objective that explicitly trades off
diversity and relevance. We propose a novel simple optimization metric based on
relevant information gain, a probabilistic measure of the total information
relevant to a query for a set of retrieved results. By optimizing this metric,
diversity organically emerges from our system. When used as a drop-in
replacement for the retrieval component of a RAG system, this method yields
state-of-the-art performance on question answering tasks from the Retrieval
Augmented Generation Benchmark (RGB), outperforming existing metrics that
directly optimize for relevance and diversity.","[{'name': 'Marc Pickett'}, {'name': 'Jeremy Hartman'}, {'name': 'Ayan Kumar Bhowmick'}, {'name': 'Raquib-ul Alam'}, {'name': 'Aditya Vempaty'}]",2024-07-16T18:09:21Z
http://arxiv.org/abs/2407.12094v1,http://arxiv.org/abs/2407.12094v1,"Identifying Speakers in Dialogue Transcripts: A Text-based Approach
  Using Pretrained Language Models","We introduce an approach to identifying speaker names in dialogue
transcripts, a crucial task for enhancing content accessibility and
searchability in digital media archives. Despite the advancements in speech
recognition, the task of text-based speaker identification (SpeakerID) has
received limited attention, lacking large-scale, diverse datasets for effective
model training. Addressing these gaps, we present a novel, large-scale dataset
derived from the MediaSum corpus, encompassing transcripts from a wide range of
media sources. We propose novel transformer-based models tailored for
SpeakerID, leveraging contextual cues within dialogues to accurately attribute
speaker names. Through extensive experiments, our best model achieves a great
precision of 80.3\%, setting a new benchmark for SpeakerID. The data and code
are publicly available here:
\url{https://github.com/adobe-research/speaker-identification}","[{'name': 'Minh Nguyen'}, {'name': 'Franck Dernoncourt'}, {'name': 'Seunghyun Yoon'}, {'name': 'Hanieh Deilamsalehy'}, {'name': 'Hao Tan'}, {'name': 'Ryan Rossi'}, {'name': 'Quan Hung Tran'}, {'name': 'Trung Bui'}, {'name': 'Thien Huu Nguyen'}]",2024-07-16T18:03:58Z
http://arxiv.org/abs/2407.12077v1,http://arxiv.org/abs/2407.12077v1,"GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill
  and Extreme KV-Cache Compression","We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model
that uses a new technique to efficiently generate a highly compressed and
reusable KV-Cache in linear time and space with respect to sequence length.
GoldFinch stacks our new GOLD transformer on top of an enhanced version of the
Finch (RWKV-6) architecture. We train up to 1.5B parameter class models of the
Finch, Llama, and GoldFinch architectures, and find dramatically improved
modeling performance relative to both Finch and Llama. Our cache size savings
increase linearly with model layer count, ranging from 756-2550 times smaller
than the traditional transformer cache for common sizes, enabling inference of
extremely large context lengths even on limited hardware. Although
autoregressive generation has O(n) time complexity per token because of
attention, pre-fill computation of the entire initial cache state for a
submitted context costs only O(1) time per token due to the use of a recurrent
neural network (RNN) to generate this cache. We release our trained weights and
training code under the Apache 2.0 license for community use.","[{'name': 'Daniel Goldstein'}, {'name': 'Fares Obeid'}, {'name': 'Eric Alcaide'}, {'name': 'Guangyu Song'}, {'name': 'Eugene Cheah'}]",2024-07-16T18:00:00Z
http://arxiv.org/abs/2407.11969v2,http://arxiv.org/abs/2407.11969v2,Does Refusal Training in LLMs Generalize to the Past Tense?,"Refusal training is widely used to prevent LLMs from generating harmful,
undesirable, or illegal outputs. We reveal a curious generalization gap in the
current refusal training approaches: simply reformulating a harmful request in
the past tense (e.g., ""How to make a Molotov cocktail?"" to ""How did people make
a Molotov cocktail?"") is often sufficient to jailbreak many state-of-the-art
LLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,
GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, and R2D2 models
using GPT-3.5 Turbo as a reformulation model. For example, the success rate of
this simple attack on GPT-4o increases from 1% using direct requests to 88%
using 20 past tense reformulation attempts on harmful requests from
JailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find
that reformulations in the future tense are less effective, suggesting that
refusal guardrails tend to consider past historical questions more benign than
hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5
Turbo show that defending against past reformulations is feasible when past
tense examples are explicitly included in the fine-tuning data. Overall, our
findings highlight that the widely used alignment techniques -- such as SFT,
RLHF, and adversarial training -- employed to align the studied models can be
brittle and do not always generalize as intended. We provide code and jailbreak
artifacts at https://github.com/tml-epfl/llm-past-tense.","[{'name': 'Maksym Andriushchenko'}, {'name': 'Nicolas Flammarion'}]",2024-07-16T17:59:55Z
http://arxiv.org/abs/2407.11963v1,http://arxiv.org/abs/2407.11963v1,"NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context
  Window?","In evaluating the long-context capabilities of large language models (LLMs),
identifying content relevant to a user's query from original long documents is
a crucial prerequisite for any LLM to answer questions based on long text. We
present NeedleBench, a framework consisting of a series of progressively more
challenging tasks for assessing bilingual long-context capabilities, spanning
multiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and
different depth ranges, allowing the strategic insertion of critical data
points in different text depth zones to rigorously test the retrieval and
reasoning capabilities of models in diverse contexts. We use the NeedleBench
framework to assess how well the leading open-source models can identify key
information relevant to the question and apply that information to reasoning in
bilingual long texts. Furthermore, we propose the Ancestral Trace Challenge
(ATC) to mimic the complexity of logical reasoning challenges that are likely
to be present in real-world long-context tasks, providing a simple method for
evaluating LLMs in dealing with complex long-context situations. Our results
suggest that current LLMs have significant room for improvement in practical
long-context applications, as they struggle with the complexity of logical
reasoning challenges that are likely to be present in real-world long-context
tasks. All codes and resources are available at OpenCompass:
https://github.com/open-compass/opencompass.","[{'name': 'Mo Li'}, {'name': 'Songyang Zhang'}, {'name': 'Yunxin Liu'}, {'name': 'Kai Chen'}]",2024-07-16T17:59:06Z
http://arxiv.org/abs/2407.12883v1,http://arxiv.org/abs/2407.12883v1,"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive
  Retrieval","Existing retrieval benchmarks primarily consist of information-seeking
queries (e.g., aggregated questions from search engines) where keyword or
semantic-based retrieval is usually sufficient. However, many complex
real-world queries require in-depth reasoning to identify relevant documents
that go beyond surface form matching. For example, finding documentation for a
coding question requires understanding the logic and syntax of the functions
involved. To better benchmark retrieval on such challenging queries, we
introduce BRIGHT, the first text retrieval benchmark that requires intensive
reasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398
real-world queries collected from diverse domains (such as economics,
psychology, robotics, software engineering, earth sciences, etc.), sourced from
naturally occurring or carefully curated human data. Extensive evaluation
reveals that even state-of-the-art retrieval models perform poorly on BRIGHT.
The leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0
nDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate
that augmenting queries with Chain-of-Thought reasoning generated by large
language models (LLMs) improves performance by up to 12.2 points. Moreover,
BRIGHT is robust against data leakage during pretraining of the benchmarked
models as we validate by showing similar performance even when documents from
the benchmark are included in the training data. We believe that BRIGHT paves
the way for future research on retrieval systems in more realistic and
challenging settings. Our code and data are available at
https://brightbenchmark.github.io.","[{'name': 'Hongjin Su'}, {'name': 'Howard Yen'}, {'name': 'Mengzhou Xia'}, {'name': 'Weijia Shi'}, {'name': 'Niklas Muennighoff'}, {'name': 'Han-yu Wang'}, {'name': 'Haisu Liu'}, {'name': 'Quan Shi'}, {'name': 'Zachary S. Siegel'}, {'name': 'Michael Tang'}, {'name': 'Ruoxi Sun'}, {'name': 'Jinsung Yoon'}, {'name': 'Sercan O. Arik'}, {'name': 'Danqi Chen'}, {'name': 'Tao Yu'}]",2024-07-16T17:58:27Z
http://arxiv.org/abs/2407.11948v1,http://arxiv.org/abs/2407.11948v1,"Rethinking Transformer-based Multi-document Summarization: An Empirical
  Investigation","The utilization of Transformer-based models prospers the growth of
multi-document summarization (MDS). Given the huge impact and widespread
adoption of Transformer-based models in various natural language processing
tasks, investigating their performance and behaviors in the context of MDS
becomes crucial for advancing the field and enhancing the quality of summary.
To thoroughly examine the behaviours of Transformer-based MDS models, this
paper presents five empirical studies on (1) measuring the impact of document
boundary separators quantitatively; (2) exploring the effectiveness of
different mainstream Transformer structures; (3) examining the sensitivity of
the encoder and decoder; (4) discussing different training strategies; and (5)
discovering the repetition in a summary generation. The experimental results on
prevalent MDS datasets and eleven evaluation metrics show the influence of
document boundary separators, the granularity of different level features and
different model training strategies. The results also reveal that the decoder
exhibits greater sensitivity to noises compared to the encoder. This
underscores the important role played by the decoder, suggesting a potential
direction for future research in MDS. Furthermore, the experimental results
indicate that the repetition problem in the generated summaries has
correlations with the high uncertainty scores.","[{'name': 'Congbo Ma'}, {'name': 'Wei Emma Zhang'}, {'name': 'Dileepa Pitawela'}, {'name': 'Haojie Zhuang'}, {'name': 'Yanfeng Shu'}]",2024-07-16T17:42:37Z
http://arxiv.org/abs/2407.11930v1,http://arxiv.org/abs/2407.11930v1,"Fine-grained Hallucination Detection and Mitigation in Long-form
  Question Answering","Long-form question answering (LFQA) aims to provide thorough and in-depth
answers to complex questions, enhancing comprehension. However, such detailed
responses are prone to hallucinations and factual inconsistencies, challenging
their faithful evaluation. This work introduces HaluQuestQA, the first
hallucination dataset with localized error annotations for human-written and
model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7k
span-level error annotations for five different error types by expert
annotators, along with preference judgments. Using our collected data, we
thoroughly analyze the shortcomings of long-form answers and find that they
lack comprehensiveness and provide unhelpful references. We train an automatic
feedback model on this dataset that predicts error spans with incomplete
information and provides associated explanations. Finally, we propose a
prompt-based approach, Error-informed refinement, that uses signals from the
learned feedback model to refine generated answers, which we show reduces
hallucination and improves answer quality. Furthermore, humans find answers
generated by our approach comprehensive and highly prefer them (84%) over the
baseline answers.","[{'name': 'Rachneet Sachdeva'}, {'name': 'Yixiao Song'}, {'name': 'Mohit Iyyer'}, {'name': 'Iryna Gurevych'}]",2024-07-16T17:23:16Z
http://arxiv.org/abs/2407.11919v1,http://arxiv.org/abs/2407.11919v1,What's Wrong? Refining Meeting Summaries with LLM Feedback,"Meeting summarization has become a critical task since digital encounters
have become a common practice. Large language models (LLMs) show great
potential in summarization, offering enhanced coherence and context
understanding compared to traditional methods. However, they still struggle to
maintain relevance and avoid hallucination. We introduce a multi-LLM correction
approach for meeting summarization using a two-phase process that mimics the
human review process: mistake identification and summary refinement. We release
QMSum Mistake, a dataset of 200 automatically generated meeting summaries
annotated by humans on nine error types, including structural, omission, and
irrelevance errors. Our experiments show that these errors can be identified
with high accuracy by an LLM. We transform identified mistakes into actionable
feedback to improve the quality of a given summary measured by relevance,
informativeness, conciseness, and coherence. This post-hoc refinement
effectively improves summary quality by leveraging multiple LLMs to validate
output quality. Our multi-LLM approach for meeting summarization shows
potential for similar complex text generation tasks requiring robustness,
action planning, and discussion towards a goal.","[{'name': 'Frederic Kirstein'}, {'name': 'Terry Ruas'}, {'name': 'Bela Gipp'}]",2024-07-16T17:10:16Z
http://arxiv.org/abs/2407.11857v1,http://arxiv.org/abs/2407.11857v1,"Evaluating Task-Oriented Dialogue Consistency through Constraint
  Satisfaction","Task-oriented dialogues must maintain consistency both within the dialogue
itself, ensuring logical coherence across turns, and with the conversational
domain, accurately reflecting external knowledge. We propose to conceptualize
dialogue consistency as a Constraint Satisfaction Problem (CSP), wherein
variables represent segments of the dialogue referencing the conversational
domain, and constraints among variables reflect dialogue properties, including
linguistic, conversational, and domain-based aspects. To demonstrate the
feasibility of the approach, we utilize a CSP solver to detect inconsistencies
in dialogues re-lexicalized by an LLM. Our findings indicate that: (i) CSP is
effective to detect dialogue inconsistencies; and (ii) consistent dialogue
re-lexicalization is challenging for state-of-the-art LLMs, achieving only a
0.15 accuracy rate when compared to a CSP solver. Furthermore, through an
ablation study, we reveal that constraints derived from domain knowledge pose
the greatest difficulty in being respected. We argue that CSP captures core
properties of dialogue consistency that have been poorly considered by
approaches based on component pipelines.","[{'name': 'Tiziano Labruna'}, {'name': 'Bernardo Magnini'}]",2024-07-16T15:38:41Z
http://arxiv.org/abs/2407.11855v1,http://arxiv.org/abs/2407.11855v1,Scaling Sign Language Translation,"Sign language translation (SLT) addresses the problem of translating
information from a sign language in video to a spoken language in text.
Existing studies, while showing progress, are often limited to narrow domains
and/or few sign languages and struggle with open-domain tasks. In this paper,
we push forward the frontier of SLT by scaling pretraining data, model size,
and number of translation directions. We perform large-scale SLT pretraining on
different data including 1) noisy multilingual YouTube SLT data, 2) parallel
text corpora, and 3) SLT data augmented by translating video captions to other
languages with off-the-shelf machine translation models. We unify different
pretraining tasks with task-specific prompts under the encoder-decoder
architecture, and initialize the SLT model with pretrained (m/By)T5 models
across model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL
to 42 spoken languages) demonstrate the significance of data/model scaling and
cross-lingual cross-modal transfer, as well as the feasibility of zero-shot
SLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT
benchmarks covering 5 sign languages. Experiments show substantial quality
improvements over the vanilla baselines, surpassing the previous
state-of-the-art (SOTA) by wide margins.","[{'name': 'Biao Zhang'}, {'name': 'Garrett Tanzer'}, {'name': 'Orhan Firat'}]",2024-07-16T15:36:58Z
http://arxiv.org/abs/2407.11833v1,http://arxiv.org/abs/2407.11833v1,LoFTI: Localization and Factuality Transfer to Indian Locales,"Large language models (LLMs) encode vast amounts of world knowledge acquired
via training on large web-scale datasets crawled from the internet. However,
these datasets typically exhibit a geographical bias towards English-speaking
Western countries. This results in LLMs producing biased or hallucinated
responses to queries that require answers localized to other geographical
regions. In this work, we introduce a new benchmark named LoFTI (Localization
and Factuality Transfer to Indian Locales) that can be used to evaluate an
LLM's localization and factual text transfer capabilities. LoFTI consists of
factual statements about entities in source and target locations; the source
locations are spread across the globe and the target locations are all within
India with varying degrees of hyperlocality (country, states, cities). The
entities span a wide variety of categories. We use LoFTI to evaluate Mixtral,
GPT-4 and two other Mixtral-based approaches well-suited to the task of
localized factual transfer. We demonstrate that LoFTI is a high-quality
evaluation benchmark and all the models, including GPT-4, produce skewed
results across varying levels of hyperlocality.","[{'name': 'Sona Elza Simon'}, {'name': 'Soumen Kumar Mondal'}, {'name': 'Abhishek Singhania'}, {'name': 'Sayambhu Sen'}, {'name': 'Preethi Jyothi'}]",2024-07-16T15:20:43Z
http://arxiv.org/abs/2407.11827v1,http://arxiv.org/abs/2407.11827v1,"GPT Assisted Annotation of Rhetorical and Linguistic Features for
  Interpretable Propaganda Technique Detection in News Text","While the use of machine learning for the detection of propaganda techniques
in text has garnered considerable attention, most approaches focus on
""black-box"" solutions with opaque inner workings. Interpretable approaches
provide a solution, however, they depend on careful feature engineering and
costly expert annotated data. Additionally, language features specific to
propagandistic text are generally the focus of rhetoricians or linguists, and
there is no data set labeled with such features suitable for machine learning.
This study codifies 22 rhetorical and linguistic features identified in
literature related to the language of persuasion for the purpose of annotating
an existing data set labeled with propaganda techniques. To help human experts
annotate natural language sentences with these features, RhetAnn, a web
application, was specifically designed to minimize an otherwise considerable
mental effort. Finally, a small set of annotated data was used to fine-tune
GPT-3.5, a generative large language model (LLM), to annotate the remaining
data while optimizing for financial cost and classification accuracy. This
study demonstrates how combining a small number of human annotated examples
with GPT can be an effective strategy for scaling the annotation process at a
fraction of the cost of traditional annotation relying solely on human experts.
The results are on par with the best performing model at the time of writing,
namely GPT-4, at 10x less the cost. Our contribution is a set of features,
their properties, definitions, and examples in a machine-readable format, along
with the code for RhetAnn and the GPT prompts and fine-tuning procedures for
advancing state-of-the-art interpretable propaganda technique detection.","[{'name': 'Kyle Hamilton'}, {'name': 'Luca Longo'}, {'name': 'Bojan Bozic'}]",2024-07-16T15:15:39Z
http://arxiv.org/abs/2407.12881v1,http://arxiv.org/abs/2407.12881v1,BinaryAlign: Word Alignment as Binary Sequence Labeling,"Real world deployments of word alignment are almost certain to cover both
high and low resource languages. However, the state-of-the-art for this task
recommends a different model class depending on the availability of gold
alignment training data for a particular language pair. We propose BinaryAlign,
a novel word alignment technique based on binary sequence labeling that
outperforms existing approaches in both scenarios, offering a unifying approach
to the task. Additionally, we vary the specific choice of multilingual
foundation model, perform stratified error analysis over alignment error type,
and explore the performance of BinaryAlign on non-English language pairs. We
make our source code publicly available.","[{'name': 'Gaetan Lopez Latouche'}, {'name': 'Marc-André Carbonneau'}, {'name': 'Ben Swanson'}]",2024-07-16T15:11:06Z
http://arxiv.org/abs/2407.11798v1,http://arxiv.org/abs/2407.11798v1,"PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined
  Speculation","Inference of Large Language Models (LLMs) across computer clusters has become
a focal point of research in recent times, with many acceleration techniques
taking inspiration from CPU speculative execution. These techniques reduce
bottlenecks associated with memory bandwidth, but also increase end-to-end
latency per inference run, requiring high speculation acceptance rates to
improve performance. Combined with a variable rate of acceptance across tasks,
speculative inference techniques can result in reduced performance.
Additionally, pipeline-parallel designs require many user requests to maintain
maximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative
acceleration technique to reduce inter-token latency and improve system
utilization for single-request scenarios while also improving tolerance to low
speculation acceptance rates and low-bandwidth interconnects. PipeInfer
exhibits up to a 2.15$\times$ improvement in generation speed over standard
speculative inference. PipeInfer achieves its improvement through Continuous
Asynchronous Speculation and Early Inference Cancellation, the former improving
latency and generation speed by running single-token inference simultaneously
with several speculative runs, while the latter improves speed and latency by
skipping the computation of invalidated runs, even in the middle of inference.","[{'name': 'Branden Butler'}, {'name': 'Sixing Yu'}, {'name': 'Arya Mazaheri'}, {'name': 'Ali Jannesari'}]",2024-07-16T14:52:02Z
http://arxiv.org/abs/2407.11789v1,http://arxiv.org/abs/2407.11789v1,Large Language Models as Misleading Assistants in Conversation,"Large Language Models (LLMs) are able to provide assistance on a wide range
of information-seeking tasks. However, model outputs may be misleading, whether
unintentionally or in cases of intentional deception. We investigate the
ability of LLMs to be deceptive in the context of providing assistance on a
reading comprehension task, using LLMs as proxies for human users. We compare
outcomes of (1) when the model is prompted to provide truthful assistance, (2)
when it is prompted to be subtly misleading, and (3) when it is prompted to
argue for an incorrect answer. Our experiments show that GPT-4 can effectively
mislead both GPT-3.5-Turbo and GPT-4, with deceptive assistants resulting in up
to a 23% drop in accuracy on the task compared to when a truthful assistant is
used. We also find that providing the user model with additional context from
the passage partially mitigates the influence of the deceptive model. This work
highlights the ability of LLMs to produce misleading information and the
effects this may have in real-world situations.","[{'name': 'Betty Li Hou'}, {'name': 'Kejian Shi'}, {'name': 'Jason Phang'}, {'name': 'James Aung'}, {'name': 'Steven Adler'}, {'name': 'Rosie Campbell'}]",2024-07-16T14:45:22Z
http://arxiv.org/abs/2407.11780v1,http://arxiv.org/abs/2407.11780v1,"SwitchCIT: Switching for Continual Instruction Tuning of Large Language
  Models","Large language models (LLMs) have exhibited impressive capabilities in
various domains, particularly in general language understanding. However these
models, trained on massive text data, may not be finely optimized for specific
tasks triggered by instructions. Continual instruction tuning is crucial to
adapt LLMs to evolving tasks and domains, ensuring their effectiveness and
relevance across a wide range of applications. In the context of continual
instruction tuning, where models are sequentially trained on different tasks,
catastrophic forgetting can occur, leading to performance degradation on
previously learned tasks. This work addresses the catastrophic forgetting in
continual instruction learning for LLMs through a switching mechanism for
routing computations to parameter-efficient tuned models. We demonstrate the
effectiveness of our method through experiments on continual instruction tuning
of different natural language generation tasks.","[{'name': 'Xinbo Wu'}, {'name': 'Max Hartman'}, {'name': 'Vidhata Arjun Jayaraman'}, {'name': 'Lav R. Varshney'}]",2024-07-16T14:37:33Z
http://arxiv.org/abs/2407.11774v1,http://arxiv.org/abs/2407.11774v1,"Sharif-MGTD at SemEval-2024 Task 8: A Transformer-Based Approach to
  Detect Machine Generated Text","Detecting Machine-Generated Text (MGT) has emerged as a significant area of
study within Natural Language Processing. While language models generate text,
they often leave discernible traces, which can be scrutinized using either
traditional feature-based methods or more advanced neural language models. In
this research, we explore the effectiveness of fine-tuning a RoBERTa-base
transformer, a powerful neural architecture, to address MGT detection as a
binary classification task. Focusing specifically on Subtask A
(Monolingual-English) within the SemEval-2024 competition framework, our
proposed system achieves an accuracy of 78.9% on the test dataset, positioning
us at 57th among participants. Our study addresses this challenge while
considering the limited hardware resources, resulting in a system that excels
at identifying human-written texts but encounters challenges in accurately
discerning MGTs.","[{'name': 'Seyedeh Fatemeh Ebrahimi'}, {'name': 'Karim Akhavan Azari'}, {'name': 'Amirmasoud Iravani'}, {'name': 'Arian Qazvini'}, {'name': 'Pouya Sadeghi'}, {'name': 'Zeinab Sadat Taghavi'}, {'name': 'Hossein Sameti'}]",2024-07-16T14:33:01Z
http://arxiv.org/abs/2407.11773v1,http://arxiv.org/abs/2407.11773v1,"Educational Personalized Learning Path Planning with Large Language
  Models","Educational Personalized Learning Path Planning (PLPP) aims to tailor
learning experiences to individual learners' needs, enhancing learning
efficiency and engagement. Despite its potential, traditional PLPP systems
often lack adaptability, interactivity, and transparency. This paper proposes a
novel approach integrating Large Language Models (LLMs) with prompt engineering
to address these challenges. By designing prompts that incorporate
learner-specific information, our method guides LLMs like LLama-2-70B and GPT-4
to generate personalized, coherent, and pedagogically sound learning paths. We
conducted experiments comparing our method with a baseline approach across
various metrics, including accuracy, user satisfaction, and the quality of
learning paths. The results show significant improvements in all areas,
particularly with GPT-4, demonstrating the effectiveness of prompt engineering
in enhancing PLPP. Additional long-term impact analysis further validates our
method's potential to improve learner performance and retention. This research
highlights the promise of LLMs and prompt engineering in advancing personalized
education.","[{'name': 'Chee Ng'}, {'name': 'Yuen Fung'}]",2024-07-16T14:32:56Z
http://arxiv.org/abs/2407.11770v1,http://arxiv.org/abs/2407.11770v1,"Robust Utility-Preserving Text Anonymization Based on Large Language
  Models","Text anonymization is crucial for sharing sensitive data while maintaining
privacy. Existing techniques face the emerging challenges of re-identification
attack ability of Large Language Models (LLMs), which have shown advanced
capability in memorizing detailed information and patterns as well as
connecting disparate pieces of information. In defending against LLM-based
re-identification attacks, anonymization could jeopardize the utility of the
resulting anonymized data in downstream tasks -- the trade-off between privacy
and data utility requires deeper understanding within the context of LLMs. This
paper proposes a framework composed of three LLM-based components -- a privacy
evaluator, a utility evaluator, and an optimization component, which work
collaboratively to perform anonymization. To provide a practical model for
large-scale and real-time environments, we distill the anonymization
capabilities into a lightweight model using Direct Preference Optimization
(DPO). Extensive experiments demonstrate that the proposed models outperform
baseline models, showing robustness in reducing the risk of re-identification
while preserving greater data utility in downstream tasks. Our code and dataset
are available at https://github.com/UKPLab/arxiv2024-rupta.","[{'name': 'Tianyu Yang'}, {'name': 'Xiaodan Zhu'}, {'name': 'Iryna Gurevych'}]",2024-07-16T14:28:56Z
http://arxiv.org/abs/2407.11766v1,http://arxiv.org/abs/2407.11766v1,Vectoring Languages,"Recent breakthroughs in large language models (LLM) have stirred up global
attention, and the research has been accelerating non-stop since then.
Philosophers and psychologists have also been researching the structure of
language for decades, but they are having a hard time finding a theory that
directly benefits from the breakthroughs of LLMs. In this article, we propose a
novel structure of language that reflects well on the mechanisms behind
language models and go on to show that this structure is also better at
capturing the diverse nature of language compared to previous methods. An
analogy of linear algebra is adapted to strengthen the basis of this
perspective. We further argue about the difference between this perspective and
the design philosophy for current language models. Lastly, we discuss how this
perspective can lead us to research directions that may accelerate the
improvements of science fastest.",[{'name': 'Joseph Chen'}],2024-07-16T14:25:55Z
http://arxiv.org/abs/2407.11733v2,http://arxiv.org/abs/2407.11733v2,"How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine
  Studies","With the widespread availability of LLMs since the release of ChatGPT and
increased public scrutiny, commercial model development appears to have focused
their efforts on 'safety' training concerning legal liabilities at the expense
of social impact evaluation. This mimics a similar trend which we could observe
for search engine autocompletion some years prior. We draw on scholarship from
NLP and search engine auditing and present a novel evaluation task in the style
of autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by
using four metrics, namely refusal rates, toxicity, sentiment and regard, with
and without safety system prompts. Our findings indicate an improvement to
stereotyping outputs with the system prompt, but overall a lack of attention by
LLMs under study to certain harms classified as toxic, particularly for prompts
about peoples/ethnicities and sexual orientation. Mentions of intersectional
identities trigger a disproportionate amount of stereotyping. Finally, we
discuss the implications of these findings about stereotyping harms in light of
the coming intermingling of LLMs and search and the choice of stereotyping
mitigation policy to adopt. We address model builders, academics, NLP
practitioners and policy makers, calling for accountability and awareness
concerning stereotyping harms, be it for training data curation, leader board
design and usage, or social impact measurement.","[{'name': 'Alina Leidinger'}, {'name': 'Richard Rogers'}]",2024-07-16T14:04:35Z
http://arxiv.org/abs/2407.11686v3,http://arxiv.org/abs/2407.11686v3,CCoE: A Compact LLM with Collaboration of Experts,"In the domain of Large Language Model (LLM), LLMs demonstrate significant
capabilities in natural language understanding and generation. With the growing
needs of applying LLMs on various domains, it is a research question that how
to efficiently train and build a model that has expertise in different domains
but with a low training cost. We propose CCoE architecture, a framework of
easily coupling multiple strong domain experts together to fuse into a big LLM,
provides a collective way of utilizing the different domain expert LLMs.
Besides, training a large collaborative of multiple expert LLMs requires a high
requirements on training sources. CCoE bypasses this problem through isolating
other experts and train each expert separately. The design of CCoE assembles
multiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoE
layer could have one or more expert LLMs. Expert LLMs have different number of
layers and have been well-trained for different domain tasks. Each expert is
fine-tuned to be able to achieve the comparable results with SOTA domain LLMs.
We start from 5 experts in the domain of Code, Math, Law, text-to-SQL and
Medical. The results indicate that our CCoE framework can easily and
efficiently boost nearly 10%-20% performance on original base model in
different domains but using less resources on training, as well as inference.","[{'name': 'Shaomang Huang'}, {'name': 'Jianfeng Pan'}, {'name': 'Hanzhong Zheng'}]",2024-07-16T13:03:58Z
http://arxiv.org/abs/2407.11681v1,http://arxiv.org/abs/2407.11681v1,MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models,"As Large Language Models (LLMs) grow dramatically in size, there is an
increasing trend in compressing and speeding up these models. Previous studies
have highlighted the usefulness of gradients for importance scoring in neural
network compressing, especially in pruning medium-size networks. However, the
substantial memory requirements involved in calculating gradients with
backpropagation impede the utilization of gradients in guiding LLM pruning. As
a result, most pruning strategies for LLMs rely on gradient-free criteria, such
as weight magnitudes or a mix of magnitudes and activations. In this paper, we
devise a hybrid pruning criterion, which appropriately integrates magnitude,
activation, and gradient to capitalize on feature map sensitivity for pruning
LLMs. To overcome memory requirement barriers, we estimate gradients using only
forward passes. Based on this, we propose a Memory-effIcieNt structured prunIng
procedure for LLMs (MINI-LLM) to remove no-critical channels and
multi-attention heads. Experimental results demonstrate the superior
performance of MINI-LLM over existing gradient-free methods on three LLMs:
LLaMA, BLOOM, and OPT across various downstream tasks (classification,
multiple-choice, and generation), while MINI-LLM maintains a GPU memory
footprint akin to gradient-free methods.","[{'name': 'Hongrong Cheng'}, {'name': 'Miao Zhang'}, {'name': 'Javen Qinfeng Shi'}]",2024-07-16T12:59:44Z
http://arxiv.org/abs/2407.11660v1,http://arxiv.org/abs/2407.11660v1,ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues,"Despite being heralded as the new standard for dialogue evaluation, the
closed-source nature of GPT-4 poses challenges for the community. Motivated by
the need for lightweight, open source, and multilingual dialogue evaluators,
this paper introduces GenResCoh (Generated Responses targeting Coherence).
GenResCoh is a novel LLM generated dataset comprising over 130k negative and
positive responses and accompanying explanations seeded from XDailyDialog and
XPersona covering English, French, German, Italian, and Chinese. Leveraging
GenResCoh, we propose ECoh (Evaluation of Coherence), a family of evaluators
trained to assess response coherence across multiple languages. Experimental
results demonstrate that ECoh achieves multilingual detection capabilities
superior to the teacher model (GPT-3.5-Turbo) on GenResCoh, despite being based
on a much smaller architecture. Furthermore, the explanations provided by ECoh
closely align in terms of quality with those generated by the teacher model.","[{'name': 'John Mendonça'}, {'name': 'Isabel Trancoso'}, {'name': 'Alon Lavie'}]",2024-07-16T12:28:30Z
http://arxiv.org/abs/2407.11638v1,http://arxiv.org/abs/2407.11638v1,"A Comprehensive Evaluation of Large Language Models on Temporal Event
  Forecasting","Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation(RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast,
incorporating raw texts in specific complex events and fine-tuning LLMs
significantly improves performance. Moreover, enhanced with retrieval modules,
LLM can effectively capture temporal relational patterns hidden in historical
events. Meanwhile, issues such as popularity bias and the long-tail problem
still persist in LLMs, particularly in the RAG-based method. These findings not
only deepen our understanding of LLM-based event forecasting methods but also
highlight several promising research directions.We consider that this
comprehensive evaluation, along with the identified research opportunities,
will significantly contribute to future research on temporal event forecasting
through LLMs.","[{'name': 'He Chang'}, {'name': 'Chenchen Ye'}, {'name': 'Zhulin Tao'}, {'name': 'Jie Wu'}, {'name': 'Zhengmao Yang'}, {'name': 'Yunshan Ma'}, {'name': 'Xianglin Huang'}, {'name': 'Tat-Seng Chua'}]",2024-07-16T11:58:54Z
http://arxiv.org/abs/2407.11606v2,http://arxiv.org/abs/2407.11606v2,The Foundations of Tokenization: Statistical and Computational Concerns,"Tokenization - the practice of converting strings of characters over an
alphabet into sequences of tokens over a vocabulary - is a critical yet
under-theorized step in the NLP pipeline. Notably, it remains the only major
step not fully integrated into widely used end-to-end neural models. This paper
aims to address this theoretical gap by laying the foundations of tokenization
from a formal perspective. By articulating and extending basic properties about
the category of stochastic maps, we propose a unified framework for
representing and analyzing tokenizer models. This framework allows us to
establish general conditions for the use of tokenizers. In particular, we
formally establish the necessary and sufficient conditions for a tokenizer
model to preserve the consistency of statistical estimators. Additionally, we
discuss statistical and computational concerns crucial for the design and
implementation of tokenizer models. The framework and results advanced in this
paper represent a step toward a robust theoretical foundation for neural
language modeling.","[{'name': 'Juan Luis Gastaldi'}, {'name': 'John Terilla'}, {'name': 'Luca Malagutti'}, {'name': 'Brian DuSell'}, {'name': 'Tim Vieira'}, {'name': 'Ryan Cotterell'}]",2024-07-16T11:12:28Z
http://arxiv.org/abs/2407.11591v2,http://arxiv.org/abs/2407.11591v2,"AdaptEval: Evaluating Large Language Models on Domain Adaptation for
  Text Summarization","Despite the advances in the abstractive summarization task using Large
Language Models (LLM), there is a lack of research that asses their abilities
to easily adapt to different domains. We evaluate the domain adaptation
abilities of a wide range of LLMs on the summarization task across various
domains in both fine-tuning and in-context learning settings. We also present
AdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a
domain benchmark and a set of metrics to facilitate the analysis of domain
adaptation. Our results demonstrate that LLMs exhibit comparable performance in
the in-context learning setting, regardless of their parameter scale.","[{'name': 'Anum Afzal'}, {'name': 'Ribin Chalumattu'}, {'name': 'Florian Matthes'}, {'name': 'Laura Mascarell'}]",2024-07-16T10:50:39Z
http://arxiv.org/abs/2407.11550v2,http://arxiv.org/abs/2407.11550v2,"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for
  Efficient LLM Inference","Large Language Models have excelled in various fields but encounter
efficiency limitations due to the substantial Key-Value (KV) cache required for
long-sequence inference. Recent efforts try to evict non-critical cache
elements during runtime, thereby reducing cache size within given memory
budgets while preserving generation quality. Our reexamination of foundational
principles reveals that prevailing methods aim to minimize an upper bound of
eviction loss, quantified as the L1 distance between the pre- and post-eviction
outputs of multi-head self-attention mechanisms. Moreover, our analysis
indicates that the common practices of uniformly assigning budgets across
different attention heads during cache eviction hinder their budget
utilization, negatively impacting generation quality. In light of these
findings, we propose a simple yet effective adaptive budget allocation
algorithm. This algorithm not only optimizes the loss upper bound in theory but
also reduces the eviction loss in practice by aligning with the intrinsic
patterns of self-attention mechanisms. Integrating this algorithm into two
advanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations
on 16 datasets and the Needle-in-a-Haystack test confirm that they both
significantly boost performance across various tasks.","[{'name': 'Yuan Feng'}, {'name': 'Junlin Lv'}, {'name': 'Yukun Cao'}, {'name': 'Xike Xie'}, {'name': 'S. Kevin Zhou'}]",2024-07-16T09:53:32Z
http://arxiv.org/abs/2407.11549v1,http://arxiv.org/abs/2407.11549v1,"How Personality Traits Influence Negotiation Outcomes? A Simulation
  based on Large Language Models","Psychological evidence reveals the influence of personality traits on
decision-making. For instance, agreeableness is generally associated with
positive outcomes in negotiations, whereas neuroticism is often linked to less
favorable outcomes. This paper introduces a simulation framework centered on
Large Language Model (LLM) agents endowed with synthesized personality traits.
The agents negotiate within bargaining domains and possess customizable
personalities and objectives. The experimental results show that the behavioral
tendencies of LLM-based simulations could reproduce behavioral patterns
observed in human negotiations. The contribution is twofold. First, we propose
a simulation methodology that investigates the alignment between the linguistic
and economic capabilities of LLM agents. Secondly, we offer empirical insights
into the strategic impact of Big-Five personality traits on the outcomes of
bilateral negotiations. We also provide a case study based on synthesized
bargaining dialogues to reveal intriguing behaviors, including deceitful and
compromising behaviors.","[{'name': 'Yin Jou Huang'}, {'name': 'Rafik Hadfi'}]",2024-07-16T09:52:51Z
http://arxiv.org/abs/2407.11536v1,http://arxiv.org/abs/2407.11536v1,"Fine-Tuning Medical Language Models for Enhanced Long-Contextual
  Understanding and Domain Expertise","Large Language Models (LLMs) have been widely applied in various professional
fields. By fine-tuning the models using domain specific question and answer
datasets, the professional domain knowledge and Q\&A abilities of these models
have significantly improved, for example, medical professional LLMs that use
fine-tuning of doctor-patient Q\&A data exhibit extraordinary disease
diagnostic abilities. However, we observed that despite improvements in
specific domain knowledge, the performance of medical LLM in long-context
understanding has significantly declined, especially compared to general
language models with similar parameters. The purpose of this study is to
investigate the phenomenon of reduced performance in understanding long-context
in medical LLM. We designed a series of experiments to conduct open-book
professional knowledge exams on all models to evaluate their ability to read
long-context. By adjusting the proportion and quantity of general data and
medical data in the process of fine-tuning, we can determine the best data
composition to optimize the professional model and achieve a balance between
long-context performance and specific domain knowledge.","[{'name': 'Qimin Yang'}, {'name': 'Rongsheng Wang'}, {'name': 'Jiexin Chen'}, {'name': 'Runqi Su'}, {'name': 'Tao Tan'}]",2024-07-16T09:37:20Z
http://arxiv.org/abs/2407.12880v1,http://arxiv.org/abs/2407.12880v1,Cross-Modal Augmentation for Few-Shot Multimodal Fake News Detection,"The nascent topic of fake news requires automatic detection methods to
quickly learn from limited annotated samples. Therefore, the capacity to
rapidly acquire proficiency in a new task with limited guidance, also known as
few-shot learning, is critical for detecting fake news in its early stages.
Existing approaches either involve fine-tuning pre-trained language models
which come with a large number of parameters, or training a complex neural
network from scratch with large-scale annotated datasets. This paper presents a
multimodal fake news detection model which augments multimodal features using
unimodal features. For this purpose, we introduce Cross-Modal Augmentation
(CMA), a simple approach for enhancing few-shot multimodal fake news detection
by transforming n-shot classification into a more robust (n $\times$ z)-shot
problem, where z represents the number of supplementary features. The proposed
CMA achieves SOTA results over three benchmark datasets, utilizing a
surprisingly simple linear probing method to classify multimodal fake news with
only a few training samples. Furthermore, our method is significantly more
lightweight than prior approaches, particularly in terms of the number of
trainable parameters and epoch times. The code is available here:
\url{https://github.com/zgjiangtoby/FND_fewshot}","[{'name': 'Ye Jiang'}, {'name': 'Taihang Wang'}, {'name': 'Xiaoman Xu'}, {'name': 'Yimin Wang'}, {'name': 'Xingyi Song'}, {'name': 'Diana Maynard'}]",2024-07-16T09:32:11Z
http://arxiv.org/abs/2407.12879v1,http://arxiv.org/abs/2407.12879v1,"Large Visual-Language Models Are Also Good Classifiers: A Study of
  In-Context Multimodal Fake News Detection","Large visual-language models (LVLMs) exhibit exceptional performance in
visual-language reasoning across diverse cross-modal benchmarks. Despite these
advances, recent research indicates that Large Language Models (LLMs), like
GPT-3.5-turbo, underachieve compared to well-trained smaller models, such as
BERT, in Fake News Detection (FND), prompting inquiries into LVLMs' efficacy in
FND tasks. Although performance could improve through fine-tuning LVLMs, the
substantial parameters and requisite pre-trained weights render it a
resource-heavy endeavor for FND applications. This paper initially assesses the
FND capabilities of two notable LVLMs, CogVLM and GPT4V, in comparison to a
smaller yet adeptly trained CLIP model in a zero-shot context. The findings
demonstrate that LVLMs can attain performance competitive with that of the
smaller model. Next, we integrate standard in-context learning (ICL) with
LVLMs, noting improvements in FND performance, though limited in scope and
consistency. To address this, we introduce the \textbf{I}n-context
\textbf{M}ultimodal \textbf{F}ake \textbf{N}ews \textbf{D}etection (IMFND)
framework, enriching in-context examples and test inputs with predictions and
corresponding probabilities from a well-trained smaller model. This strategic
integration directs the LVLMs' focus towards news segments associated with
higher probabilities, thereby improving their analytical accuracy. The
experimental results suggest that the IMFND framework significantly boosts the
FND efficiency of LVLMs, achieving enhanced accuracy over the standard ICL
approach across three publicly available FND datasets.","[{'name': 'Ye Jiang'}, {'name': 'Yimin Wang'}]",2024-07-16T09:28:23Z
http://arxiv.org/abs/2407.12878v2,http://arxiv.org/abs/2407.12878v2,Do LLMs have Consistent Values?,"Values are a basic driving force underlying human behavior. Large Language
Models (LLM) technology is constantly improving towards human-like dialogue.
However, little research has been done to study the values exhibited in text
generated by LLMs. Here we study this question by turning to the rich
literature on value structure in psychology. We ask whether LLMs exhibit the
same value structure that has been demonstrated in humans, including the
ranking of values, and correlation between values. We show that the results of
this analysis strongly depend on how the LLM is prompted, and that under a
particular prompting strategy (referred to as 'Value Anchoring') the agreement
with human data is quite compelling. Our results serve both to improve our
understanding of values in LLMs, as well as introduce novel methods for
assessing consistency in LLM responses.","[{'name': 'Naama Rozen'}, {'name': 'Gal Elidan'}, {'name': 'Amir Globerson'}, {'name': 'Ella Daniel'}]",2024-07-16T08:58:00Z
http://arxiv.org/abs/2407.11511v1,http://arxiv.org/abs/2407.11511v1,"Reasoning with Large Language Models, a Survey","Scaling up language models to billions of parameters has opened up
possibilities for in-context learning, allowing instruction tuning and few-shot
learning on tasks that the model was not specifically trained for. This has
achieved breakthrough performance on language tasks such as translation,
summarization, and question-answering. Furthermore, in addition to these
associative ""System 1"" tasks, recent advances in Chain-of-thought prompt
learning have demonstrated strong ""System 2"" reasoning abilities, answering a
question in the field of artificial general intelligence whether LLMs can
reason. The field started with the question whether LLMs can solve grade school
math word problems. This paper reviews the rapidly expanding field of
prompt-based reasoning with LLMs. Our taxonomy identifies different ways to
generate, evaluate, and control multi-step reasoning. We provide an in-depth
coverage of core approaches and open problems, and we propose a research agenda
for the near future. Finally, we highlight the relation between reasoning and
prompt-based learning, and we discuss the relation between reasoning,
sequential decision processes, and reinforcement learning. We find that
self-improvement, self-reflection, and some metacognitive abilities of the
reasoning processes are possible through the judicious use of prompts. True
self-improvement and self-reasoning, to go from reasoning with LLMs to
reasoning by LLMs, remains future work.","[{'name': 'Aske Plaat'}, {'name': 'Annie Wong'}, {'name': 'Suzan Verberne'}, {'name': 'Joost Broekens'}, {'name': 'Niki van Stein'}, {'name': 'Thomas Back'}]",2024-07-16T08:49:35Z
http://arxiv.org/abs/2407.11492v1,http://arxiv.org/abs/2407.11492v1,MMSD-Net: Towards Multi-modal Stuttering Detection,"Stuttering is a common speech impediment that is caused by irregular
disruptions in speech production, affecting over 70 million people across the
world. Standard automatic speech processing tools do not take speech ailments
into account and are thereby not able to generate meaningful results when
presented with stuttered speech as input. The automatic detection of stuttering
is an integral step towards building efficient, context-aware speech processing
systems. While previous approaches explore both statistical and neural
approaches for stuttering detection, all of these methods are uni-modal in
nature. This paper presents MMSD-Net, the first multi-modal neural framework
for stuttering detection. Experiments and results demonstrate that
incorporating the visual signal significantly aids stuttering detection, and
our model yields an improvement of 2-17% in the F1-score over existing
state-of-the-art uni-modal approaches.","[{'name': 'Liangyu Nie'}, {'name': 'Sudarsana Reddy Kadiri'}, {'name': 'Ruchit Agrawal'}]",2024-07-16T08:26:59Z
http://arxiv.org/abs/2407.12877v1,http://arxiv.org/abs/2407.12877v1,"Review-Feedback-Reason (ReFeR): A Novel Framework for NLG Evaluation and
  Reasoning","Assessing the quality of Natural Language Generation (NLG) outputs, such as
those produced by large language models (LLMs), poses significant challenges.
Traditional approaches involve either resource-intensive human evaluations or
automatic metrics, which often exhibit a low correlation with human judgment.
In this study, we propose Review-Feedback-Reason (ReFeR), a novel evaluation
framework for NLG using LLM agents. We rigorously test ReFeR using two
pre-existing benchmark datasets on diverse NLG tasks. The proposed framework
not only enhances the accuracy of NLG evaluation, surpassing previous
benchmarks by $\sim$20\%, but also generates constructive feedback and
significantly improves collective reasoning. This feedback is then leveraged
for the creation of instruction-tuning datasets, which, when used to fine-tune
smaller models like Mistral-7B, makes them extremely good evaluators, yielding
a better correlation with human evaluations and performance nearly on par with
GPT-3.5. We highlight the effectiveness of our methodology through its
application on three reasoning benchmarks, where it outperforms most of the
state-of-the-art methods, and also outperforms the reasoning capabilities of
models like GPT-3.5 Turbo by $\sim$11.67\% and GPT-4 by $\sim$1\% on an
average.","[{'name': 'Yaswanth Narsupalli'}, {'name': 'Abhranil Chandra'}, {'name': 'Sreevatsa Muppirala'}, {'name': 'Manish Gupta'}, {'name': 'Pawan Goyal'}]",2024-07-16T08:25:26Z
http://arxiv.org/abs/2407.11485v1,http://arxiv.org/abs/2407.11485v1,Scientific QA System with Verifiable Answers,"In this paper, we introduce the VerifAI project, a pioneering open-source
scientific question-answering system, designed to provide answers that are not
only referenced but also automatically vetted and verifiable. The components of
the system are (1) an Information Retrieval system combining semantic and
lexical search techniques over scientific papers (PubMed), (2) a
Retrieval-Augmented Generation (RAG) module using fine-tuned generative model
(Mistral 7B) and retrieved articles to generate claims with references to the
articles from which it was derived, and (3) a Verification engine, based on a
fine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task
using SciFACT dataset. The verification engine cross-checks the generated claim
and the article from which the claim was derived, verifying whether there may
have been any hallucinations in generating the claim. By leveraging the
Information Retrieval and RAG modules, Verif.ai excels in generating factual
information from a vast array of scientific sources. At the same time, the
Verification engine rigorously double-checks this output, ensuring its accuracy
and reliability. This dual-stage process plays a crucial role in acquiring and
confirming factual information, significantly enhancing the information
landscape. Our methodology could significantly enhance scientists'
productivity, concurrently fostering trust in applying generative language
models within scientific domains, where hallucinations and misinformation are
unacceptable.","[{'name': 'Adela Ljajić'}, {'name': 'Miloš Košprdić'}, {'name': 'Bojana Bašaragin'}, {'name': 'Darija Medvecki'}, {'name': 'Lorenzo Cassano'}, {'name': 'Nikola Milošević'}]",2024-07-16T08:21:02Z
http://arxiv.org/abs/2407.11484v4,http://arxiv.org/abs/2407.11484v4,The Oscars of AI Theater: A Survey on Role-Playing with Language Models,"This survey explores the burgeoning field of role-playing with language
models, focusing on their development from early persona-based models to
advanced character-driven simulations facilitated by Large Language Models
(LLMs). Initially confined to simple persona consistency due to limited model
capabilities, role-playing tasks have now expanded to embrace complex character
portrayals involving character consistency, behavioral alignment, and overall
attractiveness. We provide a comprehensive taxonomy of the critical components
in designing these systems, including data, models and alignment, agent
architecture and evaluation. This survey not only outlines the current
methodologies and challenges, such as managing dynamic personal profiles and
achieving high-level persona consistency but also suggests avenues for future
research in improving the depth and realism of role-playing applications. The
goal is to guide future research by offering a structured overview of current
methodologies and identifying potential areas for improvement. Related
resources and papers are available at
https://github.com/nuochenpku/Awesome-Role-Play-Papers.","[{'name': 'Nuo Chen'}, {'name': 'Yang Deng'}, {'name': 'Jia Li'}]",2024-07-16T08:20:39Z
http://arxiv.org/abs/2407.11421v1,http://arxiv.org/abs/2407.11421v1,"States Hidden in Hidden States: LLMs Emerge Discrete State
  Representations Implicitly","Large Language Models (LLMs) exhibit various emergent abilities. Among these
abilities, some might reveal the internal working mechanisms of models. In this
paper, we uncover a novel emergent capability in models: the intrinsic ability
to perform extended sequences of calculations without relying on
chain-of-thought step-by-step solutions. Remarkably, the most advanced models
can directly output the results of two-digit number additions with lengths
extending up to 15 addends. We hypothesize that the model emerges Implicit
Discrete State Representations (IDSRs) within its hidden states and performs
symbolic calculations internally. To test this hypothesis, we design a sequence
of experiments that look into the hidden states. Specifically, we first confirm
that IDSRs exist. Then, we provide interesting observations about the formation
of IDSRs from layer, digit, and sequence perspectives. Finally, we confirm that
models indeed use IDSRs to produce the final answers. However, we also discover
that these state representations are far from lossless in current open-sourced
models, leading to inaccuracies in their final performance. Our work presents a
novel exploration of LLMs' symbolic calculation abilities and the underlying
mechanisms.","[{'name': 'Junhao Chen'}, {'name': 'Shengding Hu'}, {'name': 'Zhiyuan Liu'}, {'name': 'Maosong Sun'}]",2024-07-16T06:27:22Z
http://arxiv.org/abs/2407.11418v1,http://arxiv.org/abs/2407.11418v1,"LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured
  and Structured Data","The semantic capabilities of language models (LMs) have the potential to
enable rich analytics and reasoning over vast knowledge corpora. Unfortunately,
existing systems lack high-level abstractions to perform semantic queries at
scale. We introduce semantic operators, a declarative programming interface
that extends the relational model with composable AI-based operations for
semantic queries over datasets (e.g., sorting or aggregating records using
natural language criteria). Each operator can be implemented and optimized in
multiple ways, opening a rich space for execution plans similar to relational
operators. We implement our operators and several optimizations for them in
LOTUS, an open-source query engine with a Pandas-like API.
  We demonstrate LOTUS' effectiveness across a series of real applications,
including fact-checking, extreme multi-label classification, and search. We
find that LOTUS' programming model is highly expressive, capturing
state-of-the-art query pipelines with low development overhead. Specifically,
on the FEVER dataset, LOTUS' programs can reproduce FacTool, a recent
state-of-the-art fact-checking pipeline, in few lines of code, and implement a
new pipeline that improves accuracy by $9.5\%$, while offering $7-34\times$
lower execution time. In the extreme multi-label classification task on the
BioDEX dataset, LOTUS reproduces state-of-the art result quality with its join
operator, while providing an efficient algorithm that runs $800\times$ faster
than a naive join. In the search and ranking application, LOTUS allows a simple
composition of operators to achieve $5.9 - 49.4\%$ higher nDCG@10 than the
vanilla retriever and re-ranker, while also providing query efficiency, with
$1.67 - 10\times$ lower execution time than LM-based ranking methods used by
prior works. LOTUS is publicly available at
https://github.com/stanford-futuredata/lotus.","[{'name': 'Liana Patel'}, {'name': 'Siddharth Jha'}, {'name': 'Carlos Guestrin'}, {'name': 'Matei Zaharia'}]",2024-07-16T06:19:14Z
http://arxiv.org/abs/2407.11417v1,http://arxiv.org/abs/2407.11417v1,"SPINACH: SPARQL-Based Information Navigation for Challenging Real-World
  Questions","Recent work integrating Large Language Models (LLMs) has led to significant
improvements in the Knowledge Base Question Answering (KBQA) task. However, we
posit that existing KBQA datasets that either have simple questions, use
synthetically generated logical forms, or are based on small knowledge base
(KB) schemas, do not capture the true complexity of KBQA tasks.
  To address this, we introduce the SPINACH dataset, an expert-annotated KBQA
dataset collected from forum discussions on Wikidata's ""Request a Query"" forum
with 320 decontextualized question-SPARQL pairs. Much more complex than
existing datasets, SPINACH calls for strong KBQA systems that do not rely on
training data to learn the KB schema, but can dynamically explore large and
often incomplete schemas and reason about them.
  Along with the dataset, we introduce the SPINACH agent, a new KBQA approach
that mimics how a human expert would write SPARQLs for such challenging
questions. Experiments on existing datasets show SPINACH's capability in KBQA,
achieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10
datasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6%
of the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH
dataset, SPINACH agent outperforms all baselines, including the best
GPT-4-based KBQA agent, by 38.1% in F1.","[{'name': 'Shicheng Liu'}, {'name': 'Sina J. Semnani'}, {'name': 'Harold Triedman'}, {'name': 'Jialiang Xu'}, {'name': 'Isaac Dan Zhao'}, {'name': 'Monica S. Lam'}]",2024-07-16T06:18:21Z
http://arxiv.org/abs/2407.12876v2,http://arxiv.org/abs/2407.12876v2,Exploring the Use of Abusive Generative AI Models on Civitai,"The rise of generative AI is transforming the landscape of digital imagery,
and exerting a significant influence on online creative communities. This has
led to the emergence of AI-Generated Content (AIGC) social platforms, such as
Civitai. These distinctive social platforms allow users to build and share
their own generative AI models, thereby enhancing the potential for more
diverse artistic expression. Designed in the vein of social networks, they also
provide artists with the means to showcase their creations (generated from the
models), engage in discussions, and obtain feedback, thus nurturing a sense of
community. Yet, this openness also raises concerns about the abuse of such
platforms, e.g., using models to disseminate deceptive deepfakes or infringe
upon copyrights. To explore this, we conduct the first comprehensive empirical
study of an AIGC social platform, focusing on its use for generating abusive
content. As an exemplar, we construct a comprehensive dataset covering Civitai,
the largest available AIGC social platform. Based on this dataset of 87K models
and 2M images, we explore the characteristics of content and discuss strategies
for moderation to better govern these platforms.","[{'name': 'Yiluo Wei'}, {'name': 'Yiming Zhu'}, {'name': 'Pan Hui'}, {'name': 'Gareth Tyson'}]",2024-07-16T06:18:03Z
http://arxiv.org/abs/2407.11409v1,http://arxiv.org/abs/2407.11409v1,"Representation Bias in Political Sample Simulations with Large Language
  Models","This study seeks to identify and quantify biases in simulating political
samples with Large Language Models, specifically focusing on vote choice and
public opinion. Using the GPT-3.5-Turbo model, we leverage data from the
American National Election Studies, German Longitudinal Election Study, Zuobiao
Dataset, and China Family Panel Studies to simulate voting behaviors and public
opinions. This methodology enables us to examine three types of representation
bias: disparities based on the the country's language, demographic groups, and
political regime types. The findings reveal that simulation performance is
generally better for vote choice than for public opinions, more accurate in
English-speaking countries, more effective in bipartisan systems than in
multi-partisan systems, and stronger in democratic settings than in
authoritarian regimes. These results contribute to enhancing our understanding
and developing strategies to mitigate biases in AI applications within the
field of computational social science.","[{'name': 'Weihong Qi'}, {'name': 'Hanjia Lyu'}, {'name': 'Jiebo Luo'}]",2024-07-16T05:52:26Z
http://arxiv.org/abs/2407.11406v1,http://arxiv.org/abs/2407.11406v1,Revisiting the Impact of Pursuing Modularity for Code Generation,"Modular programming, which aims to construct the final program by integrating
smaller, independent building blocks, has been regarded as a desirable practice
in software development. However, with the rise of recent code generation
agents built upon large language models (LLMs), a question emerges: is this
traditional practice equally effective for these new tools? In this work, we
assess the impact of modularity in code generation by introducing a novel
metric for its quantitative measurement. Surprisingly, unlike conventional
wisdom on the topic, we find that modularity is not a core factor for improving
the performance of code generation models. We also explore potential
explanations for why LLMs do not exhibit a preference for modular code compared
to non-modular code.","[{'name': 'Deokyeong Kang'}, {'name': 'Ki Jung Seo'}, {'name': 'Taeuk Kim'}]",2024-07-16T05:48:24Z
http://arxiv.org/abs/2407.11393v2,http://arxiv.org/abs/2407.11393v2,"CIC-BART-SSA: Controllable Image Captioning with Structured Semantic
  Augmentation","Controllable Image Captioning (CIC) aims at generating natural language
descriptions for an image, conditioned on information provided by end users,
e.g., regions, entities or events of interest. However, available
image-language datasets mainly contain captions that describe the entirety of
an image, making them ineffective for training CIC models that can potentially
attend to any subset of regions or relationships. To tackle this challenge, we
propose a novel, fully automatic method to sample additional focused and
visually grounded captions using a unified structured semantic representation
built on top of the existing set of captions associated with an image. We
leverage Abstract Meaning Representation (AMR), a cross-lingual graph-based
semantic formalism, to encode all possible spatio-semantic relations between
entities, beyond the typical spatial-relations-only focus of current methods.
We use this Structured Semantic Augmentation (SSA) framework to augment
existing image-caption datasets with the grounded controlled captions,
increasing their spatial and semantic diversity and focal coverage. We then
develop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that
sources its control signals from SSA-diversified datasets. We empirically show
that, compared to SOTA CIC models, CIC-BART-SSA generates captions that are
superior in diversity and text quality, are competitive in controllability,
and, importantly, minimize the gap between broad and highly focused controlled
captioning performance by efficiently generalizing to the challenging highly
focused scenarios. Code is available at
https://github.com/SamsungLabs/CIC-BART-SSA.","[{'name': 'Kalliopi Basioti'}, {'name': 'Mohamed A. Abdelsalam'}, {'name': 'Federico Fancellu'}, {'name': 'Vladimir Pavlovic'}, {'name': 'Afsaneh Fazly'}]",2024-07-16T05:26:12Z
http://arxiv.org/abs/2407.11384v1,http://arxiv.org/abs/2407.11384v1,"InvAgent: A Large Language Model based Multi-Agent System for Inventory
  Management in Supply Chains","Supply chain management (SCM) involves coordinating the flow of goods,
information, and finances across various entities to deliver products
efficiently. Effective inventory management is crucial in today's volatile,
uncertain, complex, and ambiguous (VUCA) world. Previous research has
demonstrated the superiority of heuristic methods and reinforcement learning
applications in inventory management. However, the application of large
language models (LLMs) as autonomous agents in multi-agent systems for
inventory management remains underexplored. This study introduces a novel
approach using LLMs to manage multi-agent inventory systems. Leveraging their
zero-shot learning capabilities, our model, InvAgent, enhances resilience and
improves efficiency across the supply chain network. Our contributions include
utilizing LLMs for zero-shot learning to enable adaptive and informed
decision-making without prior training, providing significant explainability
and clarity through Chain-of-Thought (CoT), and demonstrating dynamic
adaptability to varying demand scenarios while minimizing costs and avoiding
stockouts. Extensive evaluations across different scenarios highlight the
efficiency of our model in SCM.","[{'name': 'Yinzhu Quan'}, {'name': 'Zefang Liu'}]",2024-07-16T04:55:17Z
http://arxiv.org/abs/2407.11371v1,http://arxiv.org/abs/2407.11371v1,Estimating Agreement by Chance for Sequence Annotation,"In the field of natural language processing, correction of performance
assessment for chance agreement plays a crucial role in evaluating the
reliability of annotations. However, there is a notable dearth of research
focusing on chance correction for assessing the reliability of sequence
annotation tasks, despite their widespread prevalence in the field. To address
this gap, this paper introduces a novel model for generating random
annotations, which serves as the foundation for estimating chance agreement in
sequence annotation tasks. Utilizing the proposed randomization model and a
related comparison approach, we successfully derive the analytical form of the
distribution, enabling the computation of the probable location of each
annotated text segment and subsequent chance agreement estimation. Through a
combination simulation and corpus-based evaluation, we successfully assess its
applicability and validate its accuracy and efficacy.","[{'name': 'Diya Li'}, {'name': 'Carolyn Rosé'}, {'name': 'Ao Yuan'}, {'name': 'Chunxiao Zhou'}]",2024-07-16T04:32:47Z
http://arxiv.org/abs/2407.11370v1,http://arxiv.org/abs/2407.11370v1,"A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only
  Using Native Speech Corpora","We propose a method of simulating the human process of foreign accentuation
using Generative Spoken Language Model (GSLM) only with native speech corpora.
When one listens to spoken words of a foreign language and repeats them, the
repeated speech is often with the accent of that listener's L1. This is said to
be because the spoken words are mentally represented as a sequence of
phonological units of the L1, and those units are used for oral reproduction.
We simulate this process by inputting speech of language A into GSLM of
language B to add B's accent onto the input speech. The process of running ASR
of the L1 for foreign input speech and giving the ASR result to TTS of the L1
can be viewed as a naive implementation of this approach. The results of our
experiments show that the synthesized accent of the output speech is highly
natural, compared to real samples of A generated by speakers whose L1 is B, and
that the degree of accentuation is controllable.","[{'name': 'Kentaro Onda'}, {'name': 'Joonyong Park'}, {'name': 'Nobuaki Minematsu'}, {'name': 'Daisuke Saito'}]",2024-07-16T04:29:00Z
http://arxiv.org/abs/2407.11345v1,http://arxiv.org/abs/2407.11345v1,"Beyond Binary: Multiclass Paraphasia Detection with Generative
  Pretrained Transformers and End-to-End Models","Aphasia is a language disorder that can lead to speech errors known as
paraphasias, which involve the misuse, substitution, or invention of words.
Automatic paraphasia detection can help those with Aphasia by facilitating
clinical assessment and treatment planning options. However, most automatic
paraphasia detection works have focused solely on binary detection, which
involves recognizing only the presence or absence of a paraphasia. Multiclass
paraphasia detection represents an unexplored area of research that focuses on
identifying multiple types of paraphasias and where they occur in a given
speech segment. We present novel approaches that use a generative pretrained
transformer (GPT) to identify paraphasias from transcripts as well as two
end-to-end approaches that focus on modeling both automatic speech recognition
(ASR) and paraphasia classification as multiple sequences vs. a single
sequence. We demonstrate that a single sequence model outperforms GPT baselines
for multiclass paraphasia detection.","[{'name': 'Matthew Perez'}, {'name': 'Aneesha Sampath'}, {'name': 'Minxue Niu'}, {'name': 'Emily Mower Provost'}]",2024-07-16T03:24:51Z
http://arxiv.org/abs/2407.12064v1,http://arxiv.org/abs/2407.12064v1,"LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization
  and Classification Task","Vision-language models have been extensively explored across a wide range of
tasks, achieving satisfactory performance; however, their application in
medical imaging remains underexplored. In this work, we propose a unified
framework - LiteGPT - for the medical imaging. We leverage multiple pre-trained
visual encoders to enrich information and enhance the performance of
vision-language models. To the best of our knowledge, this is the first study
to utilize vision-language models for the novel task of joint localization and
classification in medical images. Besides, we are pioneers in providing
baselines for disease localization in chest X-rays. Finally, we set new
state-of-the-art performance in the image classification task on the
well-benchmarked VinDr-CXR dataset. All code and models are publicly available
online: https://github.com/leduckhai/LiteGPT","[{'name': 'Khai Le-Duc'}, {'name': 'Ryan Zhang'}, {'name': 'Ngoc Son Nguyen'}, {'name': 'Tan-Hanh Pham'}, {'name': 'Anh Dao'}, {'name': 'Ba Hung Ngo'}, {'name': 'Anh Totti Nguyen'}, {'name': 'Truong-Son Hy'}]",2024-07-16T02:19:02Z
http://arxiv.org/abs/2407.11282v3,http://arxiv.org/abs/2407.11282v3,"Uncertainty is Fragile: Manipulating Uncertainty in Large Language
  Models","Large Language Models (LLMs) are employed across various high-stakes domains,
where the reliability of their outputs is crucial. One commonly used method to
assess the reliability of LLMs' responses is uncertainty estimation, which
gauges the likelihood of their answers being correct. While many studies focus
on improving the accuracy of uncertainty estimations for LLMs, our research
investigates the fragility of uncertainty estimation and explores potential
attacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,
when activated by a specific trigger in the input, manipulates the model's
uncertainty without affecting the final output. Specifically, the proposed
backdoor attack method can alter an LLM's output probability distribution,
causing the probability distribution to converge towards an attacker-predefined
distribution while ensuring that the top-1 prediction remains unchanged. Our
experimental results demonstrate that this attack effectively undermines the
model's self-evaluation reliability in multiple-choice questions. For instance,
we achieved a 100 attack success rate (ASR) across three different triggering
strategies in four models. Further, we investigate whether this manipulation
generalizes across different prompts and domains. This work highlights a
significant threat to the reliability of LLMs and underscores the need for
future defenses against such attacks. The code is available at
https://github.com/qcznlp/uncertainty_attack.","[{'name': 'Qingcheng Zeng'}, {'name': 'Mingyu Jin'}, {'name': 'Qinkai Yu'}, {'name': 'Zhenting Wang'}, {'name': 'Wenyue Hua'}, {'name': 'Zihao Zhou'}, {'name': 'Guangyan Sun'}, {'name': 'Yanda Meng'}, {'name': 'Shiqing Ma'}, {'name': 'Qifan Wang'}, {'name': 'Felix Juefei-Xu'}, {'name': 'Kaize Ding'}, {'name': 'Fan Yang'}, {'name': 'Ruixiang Tang'}, {'name': 'Yongfeng Zhang'}]",2024-07-15T23:41:11Z
http://arxiv.org/abs/2407.11277v2,http://arxiv.org/abs/2407.11277v2,"Target conversation extraction: Source separation using turn-taking
  dynamics","Extracting the speech of participants in a conversation amidst interfering
speakers and noise presents a challenging problem. In this paper, we introduce
the novel task of target conversation extraction, where the goal is to extract
the audio of a target conversation based on the speaker embedding of one of its
participants. To accomplish this, we propose leveraging temporal patterns
inherent in human conversations, particularly turn-taking dynamics, which
uniquely characterize speakers engaged in conversation and distinguish them
from interfering speakers and noise. Using neural networks, we show the
feasibility of our approach on English and Mandarin conversation datasets. In
the presence of interfering speakers, our results show an 8.19 dB improvement
in signal-to-noise ratio for 2-speaker conversations and a 7.92 dB improvement
for 2-4-speaker conversations. Code, dataset available at
https://github.com/chentuochao/Target-Conversation-Extraction.","[{'name': 'Tuochao Chen'}, {'name': 'Qirui Wang'}, {'name': 'Bohan Wu'}, {'name': 'Malek Itani'}, {'name': 'Sefik Emre Eskimez'}, {'name': 'Takuya Yoshioka'}, {'name': 'Shyamnath Gollakota'}]",2024-07-15T22:55:27Z
http://arxiv.org/abs/2407.11242v1,http://arxiv.org/abs/2407.11242v1,"OmniGenome: Aligning RNA Sequences with Secondary Structures in Genomic
  Foundation Models","The structures of RNA sequences play a vital role in various cellular
processes, while existing genomic foundation models (FMs) have struggled with
precise sequence-structure alignment, due to the complexity of exponential
combinations of nucleotide bases. In this study, we introduce OmniGenome, a
foundation model that addresses this critical challenge of sequence-structure
alignment in RNA FMs. OmniGenome bridges the sequences with secondary
structures using structure-contextualized modeling, enabling hard in-silico
genomic tasks that existing FMs cannot handle, e.g., RNA design tasks. The
results on two comprehensive genomic benchmarks show that OmniGenome achieves
state-of-the-art performance on complex RNA subtasks. For example, OmniGenome
solved 74% of complex puzzles, compared to SpliceBERT which solved only 3% of
the puzzles. Besides, OmniGenome solves most of the puzzles within $1$ hour,
while the existing methods usually allocate $24$ hours for each puzzle.
Overall, OmniGenome establishes wide genomic application cases and offers
profound insights into biological mechanisms from the perspective of
sequence-structure alignment.","[{'name': 'Heng Yang'}, {'name': 'Ke Li'}]",2024-07-15T21:10:40Z
http://arxiv.org/abs/2407.11240v1,http://arxiv.org/abs/2407.11240v1,"Making New Connections: LLMs as Puzzle Generators for The New York
  Times' Connections Word Game","The Connections puzzle is a word association game published daily by The New
York Times (NYT). In this game, players are asked to find groups of four words
that are connected by a common theme. While solving a given Connections puzzle
requires both semantic knowledge and abstract reasoning, generating novel
puzzles additionally requires a form of metacognition: generators must be able
to accurately model the downstream reasoning of potential solvers. In this
paper, we investigate the ability of the GPT family of Large Language Models
(LLMs) to generate challenging and creative word games for human players. We
start with an analysis of the word game Connections and the unique challenges
it poses as a Procedural Content Generation (PCG) domain. We then propose a
method for generating Connections puzzles using LLMs by adapting a Tree of
Thoughts (ToT) prompting approach. We evaluate this method by conducting a user
study, asking human players to compare AI-generated puzzles against published
Connections puzzles. Our findings show that LLMs are capable puzzle creators,
and can generate diverse sets of enjoyable, challenging, and creative
Connections puzzles as judged by human users.","[{'name': 'Tim Merino'}, {'name': 'Sam Earle'}, {'name': 'Ryan Sudhakaran'}, {'name': 'Shyam Sudhakaran'}, {'name': 'Julian Togelius'}]",2024-07-15T21:05:25Z
http://arxiv.org/abs/2407.11229v1,http://arxiv.org/abs/2407.11229v1,"Unraveling the Truth: Do LLMs really Understand Charts? A Deep Dive into
  Consistency and Robustness","Chart question answering (CQA) is a crucial area of Visual Language
Understanding. However, the robustness and consistency of current Visual
Language Models (VLMs) in this field remain under-explored. This paper
evaluates state-of-the-art VLMs on comprehensive datasets, developed
specifically for this study, encompassing diverse question categories and chart
formats. We investigate two key aspects: 1) the models' ability to handle
varying levels of chart and question complexity, and 2) their robustness across
different visual representations of the same underlying data. Our analysis
reveals significant performance variations based on question and chart types,
highlighting both strengths and weaknesses of current models. Additionally, we
identify areas for improvement and propose future research directions to build
more robust and reliable CQA systems. This study sheds light on the limitations
of current models and paves the way for future advancements in the field.","[{'name': 'Srija Mukhopadhyay'}, {'name': 'Adnan Qidwai'}, {'name': 'Aparna Garimella'}, {'name': 'Pritika Ramu'}, {'name': 'Vivek Gupta'}, {'name': 'Dan Roth'}]",2024-07-15T20:29:24Z
http://arxiv.org/abs/2407.11215v1,http://arxiv.org/abs/2407.11215v1,"Mechanistic interpretability of large language models with applications
  to the financial services industry","Large Language Models such as GPTs (Generative Pre-trained Transformers)
exhibit remarkable capabilities across a broad spectrum of applications.
Nevertheless, due to their intrinsic complexity, these models present
substantial challenges in interpreting their internal decision-making
processes. This lack of transparency poses critical challenges when it comes to
their adaptation by financial institutions, where concerns and accountability
regarding bias, fairness, and reliability are of paramount importance.
Mechanistic interpretability aims at reverse engineering complex AI models such
as transformers. In this paper, we are pioneering the use of mechanistic
interpretability to shed some light on the inner workings of large language
models for use in financial services applications. We offer several examples of
how algorithmic tasks can be designed for compliance monitoring purposes. In
particular, we investigate GPT-2 Small's attention pattern when prompted to
identify potential violation of Fair Lending laws. Using direct logit
attribution, we study the contributions of each layer and its corresponding
attention heads to the logit difference in the residual stream. Finally, we
design clean and corrupted prompts and use activation patching as a causal
intervention method to localize our task completion components further. We
observe that the (positive) heads $10.2$ (head $2$, layer $10$), $10.7$, and
$11.3$, as well as the (negative) heads $9.6$ and $10.6$ play a significant
role in the task completion.","[{'name': 'Ashkan Golgoon'}, {'name': 'Khashayar Filom'}, {'name': 'Arjun Ravi Kannan'}]",2024-07-15T19:59:53Z
http://arxiv.org/abs/2407.11214v1,http://arxiv.org/abs/2407.11214v1,"PutnamBench: Evaluating Neural Theorem-Provers on the Putnam
  Mathematical Competition","We present PutnamBench, a new multilingual benchmark for evaluating the
ability of neural theorem-provers to solve competition mathematics problems.
PutnamBench consists of 1697 hand-constructed formalizations of 640 theorems
sourced from the William Lowell Putnam Mathematical Competition, the premier
undergraduate-level mathematics competition in North America. All the theorems
have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq
formalizations. Proving the theorems requires significant problem-solving
ability and proficiency in a broad range of topics taught in undergraduate
mathematics courses. We use PutnamBench to evaluate several established neural
and symbolic theorem-provers. These approaches can only solve a handful of the
PutnamBench problems, establishing the benchmark as a difficult open challenge
for research on neural theorem-proving. PutnamBench is available at
https://github.com/trishullab/PutnamBench.","[{'name': 'George Tsoukalas'}, {'name': 'Jasper Lee'}, {'name': 'John Jennings'}, {'name': 'Jimmy Xin'}, {'name': 'Michelle Ding'}, {'name': 'Michael Jennings'}, {'name': 'Amitayush Thakur'}, {'name': 'Swarat Chaudhuri'}]",2024-07-15T19:57:15Z
http://arxiv.org/abs/2407.11212v1,http://arxiv.org/abs/2407.11212v1,"Automated essay scoring in Arabic: a dataset and analysis of a
  BERT-based system","Automated Essay Scoring (AES) holds significant promise in the field of
education, helping educators to mark larger volumes of essays and provide
timely feedback. However, Arabic AES research has been limited by the lack of
publicly available essay data. This study introduces AR-AES, an Arabic AES
benchmark dataset comprising 2046 undergraduate essays, including gender
information, scores, and transparent rubric-based evaluation guidelines,
providing comprehensive insights into the scoring process. These essays come
from four diverse courses, covering both traditional and online exams.
Additionally, we pioneer the use of AraBERT for AES, exploring its performance
on different question types. We find encouraging results, particularly for
Environmental Chemistry and source-dependent essay questions. For the first
time, we examine the scale of errors made by a BERT-based AES system, observing
that 96.15 percent of the errors are within one point of the first human
marker's prediction, on a scale of one to five, with 79.49 percent of
predictions matching exactly. In contrast, additional human markers did not
exceed 30 percent exact matches with the first marker, with 62.9 percent within
one mark. These findings highlight the subjectivity inherent in essay grading,
and underscore the potential for current AES technology to assist human markers
to grade consistently across large classes.","[{'name': 'Rayed Ghazawi'}, {'name': 'Edwin Simpson'}]",2024-07-15T19:55:37Z
http://arxiv.org/abs/2407.11211v2,http://arxiv.org/abs/2407.11211v2,"Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer
  from Text to Image via CLIP Inversion","We introduce NOVIC, an innovative uNconstrained Open Vocabulary Image
Classifier that uses an autoregressive transformer to generatively output
classification labels as language. Leveraging the extensive knowledge of CLIP
models, NOVIC harnesses the embedding space to enable zero-shot transfer from
pure text to images. Traditional CLIP models, despite their ability for open
vocabulary classification, require an exhaustive prompt of potential class
labels, restricting their application to images of known content or context. To
address this, we propose an ""object decoder"" model that is trained on a
large-scale 92M-target dataset of templated object noun sets and LLM-generated
captions to always output the object noun in question. This effectively inverts
the CLIP text encoder and allows textual object labels to be generated directly
from image-derived embedding vectors, without requiring any a priori knowledge
of the potential content of an image. The trained decoders are tested on a mix
of manually and web-curated datasets, as well as standard image classification
benchmarks, and achieve fine-grained prompt-free prediction scores of up to
87.5%, a strong result considering the model must work for any conceivable
image and without any contextual clues.","[{'name': 'Philipp Allgeuer'}, {'name': 'Kyra Ahrens'}, {'name': 'Stefan Wermter'}]",2024-07-15T19:53:02Z
http://arxiv.org/abs/2407.11202v1,http://arxiv.org/abs/2407.11202v1,Actuation without production bias,"Phonetic production bias is the external force most commonly invoked in
computational models of sound change, despite the fact that it is not
responsible for all, or even most, sound changes. Furthermore, the existence of
production bias alone cannot account for how changes do or do not propagate
throughout a speech community. While many other factors have been invoked by
(socio)phoneticians, including but not limited to contact (between
subpopulations) and differences in social evaluation (of variants, groups, or
individuals), these are not typically modeled in computational simulations of
sound change. In this paper, we consider whether production biases have a
unique dynamics in terms of how they impact the population-level spread of
change in a setting where agents learn from multiple teachers. We show that,
while the dynamics conditioned by production bias are not unique, it is not the
case that all perturbing forces have the same dynamics: in particular, if
social weight is a function of individual teachers and the correlation between
a teacher's social weight and the extent to which they realize a production
bias is weak, change is unlikely to propagate. Nevertheless, it remains the
case that changes initiated from different sources may display a similar
dynamics. A more nuanced understanding of how population structure interacts
with individual biases can thus provide a (partial) solution to the
`non-phonologization problem'.","[{'name': 'James Kirby'}, {'name': 'Morgan Sonderegger'}]",2024-07-15T19:38:03Z
http://arxiv.org/abs/2407.11194v1,http://arxiv.org/abs/2407.11194v1,AstroMLab 1: Who Wins Astronomy Jeopardy!?,"We present a comprehensive evaluation of proprietary and open-weights large
language models using the first astronomy-specific benchmarking dataset. This
dataset comprises 4,425 multiple-choice questions curated from the Annual
Review of Astronomy and Astrophysics, covering a broad range of astrophysical
topics. Our analysis examines model performance across various astronomical
subfields and assesses response calibration, crucial for potential deployment
in research environments. Claude-3.5-Sonnet outperforms competitors by up to
4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we
observed a universal reduction in cost every 3-to-12 months to achieve similar
score in this particular astronomy benchmark. Open-source models have rapidly
improved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with
some of the best proprietary models. We identify performance variations across
topics, with non-English-focused models generally struggling more in
exoplanet-related fields, stellar astrophysics, and instrumentation related
questions. These challenges likely stem from less abundant training data,
limited historical context, and rapid recent developments in these areas. This
pattern is observed across both open-weights and proprietary models, with
regional dependencies evident, highlighting the impact of training data
diversity on model performance in specialized scientific domains.
Top-performing models demonstrate well-calibrated confidence, with correlations
above 0.9 between confidence and correctness, though they tend to be slightly
underconfident. The development for fast, low-cost inference of open-weights
models presents new opportunities for affordable deployment in astronomy. The
rapid progress observed suggests that LLM-driven research in astronomy may
become feasible in the near future.","[{'name': 'Yuan-Sen Ting'}, {'name': 'Tuan Dung Nguyen'}, {'name': 'Tirthankar Ghosal'}, {'name': 'Rui Pan'}, {'name': 'Hardik Arora'}, {'name': 'Zechang Sun'}, {'name': 'Tijmen de Haan'}, {'name': 'Nesar Ramachandra'}, {'name': 'Azton Wells'}, {'name': 'Sandeep Madireddy'}, {'name': 'Alberto Accomazzi'}]",2024-07-15T19:28:14Z
http://arxiv.org/abs/2407.11186v2,http://arxiv.org/abs/2407.11186v2,"FarsInstruct: Empowering Large Language Models for Persian Instruction
  Understanding","Instruction-tuned large language models, such as T0, have demonstrated
remarkable capabilities in following instructions across various domains.
However, their proficiency remains notably deficient in many low-resource
languages. To address this challenge, we introduce FarsInstruct: a
comprehensive instruction dataset designed to enhance the instruction-following
ability of large language models specifically for the Persian language, a
significant yet underrepresented language globally. FarsInstruct encompasses a
wide range of task types and datasets, each containing a mix of straightforward
to complex manual written instructions, as well as translations from Public
Pool of Prompts, ensuring a rich linguistic and cultural representation.
Furthermore, we introduce Co-CoLA, a framework designed to enhance the
multi-task adaptability of LoRA-tuned models. Through extensive experimental
analyses, our study showcases the effectiveness of FarsInstruct dataset coupled
with training by Co-CoLA framework, in improving the performance of large
language models within the Persian context. As of the current writing,
FarsInstruct comprises more than 200 templates across 21 distinct datasets, and
we intend to update it consistently, thus augmenting its applicability.","[{'name': 'Hojjat Mokhtarabadi'}, {'name': 'Ziba Zamani'}, {'name': 'Abbas Maazallahi'}, {'name': 'Hossein Manshaei'}]",2024-07-15T19:17:31Z
http://arxiv.org/abs/2408.00769v1,http://arxiv.org/abs/2408.00769v1,"Decoding AI and Human Authorship: Nuances Revealed Through NLP and
  Statistical Analysis","This research explores the nuanced differences in texts produced by AI and
those written by humans, aiming to elucidate how language is expressed
differently by AI and humans. Through comprehensive statistical data analysis,
the study investigates various linguistic traits, patterns of creativity, and
potential biases inherent in human-written and AI- generated texts. The
significance of this research lies in its contribution to understanding AI's
creative capabilities and its impact on literature, communication, and societal
frameworks. By examining a meticulously curated dataset comprising 500K essays
spanning diverse topics and genres, generated by LLMs, or written by humans,
the study uncovers the deeper layers of linguistic expression and provides
insights into the cognitive processes underlying both AI and human-driven
textual compositions. The analysis revealed that human-authored essays tend to
have a higher total word count on average than AI-generated essays but have a
shorter average word length compared to AI- generated essays, and while both
groups exhibit high levels of fluency, the vocabulary diversity of Human
authored content is higher than AI generated content. However, AI- generated
essays show a slightly higher level of novelty, suggesting the potential for
generating more original content through AI systems. The paper addresses
challenges in assessing the language generation capabilities of AI models and
emphasizes the importance of datasets that reflect the complexities of human-AI
collaborative writing. Through systematic preprocessing and rigorous
statistical analysis, this study offers valuable insights into the evolving
landscape of AI-generated content and informs future developments in natural
language processing (NLP).","[{'name': 'Mayowa Akinwande'}, {'name': 'Oluwaseyi Adeliyi'}, {'name': 'Toyyibat Yussuph'}]",2024-07-15T18:09:03Z
http://arxiv.org/abs/2407.11144v1,http://arxiv.org/abs/2407.11144v1,"YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language
  Parallel Corpus","Even for better-studied sign languages like American Sign Language (ASL),
data is the bottleneck for machine learning research. The situation is worse
yet for the many other sign languages used by Deaf/Hard of Hearing communities
around the world. In this paper, we present YouTube-SL-25, a large-scale,
open-domain multilingual corpus of sign language videos with seemingly
well-aligned captions drawn from YouTube. With >3000 hours of videos across >25
sign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest
parallel sign language dataset to date, and c) the first or largest parallel
dataset for many of its component languages. We provide baselines for
sign-to-text tasks using a unified multilingual multitask model based on T5 and
report scores on benchmarks across 4 sign languages. The results demonstrate
that multilingual transfer benefits both higher- and lower-resource sign
languages within YouTube-SL-25.","[{'name': 'Garrett Tanzer'}, {'name': 'Biao Zhang'}]",2024-07-15T18:08:34Z
http://arxiv.org/abs/2407.10969v3,http://arxiv.org/abs/2407.10969v3,Q-Sparse: All Large Language Models can be Fully Sparsely-Activated,"We introduce, Q-Sparse, a simple yet effective approach to training
sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity
of activations in LLMs which can bring significant efficiency gains in
inference. This is achieved by applying top-K sparsification to the activations
and the straight-through-estimator to the training. We also introduce Block
Q-Sparse for batch training and inference. The key results from this work are,
(1) Q-Sparse can achieve results comparable to those of baseline LLMs while
being much more efficient at inference time; (2) We present an
inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is
effective in different settings, including training-from-scratch,
continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for
both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the
synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the
cornerstone and a clear path to revolutionize the efficiency, including cost
and energy consumption, of future LLMs.","[{'name': 'Hongyu Wang'}, {'name': 'Shuming Ma'}, {'name': 'Ruiping Wang'}, {'name': 'Furu Wei'}]",2024-07-15T17:59:29Z
http://arxiv.org/abs/2407.10964v1,http://arxiv.org/abs/2407.10964v1,"No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen
  Representations","This paper introduces FUNGI, Features from UNsupervised GradIents, a method
to enhance the features of vision encoders by leveraging self-supervised
gradients. Our method is simple: given any pretrained model, we first compute
gradients from various self-supervised objectives for each input. These are
projected to a lower dimension and then concatenated with the model's
embedding. The resulting features are evaluated on k-nearest neighbor
classification over 11 datasets from vision, 5 from natural language
processing, and 2 from audio. Across backbones spanning various sizes and
pretraining strategies, FUNGI features provide consistent performance
improvements over the embeddings. We also show that using FUNGI features can
benefit linear classification and image retrieval, and that they significantly
improve the retrieval-based in-context scene understanding abilities of
pretrained models, for example improving upon DINO by +17% for semantic
segmentation - without any training.","[{'name': 'Walter Simoncini'}, {'name': 'Spyros Gidaris'}, {'name': 'Andrei Bursuc'}, {'name': 'Yuki M. Asano'}]",2024-07-15T17:58:42Z
http://arxiv.org/abs/2407.10960v1,http://arxiv.org/abs/2407.10960v1,Fast Matrix Multiplications for Lookup Table-Quantized LLMs,"The deployment of large language models (LLMs) is often constrained by memory
bandwidth, where the primary bottleneck is the cost of transferring model
parameters from the GPU's global memory to its registers. When coupled with
custom kernels that fuse the dequantization and matmul operations, weight-only
quantization can thus enable faster inference by reducing the amount of memory
movement. However, developing high-performance kernels for weight-quantized
LLMs presents substantial challenges, especially when the weights are
compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,
lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup
table engine for LUT-quantized LLMs, which uses offline restructuring of the
quantized weight matrix to minimize bit manipulations associated with
unpacking, and vectorization and duplication of the lookup table to mitigate
shared memory bandwidth constraints. At batch sizes < 32 and quantization group
size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster
than existing GEMM kernels. As an application of FLUTE, we explore a simple
extension to lookup table-based NormalFloat quantization and apply it to
quantize LLaMA3 to various configurations, obtaining competitive quantization
performance against strong baselines while obtaining an end-to-end throughput
increase of 1.5 to 2 times.","[{'name': 'Han Guo'}, {'name': 'William Brandon'}, {'name': 'Radostin Cholakov'}, {'name': 'Jonathan Ragan-Kelley'}, {'name': 'Eric P. Xing'}, {'name': 'Yoon Kim'}]",2024-07-15T17:55:42Z
http://arxiv.org/abs/2407.10956v1,http://arxiv.org/abs/2407.10956v1,"Spider2-V: How Far Are Multimodal Agents From Automating Data Science
  and Engineering Workflows?","Data science and engineering workflows often span multiple stages, from
warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As
vision language models (VLMs) advance in multimodal understanding and code
generation, VLM-based agents could potentially automate these workflows by
generating SQL queries, Python code, and GUI operations. This automation can
improve the productivity of experts while democratizing access to large-scale
data analysis. In this paper, we introduce Spider2-V, the first multimodal
agent benchmark focusing on professional data science and engineering
workflows, featuring 494 real-world tasks in authentic computer environments
and incorporating 20 enterprise-level professional applications. These tasks,
derived from real-world use cases, evaluate the ability of a multimodal agent
to perform data-related tasks by writing code and managing the GUI in
enterprise data software systems. To balance realistic simulation with
evaluation simplicity, we devote significant effort to developing automatic
configurations for task setup and carefully crafting evaluation metrics for
each task. Furthermore, we supplement multimodal agents with comprehensive
documents of these enterprise data software systems. Our empirical evaluation
reveals that existing state-of-the-art LLM/VLM-based agents do not reliably
automate full data workflows (14.0% success). Even with step-by-step guidance,
these agents still underperform in tasks that require fine-grained,
knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted
workspaces (10.6%). We hope that Spider2-V paves the way for autonomous
multimodal agents to transform the automation of data science and engineering
workflow. Our code and data are available at https://spider2-v.github.io.","[{'name': 'Ruisheng Cao'}, {'name': 'Fangyu Lei'}, {'name': 'Haoyuan Wu'}, {'name': 'Jixuan Chen'}, {'name': 'Yeqiao Fu'}, {'name': 'Hongcheng Gao'}, {'name': 'Xinzhuang Xiong'}, {'name': 'Hanchong Zhang'}, {'name': 'Yuchen Mao'}, {'name': 'Wenjing Hu'}, {'name': 'Tianbao Xie'}, {'name': 'Hongshen Xu'}, {'name': 'Danyang Zhang'}, {'name': 'Sida Wang'}, {'name': 'Ruoxi Sun'}, {'name': 'Pengcheng Yin'}, {'name': 'Caiming Xiong'}, {'name': 'Ansong Ni'}, {'name': 'Qian Liu'}, {'name': 'Victor Zhong'}, {'name': 'Lu Chen'}, {'name': 'Kai Yu'}, {'name': 'Tao Yu'}]",2024-07-15T17:54:37Z
http://arxiv.org/abs/2407.10953v1,http://arxiv.org/abs/2407.10953v1,"MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with
  Open-domain Information Extraction Large Language Models","The Mutual Reinforcement Effect (MRE) represents a promising avenue in
information extraction and multitasking research. Nevertheless, its
applicability has been constrained due to the exclusive availability of MRE mix
datasets in Japanese, thereby limiting comprehensive exploration by the global
research community. To address this limitation, we introduce a Multilingual MRE
mix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, and
Chinese. In this paper, we also propose a method for dataset translation
assisted by Large Language Models (LLMs), which significantly reduces the
manual annotation time required for dataset construction by leveraging LLMs to
translate the original Japanese datasets. Additionally, we have enriched the
dataset by incorporating open-domain Named Entity Recognition (NER) and
sentence classification tasks. Utilizing this expanded dataset, we developed a
unified input-output framework to train an Open-domain Information Extraction
Large Language Model (OIELLM). The OIELLM model demonstrates the capability to
effectively process novel MMM datasets, exhibiting significant improvements in
performance.","[{'name': 'Chengguang Gan'}, {'name': 'Qingyu Yin'}, {'name': 'Xinyang He'}, {'name': 'Hanjun Wei'}, {'name': 'Yunhao Liang'}, {'name': 'Younghun Lim'}, {'name': 'Shijian Wang'}, {'name': 'Hexiang Huang'}, {'name': 'Qinghao Zhang'}, {'name': 'Shiwen Ni'}, {'name': 'Tatsunori Mori'}]",2024-07-15T17:50:43Z
http://arxiv.org/abs/2407.10949v1,http://arxiv.org/abs/2407.10949v1,Representing Rule-based Chatbots with Transformers,"Transformer-based chatbots can conduct fluent, natural-sounding
conversations, but we have limited understanding of the mechanisms underlying
their behavior. Prior work has taken a bottom-up approach to understanding
Transformers by constructing Transformers for various synthetic and formal
language tasks, such as regular expressions and Dyck languages. However, it is
not obvious how to extend this approach to understand more naturalistic
conversational agents. In this work, we take a step in this direction by
constructing a Transformer that implements the ELIZA program, a classic,
rule-based chatbot. ELIZA illustrates some of the distinctive challenges of the
conversational setting, including both local pattern matching and long-term
dialog state tracking. We build on constructions from prior work -- in
particular, for simulating finite-state automata -- showing how simpler
constructions can be composed and extended to give rise to more sophisticated
behavior. Next, we train Transformers on a dataset of synthetically generated
ELIZA conversations and investigate the mechanisms the models learn. Our
analysis illustrates the kinds of mechanisms these models tend to prefer -- for
example, models favor an induction head mechanism over a more precise, position
based copying mechanism; and using intermediate generations to simulate
recurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing
an explicit connection between neural chatbots and interpretable, symbolic
mechanisms, our results offer a new setting for mechanistic analysis of
conversational agents.","[{'name': 'Dan Friedman'}, {'name': 'Abhishek Panigrahi'}, {'name': 'Danqi Chen'}]",2024-07-15T17:45:53Z
http://arxiv.org/abs/2407.10944v1,http://arxiv.org/abs/2407.10944v1,Learning from Naturally Occurring Feedback,"Human feedback data is a critical component in developing language models.
However, collecting this feedback is costly and ultimately not scalable. We
propose a scalable method for extracting feedback that users naturally include
when interacting with chat models, and leveraging it for model training. We are
further motivated by previous work that showed there are also qualitative
advantages to using naturalistic (rather than auto-generated) feedback, such as
less hallucinations and biases. We manually annotated conversation data to
confirm the presence of naturally occurring feedback in a standard corpus,
finding that as much as 30% of the chats include explicit feedback. We apply
our method to over 1M conversations to obtain hundreds of thousands of feedback
samples. Training with the extracted feedback shows significant performance
improvements over baseline models, demonstrating the efficacy of our approach
in enhancing model alignment to human preferences.","[{'name': 'Shachar Don-Yehiya'}, {'name': 'Leshem Choshen'}, {'name': 'Omri Abend'}]",2024-07-15T17:41:34Z
http://arxiv.org/abs/2407.12873v1,http://arxiv.org/abs/2407.12873v1,Evaluation of RAG Metrics for Question Answering in the Telecom Domain,"Retrieval Augmented Generation (RAG) is widely used to enable Large Language
Models (LLMs) perform Question Answering (QA) tasks in various domains.
However, RAG based on open-source LLM for specialized domains has challenges of
evaluating generated responses. A popular framework in the literature is the
RAG Assessment (RAGAS), a publicly available library which uses LLMs for
evaluation. One disadvantage of RAGAS is the lack of details of derivation of
numerical value of the evaluation metrics. One of the outcomes of this work is
a modified version of this package for few metrics (faithfulness, context
relevance, answer relevance, answer correctness, answer similarity and factual
correctness) through which we provide the intermediate outputs of the prompts
by using any LLMs. Next, we analyse the expert evaluations of the output of the
modified RAGAS package and observe the challenges of using it in the telecom
domain. We also study the effect of the metrics under correct vs. wrong
retrieval and observe that few of the metrics have higher values for correct
retrieval. We also study for differences in metrics between base embeddings and
those domain adapted via pre-training and fine-tuning. Finally, we comment on
the suitability and challenges of using these metrics for in-the-wild telecom
QA task.","[{'name': 'Sujoy Roychowdhury'}, {'name': 'Sumit Soman'}, {'name': 'H G Ranjani'}, {'name': 'Neeraj Gunda'}, {'name': 'Vansh Chhabra'}, {'name': 'Sai Krishna Bala'}]",2024-07-15T17:40:15Z
http://arxiv.org/abs/2407.10930v1,http://arxiv.org/abs/2407.10930v1,"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better
  Together","Natural Language Processing (NLP) systems are increasingly taking the form of
multi-stage pipelines involving multiple distinct language models (LMs) and
prompting strategies. Here we address the question of how to fine-tune such
systems to improve their performance. We cast this as a problem of optimizing
the underlying LM weights and the prompting strategies together, and consider a
challenging but highly realistic scenario in which we have no gold labels for
any intermediate stages in the pipeline. To address this challenge, we evaluate
approximate optimization strategies in which we bootstrap training labels for
all pipeline stages and use these to optimize the pipeline's prompts and
fine-tune its weights alternatingly. In experiments with multi-hop QA,
mathematical reasoning, and feature-based classification, we find that simple
approaches for optimizing the prompts and weights together outperform directly
optimizing weights alone and prompts alone by up to 65% and 5%, respectively,
on average across LMs and tasks. We will release our new optimizers in DSPy at
http://dspy.ai","[{'name': 'Dilara Soylu'}, {'name': 'Christopher Potts'}, {'name': 'Omar Khattab'}]",2024-07-15T17:30:31Z
http://arxiv.org/abs/2407.10920v2,http://arxiv.org/abs/2407.10920v2,Benchmarking Vision Language Models for Cultural Understanding,"Foundation models and vision-language pre-training have notably advanced
Vision Language Models (VLMs), enabling multimodal processing of visual and
linguistic data. However, their performance has been typically assessed on
general scene understanding - recognizing objects, attributes, and actions -
rather than cultural comprehension. This study introduces CulturalVQA, a visual
question-answering benchmark aimed at assessing VLM's geo-diverse cultural
understanding. We curate a collection of 2,378 image-question pairs with 1-5
answers per question representing cultures from 11 countries across 5
continents. The questions probe understanding of various facets of culture such
as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on
CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of
cultural understanding across regions, with strong cultural understanding
capabilities for North America while significantly lower performance for
Africa. We observe disparity in their performance across cultural facets too,
with clothing, rituals, and traditions seeing higher performances than food and
drink. These disparities help us identify areas where VLMs lack cultural
understanding and demonstrate the potential of CulturalVQA as a comprehensive
evaluation set for gauging VLM progress in understanding diverse cultures.","[{'name': 'Shravan Nayak'}, {'name': 'Kanishk Jain'}, {'name': 'Rabiul Awal'}, {'name': 'Siva Reddy'}, {'name': 'Sjoerd van Steenkiste'}, {'name': 'Lisa Anne Hendricks'}, {'name': 'Karolina Stańczak'}, {'name': 'Aishwarya Agrawal'}]",2024-07-15T17:21:41Z
http://arxiv.org/abs/2407.10899v1,http://arxiv.org/abs/2407.10899v1,Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis,"Effective educational measurement relies heavily on the curation of
well-designed item pools (i.e., possessing the right psychometric properties).
However, item calibration is time-consuming and costly, requiring a sufficient
number of respondents for the response process. We explore using six different
LLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)
and various combinations of them using sampling methods to produce responses
with psychometric properties similar to human answers. Results show that some
LLMs have comparable or higher proficiency in College Algebra than college
students. No single LLM mimics human respondents due to narrow proficiency
distributions, but an ensemble of LLMs can better resemble college students'
ability distribution. The item parameters calibrated by LLM-Respondents have
high correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated
counterparts, and closely resemble the parameters of the human subset (e.g.
0.02 Spearman correlation difference). Several augmentation strategies are
evaluated for their relative performance, with resampling methods proving most
effective, enhancing the Spearman correlation from 0.89 (human only) to 0.93
(augmented human).","[{'name': 'Yunting Liu'}, {'name': 'Shreya Bhandari'}, {'name': 'Zachary A. Pardos'}]",2024-07-15T16:49:26Z
http://arxiv.org/abs/2407.10855v1,http://arxiv.org/abs/2407.10855v1,Weighted Grouped Query Attention in Transformers,"The attention mechanism forms the foundational blocks for transformer
language models. Recent approaches show that scaling the model achieves
human-level performance. However, with increasing demands for scaling and
constraints on hardware memory, the inference costs of these models remain
high. To reduce the inference time, Multi-Query Attention (MQA) and
Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet
al., 2023) respectively. In this paper, we propose a variation of Grouped-Query
Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new
learnable parameters for each key and value head in the T5 decoder attention
blocks, enabling the model to take a weighted average during finetuning. Our
model achieves an average of 0.53% improvement over GQA, and the performance
converges to traditional Multi-head attention (MHA) with no additional overhead
during inference. We evaluated the introduction of these parameters and
subsequent finetuning informs the model about the grouping mechanism during
training, thereby enhancing performance. Additionally, we demonstrate the
scaling laws in our analysis by comparing the results between T5-small and
T5-base architecture.","[{'name': 'Sai Sena Chinnakonduru'}, {'name': 'Astarag Mohapatra'}]",2024-07-15T16:07:13Z
http://arxiv.org/abs/2407.10853v2,http://arxiv.org/abs/2407.10853v2,"An Actionable Framework for Assessing Bias and Fairness in Large
  Language Model Use Cases","Large language models (LLMs) can exhibit bias in a variety of ways. Such
biases can create or exacerbate unfair outcomes for certain groups within a
protected attribute, including, but not limited to sex, race, sexual
orientation, or age. This paper aims to provide a technical guide for
practitioners to assess bias and fairness risks in LLM use cases. The main
contribution of this work is a decision framework that allows practitioners to
determine which metrics to use for a specific LLM use case. To achieve this,
this study categorizes LLM bias and fairness risks, maps those risks to a
taxonomy of LLM use cases, and then formally defines various metrics to assess
each type of risk. As part of this work, several new bias and fairness metrics
are introduced, including innovative counterfactual metrics as well as metrics
based on stereotype classifiers. Instead of focusing solely on the model
itself, the sensitivity of both prompt-risk and model-risk are taken into
account by defining evaluations at the level of an LLM use case, characterized
by a model and a population of prompts. Furthermore, because all of the
evaluation metrics are calculated solely using the LLM output, the proposed
framework is highly practical and easily actionable for practitioners.",[{'name': 'Dylan Bouchard'}],2024-07-15T16:04:44Z
http://arxiv.org/abs/2407.10829v1,http://arxiv.org/abs/2407.10829v1,"BiasScanner: Automatic Detection and Classification of News Bias to
  Strengthen Democracy","The increasing consumption of news online in the 21st century coincided with
increased publication of disinformation, biased reporting, hate speech and
other unwanted Web content. We describe BiasScanner, an application that aims
to strengthen democracy by supporting news consumers with scrutinizing news
articles they are reading online. BiasScanner contains a server-side
pre-trained large language model to identify biased sentences of news articles
and a front-end Web browser plug-in. At the time of writing, BiasScanner can
identify and classify more than two dozen types of media bias at the sentence
level, making it the most fine-grained model and only deployed application
(automatic system in use) of its kind. It was implemented in a light-weight and
privacy-respecting manner, and in addition to highlighting likely biased
sentence it also provides explanations for each classification decision as well
as a summary analysis for each news article. While prior research has addressed
news bias detection, we are not aware of any work that resulted in a deployed
browser plug-in (c.f. also biasscanner.org for a Web demo).","[{'name': 'Tim Menzner'}, {'name': 'Jochen L. Leidner'}]",2024-07-15T15:42:22Z
http://arxiv.org/abs/2407.10827v1,http://arxiv.org/abs/2407.10827v1,LLM Circuit Analyses Are Consistent Across Training and Scale,"Most currently deployed large language models (LLMs) undergo continuous
training or additional finetuning. By contrast, most research into LLMs'
internal mechanisms focuses on models at one snapshot in time (the end of
pre-training), raising the question of whether their results generalize to
real-world settings. Existing studies of mechanisms over time focus on
encoder-only or toy models, which differ significantly from most deployed
models. In this study, we track how model mechanisms, operationalized as
circuits, emerge and evolve across 300 billion tokens of training in
decoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.
We find that task abilities and the functional components that support them
emerge consistently at similar token counts across scale. Moreover, although
such components may be implemented by different attention heads over time, the
overarching algorithm that they implement remains. Surprisingly, both these
algorithms and the types of components involved therein can replicate across
model scale. These results suggest that circuit analyses conducted on small
models at the end of pre-training can provide insights that still apply after
additional pre-training and over model scale.","[{'name': 'Curt Tigges'}, {'name': 'Michael Hanna'}, {'name': 'Qinan Yu'}, {'name': 'Stella Biderman'}]",2024-07-15T15:38:51Z
http://arxiv.org/abs/2407.10817v1,http://arxiv.org/abs/2407.10817v1,"Foundational Autoraters: Taming Large Language Models for Better
  Automatic Evaluation","As large language models (LLMs) advance, it becomes more challenging to
reliably evaluate their output due to the high costs of human evaluation. To
make progress towards better LLM autoraters, we introduce FLAMe, a family of
Foundational Large Autorater Models. FLAMe is trained on our large and diverse
collection of 100+ quality assessment tasks comprising 5M+ human judgments,
curated and standardized using publicly released human evaluations from
previous research. FLAMe significantly improves generalization to a wide
variety of held-out tasks, outperforming LLMs trained on proprietary data like
GPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a
powerful starting point for further downstream fine-tuning, using reward
modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our
FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative
model trained exclusively on permissively licensed data, outperforming both
GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more
computationally efficient approach using a novel tail-patch fine-tuning
strategy to optimize our FLAMe multitask mixture for reward modeling evaluation
(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring
approximately 25x less training datapoints. Overall, our FLAMe variants
outperform all popular proprietary LLM-as-a-Judge models we consider across 8
out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment
tasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals
that FLAMe is significantly less biased than these LLM-as-a-Judge models on the
CoBBLEr autorater bias benchmark, while effectively identifying high-quality
responses for code generation.","[{'name': 'Tu Vu'}, {'name': 'Kalpesh Krishna'}, {'name': 'Salaheddin Alzubi'}, {'name': 'Chris Tar'}, {'name': 'Manaal Faruqui'}, {'name': 'Yun-Hsuan Sung'}]",2024-07-15T15:33:45Z
http://arxiv.org/abs/2407.10804v1,http://arxiv.org/abs/2407.10804v1,"Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning
  and Format Alignment","Adapting general large language models (LLMs) to specialized domains presents
great challenges due to varied data distributions. This adaptation typically
requires continual pre-training on massive domain-specific corpora to
facilitate knowledge memorization, followed by training to apply this knowledge
following human instructions and preferences. However, this method may result
in inefficient knowledge memorization due to a lack of awareness of knowledge
utilization and imposes substantial demands on LLMs to simultaneously learn
knowledge utilization and format alignment with limited training samples. To
facilitate the domain adaptation of LLM, we revise this process and propose a
new domain adaptation framework including domain knowledge learning and general
format alignment, called Mix-CPT. Specifically, we first conduct a knowledge
mixture continual pre-training that concurrently focuses on knowledge
memorization and utilization, allowing for mutual reinforcement. To avoid
catastrophic forgetting during the continual pre-training process, we further
incorporate a logit swap self-distillation constraint. Subsequently, leveraging
the knowledge and capabilities acquired during continual pre-training, we
efficiently perform instruction tuning and alignment with a few general
training samples to achieve format alignment. Extensive experiments demonstrate
that our proposed Mix-CPT framework can simultaneously improve the task-solving
capabilities of LLMs on the target and general domains compared to the
traditional adaptation methods.","[{'name': 'Jinhao Jiang'}, {'name': 'Junyi Li'}, {'name': 'Wayne Xin Zhao'}, {'name': 'Yang Song'}, {'name': 'Tao Zhang'}, {'name': 'Ji-Rong Wen'}]",2024-07-15T15:20:13Z
http://arxiv.org/abs/2407.10795v1,http://arxiv.org/abs/2407.10795v1,Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping,"Decoding by contrasting layers (DoLa), is designed to improve the generation
quality of large language models (LLMs) by contrasting the prediction
probabilities between an early exit output (amateur logits) and the final
output (expert logits). However, we find that this approach does not work well
on non-English tasks. Inspired by previous interpretability work on language
transition during the model's forward pass, we discover that this issue arises
from a language mismatch between early exit output and final output. In this
work, we propose an improved contrastive decoding algorithm that is effective
for diverse languages beyond English. To obtain more helpful amateur logits, we
devise two strategies to skip a set of bottom, language-agnostic layers based
on our preliminary analysis. Experimental results on multilingual reasoning
benchmarks demonstrate that our proposed method outperforms previous
contrastive decoding baselines and substantially improves LLM's
chain-of-thought reasoning accuracy across 11 languages. The project will be
available at: https://github.com/NJUNLP/SkipLayerCD.","[{'name': 'Wenhao Zhu'}, {'name': 'Sizhe Liu'}, {'name': 'Shujian Huang'}, {'name': 'Shuaijie She'}, {'name': 'Chris Wendler'}, {'name': 'Jiajun Chen'}]",2024-07-15T15:14:01Z
http://arxiv.org/abs/2407.10794v1,http://arxiv.org/abs/2407.10794v1,"Graphusion: Leveraging Large Language Models for Scientific Knowledge
  Graph Fusion and Construction in NLP Education","Knowledge graphs (KGs) are crucial in the field of artificial intelligence
and are widely applied in downstream tasks, such as enhancing Question
Answering (QA) systems. The construction of KGs typically requires significant
effort from domain experts. Recently, Large Language Models (LLMs) have been
used for knowledge graph construction (KGC), however, most existing approaches
focus on a local perspective, extracting knowledge triplets from individual
sentences or documents. In this work, we introduce Graphusion, a zero-shot KGC
framework from free text. The core fusion module provides a global view of
triplets, incorporating entity merging, conflict resolution, and novel triplet
discovery. We showcase how Graphusion could be applied to the natural language
processing (NLP) domain and validate it in the educational scenario.
Specifically, we introduce TutorQA, a new expert-verified benchmark for graph
reasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our
evaluation demonstrates that Graphusion surpasses supervised baselines by up to
10% in accuracy on link prediction. Additionally, it achieves average scores of
2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and
relation recognition, respectively.","[{'name': 'Rui Yang'}, {'name': 'Boming Yang'}, {'name': 'Sixun Ouyang'}, {'name': 'Tianwei She'}, {'name': 'Aosong Feng'}, {'name': 'Yuang Jiang'}, {'name': 'Freddy Lecue'}, {'name': 'Jinghui Lu'}, {'name': 'Irene Li'}]",2024-07-15T15:13:49Z
http://arxiv.org/abs/2407.10793v1,http://arxiv.org/abs/2407.10793v1,"GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation
  Framework","Methods to evaluate Large Language Model (LLM) responses and detect
inconsistencies, also known as hallucinations, with respect to the provided
knowledge, are becoming increasingly important for LLM applications. Current
metrics fall short in their ability to provide explainable decisions,
systematically check all pieces of information in the response, and are often
too computationally expensive to be used in practice. We present GraphEval: a
hallucination evaluation framework based on representing information in
Knowledge Graph (KG) structures. Our method identifies the specific triples in
the KG that are prone to hallucinations and hence provides more insight into
where in the response a hallucination has occurred, if at all, than previous
methods. Furthermore, using our approach in conjunction with state-of-the-art
natural language inference (NLI) models leads to an improvement in balanced
accuracy on various hallucination benchmarks, compared to using the raw NLI
models. Lastly, we explore the use of GraphEval for hallucination correction by
leveraging the structure of the KG, a method we name GraphCorrect, and
demonstrate that the majority of hallucinations can indeed be rectified.","[{'name': 'Hannah Sansford'}, {'name': 'Nicholas Richardson'}, {'name': 'Hermina Petric Maretic'}, {'name': 'Juba Nait Saada'}]",2024-07-15T15:11:16Z
http://arxiv.org/abs/2407.10745v1,http://arxiv.org/abs/2407.10745v1,"What distinguishes conspiracy from critical narratives? A computational
  analysis of oppositional discourse","The current prevalence of conspiracy theories on the internet is a
significant issue, tackled by many computational approaches. However, these
approaches fail to recognize the relevance of distinguishing between texts
which contain a conspiracy theory and texts which are simply critical and
oppose mainstream narratives. Furthermore, little attention is usually paid to
the role of inter-group conflict in oppositional narratives. We contribute by
proposing a novel topic-agnostic annotation scheme that differentiates between
conspiracies and critical texts, and that defines span-level categories of
inter-group conflict. We also contribute with the multilingual
XAI-DisInfodemics corpus (English and Spanish), which contains a high-quality
annotation of Telegram messages related to COVID-19 (5,000 messages per
language). We also demonstrate the feasibility of an NLP-based automatization
by performing a range of experiments that yield strong baseline solutions.
Finally, we perform an analysis which demonstrates that the promotion of
intergroup conflict and the presence of violence and anger are key aspects to
distinguish between the two types of oppositional narratives, i.e., conspiracy
vs. critical.","[{'name': 'Damir Korenčić'}, {'name': 'Berta Chulvi'}, {'name': 'Xavier Bonet Casals'}, {'name': 'Alejandro Toselli'}, {'name': 'Mariona Taulé'}, {'name': 'Paolo Rosso'}]",2024-07-15T14:18:47Z
http://arxiv.org/abs/2407.10735v2,http://arxiv.org/abs/2407.10735v2,Transforming Agency. On the mode of existence of Large Language Models,"This paper investigates the ontological characterization of Large Language
Models (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we
pay special attention to their status as agents. This requires explaining in
detail the architecture, processing, and training procedures that enable LLMs
to display their capacities, and the extensions used to turn LLMs into
agent-like systems. After a systematic analysis we conclude that a LLM fails to
meet necessary and sufficient conditions for autonomous agency in the light of
embodied theories of mind: the individuality condition (it is not the product
of its own activity, it is not even directly affected by it), the normativity
condition (it does not generate its own norms or goals), and, partially the
interactional asymmetry condition (it is not the origin and sustained source of
its interaction with the environment). If not agents, then ... what are LLMs?
We argue that ChatGPT should be characterized as an interlocutor or linguistic
automaton, a library-that-talks, devoid of (autonomous) agency, but capable to
engage performatively on non-purposeful yet purpose-structured and
purpose-bounded tasks. When interacting with humans, a ""ghostly"" component of
the human-machine interaction makes it possible to enact genuine conversational
experiences with LLMs. Despite their lack of sensorimotor and biological
embodiment, LLMs textual embodiment (the training corpus) and resource-hungry
computational embodiment, significantly transform existing forms of human
agency. Beyond assisted and extended agency, the LLM-human coupling can produce
midtended forms of agency, closer to the production of intentional agency than
to the extended instrumentality of any previous technologies.","[{'name': 'Xabier E. Barandiaran'}, {'name': 'Lola S. Almendros'}]",2024-07-15T14:01:35Z
http://arxiv.org/abs/2407.10725v1,http://arxiv.org/abs/2407.10725v1,"CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated
  Responses","The rapid progress in Large Language Models (LLMs) poses potential risks such
as generating unethical content. Assessing LLMs' values can help expose their
misalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or
close-source ones like GPT-4, to identify values reflected in generated
responses. Nevertheless, these evaluators face two challenges in open-ended
value evaluation: they should align with changing human value definitions with
minimal annotation, against their own bias (adaptability), and detect varying
value expressions and scenarios robustly (generalizability). To handle these
challenges, we introduce CLAVE, a novel framework which integrates two
complementary LLMs, a large one to extract high-level value concepts from a few
human labels, leveraging its extensive knowledge and generalizability, and a
smaller one fine-tuned on such concepts to better align with human value
understanding. This dual-model approach enables calibration with any value
systems using <100 human-labeled samples per value type. Then we present
ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples
across diverse domains, covering three major value systems. We benchmark the
capabilities of 12+ popular LLM evaluators and analyze their strengths and
weaknesses. Our findings reveal that combining fine-tuned small models and
prompt-based large ones serves as a superior balance in value evaluation.","[{'name': 'Jing Yao'}, {'name': 'Xiaoyuan Yi'}, {'name': 'Xing Xie'}]",2024-07-15T13:51:37Z
http://arxiv.org/abs/2407.10718v2,http://arxiv.org/abs/2407.10718v2,"Sibyl: Simple yet Effective Agent Framework for Complex Real-world
  Reasoning","Existing agents based on large language models (LLMs) demonstrate robust
problem-solving capabilities by integrating LLMs' inherent knowledge, strong
in-context learning and zero-shot capabilities, and the use of tools combined
with intricately designed LLM invocation workflows by humans. However, these
agents still exhibit shortcomings in long-term reasoning and under-use the
potential of existing tools, leading to noticeable deficiencies in complex
real-world reasoning scenarios. To address these limitations, we introduce
Sibyl, a simple yet powerful LLM-based agent framework designed to tackle
complex reasoning tasks by efficiently leveraging a minimal set of tools.
Drawing inspiration from Global Workspace Theory, Sibyl incorporates a global
workspace to enhance the management and sharing of knowledge and conversation
history throughout the system. Furthermore, guided by Society of Mind Theory,
Sibyl implements a multi-agent debate-based jury to self-refine the final
answers, ensuring a comprehensive and balanced approach. This approach aims to
reduce system complexity while expanding the scope of problems solvable-from
matters typically resolved by humans in minutes to those requiring hours or
even days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl
has been designed with a focus on scalability and ease of debugging by
incorporating the concept of reentrancy from functional programming from its
inception, with the aim of seamless and low effort integration in other LLM
applications to improve capabilities. Our experimental results on the GAIA
benchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves
state-of-the-art performance with an average score of 34.55%, compared to other
agents based on GPT-4. We hope that Sibyl can inspire more reliable and
reusable LLM-based agent solutions to address complex real-world reasoning
tasks.","[{'name': 'Yulong Wang'}, {'name': 'Tianhao Shen'}, {'name': 'Lifeng Liu'}, {'name': 'Jian Xie'}]",2024-07-15T13:45:40Z
http://arxiv.org/abs/2407.10701v1,http://arxiv.org/abs/2407.10701v1,DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems,"Recently, there has been a growing interest among large language model (LLM)
developers in LLM-based document reading systems, which enable users to upload
their own documents and pose questions related to the document contents, going
beyond simple reading comprehension tasks. Consequently, these systems have
been carefully designed to tackle challenges such as file parsing, metadata
extraction, multi-modal information understanding and long-context reading.
However, no current benchmark exists to evaluate their performance in such
scenarios, where a raw file and questions are provided as input, and a
corresponding response is expected as output. In this paper, we introduce
DocBench, a new benchmark designed to evaluate LLM-based document reading
systems. Our benchmark involves a meticulously crafted process, including the
recruitment of human annotators and the generation of synthetic questions. It
includes 229 real documents and 1,102 questions, spanning across five different
domains and four major types of questions. We evaluate both proprietary
LLM-based systems accessible via web interfaces or APIs, and a parse-then-read
pipeline employing open-source LLMs. Our evaluations reveal noticeable gaps
between existing LLM-based document reading systems and human performance,
underscoring the challenges of developing proficient systems. To summarize,
DocBench aims to establish a standardized benchmark for evaluating LLM-based
document reading systems under diverse real-world scenarios, thereby guiding
future advancements in this research area.","[{'name': 'Anni Zou'}, {'name': 'Wenhao Yu'}, {'name': 'Hongming Zhang'}, {'name': 'Kaixin Ma'}, {'name': 'Deng Cai'}, {'name': 'Zhuosheng Zhang'}, {'name': 'Hai Zhao'}, {'name': 'Dong Yu'}]",2024-07-15T13:17:42Z
http://arxiv.org/abs/2407.10691v1,http://arxiv.org/abs/2407.10691v1,"$\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific
  Domain through Complementary Granularity","Recent studies show the growing significance of document retrieval in the
generation of LLMs, i.e., RAG, within the scientific domain by bridging their
knowledge gap. However, dense retrievers often struggle with domain-specific
retrieval and complex query-document relationships, particularly when query
segments correspond to various parts of a document. To alleviate such prevalent
challenges, this paper introduces $\texttt{MixGR}$, which improves dense
retrievers' awareness of query-document matching across various levels of
granularity in queries and documents using a zero-shot approach.
$\texttt{MixGR}$ fuses various metrics based on these granularities to a united
score that reflects a comprehensive query-document similarity. Our experiments
demonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by
24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,
respectively, averaged on queries containing multiple subqueries from five
scientific retrieval datasets. Moreover, the efficacy of two downstream
scientific question-answering tasks highlights the advantage of
$\texttt{MixGR}$to boost the application of LLMs in the scientific domain.","[{'name': 'Fengyu Cai'}, {'name': 'Xinran Zhao'}, {'name': 'Tong Chen'}, {'name': 'Sihao Chen'}, {'name': 'Hongming Zhang'}, {'name': 'Iryna Gurevych'}, {'name': 'Heinz Koeppl'}]",2024-07-15T13:04:09Z
http://arxiv.org/abs/2407.10671v3,http://arxiv.org/abs/2407.10671v3,Qwen2 Technical Report,"This report introduces the Qwen2 series, the latest addition to our large
language models and large multimodal models. We release a comprehensive suite
of foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base
language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1
on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2
demonstrates robust multilingual capabilities, proficient in approximately 30
languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,
Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and
global reach.
  To foster community innovation and accessibility, we have made the Qwen2
model weights openly available on Hugging Face and ModelScope, and the
supplementary materials including example code on GitHub. These platforms also
include resources for quantization, fine-tuning, and deployment, facilitating a
wide range of applications and research endeavors.","[{'name': 'An Yang'}, {'name': 'Baosong Yang'}, {'name': 'Binyuan Hui'}, {'name': 'Bo Zheng'}, {'name': 'Bowen Yu'}, {'name': 'Chang Zhou'}, {'name': 'Chengpeng Li'}, {'name': 'Chengyuan Li'}, {'name': 'Dayiheng Liu'}, {'name': 'Fei Huang'}, {'name': 'Guanting Dong'}, {'name': 'Haoran Wei'}, {'name': 'Huan Lin'}, {'name': 'Jialong Tang'}, {'name': 'Jialin Wang'}, {'name': 'Jian Yang'}, {'name': 'Jianhong Tu'}, {'name': 'Jianwei Zhang'}, {'name': 'Jianxin Ma'}, {'name': 'Jianxin Yang'}, {'name': 'Jin Xu'}, {'name': 'Jingren Zhou'}, {'name': 'Jinze Bai'}, {'name': 'Jinzheng He'}, {'name': 'Junyang Lin'}, {'name': 'Kai Dang'}, {'name': 'Keming Lu'}, {'name': 'Keqin Chen'}, {'name': 'Kexin Yang'}, {'name': 'Mei Li'}, {'name': 'Mingfeng Xue'}, {'name': 'Na Ni'}, {'name': 'Pei Zhang'}, {'name': 'Peng Wang'}, {'name': 'Ru Peng'}, {'name': 'Rui Men'}, {'name': 'Ruize Gao'}, {'name': 'Runji Lin'}, {'name': 'Shijie Wang'}, {'name': 'Shuai Bai'}, {'name': 'Sinan Tan'}, {'name': 'Tianhang Zhu'}, {'name': 'Tianhao Li'}, {'name': 'Tianyu Liu'}, {'name': 'Wenbin Ge'}, {'name': 'Xiaodong Deng'}, {'name': 'Xiaohuan Zhou'}, {'name': 'Xingzhang Ren'}, {'name': 'Xinyu Zhang'}, {'name': 'Xipin Wei'}, {'name': 'Xuancheng Ren'}, {'name': 'Xuejing Liu'}, {'name': 'Yang Fan'}, {'name': 'Yang Yao'}, {'name': 'Yichang Zhang'}, {'name': 'Yu Wan'}, {'name': 'Yunfei Chu'}, {'name': 'Yuqiong Liu'}, {'name': 'Zeyu Cui'}, {'name': 'Zhenru Zhang'}, {'name': 'Zhifang Guo'}, {'name': 'Zhihao Fan'}]",2024-07-15T12:35:42Z
http://arxiv.org/abs/2407.10670v1,http://arxiv.org/abs/2407.10670v1,"Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for
  Improved Quality and Efficiency in RAG Systems","Retrieval-augmented generation (RAG) techniques leverage the in-context
learning capabilities of large language models (LLMs) to produce more accurate
and relevant responses. Originating from the simple 'retrieve-then-read'
approach, the RAG framework has evolved into a highly flexible and modular
paradigm. A critical component, the Query Rewriter module, enhances knowledge
retrieval by generating a search-friendly query. This method aligns input
questions more closely with the knowledge base. Our research identifies
opportunities to enhance the Query Rewriter module to Query Rewriter+ by
generating multiple queries to overcome the Information Plateaus associated
with a single query and by rewriting questions to eliminate Ambiguity, thereby
clarifying the underlying intent. We also find that current RAG systems exhibit
issues with Irrelevant Knowledge; to overcome this, we propose the Knowledge
Filter. These two modules are both based on the instruction-tuned Gemma-2B
model, which together enhance response quality. The final identified issue is
Redundant Retrieval; we introduce the Memory Knowledge Reservoir and the
Retriever Trigger to solve this. The former supports the dynamic expansion of
the RAG system's knowledge base in a parameter-free manner, while the latter
optimizes the cost for accessing external knowledge, thereby improving resource
utilization and response efficiency. These four RAG modules synergistically
improve the response quality and efficiency of the RAG system. The
effectiveness of these modules has been validated through experiments and
ablation studies across six common QA datasets. The source code can be accessed
at https://github.com/Ancientshi/ERM4.","[{'name': 'Yunxiao Shi'}, {'name': 'Xing Zi'}, {'name': 'Zijing Shi'}, {'name': 'Haimin Zhang'}, {'name': 'Qiang Wu'}, {'name': 'Min Xu'}]",2024-07-15T12:35:00Z
http://arxiv.org/abs/2407.10657v2,http://arxiv.org/abs/2407.10657v2,An Empirical Study of Validating Synthetic Data for Formula Generation,"Large language models (LLMs) can be leveraged to help with writing formulas
in spreadsheets, but resources on these formulas are scarce, impacting both the
base performance of pre-trained models and limiting the ability to fine-tune
them. Given a corpus of formulas, we can use a(nother) model to generate
synthetic natural language utterances for fine-tuning. However, it is important
to validate whether the NL generated by the LLM is indeed accurate to be
beneficial for fine-tuning. In this paper, we provide empirical results on the
impact of validating these synthetic training examples with surrogate
objectives that evaluate the accuracy of the synthetic annotations. We
demonstrate that validation improves performance over raw data across four
models (2 open and 2 closed weight). Interestingly, we show that although
validation tends to prune more challenging examples, it increases the
complexity of problems that models can solve after being fine-tuned on
validated data.","[{'name': 'Usneek Singh'}, {'name': 'José Cambronero'}, {'name': 'Sumit Gulwani'}, {'name': 'Aditya Kanade'}, {'name': 'Anirudh Khatry'}, {'name': 'Vu Le'}, {'name': 'Mukul Singh'}, {'name': 'Gust Verbruggen'}]",2024-07-15T12:16:33Z
http://arxiv.org/abs/2407.12872v1,http://arxiv.org/abs/2407.12872v1,Evaluating Large Language Models with fmeval,"fmeval is an open source library to evaluate large language models (LLMs) in
a range of tasks. It helps practitioners evaluate their model for task
performance and along multiple responsible AI dimensions. This paper presents
the library and exposes its underlying design principles: simplicity, coverage,
extensibility and performance. We then present how these were implemented in
the scientific and engineering choices taken when developing fmeval. A case
study demonstrates a typical use case for the library: picking a suitable model
for a question answering task. We close by discussing limitations and further
work in the development of the library. fmeval can be found at
https://github.com/aws/fmeval.","[{'name': 'Pola Schwöbel'}, {'name': 'Luca Franceschi'}, {'name': 'Muhammad Bilal Zafar'}, {'name': 'Keerthan Vasist'}, {'name': 'Aman Malhotra'}, {'name': 'Tomer Shenhar'}, {'name': 'Pinal Tailor'}, {'name': 'Pinar Yilmaz'}, {'name': 'Michael Diamond'}, {'name': 'Michele Donini'}]",2024-07-15T12:15:08Z
http://arxiv.org/abs/2407.10645v1,http://arxiv.org/abs/2407.10645v1,"Prompt Selection Matters: Enhancing Text Annotations for Social Sciences
  with Large Language Models","Large Language Models have recently been applied to text annotation tasks
from social sciences, equalling or surpassing the performance of human workers
at a fraction of the cost. However, no inquiry has yet been made on the impact
of prompt selection on labelling accuracy. In this study, we show that
performance greatly varies between prompts, and we apply the method of
automatic prompt optimization to systematically craft high quality prompts. We
also provide the community with a simple, browser-based implementation of the
method at https://prompt-ultra.github.io/ .","[{'name': 'Louis Abraham'}, {'name': 'Charles Arnal'}, {'name': 'Antoine Marie'}]",2024-07-15T12:04:32Z
http://arxiv.org/abs/2407.10629v1,http://arxiv.org/abs/2407.10629v1,Balancing the Scales: Reinforcement Learning for Fair Classification,"Fairness in classification tasks has traditionally focused on bias removal
from neural representations, but recent trends favor algorithmic methods that
embed fairness into the training process. These methods steer models towards
fair performance, preventing potential elimination of valuable information that
arises from representation manipulation. Reinforcement Learning (RL), with its
capacity for learning through interaction and adjusting reward functions to
encourage desired behaviors, emerges as a promising tool in this domain. In
this paper, we explore the usage of RL to address bias in imbalanced
classification by scaling the reward function to mitigate bias. We employ the
contextual multi-armed bandit framework and adapt three popular RL algorithms
to suit our objectives, demonstrating a novel approach to mitigating bias.","[{'name': 'Leon Eshuijs'}, {'name': 'Shihan Wang'}, {'name': 'Antske Fokkens'}]",2024-07-15T11:28:16Z
http://arxiv.org/abs/2407.10603v1,http://arxiv.org/abs/2407.10603v1,"Leave No Knowledge Behind During Knowledge Distillation: Towards
  Practical and Effective Knowledge Distillation for Code-Switching ASR Using
  Realistic Data","Recent advances in automatic speech recognition (ASR) often rely on large
speech foundation models for generating high-quality transcriptions. However,
these models can be impractical due to limited computing resources. The
situation is even more severe in terms of more realistic or difficult
scenarios, such as code-switching ASR (CS-ASR). To address this, we present a
framework for developing more efficient models for CS-ASR through knowledge
distillation using realistic speech-only data. Our proposed method, Leave No
Knowledge Behind During Knowledge Distillation (K$^2$D), leverages both the
teacher model's knowledge and additional insights from a small auxiliary model.
We evaluate our approach on two in-domain and two out-domain datasets,
demonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled
realistic data, we have successfully obtained a 2-time smaller model with
5-time faster generation speed while outperforming the baseline methods and the
teacher model on all the testing sets. We have made our model publicly
available on Hugging Face
(https://huggingface.co/andybi7676/k2d-whisper.zh-en).","[{'name': 'Liang-Hsuan Tseng'}, {'name': 'Zih-Ching Chen'}, {'name': 'Wei-Shun Chang'}, {'name': 'Cheng-Kuang Lee'}, {'name': 'Tsung-Ren Huang'}, {'name': 'Hung-yi Lee'}]",2024-07-15T10:25:14Z
http://arxiv.org/abs/2407.12871v1,http://arxiv.org/abs/2407.12871v1,"MetaTool: Facilitating Large Language Models to Master Tools with
  Meta-task Augmentation","Utilizing complex tools with Large Language Models (LLMs) is a critical
component for grounding AI agents in various real-world scenarios. The core
challenge of manipulating tools lies in understanding their usage and
functionality. The prevailing approach involves few-shot prompting with
demonstrations or fine-tuning on expert trajectories. However, for complex
tools and tasks, mere in-context demonstrations may fail to cover sufficient
knowledge. Training-based methods are also constrained by the high cost of
dataset construction and limited generalizability. In this paper, we introduce
a new tool learning methodology (MetaTool) that is generalizable for mastering
any reusable toolset. Our approach includes a self-supervised data augmentation
technique that enables LLMs to gain a comprehensive understanding of various
tools, thereby improving their ability to complete tasks effectively. We
develop a series of meta-tasks that involve predicting masked factors of tool
execution. These self-supervised tasks enable the automatic generation of
high-quality QA data concerning tool comprehension. By incorporating meta-task
data into the instruction tuning process, the proposed MetaTool model achieves
significant superiority to open-source models and is comparable to
GPT-4/GPT-3.5 on multiple tool-oriented tasks.","[{'name': 'Xiaohan Wang'}, {'name': 'Dian Li'}, {'name': 'Yilin Zhao'}, {'name': 'Sinbadliu'}, {'name': 'Hui Wang'}]",2024-07-15T10:15:41Z
http://arxiv.org/abs/2407.10582v1,http://arxiv.org/abs/2407.10582v1,"Boosting Zero-Shot Crosslingual Performance using LLM-Based
  Augmentations with Effective Data Selection","Large language models (LLMs) are very proficient text generators. We leverage
this capability of LLMs to generate task-specific data via zero-shot prompting
and promote cross-lingual transfer for low-resource target languages. Given
task-specific data in a source language and a teacher model trained on this
data, we propose using this teacher to label LLM generations and employ a set
of simple data selection strategies that use the teacher's label probabilities.
Our data selection strategies help us identify a representative subset of
diverse generations that help boost zero-shot accuracies while being efficient,
in comparison to using all the LLM generations (without any subset selection).
We also highlight other important design choices that affect cross-lingual
performance such as the use of translations of source data and what labels are
best to use for the LLM generations. We observe significant performance gains
across sentiment analysis and natural language inference tasks (of up to a
maximum of 7.13 absolute points and 1.5 absolute points on average) across a
number of target languages (Hindi, Marathi, Urdu, Swahili) and domains.","[{'name': 'Barah Fazili'}, {'name': 'Ashish Sunil Agrawal'}, {'name': 'Preethi Jyothi'}]",2024-07-15T10:00:22Z
http://arxiv.org/abs/2407.10554v1,http://arxiv.org/abs/2407.10554v1,"Beyond Generative Artificial Intelligence: Roadmap for Natural Language
  Generation","Generative Artificial Intelligence has grown exponentially as a result of
Large Language Models (LLMs). This has been possible because of the impressive
performance of deep learning methods created within the field of Natural
Language Processing (NLP) and its subfield Natural Language Generation (NLG),
which is the focus of this paper. Within the growing LLM family are the popular
GPT-4, Bard and more specifically, tools such as ChatGPT have become a
benchmark for other LLMs when solving most of the tasks involved in NLG
research. This scenario poses new questions about the next steps for NLG and
how the field can adapt and evolve to deal with new challenges in the era of
LLMs. To address this, the present paper conducts a review of a representative
sample of surveys recently published in NLG. By doing so, we aim to provide the
scientific community with a research roadmap to identify which NLG aspects are
still not suitably addressed by LLMs, as well as suggest future lines of
research that should be addressed going forward.","[{'name': 'María Miró Maestre'}, {'name': 'Iván Martínez-Murillo'}, {'name': 'Tania J. Martin'}, {'name': 'Borja Navarro-Colorado'}, {'name': 'Antonio Ferrández'}, {'name': 'Armando Suárez Cueto'}, {'name': 'Elena Lloret'}]",2024-07-15T09:07:07Z
http://arxiv.org/abs/2407.10510v1,http://arxiv.org/abs/2407.10510v1,"TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription
  Prediction","Traditional Chinese medicine (TCM) relies on specific combinations of herbs
in prescriptions to treat symptoms and signs, a practice that spans thousands
of years. Predicting TCM prescriptions presents a fascinating technical
challenge with practical implications. However, this task faces limitations due
to the scarcity of high-quality clinical datasets and the intricate
relationship between symptoms and herbs. To address these issues, we introduce
DigestDS, a new dataset containing practical medical records from experienced
experts in digestive system diseases. We also propose a method, TCM-FTP (TCM
Fine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs)
through supervised fine-tuning on DigestDS. Additionally, we enhance
computational efficiency using a low-rank adaptation technique. TCM-FTP also
incorporates data augmentation by permuting herbs within prescriptions,
capitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves
an F1-score of 0.8031, surpassing previous methods significantly. Furthermore,
it demonstrates remarkable accuracy in dosage prediction, achieving a
normalized mean square error of 0.0604. In contrast, LLMs without fine-tuning
perform poorly. Although LLMs have shown capabilities on a wide range of tasks,
this work illustrates the importance of fine-tuning for TCM prescription
prediction, and we have proposed an effective way to do that.","[{'name': 'Xingzhi Zhou'}, {'name': 'Xin Dong'}, {'name': 'Chunhao Li'}, {'name': 'Yuning Bai'}, {'name': 'Yulong Xu'}, {'name': 'Ka Chun Cheung'}, {'name': 'Simon See'}, {'name': 'Xinpeng Song'}, {'name': 'Runshun Zhang'}, {'name': 'Xuezhong Zhou'}, {'name': 'Nevin L. Zhang'}]",2024-07-15T08:06:37Z
http://arxiv.org/abs/2407.10499v2,http://arxiv.org/abs/2407.10499v2,CIBench: Evaluating Your LLMs with a Code Interpreter Plugin,"While LLM-Based agents, which use external tools to solve complex problems,
have made significant progress, benchmarking their ability is challenging,
thereby hindering a clear understanding of their limitations. In this paper, we
propose an interactive evaluation framework, named CIBench, to comprehensively
assess LLMs' ability to utilize code interpreters for data science tasks. Our
evaluation framework includes an evaluation dataset and two evaluation modes.
The evaluation dataset is constructed using an LLM-human cooperative approach
and simulates an authentic workflow by leveraging consecutive and interactive
IPython sessions. The two evaluation modes assess LLMs' ability with and
without human assistance. We conduct extensive experiments to analyze the
ability of 24 LLMs on CIBench and provide valuable insights for future LLMs in
code interpreter utilization.","[{'name': 'Songyang Zhang'}, {'name': 'Chuyu Zhang'}, {'name': 'Yingfan Hu'}, {'name': 'Haowen Shen'}, {'name': 'Kuikun Liu'}, {'name': 'Zerun Ma'}, {'name': 'Fengzhe Zhou'}, {'name': 'Wenwei Zhang'}, {'name': 'Xuming He'}, {'name': 'Dahua Lin'}, {'name': 'Kai Chen'}]",2024-07-15T07:43:55Z
http://arxiv.org/abs/2407.10490v1,http://arxiv.org/abs/2407.10490v1,Learning Dynamics of LLM Finetuning,"Learning dynamics, which describes how the learning of specific training
examples influences the model's prediction of other examples, give us a
powerful tool for understanding the behavior of deep learning systems. We study
the learning dynamics of large language models during finetuning, by analyzing
the step-wise decomposition and accumulated influence among different
responses. Our framework allows a uniform interpretation of many interesting
observations about the training of popular algorithms for both instruction
tuning and preference tuning. The analysis not only explains where the benefits
of these methods come from but also inspires a simple, effective method to
further improve the alignment performance. Code for experiments is available at
https://github.com/Joshua-Ren/Learning_dynamics_LLM.","[{'name': 'Yi Ren'}, {'name': 'Danica J. Sutherland'}]",2024-07-15T07:30:28Z
http://arxiv.org/abs/2407.10488v1,http://arxiv.org/abs/2407.10488v1,How and where does CLIP process negation?,"Various benchmarks have been proposed to test linguistic understanding in
pre-trained vision \& language (VL) models. Here we build on the existence task
from the VALSE benchmark (Parcalabescu et al, 2022) which we use to test
models' understanding of negation, a particularly interesting issue for
multimodal models. However, while such VL benchmarks are useful for measuring
model performance, they do not reveal anything about the internal processes
through which these models arrive at their outputs in such visio-linguistic
tasks. We take inspiration from the growing literature on model
interpretability to explain the behaviour of VL models on the understanding of
negation. Specifically, we approach these questions through an in-depth
analysis of the text encoder in CLIP (Radford et al, 2021), a highly
influential VL model. We localise parts of the encoder that process negation
and analyse the role of attention heads in this task. Our contributions are
threefold. We demonstrate how methods from the language model interpretability
literature (such as causal tracing) can be translated to multimodal models and
tasks; we provide concrete insights into how CLIP processes negation on the
VALSE existence task; and we highlight inherent limitations in the VALSE
dataset as a benchmark for linguistic understanding.","[{'name': 'Vincent Quantmeyer'}, {'name': 'Pablo Mosteiro'}, {'name': 'Albert Gatt'}]",2024-07-15T07:20:06Z
http://arxiv.org/abs/2407.11100v3,http://arxiv.org/abs/2407.11100v3,"Building Intelligence Identification System via Large Language Model
  Watermarking: A Survey and Beyond","Large Language Models (LLMs) are increasingly integrated into diverse
industries, posing substantial security risks due to unauthorized replication
and misuse. To mitigate these concerns, robust identification mechanisms are
widely acknowledged as an effective strategy. Identification systems for LLMs
now rely heavily on watermarking technology to manage and protect intellectual
property and ensure data security. However, previous studies have primarily
concentrated on the basic principles of algorithms and lacked a comprehensive
analysis of watermarking theory and practice from the perspective of
intelligent identification. To bridge this gap, firstly, we explore how a
robust identity recognition system can be effectively implemented and managed
within LLMs by various participants using watermarking technology. Secondly, we
propose a mathematical framework based on mutual information theory, which
systematizes the identification process to achieve more precise and customized
watermarking. Additionally, we present a comprehensive evaluation of
performance metrics for LLM watermarking, reflecting participant preferences
and advancing discussions on its identification applications. Lastly, we
outline the existing challenges in current watermarking technologies and
theoretical frameworks, and provide directional guidance to address these
challenges. Our systematic classification and detailed exposition aim to
enhance the comparison and evaluation of various methods, fostering further
research and development toward a transparent, secure, and equitable LLM
ecosystem.","[{'name': 'Xuhong Wang'}, {'name': 'Haoyu Jiang'}, {'name': 'Yi Yu'}, {'name': 'Jingru Yu'}, {'name': 'Yilun Lin'}, {'name': 'Ping Yi'}, {'name': 'Yingchun Wang'}, {'name': 'Yu Qiao'}, {'name': 'Li Li'}, {'name': 'Fei-Yue Wang'}]",2024-07-15T07:20:02Z
http://arxiv.org/abs/2407.10486v1,http://arxiv.org/abs/2407.10486v1,"IDEAL: Leveraging Infinite and Dynamic Characterizations of Large
  Language Models for Query-focused Summarization","Query-focused summarization (QFS) aims to produce summaries that answer
particular questions of interest, enabling greater user control and
personalization. With the advent of large language models (LLMs), shows their
impressive capability of textual understanding through large-scale pretraining,
which implies the great potential of extractive snippet generation. In this
paper, we systematically investigated two indispensable characteristics that
the LLMs-based QFS models should be harnessed, Lengthy Document Summarization
and Efficiently Fine-grained Query-LLM Alignment, respectively.
Correspondingly, we propose two modules called Query-aware HyperExpert and
Query-focused Infini-attention to access the aforementioned characteristics.
These innovations pave the way for broader application and accessibility in the
field of QFS technology. Extensive experiments conducted on existing QFS
benchmarks indicate the effectiveness and generalizability of the proposed
approach. Our code is publicly available at
https://github.com/DCDmllm/IDEAL_Summary.","[{'name': 'Jie Cao'}, {'name': 'Dian Jiao'}, {'name': 'Qiang Yan'}, {'name': 'Wenqiao Zhang'}, {'name': 'Siliang Tang'}, {'name': 'Yueting Zhuang'}]",2024-07-15T07:14:56Z
http://arxiv.org/abs/2407.10481v1,http://arxiv.org/abs/2407.10481v1,"SuperPADL: Scaling Language-Directed Physics-Based Control with
  Progressive Supervised Distillation","Physically-simulated models for human motion can generate high-quality
responsive character animations, often in real-time. Natural language serves as
a flexible interface for controlling these models, allowing expert and
non-expert users to quickly create and edit their animations. Many recent
physics-based animation methods, including those that use text interfaces,
train control policies using reinforcement learning (RL). However, scaling
these methods beyond several hundred motions has remained challenging.
Meanwhile, kinematic animation models are able to successfully learn from
thousands of diverse motions by leveraging supervised learning methods.
Inspired by these successes, in this work we introduce SuperPADL, a scalable
framework for physics-based text-to-motion that leverages both RL and
supervised learning to train controllers on thousands of diverse motion clips.
SuperPADL is trained in stages using progressive distillation, starting with a
large number of specialized experts using RL. These experts are then
iteratively distilled into larger, more robust policies using a combination of
reinforcement learning and supervised learning. Our final SuperPADL controller
is trained on a dataset containing over 5000 skills and runs in real time on a
consumer GPU. Moreover, our policy can naturally transition between skills,
allowing for users to interactively craft multi-stage animations. We
experimentally demonstrate that SuperPADL significantly outperforms RL-based
baselines at this large data scale.","[{'name': 'Jordan Juravsky'}, {'name': 'Yunrong Guo'}, {'name': 'Sanja Fidler'}, {'name': 'Xue Bin Peng'}]",2024-07-15T07:07:11Z
http://arxiv.org/abs/2407.10457v1,http://arxiv.org/abs/2407.10457v1,"The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore
  Non-Determinism","Current evaluations of large language models (LLMs) often overlook
non-determinism, typically focusing on a single output per example. This limits
our understanding of LLM performance variability in real-world applications.
Our study addresses this issue by exploring key questions about the performance
differences between greedy decoding and sampling, identifying benchmarks'
consistency regarding non-determinism, and examining unique model behaviors.
Through extensive experiments, we observe that greedy decoding generally
outperforms sampling methods for most evaluated tasks. We also observe
consistent performance across different LLM sizes and alignment methods, noting
that alignment can reduce sampling variance. Moreover, our best-of-N sampling
approach demonstrates that smaller LLMs can match or surpass larger models such
as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This
research shows the importance of considering non-determinism in LLM evaluations
and provides insights for future LLM development and evaluation.","[{'name': 'Yifan Song'}, {'name': 'Guoyin Wang'}, {'name': 'Sujian Li'}, {'name': 'Bill Yuchen Lin'}]",2024-07-15T06:12:17Z
http://arxiv.org/abs/2407.10456v1,http://arxiv.org/abs/2407.10456v1,Don't Throw Away Data: Better Sequence Knowledge Distillation,"A critical component in knowledge distillation is the means of coupling the
teacher and student. The predominant sequence knowledge distillation method
involves supervised learning of the student against teacher-decoded outputs,
and is exemplified by the current state of the art, which incorporates minimum
Bayes risk (MBR) decoding. In this paper we seek to integrate MBR more tightly
in distillation training, specifically by using several high scoring MBR
translations, rather than a single selected sequence, thus capturing a rich
diversity of teacher outputs. Our experiments on English to German and English
to Japanese translation show consistent improvements over strong baseline
methods for both tasks and with varying model sizes. Additionally, we conduct a
detailed analysis focusing on data efficiency and capacity curse aspects to
elucidate MBR-n and explore its further potential.","[{'name': 'Jun Wang'}, {'name': 'Eleftheria Briakou'}, {'name': 'Hamid Dadkhahi'}, {'name': 'Rishabh Agarwal'}, {'name': 'Colin Cherry'}, {'name': 'Trevor Cohn'}]",2024-07-15T06:11:18Z
http://arxiv.org/abs/2407.10453v1,http://arxiv.org/abs/2407.10453v1,Enhancing Medication Recommendation with LLM Text Representation,"Most of the existing medication recommendation models are predicted with only
structured data such as medical codes, with the remaining other large amount of
unstructured or semi-structured data underutilization. To increase the
utilization effectively, we proposed a method of enhancing medication
recommendation with Large Language Model (LLM) text representation. LLM
harnesses powerful language understanding and generation capabilities, enabling
the extraction of information from complex and lengthy unstructured data such
as clinical notes which contain complex terminology. This method can be applied
to several existing base models we selected and improve medication
recommendation performance with the combination representation of text and
medical codes experiments on two different datasets. LLM text representation
alone can even demonstrate a comparable ability to the medical code
representation alone. Overall, this is a general method that can be applied to
other models for improved recommendations.",[{'name': 'Yu-Tzu Lee'}],2024-07-15T05:51:11Z
http://arxiv.org/abs/2407.10430v1,http://arxiv.org/abs/2407.10430v1,"Expanding the Scope: Inductive Knowledge Graph Reasoning with
  Multi-Starting Progressive Propagation","Knowledge graphs (KGs) are widely acknowledged as incomplete, and new
entities are constantly emerging in the real world. Inductive KG reasoning aims
to predict missing facts for these new entities. Among existing models, graph
neural networks (GNNs) based ones have shown promising performance for this
task. However, they are still challenged by inefficient message propagation due
to the distance and scalability issues. In this paper, we propose a new
inductive KG reasoning model, MStar, by leveraging conditional message passing
neural networks (C-MPNNs). Our key insight is to select multiple query-specific
starting entities to expand the scope of progressive propagation. To propagate
query-related messages to a farther area within limited steps, we subsequently
design a highway layer to propagate information toward these selected starting
entities. Moreover, we introduce a training strategy called LinkVerify to
mitigate the impact of noisy training samples. Experimental results validate
that MStar achieves superior performance compared with state-of-the-art models,
especially for distant entities.","[{'name': 'Zhoutian Shao'}, {'name': 'Yuanning Cui'}, {'name': 'Wei Hu'}]",2024-07-15T04:16:20Z
http://arxiv.org/abs/2408.00767v1,http://arxiv.org/abs/2408.00767v1,"Quantification and Validation for Degree of Understanding in M2M
  Semantic Communications","With the development of Artificial Intelligence (AI) and Internet of Things
(IoT) technologies, network communications based on the Shannon-Nyquist theorem
gradually reveal their limitations due to the neglect of semantic information
in the transmitted content. Semantic communication (SemCom) provides a solution
for extracting information meanings from the transmitted content. The semantic
information can be successfully interpreted by a receiver with the help of a
shared knowledge base (KB). This paper proposes a two-stage hierarchical
qualification and validation model for natural language-based
machine-to-machine (M2M) SemCom. The approach can be applied in various
applications, such as autonomous driving and edge computing. In the proposed
model, we quantitatively measure the degree of understanding (DoU) between two
communication parties at the word and sentence levels. The DoU is validated and
ensured at each level before moving to the next step. The model's effectiveness
is verified through a series of experiments, and the results show that the
quantification and validation method proposed in this paper can significantly
improve the DoU of inter-machine SemCom.","[{'name': 'Linhan Xia'}, {'name': 'Jiaxin Cai'}, {'name': 'Ricky Yuen-Tan Hou'}, {'name': 'Seon-Phil Jeong'}]",2024-07-15T03:37:42Z
http://arxiv.org/abs/2407.10385v1,http://arxiv.org/abs/2407.10385v1,"By My Eyes: Grounding Multimodal Large Language Models with Sensor Data
  via Visual Prompting","Large language models (LLMs) have demonstrated exceptional abilities across
various domains. However, utilizing LLMs for ubiquitous sensing applications
remains challenging as existing text-prompt methods show significant
performance degradation when handling long sensor data sequences. We propose a
visual prompting approach for sensor data using multimodal LLMs (MLLMs). We
design a visual prompt that directs MLLMs to utilize visualized sensor data
alongside the target sensory task descriptions. Additionally, we introduce a
visualization generator that automates the creation of optimal visualizations
tailored to a given sensory task, eliminating the need for prior task-specific
knowledge. We evaluated our approach on nine sensory tasks involving four
sensing modalities, achieving an average of 10% higher accuracy than text-based
prompts and reducing token costs by 15.8x. Our findings highlight the
effectiveness and cost-efficiency of visual prompts with MLLMs for various
sensory tasks.","[{'name': 'Hyungjun Yoon'}, {'name': 'Biniyam Aschalew Tolera'}, {'name': 'Taesik Gong'}, {'name': 'Kimin Lee'}, {'name': 'Sung-Ju Lee'}]",2024-07-15T01:33:54Z
http://arxiv.org/abs/2407.10380v1,http://arxiv.org/abs/2407.10380v1,NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models,"Cognitive textual and visual reasoning tasks, such as puzzles, series, and
analogies, demand the ability to quickly reason, decipher, and evaluate
patterns both textually and spatially. While LLMs and VLMs, through extensive
training on large amounts of human-curated data, have attained a high level of
pseudo-human intelligence in some common sense reasoning tasks, they still
struggle with more complex reasoning tasks that require cognitive
understanding. In this work, we introduce a new dataset, NTSEBench, designed to
evaluate the cognitive multi-modal reasoning and problem-solving skills of
large models. The dataset comprises 2,728 multiple-choice questions comprising
of a total of 4,642 images across 26 categories sampled from the NTSE
examination conducted nationwide in India, featuring both visual and textual
general aptitude questions that do not rely on rote learning. We establish
baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a
comparison between open source and propriety models, we propose four distinct
modeling strategies to handle different modalities (text and images) in the
dataset instances.","[{'name': 'Pranshu Pandya'}, {'name': 'Agney S Talwarr'}, {'name': 'Vatsal Gupta'}, {'name': 'Tushar Kataria'}, {'name': 'Vivek Gupta'}, {'name': 'Dan Roth'}]",2024-07-15T01:21:56Z
http://arxiv.org/abs/2407.10376v1,http://arxiv.org/abs/2407.10376v1,"Large Language Model-based FMRI Encoding of Language Functions for
  Subjects with Neurocognitive Disorder","Functional magnetic resonance imaging (fMRI) is essential for developing
encoding models that identify functional changes in language-related brain
areas of individuals with Neurocognitive Disorders (NCD). While large language
model (LLM)-based fMRI encoding has shown promise, existing studies
predominantly focus on healthy, young adults, overlooking older NCD populations
and cognitive level correlations. This paper explores language-related
functional changes in older NCD adults using LLM-based fMRI encoding and brain
scores, addressing current limitations. We analyze the correlation between
brain scores and cognitive scores at both whole-brain and language-related ROI
levels. Our findings reveal that higher cognitive abilities correspond to
better brain scores, with correlations peaking in the middle temporal gyrus.
This study highlights the potential of fMRI encoding models and brain scores
for detecting early functional changes in NCD patients.","[{'name': 'Yuejiao Wang'}, {'name': 'Xianmin Gong'}, {'name': 'Lingwei Meng'}, {'name': 'Xixin Wu'}, {'name': 'Helen Meng'}]",2024-07-15T01:09:08Z
http://arxiv.org/abs/2407.10351v1,http://arxiv.org/abs/2407.10351v1,"Comparing Complex Concepts with Transformers: Matching Patent Claims
  Against Natural Language Text","A key capability in managing patent applications or a patent portfolio is
comparing claims to other text, e.g. a patent specification. Because the
language of claims is different from language used elsewhere in the patent
application or in non-patent text, this has been challenging for computer based
natural language processing. We test two new LLM-based approaches and find that
both provide substantially better performance than previously published values.
The ability to match dense information from one domain against much more
distributed information expressed in a different vocabulary may also be useful
beyond the intellectual property space.","[{'name': 'Matthias Blume'}, {'name': 'Ghobad Heidari'}, {'name': 'Christoph Hewel'}]",2024-07-14T22:31:07Z
http://arxiv.org/abs/2407.10347v1,http://arxiv.org/abs/2407.10347v1,"MambaForGCN: Enhancing Long-Range Dependency with State Space Model and
  Kolmogorov-Arnold Networks for Aspect-Based Sentiment Analysis","Aspect-based sentiment Analysis (ABSA) identifies and evaluates sentiments
toward specific aspects of entities within text, providing detailed insights
beyond overall sentiment. However, Attention mechanisms and neural network
models struggle with syntactic constraints, and the quadratic complexity of
attention mechanisms hinders their adoption for capturing long-range
dependencies between aspect and opinion words in ABSA. This complexity can lead
to the misinterpretation of irrelevant con-textual words, restricting their
effectiveness to short-range dependencies. Some studies have investigated
merging semantic and syntactic approaches but face challenges in effectively
integrating these methods. To address the above problems, we present
MambaForGCN, a novel approach to enhance short and long-range dependencies
between aspect and opinion words in ABSA. This innovative approach incorporates
syntax-based Graph Convolutional Network (SynGCN) and MambaFormer
(Mamba-Transformer) modules to encode input with dependency relations and
semantic information. The Multihead Attention (MHA) and Mamba blocks in the
MambaFormer module serve as channels to enhance the model with short and
long-range dependencies between aspect and opinion words. We also introduce the
Kolmogorov-Arnold Networks (KANs) gated fusion, an adaptively integrated
feature representation system combining SynGCN and MambaFormer representations.
Experimental results on three benchmark datasets demonstrate MambaForGCN's
effectiveness, outperforming state-of-the-art (SOTA) baseline models.","[{'name': 'Adamu Lawan'}, {'name': 'Juhua Pu'}, {'name': 'Haruna Yunusa'}, {'name': 'Aliyu Umar'}, {'name': 'Muhammad Lawan'}]",2024-07-14T22:23:07Z
http://arxiv.org/abs/2407.10303v1,http://arxiv.org/abs/2407.10303v1,"Improving Neural Biasing for Contextual Speech Recognition by Early
  Context Injection and Text Perturbation","Existing research suggests that automatic speech recognition (ASR) models can
benefit from additional contexts (e.g., contact lists, user specified
vocabulary). Rare words and named entities can be better recognized with
contexts. In this work, we propose two simple yet effective techniques to
improve context-aware ASR models. First, we inject contexts into the encoders
at an early stage instead of merely at their last layers. Second, to enforce
the model to leverage the contexts during training, we perturb the reference
transcription with alternative spellings so that the model learns to rely on
the contexts to make correct predictions. On LibriSpeech, our techniques
together reduce the rare word error rate by 60% and 25% relatively compared to
no biasing and shallow fusion, making the new state-of-the-art performance. On
SPGISpeech and a real-world dataset ConEC, our techniques also yield good
improvements over the baselines.","[{'name': 'Ruizhe Huang'}, {'name': 'Mahsa Yarmohammadi'}, {'name': 'Sanjeev Khudanpur'}, {'name': 'Daniel Povey'}]",2024-07-14T19:32:33Z
http://arxiv.org/abs/2407.10301v1,http://arxiv.org/abs/2407.10301v1,"Does Burrows' Delta really confirm that Rowling and Galbraith are the
  same author?","The stylo package includes a frequency table that can be used to calculate
distances between texts and thus independently solve the problem of attribution
of The Cuckoo's Calling, a novel that J.K. Rowling said she wrote. However, the
set of texts for this table is very vulnerable to criticism. The authors there
are not modern, they wrote in a different genre. I set out to test the
performance of the method on texts that are more relevant to the research
question.",[{'name': 'Boris Orekhov'}],2024-07-14T19:28:48Z
http://arxiv.org/abs/2407.10275v1,http://arxiv.org/abs/2407.10275v1,"Cross-Lingual Multi-Hop Knowledge Editing -- Benchmarks, Analysis and a
  Simple Contrastive Learning based Approach","Large language models are often expected to constantly adapt to new sources
of knowledge and knowledge editing techniques aim to efficiently patch the
outdated model knowledge, with minimal modification. Most prior works focus on
monolingual knowledge editing in English, even though new information can
emerge in any language from any part of the world. We propose the Cross-Lingual
Multi-Hop Knowledge Editing paradigm, for measuring and analyzing the
performance of various SoTA knowledge editing techniques in a cross-lingual
setup. Specifically, we create a parallel cross-lingual benchmark,
CROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive
analysis over various knowledge editing techniques uncover significant gaps in
performance between the cross-lingual and English-centric setting. Following
this, we propose a significantly improved system for cross-lingual multi-hop
knowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and
generate knowledge editing framework, where a retriever is formulated to recall
edited facts and support an LLM to adhere to knowledge edits. We develop
language-aware and hard-negative based contrastive objectives for improving the
cross-lingual and fine-grained fact retrieval and verification process used in
this framework. Extensive experiments on three LLMs, eight languages, and two
datasets show CLEVER-CKE's significant gains of up to 30% over prior methods.","[{'name': 'Aditi Khandelwal'}, {'name': 'Harman Singh'}, {'name': 'Hengrui Gu'}, {'name': 'Tianlong Chen'}, {'name': 'Kaixiong Zhou'}]",2024-07-14T17:18:16Z
http://arxiv.org/abs/2407.10266v2,http://arxiv.org/abs/2407.10266v2,"psifx -- Psychological and Social Interactions Feature Extraction
  Package","psifx is a plug-and-play multi-modal feature extraction toolkit, aiming to
facilitate and democratize the use of state-of-the-art machine learning
techniques for human sciences research. It is motivated by a need (a) to
automate and standardize data annotation processes, otherwise involving
expensive, lengthy, and inconsistent human labor, such as the transcription or
coding of behavior changes from audio and video sources; (b) to develop and
distribute open-source community-driven psychology research software; and (c)
to enable large-scale access and ease of use to non-expert users. The framework
contains an array of tools for tasks, such as speaker diarization,
closed-caption transcription and translation from audio, as well as body, hand,
and facial pose estimation and gaze tracking from video. The package has been
designed with a modular and task-oriented approach, enabling the community to
add or update new tools easily. We strongly hope that this package will provide
psychologists a simple and practical solution for efficiently a range of audio,
linguistic, and visual features from audio and video, thereby creating new
opportunities for in-depth study of real-time behavioral phenomena.","[{'name': 'Guillaume Rochette'}, {'name': 'Matthew J. Vowels'}]",2024-07-14T16:20:42Z
http://arxiv.org/abs/2407.10264v2,http://arxiv.org/abs/2407.10264v2,What Makes and Breaks Safety Fine-tuning? A Mechanistic Study,"Safety fine-tuning helps align Large Language Models (LLMs) with human
preferences for their safe deployment. To better understand the underlying
factors that make models safe via safety fine-tuning, we design a synthetic
data generation framework that captures salient aspects of an unsafe input by
modeling the interaction between the task the model is asked to perform (e.g.,
""design"") versus the specific concepts the task is asked to be performed upon
(e.g., a ""cycle"" vs. a ""bomb""). Using this, we investigate three well-known
safety fine-tuning methods -- supervised safety fine-tuning, direct preference
optimization, and unlearning -- and provide significant evidence demonstrating
that these methods minimally transform MLP weights to specifically align unsafe
inputs into its weights' null space. This yields a clustering of inputs based
on whether the model deems them safe or not. Correspondingly, when an
adversarial input (e.g., a jailbreak) is provided, its activations are closer
to safer samples, leading to the model processing such an input as if it were
safe. We validate our findings, wherever possible, on real-world models --
specifically, Llama-2 7B and Llama-3 8B.","[{'name': 'Samyak Jain'}, {'name': 'Ekdeep Singh Lubana'}, {'name': 'Kemal Oksuz'}, {'name': 'Tom Joy'}, {'name': 'Philip H. S. Torr'}, {'name': 'Amartya Sanyal'}, {'name': 'Puneet K. Dokania'}]",2024-07-14T16:12:57Z
http://arxiv.org/abs/2407.10252v1,http://arxiv.org/abs/2407.10252v1,"Nullpointer at CheckThat! 2024: Identifying Subjectivity from
  Multilingual Text Sequence","This study addresses a binary classification task to determine whether a text
sequence, either a sentence or paragraph, is subjective or objective. The task
spans five languages: Arabic, Bulgarian, English, German, and Italian, along
with a multilingual category. Our approach involved several key techniques.
Initially, we preprocessed the data through parts of speech (POS) tagging,
identification of question marks, and application of attention masks. We
fine-tuned the sentiment-based Transformer model
'MarieAngeA13/Sentiment-Analysis-BERT' on our dataset. Given the imbalance with
more objective data, we implemented a custom classifier that assigned greater
weight to objective data. Additionally, we translated non-English data into
English to maintain consistency across the dataset. Our model achieved notable
results, scoring top marks for the multilingual dataset (Macro F1=0.7121) and
German (Macro F1=0.7908). It ranked second for Arabic (Macro F1=0.4908) and
Bulgarian (Macro F1=0.7169), third for Italian (Macro F1=0.7430), and ninth for
English (Macro F1=0.6893).","[{'name': 'Md. Rafiul Biswas'}, {'name': 'Abrar Tasneem Abir'}, {'name': 'Wajdi Zaghouani'}]",2024-07-14T15:37:28Z
http://arxiv.org/abs/2407.10245v1,http://arxiv.org/abs/2407.10245v1,"GenSco: Can Question Decomposition based Passage Alignment improve
  Question Answering?","Retrieval augmented generation (RAG) with large language models (LLMs) for
Question Answering (QA) entails furnishing relevant context within the prompt
to facilitate the LLM in answer generation. During the generation, inaccuracies
or hallucinations frequently occur due to two primary factors: inadequate or
distracting context in the prompts, and the inability of LLMs to effectively
reason through the facts. In this paper, we investigate whether providing
aligned context via a carefully selected passage sequence leads to better
answer generation by the LLM for multi-hop QA. We introduce, ""GenSco"", a novel
approach of selecting passages based on the predicted decomposition of the
multi-hop questions}. The framework consists of two distinct LLMs: (i)
Generator LLM, which is used for question decomposition and final answer
generation; (ii) an auxiliary open-sourced LLM, used as the scorer, to
semantically guide the Generator for passage selection. The generator is
invoked only once for the answer generation, resulting in a cost-effective and
efficient approach. We evaluate on three broadly established multi-hop question
answering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve
an absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect
to the best performing baselines over MuSiQue and 2WikiMultiHop respectively.","[{'name': 'Barah Fazili'}, {'name': 'Koustava Goswami'}, {'name': 'Natwar Modani'}, {'name': 'Inderjeet Nair'}]",2024-07-14T15:25:08Z
http://arxiv.org/abs/2407.10241v2,http://arxiv.org/abs/2407.10241v2,BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs,"Evaluating the bias in Large Language Models (LLMs) becomes increasingly
crucial with their rapid development. However, existing evaluation methods rely
on fixed-form outputs and cannot adapt to the flexible open-text generation
scenarios of LLMs (e.g., sentence completion and question answering). To
address this, we introduce BiasAlert, a plug-and-play tool designed to detect
social bias in open-text generations of LLMs. BiasAlert integrates external
human knowledge with inherent reasoning capabilities to detect bias reliably.
Extensive experiments demonstrate that BiasAlert significantly outperforms
existing state-of-the-art methods like GPT4-as-A-Judge in detecting bias.
Furthermore, through application studies, we demonstrate the utility of
BiasAlert in reliable LLM bias evaluation and bias mitigation across various
scenarios. Model and code will be publicly released.","[{'name': 'Zhiting Fan'}, {'name': 'Ruizhe Chen'}, {'name': 'Ruiling Xu'}, {'name': 'Zuozhu Liu'}]",2024-07-14T15:17:02Z
http://arxiv.org/abs/2407.10167v3,http://arxiv.org/abs/2407.10167v3,"Key-Point-Driven Mathematical Reasoning Distillation of Large Language
  Model","Large Language Models (LLMs) have demonstrated exceptional proficiency in
mathematical reasoning tasks due to their extensive parameter counts and
training on vast datasets. Despite these capabilities, deploying LLMs is
hindered by their computational demands. Distilling LLM mathematical reasoning
into Smaller Language Models (SLMs) has emerged as a solution to this
challenge, although these smaller models often suffer from errors in
calculation and semantic understanding. Prior work has proposed
Program-of-Thought Distillation (PoTD) to avoid calculation error. To further
address semantic understanding errors, we propose Key-Point-Driven Mathematical
Reasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs
by breaking down the problem-solving process into three stages: Core Question
Extraction, Problem-Solving Information Extraction, and Step-by-Step Solution.
This method is further divided into KPDD-CoT, which generates Chain-of-Thought
rationales, and KPDD-PoT, which creates Program-of-Thought rationales. The
experiment results show that KPDD-CoT significantly improves reasoning
abilities, while KPDD-PoT achieves state-of-the-art performance in mathematical
reasoning tasks. Our approach effectively mitigates misunderstanding errors,
advancing the deployment of efficient and capable SLMs.","[{'name': 'Xunyu Zhu'}, {'name': 'Jian Li'}, {'name': 'Can Ma'}, {'name': 'Weiping Wang'}]",2024-07-14T11:41:03Z
http://arxiv.org/abs/2407.10153v1,http://arxiv.org/abs/2407.10153v1,"Look Within, Why LLMs Hallucinate: A Causal Perspective","The emergence of large language models (LLMs) is a milestone in generative
artificial intelligence, achieving significant success in text comprehension
and generation tasks. Despite the tremendous success of LLMs in many downstream
tasks, they suffer from severe hallucination problems, posing significant
challenges to the practical applications of LLMs. Most of the works about LLMs'
hallucinations focus on data quality. Self-attention is a core module in
transformer-based LLMs, while its potential relationship with LLMs'
hallucination has been hardly investigated. To fill this gap, we study this
problem from a causal perspective. We propose a method to intervene in LLMs'
self-attention layers and maintain their structures and sizes intact.
Specifically, we disable different self-attention layers in several popular
open-source LLMs and then compare their degrees of hallucination with the
original ones. We evaluate the intervened LLMs on hallucination assessment
benchmarks and conclude that disabling some specific self-attention layers in
the front or tail of the LLMs can alleviate hallucination issues. The study
paves a new way for understanding and mitigating LLMs' hallucinations.","[{'name': 'He Li'}, {'name': 'Haoang Chi'}, {'name': 'Mingyu Liu'}, {'name': 'Wenjing Yang'}]",2024-07-14T10:47:44Z
http://arxiv.org/abs/2407.10152v1,http://arxiv.org/abs/2407.10152v1,"Mitigating Translationese in Low-resource Languages: The Storyboard
  Approach","Low-resource languages often face challenges in acquiring high-quality
language data due to the reliance on translation-based methods, which can
introduce the translationese effect. This phenomenon results in translated
sentences that lack fluency and naturalness in the target language. In this
paper, we propose a novel approach for data collection by leveraging
storyboards to elicit more fluent and natural sentences. Our method involves
presenting native speakers with visual stimuli in the form of storyboards and
collecting their descriptions without direct exposure to the source text. We
conducted a comprehensive evaluation comparing our storyboard-based approach
with traditional text translation-based methods in terms of accuracy and
fluency. Human annotators and quantitative metrics were used to assess
translation quality. The results indicate a preference for text translation in
terms of accuracy, while our method demonstrates worse accuracy but better
fluency in the language focused.","[{'name': 'Garry Kuwanto'}, {'name': 'Eno-Abasi E. Urua'}, {'name': 'Priscilla Amondi Amuok'}, {'name': 'Shamsuddeen Hassan Muhammad'}, {'name': 'Anuoluwapo Aremu'}, {'name': 'Verrah Otiende'}, {'name': 'Loice Emma Nanyanga'}, {'name': 'Teresiah W. Nyoike'}, {'name': 'Aniefon D. Akpan'}, {'name': 'Nsima Ab Udouboh'}, {'name': 'Idongesit Udeme Archibong'}, {'name': 'Idara Effiong Moses'}, {'name': 'Ifeoluwatayo A. Ige'}, {'name': 'Benjamin Ajibade'}, {'name': 'Olumide Benjamin Awokoya'}, {'name': 'Idris Abdulmumin'}, {'name': 'Saminu Mohammad Aliyu'}, {'name': 'Ruqayya Nasir Iro'}, {'name': 'Ibrahim Said Ahmad'}, {'name': 'Deontae Smith'}, {'name': 'Praise-EL Michaels'}, {'name': 'David Ifeoluwa Adelani'}, {'name': 'Derry Tanti Wijaya'}, {'name': 'Anietie Andy'}]",2024-07-14T10:47:03Z
http://arxiv.org/abs/2407.10118v1,http://arxiv.org/abs/2407.10118v1,Textless Dependency Parsing by Labeled Sequence Prediction,"Traditional spoken language processing involves cascading an automatic speech
recognition (ASR) system into text processing models. In contrast, ""textless""
methods process speech representations without ASR systems, enabling the direct
use of acoustic speech features. Although their effectiveness is shown in
capturing acoustic features, it is unclear in capturing lexical knowledge. This
paper proposes a textless method for dependency parsing, examining its
effectiveness and limitations. Our proposed method predicts a dependency tree
from a speech signal without transcribing, representing the tree as a labeled
sequence. scading method outperforms the textless method in overall parsing
accuracy, the latter excels in instances with important acoustic features. Our
findings highlight the importance of fusing word-level representations and
sentence-level prosody for enhanced parsing performance. The code and models
are made publicly available: https://github.com/mynlp/SpeechParser.","[{'name': 'Shunsuke Kando'}, {'name': 'Yusuke Miyao'}, {'name': 'Jason Naradowsky'}, {'name': 'Shinnosuke Takamichi'}]",2024-07-14T08:38:14Z
http://arxiv.org/abs/2407.10114v2,http://arxiv.org/abs/2407.10114v2,"TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley
  Value Estimation","As large language models (LLMs) become increasingly prevalent in critical
applications, the need for interpretable AI has grown. We introduce TokenSHAP,
a novel method for interpreting LLMs by attributing importance to individual
tokens or substrings within input prompts. This approach adapts Shapley values
from cooperative game theory to natural language processing, offering a
rigorous framework for understanding how different parts of an input contribute
to a model's response. TokenSHAP leverages Monte Carlo sampling for
computational efficiency, providing interpretable, quantitative measures of
token importance. We demonstrate its efficacy across diverse prompts and LLM
architectures, showing consistent improvements over existing baselines in
alignment with human judgments, faithfulness to model behavior, and
consistency.
  Our method's ability to capture nuanced interactions between tokens provides
valuable insights into LLM behavior, enhancing model transparency, improving
prompt engineering, and aiding in the development of more reliable AI systems.
TokenSHAP represents a significant step towards the necessary interpretability
for responsible AI deployment, contributing to the broader goal of creating
more transparent, accountable, and trustworthy AI systems.","[{'name': 'Roni Goldshmidt'}, {'name': 'Miriam Horovicz'}]",2024-07-14T08:07:50Z
http://arxiv.org/abs/2407.10091v1,http://arxiv.org/abs/2407.10091v1,"Enhancing Emotion Prediction in News Headlines: Insights from ChatGPT
  and Seq2Seq Models for Free-Text Generation","Predicting emotions elicited by news headlines can be challenging as the task
is largely influenced by the varying nature of people's interpretations and
backgrounds. Previous works have explored classifying discrete emotions
directly from news headlines. We provide a different approach to tackling this
problem by utilizing people's explanations of their emotion, written in
free-text, on how they feel after reading a news headline. Using the dataset
BU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the
free-text explanations have a strong correlation with the dominant emotion
elicited by the headlines. The free-text explanations also contain more
sentimental context than the news headlines alone and can serve as a better
input to emotion classification models. Therefore, in this work we explored
generating emotion explanations from headlines by training a
sequence-to-sequence transformer model and by using pretrained large language
model, ChatGPT (GPT-4). We then used the generated emotion explanations for
emotion classification. In addition, we also experimented with training the
pretrained T5 model for the intermediate task of explanation generation before
fine-tuning it for emotion classification. Using McNemar's significance test,
methods that incorporate GPT-generated free-text emotion explanations
demonstrated significant improvement (P-value < 0.05) in emotion classification
from headlines, compared to methods that only use headlines. This underscores
the value of using intermediate free-text explanations for emotion prediction
tasks with headlines.","[{'name': 'Ge Gao'}, {'name': 'Jongin Kim'}, {'name': 'Sejin Paik'}, {'name': 'Ekaterina Novozhilova'}, {'name': 'Yi Liu'}, {'name': 'Sarah T. Bonna'}, {'name': 'Margrit Betke'}, {'name': 'Derry Tanti Wijaya'}]",2024-07-14T06:04:11Z
http://arxiv.org/abs/2407.10086v2,http://arxiv.org/abs/2407.10086v2,"Rapid Biomedical Research Classification: The Pandemic PACT Advanced
  Categorisation Engine","This paper introduces the Pandemic PACT Advanced Categorisation Engine
(PPACE) along with its associated dataset. PPACE is a fine-tuned model
developed to automatically classify research abstracts from funded biomedical
projects according to WHO-aligned research priorities. This task is crucial for
monitoring research trends and identifying gaps in global health preparedness
and response. Our approach builds on human-annotated projects, which are
allocated one or more categories from a predefined list. A large language model
is then used to generate `rationales' explaining the reasoning behind these
annotations. This augmented data, comprising expert annotations and rationales,
is subsequently used to fine-tune a smaller, more efficient model. Developed as
part of the Pandemic PACT project, which aims to track and analyse research
funding and clinical evidence for a wide range of diseases with outbreak
potential, PPACE supports informed decision-making by research funders,
policymakers, and independent researchers. We introduce and release both the
trained model and the instruction-based dataset used for its training. Our
evaluation shows that PPACE significantly outperforms its baselines. The
release of PPACE and its associated dataset offers valuable resources for
researchers in multilabel biomedical document classification and supports
advancements in aligning biomedical research with key global health priorities.","[{'name': 'Omid Rohanian'}, {'name': 'Mohammadmahdi Nouriborji'}, {'name': 'Olena Seminog'}, {'name': 'Rodrigo Furst'}, {'name': 'Thomas Mendy'}, {'name': 'Shanthi Levanita'}, {'name': 'Zaharat Kadri-Alabi'}, {'name': 'Nusrat Jabin'}, {'name': 'Daniela Toale'}, {'name': 'Georgina Humphreys'}, {'name': 'Emilia Antonio'}, {'name': 'Adrian Bucher'}, {'name': 'Alice Norton'}, {'name': 'David A. Clifton'}]",2024-07-14T05:22:53Z
http://arxiv.org/abs/2407.10068v1,http://arxiv.org/abs/2407.10068v1,"Multi-Granularity Semantic Revision for Large Language Model
  Distillation","Knowledge distillation plays a key role in compressing the Large Language
Models (LLMs), which boosts a small-size student model under large teacher
models' guidance. However, existing LLM distillation methods overly rely on
student-generated outputs, which may introduce generation errors and misguide
the distillation process. Moreover, the distillation loss functions introduced
in previous art struggle to align the most informative part due to the complex
distribution of LLMs' outputs. To address these problems, we propose a
multi-granularity semantic revision method for LLM distillation. At the
sequence level, we propose a sequence correction and re-generation (SCRG)
strategy. SCRG first calculates the semantic cognitive difference between the
teacher and student to detect the error token, then corrects it with the
teacher-generated one, and re-generates the sequence to reduce generation
errors and enhance generation diversity. At the token level, we design a
distribution adaptive clipping Kullback-Leibler (DAC-KL) loss as the
distillation objective function. DAC-KL loss exploits a learnable sub-network
to adaptively extract semantically dense areas from the teacher's output,
avoiding the interference of redundant information in the distillation process.
Finally, at the span level, we leverage the span priors of a sequence to
compute the probability correlations within spans, and constrain the teacher
and student's probability correlations to be consistent, further enhancing the
transfer of semantic information. Extensive experiments across different model
families with parameters ranging from 0.1B to 13B demonstrate the superiority
of our method compared to existing methods.","[{'name': 'Xiaoyu Liu'}, {'name': 'Yun Zhang'}, {'name': 'Wei Li'}, {'name': 'Simiao Li'}, {'name': 'Xudong Huang'}, {'name': 'Hanting Chen'}, {'name': 'Yehui Tang'}, {'name': 'Jie Hu'}, {'name': 'Zhiwei Xiong'}, {'name': 'Yunhe Wang'}]",2024-07-14T03:51:49Z
http://arxiv.org/abs/2407.10058v1,http://arxiv.org/abs/2407.10058v1,Learning to Refuse: Towards Mitigating Privacy Risks in LLMs,"Large language models (LLMs) exhibit remarkable capabilities in understanding
and generating natural language. However, these models can inadvertently
memorize private information, posing significant privacy risks. This study
addresses the challenge of enabling LLMs to protect specific individuals'
private data without the need for complete retraining. We propose \return, a
Real-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from
Wikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods
for protecting personal data in a realistic scenario. Additionally, we
introduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,
which enables the model to learn which individuals' information should be
protected without affecting its ability to answer questions related to other
unrelated individuals. Our extensive experiments demonstrate that NAUF achieves
a state-of-the-art average unlearning score, surpassing the best baseline
method by 5.65 points, effectively protecting target individuals' personal data
while maintaining the model's general capabilities.","[{'name': 'Zhenhua Liu'}, {'name': 'Tong Zhu'}, {'name': 'Chuanyuan Tan'}, {'name': 'Wenliang Chen'}]",2024-07-14T03:05:53Z
http://arxiv.org/abs/2407.10049v1,http://arxiv.org/abs/2407.10049v1,AutoGRAMS: Autonomous Graphical Agent Modeling Software,"We introduce the AutoGRAMS framework for programming multi-step interactions
with language models. AutoGRAMS represents AI agents as a graph, where each
node can execute either a language modeling instruction or traditional code.
Likewise, transitions in the graph can be governed by either language modeling
decisions or traditional branch logic. AutoGRAMS supports using variables as
memory and allows nodes to call other AutoGRAMS graphs as functions. We show
how AutoGRAMS can be used to design highly sophisticated agents, including
self-referential agents that can modify their own graph. AutoGRAMS's
graph-centric approach aids interpretability, controllability, and safety
during the design, development, and deployment of AI agents. We provide our
framework as open source at https://github.com/autograms/autograms .","[{'name': 'Ben Krause'}, {'name': 'Lucia Chen'}, {'name': 'Emmanuel Kahembwe'}]",2024-07-14T02:25:45Z
http://arxiv.org/abs/2407.10021v1,http://arxiv.org/abs/2407.10021v1,"Document-level Clinical Entity and Relation Extraction via Knowledge
  Base-Guided Generation","Generative pre-trained transformer (GPT) models have shown promise in
clinical entity and relation extraction tasks because of their precise
extraction and contextual understanding capability. In this work, we further
leverage the Unified Medical Language System (UMLS) knowledge base to
accurately identify medical concepts and improve clinical entity and relation
extraction at the document level. Our framework selects UMLS concepts relevant
to the text and combines them with prompts to guide language models in
extracting entities. Our experiments demonstrate that this initial concept
mapping and the inclusion of these mapped concepts in the prompts improves
extraction results compared to few-shot extraction tasks on generic language
models that do not leverage UMLS. Further, our results show that this approach
is more effective than the standard Retrieval Augmented Generation (RAG)
technique, where retrieved data is compared with prompt embeddings to generate
results. Overall, we find that integrating UMLS concepts with GPT models
significantly improves entity and relation identification, outperforming the
baseline and RAG models. By combining the precise concept mapping capability of
knowledge-based approaches like UMLS with the contextual understanding
capability of GPT, our method highlights the potential of these approaches in
specialized domains like healthcare.","[{'name': 'Kriti Bhattarai'}, {'name': 'Inez Y. Oh'}, {'name': 'Zachary B. Abrams'}, {'name': 'Albert M. Lai'}]",2024-07-13T22:45:46Z
http://arxiv.org/abs/2407.10020v1,http://arxiv.org/abs/2407.10020v1,"Causality extraction from medical text using Large Language Models
  (LLMs)","This study explores the potential of natural language models, including large
language models, to extract causal relations from medical texts, specifically
from Clinical Practice Guidelines (CPGs). The outcomes causality extraction
from Clinical Practice Guidelines for gestational diabetes are presented,
marking a first in the field. We report on a set of experiments using variants
of BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs),
namely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better
than other models, including the Large Language Models, with an average
F1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less
consistency. We also release the code and an annotated a corpus of causal
statements within the Clinical Practice Guidelines for gestational diabetes.","[{'name': 'Seethalakshmi Gopalakrishnan'}, {'name': 'Luciana Garbayo'}, {'name': 'Wlodek Zadrozny'}]",2024-07-13T22:33:29Z
http://arxiv.org/abs/2407.10005v1,http://arxiv.org/abs/2407.10005v1,"Fine-grained Analysis of In-context Linear Estimation: Data,
  Architecture, and Beyond","Recent research has shown that Transformers with linear attention are capable
of in-context learning (ICL) by implementing a linear estimator through
gradient descent steps. However, the existing results on the optimization
landscape apply under stylized settings where task and feature vectors are
assumed to be IID and the attention weights are fully parameterized. In this
work, we develop a stronger characterization of the optimization and
generalization landscape of ICL through contributions on architectures,
low-rank parameterization, and correlated designs: (1) We study the landscape
of 1-layer linear attention and 1-layer H3, a state-space model. Under a
suitable correlated design assumption, we prove that both implement 1-step
preconditioned gradient descent. We show that thanks to its native convolution
filters, H3 also has the advantage of implementing sample weighting and
outperforming linear attention in suitable settings. (2) By studying correlated
designs, we provide new risk bounds for retrieval augmented generation (RAG)
and task-feature alignment which reveal how ICL sample complexity benefits from
distributional alignment. (3) We derive the optimal risk for low-rank
parameterized attention weights in terms of covariance spectrum. Through this,
we also shed light on how LoRA can adapt to a new distribution by capturing the
shift between task covariances. Experimental results corroborate our
theoretical findings. Overall, this work explores the optimization and risk
landscape of ICL in practically meaningful settings and contributes to a more
thorough understanding of its mechanics.","[{'name': 'Yingcong Li'}, {'name': 'Ankit Singh Rawat'}, {'name': 'Samet Oymak'}]",2024-07-13T21:13:55Z
http://arxiv.org/abs/2407.12869v2,http://arxiv.org/abs/2407.12869v2,Bilingual Adaptation of Monolingual Foundation Models,"We present an efficient method for adapting a monolingual Large Language
Model (LLM) to another language, addressing challenges of catastrophic
forgetting and tokenizer limitations. We focus this study on adapting Llama 2
to Arabic. Our two-stage approach begins with expanding the vocabulary and
training only the embeddings matrix, followed by full model continual
pre-training on a bilingual corpus. By continually pre-training on a mix of
Arabic and English corpora, the model retains its proficiency in English while
acquiring capabilities in Arabic. Our approach results in significant
improvements in Arabic and slight enhancements in English, demonstrating
cost-effective cross-lingual transfer. We perform ablations on embedding
initialization techniques, data mix ratios, and learning rates and release a
detailed training recipe. To demonstrate generalizability of this approach we
also adapted Llama 3 8B to Arabic and Llama 2 13B to Hindi.","[{'name': 'Gurpreet Gosal'}, {'name': 'Yishi Xu'}, {'name': 'Gokul Ramakrishnan'}, {'name': 'Rituraj Joshi'}, {'name': 'Avraham Sheinin'}, {'name': 'Zhiming'}, {'name': 'Chen'}, {'name': 'Biswajit Mishra'}, {'name': 'Natalia Vassilieva'}, {'name': 'Joel Hestness'}, {'name': 'Neha Sengupta'}, {'name': 'Sunil Kumar Sahu'}, {'name': 'Bokang Jia'}, {'name': 'Onkar Pandit'}, {'name': 'Satheesh Katipomu'}, {'name': 'Samta Kamboj'}, {'name': 'Samujjwal Ghosh'}, {'name': 'Rahul Pal'}, {'name': 'Parvez Mullah'}, {'name': 'Soundar Doraiswamy'}, {'name': 'Mohamed El Karim Chami'}, {'name': 'Preslav Nakov'}]",2024-07-13T21:09:38Z
http://arxiv.org/abs/2407.09943v1,http://arxiv.org/abs/2407.09943v1,Minimizing PLM-Based Few-Shot Intent Detectors,"Recent research has demonstrated the feasibility of training efficient intent
detectors based on pre-trained language model~(PLM) with limited labeled data.
However, deploying these detectors in resource-constrained environments such as
mobile devices poses challenges due to their large sizes. In this work, we aim
to address this issue by exploring techniques to minimize the size of PLM-based
intent detectors trained with few-shot data. Specifically, we utilize large
language models (LLMs) for data augmentation, employ a cutting-edge model
compression method for knowledge distillation, and devise a vocabulary pruning
mechanism called V-Prune. Through these approaches, we successfully achieve a
compression ratio of 21 in model memory usage, including both Transformer and
the vocabulary, while maintaining almost identical performance levels on four
real-world benchmarks.","[{'name': 'Haode Zhang'}, {'name': 'Xiao-Ming Wu'}, {'name': 'Albert Y. S. Lam'}]",2024-07-13T16:47:20Z
http://arxiv.org/abs/2407.09936v1,http://arxiv.org/abs/2407.09936v1,WojoodNER 2024: The Second Arabic Named Entity Recognition Shared Task,"We present WojoodNER-2024, the second Arabic Named Entity Recognition (NER)
Shared Task. In WojoodNER-2024, we focus on fine-grained Arabic NER. We
provided participants with a new Arabic fine-grained NER dataset called
wojoodfine, annotated with subtypes of entities. WojoodNER-2024 encompassed
three subtasks: (i) Closed-Track Flat Fine-Grained NER, (ii) Closed-Track
Nested Fine-Grained NER, and (iii) an Open-Track NER for the Israeli War on
Gaza. A total of 43 unique teams registered for this shared task. Five teams
participated in the Flat Fine-Grained Subtask, among which two teams tackled
the Nested Fine-Grained Subtask and one team participated in the Open-Track NER
Subtask. The winning teams achieved F-1 scores of 91% and 92% in the Flat
Fine-Grained and Nested Fine-Grained Subtasks, respectively. The sole team in
the Open-Track Subtask achieved an F-1 score of 73.7%.","[{'name': 'Mustafa Jarrar'}, {'name': 'Nagham Hamad'}, {'name': 'Mohammed Khalilia'}, {'name': 'Bashar Talafha'}, {'name': 'AbdelRahim Elmadany'}, {'name': 'Muhammad Abdul-Mageed'}]",2024-07-13T16:17:08Z
http://arxiv.org/abs/2407.09897v2,http://arxiv.org/abs/2407.09897v2,"Cohesive Conversations: Enhancing Authenticity in Multi-Agent Simulated
  Dialogues","This paper investigates the quality of multi-agent dialogues in simulations
powered by Large Language Models (LLMs). Analyzing dialogues and memory over
multiple sessions revealed significant issues such as repetition,
inconsistency, and hallucination, exacerbated by the propagation of erroneous
information. To combat these challenges, we propose a novel Screening,
Diagnosis, and Regeneration (SDR) framework that detects and corrects utterance
errors through a comprehensive process involving immediate issue
identification, evidence gathering from past dialogues, and LLM analysis for
utterance revision. By incorporating our SDR framework to Generative Agents
(Park et al., 2023), we enhance the diversity, consistency, and factualness of
the generated dialogues. This work presents a pioneering approach to enhancing
dialogue quality in multi-agent simulations, establishing a new standard for
future research in the field.","[{'name': 'KuanChao Chu'}, {'name': 'Yi-Pei Chen'}, {'name': 'Hideki Nakayama'}]",2024-07-13T14:24:45Z
http://arxiv.org/abs/2407.21024v2,http://arxiv.org/abs/2407.21024v2,An Autonomous GIS Agent Framework for Geospatial Data Retrieval,"Powered by the emerging large language models (LLMs), autonomous geographic
information systems (GIS) agents have the potential to accomplish spatial
analyses and cartographic tasks. However, a research gap exists to support
fully autonomous GIS agents: how to enable agents to discover and download the
necessary data for geospatial analyses. This study proposes an autonomous GIS
agent framework capable of retrieving required geospatial data by generating,
executing, and debugging programs. The framework utilizes the LLM as the
decision-maker, selects the appropriate data source (s) from a pre-defined
source list, and fetches the data from the chosen source. Each data source has
a handbook that records the metadata and technical details for data retrieval.
The proposed framework is designed in a plug-and-play style to ensure
flexibility and extensibility. Human users or autonomous data scrawlers can add
new data sources by adding new handbooks. We developed a prototype agent based
on the framework, released as a QGIS plugin (GeoData Retrieve Agent) and a
Python program. Experiment results demonstrate its capability of retrieving
data from various sources including OpenStreetMap, administrative boundaries
and demographic data from the US Census Bureau, satellite basemaps from ESRI
World Imagery, global digital elevation model (DEM) from OpenTopography.org,
weather data from a commercial provider, the COVID-19 cases from the NYTimes
GitHub. Our study is among the first attempts to develop an autonomous
geospatial data retrieval agent.","[{'name': 'Huan Ning'}, {'name': 'Zhenlong Li'}, {'name': 'Temitope Akinboyewa'}, {'name': 'M. Naser Lessani'}]",2024-07-13T14:23:57Z
http://arxiv.org/abs/2407.09894v1,http://arxiv.org/abs/2407.09894v1,"Transferring Structure Knowledge: A New Task to Fake news Detection
  Towards Cold-Start Propagation","Many fake news detection studies have achieved promising performance by
extracting effective semantic and structure features from both content and
propagation trees. However, it is challenging to apply them to practical
situations, especially when using the trained propagation-based models to
detect news with no propagation data. Towards this scenario, we study a new
task named cold-start fake news detection, which aims to detect content-only
samples with missing propagation. To achieve the task, we design a simple but
effective Structure Adversarial Net (SAN) framework to learn transferable
features from available propagation to boost the detection of content-only
samples. SAN introduces a structure discriminator to estimate dissimilarities
among learned features with and without propagation, and further learns
structure-invariant features to enhance the generalization of existing
propagation-based methods for content-only samples. We conduct qualitative and
quantitative experiments on three datasets. Results show the challenge of the
new task and the effectiveness of our SAN framework.","[{'name': 'Lingwei Wei'}, {'name': 'Dou Hu'}, {'name': 'Wei Zhou'}, {'name': 'Songlin Hu'}]",2024-07-13T14:04:55Z
http://arxiv.org/abs/2407.09893v1,http://arxiv.org/abs/2407.09893v1,"Synergistic Multi-Agent Framework with Trajectory Learning for
  Knowledge-Intensive Tasks","Recent advancements in Large Language Models (LLMs) have led to significant
breakthroughs in various natural language processing tasks. However, generating
factually consistent responses in knowledge-intensive scenarios remains a
challenge due to issues such as hallucination, difficulty in acquiring
long-tailed knowledge, and limited memory expansion. This paper introduces
SMART, a novel multi-agent framework that leverages external knowledge to
enhance the interpretability and factual consistency of LLM-generated
responses. SMART comprises four specialized agents, each performing a specific
sub-trajectory action to navigate complex knowledge-intensive tasks. We propose
a multi-agent co-training paradigm, Long- and Short-Trajectory Learning, which
ensures synergistic collaboration among agents while maintaining fine-grained
execution by each agent. Extensive experiments on 5 tasks demonstrate SMART's
superior performance compared to previous widely adopted methods.","[{'name': 'Shengbin Yue'}, {'name': 'Siyuan Wang'}, {'name': 'Wei Chen'}, {'name': 'Xuanjing Huang'}, {'name': 'Zhongyu Wei'}]",2024-07-13T13:58:24Z
http://arxiv.org/abs/2407.09888v1,http://arxiv.org/abs/2407.09888v1,"FarFetched: Entity-centric Reasoning and Claim Validation for the Greek
  Language based on Textually Represented Environments","Our collective attention span is shortened by the flood of online
information. With \textit{FarFetched}, we address the need for automated claim
validation based on the aggregated evidence derived from multiple online news
sources. We introduce an entity-centric reasoning framework in which latent
connections between events, actions, or statements are revealed via entity
mentions and represented in a graph database. Using entity linking and semantic
similarity, we offer a way for collecting and combining information from
diverse sources in order to generate evidence relevant to the user's claim.
Then, we leverage textual entailment recognition to quantitatively determine
whether this assertion is credible, based on the created evidence. Our approach
tries to fill the gap in automated claim validation for less-resourced
languages and is showcased on the Greek language, complemented by the training
of relevant semantic textual similarity (STS) and natural language inference
(NLI) models that are evaluated on translated versions of common benchmarks.","[{'name': 'Dimitris Papadopoulos'}, {'name': 'Katerina Metropoulou'}, {'name': 'Nikolaos Matsatsinis'}, {'name': 'Nikolaos Papadakis'}]",2024-07-13T13:30:20Z
http://arxiv.org/abs/2407.09886v1,http://arxiv.org/abs/2407.09886v1,"Speech-Copilot: Leveraging Large Language Models for Speech Processing
  via Task Decomposition, Modularization, and Program Generation","In this work, we introduce Speech-Copilot, a modular framework for
instruction-oriented speech-processing tasks that minimizes human effort in
toolset construction. Unlike end-to-end methods using large audio-language
models, Speech-Copilot builds speech processing-specific toolsets by analyzing
pre-collected task instructions and breaking tasks into manageable sub-tasks.
It features a flexible agent based on large language models that performs tasks
through program generation. Our approach achieves state-of-the-art performance
on the Dynamic-SUPERB benchmark, demonstrating its effectiveness across diverse
speech-processing tasks. Key contributions include: 1) developing an innovative
framework for speech processing-specific toolset construction, 2) establishing
a high-performing agent based on large language models, and 3) offering a new
perspective on addressing challenging instruction-oriented speech-processing
tasks. Without additional training processes required by end-to-end approaches,
our method provides a flexible and extendable solution for a wide range of
speech-processing applications.","[{'name': 'Chun-Yi Kuan'}, {'name': 'Chih-Kai Yang'}, {'name': 'Wei-Ping Huang'}, {'name': 'Ke-Han Lu'}, {'name': 'Hung-yi Lee'}]",2024-07-13T13:26:43Z
http://arxiv.org/abs/2407.09879v2,http://arxiv.org/abs/2407.09879v2,"sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through
  N-shot Guided Prompting","Despite the remarkable success of LLMs in English, there is a significant gap
in performance in non-English languages. In order to address this, we introduce
a novel recipe for creating a multilingual synthetic instruction tuning
dataset, sPhinX, which is created by selectively translating instruction
response pairs from English into 50 languages. We test the effectiveness of
sPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and
Mistral-7B and then evaluating them across a comprehensive suite of
multilingual benchmarks that test reasoning, question answering, and reading
comprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with
sPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared
to the baselines. We also devise a strategy to incorporate N-shot examples in
each fine-tuning sample which further boosts the performance of these models by
3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other
multilingual instruction tuning datasets on the same benchmarks along with
being sample efficient and diverse, thereby reducing dataset creation costs.
Additionally, instruction tuning with sPhinX does not lead to regression on
most standard LLM benchmarks.","[{'name': 'Sanchit Ahuja'}, {'name': 'Kumar Tanmay'}, {'name': 'Hardik Hansrajbhai Chauhan'}, {'name': 'Barun Patra'}, {'name': 'Kriti Aggarwal'}, {'name': 'Luciano Del Corro'}, {'name': 'Arindam Mitra'}, {'name': 'Tejas Indulal Dhamecha'}, {'name': 'Ahmed Awadallah'}, {'name': 'Monojit Choudhary'}, {'name': 'Vishrav Chaudhary'}, {'name': 'Sunayana Sitaram'}]",2024-07-13T13:03:45Z
http://arxiv.org/abs/2407.09861v1,http://arxiv.org/abs/2407.09861v1,Towards Systematic Monolingual NLP Surveys: GenA of Greek NLP,"Natural Language Processing (NLP) research has traditionally been
predominantly focused on English, driven by the availability of resources, the
size of the research community, and market demands. Recently, there has been a
noticeable shift towards multilingualism in NLP, recognizing the need for
inclusivity and effectiveness across diverse languages and cultures.
Monolingual surveys have the potential to complement the broader trend towards
multilingualism in NLP by providing foundational insights and resources
necessary for effectively addressing the linguistic diversity of global
communication. However, monolingual NLP surveys are extremely rare in
literature. This study fills the gap by introducing a method for creating
systematic and comprehensive monolingual NLP surveys. Characterized by a
structured search protocol, it can be used to select publications and organize
them through a taxonomy of NLP tasks. We include a classification of Language
Resources (LRs), according to their availability, and datasets, according to
their annotation, to highlight publicly-available and machine-actionable LRs.
By applying our method, we conducted a systematic literature review of Greek
NLP from 2012 to 2022, providing a comprehensive overview of the current state
and challenges of Greek NLP research. We discuss the progress of Greek NLP and
outline encountered Greek LRs, classified by availability and usability. As we
show, our proposed method helps avoid common pitfalls, such as data leakage and
contamination, and to assess language support per NLP task. We consider this
systematic literature review of Greek NLP an application of our method that
showcases the benefits of a monolingual NLP survey. Similar applications could
be regard the myriads of languages whose progress in NLP lags behind that of
well-supported languages.","[{'name': 'Juli Bakagianni'}, {'name': 'Kanella Pouli'}, {'name': 'Maria Gavriilidou'}, {'name': 'John Pavlopoulos'}]",2024-07-13T12:01:52Z
http://arxiv.org/abs/2407.09855v1,http://arxiv.org/abs/2407.09855v1,"Building pre-train LLM Dataset for the INDIC Languages: a case study on
  Hindi","Large language models (LLMs) demonstrated transformative capabilities in many
applications that require automatically generating responses based on human
instruction. However, the major challenge for building LLMs, particularly in
Indic languages, is the availability of high-quality data for building
foundation LLMs. In this paper, we are proposing a large pre-train dataset in
Hindi useful for the Indic language Hindi. We have collected the data span
across several domains including major dialects in Hindi. The dataset contains
1.28 billion Hindi tokens. We have explained our pipeline including data
collection, pre-processing, and availability for LLM pre-training. The proposed
approach can be easily extended to other Indic and low-resource languages and
will be available freely for LLM pre-training and LLM research purposes.","[{'name': 'Shantipriya Parida'}, {'name': 'Shakshi Panwar'}, {'name': 'Kusum Lata'}, {'name': 'Sanskruti Mishra'}, {'name': 'Sambit Sekhar'}]",2024-07-13T11:29:20Z
http://arxiv.org/abs/2407.09849v1,http://arxiv.org/abs/2407.09849v1,Text-Based Detection of On-Hold Scripts in Contact Center Calls,"Average hold time is a concern for call centers because it affects customer
satisfaction. Contact centers should instruct their agents to use special
on-hold scripts to maintain positive interactions with clients. This study
presents a natural language processing model that detects on-hold phrases in
customer service calls transcribed by automatic speech recognition technology.
The task of finding hold scripts in dialogue was formulated as a multiclass
text classification problem with three mutually exclusive classes: scripts for
putting a client on hold, scripts for returning to a client, and phrases
irrelevant to on-hold scripts. We collected an in-house dataset of calls and
labeled each dialogue turn in each call. We fine-tuned RuBERT on the dataset by
exploring various hyperparameter sets and achieved high model performance. The
developed model can help agent monitoring by providing a way to check whether
an agent follows predefined on-hold scripts.","[{'name': 'Dmitrii Galimzianov'}, {'name': 'Viacheslav Vyshegorodtsev'}]",2024-07-13T11:11:41Z
http://arxiv.org/abs/2407.09835v2,http://arxiv.org/abs/2407.09835v2,"Investigating Low-Rank Training in Transformer Language Models:
  Efficiency and Scaling Analysis","State-of-the-art LLMs often rely on scale with high computational costs,
which has sparked a research agenda to reduce parameter counts and costs
without significantly impacting performance. Our study focuses on
Transformer-based LLMs, specifically applying low-rank parametrization to the
computationally intensive feedforward networks (FFNs), which are less studied
than attention blocks. In contrast to previous works, (i) we explore low-rank
parametrization at scale, up to 1.3B parameters; (ii) within Transformer
language models rather than convolutional architectures; and (iii) starting
from training from scratch. Experiments on the large RefinedWeb dataset show
that low-rank parametrization is both efficient (e.g., 2.6$\times$ FFN speed-up
with 32\% parameters) and effective during training. Interestingly, these
structured FFNs exhibit steeper scaling curves than the original models.
Motivated by this finding, we develop the wide and structured networks
surpassing the current medium-sized and large-sized Transformer in perplexity
and throughput performance. Our code is available at
https://github.com/CLAIRE-Labo/StructuredFFN/tree/main.","[{'name': 'Xiuying Wei'}, {'name': 'Skander Moalla'}, {'name': 'Razvan Pascanu'}, {'name': 'Caglar Gulcehre'}]",2024-07-13T10:08:55Z
http://arxiv.org/abs/2407.09817v1,http://arxiv.org/abs/2407.09817v1,"Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech
  Recognition System","Multi-talker speech recognition and target-talker speech recognition, both
involve transcription in multi-talker contexts, remain significant challenges.
However, existing methods rarely attempt to simultaneously address both tasks.
In this study, we propose a pioneering approach to empower Whisper, which is a
speech foundation model, to tackle joint multi-talker and target-talker speech
recognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar
separator into its encoder to separate mixed embedding for multiple talkers;
(ii) a Target Talker Identifier is introduced to identify the embedding flow of
the target talker on the fly, requiring only three-second enrollment speech as
a cue; (iii) soft prompt tuning for decoder is explored for better task
adaptation. Our method outperforms previous methods on two- and three-talker
LibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable
zero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.","[{'name': 'Lingwei Meng'}, {'name': 'Jiawen Kang'}, {'name': 'Yuejiao Wang'}, {'name': 'Zengrui Jin'}, {'name': 'Xixin Wu'}, {'name': 'Xunying Liu'}, {'name': 'Helen Meng'}]",2024-07-13T09:28:24Z
http://arxiv.org/abs/2407.09816v2,http://arxiv.org/abs/2407.09816v2,"MaskMoE: Boosting Token-Level Learning via Routing Mask in
  Mixture-of-Experts","Scaling the size of a model enhances its capabilities but significantly
increases computation complexity. Mixture-of-Experts models (MoE) address the
issue by allowing model size to scale up without substantially increasing
training or inference costs. Despite their promising results, MoE models
encounter several challenges. Primarily, for dynamic routing methods, the
dispersion of training tokens across multiple experts can lead to underfitting,
particularly for infrequent tokens. Additionally, while fixed routing methods
can mitigate that issue, they compromise on the diversity of representations.
In this paper, we propose \textbf{MaskMoE}, a method designed to enhance
token-level learning by employing a routing \textbf{mask}ing technique within
the \textbf{M}ixture-\textbf{o}f-\textbf{E}xperts model. MaskMoE is capable of
maintaining representation diversity while achieving more comprehensive
training. Experimental results demonstrate that our method outperforms previous
dominant Mixture-of-Experts models in terms of both perplexity (PPL) and
downstream task performance.","[{'name': 'Zhenpeng Su'}, {'name': 'Zijia Lin'}, {'name': 'Xue Bai'}, {'name': 'Xing Wu'}, {'name': 'Yizhe Xiong'}, {'name': 'Haoran Lian'}, {'name': 'Guangyuan Ma'}, {'name': 'Hui Chen'}, {'name': 'Guiguang Ding'}, {'name': 'Wei Zhou'}, {'name': 'Songlin Hu'}]",2024-07-13T09:22:33Z
http://arxiv.org/abs/2407.09801v1,http://arxiv.org/abs/2407.09801v1,IoT-LM: Large Multisensory Language Models for the Internet of Things,"The Internet of Things (IoT) network integrating billions of smart physical
devices embedded with sensors, software, and communication technologies is a
critical and rapidly expanding component of our modern world. The IoT ecosystem
provides a rich source of real-world modalities such as motion, thermal,
geolocation, imaging, depth, sensors, and audio to recognize the states of
humans and physical objects. Machine learning presents a rich opportunity to
automatically process IoT data at scale, enabling efficient inference for
understanding human wellbeing, controlling physical devices, and
interconnecting smart cities. To realize this potential, we introduce IoT-LM,
an open-source large multisensory language model tailored for the IoT
ecosystem. IoT-LM is enabled by two technical contributions: the first is
MultiIoT, the most expansive unified IoT dataset to date, encompassing over
1.15 million samples from 12 modalities and 8 tasks prepared for multisensory
pre-training and instruction-tuning. The second is a new multisensory multitask
adapter layer to condition pre-trained large language models on multisensory
IoT data. Not only does IoT-LM yield substantial improvements on 8 supervised
IoT classification tasks, but it also demonstrates new interactive
question-answering, reasoning, and dialog capabilities conditioned on IoT
sensors. We release IoT-LM's data sources and new multisensory language
modeling framework.","[{'name': 'Shentong Mo'}, {'name': 'Russ Salakhutdinov'}, {'name': 'Louis-Philippe Morency'}, {'name': 'Paul Pu Liang'}]",2024-07-13T08:20:37Z
http://arxiv.org/abs/2407.12866v1,http://arxiv.org/abs/2407.12866v1,Beyond KV Caching: Shared Attention for Efficient LLMs,"The efficiency of large language models (LLMs) remains a critical challenge,
particularly in contexts where computational resources are limited. Traditional
attention mechanisms in these models, while powerful, require significant
computational and memory resources due to the necessity of recalculating and
storing attention weights across different layers. This paper introduces a
novel Shared Attention (SA) mechanism, designed to enhance the efficiency of
LLMs by directly sharing computed attention weights across multiple layers.
Unlike previous methods that focus on sharing intermediate Key-Value (KV)
caches, our approach utilizes the isotropic tendencies of attention
distributions observed in advanced LLMs post-pretraining to reduce both the
computational flops and the size of the KV cache required during inference. We
empirically demonstrate that implementing SA across various LLMs results in
minimal accuracy loss on standard benchmarks. Our findings suggest that SA not
only conserves computational resources but also maintains robust model
performance, thereby facilitating the deployment of more efficient LLMs in
resource-constrained environments.","[{'name': 'Bingli Liao'}, {'name': 'Danilo Vasconcellos Vargas'}]",2024-07-13T07:23:07Z
http://arxiv.org/abs/2407.09756v1,http://arxiv.org/abs/2407.09756v1,"LLM-Collaboration on Automatic Science Journalism for the General
  Audience","Science journalism reports current scientific discoveries to non-specialists,
aiming to enable public comprehension of the state of the art. However, this
task can be challenging as the audience often lacks specific knowledge about
the presented research. To address this challenge, we propose a framework that
integrates three LLMs mimicking the real-world
writing-reading-feedback-revision workflow, with one LLM acting as the
journalist, a smaller LLM as the general public reader, and the third LLM as an
editor. The journalist's writing is iteratively refined by feedback from the
reader and suggestions from the editor. Our experiments demonstrate that by
leveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can
generate articles that are more accessible than those generated by existing
methods, including advanced models such as GPT-4.","[{'name': 'Gongyao Jiang'}, {'name': 'Xinran Shi'}, {'name': 'Qiong Luo'}]",2024-07-13T03:31:35Z
http://arxiv.org/abs/2407.09726v1,http://arxiv.org/abs/2407.09726v1,On Mitigating Code LLM Hallucinations with API Documentation,"In this study, we address the issue of API hallucinations in various software
engineering contexts. We introduce CloudAPIBench, a new benchmark designed to
measure API hallucination occurrences. CloudAPIBench also provides annotations
for frequencies of API occurrences in the public domain, allowing us to study
API hallucinations at various frequency levels. Our findings reveal that Code
LLMs struggle with low frequency APIs: for e.g., GPT-4o achieves only 38.58%
valid low frequency API invocations. We demonstrate that Documentation
Augmented Generation (DAG) significantly improves performance for low frequency
APIs (increase to 47.94% with DAG) but negatively impacts high frequency APIs
when using sub-optimal retrievers (a 39.02% absolute drop). To mitigate this,
we propose to intelligently trigger DAG where we check against an API index or
leverage Code LLMs' confidence scores to retrieve only when needed. We
demonstrate that our proposed methods enhance the balance between low and high
frequency API performance, resulting in more reliable API invocations (8.20%
absolute improvement on CloudAPIBench for GPT-4o).","[{'name': 'Nihal Jain'}, {'name': 'Robert Kwiatkowski'}, {'name': 'Baishakhi Ray'}, {'name': 'Murali Krishna Ramanathan'}, {'name': 'Varun Kumar'}]",2024-07-13T00:16:26Z
http://arxiv.org/abs/2407.09722v1,http://arxiv.org/abs/2407.09722v1,"Multi-Token Joint Speculative Decoding for Accelerating Large Language
  Model Inference","Transformer-based Large language models (LLMs) have demonstrated their power
in various tasks, but their inference incurs significant time and energy costs.
To accelerate LLM inference, speculative decoding uses a smaller model to
propose one sequence of tokens, which are subsequently validated in batch by
the target large model. Compared with autoregressive decoding, speculative
decoding generates the same number of tokens with fewer runs of the large
model, hence accelerating the overall inference by $1$-$2\times$. However,
greedy decoding is not the optimal decoding algorithm in terms of output
perplexity, which is a direct measurement of the effectiveness of a decoding
algorithm. An algorithm that has better output perplexity and even better
efficiency than speculative decoding can be more useful in practice. To achieve
this seemingly contradictory goal, we first introduce multi-token joint greedy
decoding (MJGD), which greedily generates multiple tokens at each step based on
their joint perplexity. We show that it leads to better perplexity for the
whole output. But the computation cost of MJGD is infeasible in practice. So we
further propose multi-token joint speculative decoding (MJSD), which
approximates and accelerates the MJGD from two aspects: it approximates the
joint distribution of the large model with that of a small model, and uses a
verification step to guarantee the accuracy of approximation; then it uses beam
decoding to accelerate the sequence generation from the joint distribution.
Compared with vanilla speculative decoding, MJSD has two advantages: (1) it is
an approximation of MJGD, thus achieving better output perplexity; (2)
verification with joint likelihood allows it to accept the longest prefix
sub-sequence of the draft tokens with valid perplexity, leading to better
efficiency...","[{'name': 'Zongyue Qin'}, {'name': 'Ziniu Hu'}, {'name': 'Zifan He'}, {'name': 'Neha Prakriya'}, {'name': 'Jason Cong'}, {'name': 'Yizhou Sun'}]",2024-07-12T23:29:54Z
http://arxiv.org/abs/2407.09709v1,http://arxiv.org/abs/2407.09709v1,GOFA: A Generative One-For-All Model for Joint Graph Language Modeling,"Foundation models, such as Large Language Models (LLMs) or Large Vision
Models (LVMs), have emerged as one of the most powerful tools in the respective
fields. However, unlike text and image data, graph data do not have a
definitive structure, posing great challenges to developing a Graph Foundation
Model (GFM). For example, current attempts at designing general graph models
either transform graph data into a language format for LLM-based prediction or
still train a GNN model with LLM as an assistant. The former can handle
unlimited tasks, while the latter captures graph structure much better -- yet,
no existing work can achieve both simultaneously. In this paper, we identify
three key desirable properties of a GFM: self-supervised pretraining, fluidity
in tasks, and graph awareness. To account for these properties, we extend the
conventional language modeling to the graph domain and propose a novel
generative graph language model GOFA to solve the problem. The model
interleaves randomly initialized GNN layers into a frozen pre-trained LLM so
that the semantic and structural modeling abilities are organically combined.
GOFA is pre-trained on newly proposed graph-level next-word prediction,
question-answering, and structural tasks to obtain the above GFM properties.
The pre-trained model is further fine-tuned on downstream tasks to obtain
task-solving ability. The fine-tuned model is evaluated on various downstream
tasks, demonstrating a strong ability to solve structural and contextual
problems in zero-shot scenarios. The code is available at
https://github.com/JiaruiFeng/GOFA.","[{'name': 'Lecheng Kong'}, {'name': 'Jiarui Feng'}, {'name': 'Hao Liu'}, {'name': 'Chengsong Huang'}, {'name': 'Jiaxin Huang'}, {'name': 'Yixin Chen'}, {'name': 'Muhan Zhang'}]",2024-07-12T22:23:51Z
http://arxiv.org/abs/2407.09704v1,http://arxiv.org/abs/2407.09704v1,"What an Elegant Bridge: Multilingual LLMs are Biased Similarly in
  Different Languages","This paper investigates biases of Large Language Models (LLMs) through the
lens of grammatical gender. Drawing inspiration from seminal works in
psycholinguistics, particularly the study of gender's influence on language
perception, we leverage multilingual LLMs to revisit and expand upon the
foundational experiments of Boroditsky (2003). Employing LLMs as a novel method
for examining psycholinguistic biases related to grammatical gender, we prompt
a model to describe nouns with adjectives in various languages, focusing
specifically on languages with grammatical gender. In particular, we look at
adjective co-occurrences across gender and languages, and train a binary
classifier to predict grammatical gender given adjectives an LLM uses to
describe a noun. Surprisingly, we find that a simple classifier can not only
predict noun gender above chance but also exhibit cross-language
transferability. We show that while LLMs may describe words differently in
different languages, they are biased similarly.","[{'name': 'Viktor Mihaylov'}, {'name': 'Aleksandar Shtedritski'}]",2024-07-12T22:10:16Z
http://arxiv.org/abs/2407.09688v1,http://arxiv.org/abs/2407.09688v1,"Large Language Models for Integrating Social Determinant of Health Data:
  A Case Study on Heart Failure 30-Day Readmission Prediction","Social determinants of health (SDOH) $-$ the myriad of circumstances in which
people live, grow, and age $-$ play an important role in health outcomes.
However, existing outcome prediction models often only use proxies of SDOH as
features. Recent open data initiatives present an opportunity to construct a
more comprehensive view of SDOH, but manually integrating the most relevant
data for individual patients becomes increasingly challenging as the volume and
diversity of public SDOH data grows. Large language models (LLMs) have shown
promise at automatically annotating structured data. Here, we conduct an
end-to-end case study evaluating the feasibility of using LLMs to integrate
SDOH data, and the utility of these SDOH features for clinical prediction. We
first manually label 700+ variables from two publicly-accessible SDOH data
sources to one of five semantic SDOH categories. Then, we benchmark performance
of 9 open-source LLMs on this classification task. Finally, we train ML models
to predict 30-day hospital readmission among 39k heart failure (HF) patients,
and we compare the prediction performance of the categorized SDOH variables
with standard clinical variables. Additionally, we investigate the impact of
few-shot LLM prompting on LLM annotation performance, and perform a metadata
ablation study on prompts to evaluate which information helps LLMs accurately
annotate these variables. We find that some open-source LLMs can effectively,
accurately annotate SDOH variables with zero-shot prompting without the need
for fine-tuning. Crucially, when combined with standard clinical features, the
LLM-annotated Neighborhood and Built Environment subset of the SDOH variables
shows the best performance predicting 30-day readmission of HF patients.","[{'name': 'Chase Fensore'}, {'name': 'Rodrigo M. Carrillo-Larco'}, {'name': 'Shivani A. Patel'}, {'name': 'Alanna A. Morris'}, {'name': 'Joyce C. Ho'}]",2024-07-12T21:14:06Z
http://arxiv.org/abs/2407.09661v1,http://arxiv.org/abs/2407.09661v1,Bridging Dictionary: AI-Generated Dictionary of Partisan Language Use,"Words often carry different meanings for people from diverse backgrounds.
Today's era of social polarization demands that we choose words carefully to
prevent miscommunication, especially in political communication and journalism.
To address this issue, we introduce the Bridging Dictionary, an interactive
tool designed to illuminate how words are perceived by people with different
political views. The Bridging Dictionary includes a static, printable document
featuring 796 terms with summaries generated by a large language model. These
summaries highlight how the terms are used distinctively by Republicans and
Democrats. Additionally, the Bridging Dictionary offers an interactive
interface that lets users explore selected words, visualizing their frequency,
sentiment, summaries, and examples across political divides. We present a use
case for journalists and emphasize the importance of human agency and trust in
further enhancing this tool. The deployed version of Bridging Dictionary is
available at https://dictionary.ccc-mit.org/.","[{'name': 'Hang Jiang'}, {'name': 'Doug Beeferman'}, {'name': 'William Brannon'}, {'name': 'Andrew Heyward'}, {'name': 'Deb Roy'}]",2024-07-12T19:44:40Z
http://arxiv.org/abs/2407.09653v2,http://arxiv.org/abs/2407.09653v2,"Bridging the Gap Between Information Seeking and Product Search Systems:
  Q&A Recommendation for E-commerce","Consumers on a shopping mission often leverage both product search and
information seeking systems, such as web search engines and Question Answering
(QA) systems, in an iterative process to improve their understanding of
available products and reach a purchase decision. While product search is
useful for shoppers to find the actual products meeting their requirements in
the catalog, information seeking systems can be utilized to answer any
questions they may have to refine those requirements. The recent success of
Large Language Models (LLMs) has opened up an opportunity to bridge the gap
between the two tasks to help customers achieve their goals quickly and
effectively by integrating conversational QA within product search. In this
paper, we propose to recommend users Question-Answer (Q&A) pairs that are
relevant to their product search and can help them make a purchase decision. We
discuss the different aspects of the problem including the requirements and
characteristics of the Q&A pairs, their generation, and the optimization of the
Q&A recommendation task. We highlight the challenges, open problems, and
suggested solutions to encourage future research in this emerging area.","[{'name': 'Saar Kuzi'}, {'name': 'Shervin Malmasi'}]",2024-07-12T19:22:17Z
http://arxiv.org/abs/2407.09652v1,http://arxiv.org/abs/2407.09652v1,"How Chinese are Chinese Language Models? The Puzzling Lack of Language
  Policy in China's LLMs","Contemporary language models are increasingly multilingual, but Chinese LLM
developers must navigate complex political and business considerations of
language diversity. Language policy in China aims at influencing the public
discourse and governing a multi-ethnic society, and has gradually transitioned
from a pluralist to a more assimilationist approach since 1949. We explore the
impact of these influences on current language technology. We evaluate six
open-source multilingual LLMs pre-trained by Chinese companies on 18 languages,
spanning a wide range of Chinese, Asian, and Anglo-European languages. Our
experiments show Chinese LLMs performance on diverse languages is
indistinguishable from international LLMs. Similarly, the models' technical
reports also show lack of consideration for pretraining data language coverage
except for English and Mandarin Chinese. Examining Chinese AI policy, model
experiments, and technical reports, we find no sign of any consistent policy,
either for or against, language diversity in China's LLM development. This
leaves a puzzling fact that while China regulates both the languages people use
daily as well as language model development, they do not seem to have any
policy on the languages in language models.","[{'name': 'Andrea W Wen-Yi'}, {'name': 'Unso Eun Seo Jo'}, {'name': 'Lu Jia Lin'}, {'name': 'David Mimno'}]",2024-07-12T19:21:40Z
http://arxiv.org/abs/2407.12865v1,http://arxiv.org/abs/2407.12865v1,"GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt
  Engineering","Prompt engineering for large language models (LLMs) is often a manual
time-intensive process that involves generating, evaluating, and refining
prompts iteratively to ensure high-quality outputs. While there has been work
on automating prompt engineering, the solutions generally are either tuned to
specific tasks with given answers or are quite costly. We introduce GRAD-SUM, a
scalable and flexible method for automatic prompt engineering that builds on
gradient-based optimization techniques. Our approach incorporates user-defined
task descriptions and evaluation criteria, and features a novel gradient
summarization module to generalize feedback effectively. Our results
demonstrate that GRAD-SUM consistently outperforms existing methods across
various benchmarks, highlighting its versatility and effectiveness in automatic
prompt optimization.","[{'name': 'Derek Austin'}, {'name': 'Elliott Chartock'}]",2024-07-12T19:11:21Z
http://arxiv.org/abs/2407.09453v1,http://arxiv.org/abs/2407.09453v1,"Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators","Nowadays, increasingly larger Deep Neural Networks (DNNs) are being
developed, trained, and utilized. These networks require significant
computational resources, putting a strain on both advanced and limited devices.
Our solution is to implement {\em weight block sparsity}, which is a structured
sparsity that is friendly to hardware. By zeroing certain sections of the
convolution and fully connected layers parameters of pre-trained DNN models, we
can efficiently speed up the DNN's inference process. This results in a smaller
memory footprint, faster communication, and fewer operations.
  Our work presents a vertical system that allows for the training of
convolution and matrix multiplication weights to exploit 8x8 block sparsity on
a single GPU within a reasonable amount of time. Compilers recognize this
sparsity and use it for both data compaction and computation splitting into
threads. Blocks like these take full advantage of both spatial and temporal
locality, paving the way for fast vector operations and memory reuse. By using
this system on a Resnet50 model, we were able to reduce the weight by half with
minimal accuracy loss, resulting in a two-times faster inference speed. We will
present performance estimates using accurate and complete code generation for
AIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, and
VGG16 to demonstrate the necessary synergy between hardware overlay designs and
software stacks for compiling and executing machine learning applications.","[{'name': ""Paolo D'Alberto""}, {'name': 'Taehee Jeong'}, {'name': 'Akshai Jain'}, {'name': 'Shreyas Manjunath'}, {'name': 'Mrinal Sarmah'}, {'name': 'Samuel Hsu'}, {'name': 'Yaswanth Raparti'}, {'name': 'Nitesh Pipralia'}]",2024-07-12T17:37:49Z
http://arxiv.org/abs/2407.09450v1,http://arxiv.org/abs/2407.09450v1,Human-like Episodic Memory for Infinite Context LLMs,"Large language models (LLMs) have shown remarkable capabilities, but still
struggle with processing extensive contexts, limiting their ability to maintain
coherence and accuracy over long sequences. In contrast, the human brain excels
at organising and retrieving episodic experiences across vast temporal scales,
spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that
integrates key aspects of human episodic memory and event cognition into LLMs,
enabling them to effectively handle practically infinite context lengths while
maintaining computational efficiency. EM-LLM organises sequences of tokens into
coherent episodic events using a combination of Bayesian surprise and
graph-theoretic boundary refinement in an on-line fashion. When needed, these
events are retrieved through a two-stage memory process, combining
similarity-based and temporally contiguous retrieval for efficient and
human-like access to relevant information. Experiments on the LongBench dataset
demonstrate EM-LLM's superior performance, outperforming the state-of-the-art
InfLLM model with an overall relative improvement of 4.3% across various tasks,
including a 33% improvement on the PassageRetrieval task. Furthermore, our
analysis reveals strong correlations between EM-LLM's event segmentation and
human-perceived events, suggesting a bridge between this artificial system and
its biological counterpart. This work not only advances LLM capabilities in
processing extended contexts but also provides a computational framework for
exploring human memory mechanisms, opening new avenues for interdisciplinary
research in AI and cognitive science.","[{'name': 'Zafeirios Fountas'}, {'name': 'Martin A Benfeghoul'}, {'name': 'Adnan Oomerjee'}, {'name': 'Fenia Christopoulou'}, {'name': 'Gerasimos Lampouras'}, {'name': 'Haitham Bou-Ammar'}, {'name': 'Jun Wang'}]",2024-07-12T17:34:03Z
http://arxiv.org/abs/2407.09447v1,http://arxiv.org/abs/2407.09447v1,"ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to
  Identify Likely Toxic Prompts","Typical schemes for automated red-teaming large language models (LLMs) focus
on discovering prompts that trigger a frozen language model (the defender) to
generate toxic text. This often results in the prompting model (the adversary)
producing text that is unintelligible and unlikely to arise. Here, we propose a
reinforcement learning formulation of the LLM red-teaming task which allows us
to discover prompts that both (1) trigger toxic outputs from a frozen defender
and (2) have low perplexity as scored by the defender. We argue these cases are
most pertinent in a red-teaming setting because of their likelihood to arise
during normal use of the defender model. We solve this formulation through a
novel online and weakly supervised variant of Identity Preference Optimization
(IPO) on GPT-2 and GPT-2 XL defenders. We demonstrate that our policy is
capable of generating likely prompts that also trigger toxicity. Finally, we
qualitatively analyze learned strategies, trade-offs of likelihood and
toxicity, and discuss implications. Source code is available for this project
at: https://github.com/sisl/ASTPrompter/.","[{'name': 'Amelia F. Hardy'}, {'name': 'Houjun Liu'}, {'name': 'Bernard Lange'}, {'name': 'Mykel J. Kochenderfer'}]",2024-07-12T17:33:34Z
http://arxiv.org/abs/2407.09590v1,http://arxiv.org/abs/2407.09590v1,"Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse
  Mixture-of-Experts","By increasing model parameters but activating them sparsely when performing a
task, the use of Mixture-of-Experts (MoE) architecture significantly improves
the performance of Large Language Models (LLMs) without increasing the
inference cost. However, the memory consumption due to the growing number of
experts presents a challenge to the deployment of these models in many real
world settings. Our empirical study reveals that some experts encode redundant
knowledge during pre-training. We thus propose a method of grouping and pruning
similar experts to improve model's parameter efficiency. We validate the
effectiveness of our method by pruning two state-of-the-art MoE models,
Mixtral-8x7B and Mixtral-8x22B. Evaluation shows that our method outperforms
other model pruning methods on a range of natural language tasks. To facilitate
future research, we will release our code and the pruned MoE models.","[{'name': 'Zeliang Zhang'}, {'name': 'Xiaodong Liu'}, {'name': 'Hao Cheng'}, {'name': 'Chenliang Xu'}, {'name': 'Jianfeng Gao'}]",2024-07-12T17:25:02Z
http://arxiv.org/abs/2407.09429v1,http://arxiv.org/abs/2407.09429v1,Open (Clinical) LLMs are Sensitive to Instruction Phrasings,"Instruction-tuned Large Language Models (LLMs) can perform a wide range of
tasks given natural language instructions to do so, but they are sensitive to
how such instructions are phrased. This issue is especially concerning in
healthcare, as clinicians are unlikely to be experienced prompt engineers and
the potential consequences of inaccurate outputs are heightened in this domain.
  This raises a practical question: How robust are instruction-tuned LLMs to
natural variations in the instructions provided for clinical NLP tasks? We
collect prompts from medical doctors across a range of tasks and quantify the
sensitivity of seven LLMs -- some general, others specialized -- to natural
(i.e., non-adversarial) instruction phrasings. We find that performance varies
substantially across all models, and that -- perhaps surprisingly --
domain-specific models explicitly trained on clinical data are especially
brittle, compared to their general domain counterparts. Further, arbitrary
phrasing differences can affect fairness, e.g., valid but distinct instructions
for mortality prediction yield a range both in overall performance, and in
terms of differences between demographic groups.","[{'name': 'Alberto Mario Ceballos Arroyo'}, {'name': 'Monica Munnangi'}, {'name': 'Jiuding Sun'}, {'name': 'Karen Y. C. Zhang'}, {'name': 'Denis Jered McInerney'}, {'name': 'Byron C. Wallace'}, {'name': 'Silvio Amir'}]",2024-07-12T17:00:44Z
http://arxiv.org/abs/2407.09417v2,http://arxiv.org/abs/2407.09417v2,Mitigating Entity-Level Hallucination in Large Language Models,"The emergence of Large Language Models (LLMs) has revolutionized how users
access information, shifting from traditional search engines to direct
question-and-answer interactions with LLMs. However, the widespread adoption of
LLMs has revealed a significant challenge known as hallucination, wherein LLMs
generate coherent yet factually inaccurate responses. This hallucination
phenomenon has led to users' distrust in information retrieval systems based on
LLMs. To tackle this challenge, this paper proposes Dynamic Retrieval
Augmentation based on hallucination Detection (DRAD) as a novel method to
detect and mitigate hallucinations in LLMs. DRAD improves upon traditional
retrieval augmentation by dynamically adapting the retrieval process based on
real-time hallucination detection. It features two main components: Real-time
Hallucination Detection (RHD) for identifying potential hallucinations without
external models, and Self-correction based on External Knowledge (SEK) for
correcting these errors using external knowledge. Experiment results show that
DRAD demonstrates superior performance in both detecting and mitigating
hallucinations in LLMs. All of our code and data are open-sourced at
https://github.com/oneal2000/EntityHallucination.","[{'name': 'Weihang Su'}, {'name': 'Yichen Tang'}, {'name': 'Qingyao Ai'}, {'name': 'Changyue Wang'}, {'name': 'Zhijing Wu'}, {'name': 'Yiqun Liu'}]",2024-07-12T16:47:34Z
http://arxiv.org/abs/2407.09413v1,http://arxiv.org/abs/2407.09413v1,SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers,"Seeking answers to questions within long scientific research articles is a
crucial area of study that aids readers in quickly addressing their inquiries.
However, existing question-answering (QA) datasets based on scientific papers
are limited in scale and focus solely on textual content. To address this
limitation, we introduce SPIQA (Scientific Paper Image Question Answering), the
first large-scale QA dataset specifically designed to interpret complex figures
and tables within the context of scientific research articles across various
domains of computer science. Leveraging the breadth of expertise and ability of
multimodal large language models (MLLMs) to understand figures, we employ
automatic and manual curation to create the dataset. We craft an
information-seeking task involving multiple images that cover a wide variety of
plots, charts, tables, schematic diagrams, and result visualizations. SPIQA
comprises 270K questions divided into training, validation, and three different
evaluation splits. Through extensive experiments with 12 prominent foundational
models, we evaluate the ability of current multimodal systems to comprehend the
nuanced aspects of research articles. Additionally, we propose a
Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that
allows fine-grained, step-by-step assessment and improves model performance. We
further explore the upper bounds of performance enhancement with additional
textual information, highlighting its promising potential for future research
and the dataset's impact on revolutionizing how we interact with scientific
literature.","[{'name': 'Shraman Pramanick'}, {'name': 'Rama Chellappa'}, {'name': 'Subhashini Venugopalan'}]",2024-07-12T16:37:59Z
http://arxiv.org/abs/2407.09395v1,http://arxiv.org/abs/2407.09395v1,"Deep Bag-of-Words Model: An Efficient and Interpretable Relevance
  Architecture for Chinese E-Commerce","Text relevance or text matching of query and product is an essential
technique for the e-commerce search system to ensure that the displayed
products can match the intent of the query. Many studies focus on improving the
performance of the relevance model in search system. Recently, pre-trained
language models like BERT have achieved promising performance on the text
relevance task. While these models perform well on the offline test dataset,
there are still obstacles to deploy the pre-trained language model to the
online system as their high latency. The two-tower model is extensively
employed in industrial scenarios, owing to its ability to harmonize performance
with computational efficiency. Regrettably, such models present an opaque
``black box'' nature, which prevents developers from making special
optimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an
efficient and interpretable relevance architecture for Chinese e-commerce. Our
approach proposes to encode the query and the product into the sparse BoW
representation, which is a set of word-weight pairs. The weight means the
important or the relevant score between the corresponding word and the raw
text. The relevance score is measured by the accumulation of the matched word
between the sparse BoW representation of the query and the product. Compared to
popular dense distributed representation that usually suffers from the drawback
of black-box, the most advantage of the proposed representation model is highly
explainable and interventionable, which is a superior advantage to the
deployment and operation of online search engines. Moreover, the online
efficiency of the proposed model is even better than the most efficient inner
product form of dense representation ...","[{'name': 'Zhe Lin'}, {'name': 'Jiwei Tan'}, {'name': 'Dan Ou'}, {'name': 'Xi Chen'}, {'name': 'Shaowei Yao'}, {'name': 'Bo Zheng'}]",2024-07-12T16:18:05Z
http://arxiv.org/abs/2407.09364v1,http://arxiv.org/abs/2407.09364v1,"Is Contrasting All You Need? Contrastive Learning for the Detection and
  Attribution of AI-generated Text","The significant progress in the development of Large Language Models has
contributed to blurring the distinction between human and AI-generated text.
The increasing pervasiveness of AI-generated text and the difficulty in
detecting it poses new challenges for our society. In this paper, we tackle the
problem of detecting and attributing AI-generated text by proposing WhosAI, a
triplet-network contrastive learning framework designed to predict whether a
given input text has been generated by humans or AI and to unveil the
authorship of the text. Unlike most existing approaches, our proposed framework
is conceived to learn semantic similarity representations from multiple
generators at once, thus equally handling both detection and attribution tasks.
Furthermore, WhosAI is model-agnostic and scalable to the release of new AI
text-generation models by incorporating their generated instances into the
embedding space learned by our framework. Experimental results on the
TuringBench benchmark of 200K news articles show that our proposed framework
achieves outstanding results in both the Turing Test and Authorship Attribution
tasks, outperforming all the methods listed in the TuringBench benchmark
leaderboards.","[{'name': 'Lucio La Cava'}, {'name': 'Davide Costa'}, {'name': 'Andrea Tagarelli'}]",2024-07-12T15:44:56Z
http://arxiv.org/abs/2407.09327v1,http://arxiv.org/abs/2407.09327v1,"Sina at FigNews 2024: Multilingual Datasets Annotated with Bias and
  Propaganda","The proliferation of bias and propaganda on social media is an increasingly
significant concern, leading to the development of techniques for automatic
detection. This article presents a multilingual corpus of 12, 000 Facebook
posts fully annotated for bias and propaganda. The corpus was created as part
of the FigNews 2024 Shared Task on News Media Narratives for framing the
Israeli War on Gaza. It covers various events during the War from October 7,
2023 to January 31, 2024. The corpus comprises 12, 000 posts in five languages
(Arabic, Hebrew, English, French, and Hindi), with 2, 400 posts for each
language. The annotation process involved 10 graduate students specializing in
Law. The Inter-Annotator Agreement (IAA) was used to evaluate the annotations
of the corpus, with an average IAA of 80.8% for bias and 70.15% for propaganda
annotations. Our team was ranked among the bestperforming teams in both Bias
and Propaganda subtasks. The corpus is open-source and available at
https://sina.birzeit.edu/fada","[{'name': 'Lina Duaibes'}, {'name': 'Areej Jaber'}, {'name': 'Mustafa Jarrar'}, {'name': 'Ahmad Qadi'}, {'name': 'Mais Qandeel'}]",2024-07-12T15:04:09Z
http://arxiv.org/abs/2407.09311v1,http://arxiv.org/abs/2407.09311v1,"Scalability of Bayesian Network Structure Elicitation with Large
  Language Models: a Novel Methodology and Comparative Analysis","In this work, we propose a novel method for Bayesian Networks (BNs) structure
elicitation that is based on the initialization of several LLMs with different
experiences, independently querying them to create a structure of the BN, and
further obtaining the final structure by majority voting. We compare the method
with one alternative method on various widely and not widely known BNs of
different sizes and study the scalability of both methods on them. We also
propose an approach to check the contamination of BNs in LLM, which shows that
some widely known BNs are inapplicable for testing the LLM usage for BNs
structure elicitation. We also show that some BNs may be inapplicable for such
experiments because their node names are indistinguishable. The experiments on
the other BNs show that our method performs better than the existing method
with one of the three studied LLMs; however, the performance of both methods
significantly decreases with the increase in BN size.","[{'name': 'Nikolay Babakov'}, {'name': 'Ehud Reiter'}, {'name': 'Alberto Bugarin'}]",2024-07-12T14:52:13Z
http://arxiv.org/abs/2407.09298v2,http://arxiv.org/abs/2407.09298v2,Transformer Layers as Painters,"Despite their nearly universal adoption for large language models, the
internal workings of transformers are not well understood. We aim to better
understand the impact of removing or reorganizing information throughout the
layers of a pretrained transformer. Such an understanding could both yield
better usage of existing models as well as to make architectural improvements
to produce new variants. We present a series of empirical studies on frozen
models that show that the lower and final layers of pretrained transformers
differ from middle layers, but that middle layers have a surprising amount of
uniformity. We further show that some classes of problems have robustness to
skipping layers, running the layers in an order different from how they were
trained, or running the layers in parallel. Our observations suggest that even
frozen pretrained models may gracefully trade accuracy for latency by skipping
layers or running layers in parallel.","[{'name': 'Qi Sun'}, {'name': 'Marc Pickett'}, {'name': 'Aakash Kumar Nain'}, {'name': 'Llion Jones'}]",2024-07-12T14:31:05Z
http://arxiv.org/abs/2407.09276v1,http://arxiv.org/abs/2407.09276v1,H2O-Danube3 Technical Report,"We present H2O-Danube3, a series of small language models consisting of
H2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T
tokens. Our models are pre-trained on high quality Web data consisting of
primarily English tokens in three stages with different data mixes before final
supervised tuning for chat version. The models exhibit highly competitive
metrics across a multitude of academic, chat, and fine-tuning benchmarks.
Thanks to its compact architecture, H2O-Danube3 can be efficiently run on a
modern smartphone, enabling local inference and rapid processing capabilities
even on mobile devices. We make all models openly available under Apache 2.0
license further democratizing LLMs to a wider audience economically.","[{'name': 'Pascal Pfeiffer'}, {'name': 'Philipp Singer'}, {'name': 'Yauhen Babakhin'}, {'name': 'Gabor Fodor'}, {'name': 'Nischay Dhankhar'}, {'name': 'Sri Satish Ambati'}]",2024-07-12T14:09:40Z
http://arxiv.org/abs/2407.09252v2,http://arxiv.org/abs/2407.09252v2,Context Embeddings for Efficient Answer Generation in RAG,"Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge
of LLMs by extending the input with external information. As a consequence, the
contextual inputs to the model become much longer which slows down decoding
time directly translating to the time a user has to wait for an answer. We
address this challenge by presenting COCOM, an effective context compression
method, reducing long contexts to only a handful of Context Embeddings speeding
up the generation time by a large margin. Our method allows for different
compression rates trading off decoding time for answer quality. Compared to
earlier methods, COCOM allows for handling multiple contexts more effectively,
significantly reducing decoding time for long inputs. Our method demonstrates a
speed-up of up to 5.69 $\times$ while achieving higher performance compared to
existing efficient context compression methods.","[{'name': 'David Rau'}, {'name': 'Shuai Wang'}, {'name': 'Hervé Déjean'}, {'name': 'Stéphane Clinchant'}]",2024-07-12T13:30:44Z
http://arxiv.org/abs/2407.12863v1,http://arxiv.org/abs/2407.12863v1,"Token-Supervised Value Models for Enhancing Mathematical Reasoning
  Capabilities of Large Language Models","Large Language Models (LLMs) have demonstrated impressive problem-solving
capabilities in mathematics through step-by-step reasoning chains. However,
they are susceptible to reasoning errors that impact the quality of subsequent
reasoning chains and the final answer due to language models' autoregressive
token-by-token generating nature. Recent works have proposed adopting external
verifiers to guide the generation of reasoning paths, but existing works
utilize models that have been trained with step-by-step labels to assess the
correctness of token-by-token reasoning chains. Consequently, they struggle to
recognize discriminative details of tokens within a reasoning path and lack the
ability to evaluate whether an intermediate reasoning path is on a promising
track toward the correct final answer. To amend the lack of sound and
token-grained math-verification signals, we devise a novel training scheme for
verifiers that apply token-level supervision with the expected cumulative
reward (i.e., value). Furthermore, we propose a practical formulation of the
cumulative reward by reducing it to finding the probability of future
correctness of the final answer and thereby enabling the empirical estimation
of the value. Experimental results on mathematical reasoning benchmarks show
that Token-Supervised Value Model (TVM) can outperform step-by-step verifiers
on GSM8K and MATH with Mistral and Llama.","[{'name': 'Jung Hyun Lee'}, {'name': 'June Yong Yang'}, {'name': 'Byeongho Heo'}, {'name': 'Dongyoon Han'}, {'name': 'Kang Min Yoo'}]",2024-07-12T13:16:50Z
http://arxiv.org/abs/2407.09241v1,http://arxiv.org/abs/2407.09241v1,The Sociolinguistic Foundations of Language Modeling,"In this paper, we introduce a sociolinguistic perspective on language
modeling. We claim that large language models are inherently models of
varieties of language, and we consider how this insight can inform the
development and deployment of large language models. We begin by presenting a
technical definition of the concept of a variety of language as developed in
sociolinguistics. We then discuss how this perspective can help address five
basic challenges in language modeling: social bias, domain adaptation,
alignment, language change, and scale. Ultimately, we argue that it is crucial
to carefully define and compile training corpora that accurately represent the
specific varieties of language being modeled to maximize the performance and
societal value of large language models.","[{'name': 'Jack Grieve'}, {'name': 'Sara Bartl'}, {'name': 'Matteo Fuoli'}, {'name': 'Jason Grafmiller'}, {'name': 'Weihang Huang'}, {'name': 'Alejandro Jawerbaum'}, {'name': 'Akira Murakami'}, {'name': 'Marcus Perlman'}, {'name': 'Dana Roemling'}, {'name': 'Bodo Winter'}]",2024-07-12T13:12:55Z
http://arxiv.org/abs/2407.09209v2,http://arxiv.org/abs/2407.09209v2,Pronunciation Assessment with Multi-modal Large Language Models,"Large language models (LLMs), renowned for their powerful conversational
abilities, are widely recognized as exceptional tools in the field of
education, particularly in the context of automated intelligent instruction
systems for language learning. In this paper, we propose a scoring system based
on LLMs, motivated by their positive impact on text-related scoring tasks.
Specifically, the speech encoder first maps the learner's speech into
contextual features. The adapter layer then transforms these features to align
with the text embedding in latent space. The assessment task-specific prefix
and prompt text are embedded and concatenated with the features generated by
the modality adapter layer, enabling the LLMs to predict accuracy and fluency
scores. Our experiments demonstrate that the proposed scoring systems achieve
competitive results compared to the baselines on the Speechocean762 datasets.
Moreover, we also conducted an ablation study to better understand the
contributions of the prompt text and training strategy in the proposed scoring
system.","[{'name': 'Kaiqi Fu'}, {'name': 'Linkai Peng'}, {'name': 'Nan Yang'}, {'name': 'Shuran Zhou'}]",2024-07-12T12:16:14Z
http://arxiv.org/abs/2407.09197v1,http://arxiv.org/abs/2407.09197v1,A Chatbot for Asylum-Seeking Migrants in Europe,"We present ACME: A Chatbot for asylum-seeking Migrants in Europe. ACME relies
on computational argumentation and aims to help migrants identify the highest
level of protection they can apply for. This would contribute to a more
sustainable migration by reducing the load on territorial commissions, Courts,
and humanitarian organizations supporting asylum applicants. We describe the
context, system architectures, technologies, and the case study used to run the
demonstration.","[{'name': 'Bettina Fazzinga'}, {'name': 'Elena Palmieri'}, {'name': 'Margherita Vestoso'}, {'name': 'Luca Bolognini'}, {'name': 'Andrea Galassi'}, {'name': 'Filippo Furfaro'}, {'name': 'Paolo Torroni'}]",2024-07-12T11:53:40Z
http://arxiv.org/abs/2407.09187v1,http://arxiv.org/abs/2407.09187v1,"Enhancing Depressive Post Detection in Bangla: A Comparative Study of
  TF-IDF, BERT and FastText Embeddings","Due to massive adoption of social media, detection of users' depression
through social media analytics bears significant importance, particularly for
underrepresented languages, such as Bangla. This study introduces a
well-grounded approach to identify depressive social media posts in Bangla, by
employing advanced natural language processing techniques. The dataset used in
this work, annotated by domain experts, includes both depressive and
non-depressive posts, ensuring high-quality data for model training and
evaluation. To address the prevalent issue of class imbalance, we utilised
random oversampling for the minority class, thereby enhancing the model's
ability to accurately detect depressive posts. We explored various numerical
representation techniques, including Term Frequency-Inverse Document Frequency
(TF-IDF), Bidirectional Encoder Representations from Transformers (BERT)
embedding and FastText embedding, by integrating them with a deep
learning-based Convolutional Neural Network-Bidirectional Long Short-Term
Memory (CNN-BiLSTM) model. The results obtained through extensive
experimentation, indicate that the BERT approach performed better the others,
achieving a F1-score of 84%. This indicates that BERT, in combination with the
CNN-BiLSTM architecture, effectively recognises the nuances of Bangla texts
relevant to depressive contents. Comparative analysis with the existing
state-of-the-art methods demonstrates that our approach with BERT embedding
performs better than others in terms of evaluation metrics and the reliability
of dataset annotations. Our research significantly contribution to the
development of reliable tools for detecting depressive posts in the Bangla
language. By highlighting the efficacy of different embedding techniques and
deep learning models, this study paves the way for improved mental health
monitoring through social media platforms.","[{'name': 'Saad Ahmed Sazan'}, {'name': 'Mahdi H. Miraz'}, {'name': 'A B M Muntasir Rahman'}]",2024-07-12T11:40:17Z
http://arxiv.org/abs/2407.09184v1,http://arxiv.org/abs/2407.09184v1,"Does Incomplete Syntax Influence Korean Language Model? Focusing on Word
  Order and Case Markers","Syntactic elements, such as word order and case markers, are fundamental in
natural language processing. Recent studies show that syntactic information
boosts language model performance and offers clues for people to understand
their learning mechanisms. Unlike languages with a fixed word order such as
English, Korean allows for varied word sequences, despite its canonical
structure, due to case markers that indicate the functions of sentence
components. This study explores whether Korean language models can accurately
capture this flexibility. We note that incomplete word orders and omitted case
markers frequently appear in ordinary Korean communication. To investigate this
further, we introduce the Syntactically Incomplete Korean (SIKO) dataset.
Through SIKO, we assessed Korean language models' flexibility with incomplete
syntax and confirmed the dataset's training value. Results indicate these
models reflect Korean's inherent flexibility, accurately handling incomplete
inputs. Moreover, fine-tuning with SIKO enhances the ability to handle common
incomplete Korean syntactic forms. The dataset's simple construction process,
coupled with significant performance enhancements, solidifies its standing as
an effective data augmentation technique.","[{'name': 'Jong Myoung Kim'}, {'name': 'Young-Jun Lee'}, {'name': 'Yong-jin Han'}, {'name': 'Sangkeun Jung'}, {'name': 'Ho-Jin Choi'}]",2024-07-12T11:33:41Z
http://arxiv.org/abs/2407.13787v2,http://arxiv.org/abs/2407.13787v2,"The Honorific Effect: Exploring the Impact of Japanese Linguistic
  Formalities on AI-Generated Physics Explanations","This study investigates the influence of Japanese honorifics on the responses
of large language models (LLMs) when explaining the law of conservation of
momentum. We analyzed the outputs of six state-of-the-art AI models, including
variations of ChatGPT, Coral, and Gemini, using 14 different honorific forms.
Our findings reveal that honorifics significantly affect the quality,
consistency, and formality of AI-generated responses, demonstrating LLMs'
ability to interpret and adapt to social context cues embedded in language.
Notable variations were observed across different models, with some emphasizing
historical context and derivations, while others focused on intuitive
explanations. The study highlights the potential for using honorifics to adjust
the depth and complexity of AI-generated explanations in educational contexts.
Furthermore, the responsiveness of AI models to cultural linguistic elements
underscores the importance of considering cultural factors in AI development
for educational applications. These results open new avenues for research in
AI-assisted education and cultural adaptation in AI systems, with significant
implications for personalizing learning experiences and developing culturally
sensitive AI tools for global education.",[{'name': 'Keisuke Sato'}],2024-07-12T11:31:00Z
http://arxiv.org/abs/2407.09181v1,http://arxiv.org/abs/2407.09181v1,Exploring the Effectiveness of Methods for Persona Extraction,"The paper presents a study of methods for extracting information about
dialogue participants and evaluating their performance in Russian. To train
models for this task, the Multi-Session Chat dataset was translated into
Russian using multiple translation models, resulting in improved data quality.
A metric based on the F-score concept is presented to evaluate the
effectiveness of the extraction models. The metric uses a trained classifier to
identify the dialogue participant to whom the persona belongs. Experiments were
conducted on MBart, FRED-T5, Starling-7B, which is based on the Mistral, and
Encoder2Encoder models. The results demonstrated that all models exhibited an
insufficient level of recall in the persona extraction task. The incorporation
of the NCE Loss improved the model's precision at the expense of its recall.
Furthermore, increasing the model's size led to enhanced extraction of
personas.",[{'name': 'Konstantin Zaitsev'}],2024-07-12T11:30:10Z
http://arxiv.org/abs/2407.09152v1,http://arxiv.org/abs/2407.09152v1,"The Two Sides of the Coin: Hallucination Generation and Detection with
  LLMs as Evaluators for LLMs","Hallucination detection in Large Language Models (LLMs) is crucial for
ensuring their reliability. This work presents our participation in the CLEF
ELOQUENT HalluciGen shared task, where the goal is to develop evaluators for
both generating and detecting hallucinated content. We explored the
capabilities of four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, for this
purpose. We also employed ensemble majority voting to incorporate all four
models for the detection task. The results provide valuable insights into the
strengths and weaknesses of these LLMs in handling hallucination generation and
detection tasks.","[{'name': 'Anh Thu Maria Bui'}, {'name': 'Saskia Felizitas Brech'}, {'name': 'Natalie Hußfeldt'}, {'name': 'Tobias Jennert'}, {'name': 'Melanie Ullrich'}, {'name': 'Timo Breuer'}, {'name': 'Narjes Nikzad Khasmakhi'}, {'name': 'Philipp Schaer'}]",2024-07-12T10:34:46Z
http://arxiv.org/abs/2407.09137v1,http://arxiv.org/abs/2407.09137v1,"A Look Into News Avoidance Through AWRS: An Avoidance-Aware Recommender
  System","In recent years, journalists have expressed concerns about the increasing
trend of news article avoidance, especially within specific domains. This issue
has been exacerbated by the rise of recommender systems. Our research indicates
that recommender systems should consider avoidance as a fundamental factor. We
argue that news articles can be characterized by three principal elements:
exposure, relevance, and avoidance, all of which are closely interconnected. To
address these challenges, we introduce AWRS, an Avoidance-Aware Recommender
System. This framework incorporates avoidance awareness when recommending news,
based on the premise that news article avoidance conveys significant
information about user preferences. Evaluation results on three news datasets
in different languages (English, Norwegian, and Japanese) demonstrate that our
method outperforms existing approaches.","[{'name': 'Igor L. R. Azevedo'}, {'name': 'Toyotaro Suzumura'}, {'name': 'Yuichiro Yasui'}]",2024-07-12T10:16:03Z
http://arxiv.org/abs/2407.09136v1,http://arxiv.org/abs/2407.09136v1,"Stepwise Verification and Remediation of Student Reasoning Errors with
  Large Language Model Tutors","Large language models (LLMs) present an opportunity to scale high-quality
personalized education to all. A promising approach towards this means is to
build dialog tutoring models that scaffold students' problem-solving. However,
even though existing LLMs perform well in solving reasoning questions, they
struggle to precisely detect student's errors and tailor their feedback to
these errors. Inspired by real-world teaching practice where teachers identify
student errors and customize their response based on them, we focus on
verifying student solutions and show how grounding to such verification
improves the overall quality of tutor response generation. We collect a dataset
of 1K stepwise math reasoning chains with the first error step annotated by
teachers. We show empirically that finding the mistake in a student solution is
challenging for current models. We propose and evaluate several verifiers for
detecting these errors. Using both automatic and human evaluation we show that
the student solution verifiers steer the generation model towards highly
targeted responses to student errors which are more often correct with less
hallucinations compared to existing baselines.","[{'name': 'Nico Daheim'}, {'name': 'Jakub Macina'}, {'name': 'Manu Kapur'}, {'name': 'Iryna Gurevych'}, {'name': 'Mrinmaya Sachan'}]",2024-07-12T10:11:40Z
http://arxiv.org/abs/2407.09121v1,http://arxiv.org/abs/2407.09121v1,"Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled
  Refusal Training","This study addresses a critical gap in safety tuning practices for Large
Language Models (LLMs) by identifying and tackling a refusal position bias
within safety tuning data, which compromises the models' ability to
appropriately refuse generating unsafe content. We introduce a novel approach,
Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse
compliance to harmful prompts at any response position, significantly enhancing
their safety capabilities. DeRTa incorporates two novel components: (1) Maximum
Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models
to recognize and avoid unsafe content by appending a segment of harmful
response to the beginning of a safe response, and (2) Reinforced Transition
Optimization (RTO), which equips models with the ability to transition from
potential harm to safety refusal consistently throughout the harmful response
sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model
families across six attack scenarios, demonstrates that our method not only
improves model safety without compromising performance but also surpasses
well-known models such as GPT-4 in defending against attacks. Importantly, our
approach successfully defends recent advanced attack methods (e.g., CodeAttack)
that have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be
found at https://github.com/RobustNLP/DeRTa.","[{'name': 'Youliang Yuan'}, {'name': 'Wenxiang Jiao'}, {'name': 'Wenxuan Wang'}, {'name': 'Jen-tse Huang'}, {'name': 'Jiahao Xu'}, {'name': 'Tian Liang'}, {'name': 'Pinjia He'}, {'name': 'Zhaopeng Tu'}]",2024-07-12T09:36:33Z
http://arxiv.org/abs/2407.09120v1,http://arxiv.org/abs/2407.09120v1,"URRL-IMVC: Unified and Robust Representation Learning for Incomplete
  Multi-View Clustering","Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that
are only partially available. This poses two main challenges: effectively
leveraging multi-view information and mitigating the impact of missing views.
Prevailing solutions employ cross-view contrastive learning and missing view
recovery techniques. However, they either neglect valuable complementary
information by focusing only on consensus between views or provide unreliable
recovered views due to the absence of supervision. To address these
limitations, we propose a novel Unified and Robust Representation Learning for
Incomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a
unified embedding that is robust to view missing conditions by integrating
information from multiple views and neighboring samples. Firstly, to overcome
the limitations of cross-view contrastive learning, URRL-IMVC incorporates an
attention-based auto-encoder framework to fuse multi-view information and
generate unified embeddings. Secondly, URRL-IMVC directly enhances the
robustness of the unified embedding against view-missing conditions through KNN
imputation and data augmentation techniques, eliminating the need for explicit
missing view recovery. Finally, incremental improvements are introduced to
further enhance the overall performance, such as the Clustering Module and the
customization of the Encoder. We extensively evaluate the proposed URRL-IMVC
framework on various benchmark datasets, demonstrating its state-of-the-art
performance. Furthermore, comprehensive ablation studies are performed to
validate the effectiveness of our design.","[{'name': 'Ge Teng'}, {'name': 'Ting Mao'}, {'name': 'Chen Shen'}, {'name': 'Xiang Tian'}, {'name': 'Xuesong Liu'}, {'name': 'Yaowu Chen'}, {'name': 'Jieping Ye'}]",2024-07-12T09:35:25Z
http://arxiv.org/abs/2407.09072v1,http://arxiv.org/abs/2407.09072v1,New Desiderata for Direct Preference Optimization,"Large language models in the past have typically relied on some form of
reinforcement learning with human feedback (RLHF) to better align model
responses with human preferences. However, because of oft-observed
instabilities when implementing these RLHF pipelines, various
reparameterization techniques have recently been introduced to sidestep the
need for separately learning an RL reward model. Instead, directly fine-tuning
for human preferences is achieved via the minimization of a single closed-form
training objective, a process originally referred to as direct preference
optimization (DPO) and followed by several notable descendants. Although
effective in certain real-world settings, we introduce new evaluation criteria
that serve to highlight unresolved shortcomings in the ability of existing DPO
methods to interpolate between a pre-trained reference model and empirical
measures of human preferences, as well as unavoidable trade-offs in how low-
and high-quality responses are regularized and constraints are handled. Our
insights then motivate an alternative DPO-like loss that provably mitigates
these limitations. Empirical results serve to corroborate notable aspects of
our analyses.","[{'name': 'Xiangkun Hu'}, {'name': 'Tong He'}, {'name': 'David Wipf'}]",2024-07-12T07:52:32Z
http://arxiv.org/abs/2407.09020v3,http://arxiv.org/abs/2407.09020v3,"3M-Health: Multimodal Multi-Teacher Knowledge Distillation for Mental
  Health Detection","The significance of mental health classification is paramount in contemporary
society, where digital platforms serve as crucial sources for monitoring
individuals' well-being. However, existing social media mental health datasets
primarily consist of text-only samples, potentially limiting the efficacy of
models trained on such data. Recognising that humans utilise cross-modal
information to comprehend complex situations or issues, we present a novel
approach to address the limitations of current methodologies. In this work, we
introduce a Multimodal and Multi-Teacher Knowledge Distillation model for
Mental Health Classification, leveraging insights from cross-modal human
understanding. Unlike conventional approaches that often rely on simple
concatenation to integrate diverse features, our model addresses the challenge
of appropriately representing inputs of varying natures (e.g., texts and
sounds). To mitigate the computational complexity associated with integrating
all features into a single model, we employ a multimodal and multi-teacher
architecture. By distributing the learning process across multiple teachers,
each specialising in a particular feature extraction aspect, we enhance the
overall mental health classification performance. Through experimental
validation, we demonstrate the efficacy of our model in achieving improved
performance.","[{'name': 'Rina Carines Cabral'}, {'name': 'Siwen Luo'}, {'name': 'Josiah Poon'}, {'name': 'Soyeon Caren Han'}]",2024-07-12T06:22:45Z
http://arxiv.org/abs/2407.09014v2,http://arxiv.org/abs/2407.09014v2,CompAct: Compressing Retrieved Documents Actively for Question Answering,"Retrieval-augmented generation supports language models to strengthen their
factual groundings by providing external contexts. However, language models
often face challenges when given extensive information, diminishing their
effectiveness in solving questions. Context compression tackles this issue by
filtering out irrelevant information, but current methods still struggle in
realistic scenarios where crucial information cannot be captured with a
single-step approach. To overcome this limitation, we introduce CompAct, a
novel framework that employs an active strategy to condense extensive documents
without losing key information. Our experiments demonstrate that CompAct brings
significant improvements in both performance and compression rate on multi-hop
question-answering (QA) benchmarks. CompAct flexibly operates as a
cost-efficient plug-in module with various off-the-shelf retrievers or readers,
achieving exceptionally high compression rates (47x).","[{'name': 'Chanwoong Yoon'}, {'name': 'Taewhoo Lee'}, {'name': 'Hyeon Hwang'}, {'name': 'Minbyul Jeong'}, {'name': 'Jaewoo Kang'}]",2024-07-12T06:06:54Z
http://arxiv.org/abs/2407.09011v1,http://arxiv.org/abs/2407.09011v1,"One Stone, Four Birds: A Comprehensive Solution for QA System Using
  Supervised Contrastive Learning","This paper presents a novel and comprehensive solution to enhance both the
robustness and efficiency of question answering (QA) systems through supervised
contrastive learning (SCL). Training a high-performance QA system has become
straightforward with pre-trained language models, requiring only a small amount
of data and simple fine-tuning. However, despite recent advances, existing QA
systems still exhibit significant deficiencies in functionality and training
efficiency. We address the functionality issue by defining four key tasks: user
input intent classification, out-of-domain input detection, new intent
discovery, and continual learning. We then leverage a unified SCL-based
representation learning method to efficiently build an intra-class compact and
inter-class scattered feature space, facilitating both known intent
classification and unknown intent detection and discovery. Consequently, with
minimal additional tuning on downstream tasks, our approach significantly
improves model efficiency and achieves new state-of-the-art performance across
all tasks.","[{'name': 'Bo Wang'}, {'name': 'Tsunenori Mine'}]",2024-07-12T06:01:51Z
http://arxiv.org/abs/2407.09007v1,http://arxiv.org/abs/2407.09007v1,Benchmarking Language Model Creativity: A Case Study on Code Generation,"As LLMs become increasingly prevalent, it is interesting to consider how
``creative'' these models can be. From cognitive science, creativity consists
of at least two key characteristics: \emph{convergent} thinking (purposefulness
to achieve a given goal) and \emph{divergent} thinking (adaptability to new
environments or constraints) \citep{runco2003critical}. In this work, we
introduce a framework for quantifying LLM creativity that incorporates the two
characteristics. This is achieved by (1) Denial Prompting pushes LLMs to come
up with more creative solutions to a given problem by incrementally imposing
new constraints on the previous solution, compelling LLMs to adopt new
strategies, and (2) defining and computing the NeoGauge metric which examines
both convergent and divergent thinking in the generated creative responses by
LLMs. We apply the proposed framework on Codeforces problems, a natural data
source for collecting human coding solutions. We quantify NeoGauge for various
proprietary and open-source models and find that even the most creative model,
GPT-4, still falls short of demonstrating human-like creativity. We also
experiment with advanced reasoning strategies (MCTS, self-correction, etc.) and
observe no significant improvement in creativity. As a by-product of our
analysis, we release NeoCoder dataset for reproducing our results on future
models.","[{'name': 'Yining Lu'}, {'name': 'Dixuan Wang'}, {'name': 'Tianjian Li'}, {'name': 'Dongwei Jiang'}, {'name': 'Daniel Khashabi'}]",2024-07-12T05:55:22Z
http://arxiv.org/abs/2407.08995v1,http://arxiv.org/abs/2407.08995v1,Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs,"Recent advancements in LLMs have showcased their remarkable role-playing
capabilities, able to accurately simulate the dialogue styles and cognitive
processes of various roles based on different instructions and contexts.
Studies indicate that assigning LLMs the roles of experts, a strategy known as
role-play prompting, can enhance their performance in the corresponding
domains. However, the prompt needs to be manually designed for the given
problem, requiring certain expertise and iterative modifications. To this end,
we propose self-prompt tuning, making LLMs themselves generate role-play
prompts through fine-tuning. Leveraging the LIMA dataset as our foundational
corpus, we employ GPT-4 to annotate role-play prompts for each data points,
resulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like
Llama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned
LLMs can automatically generate expert role prompts for any given question. We
extensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and
open-ended question test. Our empirical results illustrate that self-prompt
tuned LLMs outperform standard instruction tuned baselines across most
datasets. This highlights the great potential of utilizing fine-tuning to
enable LLMs to self-prompt, thereby automating complex prompting strategies. We
release the dataset, models, and code at this
\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.","[{'name': 'Aobo Kong'}, {'name': 'Shiwan Zhao'}, {'name': 'Hao Chen'}, {'name': 'Qicheng Li'}, {'name': 'Yong Qin'}, {'name': 'Ruiqi Sun'}, {'name': 'Xin Zhou'}, {'name': 'Jiaming Zhou'}, {'name': 'Haoqin Sun'}]",2024-07-12T05:26:24Z
http://arxiv.org/abs/2407.08989v1,http://arxiv.org/abs/2407.08989v1,Robustness of LLMs to Perturbations in Text,"Having a clean dataset has been the foundational assumption of most natural
language processing (NLP) systems. However, properly written text is rarely
found in real-world scenarios and hence, oftentimes invalidates the
aforementioned foundational assumption. Recently, Large language models (LLMs)
have shown impressive performance, but can they handle the inevitable noise in
real-world data? This work tackles this critical question by investigating
LLMs' resilience against morphological variations in text. To that end, we
artificially introduce varying levels of noise into a diverse set of datasets
and systematically evaluate LLMs' robustness against the corrupt variations of
the original text. Our findings show that contrary to popular beliefs,
generative LLMs are quiet robust to noisy perturbations in text. This is a
departure from pre-trained models like BERT or RoBERTa whose performance has
been shown to be sensitive to deteriorating noisy text. Additionally, we test
LLMs' resilience on multiple real-world benchmarks that closely mimic commonly
found errors in the wild. With minimal prompting, LLMs achieve a new
state-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and
Lexical Semantic Change (LSC). To empower future research, we also release a
dataset annotated by humans stating their preference for LLM vs.
human-corrected outputs along with the code to reproduce our results.","[{'name': 'Ayush Singh'}, {'name': 'Navpreet Singh'}, {'name': 'Shubham Vatsal'}]",2024-07-12T04:50:17Z
http://arxiv.org/abs/2407.08978v1,http://arxiv.org/abs/2407.08978v1,"Towards Chapter-to-Chapter Context-Aware Literary Translation via Large
  Language Models","Discourse phenomena in existing document-level translation datasets are
sparse, which has been a fundamental obstacle in the development of
context-aware machine translation models. Moreover, most existing
document-level corpora and context-aware machine translation methods rely on an
unrealistic assumption on sentence-level alignments. To mitigate these issues,
we first curate a novel dataset of Chinese-English literature, which consists
of 160 books with intricate discourse structures. Then, we propose a more
pragmatic and challenging setting for context-aware translation, termed
chapter-to-chapter (Ch2Ch) translation, and investigate the performance of
commonly-used machine translation models under this setting. Furthermore, we
introduce a potential approach of finetuning large language models (LLMs)
within the domain of Ch2Ch literary translation, yielding impressive
improvements over baselines. Through our comprehensive analysis, we unveil that
literary translation under the Ch2Ch setting is challenging in nature, with
respect to both model learning methods and translation decoding algorithms.","[{'name': 'Linghao Jin'}, {'name': 'Li An'}, {'name': 'Xuezhe Ma'}]",2024-07-12T04:18:22Z
http://arxiv.org/abs/2407.08967v1,http://arxiv.org/abs/2407.08967v1,"Empowering Few-Shot Relation Extraction with The Integration of
  Traditional RE Methods and Large Language Models","Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)
that utilizes limited training instances, appeals to more researchers in
Natural Language Processing (NLP) due to its capability to extract textual
information in extremely low-resource scenarios. The primary methodologies
employed for FSRE have been fine-tuning or prompt tuning techniques based on
Pre-trained Language Models (PLMs). Recently, the emergence of Large Language
Models (LLMs) has prompted numerous researchers to explore FSRE through
In-Context Learning (ICL). However, there are substantial limitations
associated with methods based on either traditional RE models or LLMs.
Traditional RE models are hampered by a lack of necessary prior knowledge,
while LLMs fall short in their task-specific capabilities for RE. To address
these shortcomings, we propose a Dual-System Augmented Relation Extractor
(DSARE), which synergistically combines traditional RE models with LLMs.
Specifically, DSARE innovatively injects the prior knowledge of LLMs into
traditional RE models, and conversely enhances LLMs' task-specific aptitude for
RE through relation extraction augmentation. Moreover, an Integrated Prediction
module is employed to jointly consider these two respective predictions and
derive the final results. Extensive experiments demonstrate the efficacy of our
proposed method.","[{'name': 'Ye Liu'}, {'name': 'Kai Zhang'}, {'name': 'Aoran Gan'}, {'name': 'Linan Yue'}, {'name': 'Feng Hu'}, {'name': 'Qi Liu'}, {'name': 'Enhong Chen'}]",2024-07-12T03:31:11Z
http://arxiv.org/abs/2407.08959v1,http://arxiv.org/abs/2407.08959v1,"Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for
  Few-shot Hierarchical Text Classification","Recently, various pre-trained language models (PLMs) have been proposed to
prove their impressive performances on a wide range of few-shot tasks. However,
limited by the unstructured prior knowledge in PLMs, it is difficult to
maintain consistent performance on complex structured scenarios, such as
hierarchical text classification (HTC), especially when the downstream data is
extremely scarce. The main challenge is how to transfer the unstructured
semantic space in PLMs to the downstream domain hierarchy. Unlike previous work
on HTC which directly performs multi-label classification or uses graph neural
network (GNN) to inject label hierarchy, in this work, we study the HTC problem
under a few-shot setting to adapt knowledge in PLMs from an unstructured manner
to the downstream hierarchy. Technically, we design a simple yet effective
method named Hierarchical Iterative Conditional Random Field (HierICRF) to
search the most domain-challenging directions and exquisitely crafts
domain-hierarchy adaptation as a hierarchical iterative language modeling
problem, and then it encourages the model to make hierarchical consistency
self-correction during the inference, thereby achieving knowledge transfer with
hierarchical consistency preservation. We perform HierICRF on various
architectures, and extensive experiments on two popular HTC datasets
demonstrate that prompt with HierICRF significantly boosts the few-shot HTC
performance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%
to 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot
settings, while remaining SOTA hierarchical consistency performance.","[{'name': 'Ke Ji'}, {'name': 'Peng Wang'}, {'name': 'Wenjun Ke'}, {'name': 'Guozheng Li'}, {'name': 'Jiajun Liu'}, {'name': 'Jingsheng Gao'}, {'name': 'Ziyu Shang'}]",2024-07-12T03:21:57Z
http://arxiv.org/abs/2407.08952v1,http://arxiv.org/abs/2407.08952v1,"Detect, Investigate, Judge and Determine: A Novel LLM-based Framework
  for Few-shot Fake News Detection","Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news
from real ones in extremely low-resource scenarios. This task has garnered
increased attention due to the widespread dissemination and harmful impact of
fake news on social media. Large Language Models (LLMs) have demonstrated
competitive performance with the help of their rich prior knowledge and
excellent in-context learning abilities. However, existing methods face
significant limitations, such as the Understanding Ambiguity and Information
Scarcity, which significantly undermine the potential of LLMs. To address these
shortcomings, we propose a Dual-perspective Augmented Fake News Detection
(DAFND) model, designed to enhance LLMs from both inside and outside
perspectives. Specifically, DAFND first identifies the keywords of each news
article through a Detection Module. Subsequently, DAFND creatively designs an
Investigation Module to retrieve inside and outside valuable information
concerning to the current news, followed by another Judge Module to derive its
respective two prediction results. Finally, a Determination Module further
integrates these two predictions and derives the final result. Extensive
experiments on two publicly available datasets show the efficacy of our
proposed method, particularly in low-resource settings.","[{'name': 'Ye Liu'}, {'name': 'Jiajun Zhu'}, {'name': 'Kai Zhang'}, {'name': 'Haoyu Tang'}, {'name': 'Yanghai Zhang'}, {'name': 'Xukai Liu'}, {'name': 'Qi Liu'}, {'name': 'Enhong Chen'}]",2024-07-12T03:15:01Z
http://arxiv.org/abs/2407.08940v2,http://arxiv.org/abs/2407.08940v2,"Large Language Models as Biomedical Hypothesis Generators: A
  Comprehensive Evaluation","The rapid growth of biomedical knowledge has outpaced our ability to
efficiently extract insights and generate novel hypotheses. Large language
models (LLMs) have emerged as a promising tool to revolutionize knowledge
interaction and potentially accelerate biomedical discovery. In this paper, we
present a comprehensive evaluation of LLMs as biomedical hypothesis generators.
We construct a dataset of background-hypothesis pairs from biomedical
literature, carefully partitioned into training, seen, and unseen test sets
based on publication date to mitigate data contamination. Using this dataset,
we assess the hypothesis generation capabilities of top-tier instructed models
in zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of
uncertainty, a crucial aspect of scientific discovery, we incorporate tool use
and multi-agent interactions in our evaluation framework. Furthermore, we
propose four novel metrics grounded in extensive literature review to evaluate
the quality of generated hypotheses, considering both LLM-based and human
assessments. Our experiments yield two key findings: 1) LLMs can generate novel
and validated hypotheses, even when tested on literature unseen during
training, and 2) Increasing uncertainty through multi-agent interactions and
tool use can facilitate diverse candidate generation and improve zero-shot
hypothesis generation performance. However, we also observe that the
integration of additional knowledge through few-shot learning and tool use may
not always lead to performance gains, highlighting the need for careful
consideration of the type and scope of external knowledge incorporated. These
findings underscore the potential of LLMs as powerful aids in biomedical
hypothesis generation and provide valuable insights to guide further research
in this area.","[{'name': 'Biqing Qi'}, {'name': 'Kaiyan Zhang'}, {'name': 'Kai Tian'}, {'name': 'Haoxiang Li'}, {'name': 'Zhang-Ren Chen'}, {'name': 'Sihang Zeng'}, {'name': 'Ermo Hua'}, {'name': 'Hu Jinfang'}, {'name': 'Bowen Zhou'}]",2024-07-12T02:55:13Z
http://arxiv.org/abs/2407.08937v1,http://arxiv.org/abs/2407.08937v1,Self-Evolving GPT: A Lifelong Autonomous Experiential Learner,"To improve the performance of large language models (LLMs), researchers have
explored providing LLMs with textual task-solving experience via prompts.
However, they rely on manual efforts to acquire and apply such experience for
each task, which is not feasible for the growing demand for LLMs and the
variety of user questions. To address this issue, we design a lifelong
autonomous experiential learning framework based on LLMs to explore whether
LLMs can imitate human ability for learning and utilizing experience. It
autonomously learns and accumulates experience through experience transfer and
induction, categorizing the types of input questions to select which
accumulated experience to employ for them. Experimental results on six widely
used NLP datasets show that our framework performs reliably in each
intermediate step and effectively improves the performance of GPT-3.5 and
GPT-4. This validates the feasibility of using LLMs to mimic human experiential
learning and application capabilities. Additionally, we provide a detailed
analysis of the behavior of our framework at each step.","[{'name': 'Jinglong Gao'}, {'name': 'Xiao Ding'}, {'name': 'Yiming Cui'}, {'name': 'Jianbai Zhao'}, {'name': 'Hepeng Wang'}, {'name': 'Ting Liu'}, {'name': 'Bing Qin'}]",2024-07-12T02:49:13Z
http://arxiv.org/abs/2407.08887v1,http://arxiv.org/abs/2407.08887v1,"Automatic Pruning of Fine-tuning Datasets for Transformer-based Language
  Models","Transformer-based language models have shown state-of-the-art performance on
a variety of natural language understanding tasks. To achieve this performance,
these models are first pre-trained on general corpus and then fine-tuned on
downstream tasks. Previous work studied the effect of pruning the training set
of the downstream tasks on the performance of the model on its evaluation set.
In this work, we propose an automatic dataset pruning method for the training
set of fine-tuning tasks. Our method is based on the model's success rate in
correctly classifying each training data point. Unlike previous work which
relies on user feedback to determine subset size, our method automatically
extracts training subsets that are adapted for each pair of model and
fine-tuning task. Our method provides multiple subsets for use in dataset
pruning that navigate the trade-off between subset size and evaluation
accuracy. Our largest subset, which we also refer to as the winning ticket
subset, is on average $3 \times$ smaller than the original training set of the
fine-tuning task. Our experiments on 5 downstream tasks and 2 language models
show that, on average, fine-tuning on the winning ticket subsets results in a
$0.1 \%$ increase in the evaluation performance of the model.","[{'name': 'Mohammadreza Tayaranian'}, {'name': 'Seyyed Hasan Mozafari'}, {'name': 'Brett H. Meyer'}, {'name': 'James J. Clark'}, {'name': 'Warren J. Gross'}]",2024-07-11T22:46:18Z
http://arxiv.org/abs/2407.08853v1,http://arxiv.org/abs/2407.08853v1,"GPT-4 is judged more human than humans in displaced and inverted Turing
  tests","Everyday AI detection requires differentiating between people and AI in
informal, online conversations. In many cases, people will not interact
directly with AI systems but instead read conversations between AI systems and
other people. We measured how well people and large language models can
discriminate using two modified versions of the Turing test: inverted and
displaced. GPT-3.5, GPT-4, and displaced human adjudicators judged whether an
agent was human or AI on the basis of a Turing test transcript. We found that
both AI and displaced human judges were less accurate than interactive
interrogators, with below chance accuracy overall. Moreover, all three judged
the best-performing GPT-4 witness to be human more often than human witnesses.
This suggests that both humans and current LLMs struggle to distinguish between
the two when they are not actively interrogating the person, underscoring an
urgent need for more accurate tools to detect AI in conversations.","[{'name': 'Ishika Rathi'}, {'name': 'Sydney Taylor'}, {'name': 'Benjamin K. Bergen'}, {'name': 'Cameron R. Jones'}]",2024-07-11T20:28:24Z
http://arxiv.org/abs/2407.08842v1,http://arxiv.org/abs/2407.08842v1,Evaluating Nuanced Bias in Large Language Model Free Response Answers,"Pre-trained large language models (LLMs) can now be easily adapted for
specific business purposes using custom prompts or fine tuning. These
customizations are often iteratively re-engineered to improve some aspect of
performance, but after each change businesses want to ensure that there has
been no negative impact on the system's behavior around such critical issues as
bias. Prior methods of benchmarking bias use techniques such as word masking
and multiple choice questions to assess bias at scale, but these do not capture
all of the nuanced types of bias that can occur in free response answers, the
types of answers typically generated by LLM systems. In this paper, we identify
several kinds of nuanced bias in free text that cannot be similarly identified
by multiple choice tests. We describe these as: confidence bias, implied bias,
inclusion bias and erasure bias. We present a semi-automated pipeline for
detecting these types of bias by first eliminating answers that can be
automatically classified as unbiased and then co-evaluating name reversed pairs
using crowd workers. We believe that the nuanced classifications our method
generates can be used to give better feedback to LLMs, especially as LLM
reasoning capabilities become more advanced.","[{'name': 'Jennifer Healey'}, {'name': 'Laurie Byrum'}, {'name': 'Md Nadeem Akhtar'}, {'name': 'Moumita Sinha'}]",2024-07-11T19:58:13Z
http://arxiv.org/abs/2407.08836v1,http://arxiv.org/abs/2407.08836v1,Fault Diagnosis in Power Grids with Large Language Model,"Power grid fault diagnosis is a critical task for ensuring the reliability
and stability of electrical infrastructure. Traditional diagnostic systems
often struggle with the complexity and variability of power grid data. This
paper proposes a novel approach that leverages Large Language Models (LLMs),
specifically ChatGPT and GPT-4, combined with advanced prompt engineering to
enhance fault diagnosis accuracy and explainability. We designed comprehensive,
context-aware prompts to guide the LLMs in interpreting complex data and
providing detailed, actionable insights. Our method was evaluated against
baseline techniques, including standard prompting, Chain-of-Thought (CoT), and
Tree-of-Thought (ToT) methods, using a newly constructed dataset comprising
real-time sensor data, historical fault records, and component descriptions.
Experimental results demonstrate significant improvements in diagnostic
accuracy, explainability quality, response coherence, and contextual
understanding, underscoring the effectiveness of our approach. These findings
suggest that prompt-engineered LLMs offer a promising solution for robust and
reliable power grid fault diagnosis.","[{'name': 'Liu Jing'}, {'name': 'Amirul Rahman'}]",2024-07-11T19:44:18Z
http://arxiv.org/abs/2407.08824v1,http://arxiv.org/abs/2407.08824v1,Proving that Cryptic Crossword Clue Answers are Correct,"Cryptic crossword clues are challenging cognitive tasks, for which new test
sets are released on a daily basis by multiple international newspapers. Each
cryptic clue contains both the definition of the answer to be placed in the
crossword grid (in common with regular crosswords), and `wordplay' that proves
that the answer is correct (i.e. a human solver can be confident that an answer
is correct without needing crossing words to confirm it). Using an existing
cryptic wordplay proving framework (operating on Python proofs created by an
LLM), we show that it is possible to distinguish between correct answers and
almost-correct ones based upon whether the wordplay `works'.","[{'name': 'Martin Andrews'}, {'name': 'Sam Witteveen'}]",2024-07-11T19:13:16Z
http://arxiv.org/abs/2407.08819v1,http://arxiv.org/abs/2407.08819v1,"Rule-Based, Neural and LLM Back-Translation: Comparative Insights from a
  Variant of Ladin","This paper explores the impact of different back-translation approaches on
machine translation for Ladin, specifically the Val Badia variant. Given the
limited amount of parallel data available for this language (only 18k
Ladin-Italian sentence pairs), we investigate the performance of a multilingual
neural machine translation model fine-tuned for Ladin-Italian. In addition to
the available authentic data, we synthesise further translations by using three
different models: a fine-tuned neural model, a rule-based system developed
specifically for this language pair, and a large language model. Our
experiments show that all approaches achieve comparable translation quality in
this low-resource scenario, yet round-trip translations highlight differences
in model performance.","[{'name': 'Samuel Frontull'}, {'name': 'Georg Moser'}]",2024-07-11T19:05:43Z
http://arxiv.org/abs/2407.08818v1,http://arxiv.org/abs/2407.08818v1,"MAGNET: Improving the Multilingual Fairness of Language Models with
  Adaptive Gradient-Based Tokenization","In multilingual settings, non-Latin scripts and low-resource languages are
usually disadvantaged in terms of language models' utility, efficiency, and
cost. Specifically, previous studies have reported multiple modeling biases
that the current tokenization algorithms introduce to non-Latin script
languages, the main one being over-segmentation. In this work, we propose
MAGNET; multilingual adaptive gradient-based tokenization to reduce
over-segmentation via adaptive gradient-based subword tokenization. MAGNET
learns to predict segment boundaries between byte tokens in a sequence via
sub-modules within the model, which act as internal boundary predictors
(tokenizers). Previous gradient-based tokenization methods aimed for uniform
compression across sequences by integrating a single boundary predictor during
training and optimizing it end-to-end through stochastic reparameterization
alongside the next token prediction objective. However, this approach still
results in over-segmentation for non-Latin script languages in multilingual
settings. In contrast, MAGNET offers a customizable architecture where
byte-level sequences are routed through language-script-specific predictors,
each optimized for its respective language script. This modularity enforces
equitable segmentation granularity across different language scripts compared
to previous methods. Through extensive experiments, we demonstrate that in
addition to reducing segmentation disparities, MAGNET also enables faster
language modelling and improves downstream utility.","[{'name': 'Orevaoghene Ahia'}, {'name': 'Sachin Kumar'}, {'name': 'Hila Gonen'}, {'name': 'Valentin Hoffman'}, {'name': 'Tomasz Limisiewicz'}, {'name': 'Yulia Tsvetkov'}, {'name': 'Noah A. Smith'}]",2024-07-11T18:59:21Z
http://arxiv.org/abs/2408.01911v1,http://arxiv.org/abs/2408.01911v1,"Brief state of the art in social information mining: Practical
  application in analysis of trends in French legislative 2024","The analysis of social media information has undergone significant evolution
in the last decade due to advancements in artificial intelligence (AI) and
machine learning (ML). This paper provides an overview of the state-of-the-art
techniques in social media mining, with a practical application in analyzing
trends in the 2024 French legislative elections. We leverage natural language
processing (NLP) tools to gauge public opinion by extracting and analyzing
comments and reactions from the AgoraVox platform. The study reveals that the
National Rally party, led by Marine Le Pen, maintains a high level of
engagement on social media, outperforming traditional parties. This trend is
corroborated by user interactions, indicating a strong digital presence. The
results highlight the utility of advanced AI models, such as transformers and
large language models (LLMs), in capturing nuanced public sentiments and
predicting political leanings, demonstrating their potential in real-time
reputation management and crisis response.",[{'name': 'Jose A. Garcia Gutierrez'}],2024-07-11T18:22:58Z
http://arxiv.org/abs/2407.08790v1,http://arxiv.org/abs/2407.08790v1,"Large Models of What? Mistaking Engineering Achievements for Human
  Linguistic Agency","In this paper we argue that key, often sensational and misleading, claims
regarding linguistic capabilities of Large Language Models (LLMs) are based on
at least two unfounded assumptions; the assumption of language completeness and
the assumption of data completeness. Language completeness assumes that a
distinct and complete thing such as `a natural language' exists, the essential
characteristics of which can be effectively and comprehensively modelled by an
LLM. The assumption of data completeness relies on the belief that a language
can be quantified and wholly captured by data. Work within the enactive
approach to cognitive science makes clear that, rather than a distinct and
complete thing, language is a means or way of acting. Languaging is not the
kind of thing that can admit of a complete or comprehensive modelling. From an
enactive perspective we identify three key characteristics of enacted language;
embodiment, participation, and precariousness, that are absent in LLMs, and
likely incompatible in principle with current architectures. We argue that
these absences imply that LLMs are not now and cannot in their present form be
linguistic agents the way humans are. We illustrate the point in particular
through the phenomenon of `algospeak', a recently described pattern of high
stakes human language activity in heavily controlled online environments. On
the basis of these points, we conclude that sensational and misleading claims
about LLM agency and capabilities emerge from a deep misconception of both what
human language is and what LLMs are.","[{'name': 'Abeba Birhane'}, {'name': 'Marek McGann'}]",2024-07-11T18:06:01Z
http://arxiv.org/abs/2407.08734v1,http://arxiv.org/abs/2407.08734v1,Transformer Circuit Faithfulness Metrics are not Robust,"Mechanistic interpretability work attempts to reverse engineer the learned
algorithms present inside neural networks. One focus of this work has been to
discover 'circuits' -- subgraphs of the full model that explain behaviour on
specific tasks. But how do we measure the performance of such circuits? Prior
work has attempted to measure circuit 'faithfulness' -- the degree to which the
circuit replicates the performance of the full model. In this work, we survey
many considerations for designing experiments that measure circuit faithfulness
by ablating portions of the model's computation. Concerningly, we find existing
methods are highly sensitive to seemingly insignificant changes in the ablation
methodology. We conclude that existing circuit faithfulness scores reflect both
the methodological choices of researchers as well as the actual components of
the circuit - the task a circuit is required to perform depends on the ablation
used to test it. The ultimate goal of mechanistic interpretability work is to
understand neural networks, so we emphasize the need for more clarity in the
precise claims being made about circuits. We open source a library at
https://github.com/UFO-101/auto-circuit that includes highly efficient
implementations of a wide range of ablation methodologies and circuit discovery
algorithms.","[{'name': 'Joseph Miller'}, {'name': 'Bilal Chughtai'}, {'name': 'William Saunders'}]",2024-07-11T17:59:00Z
http://arxiv.org/abs/2407.08733v1,http://arxiv.org/abs/2407.08733v1,"Is Your Model Really A Good Math Reasoner? Evaluating Mathematical
  Reasoning with Checklist","Exceptional mathematical reasoning ability is one of the key features that
demonstrate the power of large language models (LLMs). How to comprehensively
define and evaluate the mathematical abilities of LLMs, and even reflect the
user experience in real-world scenarios, has emerged as a critical issue.
Current benchmarks predominantly concentrate on problem-solving capabilities,
which presents a substantial risk of model overfitting and fails to accurately
represent genuine mathematical reasoning abilities. In this paper, we argue
that if a model really understands a problem, it should be robustly and readily
applied across a diverse array of tasks. Motivated by this, we introduce
MATHCHECK, a well-designed checklist for testing task generalization and
reasoning robustness, as well as an automatic tool to generate checklists
efficiently. MATHCHECK includes multiple mathematical reasoning tasks and
robustness test types to facilitate a comprehensive evaluation of both
mathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we
develop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual
reasoning and multi-modal reasoning capabilities, respectively, serving as
upgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K.
We adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs,
assessing their comprehensive mathematical reasoning abilities. Our results
demonstrate that while frontier LLMs like GPT-4o continue to excel in various
abilities on the checklist, many other model families exhibit a significant
decline. Further experiments indicate that, compared to traditional math
benchmarks, MATHCHECK better reflects true mathematical abilities and
represents mathematical intelligence more linearly, thereby supporting our
design. On our MATHCHECK, we can easily conduct detailed behavior analysis to
deeply investigate models.","[{'name': 'Zihao Zhou'}, {'name': 'Shudong Liu'}, {'name': 'Maizhen Ning'}, {'name': 'Wei Liu'}, {'name': 'Jindong Wang'}, {'name': 'Derek F. Wong'}, {'name': 'Xiaowei Huang'}, {'name': 'Qiufeng Wang'}, {'name': 'Kaizhu Huang'}]",2024-07-11T17:58:58Z
http://arxiv.org/abs/2407.08716v1,http://arxiv.org/abs/2407.08716v1,A Taxonomy for Data Contamination in Large Language Models,"Large language models pretrained on extensive web corpora demonstrate
remarkable performance across a wide range of downstream tasks. However, a
growing concern is data contamination, where evaluation datasets may be
contained in the pretraining corpus, inflating model performance.
Decontamination, the process of detecting and removing such data, is a
potential solution; yet these contaminants may originate from altered versions
of the test set, evading detection during decontamination. How different types
of contamination impact the performance of language models on downstream tasks
is not fully understood. We present a taxonomy that categorizes the various
types of contamination encountered by LLMs during the pretraining phase and
identify which types pose the highest risk. We analyze the impact of
contamination on two key NLP tasks -- summarization and question answering --
revealing how different types of contamination influence task performance
during evaluation.","[{'name': 'Medha Palavalli'}, {'name': 'Amanda Bertsch'}, {'name': 'Matthew R. Gormley'}]",2024-07-11T17:50:34Z
http://arxiv.org/abs/2407.08642v1,http://arxiv.org/abs/2407.08642v1,"Towards Building Specialized Generalist AI with System 1 and System 2
  Fusion","In this perspective paper, we introduce the concept of Specialized Generalist
Artificial Intelligence (SGAI or simply SGI) as a crucial milestone toward
Artificial General Intelligence (AGI). Compared to directly scaling general
abilities, SGI is defined as AI that specializes in at least one task,
surpassing human experts, while also retaining general abilities. This fusion
path enables SGI to rapidly achieve high-value areas. We categorize SGI into
three stages based on the level of mastery over professional skills and
generality performance. Additionally, we discuss the necessity of SGI in
addressing issues associated with large language models, such as their
insufficient generality, specialized capabilities, uncertainty in innovation,
and practical applications. Furthermore, we propose a conceptual framework for
developing SGI that integrates the strengths of Systems 1 and 2 cognitive
processing. This framework comprises three layers and four key components,
which focus on enhancing individual abilities and facilitating collaborative
evolution. We conclude by summarizing the potential challenges and suggesting
future directions. We hope that the proposed SGI will provide insights into
further research and applications towards achieving AGI.","[{'name': 'Kaiyan Zhang'}, {'name': 'Biqing Qi'}, {'name': 'Bowen Zhou'}]",2024-07-11T16:23:16Z
http://arxiv.org/abs/2407.08618v2,http://arxiv.org/abs/2407.08618v2,Tamil Language Computing: the Present and the Future,"This paper delves into the text processing aspects of Language Computing,
which enables computers to understand, interpret, and generate human language.
Focusing on tasks such as speech recognition, machine translation, sentiment
analysis, text summarization, and language modelling, language computing
integrates disciplines including linguistics, computer science, and cognitive
psychology to create meaningful human-computer interactions. Recent
advancements in deep learning have made computers more accessible and capable
of independent learning and adaptation. In examining the landscape of language
computing, the paper emphasises foundational work like encoding, where Tamil
transitioned from ASCII to Unicode, enhancing digital communication. It
discusses the development of computational resources, including raw data,
dictionaries, glossaries, annotated data, and computational grammars, necessary
for effective language processing. The challenges of linguistic annotation, the
creation of treebanks, and the training of large language models are also
covered, emphasising the need for high-quality, annotated data and advanced
language models. The paper underscores the importance of building practical
applications for languages like Tamil to address everyday communication needs,
highlighting gaps in current technology. It calls for increased research
collaboration, digitization of historical texts, and fostering digital usage to
ensure the comprehensive development of Tamil language processing, ultimately
enhancing global communication and access to digital services.",[{'name': 'Kengatharaiyer Sarveswaran'}],2024-07-11T15:56:02Z
http://arxiv.org/abs/2407.08607v1,http://arxiv.org/abs/2407.08607v1,Turn-Level Empathy Prediction Using Psychological Indicators,"For the WASSA 2024 Empathy and Personality Prediction Shared Task, we propose
a novel turn-level empathy detection method that decomposes empathy into six
psychological indicators: Emotional Language, Perspective-Taking, Sympathy and
Compassion, Extroversion, Openness, and Agreeableness. A pipeline of text
enrichment using a Large Language Model (LLM) followed by DeBERTA fine-tuning
demonstrates a significant improvement in the Pearson Correlation Coefficient
and F1 scores for empathy detection, highlighting the effectiveness of our
approach. Our system officially ranked 7th at the CONV-turn track.","[{'name': 'Shaz Furniturewala'}, {'name': 'Kokil Jaidka'}]",2024-07-11T15:43:27Z
http://arxiv.org/abs/2407.08582v1,http://arxiv.org/abs/2407.08582v1,On the Universal Truthfulness Hyperplane Inside LLMs,"While large language models (LLMs) have demonstrated remarkable abilities
across various fields, hallucination remains a significant challenge. Recent
studies have explored hallucinations through the lens of internal
representations, proposing mechanisms to decipher LLMs' adherence to facts.
However, these approaches often fail to generalize to out-of-distribution data,
leading to concerns about whether internal representation patterns reflect
fundamental factual awareness, or only overfit spurious correlations on the
specific datasets. In this work, we investigate whether a universal
truthfulness hyperplane that distinguishes the model's factually correct and
incorrect outputs exists within the model. To this end, we scale up the number
of training datasets and conduct an extensive evaluation -- we train the
truthfulness hyperplane on a diverse collection of over 40 datasets and examine
its cross-task, cross-domain, and in-domain generalization. Our results
indicate that increasing the diversity of the training datasets significantly
enhances the performance in all scenarios, while the volume of data samples
plays a less critical role. This finding supports the optimistic hypothesis
that a universal truthfulness hyperplane may indeed exist within the model,
offering promising directions for future research.","[{'name': 'Junteng Liu'}, {'name': 'Shiqi Chen'}, {'name': 'Yu Cheng'}, {'name': 'Junxian He'}]",2024-07-11T15:07:26Z
http://arxiv.org/abs/2407.08551v1,http://arxiv.org/abs/2407.08551v1,Autoregressive Speech Synthesis without Vector Quantization,"We present MELLE, a novel continuous-valued tokens based language modeling
approach for text to speech synthesis (TTS). MELLE autoregressively generates
continuous mel-spectrogram frames directly from text condition, bypassing the
need for vector quantization, which are originally designed for audio
compression and sacrifice fidelity compared to mel-spectrograms. Specifically,
(i) instead of cross-entropy loss, we apply regression loss with a proposed
spectrogram flux loss function to model the probability distribution of the
continuous-valued tokens. (ii) we have incorporated variational inference into
MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity
and model robustness. Experiments demonstrate that, compared to the two-stage
codec language models VALL-E and its variants, the single-stage MELLE mitigates
robustness issues by avoiding the inherent flaws of sampling discrete codes,
achieves superior performance across multiple metrics, and, most importantly,
offers a more streamlined paradigm. See https://aka.ms/melle for demos of our
work.","[{'name': 'Lingwei Meng'}, {'name': 'Long Zhou'}, {'name': 'Shujie Liu'}, {'name': 'Sanyuan Chen'}, {'name': 'Bing Han'}, {'name': 'Shujie Hu'}, {'name': 'Yanqing Liu'}, {'name': 'Jinyu Li'}, {'name': 'Sheng Zhao'}, {'name': 'Xixin Wu'}, {'name': 'Helen Meng'}, {'name': 'Furu Wei'}]",2024-07-11T14:36:53Z
http://arxiv.org/abs/2407.08521v2,http://arxiv.org/abs/2407.08521v2,Emergent Visual-Semantic Hierarchies in Image-Text Representations,"While recent vision-and-language models (VLMs) like CLIP are a powerful tool
for analyzing text and images in a shared semantic space, they do not
explicitly model the hierarchical nature of the set of texts which may describe
an image. Conversely, existing multimodal hierarchical representation learning
methods require costly training from scratch, failing to leverage the knowledge
encoded by state-of-the-art multimodal foundation models. In this work, we
study the knowledge of existing foundation models, finding that they exhibit
emergent understanding of visual-semantic hierarchies despite not being
directly trained for this purpose. We propose the Radial Embedding (RE)
framework for probing and optimizing hierarchical understanding, and contribute
the HierarCaps dataset, a benchmark facilitating the study of hierarchical
knowledge in image--text representations, constructed automatically via large
language models. Our results show that foundation VLMs exhibit zero-shot
hierarchical understanding, surpassing the performance of prior models
explicitly designed for this purpose. Furthermore, we show that foundation
models may be better aligned to hierarchical reasoning via a text-only
fine-tuning phase, while retaining pretraining knowledge.","[{'name': 'Morris Alper'}, {'name': 'Hadar Averbuch-Elor'}]",2024-07-11T14:09:42Z
http://arxiv.org/abs/2407.08475v1,http://arxiv.org/abs/2407.08475v1,"Investigating Public Fine-Tuning Datasets: A Complex Review of Current
  Practices from a Construction Perspective","With the rapid development of the large model domain, research related to
fine-tuning has concurrently seen significant advancement, given that
fine-tuning is a constituent part of the training process for large-scale
models. Data engineering plays a fundamental role in the training process of
models, which includes data infrastructure, data processing, etc. Data during
fine-tuning likewise forms the base for large models. In order to embrace the
power and explore new possibilities of fine-tuning datasets, this paper reviews
current public fine-tuning datasets from the perspective of data construction.
An overview of public fine-tuning datasets from two sides: evolution and
taxonomy, is provided in this review, aiming to chart the development
trajectory. Construction techniques and methods for public fine-tuning datasets
of Large Language Models (LLMs), including data generation and data
augmentation among others, are detailed. This elaboration follows the
aforementioned taxonomy, specifically across demonstration, comparison, and
generalist categories. Additionally, a category tree of data generation
techniques has been abstracted in our review to assist researchers in gaining a
deeper understanding of fine-tuning datasets from the construction dimension.
Our review also summarizes the construction features in different data
preparation phases of current practices in this field, aiming to provide a
comprehensive overview and inform future research. Fine-tuning dataset
practices, encompassing various data modalities, are also discussed from a
construction perspective in our review. Towards the end of the article, we
offer insights and considerations regarding the future construction and
developments of fine-tuning datasets.","[{'name': 'Runyuan Ma'}, {'name': 'Wei Li'}, {'name': 'Fukai Shang'}]",2024-07-11T13:11:16Z
http://arxiv.org/abs/2407.08454v2,http://arxiv.org/abs/2407.08454v2,"Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on
  Long-Context Tasks","How to efficiently serve Large Language Models (LLMs) has become a pressing
issue because of their huge computational cost in their autoregressive
generation process. To mitigate computational costs, LLMs often employ the KV
Cache technique to improve the generation speed. While improving the
computational efficiency, the storage requirements of the KV cache are
substantial, particularly in long-context scenarios, leading to significant
memory consumption. Existing KV cache eviction methods often degrade the
performance of LLMs in long-context scenarios due to the information loss
introduced by eviction. In this paper, we propose a novel KV cache merging
approach, called KVMerger, to achieve adaptive KV cache compression for
long-context tasks without significant performance degradation under
constrained memory budgets. Our approach is inspired by the intriguing
observation that key states exhibit high similarity at the token level within a
single sequence. To facilitate merging, we develop an effective yet
straightforward merging set identification algorithm to identify suitable KV
states for merging. Our merging set identification algorithm stimulates the
second observation that KV cache sparsity, from similarity perspective, is
independent of the dataset and remains persistent at the model level.
Subsequently, we propose a Gaussian kernel weighted merging algorithm to
selectively merge all states within each merging set. We conduct extensive
experiments to demonstrate the effectiveness of KVMerger for long-context tasks
under constrained memory budgets, applying it to models including
Llama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll
benchmarks, we compare our method with other KV cache compression techniques,
including H2O and CaM, showing that our method achieves superior performance
across tasks with both 50% and 35% KV cache budgets.","[{'name': 'Zheng Wang'}, {'name': 'Boxiao Jin'}, {'name': 'Zhongzhi Yu'}, {'name': 'Minjia Zhang'}]",2024-07-11T12:50:42Z
http://arxiv.org/abs/2407.08400v1,http://arxiv.org/abs/2407.08400v1,Self-training Language Models for Arithmetic Reasoning,"Language models achieve impressive results in tasks involving complex
multistep reasoning, but scaling these capabilities further traditionally
requires expensive collection of more annotated data. In this work, we explore
the potential of improving the capabilities of language models without new
data, merely using automated feedback to the validity of their predictions in
arithmetic reasoning (self-training). We find that models can substantially
improve in both single-round (offline) and online self-training. In the offline
setting, supervised methods are able to deliver gains comparable to preference
optimization, but in online self-training, preference optimization shows to
largely outperform supervised training thanks to superior stability and
robustness on unseen types of problems.","[{'name': 'Marek Kadlčík'}, {'name': 'Michal Štefánik'}]",2024-07-11T11:06:05Z
http://arxiv.org/abs/2407.08388v1,http://arxiv.org/abs/2407.08388v1,On the attribution of confidence to large language models,"Credences are mental states corresponding to degrees of confidence in
propositions. Attribution of credences to Large Language Models (LLMs) is
commonplace in the empirical literature on LLM evaluation. Yet the theoretical
basis for LLM credence attribution is unclear. We defend three claims. First,
our semantic claim is that LLM credence attributions are (at least in general)
correctly interpreted literally, as expressing truth-apt beliefs on the part of
scientists that purport to describe facts about LLM credences. Second, our
metaphysical claim is that the existence of LLM credences is at least
plausible, although current evidence is inconclusive. Third, our epistemic
claim is that LLM credence attributions made in the empirical literature on LLM
evaluation are subject to non-trivial sceptical concerns. It is a distinct
possibility that even if LLMs have credences, LLM credence attributions are
generally false because the experimental techniques used to assess LLM
credences are not truth-tracking.","[{'name': 'Geoff Keeling'}, {'name': 'Winnie Street'}]",2024-07-11T10:51:06Z
http://arxiv.org/abs/2407.08351v1,http://arxiv.org/abs/2407.08351v1,"AutoBencher: Creating Salient, Novel, Difficult Datasets for Language
  Models","Evaluation is critical for assessing capabilities, tracking scientific
progress, and informing model selection. In this paper, we present three
desiderata for a good benchmark for language models: (i) salience (e.g.,
knowledge about World War II is more salient than a random day in history),
(ii) novelty (i.e., the benchmark reveals new trends in model rankings not
shown by previous benchmarks), and (iii) difficulty (i.e., the benchmark should
be difficult for existing models, leaving headroom for future improvement). We
operationalize these three desiderata and cast benchmark creation as a search
problem, that of finding benchmarks that that satisfy all three desiderata. To
tackle this search problem, we present AutoBencher, which uses a language model
to automatically search for datasets that meet the three desiderata.
AutoBencher uses privileged information (e.g. relevant documents) to construct
reliable datasets, and adaptivity with reranking to optimize for the search
objective. We use AutoBencher to create datasets for math, multilingual, and
knowledge-intensive question answering. The scalability of AutoBencher allows
it to test fine-grained categories and tail knowledge, creating datasets that
are on average 27% more novel and 22% more difficult than existing benchmarks.
A closer investigation of our constructed datasets shows that we can identify
specific gaps in LM knowledge in language models that are not captured by
existing benchmarks, such as Gemini Pro performing much worse on question
answering about the Permian Extinction and Fordism, while OpenAGI-7B performing
surprisingly well on QA about COVID-19.","[{'name': 'Xiang Lisa Li'}, {'name': 'Evan Zheran Liu'}, {'name': 'Percy Liang'}, {'name': 'Tatsunori Hashimoto'}]",2024-07-11T10:03:47Z
http://arxiv.org/abs/2407.08348v2,http://arxiv.org/abs/2407.08348v2,"Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large
  Language Models -- The Story Goes On","In this paper, we investigate the underlying factors that potentially enhance
the mathematical reasoning capabilities of large language models (LLMs). We
argue that the data scaling law for math reasoning capabilities in modern LLMs
is far from being saturated, highlighting how the model's quality improves with
increases in data quantity. To support this claim, we introduce the
Skywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using
our proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved
impressive accuracies of 51.2% on the competition-level MATH benchmark and
83.9% on the GSM8K benchmark using only SFT data, outperforming an early
version of GPT-4 on MATH. The superior performance of Skywork-Math models
contributes to our novel two-stage data synthesis and model SFT pipelines,
which include three different augmentation methods and a diverse seed problem
set, ensuring both the quantity and quality of Skywork-MathQA dataset across
varying difficulty levels. Most importantly, we provide several practical
takeaways to enhance math reasoning abilities in LLMs for both research and
industry applications.","[{'name': 'Liang Zeng'}, {'name': 'Liangjun Zhong'}, {'name': 'Liang Zhao'}, {'name': 'Tianwen Wei'}, {'name': 'Liu Yang'}, {'name': 'Jujie He'}, {'name': 'Cheng Cheng'}, {'name': 'Rui Hu'}, {'name': 'Yang Liu'}, {'name': 'Shuicheng Yan'}, {'name': 'Han Fang'}, {'name': 'Yahui Zhou'}]",2024-07-11T09:56:51Z
http://arxiv.org/abs/2407.08331v2,http://arxiv.org/abs/2407.08331v2,Towards Explainable Evolution Strategies with Large Language Models,"This paper introduces an approach that integrates self-adaptive Evolution
Strategies (ES) with Large Language Models (LLMs) to enhance the explainability
of complex optimization processes. By employing a self-adaptive ES equipped
with a restart mechanism, we effectively navigate the challenging landscapes of
benchmark functions, capturing detailed logs of the optimization journey. The
logs include fitness evolution, step-size adjustments and restart events due to
stagnation. An LLM is then utilized to process these logs, generating concise,
user-friendly summaries that highlight key aspects such as convergence
behavior, optimal fitness achievements, and encounters with local optima. Our
case study on the Rastrigin function demonstrates how our approach makes the
complexities of ES optimization transparent. Our findings highlight the
potential of using LLMs to bridge the gap between advanced optimization
algorithms and their interpretability.","[{'name': 'Jill Baumann'}, {'name': 'Oliver Kramer'}]",2024-07-11T09:28:27Z
http://arxiv.org/abs/2407.08273v2,http://arxiv.org/abs/2407.08273v2,RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL,"Large language models (LLMs) with in-context learning have significantly
improved the performance of text-to-SQL task. Previous works generally focus on
using exclusive SQL generation prompt to improve the LLMs' reasoning ability.
However, they are mostly hard to handle large databases with numerous tables
and columns, and usually ignore the significance of pre-processing database and
extracting valuable information for more efficient prompt engineering. Based on
above analysis, we propose RB-SQL, a novel retrieval-based LLM framework for
in-context prompt engineering, which consists of three modules that retrieve
concise tables and columns as schema, and targeted examples for in-context
learning. Experiment results demonstrate that our model achieves better
performance than several competitive baselines on public datasets BIRD and
Spider.","[{'name': 'Zhenhe Wu'}, {'name': 'Zhongqiu Li'}, {'name': 'Jie Zhang'}, {'name': 'Mengxiang Li'}, {'name': 'Yu Zhao'}, {'name': 'Ruiyu Fang'}, {'name': 'Zhongjiang He'}, {'name': 'Xuelong Li'}, {'name': 'Zhoujun Li'}, {'name': 'Shuangyong Song'}]",2024-07-11T08:19:58Z
http://arxiv.org/abs/2407.08269v1,http://arxiv.org/abs/2407.08269v1,LLMs' morphological analyses of complex FST-generated Finnish words,"Rule-based language processing systems have been overshadowed by neural
systems in terms of utility, but it remains unclear whether neural NLP systems,
in practice, learn the grammar rules that humans use. This work aims to shed
light on the issue by evaluating state-of-the-art LLMs in a task of
morphological analysis of complex Finnish noun forms. We generate the forms
using an FST tool, and they are unlikely to have occurred in the training sets
of the LLMs, therefore requiring morphological generalisation capacity. We find
that GPT-4-turbo has some difficulties in the task while GPT-3.5-turbo
struggles and smaller models Llama2-70B and Poro-34B fail nearly completely.","[{'name': 'Anssi Moisio'}, {'name': 'Mathias Creutz'}, {'name': 'Mikko Kurimo'}]",2024-07-11T08:12:28Z
http://arxiv.org/abs/2407.08223v1,http://arxiv.org/abs/2407.08223v1,"Speculative RAG: Enhancing Retrieval Augmented Generation through
  Drafting","Retrieval augmented generation (RAG) combines the generative abilities of
large language models (LLMs) with external knowledge sources to provide more
accurate and up-to-date responses. Recent RAG advancements focus on improving
retrieval outcomes through iterative LLM refinement or self-critique
capabilities acquired through additional instruction tuning of LLMs. In this
work, we introduce Speculative RAG - a framework that leverages a larger
generalist LM to efficiently verify multiple RAG drafts produced in parallel by
a smaller, distilled specialist LM. Each draft is generated from a distinct
subset of retrieved documents, offering diverse perspectives on the evidence
while reducing input token counts per draft. This approach enhances
comprehension of each subset and mitigates potential position bias over long
context. Our method accelerates RAG by delegating drafting to the smaller
specialist LM, with the larger generalist LM performing a single verification
pass over the drafts. Extensive experiments demonstrate that Speculative RAG
achieves state-of-the-art performance with reduced latency on TriviaQA,
MuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy
by up to 12.97% while reducing latency by 51% compared to conventional RAG
systems on PubHealth.","[{'name': 'Zilong Wang'}, {'name': 'Zifeng Wang'}, {'name': 'Long Le'}, {'name': 'Huaixiu Steven Zheng'}, {'name': 'Swaroop Mishra'}, {'name': 'Vincent Perot'}, {'name': 'Yuwei Zhang'}, {'name': 'Anush Mattapalli'}, {'name': 'Ankur Taly'}, {'name': 'Jingbo Shang'}, {'name': 'Chen-Yu Lee'}, {'name': 'Tomas Pfister'}]",2024-07-11T06:50:19Z
http://arxiv.org/abs/2407.08195v1,http://arxiv.org/abs/2407.08195v1,A Text-to-Game Engine for UGC-Based Role-Playing Games,"The shift from professionally generated content (PGC) to user-generated
content (UGC) has revolutionized various media formats, from text to video.
With the rapid advancements in generative AI, a similar shift is set to
transform the game industry, particularly in the realm of role-playing games
(RPGs). This paper introduces a new framework for a text-to-game engine that
utilizes foundation models to convert simple textual inputs into complex,
interactive RPG experiences. The engine dynamically renders the game story in a
multi-modal format and adjusts the game character, environment, and mechanics
in real-time in response to player actions. Using this framework, we developed
the ""Zagii"" game engine, which has successfully supported hundreds of RPG games
across a diverse range of genres and facilitated tens of thousands of online
user gameplay instances. This validates the effectiveness of our frame-work.
Our work showcases the potential for a more open and democratized gaming
paradigm, highlighting the transformative impact of generative AI on the game
life cycle.","[{'name': 'Lei Zhang'}, {'name': 'Xuezheng Peng'}, {'name': 'Shuyi Yang'}, {'name': 'Feiyang Wang'}]",2024-07-11T05:33:19Z
http://arxiv.org/abs/2407.08189v1,http://arxiv.org/abs/2407.08189v1,"fairBERTs: Erasing Sensitive Information Through Semantic and
  Fairness-aware Perturbations","Pre-trained language models (PLMs) have revolutionized both the natural
language processing research and applications. However, stereotypical biases
(e.g., gender and racial discrimination) encoded in PLMs have raised negative
ethical implications for PLMs, which critically limits their broader
applications. To address the aforementioned unfairness issues, we present
fairBERTs, a general framework for learning fair fine-tuned BERT series models
by erasing the protected sensitive information via semantic and fairness-aware
perturbations generated by a generative adversarial network. Through extensive
qualitative and quantitative experiments on two real-world tasks, we
demonstrate the great superiority of fairBERTs in mitigating unfairness while
maintaining the model utility. We also verify the feasibility of transferring
adversarial components in fairBERTs to other conventionally trained BERT-like
models for yielding fairness improvements. Our findings may shed light on
further research on building fairer fine-tuned PLMs.","[{'name': 'Jinfeng Li'}, {'name': 'Yuefeng Chen'}, {'name': 'Xiangyu Liu'}, {'name': 'Longtao Huang'}, {'name': 'Rong Zhang'}, {'name': 'Hui Xue'}]",2024-07-11T05:13:38Z
http://arxiv.org/abs/2407.08182v1,http://arxiv.org/abs/2407.08182v1,"Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal
  Theory for Post-Purchase Intention Analysis","Supervised machine-learning models for predicting user behavior offer a
challenging classification problem with lower average prediction performance
scores than other text classification tasks. This study evaluates multi-task
learning frameworks grounded in Cognitive Appraisal Theory to predict user
behavior as a function of users' self-expression and psychological attributes.
Our experiments show that users' language and traits improve predictions above
and beyond models predicting only from text. Our findings highlight the
importance of integrating psychological constructs into NLP to enhance the
understanding and prediction of user actions. We close with a discussion of the
implications for future applications of large language models for computational
psychology.","[{'name': 'Gerard Christopher Yeo'}, {'name': 'Shaz Furniturewala'}, {'name': 'Kokil Jaidka'}]",2024-07-11T04:57:52Z
http://arxiv.org/abs/2407.08152v1,http://arxiv.org/abs/2407.08152v1,"Privacy-Preserving Data Deduplication for Enhancing Federated Learning
  of Language Models","Deduplication is a vital preprocessing step that enhances machine learning
model performance and saves training time and energy. However, enhancing
federated learning through deduplication poses challenges, especially regarding
scalability and potential privacy violations if deduplication involves sharing
all clients' data. In this paper, we address the problem of deduplication in a
federated setup by introducing a pioneering protocol, Efficient
Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes
duplicates from multiple clients' datasets without compromising data privacy.
EP-MPD is constructed in a modular fashion, utilizing two novel variants of the
Private Set Intersection protocol. Our extensive experiments demonstrate the
significant benefits of deduplication in federated learning of large language
models. For instance, we observe up to 19.61% improvement in perplexity and up
to 27.95% reduction in running time. EP-MPD effectively balances privacy and
performance in federated learning, making it a valuable solution for
large-scale applications.","[{'name': 'Aydin Abadi'}, {'name': 'Vishnu Asutosh Dasu'}, {'name': 'Sumanta Sarkar'}]",2024-07-11T03:10:27Z
http://arxiv.org/abs/2407.08147v1,http://arxiv.org/abs/2407.08147v1,"Looks can be Deceptive: Distinguishing Repetition Disfluency from
  Reduplication","Reduplication and repetition, though similar in form, serve distinct
linguistic purposes. Reduplication is a deliberate morphological process used
to express grammatical, semantic, or pragmatic nuances, while repetition is
often unintentional and indicative of disfluency. This paper presents the first
large-scale study of reduplication and repetition in speech using computational
linguistics. We introduce IndicRedRep, a new publicly available dataset
containing Hindi, Telugu, and Marathi text annotated with reduplication and
repetition at the word level. We evaluate transformer-based models for
multi-class reduplication and repetition token classification, utilizing the
Reparandum-Interregnum-Repair structure to distinguish between the two
phenomena. Our models achieve macro F1 scores of up to 85.62% in Hindi, 83.95%
in Telugu, and 84.82% in Marathi for reduplication-repetition classification.","[{'name': 'Arif Ahmad'}, {'name': 'Mothika Gayathri Khyathi'}, {'name': 'Pushpak Bhattacharyya'}]",2024-07-11T03:00:14Z
http://arxiv.org/abs/2407.08112v2,http://arxiv.org/abs/2407.08112v2,"How Well Can a Long Sequence Model Model Long Sequences? Comparing
  Architechtural Inductive Biases on Long-Context Abilities","Long sequences occur in abundance within real-world scenarios, hence properly
modelling them opens numerous down-stream use-cases. Deep neural networks,
however, have often struggled with these for a variety of reasons. Recent
advances, both in system engineering as well as model design, have enabled the
scaling up of model that are purported to support extended context length. In
particular, the state-space and linear recurrent neural network families of
models hypothetically can entend to infinite sequence lenth. However, is this
too good to be true? We conduct an evaluation to show that while such claims
may be sound theoretically, there remain large practical gaps that are
empirically observed. In particular, recurrent models still suffer in the same
settings as long-context LLMs with attention. We further show that different
inductive biases have inconsistent extrapolation capabilities, highlighting the
need to further study such paradigms and investigate why long-context models
seemingly fail to behave as one might expect.",[{'name': 'Jerry Huang'}],2024-07-11T01:08:39Z
http://arxiv.org/abs/2407.08103v3,http://arxiv.org/abs/2407.08103v3,Automata-based constraints for language model decoding,"Language models (LMs) are often expected to generate strings in some formal
language; for example, structured data, API calls, or code snippets. Although
LMs can be tuned to improve their adherence to formal syntax, this does not
guarantee conformance, especially with smaller LMs suitable for large-scale
deployment. In addition, tuning requires significant resources, making it
impractical for uncommon or task-specific formats. To prevent downstream
parsing errors we would ideally constrain the LM to only produce valid output,
but this is severely complicated by tokenization, which is typically both
ambiguous and misaligned with the formal grammar. We solve these issues through
the application of automata theory, deriving an efficient closed-form solution
for the regular languages, a broad class of formal languages with many
practical applications, including API calls or schema-guided JSON and YAML. We
also discuss pragmatic extensions for coping with the issue of high branching
factor, and extend our techniques to deterministic context-free languages,
which similarly admit an efficient closed-form solution. Previous work on this
topic (Willard and Louf, 2023) layers bespoke solutions onto automata, leading
to problems with speed, correctness, and extensibility. Instead, we reformulate
the entire task in terms of automata so we can leverage well-studied and
well-optimized algorithms. Our system compiles constraints ~7,000x faster, is
provably correct, and can be extended in a modular fashion.","[{'name': 'Terry Koo'}, {'name': 'Frederick Liu'}, {'name': 'Luheng He'}]",2024-07-11T00:25:01Z
http://arxiv.org/abs/2407.08099v1,http://arxiv.org/abs/2407.08099v1,How does Burrows' Delta work on medieval Chinese poetic texts?,"Burrows' Delta was introduced in 2002 and has proven to be an effective tool
for author attribution. Despite the fact that these are different languages,
they mostly belong to the same grammatical type and use the same graphic
principle to convey speech in writing: a phonemic alphabet with word separation
using spaces. The question I want to address in this article is how well this
attribution method works with texts in a language with a different grammatical
structure and a script based on different principles. There are fewer studies
analyzing the effectiveness of the Delta method on Chinese texts than on texts
in European languages. I believe that such a low level of attention to Delta
from sinologists is due to the structure of the scientific field dedicated to
medieval Chinese poetry. Clustering based on intertextual distances worked
flawlessly. Delta produced results where clustering showed that the samples of
one author were most similar to each other, and Delta never confused different
poets. Despite the fact that I used an unconventional approach and applied the
Delta method to a language poorly suited for it, the method demonstrated its
effectiveness. Tang dynasty poets are correctly identified using Delta, and the
empirical pattern observed for authors writing in European standard languages
has been confirmed once again.",[{'name': 'Boris Orekhov'}],2024-07-11T00:07:14Z
http://arxiv.org/abs/2407.08095v1,http://arxiv.org/abs/2407.08095v1,"Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered
  Motivational Interviewing","We introduce a novel application of large language models (LLMs) in
developing a virtual counselor capable of conducting motivational interviewing
(MI) for alcohol use counseling. Access to effective counseling remains
limited, particularly for substance abuse, and virtual agents offer a promising
solution by leveraging LLM capabilities to simulate nuanced communication
techniques inherent in MI. Our approach combines prompt engineering and
integration into a user-friendly virtual platform to facilitate realistic,
empathetic interactions. We evaluate the effectiveness of our virtual agent
through a series of studies focusing on replicating MI techniques and human
counselor dialog. Initial findings suggest that our LLM-powered virtual agent
matches human counselors' empathetic and adaptive conversational skills,
presenting a significant step forward in virtual health counseling and
providing insights into the design and implementation of LLM-based therapeutic
interactions.","[{'name': 'Ian Steenstra'}, {'name': 'Farnaz Nouraei'}, {'name': 'Mehdi Arjmand'}, {'name': 'Timothy W. Bickmore'}]",2024-07-10T23:50:08Z
http://arxiv.org/abs/2407.08044v1,http://arxiv.org/abs/2407.08044v1,"RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
  Weight-Activation Quantization","Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient
Fine-Tuning (PEFT)method, significantly enhances the training efficiency by
updating only a small portion of the weights in Large Language Models (LLMs).
Recently, weight-only quantization techniques have also been applied to LoRA
methods to reduce the memory footprint of fine-tuning. However, applying
weight-activation quantization to the LoRA pipeline is under-explored, and we
observe substantial performance degradation primarily due to the presence of
activation outliers. In this work, we propose RoLoRA, the first LoRA-based
scheme for effective weight-activation quantization. RoLoRA utilizes rotation
for outlier elimination and proposes rotation-aware fine-tuning to preserve the
outlier-free characteristics in rotated LLMs. Experimental results show RoLoRA
consistently improves low-bit LoRA convergence and post-training quantization
robustness in weight-activation settings. We evaluate RoLoRA across
LLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain
of 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks
compared to LoRA baseline. We further demonstrate its effectiveness on Large
Multimodal Models (LLaVA-1.5-7B). Codes are available at
https://github.com/HuangOwen/RoLoRA","[{'name': 'Xijie Huang'}, {'name': 'Zechun Liu'}, {'name': 'Shih-Yang Liu'}, {'name': 'Kwang-Ting Cheng'}]",2024-07-10T20:52:18Z
http://arxiv.org/abs/2407.08039v1,http://arxiv.org/abs/2407.08039v1,"Knowledge Overshadowing Causes Amalgamated Hallucination in Large
  Language Models","Hallucination is often regarded as a major impediment for using large
language models (LLMs), especially for knowledge-intensive tasks. Even when the
training corpus consists solely of true statements, language models still
generate hallucinations in the form of amalgamations of multiple facts. We coin
this phenomenon as ``knowledge overshadowing'': when we query knowledge from a
language model with multiple conditions, some conditions overshadow others,
leading to hallucinated outputs. This phenomenon partially stems from training
data imbalance, which we verify on both pretrained models and fine-tuned
models, over a wide range of LM model families and sizes.From a theoretical
point of view, knowledge overshadowing can be interpreted as
over-generalization of the dominant conditions (patterns). We show that the
hallucination rate grows with both the imbalance ratio (between the popular and
unpopular condition) and the length of dominant condition description,
consistent with our derived generalization bound. Finally, we propose to
utilize overshadowing conditions as a signal to catch hallucination before it
is produced, along with a training-free self-contrastive decoding method to
alleviate hallucination during inference. Our proposed approach showcases up to
82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,
with different models and datasets.","[{'name': 'Yuji Zhang'}, {'name': 'Sha Li'}, {'name': 'Jiateng Liu'}, {'name': 'Pengfei Yu'}, {'name': 'Yi R. Fung'}, {'name': 'Jing Li'}, {'name': 'Manling Li'}, {'name': 'Heng Ji'}]",2024-07-10T20:37:42Z
http://arxiv.org/abs/2407.08035v1,http://arxiv.org/abs/2407.08035v1,"FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in
  Domain-specific Scenarios","Large Language Models (LLMs) have provided a new pathway for Named Entity
Recognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting
methods avoid the need for training, conserve substantial computational
resources, and rely on minimal annotated data. Previous studies have achieved
comparable performance to fully supervised BERT-based fine-tuning approaches on
general NER benchmarks. However, none of the previous approaches has
investigated the efficiency of LLM-based few-shot learning in domain-specific
scenarios. To address this gap, we introduce FsPONER, a novel approach for
optimizing few-shot prompts, and evaluate its performance on domain-specific
NER datasets, with a focus on industrial manufacturing and maintenance, while
using multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna.
FsPONER consists of three few-shot selection methods based on random sampling,
TF-IDF vectors, and a combination of both. We compare these methods with a
general-purpose GPT-NER method as the number of few-shot examples increases and
evaluate their optimal NER performance against fine-tuned BERT and LLaMA
2-chat. In the considered real-world scenarios with data scarcity, FsPONER with
TF-IDF surpasses fine-tuned models by approximately 10% in F1 score.","[{'name': 'Yongjian Tang'}, {'name': 'Rakebul Hasan'}, {'name': 'Thomas Runkler'}]",2024-07-10T20:32:50Z
http://arxiv.org/abs/2407.08029v1,http://arxiv.org/abs/2407.08029v1,"A Critical Review of Causal Reasoning Benchmarks for Large Language
  Models","Numerous benchmarks aim to evaluate the capabilities of Large Language Models
(LLMs) for causal inference and reasoning. However, many of them can likely be
solved through the retrieval of domain knowledge, questioning whether they
achieve their purpose. In this review, we present a comprehensive overview of
LLM benchmarks for causality. We highlight how recent benchmarks move towards a
more thorough definition of causal reasoning by incorporating interventional or
counterfactual reasoning. We derive a set of criteria that a useful benchmark
or set of benchmarks should aim to satisfy. We hope this work will pave the way
towards a general framework for the assessment of causal understanding in LLMs
and the design of novel benchmarks.","[{'name': 'Linying Yang'}, {'name': 'Vik Shirvaikar'}, {'name': 'Oscar Clivio'}, {'name': 'Fabian Falck'}]",2024-07-10T20:11:51Z
http://arxiv.org/abs/2407.08008v1,http://arxiv.org/abs/2407.08008v1,DS@GT eRisk 2024: Sentence Transformers for Social Media Risk Assessment,"We present working notes for DS@GT team in the eRisk 2024 for Tasks 1 and 3.
We propose a ranking system for Task 1 that predicts symptoms of depression
based on the Beck Depression Inventory (BDI-II) questionnaire using binary
classifiers trained on question relevancy as a proxy for ranking. We find that
binary classifiers are not well calibrated for ranking, and perform poorly
during evaluation. For Task 3, we use embeddings from BERT to predict the
severity of eating disorder symptoms based on user post history. We find that
classical machine learning models perform well on the task, and end up
competitive with the baseline models. Representation of text data is crucial in
both tasks, and we find that sentence transformers are a powerful tool for
downstream modeling. Source code and models are available at
\url{https://github.com/dsgt-kaggle-clef/erisk-2024}.","[{'name': 'David Guecha'}, {'name': 'Aaryan Potdar'}, {'name': 'Anthony Miyaguchi'}]",2024-07-10T19:30:16Z
http://arxiv.org/abs/2407.08001v1,http://arxiv.org/abs/2407.08001v1,Automated Neural Patent Landscaping in the Small Data Regime,"Patent landscaping is the process of identifying all patents related to a
particular technological area, and is important for assessing various aspects
of the intellectual property context. Traditionally, constructing patent
landscapes is intensely laborious and expensive, and the rapid expansion of
patenting activity in recent decades has driven an increasing need for
efficient and effective automated patent landscaping approaches. In particular,
it is critical that we be able to construct patent landscapes using a minimal
number of labeled examples, as labeling patents for a narrow technology area
requires highly specialized (and hence expensive) technical knowledge. We
present an automated neural patent landscaping system that demonstrates
significantly improved performance on difficult examples (0.69 $F_1$ on 'hard'
examples, versus 0.6 for previously reported systems), and also significant
improvements with much less training data (overall 0.75 $F_1$ on as few as 24
examples). Furthermore, in evaluating such automated landscaping systems,
acquiring good data is challenge; we demonstrate a higher-quality training data
generation procedure by merging Abood and Feltenberger's (2018)
""seed/anti-seed"" approach with active learning to collect difficult labeled
examples near the decision boundary. Using this procedure we created a new
dataset of labeled AI patents for training and testing. As in prior work we
compare our approach with a number of baseline systems, and we release our code
and data for others to build upon.","[{'name': 'Tisa Islam Erana'}, {'name': 'Mark A. Finlayson'}]",2024-07-10T19:13:37Z
http://arxiv.org/abs/2407.07950v1,http://arxiv.org/abs/2407.07950v1,"Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM
  Reliance","The reconfiguration of human-LM interactions from simple sentence completions
to complex, multi-domain, humanlike engagements necessitates new methodologies
to understand how humans choose to rely on LMs. In our work, we contend that
reliance is influenced by numerous factors within the interactional context of
a generation, a departure from prior work that used verbalized confidence
(e.g., ""I'm certain the answer is..."") as the key determinant of reliance.
Here, we introduce Rel-A.I., an in situ, system-level evaluation approach to
measure human reliance on LM-generated epistemic markers (e.g., ""I think
it's.."", ""Undoubtedly it's...""). Using this methodology, we measure reliance
rates in three emergent human-LM interaction settings: long-term interactions,
anthropomorphic generations, and variable subject matter. Our findings reveal
that reliance is not solely based on verbalized confidence but is significantly
affected by other features of the interaction context. Prior interactions,
anthropomorphic cues, and subject domain all contribute to reliance
variability. An expression such as, ""I'm pretty sure it's..."", can vary up to
20% in reliance frequency depending on its interactional context. Our work
underscores the importance of context in understanding human reliance and
offers future designers and researchers with a methodology to conduct such
measurements.","[{'name': 'Kaitlyn Zhou'}, {'name': 'Jena D. Hwang'}, {'name': 'Xiang Ren'}, {'name': 'Nouha Dziri'}, {'name': 'Dan Jurafsky'}, {'name': 'Maarten Sap'}]",2024-07-10T18:00:05Z
http://arxiv.org/abs/2407.18940v1,http://arxiv.org/abs/2407.18940v1,LitSearch: A Retrieval Benchmark for Scientific Literature Search,"Literature search questions, such as ""where can I find research on the
evaluation of consistency in generated summaries?"" pose significant challenges
for modern search engines and retrieval systems. These questions often require
a deep understanding of research concepts and the ability to reason over entire
articles. In this work, we introduce LitSearch, a retrieval benchmark
comprising 597 realistic literature search queries about recent ML and NLP
papers. LitSearch is constructed using a combination of (1) questions generated
by GPT-4 based on paragraphs containing inline citations from research papers
and (2) questions about recently published papers, manually written by their
authors. All LitSearch questions were manually examined or edited by experts to
ensure high quality. We extensively benchmark state-of-the-art retrieval models
and also evaluate two LLM-based reranking pipelines. We find a significant
performance gap between BM25 and state-of-the-art dense retrievers, with a
24.8% difference in absolute recall@5. The LLM-based reranking strategies
further improve the best-performing dense retriever by 4.4%. Additionally,
commercial search engines and research tools like Google Search perform poorly
on LitSearch, lagging behind the best dense retriever by 32 points. Taken
together, these results show that LitSearch is an informative new testbed for
retrieval systems while catering to a real-world use case.","[{'name': 'Anirudh Ajith'}, {'name': 'Mengzhou Xia'}, {'name': 'Alexis Chevalier'}, {'name': 'Tanya Goyal'}, {'name': 'Danqi Chen'}, {'name': 'Tianyu Gao'}]",2024-07-10T18:00:03Z
http://arxiv.org/abs/2407.07895v2,http://arxiv.org/abs/2407.07895v2,"LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large
  Multimodal Models","Visual instruction tuning has made considerable strides in enhancing the
capabilities of Large Multimodal Models (LMMs). However, existing open LMMs
largely focus on single-image tasks, their applications to multi-image
scenarios remains less explored. Additionally, prior LMM research separately
tackles different scenarios, leaving it impossible to generalize cross
scenarios with new emerging capabilities. To this end, we introduce
LLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame
(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To
enable these capabilities, we regard the interleaved data format as a general
template and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4
primary domains with 14 tasks and 41 datasets. We also curate the
LLaVA-Interleave Bench to comprehensively evaluate the multi-image performance
of LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading
results in multi-image, video, and 3D benchmarks, while maintaining the
performance of single-image tasks. Besides, our model also exhibits several
emerging capabilities, e.g., transferring tasks across different settings and
modalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT","[{'name': 'Feng Li'}, {'name': 'Renrui Zhang'}, {'name': 'Hao Zhang'}, {'name': 'Yuanhan Zhang'}, {'name': 'Bo Li'}, {'name': 'Wei Li'}, {'name': 'Zejun Ma'}, {'name': 'Chunyuan Li'}]",2024-07-10T17:59:43Z
http://arxiv.org/abs/2407.07890v1,http://arxiv.org/abs/2407.07890v1,Training on the Test Task Confounds Evaluation and Emergence,"We study a fundamental problem in the evaluation of large language models
that we call training on the test task. Unlike wrongful practices like training
on the test data, leakage, or data contamination, training on the test task is
not a malpractice. Rather, the term describes a growing set of techniques to
include task-relevant data in the pretraining stage of a language model. We
demonstrate that training on the test task confounds both relative model
evaluations and claims about emergent capabilities. We argue that the seeming
superiority of one model family over another may be explained by a different
degree of training on the test task. To this end, we propose an effective
method to adjust for training on the test task by fine-tuning each model under
comparison on the same task-relevant data before evaluation. We then show that
instances of emergent behavior largely vanish once we adjust for training on
the test task. This also applies to reported instances of emergent behavior
that cannot be explained by the choice of evaluation metric. Our work promotes
a new perspective on the evaluation of large language models with broad
implications for benchmarking and the study of emergent capabilities.","[{'name': 'Ricardo Dominguez-Olmedo'}, {'name': 'Florian E. Dorner'}, {'name': 'Moritz Hardt'}]",2024-07-10T17:57:58Z
http://arxiv.org/abs/2407.11062v1,http://arxiv.org/abs/2407.11062v1,"EfficientQAT: Efficient Quantization-Aware Training for Large Language
  Models","Large language models (LLMs) are integral to modern natural language
processing and artificial intelligence. However, they face challenges in
managing their significant memory requirements. Although quantization-aware
training (QAT) offers a solution by reducing memory consumption through low-bit
representations with minimal accuracy loss, it demands substantial training
resources to optimize model weights and quantization parameters. To address
this, we propose Efficient Quantization-Aware Training (EfficientQAT), a novel
quantization technique for compressing LLMs. EfficientQAT involves two
consecutive phases: Block-wise training of all parameters (Block-AP) and
end-to-end training of quantization parameters (E2E-QP). Block-AP sequentially
conducts quantization-aware training for all parameters in each transformer
block with block-wise reconstruction, maintaining efficiency by avoiding
training the entire LLM. Initialized with quantized model, E2E-QP then trains
only quantization parameters (step sizes) end-to-end, enhancing efficiency with
a fixed quantized backbone and reduced trainable parameter count. Extensive
experiments demonstrate that EfficientQAT outperforms previous quantization
methods across a range of models, including base LLMs, instruction-tuned LLMs,
and multimodal LLMs, with scales from 7B to 70B parameters at various
quantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model
on a single A100-80GB GPU in 41 hours, with less than 3\% accuracy degradation
compared to the full precision (69.48 vs. 72.41). Notably, this INT2 quantized
70B model obtains a 1.67 accuracy gain over the Llama-2-13B model (69.48 vs.
67.81) while requiring less memory (19.2GB vs. 24.2GB). Code is available at
https://github.com/OpenGVLab/EfficientQAT.","[{'name': 'Mengzhao Chen'}, {'name': 'Wenqi Shao'}, {'name': 'Peng Xu'}, {'name': 'Jiahao Wang'}, {'name': 'Peng Gao'}, {'name': 'Kaipeng Zhang'}, {'name': 'Yu Qiao'}, {'name': 'Ping Luo'}]",2024-07-10T17:53:30Z
http://arxiv.org/abs/2407.07880v1,http://arxiv.org/abs/2407.07880v1,"Towards Robust Alignment of Language Models: Distributionally
  Robustifying Direct Preference Optimization","This study addresses the challenge of noise in training datasets for Direct
Preference Optimization (DPO), a method for aligning Large Language Models
(LLMs) with human preferences. We categorize noise into pointwise noise, which
includes low-quality data points, and pairwise noise, which encompasses
erroneous data pair associations that affect preference rankings. Utilizing
Distributionally Robust Optimization (DRO), we enhance DPO's resilience to
these types of noise. Our theoretical insights reveal that DPO inherently
embeds DRO principles, conferring robustness to pointwise noise, with the
regularization coefficient $\beta$ playing a critical role in its noise
resistance. Extending this framework, we introduce Distributionally
Robustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing
against worst-case pairwise scenarios. The novel hyperparameter $\beta'$ in Dr.
DPO allows for fine-tuned control over data pair reliability, providing a
strategic balance between exploration and exploitation in noisy training
environments. Empirical evaluations demonstrate that Dr. DPO substantially
improves the quality of generated text and response accuracy in preference
datasets, showcasing enhanced performance in both noisy and noise-free
settings. The code is available at https://github.com/junkangwu/Dr_DPO.","[{'name': 'Junkang Wu'}, {'name': 'Yuexiang Xie'}, {'name': 'Zhengyi Yang'}, {'name': 'Jiancan Wu'}, {'name': 'Jiawei Chen'}, {'name': 'Jinyang Gao'}, {'name': 'Bolin Ding'}, {'name': 'Xiang Wang'}, {'name': 'Xiangnan He'}]",2024-07-10T17:48:25Z
http://arxiv.org/abs/2407.07875v1,http://arxiv.org/abs/2407.07875v1,Generative Image as Action Models,"Image-generation diffusion models have been fine-tuned to unlock new
capabilities such as image-editing and novel view synthesis. Can we similarly
unlock image-generation models for visuomotor control? We present GENIMA, a
behavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'
as targets on RGB images. These images are fed into a controller that maps the
visual targets into a sequence of joint-positions. We study GENIMA on 25
RLBench and 9 real-world manipulation tasks. We find that, by lifting actions
into image-space, internet pre-trained diffusion models can generate policies
that outperform state-of-the-art visuomotor approaches, especially in
robustness to scene perturbations and generalizing to novel objects. Our method
is also competitive with 3D agents, despite lacking priors such as depth,
keypoints, or motion-planners.","[{'name': 'Mohit Shridhar'}, {'name': 'Yat Long Lo'}, {'name': 'Stephen James'}]",2024-07-10T17:41:10Z
http://arxiv.org/abs/2407.07858v1,http://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key
applications to enhance employee productivity. Retrieval Augmented Generation
(RAG), Large Language Models (LLMs), and orchestration frameworks like
Langchain and Llamaindex are crucial for building these chatbots. However,
creating effective enterprise chatbots is challenging and requires meticulous
RAG pipeline engineering. This includes fine-tuning embeddings and LLMs,
extracting documents from vector databases, rephrasing queries, reranking
results, designing prompts, honoring document access controls, providing
concise responses, including references, safeguarding personal information, and
building orchestration agents. We present a framework for building RAG-based
chatbots based on our experience with three NVIDIA chatbots: for IT/HR
benefits, financial earnings, and general content. Our contributions are
three-fold: introducing the FACTS framework (Freshness, Architectures, Cost,
Testing, Security), presenting fifteen RAG pipeline control points, and
providing empirical results on accuracy-latency tradeoffs between large and
small LLMs. To the best of our knowledge, this is the first paper of its kind
that provides a holistic view of the factors as well as solutions for building
secure enterprise-grade chatbots.""","[{'name': 'Rama Akkiraju'}, {'name': 'Anbang Xu'}, {'name': 'Deepak Bora'}, {'name': 'Tan Yu'}, {'name': 'Lu An'}, {'name': 'Vishal Seth'}, {'name': 'Aaditya Shukla'}, {'name': 'Pritam Gundecha'}, {'name': 'Hridhay Mehta'}, {'name': 'Ashwin Jha'}, {'name': 'Prithvi Raj'}, {'name': 'Abhinav Balasubramanian'}, {'name': 'Murali Maram'}, {'name': 'Guru Muthusamy'}, {'name': 'Shivakesh Reddy Annepally'}, {'name': 'Sidney Knowles'}, {'name': 'Min Du'}, {'name': 'Nick Burnett'}, {'name': 'Sean Javiya'}, {'name': 'Ashok Marannan'}, {'name': 'Mamta Kumari'}, {'name': 'Surbhi Jha'}, {'name': 'Ethan Dereszenski'}, {'name': 'Anupam Chakraborty'}, {'name': 'Subhash Ranjan'}, {'name': 'Amina Terfai'}, {'name': 'Anoop Surya'}, {'name': 'Tracey Mercer'}, {'name': 'Vinodh Kumar Thanigachalam'}, {'name': 'Tamar Bar'}, {'name': 'Sanjana Krishnan'}, {'name': 'Samy Kilaru'}, {'name': 'Jasmine Jaksic'}, {'name': 'Nave Algarici'}, {'name': 'Jacob Liberman'}, {'name': 'Joey Conway'}, {'name': 'Sonu Nayyar'}, {'name': 'Justin Boitano'}]",2024-07-10T17:20:59Z
http://arxiv.org/abs/2407.07840v2,http://arxiv.org/abs/2407.07840v2,"Decompose and Compare Consistency: Measuring VLMs' Answer Reliability
  via Task-Decomposition Consistency Comparison","Despite tremendous advancements, current state-of-the-art Vision-Language
Models (VLMs) are still far from perfect. They tend to hallucinate and may
generate biased responses. In such circumstances, having a way to assess the
reliability of a given response generated by a VLM is quite useful. Existing
methods, such as estimating uncertainty using answer likelihoods or
prompt-based confidence generation, often suffer from overconfidence. Other
methods use self-consistency comparison but are affected by confirmation
biases. To alleviate these, we propose \textbf{De}compose and \textbf{C}ompare
\textbf{C}onsistency (\texttt{DeCC}) for reliability measurement. By comparing
the consistency between the direct answer generated using the VLM's internal
reasoning process, and the indirect answers obtained by decomposing the
question into sub-questions and reasoning over the sub-answers produced by the
VLM, \texttt{DeCC} measures the reliability of VLM's direct answer. Experiments
across six vision-language tasks with three VLMs show \texttt{DeCC}'s
reliability estimation achieves better correlation with task accuracy compared
to the existing methods.","[{'name': 'Qian Yang'}, {'name': 'Weixiang Yan'}, {'name': 'Aishwarya Agrawal'}]",2024-07-10T17:00:29Z
http://arxiv.org/abs/2407.07810v1,http://arxiv.org/abs/2407.07810v1,Transformer Alignment in Large Language Models,"Large Language Models (LLMs) have made significant strides in natural
language processing, and a precise understanding of the internal mechanisms
driving their success is essential. We regard LLMs as transforming embeddings
via a discrete, coupled, nonlinear, dynamical system in high dimensions. This
perspective motivates tracing the trajectories of individual tokens as they
pass through transformer blocks, and linearizing the system along these
trajectories through their Jacobian matrices. In our analysis of 38 openly
available LLMs, we uncover the alignment of top left and right singular vectors
of Residual Jacobians, as well as the emergence of linearity and layer-wise
exponential growth. Notably, we discover that increased alignment
$\textit{positively correlates}$ with model performance. Metrics evaluated
post-training show significant improvement in comparison to measurements made
with randomly initialized weights, highlighting the significant effects of
training in transformers. These findings reveal a remarkable level of
regularity that has previously been overlooked, reinforcing the dynamical
interpretation and paving the way for deeper understanding and optimization of
LLM architectures.","[{'name': 'Murdock Aubry'}, {'name': 'Haoming Meng'}, {'name': 'Anton Sugolov'}, {'name': 'Vardan Papyan'}]",2024-07-10T16:30:27Z
http://arxiv.org/abs/2407.07802v1,http://arxiv.org/abs/2407.07802v1,ROSA: Random Subspace Adaptation for Efficient Fine-Tuning,"Model training requires significantly more memory, compared with inference.
Parameter efficient fine-tuning (PEFT) methods provide a means of adapting
large models to downstream tasks using less memory. However, existing methods
such as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce
latency overhead at inference time or achieve subpar downstream performance
compared with full fine-tuning. In this work we propose Random Subspace
Adaptation (ROSA), a method that outperforms previous PEFT methods by a
significant margin, while maintaining a zero latency overhead during inference
time. In contrast to previous methods, ROSA is able to adapt subspaces of
arbitrarily large dimension, better approximating full-finetuning. We
demonstrate both theoretically and experimentally that this makes ROSA strictly
more expressive than LoRA, without consuming additional memory during runtime.
As PEFT methods are especially useful in the natural language processing
domain, where models operate on scales that make full fine-tuning very
expensive, we evaluate ROSA in two common NLP scenarios: natural language
generation (NLG) and natural language understanding (NLU) with GPT-2 and
RoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms
LoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our
code is available at https://github.com/rosa-paper/rosa","[{'name': 'Marawan Gamal Abdel Hameed'}, {'name': 'Aristides Milios'}, {'name': 'Siva Reddy'}, {'name': 'Guillaume Rabusseau'}]",2024-07-10T16:20:53Z
http://arxiv.org/abs/2407.07801v2,http://arxiv.org/abs/2407.07801v2,AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning,"In recent years, advancements in representation learning and language models
have propelled Automated Captioning (AC) to new heights, enabling the
generation of human-level descriptions. Leveraging these advancements, we
propose AVCap, an Audio-Visual Captioning framework, a simple yet powerful
baseline approach applicable to audio-visual captioning. AVCap utilizes
audio-visual features as text tokens, which has many advantages not only in
performance but also in the extensibility and scalability of the model. AVCap
is designed around three pivotal dimensions: the exploration of optimal
audio-visual encoder architectures, the adaptation of pre-trained models
according to the characteristics of generated text, and the investigation into
the efficacy of modality fusion in captioning. Our method outperforms existing
audio-visual captioning methods across all metrics and the code is available on
https://github.com/JongSuk1/AVCap","[{'name': 'Jongsuk Kim'}, {'name': 'Jiwon Shin'}, {'name': 'Junmo Kim'}]",2024-07-10T16:17:49Z
http://arxiv.org/abs/2407.07799v1,http://arxiv.org/abs/2407.07799v1,Attribute or Abstain: Large Language Models as Long Document Assistants,"LLMs can help humans working with long documents, but are known to
hallucinate. Attribution can increase trust in LLM responses: The LLM provides
evidence that supports its response, which enhances verifiability. Existing
approaches to attribution have only been evaluated in RAG settings, where the
initial retrieval confounds LLM performance. This is crucially different from
the long document setting, where retrieval is not needed, but could help. Thus,
a long document specific evaluation of attribution is missing. To fill this
gap, we present LAB, a benchmark of 6 diverse long document tasks with
attribution, and experiment with different approaches to attribution on 4 LLMs
of different sizes, both prompted and fine-tuned. We find that citation, i.e.
response generation and evidence extraction in one step, mostly performs best.
We investigate whether the ``Lost in the Middle'' phenomenon exists for
attribution, but do not find this. We also find that evidence quality can
predict response quality on datasets with simple responses, but not so for
complex responses, as models struggle with providing evidence for complex
claims. We release code and data for further investigation.","[{'name': 'Jan Buchmann'}, {'name': 'Xiao Liu'}, {'name': 'Iryna Gurevych'}]",2024-07-10T16:16:02Z
http://arxiv.org/abs/2407.07796v2,http://arxiv.org/abs/2407.07796v2,"Evaluating Large Language Models with Grid-Based Game Competitions: An
  Extensible LLM Benchmark and Leaderboard","We introduce a novel and extensible benchmark for large language models
(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.
The open-source game simulation code, available on GitHub, allows LLMs to
compete and generates detailed data files in JSON, CSV, TXT, and PNG formats
for leaderboard rankings and further analysis. We present the results of games
among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by
Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and
GPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of
results from other LLMs. In total, we simulated 2,310 matches (5 sessions for
each pair among 7 LLMs and a random player) across three types of games, using
three distinct prompt types: list, illustration, and image. The results
revealed significant variations in LLM performance across different games and
prompt types, with analysis covering win and disqualification rates, missed
opportunity analysis, and invalid move analysis. The details of the leaderboard
and result matrix data are available as open-access data on GitHub. This study
enhances our understanding of LLMs' capabilities in playing games they were not
specifically trained for, helping to assess their rule comprehension and
strategic thinking. On the path to Artificial General Intelligence (AGI), this
study lays the groundwork for future exploration into their utility in complex
decision-making scenarios, illuminating their strategic thinking abilities and
offering directions for further inquiry into the limits of LLMs within
game-based frameworks.","[{'name': 'Oguzhan Topsakal'}, {'name': 'Colby Jacob Edell'}, {'name': 'Jackson Bailey Harper'}]",2024-07-10T16:14:34Z
http://arxiv.org/abs/2407.07791v2,http://arxiv.org/abs/2407.07791v2,"Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent
  Communities","The rapid adoption of large language models (LLMs) in multi-agent systems has
highlighted their impressive capabilities in various applications, such as
collaborative problem-solving and autonomous negotiation. However, the security
implications of these LLM-based multi-agent systems have not been thoroughly
investigated, particularly concerning the spread of manipulated knowledge. In
this paper, we investigate this critical issue by constructing a detailed
threat model and a comprehensive simulation environment that mirrors real-world
multi-agent deployments in a trusted platform. Subsequently, we propose a novel
two-stage attack method involving Persuasiveness Injection and Manipulated
Knowledge Injection to systematically explore the potential for manipulated
knowledge (i.e., counterfactual and toxic knowledge) spread without explicit
prompt manipulation.
  Our method leverages the inherent vulnerabilities of LLMs in handling world
knowledge, which can be exploited by attackers to unconsciously spread
fabricated information. Through extensive experiments, we demonstrate that our
attack method can successfully induce LLM-based agents to spread both
counterfactual and toxic knowledge without degrading their foundational
capabilities during agent communication. Furthermore, we show that these
manipulations can persist through popular retrieval-augmented generation
frameworks, where several benign agents store and retrieve manipulated chat
histories for future interactions. This persistence indicates that even after
the interaction has ended, the benign agents may continue to be influenced by
manipulated knowledge. Our findings reveal significant security risks in
LLM-based multi-agent systems, emphasizing the imperative need for robust
defenses against manipulated knowledge spread, such as introducing ``guardian''
agents and advanced fact-checking tools.","[{'name': 'Tianjie Ju'}, {'name': 'Yiting Wang'}, {'name': 'Xinbei Ma'}, {'name': 'Pengzhou Cheng'}, {'name': 'Haodong Zhao'}, {'name': 'Yulong Wang'}, {'name': 'Lifeng Liu'}, {'name': 'Jian Xie'}, {'name': 'Zhuosheng Zhang'}, {'name': 'Gongshen Liu'}]",2024-07-10T16:08:46Z
http://arxiv.org/abs/2407.07771v1,http://arxiv.org/abs/2407.07771v1,Multi-task Prompt Words Learning for Social Media Content Generation,"The rapid development of the Internet has profoundly changed human life.
Humans are increasingly expressing themselves and interacting with others on
social media platforms. However, although artificial intelligence technology
has been widely used in many aspects of life, its application in social media
content creation is still blank. To solve this problem, we propose a new prompt
word generation framework based on multi-modal information fusion, which
combines multiple tasks including topic classification, sentiment analysis,
scene recognition and keyword extraction to generate more comprehensive prompt
words. Subsequently, we use a template containing a set of prompt words to
guide ChatGPT to generate high-quality tweets. Furthermore, in the absence of
effective and objective evaluation criteria in the field of content generation,
we use the ChatGPT tool to evaluate the results generated by the algorithm,
making large-scale evaluation of content generation algorithms possible.
Evaluation results on extensive content generation demonstrate that our cue
word generation framework generates higher quality content compared to manual
methods and other cueing techniques, while topic classification, sentiment
analysis, and scene recognition significantly enhance content clarity and its
consistency with the image.","[{'name': 'Haochen Xue'}, {'name': 'Chong Zhang'}, {'name': 'Chengzhi Liu'}, {'name': 'Fangyu Wu'}, {'name': 'Xiaobo Jin'}]",2024-07-10T15:46:32Z
http://arxiv.org/abs/2407.07737v1,http://arxiv.org/abs/2407.07737v1,Fine-Tuning Large Language Models with User-Level Differential Privacy,"We investigate practical and scalable algorithms for training large language
models (LLMs) with user-level differential privacy (DP) in order to provably
safeguard all the examples contributed by each user. We study two variants of
DP-SGD with: (1) example-level sampling (ELS) and per-example gradient
clipping, and (2) user-level sampling (ULS) and per-user gradient clipping. We
derive a novel user-level DP accountant that allows us to compute provably
tight privacy guarantees for ELS. Using this, we show that while ELS can
outperform ULS in specific settings, ULS generally yields better results when
each user has a diverse collection of examples. We validate our findings
through experiments in synthetic mean estimation and LLM fine-tuning tasks
under fixed compute budgets. We find that ULS is significantly better in
settings where either (1) strong privacy guarantees are required, or (2) the
compute budget is large. Notably, our focus on LLM-compatible training
algorithms allows us to scale to models with hundreds of millions of parameters
and datasets with hundreds of thousands of users.","[{'name': 'Zachary Charles'}, {'name': 'Arun Ganesh'}, {'name': 'Ryan McKenna'}, {'name': 'H. Brendan McMahan'}, {'name': 'Nicole Mitchell'}, {'name': 'Krishna Pillutla'}, {'name': 'Keith Rush'}]",2024-07-10T15:07:58Z
http://arxiv.org/abs/2407.07726v1,http://arxiv.org/abs/2407.07726v1,PaliGemma: A versatile 3B VLM for transfer,"PaliGemma is an open Vision-Language Model (VLM) that is based on the
SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to
be a versatile and broadly knowledgeable base model that is effective to
transfer. It achieves strong performance on a wide variety of open-world tasks.
We evaluate PaliGemma on almost 40 diverse tasks including standard VLM
benchmarks, but also more specialized tasks such as remote-sensing and
segmentation.","[{'name': 'Lucas Beyer'}, {'name': 'Andreas Steiner'}, {'name': 'André Susano Pinto'}, {'name': 'Alexander Kolesnikov'}, {'name': 'Xiao Wang'}, {'name': 'Daniel Salz'}, {'name': 'Maxim Neumann'}, {'name': 'Ibrahim Alabdulmohsin'}, {'name': 'Michael Tschannen'}, {'name': 'Emanuele Bugliarello'}, {'name': 'Thomas Unterthiner'}, {'name': 'Daniel Keysers'}, {'name': 'Skanda Koppula'}, {'name': 'Fangyu Liu'}, {'name': 'Adam Grycner'}, {'name': 'Alexey Gritsenko'}, {'name': 'Neil Houlsby'}, {'name': 'Manoj Kumar'}, {'name': 'Keran Rong'}, {'name': 'Julian Eisenschlos'}, {'name': 'Rishabh Kabra'}, {'name': 'Matthias Bauer'}, {'name': 'Matko Bošnjak'}, {'name': 'Xi Chen'}, {'name': 'Matthias Minderer'}, {'name': 'Paul Voigtlaender'}, {'name': 'Ioana Bica'}, {'name': 'Ivana Balazevic'}, {'name': 'Joan Puigcerver'}, {'name': 'Pinelopi Papalampidi'}, {'name': 'Olivier Henaff'}, {'name': 'Xi Xiong'}, {'name': 'Radu Soricut'}, {'name': 'Jeremiah Harmsen'}, {'name': 'Xiaohua Zhai'}]",2024-07-10T14:57:46Z
http://arxiv.org/abs/2407.07683v1,http://arxiv.org/abs/2407.07683v1,"The Language of Weather: Social Media Reactions to Weather Accounting
  for Climatic and Linguistic Baselines","This study explores how different weather conditions influence public
sentiment on social media, focusing on Twitter data from the UK. By considering
climate and linguistic baselines, we improve the accuracy of weather-related
sentiment analysis. Our findings show that emotional responses to weather are
complex, influenced by combinations of weather variables and regional language
differences. The results highlight the importance of context-sensitive methods
for better understanding public mood in response to weather, which can enhance
impact-based forecasting and risk communication in the context of climate
change.","[{'name': 'James C. Young'}, {'name': 'Rudy Arthur'}, {'name': 'Hywel T. P. Williams'}]",2024-07-10T14:08:24Z
http://arxiv.org/abs/2407.07666v1,http://arxiv.org/abs/2407.07666v1,"A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models :
  Safety, Consensus, Objectivity, Reproducibility and Explainability","A comprehensive qualitative evaluation framework for large language models
(LLM) in healthcare that expands beyond traditional accuracy and quantitative
metrics needed. We propose 5 key aspects for evaluation of LLMs: Safety,
Consensus, Objectivity, Reproducibility and Explainability (S.C.O.R.E.). We
suggest that S.C.O.R.E. may form the basis for an evaluation framework for
future LLM-based models that are safe, reliable, trustworthy, and ethical for
healthcare and clinical applications.","[{'name': 'Ting Fang Tan'}, {'name': 'Kabilan Elangovan'}, {'name': 'Jasmine Ong'}, {'name': 'Nigam Shah'}, {'name': 'Joseph Sung'}, {'name': 'Tien Yin Wong'}, {'name': 'Lan Xue'}, {'name': 'Nan Liu'}, {'name': 'Haibo Wang'}, {'name': 'Chang Fu Kuo'}, {'name': 'Simon Chesterman'}, {'name': 'Zee Kin Yeong'}, {'name': 'Daniel SW Ting'}]",2024-07-10T13:45:16Z
http://arxiv.org/abs/2407.07630v1,http://arxiv.org/abs/2407.07630v1,"A Review of the Challenges with Massive Web-mined Corpora Used in Large
  Language Models Pre-Training","This article presents a comprehensive review of the challenges associated
with using massive web-mined corpora for the pre-training of large language
models (LLMs). This review identifies key challenges in this domain, including
challenges such as noise (irrelevant or misleading information), duplication of
content, the presence of low-quality or incorrect information, biases, and the
inclusion of sensitive or personal information in web-mined corpora. Addressing
these issues is crucial for the development of accurate, reliable, and
ethically responsible language models. Through an examination of current
methodologies for data cleaning, pre-processing, bias detection and mitigation,
we highlight the gaps in existing approaches and suggest directions for future
research. Our discussion aims to catalyze advancements in developing more
sophisticated and ethically responsible LLMs.","[{'name': 'Michał Perełkiewicz'}, {'name': 'Rafał Poświata'}]",2024-07-10T13:09:23Z
http://arxiv.org/abs/2407.07617v1,http://arxiv.org/abs/2407.07617v1,"Psycho-linguistic Experiment on Universal Semantic Components of Verbal
  Humor: System Description and Annotation","Objective criteria for universal semantic components that distinguish a
humorous utterance from a non-humorous one are presently under debate. In this
article, we give an in-depth observation of our system of self-paced reading
for annotation of humor, that collects readers' annotations while they open a
text word by word. The system registers keys that readers press to open the
next word, choose a class (humorous versus non-humorous texts), change their
choice. We also touch upon our psycho-linguistic experiment conducted with the
system and the data collected during it.","[{'name': 'Elena Mikhalkova'}, {'name': 'Nadezhda Ganzherli'}, {'name': 'Julia Murzina'}]",2024-07-10T12:56:17Z
http://arxiv.org/abs/2407.07612v1,http://arxiv.org/abs/2407.07612v1,Teaching Transformers Causal Reasoning through Axiomatic Training,"For text-based AI systems to interact in the real world, causal reasoning is
an essential skill. Since interventional data is costly to generate, we study
to what extent an agent can learn causal reasoning from passive data.
Specifically, we consider an axiomatic training setup where an agent learns
from multiple demonstrations of a causal axiom (or rule), rather than
incorporating the axiom as an inductive bias or inferring it from data values.
A key question is whether the agent would learn to generalize from the axiom
demonstrations to new scenarios. For example, if a transformer model is trained
on demonstrations of the causal transitivity axiom over small graphs, would it
generalize to applying the transitivity axiom over large graphs? Our results,
based on a novel axiomatic training scheme, indicate that such generalization
is possible. We consider the task of inferring whether a variable causes
another variable, given a causal graph structure. We find that a 67 million
parameter transformer model, when trained on linear causal chains (along with
some noisy variations) can generalize well to new kinds of graphs, including
longer causal chains, causal chains with reversed order, and graphs with
branching; even when it is not explicitly trained for such settings. Our model
performs at par (or even better) than many larger language models such as
GPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework
provides a new paradigm of learning causal reasoning from passive data that can
be used to learn arbitrary axioms, as long as sufficient demonstrations can be
generated.","[{'name': 'Aniket Vashishtha'}, {'name': 'Abhinav Kumar'}, {'name': 'Abbavaram Gowtham Reddy'}, {'name': 'Vineeth N Balasubramanian'}, {'name': 'Amit Sharma'}]",2024-07-10T12:50:44Z
http://arxiv.org/abs/2407.07606v1,http://arxiv.org/abs/2407.07606v1,"The Computational Learning of Construction Grammars: State of the Art
  and Prospective Roadmap","This paper documents and reviews the state of the art concerning
computational models of construction grammar learning. It brings together prior
work on the computational learning of form-meaning pairings, which has so far
been studied in several distinct areas of research. The goal of this paper is
threefold. First of all, it aims to synthesise the variety of methodologies
that have been proposed to date and the results that have been obtained.
Second, it aims to identify those parts of the challenge that have been
successfully tackled and reveal those that require further research. Finally,
it aims to provide a roadmap which can help to boost and streamline future
research efforts on the computational learning of large-scale, usage-based
construction grammars.","[{'name': 'Jonas Doumen'}, {'name': 'Veronica Juliana Schmalz'}, {'name': 'Katrien Beuls'}, {'name': 'Paul Van Eecke'}]",2024-07-10T12:45:02Z
http://arxiv.org/abs/2407.07566v1,http://arxiv.org/abs/2407.07566v1,HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing,"We present HebDB, a weakly supervised dataset for spoken language processing
in the Hebrew language. HebDB offers roughly 2500 hours of natural and
spontaneous speech recordings in the Hebrew language, consisting of a large
variety of speakers and topics. We provide raw recordings together with a
pre-processed, weakly supervised, and filtered version. The goal of HebDB is to
further enhance research and development of spoken language processing tools
for the Hebrew language. Hence, we additionally provide two baseline systems
for Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a
fully supervised model. We present the performance of these two methods
optimized on HebDB and compare them to current multi-lingual ASR alternatives.
Results suggest the proposed method reaches better results than the evaluated
baselines considering similar model sizes. Dataset, code, and models are
publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.","[{'name': 'Arnon Turetzky'}, {'name': 'Or Tal'}, {'name': 'Yael Segal-Feldman'}, {'name': 'Yehoshua Dissen'}, {'name': 'Ella Zeldes'}, {'name': 'Amit Roth'}, {'name': 'Eyal Cohen'}, {'name': 'Yosi Shrem'}, {'name': 'Bronya R. Chernyak'}, {'name': 'Olga Seleznova'}, {'name': 'Joseph Keshet'}, {'name': 'Yossi Adi'}]",2024-07-10T11:51:26Z
http://arxiv.org/abs/2407.07565v2,http://arxiv.org/abs/2407.07565v2,On Leakage of Code Generation Evaluation Datasets,"In this paper we consider contamination by code generation test sets, in
particular in their use in modern large language models. We discuss three
possible sources of such contamination and show findings supporting each of
them: (i) direct data leakage, (ii) indirect data leakage through the use of
synthetic data and (iii) overfitting to evaluation sets during model selection.
Key to our findings is a new dataset of 161 prompts with their associated
python solutions, dataset which is released at
https://huggingface.co/datasets/CohereForAI/lbpp .","[{'name': 'Alexandre Matton'}, {'name': 'Tom Sherborne'}, {'name': 'Dennis Aumiller'}, {'name': 'Elena Tommasone'}, {'name': 'Milad Alizadeh'}, {'name': 'Jingyi He'}, {'name': 'Raymond Ma'}, {'name': 'Maxime Voisin'}, {'name': 'Ellen Gilsenan-McMahon'}, {'name': 'Matthias Gallé'}]",2024-07-10T11:50:20Z
http://arxiv.org/abs/2407.12861v1,http://arxiv.org/abs/2407.12861v1,CiteME: Can Language Models Accurately Cite Scientific Claims?,"Thousands of new scientific papers are published each month. Such information
overload complicates researcher efforts to stay current with the
state-of-the-art as well as to verify and correctly attribute claims. We pose
the following research question: Given a text excerpt referencing a paper,
could an LM act as a research assistant to correctly identify the referenced
paper? We advance efforts to answer this question by building a benchmark that
evaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,
consists of text excerpts from recent machine learning papers, each referencing
a single other paper. CiteME use reveals a large gap between frontier LMs and
human performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.
We close this gap by introducing CiteAgent, an autonomous system built on the
GPT-4o LM that can also search and read papers, which achieves an accuracy of
35.3\% on CiteME. Overall, CiteME serves as a challenging testbed for
open-ended claim attribution, driving the research community towards a future
where any claim made by an LM can be automatically verified and discarded if
found to be incorrect.","[{'name': 'Ori Press'}, {'name': 'Andreas Hochlehnert'}, {'name': 'Ameya Prabhu'}, {'name': 'Vishaal Udandarao'}, {'name': 'Ofir Press'}, {'name': 'Matthias Bethge'}]",2024-07-10T11:31:20Z
http://arxiv.org/abs/2407.07551v1,http://arxiv.org/abs/2407.07551v1,Arabic Automatic Story Generation with Large Language Models,"Large language models (LLMs) have recently emerged as a powerful tool for a
wide range of language generation tasks. Nevertheless, this progress has been
slower in Arabic. In this work, we focus on the task of generating stories from
LLMs. For our training, we use stories acquired through machine translation
(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that
ensures we acquire high-quality stories. For our GPT-41 data, we introduce
crafted prompts that allow us to generate data well-suited to the Arabic
context in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian
and Moroccan). For example, we generate stories tailored to various Arab
countries on a wide host of topics. Our manual evaluation shows that our model
fine-tuned on these training datasets can generate coherent stories that adhere
to our instructions. We also conduct an extensive automatic and human
evaluation comparing our models against state-of-the-art proprietary and
open-source models. Our datasets and models will be made publicly available at
https: //github.com/UBC-NLP/arastories.","[{'name': 'Ahmed Oumar El-Shangiti'}, {'name': 'Fakhraddin Alwajih'}, {'name': 'Muhammad Abdul-Mageed'}]",2024-07-10T11:26:10Z
http://arxiv.org/abs/2407.11059v1,http://arxiv.org/abs/2407.11059v1,Was it Slander? Towards Exact Inversion of Generative Language Models,"Training large language models (LLMs) requires a substantial investment of
time and money. To get a good return on investment, the developers spend
considerable effort ensuring that the model never produces harmful and
offensive outputs. However, bad-faith actors may still try to slander the
reputation of an LLM by publicly reporting a forged output. In this paper, we
show that defending against such slander attacks requires reconstructing the
input of the forged output or proving that it does not exist. To do so, we
propose and evaluate a search based approach for targeted adversarial attacks
for LLMs. Our experiments show that we are rarely able to reconstruct the exact
input of an arbitrary output, thus demonstrating that LLMs are still vulnerable
to slander attacks.","[{'name': 'Adrians Skapars'}, {'name': 'Edoardo Manino'}, {'name': 'Youcheng Sun'}, {'name': 'Lucas C. Cordeiro'}]",2024-07-10T11:08:06Z
http://arxiv.org/abs/2407.07531v1,http://arxiv.org/abs/2407.07531v1,"Beyond Benchmarking: A New Paradigm for Evaluation and Assessment of
  Large Language Models","In current benchmarks for evaluating large language models (LLMs), there are
issues such as evaluation content restriction, untimely updates, and lack of
optimization guidance. In this paper, we propose a new paradigm for the
measurement of LLMs: Benchmarking-Evaluation-Assessment. Our paradigm shifts
the ""location"" of LLM evaluation from the ""examination room"" to the ""hospital"".
Through conducting a ""physical examination"" on LLMs, it utilizes specific
task-solving as the evaluation content, performs deep attribution of existing
problems within LLMs, and provides recommendation for optimization.","[{'name': 'Jin Liu'}, {'name': 'Qingquan Li'}, {'name': 'Wenlong Du'}]",2024-07-10T10:42:02Z
http://arxiv.org/abs/2407.07495v1,http://arxiv.org/abs/2407.07495v1,Bucket Pre-training is All You Need,"Large language models (LLMs) have demonstrated exceptional performance across
various natural language processing tasks. However, the conventional
fixed-length data composition strategy for pretraining, which involves
concatenating and splitting documents, can introduce noise and limit the
model's ability to capture long-range dependencies. To address this, we first
introduce three metrics for evaluating data composition quality: padding ratio,
truncation ratio, and concatenation ratio. We further propose a multi-bucket
data composition method that moves beyond the fixed-length paradigm, offering a
more flexible and efficient approach to pretraining. Extensive experiments
demonstrate that our proposed method could significantly improving both the
efficiency and efficacy of LLMs pretraining. Our approach not only reduces
noise and preserves context but also accelerates training, making it a
promising solution for LLMs pretraining.","[{'name': 'Hongtao Liu'}, {'name': 'Qiyao Peng'}, {'name': 'Qing Yang'}, {'name': 'Kai Liu'}, {'name': 'Hongyan Xu'}]",2024-07-10T09:27:23Z
http://arxiv.org/abs/2407.07487v1,http://arxiv.org/abs/2407.07487v1,"Review-LLM: Harnessing Large Language Models for Personalized Review
  Generation","Product review generation is an important task in recommender systems, which
could provide explanation and persuasiveness for the recommendation. Recently,
Large Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling
and generating ability, which could be applied in review generation. However,
directly applying the LLMs for generating reviews might be troubled by the
``polite'' phenomenon of the LLMs and could not generate personalized reviews
(e.g., negative reviews). In this paper, we propose Review-LLM that customizes
LLMs for personalized review generation. Firstly, we construct the prompt input
by aggregating user historical behaviors, which include corresponding item
titles and reviews. This enables the LLMs to capture user interest features and
review writing style. Secondly, we incorporate ratings as indicators of
satisfaction into the prompt, which could further improve the model's
understanding of user preferences and the sentiment tendency control of
generated reviews. Finally, we feed the prompt text into LLMs, and use
Supervised Fine-Tuning (SFT) to make the model generate personalized reviews
for the given user and target item. Experimental results on the real-world
dataset show that our fine-tuned model could achieve better review generation
performance than existing close-source LLMs.","[{'name': 'Qiyao Peng'}, {'name': 'Hongtao Liu'}, {'name': 'Hongyan Xu'}, {'name': 'Qing Yang'}, {'name': 'Minglai Shao'}, {'name': 'Wenjun Wang'}]",2024-07-10T09:22:19Z
http://arxiv.org/abs/2407.12860v1,http://arxiv.org/abs/2407.12860v1,"STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained
  LLMs","We present Simplified Text-Attributed Graph Embeddings (STAGE), a
straightforward yet effective method for enhancing node features in Graph
Neural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our
approach leverages Large-Language Models (LLMs) to generate embeddings for
textual attributes. STAGE achieves competitive results on various node
classification benchmarks while also maintaining a simplicity in implementation
relative to current state-of-the-art (SoTA) techniques. We show that utilizing
pre-trained LLMs as embedding generators provides robust features for ensemble
GNN training, enabling pipelines that are simpler than current SoTA approaches
which require multiple expensive training and prompting stages. We also
implement diffusion-pattern GNNs in an effort to make this pipeline scalable to
graphs beyond academic benchmarks.","[{'name': 'Aaron Zolnai-Lucas'}, {'name': 'Jack Boylan'}, {'name': 'Chris Hokamp'}, {'name': 'Parsa Ghaffari'}]",2024-07-10T08:50:25Z
http://arxiv.org/abs/2407.18930v1,http://arxiv.org/abs/2407.18930v1,"Dynamic Encoder Size Based on Data-Driven Layer-wise Pruning for Speech
  Recognition","Varying-size models are often required to deploy ASR systems under different
hardware and/or application constraints such as memory and latency. To avoid
redundant training and optimization efforts for individual models of different
sizes, we present the dynamic encoder size approach, which jointly trains
multiple performant models within one supernet from scratch. These subnets of
various sizes are layer-wise pruned from the supernet, and thus, enjoy full
parameter sharing. By combining score-based pruning with supernet training, we
propose two novel methods, Simple-Top-k and Iterative-Zero-Out, to
automatically select the best-performing subnets in a data-driven manner,
avoiding resource-intensive search efforts. Our experiments using CTC on both
Librispeech and TED-LIUM-v2 corpora show that our methods can achieve on-par
performance as individually trained models of each size category. Also, our
approach consistently brings small performance improvements for the full-size
supernet.","[{'name': 'Jingjing Xu'}, {'name': 'Wei Zhou'}, {'name': 'Zijian Yang'}, {'name': 'Eugen Beck'}, {'name': 'Ralf Schlueter'}]",2024-07-10T08:35:21Z
http://arxiv.org/abs/2407.07457v2,http://arxiv.org/abs/2407.07457v2,GLBench: A Comprehensive Benchmark for Graph with Large Language Models,"The emergence of large language models (LLMs) has revolutionized the way we
interact with graphs, leading to a new paradigm called GraphLLM. Despite the
rapid development of GraphLLM methods in recent years, the progress and
understanding of this field remain unclear due to the lack of a benchmark with
consistent experimental protocols. To bridge this gap, we introduce GLBench,
the first comprehensive benchmark for evaluating GraphLLM methods in both
supervised and zero-shot scenarios. GLBench provides a fair and thorough
evaluation of different categories of GraphLLM methods, along with traditional
baselines such as graph neural networks. Through extensive experiments on a
collection of real-world datasets with consistent data processing and splitting
strategies, we have uncovered several key findings. Firstly, GraphLLM methods
outperform traditional baselines in supervised settings, with LLM-as-enhancers
showing the most robust performance. However, using LLMs as predictors is less
effective and often leads to uncontrollable output issues. We also notice that
no clear scaling laws exist for current GraphLLM methods. In addition, both
structures and semantics are crucial for effective zero-shot transfer, and our
proposed simple baseline can even outperform several models tailored for
zero-shot scenarios. The data and code of the benchmark can be found at
https://github.com/NineAbyss/GLBench.","[{'name': 'Yuhan Li'}, {'name': 'Peisong Wang'}, {'name': 'Xiao Zhu'}, {'name': 'Aochuan Chen'}, {'name': 'Haiyun Jiang'}, {'name': 'Deng Cai'}, {'name': 'Victor Wai Kin Chan'}, {'name': 'Jia Li'}]",2024-07-10T08:20:47Z
http://arxiv.org/abs/2407.12859v1,http://arxiv.org/abs/2407.12859v1,"Automated Question Generation on Tabular Data for Conversational Data
  Exploration","Exploratory data analysis (EDA) is an essential step for analyzing a dataset
to derive insights. Several EDA techniques have been explored in the
literature. Many of them leverage visualizations through various plots. But it
is not easy to interpret them for a non-technical user, and producing
appropriate visualizations is also tough when there are a large number of
columns. Few other works provide a view of some interesting slices of data but
it is still difficult for the user to draw relevant insights from them. Of
late, conversational data exploration is gaining a lot of traction among
non-technical users. It helps the user to explore the dataset without having
deep technical knowledge about the data. Towards this, we propose a system that
recommends interesting questions in natural language based on relevant slices
of a dataset in a conversational setting. Specifically, given a dataset, we
pick a select set of interesting columns and identify interesting slices of
such columns and column combinations based on few interestingness measures. We
use our own fine-tuned variation of a pre-trained language model(T5) to
generate natural language questions in a specific manner. We then slot-fill
values in the generated questions and rank them for recommendations. We show
the utility of our proposed system in a coversational setting with a collection
of real datasets.","[{'name': 'Ritwik Chaudhuri'}, {'name': 'Rajmohan C'}, {'name': 'Kirushikesh DB'}, {'name': 'Arvind Agarwal'}]",2024-07-10T08:07:05Z
http://arxiv.org/abs/2407.17416v1,http://arxiv.org/abs/2407.17416v1,"Explaining Spectrograms in Machine Learning: A Study on Neural Networks
  for Speech Classification","This study investigates discriminative patterns learned by neural networks
for accurate speech classification, with a specific focus on vowel
classification tasks. By examining the activations and features of neural
networks for vowel classification, we gain insights into what the networks
""see"" in spectrograms. Through the use of class activation mapping, we identify
the frequencies that contribute to vowel classification and compare these
findings with linguistic knowledge. Experiments on a American English dataset
of vowels showcases the explainability of neural networks and provides valuable
insights into the causes of misclassifications and their characteristics when
differentiating them from unvoiced speech. This study not only enhances our
understanding of the underlying acoustic cues in vowel classification but also
offers opportunities for improving speech recognition by bridging the gap
between abstract representations in neural networks and established linguistic
knowledge","[{'name': 'Jesin James'}, {'name': 'Balamurali B. T.'}, {'name': 'Binu Abeysinghe'}, {'name': 'Junchen Liu'}]",2024-07-10T07:37:18Z
http://arxiv.org/abs/2407.07425v1,http://arxiv.org/abs/2407.07425v1,Out-of-distribution generalisation in spoken language understanding,"Test data is said to be out-of-distribution (OOD) when it unexpectedly
differs from the training data, a common challenge in real-world use cases of
machine learning. Although OOD generalisation has gained interest in recent
years, few works have focused on OOD generalisation in spoken language
understanding (SLU) tasks. To facilitate research on this topic, we introduce a
modified version of the popular SLU dataset SLURP, featuring data splits for
testing OOD generalisation in the SLU task. We call our modified dataset SLURP
For OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find
end-to-end SLU models to have limited capacity for generalisation. Furthermore,
by employing model interpretability techniques, we shed light on the factors
contributing to the generalisation difficulties of the models. To improve the
generalisation, we experiment with two techniques, which improve the results on
some, but not all the splits, emphasising the need for new techniques.","[{'name': 'Dejan Porjazovski'}, {'name': 'Anssi Moisio'}, {'name': 'Mikko Kurimo'}]",2024-07-10T07:27:38Z
http://arxiv.org/abs/2407.07931v1,http://arxiv.org/abs/2407.07931v1,"Search, Examine and Early-Termination: Fake News Detection with
  Annotation-Free Evidences","Pioneer researches recognize evidences as crucial elements in fake news
detection apart from patterns. Existing evidence-aware methods either require
laborious pre-processing procedures to assure relevant and high-quality
evidence data, or incorporate the entire spectrum of available evidences in all
news cases, regardless of the quality and quantity of the retrieved data. In
this paper, we propose an approach named \textbf{SEE} that retrieves useful
information from web-searched annotation-free evidences with an
early-termination mechanism. The proposed SEE is constructed by three main
phases: \textbf{S}earching online materials using the news as a query and
directly using their titles as evidences without any annotating or filtering
procedure, sequentially \textbf{E}xamining the news alongside with each piece
of evidence via attention mechanisms to produce new hidden states with
retrieved information, and allowing \textbf{E}arly-termination within the
examining loop by assessing whether there is adequate confidence for producing
a correct prediction. We have conducted extensive experiments on datasets with
unprocessed evidences, i.e., Weibo21, GossipCop, and pre-processed evidences,
namely Snopes and PolitiFact. The experimental results demonstrate that the
proposed method outperforms state-of-the-art approaches.","[{'name': 'Yuzhou Yang'}, {'name': 'Yangming Zhou'}, {'name': 'Qichao Ying'}, {'name': 'Zhenxing Qian'}, {'name': 'Xinpeng Zhang'}]",2024-07-10T07:22:30Z
http://arxiv.org/abs/2407.07413v1,http://arxiv.org/abs/2407.07413v1,KpopMT: Translation Dataset with Terminology for Kpop Fandom,"While machines learn from existing corpora, humans have the unique capability
to establish and accept new language systems. This makes human form unique
language systems within social groups. Aligning with this, we focus on a gap
remaining in addressing translation challenges within social groups, where
in-group members utilize unique terminologies. We propose KpopMT dataset, which
aims to fill this gap by enabling precise terminology translation, choosing
Kpop fandom as an initiative for social groups given its global popularity.
Expert translators provide 1k English translations for Korean posts and
comments, each annotated with specific terminology within social groups'
language systems. We evaluate existing translation systems including GPT models
on KpopMT to identify their failure cases. Results show overall low scores,
underscoring the challenges of reflecting group-specific terminologies and
styles in translation. We make KpopMT publicly available.","[{'name': 'JiWoo Kim'}, {'name': 'Yunsu Kim'}, {'name': 'JinYeong Bak'}]",2024-07-10T07:14:51Z
http://arxiv.org/abs/2407.07373v1,http://arxiv.org/abs/2407.07373v1,Automatic Extraction of Disease Risk Factors from Medical Publications,"We present a novel approach to automating the identification of risk factors
for diseases from medical literature, leveraging pre-trained models in the
bio-medical domain, while tuning them for the specific task. Faced with the
challenges of the diverse and unstructured nature of medical articles, our
study introduces a multi-step system to first identify relevant articles, then
classify them based on the presence of risk factor discussions and, finally,
extract specific risk factor information for a disease through a
question-answering model.
  Our contributions include the development of a comprehensive pipeline for the
automated extraction of risk factors and the compilation of several datasets,
which can serve as valuable resources for further research in this area. These
datasets encompass a wide range of diseases, as well as their associated risk
factors, meticulously identified and validated through a fine-grained
evaluation scheme. We conducted both automatic and thorough manual evaluation,
demonstrating encouraging results. We also highlight the importance of
improving models and expanding dataset comprehensiveness to keep pace with the
rapidly evolving field of medical research.","[{'name': 'Maxim Rubchinsky'}, {'name': 'Ella Rabinovich'}, {'name': 'Adi Shraibman'}, {'name': 'Netanel Golan'}, {'name': 'Tali Sahar'}, {'name': 'Dorit Shweiki'}]",2024-07-10T05:17:55Z
http://arxiv.org/abs/2407.07370v1,http://arxiv.org/abs/2407.07370v1,LokiLM: Technical Report,"In this work, we introduce LokiLM, a 1.4B parameter large language model
trained on 500B tokens. Our model performs strongly in natural language
reasoning tasks and achieves state-of-the-art performance among models with
1.5B parameters or less. LokiLM is trained using multi-teacher knowledge
distillation and high-quality training data to achieve benchmark results
competitive with larger models trained on significantly more tokens. We support
these findings by introducing steps to avoid benchmark contamination and
overfitting throughout our development process. Despite its promising
performance, LokiLM exhibits a concerning amount of hallucinations and scores
poorly on the TruthfulQA benchmark, so we do not release the model publicly.","[{'name': 'Justin Kiefel'}, {'name': 'Shrey Shah'}]",2024-07-10T05:05:47Z
http://arxiv.org/abs/2407.07342v1,http://arxiv.org/abs/2407.07342v1,"Multilingual Blending: LLM Safety Alignment Evaluation with Language
  Mixture","As safety remains a crucial concern throughout the development lifecycle of
Large Language Models (LLMs), researchers and industrial practitioners have
increasingly focused on safeguarding and aligning LLM behaviors with human
preferences and ethical standards. LLMs, trained on extensive multilingual
corpora, exhibit powerful generalization abilities across diverse languages and
domains. However, current safety alignment practices predominantly focus on
single-language scenarios, which leaves their effectiveness in complex
multilingual contexts, especially for those complex mixed-language formats,
largely unexplored. In this study, we introduce Multilingual Blending, a
mixed-language query-response scheme designed to evaluate the safety alignment
of various state-of-the-art LLMs (e.g., GPT-4o, GPT-3.5, Llama3) under
sophisticated, multilingual conditions. We further investigate language
patterns such as language availability, morphology, and language family that
could impact the effectiveness of Multilingual Blending in compromising the
safeguards of LLMs. Our experimental results show that, without meticulously
crafted prompt templates, Multilingual Blending significantly amplifies the
detriment of malicious queries, leading to dramatically increased bypass rates
in LLM safety alignment (67.23% on GPT-3.5 and 40.34% on GPT-4o), far exceeding
those of single-language baselines. Moreover, the performance of Multilingual
Blending varies notably based on intrinsic linguistic properties, with
languages of different morphology and from diverse families being more prone to
evading safety alignments. These findings underscore the necessity of
evaluating LLMs and developing corresponding safety alignment strategies in a
complex, multilingual context to align with their superior cross-language
generalization capabilities.","[{'name': 'Jiayang Song'}, {'name': 'Yuheng Huang'}, {'name': 'Zhehua Zhou'}, {'name': 'Lei Ma'}]",2024-07-10T03:26:15Z
http://arxiv.org/abs/2407.07341v1,http://arxiv.org/abs/2407.07341v1,"MixSumm: Topic-based Data Augmentation using LLMs for Low-resource
  Extractive Text Summarization","Low-resource extractive text summarization is a vital but heavily
underexplored area of research. Prior literature either focuses on abstractive
text summarization or prompts a large language model (LLM) like GPT-3 directly
to generate summaries. In this work, we propose MixSumm for low-resource
extractive text summarization. Specifically, MixSumm prompts an open-source
LLM, LLaMA-3-70b, to generate documents that mix information from multiple
topics as opposed to generating documents without mixup, and then trains a
summarization model on the generated dataset. We use ROUGE scores and L-Eval, a
reference-free LLaMA-3-based evaluation method to measure the quality of
generated summaries. We conduct extensive experiments on a challenging text
summarization benchmark comprising the TweetSumm, WikiHow, and ArXiv/PubMed
datasets and show that our LLM-based data augmentation framework outperforms
recent prompt-based approaches for low-resource extractive summarization.
Additionally, our results also demonstrate effective knowledge distillation
from LLaMA-3-70b to a small BERT-based extractive summarizer.","[{'name': 'Gaurav Sahu'}, {'name': 'Issam H. Laradji'}]",2024-07-10T03:25:47Z
http://arxiv.org/abs/2407.07330v1,http://arxiv.org/abs/2407.07330v1,"Interpretable Differential Diagnosis with Dual-Inference Large Language
  Models","Methodological advancements to automate the generation of differential
diagnosis (DDx) to predict a list of potential diseases as differentials given
patients' symptom descriptions are critical to clinical reasoning and
applications such as decision support. However, providing reasoning or
interpretation for these differential diagnoses is more meaningful.
Fortunately, large language models (LLMs) possess powerful language processing
abilities and have been proven effective in various related tasks. Motivated by
this potential, we investigate the use of LLMs for interpretable DDx. First, we
develop a new DDx dataset with expert-derived interpretation on 570 public
clinical notes. Second, we propose a novel framework, named Dual-Inf, that
enables LLMs to conduct bidirectional inference for interpretation. Both human
and automated evaluation demonstrate the effectiveness of Dual-Inf in
predicting differentials and diagnosis explanations. Specifically, the
performance improvement of Dual-Inf over the baseline methods exceeds 32%
w.r.t. BERTScore in DDx interpretation. Furthermore, experiments verify that
Dual-Inf (1) makes fewer errors in interpretation, (2) has great
generalizability, (3) is promising for rare disease diagnosis and explanation.","[{'name': 'Shuang Zhou'}, {'name': 'Sirui Ding'}, {'name': 'Jiashuo Wang'}, {'name': 'Mingquan Lin'}, {'name': 'Genevieve B. Melton'}, {'name': 'Rui Zhang'}]",2024-07-10T02:58:37Z
http://arxiv.org/abs/2407.07329v1,http://arxiv.org/abs/2407.07329v1,"Probability of Differentiation Reveals Brittleness of Homogeneity Bias
  in Large Language Models","Homogeneity bias in Large Language Models (LLMs) refers to their tendency to
homogenize the representations of some groups compared to others. Previous
studies documenting this bias have predominantly used encoder models, which may
have inadvertently introduced biases. To address this limitation, we prompted
GPT-4 to generate single word/expression completions associated with 18
situation cues - specific, measurable elements of environments that influence
how individuals perceive situations and compared the variability of these
completions using probability of differentiation. This approach directly
assessed homogeneity bias from the model's outputs, bypassing encoder models.
Across five studies, we find that homogeneity bias is highly volatile across
situation cues and writing prompts, suggesting that the bias observed in past
work may reflect those within encoder models rather than LLMs. Furthermore,
these results suggest that homogeneity bias in LLMs is brittle, as even minor
and arbitrary changes in prompts can significantly alter the expression of
biases. Future work should further explore how variations in syntactic features
and topic choices in longer text generations influence homogeneity bias in
LLMs.","[{'name': 'Messi H. J. Lee'}, {'name': 'Calvin K. Lai'}]",2024-07-10T02:56:55Z
http://arxiv.org/abs/2407.07325v2,http://arxiv.org/abs/2407.07325v2,HiLight: Technical Report on the Motern AI Video Language Model,"This technical report presents the implementation of a state-of-the-art video
encoder for video-text modal alignment and a video conversation framework
called HiLight, which features dual visual towers. The work is divided into two
main parts: 1.alignment of video and text modalities; 2.convenient and
efficient way to interact with users. Our goal is to address the task of video
comprehension in the context of billiards. The report includes a discussion of
the concepts and the final solution developed during the task's implementation.","[{'name': 'Zhiting Wang'}, {'name': 'Qiangong Zhou'}, {'name': 'Kangjie Yang'}, {'name': 'Zongyang Liu'}, {'name': 'Xin Mao'}]",2024-07-10T02:43:18Z
http://arxiv.org/abs/2407.07321v1,http://arxiv.org/abs/2407.07321v1,"RAG vs. Long Context: Examining Frontier Large Language Models for
  Environmental Review Document Comprehension","Large Language Models (LLMs) have been applied to many research problems
across various domains. One of the applications of LLMs is providing
question-answering systems that cater to users from different fields. The
effectiveness of LLM-based question-answering systems has already been
established at an acceptable level for users posing questions in popular and
public domains such as trivia and literature. However, it has not often been
established in niche domains that traditionally require specialized expertise.
To this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performance
of three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answering
questions originating from Environmental Impact Statements prepared by U.S.
federal government agencies in accordance with the National Environmental
Environmental Act (NEPA). We specifically measure the ability of LLMs to
understand the nuances of legal, technical, and compliance-related information
present in NEPA documents in different contextual scenarios. For example, we
test the LLMs' internal prior NEPA knowledge by providing questions without any
context, as well as assess how LLMs synthesize the contextual information
present in long NEPA documents to facilitate the question/answering task. We
compare the performance of the long context LLMs and RAG powered models in
handling different types of questions (e.g., problem-solving, divergent). Our
results suggest that RAG powered models significantly outperform the long
context models in the answer accuracy regardless of the choice of the frontier
LLM. Our further analysis reveals that many models perform better answering
closed questions than divergent and problem-solving questions.","[{'name': 'Hung Phan'}, {'name': 'Anurag Acharya'}, {'name': 'Sarthak Chaturvedi'}, {'name': 'Shivam Sharma'}, {'name': 'Mike Parker'}, {'name': 'Dan Nally'}, {'name': 'Ali Jannesari'}, {'name': 'Karl Pazdernik'}, {'name': 'Mahantesh Halappanavar'}, {'name': 'Sai Munikoti'}, {'name': 'Sameera Horawalavithana'}]",2024-07-10T02:33:09Z
http://arxiv.org/abs/2407.07313v1,http://arxiv.org/abs/2407.07313v1,"ESM+: Modern Insights into Perspective on Text-to-SQL Evaluation in the
  Age of Large Language Models","The task of Text-to-SQL enables anyone to retrieve information from SQL
databases using natural language. Despite several challenges, recent models
have made remarkable advancements in this task using large language models
(LLMs). Interestingly, we find that LLM-based models without fine-tuning
exhibit distinct natures compared to their fine-tuned counterparts, leading to
inadequacies in current evaluation metrics to accurately convey their
performance. Thus, we analyze the two primary metrics, Test Suite Execution
Accuracy (EXE) and Exact Set Matching Accuracy (ESM), to examine their
robustness for this task and address shortcomings. We compare the performance
of 9 LLM-based models using EXE, the original ESM, and our improved ESM (called
ESM+). Our results show that EXE and ESM have high false positive and negative
rates of 11.3% and 13.9%, while ESM+ gives those of 0.1% and 2.6% respectively,
providing a significantly more stable evaluation. We release the ESM+ script as
open-source for the community to contribute, while enjoying a more reliable
assessment of Text-to-SQL.","[{'name': 'Benjamin Ascoli'}, {'name': 'Ram Kandikonda'}, {'name': 'Jinho D. Choi'}]",2024-07-10T02:20:19Z
http://arxiv.org/abs/2407.07263v1,http://arxiv.org/abs/2407.07263v1,"Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language
  Models","As language models have scaled both their number of parameters and
pretraining dataset sizes, the computational cost for pretraining has become
intractable except for the most well-resourced teams. This increasing cost
makes it ever more important to be able to reuse a model after it has completed
pretraining; allowing for a model's abilities to further improve without
needing to train from scratch. In this work, we detail a set of guidelines that
cover how to design efficacious data distributions and learning rate schedules
for continued pretraining of language models. When applying these findings
within a continued pretraining run on top of a well-trained 15B parameter
model, we show an improvement of 9\% in average model accuracy compared to the
baseline of continued training on the pretraining set. The resulting recipe
provides a practical starting point with which to begin developing language
models through reuse rather than retraining.","[{'name': 'Jupinder Parmar'}, {'name': 'Sanjev Satheesh'}, {'name': 'Mostofa Patwary'}, {'name': 'Mohammad Shoeybi'}, {'name': 'Bryan Catanzaro'}]",2024-07-09T22:37:59Z
http://arxiv.org/abs/2407.07258v1,http://arxiv.org/abs/2407.07258v1,"Identification of emotions on Twitter during the 2022 electoral process
  in Colombia","The study of Twitter as a means for analyzing social phenomena has gained
interest in recent years due to the availability of large amounts of data in a
relatively spontaneous environment. Within opinion-mining tasks, emotion
detection is specially relevant, as it allows for the identification of
people's subjective responses to different social events in a more granular way
than traditional sentiment analysis based on polarity. In the particular case
of political events, the analysis of emotions in social networks can provide
valuable information on the perception of candidates, proposals, and other
important aspects of the public debate. In spite of this importance, there are
few studies on emotion detection in Spanish and, to the best of our knowledge,
few resources are public for opinion mining in Colombian Spanish, highlighting
the need for generating resources addressing the specific cultural
characteristics of this variety. In this work, we present a small corpus of
tweets in Spanish related to the 2022 Colombian presidential elections,
manually labeled with emotions using a fine-grained taxonomy. We perform
classification experiments using supervised state-of-the-art models (BERT
models) and compare them with GPT-3.5 in few-shot learning settings. We make
our dataset and code publicly available for research purposes.","[{'name': 'Juan Jose Iguaran Fernandez'}, {'name': 'Juan Manuel Perez'}, {'name': 'German Rosati'}]",2024-07-09T22:26:42Z
http://arxiv.org/abs/2407.07080v1,http://arxiv.org/abs/2407.07080v1,"Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary
  and Instruction Capabilities","Training large language models (LLMs) in low-resource languages such as
Hebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and
DictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a
substantial corpus of approximately 200 billion tokens in both Hebrew and
English. Adapting a pre-trained model to a new language involves specialized
techniques that differ significantly from training a model from scratch or
further training existing models on well-resourced languages such as English.
We outline these novel training methodologies, which facilitate effective
learning and adaptation to the linguistic properties of Hebrew. Additionally,
we fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to
enhance its performance on task-specific instructions. To rigorously evaluate
our models, we introduce a new benchmark suite for Hebrew LLM evaluation,
covering a diverse set of tasks including Question Answering, Sentiment
Analysis, Winograd Schema Challenge, Translation, and Summarization. Our work
not only addresses the intricacies of training LLMs in low-resource languages
but also proposes a framework that can be leveraged for adapting other LLMs to
various non-English languages, contributing to the broader field of
multilingual NLP.","[{'name': 'Shaltiel Shmidman'}, {'name': 'Avi Shmidman'}, {'name': 'Amir DN Cohen'}, {'name': 'Moshe Koppel'}]",2024-07-09T17:51:37Z
http://arxiv.org/abs/2407.07071v1,http://arxiv.org/abs/2407.07071v1,"Lookback Lens: Detecting and Mitigating Contextual Hallucinations in
  Large Language Models Using Only Attention Maps","When asked to summarize articles or answer questions given a passage, large
language models (LLMs) can hallucinate details and respond with unsubstantiated
answers that are inaccurate with respect to the input context. This paper
describes a simple approach for detecting such contextual hallucinations. We
hypothesize that contextual hallucinations are related to the extent to which
an LLM attends to information in the provided context versus its own
generations. Based on this intuition, we propose a simple hallucination
detection model whose input features are given by the ratio of attention
weights on the context versus newly generated tokens (for each attention head).
We find that a linear classifier based on these lookback ratio features is as
effective as a richer detector that utilizes the entire hidden states of an LLM
or a text-based entailment model. The lookback ratio-based detector -- Lookback
Lens -- is found to transfer across tasks and even models, allowing a detector
that is trained on a 7B model to be applied (without retraining) to a larger
13B model. We further apply this detector to mitigate contextual
hallucinations, and find that a simple classifier-guided decoding approach is
able to reduce the amount of hallucination, for example by 9.6% in the XSum
summarization task.","[{'name': 'Yung-Sung Chuang'}, {'name': 'Linlu Qiu'}, {'name': 'Cheng-Yu Hsieh'}, {'name': 'Ranjay Krishna'}, {'name': 'Yoon Kim'}, {'name': 'James Glass'}]",2024-07-09T17:44:34Z
http://arxiv.org/abs/2407.07019v1,http://arxiv.org/abs/2407.07019v1,"Using Large Language Models for Generating Smart Contracts for Health
  Insurance from Textual Policies","We explore using Large Language Models (LLMs) to generate application code
that automates health insurance processes from text-based policies. We target
blockchain-based smart contracts as they offer immutability, verifiability,
scalability, and a trustless setting: any number of parties can use the smart
contracts, and they need not have previously established trust relationships
with each other. Our methodology generates outputs at increasing levels of
technical detail: (1) textual summaries, (2) declarative decision logic, and
(3) smart contract code with unit tests. We ascertain LLMs are good at the task
(1), and the structured output is useful to validate tasks (2) and (3).
Declarative languages (task 2) are often used to formalize healthcare policies,
but their execution on blockchain is non-trivial. Hence, task (3) attempts to
directly automate the process using smart contracts. To assess the LLM output,
we propose completeness, soundness, clarity, syntax, and functioning code as
metrics. Our evaluation employs three health insurance policies (scenarios)
with increasing difficulty from Medicare's official booklet. Our evaluation
uses GPT-3.5 Turbo, GPT-3.5 Turbo 16K, GPT-4, GPT-4 Turbo and CodeLLaMA. Our
findings confirm that LLMs perform quite well in generating textual summaries.
Although outputs from tasks (2)-(3) are useful starting points, they require
human oversight: in multiple cases, even ""runnable"" code will not yield sound
results; the popularity of the target language affects the output quality; and
more complex scenarios still seem a bridge too far. Nevertheless, our
experiments demonstrate the promise of LLMs for translating textual process
descriptions into smart contracts.","[{'name': 'Inwon Kang'}, {'name': 'William Van Woensel'}, {'name': 'Oshani Seneviratne'}]",2024-07-09T16:40:44Z
http://arxiv.org/abs/2407.07018v1,http://arxiv.org/abs/2407.07018v1,"End-To-End Causal Effect Estimation from Unstructured Natural Language
  Data","Knowing the effect of an intervention is critical for human decision-making,
but current approaches for causal effect estimation rely on manual data
collection and structuring, regardless of the causal assumptions. This
increases both the cost and time-to-completion for studies. We show how large,
diverse observational text data can be mined with large language models (LLMs)
to produce inexpensive causal effect estimates under appropriate causal
assumptions. We introduce NATURAL, a novel family of causal effect estimators
built with LLMs that operate over datasets of unstructured text. Our estimators
use LLM conditional distributions (over variables of interest, given the text
data) to assist in the computation of classical estimators of causal effect. We
overcome a number of technical challenges to realize this idea, such as
automating data curation and using LLMs to impute missing information. We
prepare six (two synthetic and four real) observational datasets, paired with
corresponding ground truth in the form of randomized trials, which we used to
systematically evaluate each step of our pipeline. NATURAL estimators
demonstrate remarkable performance, yielding causal effect estimates that fall
within 3 percentage points of their ground truth counterparts, including on
real-world Phase 3/4 clinical trials. Our results suggest that unstructured
text data is a rich source of causal effect information, and NATURAL is a first
step towards an automated pipeline to tap this resource.","[{'name': 'Nikita Dhawan'}, {'name': 'Leonardo Cotta'}, {'name': 'Karen Ullrich'}, {'name': 'Rahul G. Krishnan'}, {'name': 'Chris J. Maddison'}]",2024-07-09T16:38:48Z
http://arxiv.org/abs/2407.07011v1,http://arxiv.org/abs/2407.07011v1,"Induction Heads as an Essential Mechanism for Pattern Matching in
  In-context Learning","Large language models (LLMs) have shown a remarkable ability to learn and
perform complex tasks through in-context learning (ICL). However, a
comprehensive understanding of its internal mechanisms is still lacking. This
paper explores the role of induction heads in a few-shot ICL setting. We
analyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract
pattern recognition and NLP tasks. Our results show that even a minimal
ablation of induction heads leads to ICL performance decreases of up to ~32%
for abstract pattern recognition tasks, bringing the performance close to
random. For NLP tasks, this ablation substantially decreases the model's
ability to benefit from examples, bringing few-shot ICL performance close to
that of zero-shot prompts. We further use attention knockout to disable
specific induction patterns, and present fine-grained evidence for the role
that the induction mechanism plays in ICL.","[{'name': 'J. Crosbie'}, {'name': 'E. Shutova'}]",2024-07-09T16:29:21Z
http://arxiv.org/abs/2407.07004v2,http://arxiv.org/abs/2407.07004v2,"Empirical analysis of Binding Precedent efficiency in the Brazilian
  Supreme Court via Similar Case Retrieval","Binding precedents (S\'umulas Vinculantes) constitute a juridical instrument
unique to the Brazilian legal system and whose objectives include the
protection of the Federal Supreme Court against repetitive demands. Studies of
the effectiveness of these instruments in decreasing the Court's exposure to
similar cases, however, indicate that they tend to fail in such a direction,
with some of the binding precedents seemingly creating new demands. We
empirically assess the legal impact of five binding precedents, 11, 14, 17, 26
and 37, at the highest court level through their effects on the legal subjects
they address. This analysis is only possible through the comparison of the
Court's ruling about the precedents' themes before they are created, which
means that these decisions should be detected through techniques of Similar
Case Retrieval. The contributions of this article are therefore twofold: on the
mathematical side, we compare the uses of different methods of Natural Language
Processing -- TF-IDF, LSTM, BERT, and regex -- for Similar Case Retrieval,
whereas on the legal side, we contrast the inefficiency of these binding
precedents with a set of hypotheses that may justify their repeated usage. We
observe that the deep learning models performed significantly worse in the
specific Similar Case Retrieval task and that the reasons for binding
precedents to fail in responding to repetitive demand are heterogeneous and
case-dependent, making it impossible to single out a specific cause.","[{'name': 'Raphaël Tinarrage'}, {'name': 'Henrique Ennes'}, {'name': 'Lucas E. Resck'}, {'name': 'Lucas T. Gomes'}, {'name': 'Jean R. Ponciano'}, {'name': 'Jorge Poco'}]",2024-07-09T16:17:16Z
http://arxiv.org/abs/2407.07000v1,http://arxiv.org/abs/2407.07000v1,"Metron: Holistic Performance Evaluation Framework for LLM Inference
  Systems","Serving large language models (LLMs) in production can incur substantial
costs, which has prompted recent advances in inference system optimizations.
Today, these systems are evaluated against conventional latency and throughput
metrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics
fail to fully capture the nuances of LLM inference, leading to an incomplete
assessment of user-facing performance crucial for real-time applications such
as chat and translation. In this paper, we first identify the pitfalls of
current performance metrics in evaluating LLM inference systems. We then
propose Metron, a comprehensive performance evaluation framework that includes
fluidity-index -- a novel metric designed to reflect the intricacies of the LLM
inference process and its impact on real-time user experience. Finally, we
evaluate various existing open-source platforms and model-as-a-service
offerings using Metron, discussing their strengths and weaknesses. Metron is
available at https://github.com/project-metron/metron.","[{'name': 'Amey Agrawal'}, {'name': 'Anmol Agarwal'}, {'name': 'Nitin Kedia'}, {'name': 'Jayashree Mohan'}, {'name': 'Souvik Kundu'}, {'name': 'Nipun Kwatra'}, {'name': 'Ramachandran Ramjee'}, {'name': 'Alexey Tumanov'}]",2024-07-09T16:13:26Z
http://arxiv.org/abs/2407.06992v1,http://arxiv.org/abs/2407.06992v1,"Robust Neural Information Retrieval: An Adversarial and
  Out-of-distribution Perspective","Recent advances in neural information retrieval (IR) models have
significantly enhanced their effectiveness over various IR tasks. The
robustness of these models, essential for ensuring their reliability in
practice, has also garnered significant attention. With a wide array of
research on robust IR being proposed, we believe it is the opportune moment to
consolidate the current status, glean insights from existing methodologies, and
lay the groundwork for future development. We view the robustness of IR to be a
multifaceted concept, emphasizing its necessity against adversarial attacks,
out-of-distribution (OOD) scenarios and performance variance. With a focus on
adversarial and OOD robustness, we dissect robustness solutions for dense
retrieval models (DRMs) and neural ranking models (NRMs), respectively,
recognizing them as pivotal components of the neural IR pipeline. We provide an
in-depth discussion of existing methods, datasets, and evaluation metrics,
shedding light on challenges and future directions in the era of large language
models. To the best of our knowledge, this is the first comprehensive survey on
the robustness of neural IR models, and we will also be giving our first
tutorial presentation at SIGIR 2024
\url{https://sigir2024-robust-information-retrieval.github.io}. Along with the
organization of existing work, we introduce a Benchmark for robust IR (BestIR),
a heterogeneous evaluation benchmark for robust neural information retrieval,
which is publicly available at \url{https://github.com/Davion-Liu/BestIR}. We
hope that this study provides useful clues for future research on the
robustness of IR models and helps to develop trustworthy search engines
\url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}.","[{'name': 'Yu-An Liu'}, {'name': 'Ruqing Zhang'}, {'name': 'Jiafeng Guo'}, {'name': 'Maarten de Rijke'}, {'name': 'Yixing Fan'}, {'name': 'Xueqi Cheng'}]",2024-07-09T16:07:01Z
http://arxiv.org/abs/2407.06990v1,http://arxiv.org/abs/2407.06990v1,Segment-Based Interactive Machine Translation for Pre-trained Models,"Pre-trained large language models (LLM) are starting to be widely used in
many applications. In this work, we explore the use of these models in
interactive machine translation (IMT) environments. In particular, we have
chosen mBART (multilingual Bidirectional and Auto-Regressive Transformer) and
mT5 (multilingual Text-to-Text Transfer Transformer) as the LLMs to perform our
experiments. The system generates perfect translations interactively using the
feedback provided by the user at each iteration. The Neural Machine Translation
(NMT) model generates a preliminary hypothesis with the feedback, and the user
validates new correct segments and performs a word correction--repeating the
process until the sentence is correctly translated. We compared the performance
of mBART, mT5, and a state-of-the-art (SoTA) machine translation model on a
benchmark dataset regarding user effort, Word Stroke Ratio (WSR), Key Stroke
Ratio (KSR), and Mouse Action Ratio (MAR). The experimental results indicate
that mBART performed comparably with SoTA models, suggesting that it is a
viable option for this field of IMT. The implications of this finding extend to
the development of new machine translation models for interactive environments,
as it indicates that some novel pre-trained models exhibit SoTA performance in
this domain, highlighting the potential benefits of adapting these models to
specific needs.","[{'name': 'Angel Navarro'}, {'name': 'Francisco Casacuberta'}]",2024-07-09T16:04:21Z
http://arxiv.org/abs/2407.06957v1,http://arxiv.org/abs/2407.06957v1,"Listen and Speak Fairly: A Study on Semantic Gender Bias in Speech
  Integrated Large Language Models","Speech Integrated Large Language Models (SILLMs) combine large language
models with speech perception to perform diverse tasks, such as emotion
recognition to speaker verification, demonstrating universal audio
understanding capability. However, these models may amplify biases present in
training data, potentially leading to biased access to information for
marginalized groups. This work introduces a curated spoken bias evaluation
toolkit and corresponding dataset. We evaluate gender bias in SILLMs across
four semantic-related tasks: speech-to-text translation (STT), spoken
coreference resolution (SCR), spoken sentence continuation (SSC), and spoken
question answering (SQA). Our analysis reveals that bias levels are
language-dependent and vary with different evaluation methods. Our findings
emphasize the necessity of employing multiple approaches to comprehensively
assess biases in SILLMs, providing insights for developing fairer SILLM
systems.","[{'name': 'Yi-Cheng Lin'}, {'name': 'Tzu-Quan Lin'}, {'name': 'Chih-Kai Yang'}, {'name': 'Ke-Han Lu'}, {'name': 'Wei-Chih Chen'}, {'name': 'Chun-Yi Kuan'}, {'name': 'Hung-yi Lee'}]",2024-07-09T15:35:43Z
http://arxiv.org/abs/2407.06955v1,http://arxiv.org/abs/2407.06955v1,"ICLGuard: Controlling In-Context Learning Behavior for Applicability
  Authorization","In-context learning (ICL) is a recent advancement in the capabilities of
large language models (LLMs). This feature allows users to perform a new task
without updating the model. Concretely, users can address tasks during the
inference time by conditioning on a few input-label pair demonstrations along
with the test input. It is different than the conventional fine-tuning paradigm
and offers more flexibility. However, this capability also introduces potential
issues. For example, users may use the model on any data without restriction,
such as performing tasks with improper or sensitive content, which might
violate the model policy or conflict with the model owner's interests. As a
model owner, it is crucial to establish a mechanism to control the model's
behavior under ICL, depending on the model owner's requirements for various
content. To this end, we introduce the concept of ""applicability authorization""
tailored for LLMs, particularly for ICL behavior, and propose a simple
approach, ICLGuard. It is a fine-tuning framework designed to allow the model
owner to regulate ICL behavior on different data. ICLGuard preserves the
original LLM and fine-tunes only a minimal set of additional trainable
parameters to ""guard"" the LLM. Empirical results show that the guarded LLM can
deactivate its ICL ability on target data without affecting its ICL ability on
other data and its general functionality across all data.","[{'name': 'Wai Man Si'}, {'name': 'Michael Backes'}, {'name': 'Yang Zhang'}]",2024-07-09T15:35:06Z
http://arxiv.org/abs/2407.06950v1,http://arxiv.org/abs/2407.06950v1,Spanish TrOCR: Leveraging Transfer Learning for Language Adaptation,"This study explores the transfer learning capabilities of the TrOCR
architecture to Spanish. TrOCR is a transformer-based Optical Character
Recognition (OCR) model renowned for its state-of-the-art performance in
English benchmarks. Inspired by Li et al. assertion regarding its adaptability
to multilingual text recognition, we investigate two distinct approaches to
adapt the model to a new language: integrating an English TrOCR encoder with a
language specific decoder and train the model on this specific language, and
fine-tuning the English base TrOCR model on a new language data. Due to the
scarcity of publicly available datasets, we present a resource-efficient
pipeline for creating OCR datasets in any language, along with a comprehensive
benchmark of the different image generation methods employed with a focus on
Visual Rich Documents (VRDs). Additionally, we offer a comparative analysis of
the two approaches for the Spanish language, demonstrating that fine-tuning the
English TrOCR on Spanish yields superior recognition than the language specific
decoder for a fixed dataset size. We evaluate our model employing character and
word error rate metrics on a public available printed dataset, comparing the
performance against other open-source and cloud OCR spanish models. As far as
we know, these resources represent the best open-source model for OCR in
Spanish. The Spanish TrOCR models are publicly available on HuggingFace [20]
and the code to generate the dataset is available on Github [25].","[{'name': 'Filipe Lauar'}, {'name': 'Valentin Laurent'}]",2024-07-09T15:31:41Z
http://arxiv.org/abs/2407.06946v1,http://arxiv.org/abs/2407.06946v1,Self-Recognition in Language Models,"A rapidly growing number of applications rely on a small set of closed-source
language models (LMs). This dependency might introduce novel security risks if
LMs develop self-recognition capabilities. Inspired by human identity
verification methods, we propose a novel approach for assessing
self-recognition in LMs using model-generated ""security questions"". Our test
can be externally administered to keep track of frontier models as it does not
require access to internal model parameters or output probabilities. We use our
test to examine self-recognition in ten of the most capable open- and
closed-source LMs currently publicly available. Our extensive experiments found
no empirical evidence of general or consistent self-recognition in any examined
LM. Instead, our results suggest that given a set of alternatives, LMs seek to
pick the ""best"" answer, regardless of its origin. Moreover, we find indications
that preferences about which models produce the best answers are consistent
across LMs. We additionally uncover novel insights on position bias
considerations for LMs in multiple-choice settings.","[{'name': 'Tim R. Davidson'}, {'name': 'Viacheslav Surkov'}, {'name': 'Veniamin Veselovsky'}, {'name': 'Giuseppe Russo'}, {'name': 'Robert West'}, {'name': 'Caglar Gulcehre'}]",2024-07-09T15:23:28Z
http://arxiv.org/abs/2407.06941v1,http://arxiv.org/abs/2407.06941v1,Raply: A profanity-mitigated rap generator,"The task of writing rap is challenging and involves producing complex rhyming
schemes, yet meaningful lyrics. In this work, we propose Raply, a fine-tuned
GPT-2 model capable of producing meaningful rhyming text in the style of rap.
In addition to its rhyming capabilities, the model is able to generate less
offensive content. It was achieved through the fine-tuning the model on a new
dataset Mitislurs, a profanity-mitigated corpus. We evaluate the output of the
model on two criteria: 1) rhyming based on the rhyme density metric; 2)
profanity content, using the list of profanities for the English language. To
our knowledge, this is the first attempt at profanity mitigation for rap lyrics
generation.","[{'name': 'Omar Manil Bendali'}, {'name': 'Samir Ferroum'}, {'name': 'Ekaterina Kozachenko'}, {'name': 'Youssef Parviz'}, {'name': 'Hanna Shcharbakova'}, {'name': 'Anna Tokareva'}, {'name': 'Shemair Williams'}]",2024-07-09T15:18:56Z
http://arxiv.org/abs/2407.06908v1,http://arxiv.org/abs/2407.06908v1,"Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion
  Representation of Religion in Large Language Models","Emotions play important epistemological and cognitive roles in our lives,
revealing our values and guiding our actions. Previous work has shown that LLMs
display biases in emotion attribution along gender lines. However, unlike
gender, which says little about our values, religion, as a socio-cultural
system, prescribes a set of beliefs and values for its followers. Religions,
therefore, cultivate certain emotions. Moreover, these rules are explicitly
laid out and interpreted by religious leaders. Using emotion attribution, we
explore how different religions are represented in LLMs. We find that: Major
religions in the US and European countries are represented with more nuance,
displaying a more shaded model of their beliefs. Eastern religions like
Hinduism and Buddhism are strongly stereotyped. Judaism and Islam are
stigmatized -- the models' refusal skyrocket. We ascribe these to cultural bias
in LLMs and the scarcity of NLP literature on religion. In the rare instances
where religion is discussed, it is often in the context of toxic language,
perpetuating the perception of these religions as inherently toxic. This
finding underscores the urgent need to address and rectify these biases. Our
research underscores the crucial role emotions play in our lives and how our
values influence them.","[{'name': 'Flor Miriam Plaza-del-Arco'}, {'name': 'Amanda Cercas Curry'}, {'name': 'Susanna Paoli'}, {'name': 'Alba Curry'}, {'name': 'Dirk Hovy'}]",2024-07-09T14:45:15Z
http://arxiv.org/abs/2407.06893v1,http://arxiv.org/abs/2407.06893v1,"Measuring Sustainability Intention of ESG Fund Disclosure using Few-Shot
  Learning","Global sustainable fund universe encompasses open-end funds and
exchange-traded funds (ETF) that, by prospectus or other regulatory filings,
claim to focus on Environment, Social and Governance (ESG). Challengingly, the
claims can only be confirmed by examining the textual disclosures to check if
there is presence of intentionality and ESG focus on its investment strategy.
Currently, there is no regulation to enforce sustainability in ESG products
space. This paper proposes a unique method and system to classify and score the
fund prospectuses in the sustainable universe regarding specificity and
transparency of language. We aim to employ few-shot learners to identify
specific, ambiguous, and generic sustainable investment-related language.
Additionally, we construct a ratio metric to determine language score and
rating to rank products and quantify sustainability claims for US sustainable
universe. As a by-product, we publish manually annotated quality training
dataset on Hugging Face (ESG-Prospectus-Clarity-Category under cc-by-nc-sa-4.0)
of more than 1K ESG textual statements. The performance of the few-shot
finetuning approach is compared with zero-shot models e.g., Llama-13B, GPT 3.5
Turbo etc. We found that prompting large language models are not accurate for
domain specific tasks due to misalignment issues. The few-shot finetuning
techniques outperform zero-shot models by large margins of more than absolute
~30% in precision, recall and F1 metrics on completely unseen ESG languages
(test set). Overall, the paper attempts to establish a systematic and scalable
approach to measure and rate sustainability intention quantitatively for
sustainable funds using texts in prospectus. Regulatory bodies, investors, and
advisors may utilize the findings of this research to reduce cognitive load in
investigating or screening of ESG funds which accurately reflects the ESG
intention.","[{'name': 'Mayank Singh'}, {'name': 'Nazia Nafis'}, {'name': 'Abhijeet Kumar'}, {'name': 'Mridul Mishra'}]",2024-07-09T14:25:23Z
http://arxiv.org/abs/2407.12856v1,http://arxiv.org/abs/2407.12856v1,AI AI Bias: Large Language Models Favor Their Own Generated Content,"Are large language models (LLMs) biased towards text generated by LLMs over
text authored by humans, leading to possible anti-human bias? Utilizing a
classical experimental design inspired by employment discrimination studies, we
tested widely-used LLMs, including GPT-3.5 and GPT4, in binary-choice
scenarios. These involved LLM-based agents selecting between products and
academic papers described either by humans or LLMs under identical conditions.
Our results show a consistent tendency for LLM-based AIs to prefer
LLM-generated content. This suggests the possibility of AI systems implicitly
discriminating against humans, giving AI agents an unfair advantage.","[{'name': 'Walter Laurito'}, {'name': 'Benjamin Davis'}, {'name': 'Peli Grietzer'}, {'name': 'Tomáš Gavenčiak'}, {'name': 'Ada Böhm'}, {'name': 'Jan Kulveit'}]",2024-07-09T13:15:14Z
http://arxiv.org/abs/2407.06800v2,http://arxiv.org/abs/2407.06800v2,Learn and Don't Forget: Adding a New Language to ASR Foundation Models,"Foundation ASR models often support many languages, e.g. 100 languages in
Whisper. However, there has been limited work on integrating an additional,
typically low-resource, language, while maintaining performance on the original
language set. Fine-tuning, while simple, may degrade the accuracy of the
original set. We compare three approaches that exploit adaptation parameters:
soft language code tuning, train only the language code; soft prompt tuning,
train prepended tokens; and LoRA where a small set of additional parameters are
optimised. Elastic Weight Consolidation (EWC) offers an alternative compromise
with the potential to maintain performance in specific target languages.
Results show that direct fine-tuning yields the best performance for the new
language but degrades existing language capabilities. EWC can address this
issue for specific languages. If only adaptation parameters are used, the
language capabilities are maintained but at the cost of performance in the new
language.","[{'name': 'Mengjie Qian'}, {'name': 'Siyuan Tang'}, {'name': 'Rao Ma'}, {'name': 'Kate M. Knill'}, {'name': 'Mark J. F. Gales'}]",2024-07-09T12:14:48Z
http://arxiv.org/abs/2407.17425v1,http://arxiv.org/abs/2407.17425v1,"Media Manipulations in the Coverage of Events of the Ukrainian
  Revolution of Dignity: Historical, Linguistic, and Psychological Approaches","This article examines the use of manipulation in the coverage of events of
the Ukrainian Revolution of Dignity in the mass media, namely in the content of
the online newspaper Ukrainian Truth (Ukrainska pravda), online newspaper High
Castle (Vysokyi Zamok), and online newspaper ZIK during the public protest,
namely during the Ukrainian Revolution of Dignity. Contents of these online
newspapers the historical, linguistic, and psychological approaches are used.
Also media manipulations in the coverage of events of the Ukrainian Revolution
of Dignity are studied. Internet resources that cover news are analyzed.
Current and most popular Internet resources are identified. The content of
online newspapers is analyzed and statistically processed. Internet content of
newspapers by the level of significance of data (very significant data,
significant data and insignificant data) is classified. The algorithm of
detection of the media manipulations in the highlighting the course of the
Ukrainian revolutions based on historical, linguistic, and psychological
approaches is designed. Methods of counteracting information attacks in online
newspapers are developed.","[{'name': 'Ivan Khoma'}, {'name': 'Solomia Fedushko'}, {'name': 'Zoryana Kunch'}]",2024-07-09T09:46:27Z
http://arxiv.org/abs/2407.06699v1,http://arxiv.org/abs/2407.06699v1,Consistent Document-Level Relation Extraction via Counterfactuals,"Many datasets have been developed to train and evaluate document-level
relation extraction (RE) models. Most of these are constructed using real-world
data. It has been shown that RE models trained on real-world data suffer from
factual biases. To evaluate and address this issue, we present CovEReD, a
counterfactual data generation approach for document-level relation extraction
datasets using entity replacement. We first demonstrate that models trained on
factual data exhibit inconsistent behavior: while they accurately extract
triples from factual data, they fail to extract the same triples after
counterfactual modification. This inconsistency suggests that models trained on
factual data rely on spurious signals such as specific entities and external
knowledge $\unicode{x2013}$ rather than on the input context $\unicode{x2013}$
to extract triples. We show that by generating document-level counterfactual
data with CovEReD and training models on them, consistency is maintained with
minimal impact on RE performance. We release our CovEReD pipeline as well as
Re-DocRED-CF, a dataset of counterfactual RE documents, to assist in evaluating
and addressing inconsistency in document-level RE.","[{'name': 'Ali Modarressi'}, {'name': 'Abdullatif Köksal'}, {'name': 'Hinrich Schütze'}]",2024-07-09T09:21:55Z
http://arxiv.org/abs/2407.06677v1,http://arxiv.org/abs/2407.06677v1,"Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of
  Modules","Is it always necessary to compute tokens from shallow to deep layers in
Transformers? The continued success of vanilla Transformers and their variants
suggests an undoubted ""yes"". In this work, however, we attempt to break the
depth-ordered convention by proposing a novel architecture dubbed
mixture-of-modules (MoM), which is motivated by an intuition that any layer,
regardless of its position, can be used to compute a token as long as it
possesses the needed processing capabilities. The construction of MoM starts
from a finite set of modules defined by multi-head attention and feed-forward
networks, each distinguished by its unique parameterization. Two routers then
iteratively select attention modules and feed-forward modules from the set to
process a token. The selection dynamically expands the computation graph in the
forward pass of the token, culminating in an assembly of modules. We show that
MoM provides not only a unified framework for Transformers and their numerous
variants but also a flexible and learnable approach for reducing redundancy in
Transformer parameterization. We pre-train various MoMs using OpenWebText.
Empirical results demonstrate that MoMs, of different parameter counts,
consistently outperform vanilla transformers on both GLUE and XSUM benchmarks.
More interestingly, with a fixed parameter budget, MoM-large enables an over
38% increase in depth for computation graphs compared to GPT-2-large, resulting
in absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large
also enables an over 60% reduction in depth while involving more modules per
layer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage
compared to GPT-2-large, while maintaining comparable performance.","[{'name': 'Zhuocheng Gong'}, {'name': 'Ang Lv'}, {'name': 'Jian Guan'}, {'name': 'Junxi Yan'}, {'name': 'Wei Wu'}, {'name': 'Huishuai Zhang'}, {'name': 'Minlie Huang'}, {'name': 'Dongyan Zhao'}, {'name': 'Rui Yan'}]",2024-07-09T08:50:18Z
http://arxiv.org/abs/2407.12854v1,http://arxiv.org/abs/2407.12854v1,Scaling Retrieval-Based Language Models with a Trillion-Token Datastore,"Scaling laws with respect to the amount of training data and the number of
parameters allow us to predict the cost-benefit trade-offs of pretraining
language models (LMs) in different configurations. In this paper, we consider
another dimension of scaling: the amount of data available at inference time.
Specifically, we find that increasing the size of the datastore used by a
retrieval-based LM monotonically improves language modeling and several
downstream tasks without obvious saturation, such that a smaller model
augmented with a large datastore outperforms a larger LM-only model on
knowledge-intensive tasks. By plotting compute-optimal scaling curves with
varied datastore, model, and pretraining data sizes, we show that using larger
datastores can significantly improve model performance for the same training
compute budget. We carry out our study by constructing a 1.4 trillion-token
datastore named MassiveDS, which is the largest and the most diverse
open-sourced datastore for retrieval-based LMs to date, and designing an
efficient pipeline for studying datastore scaling in a computationally
accessible manner. Finally, we analyze the effect of improving the retriever,
datastore quality filtering, and other design choices on our observed scaling
trends. Overall, our results show that datastore size should be considered as
an integral part of LM efficiency and performance trade-offs. To facilitate
future research, we open-source our datastore and code at
https://github.com/RulinShao/retrieval-scaling.","[{'name': 'Rulin Shao'}, {'name': 'Jacqueline He'}, {'name': 'Akari Asai'}, {'name': 'Weijia Shi'}, {'name': 'Tim Dettmers'}, {'name': 'Sewon Min'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Pang Wei Koh'}]",2024-07-09T08:27:27Z
http://arxiv.org/abs/2407.06654v1,http://arxiv.org/abs/2407.06654v1,"SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language
  Model Pre-training","The effectiveness of large language models (LLMs) is often hindered by
duplicated data in their extensive pre-training datasets. Current approaches
primarily focus on detecting and removing duplicates, which risks the loss of
valuable information and neglects the varying degrees of duplication. To
address this, we propose a soft deduplication method that maintains dataset
integrity while selectively reducing the sampling weight of data with high
commonness. Central to our approach is the concept of ""data commonness"", a
metric we introduce to quantify the degree of duplication by measuring the
occurrence probabilities of samples using an n-gram model. Empirical analysis
shows that this method significantly improves training efficiency, achieving
comparable perplexity scores with at least a 26% reduction in required training
steps. Additionally, it enhances average few-shot downstream accuracy by 1.77%
when trained for an equivalent duration. Importantly, this approach
consistently improves performance, even on rigorously deduplicated datasets,
indicating its potential to complement existing methods and become a standard
pre-training process for LLMs.","[{'name': 'Nan He'}, {'name': 'Weichen Xiong'}, {'name': 'Hanwen Liu'}, {'name': 'Yi Liao'}, {'name': 'Lei Ding'}, {'name': 'Kai Zhang'}, {'name': 'Guohua Tang'}, {'name': 'Xiao Han'}, {'name': 'Wei Yang'}]",2024-07-09T08:26:39Z
http://arxiv.org/abs/2407.06650v1,http://arxiv.org/abs/2407.06650v1,"A Word Order Synchronization Metric for Evaluating Simultaneous
  Interpretation and Translation","Simultaneous interpretation (SI), the translation of one language to another
in real time, starts translation before the original speech has finished. Its
evaluation needs to consider both latency and quality. This trade-off is
challenging especially for distant word order language pairs such as English
and Japanese. To handle this word order gap, interpreters maintain the word
order of the source language as much as possible to keep up with original
language to minimize its latency while maintaining its quality, whereas in
translation reordering happens to keep fluency in the target language. This
means outputs synchronized with the source language are desirable based on the
real SI situation, and it's a key for further progress in computational SI and
simultaneous machine translation (SiMT). In this work, we propose an automatic
evaluation metric for SI and SiMT focusing on word order synchronization. Our
evaluation metric is based on rank correlation coefficients, leveraging
cross-lingual pre-trained language models. Our experimental results on
NAIST-SIC-Aligned and JNPC showed our metrics' effectiveness to measure word
order synchronization between source and target language.","[{'name': 'Mana Makinae'}, {'name': 'Katsuhito Sudoh'}, {'name': 'Mararu Yamada'}, {'name': 'Satoshi Nakamura'}]",2024-07-09T08:21:40Z
http://arxiv.org/abs/2407.06579v1,http://arxiv.org/abs/2407.06579v1,"NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in
  Text Classification","Existing research on learning with noisy labels predominantly focuses on
synthetic label noise. Although synthetic noise possesses well-defined
structural properties, it often fails to accurately replicate real-world noise
patterns. In recent years, there has been a concerted effort to construct
generalizable and controllable instance-dependent noise datasets for image
classification, significantly advancing the development of noise-robust
learning in this area. However, studies on noisy label learning for text
classification remain scarce. To better understand label noise in real-world
text classification settings, we constructed the benchmark dataset NoisyAG-News
through manual annotation. Initially, we analyzed the annotated data to gather
observations about real-world noise. We qualitatively and quantitatively
demonstrated that real-world noisy labels adhere to instance-dependent
patterns. Subsequently, we conducted comprehensive learning experiments on
NoisyAG-News and its corresponding synthetic noise datasets using pre-trained
language models and noise-handling techniques. Our findings reveal that while
pre-trained models are resilient to synthetic noise, they struggle against
instance-dependent noise, with samples of varying confusion levels showing
inconsistent performance during training and testing. These real-world noise
patterns pose new, significant challenges, prompting a reevaluation of noisy
label handling methods. We hope that NoisyAG-News will facilitate the
development and evaluation of future solutions for learning with noisy labels.","[{'name': 'Hongfei Huang'}, {'name': 'Tingting Liang'}, {'name': 'Xixi Sun'}, {'name': 'Zikang Jin'}, {'name': 'Yuyu Yin'}]",2024-07-09T06:18:40Z
http://arxiv.org/abs/2407.06576v1,http://arxiv.org/abs/2407.06576v1,Virtual Personas for Language Models via an Anthology of Backstories,"Large language models (LLMs) are trained from vast repositories of text
authored by millions of distinct authors, reflecting an enormous diversity of
human traits. While these models bear the potential to be used as
approximations of human subjects in behavioral studies, prior efforts have been
limited in steering model responses to match individual human users. In this
work, we introduce ""Anthology"", a method for conditioning LLMs to particular
virtual personas by harnessing open-ended life narratives, which we refer to as
""backstories."" We show that our methodology enhances the consistency and
reliability of experimental outcomes while ensuring better representation of
diverse sub-populations. Across three nationally representative human surveys
conducted as part of Pew Research Center's American Trends Panel (ATP), we
demonstrate that Anthology achieves up to 18% improvement in matching the
response distributions of human respondents and 27% improvement in consistency
metrics. Our code and generated backstories are available at
https://github.com/CannyLab/anthology.","[{'name': 'Suhong Moon'}, {'name': 'Marwa Abdulhai'}, {'name': 'Minwoo Kang'}, {'name': 'Joseph Suh'}, {'name': 'Widyadewi Soedarmadji'}, {'name': 'Eran Kohen Behar'}, {'name': 'David M. Chan'}]",2024-07-09T06:11:18Z
http://arxiv.org/abs/2407.06567v2,http://arxiv.org/abs/2407.06567v2,"FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal
  Reinforcement for Enhanced Financial Decision Making","Large language models (LLMs) have demonstrated notable potential in
conducting complex tasks and are increasingly utilized in various financial
applications. However, high-quality sequential financial investment
decision-making remains challenging. These tasks require multiple interactions
with a volatile environment for every decision, demanding sufficient
intelligence to maximize returns and manage risks. Although LLMs have been used
to develop agent systems that surpass human teams and yield impressive
investment returns, opportunities to enhance multi-sourced information
synthesis and optimize decision-making outcomes through timely experience
refinement remain unexplored. Here, we introduce the FinCon, an LLM-based
multi-agent framework with CONceptual verbal reinforcement tailored for diverse
FINancial tasks. Inspired by effective real-world investment firm
organizational structures, FinCon utilizes a manager-analyst communication
hierarchy. This structure allows for synchronized cross-functional agent
collaboration towards unified goals through natural language interactions and
equips each agent with greater memory capacity than humans. Additionally, a
risk-control component in FinCon enhances decision quality by episodically
initiating a self-critiquing mechanism to update systematic investment beliefs.
The conceptualized beliefs serve as verbal reinforcement for the future agent's
behavior and can be selectively propagated to the appropriate node that
requires knowledge updates. This feature significantly improves performance
while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon
demonstrates strong generalization capabilities in various financial tasks,
including single stock trading and portfolio management.","[{'name': 'Yangyang Yu'}, {'name': 'Zhiyuan Yao'}, {'name': 'Haohang Li'}, {'name': 'Zhiyang Deng'}, {'name': 'Yupeng Cao'}, {'name': 'Zhi Chen'}, {'name': 'Jordan W. Suchow'}, {'name': 'Rong Liu'}, {'name': 'Zhenyu Cui'}, {'name': 'Denghui Zhang'}, {'name': 'Koduvayur Subbalakshmi'}, {'name': 'Guojun Xiong'}, {'name': 'Yueru He'}, {'name': 'Jimin Huang'}, {'name': 'Dong Li'}, {'name': 'Qianqian Xie'}]",2024-07-09T05:52:26Z
http://arxiv.org/abs/2407.06564v1,http://arxiv.org/abs/2407.06564v1,Combining Knowledge Graphs and Large Language Models,"In recent years, Natural Language Processing (NLP) has played a significant
role in various Artificial Intelligence (AI) applications such as chatbots,
text generation, and language translation. The emergence of large language
models (LLMs) has greatly improved the performance of these applications,
showing astonishing results in language understanding and generation. However,
they still show some disadvantages, such as hallucinations and lack of
domain-specific knowledge, that affect their performance in real-world tasks.
These issues can be effectively mitigated by incorporating knowledge graphs
(KGs), which organise information in structured formats that capture
relationships between entities in a versatile and interpretable fashion.
Likewise, the construction and validation of KGs present challenges that LLMs
can help resolve. The complementary relationship between LLMs and KGs has led
to a trend that combines these technologies to achieve trustworthy results.
This work collected 28 papers outlining methods for KG-powered LLMs, LLM-based
KGs, and LLM-KG hybrid approaches. We systematically analysed and compared
these approaches to provide a comprehensive overview highlighting key trends,
innovative techniques, and common challenges. This synthesis will benefit
researchers new to the field and those seeking to deepen their understanding of
how KGs and LLMs can be effectively combined to enhance AI applications
capabilities.","[{'name': 'Amanda Kau'}, {'name': 'Xuzeng He'}, {'name': 'Aishwarya Nambissan'}, {'name': 'Aland Astudillo'}, {'name': 'Hui Yin'}, {'name': 'Amir Aryani'}]",2024-07-09T05:42:53Z
http://arxiv.org/abs/2407.06551v1,http://arxiv.org/abs/2407.06551v1,OffsetBias: Leveraging Debiased Data for Tuning Evaluators,"Employing Large Language Models (LLMs) to assess the quality of generated
responses, such as prompting instruct-tuned models or fine-tuning judge models,
has become a widely adopted evaluation method. It is also known that such
evaluators are vulnerable to biases, such as favoring longer responses. While
it is important to overcome this problem, the specifics of these biases remain
under-explored. In this work, we qualitatively identify six types of biases
inherent in various judge models. We propose EvalBiasBench as a meta-evaluation
collection of hand-crafted test cases for each bias type. Additionally, we
present de-biasing dataset construction methods and the associated preference
dataset OffsetBias. Experimental results demonstrate that fine-tuning on our
dataset significantly enhances the robustness of judge models against biases
and improves performance across most evaluation scenarios. We release our
datasets and the fine-tuned judge model to public.","[{'name': 'Junsoo Park'}, {'name': 'Seungyeon Jwa'}, {'name': 'Meiying Ren'}, {'name': 'Daeyoung Kim'}, {'name': 'Sanghyuk Choi'}]",2024-07-09T05:16:22Z
http://arxiv.org/abs/2407.06549v1,http://arxiv.org/abs/2407.06549v1,"AutoTask: Task Aware Multi-Faceted Single Model for Multi-Task Ads
  Relevance","Ads relevance models are crucial in determining the relevance between user
search queries and ad offers, often framed as a classification problem. The
complexity of modeling increases significantly with multiple ad types and
varying scenarios that exhibit both similarities and differences. In this work,
we introduce a novel multi-faceted attention model that performs task aware
feature combination and cross task interaction modeling. Our technique
formulates the feature combination problem as ""language"" modeling with
auto-regressive attentions across both feature and task dimensions.
Specifically, we introduce a new dimension of task ID encoding for task
representations, thereby enabling precise relevance modeling across diverse ad
scenarios with substantial improvement in generality capability for unseen
tasks. We demonstrate that our model not only effectively handles the increased
computational and maintenance demands as scenarios proliferate, but also
outperforms generalized DNN models and even task-specific models across a
spectrum of ad applications using a single unified model.","[{'name': 'Shouchang Guo'}, {'name': 'Sonam Damani'}, {'name': 'Keng-hao Chang'}]",2024-07-09T05:13:45Z
http://arxiv.org/abs/2407.06547v1,http://arxiv.org/abs/2407.06547v1,Deciphering Assamese Vowel Harmony with Featural InfoWaveGAN,"Traditional approaches for understanding phonological learning have
predominantly relied on curated text data. Although insightful, such approaches
limit the knowledge captured in textual representations of the spoken language.
To overcome this limitation, we investigate the potential of the Featural
InfoWaveGAN model to learn iterative long-distance vowel harmony using raw
speech data. We focus on Assamese, a language known for its phonologically
regressive and word-bound vowel harmony. We demonstrate that the model is adept
at grasping the intricacies of Assamese phonotactics, particularly iterative
long-distance harmony with regressive directionality. It also produced
non-iterative illicit forms resembling speech errors during human language
acquisition. Our statistical analysis reveals a preference for a specific
[+high,+ATR] vowel as a trigger across novel items, indicative of feature
learning. More data and control could improve model proficiency, contrasting
the universality of learning.","[{'name': 'Sneha Ray Barman'}, {'name': 'Shakuntala Mahanta'}, {'name': 'Neeraj Kumar Sharma'}]",2024-07-09T05:01:13Z
http://arxiv.org/abs/2407.06542v1,http://arxiv.org/abs/2407.06542v1,LIONs: An Empirically Optimized Approach to Align Language Models,"Alignment is a crucial step to enhance the instruction-following and
conversational abilities of language models. Despite many recent work proposing
new algorithms, datasets, and training pipelines, there is a lack of
comprehensive studies measuring the impact of various design choices throughout
the whole training process. We first conduct a rigorous analysis over a
three-stage training pipeline consisting of supervised fine-tuning, offline
preference learning, and online preference learning. We have found that using
techniques like sequence packing, loss masking in SFT, increasing the
preference dataset size in DPO, and online DPO training can significantly
improve the performance of language models. We then train from Gemma-2b-base
and LLama-3-8b-base, and find that our best models exceed the performance of
the official instruct models tuned with closed-source data and algorithms. Our
code and models can be found at
https://github.com/Columbia-NLP-Lab/LionAlignment.","[{'name': 'Xiao Yu'}, {'name': 'Qingyang Wu'}, {'name': 'Yu Li'}, {'name': 'Zhou Yu'}]",2024-07-09T04:34:39Z
http://arxiv.org/abs/2407.06538v1,http://arxiv.org/abs/2407.06538v1,"Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge
  Distillation: A Case Study","Neural Machine Translation (NMT) remains a formidable challenge, especially
when dealing with low-resource languages. Pre-trained sequence-to-sequence
(seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive
performance in various low-resource NMT tasks. However, their pre-training has
been confined to 50 languages, leaving out support for numerous low-resource
languages, particularly those spoken in the Indian subcontinent. Expanding
mBART-50's language support requires complex pre-training, risking performance
decline due to catastrophic forgetting. Considering these expanding challenges,
this paper explores a framework that leverages the benefits of a pre-trained
language model along with knowledge distillation in a seq2seq architecture to
facilitate translation for low-resource languages, including those not covered
by mBART-50. The proposed framework employs a multilingual encoder-based
seq2seq model as the foundational architecture and subsequently uses
complementary knowledge distillation techniques to mitigate the impact of
imbalanced training. Our framework is evaluated on three low-resource Indic
languages in four Indic-to-Indic directions, yielding significant BLEU-4 and
chrF improvements over baselines. Further, we conduct human evaluation to
confirm effectiveness of our approach. Our code is publicly available at
https://github.com/raypretam/Two-step-low-res-NMT.","[{'name': 'Aniruddha Roy'}, {'name': 'Pretam Ray'}, {'name': 'Ayush Maheshwari'}, {'name': 'Sudeshna Sarkar'}, {'name': 'Pawan Goyal'}]",2024-07-09T04:19:52Z
http://arxiv.org/abs/2407.06537v1,http://arxiv.org/abs/2407.06537v1,"Efficient and Accurate Memorable Conversation Model using DPO based on
  sLLM","In multi-session dialog system, it is essential to continuously update the
memory as the session progresses. Simply accumulating memory can make it
difficult to focus on the content of the conversation for inference due to the
limited input sentence size. Therefore, efficient and accurate conversation
model that is capable of managing memory to reflect the conversation history
continuously is necessary. This paper presents a conversation model that
efficiently manages memory as sessions progress and incorporates this into the
model to reflect the conversation history accurately with 3 methodologies: SFT,
DPO and DPO with SFT model. Our model using DPO algorithm shows an improvement
about 0.0591 of BERTScore in memory accuracy, and the rate of responses
reflecting the memory increased as well. Also, response generation performance
enhanced about 4.292 in fluency, 3.935 in coherence, and 2.896 in consistency.
This paper describes a training method that yields better performance than
models with more than twice the parameter size, even when the model size is
smaller. Thus, our model demonstrates efficiency not only in terms of accuracy
but also in resource utilization.","[{'name': 'Youngkyung Seo'}, {'name': 'Yoonseok Heo'}, {'name': 'Jun-Seok Koh'}, {'name': 'Du-Seoung Chang'}]",2024-07-09T04:17:39Z
http://arxiv.org/abs/2407.06533v1,http://arxiv.org/abs/2407.06533v1,LETS-C: Leveraging Language Embedding for Time Series Classification,"Recent advancements in language modeling have shown promising results when
applied to time series data. In particular, fine-tuning pre-trained large
language models (LLMs) for time series classification tasks has achieved
state-of-the-art (SOTA) performance on standard benchmarks. However, these
LLM-based models have a significant drawback due to the large model size, with
the number of trainable parameters in the millions. In this paper, we propose
an alternative approach to leveraging the success of language modeling in the
time series domain. Instead of fine-tuning LLMs, we utilize a language
embedding model to embed time series and then pair the embeddings with a simple
classification head composed of convolutional neural networks (CNN) and
multilayer perceptron (MLP). We conducted extensive experiments on
well-established time series classification benchmark datasets. We demonstrated
LETS-C not only outperforms the current SOTA in classification accuracy but
also offers a lightweight solution, using only 14.5% of the trainable
parameters on average compared to the SOTA model. Our findings suggest that
leveraging language encoders to embed time series data, combined with a simple
yet effective classification head, offers a promising direction for achieving
high-performance time series classification while maintaining a lightweight
model architecture.","[{'name': 'Rachneet Kaur'}, {'name': 'Zhen Zeng'}, {'name': 'Tucker Balch'}, {'name': 'Manuela Veloso'}]",2024-07-09T04:07:57Z
http://arxiv.org/abs/2407.06501v1,http://arxiv.org/abs/2407.06501v1,STORYSUMM: Evaluating Faithfulness in Story Summarization,"Human evaluation has been the gold standard for checking faithfulness in
abstractive summarization. However, with a challenging source domain like
narrative, multiple annotators can agree a summary is faithful, while missing
details that are obvious errors only once pointed out. We therefore introduce a
new dataset, STORYSUMM, comprising LLM summaries of short stories with
localized faithfulness labels and error explanations. This benchmark is for
evaluation methods, testing whether a given method can detect challenging
inconsistencies. Using this dataset, we first show that any one human
annotation protocol is likely to miss inconsistencies, and we advocate for
pursuing a range of methods when establishing ground truth for a summarization
dataset. We finally test recent automatic metrics and find that none of them
achieve more than 70% balanced accuracy on this task, demonstrating that it is
a challenging benchmark for future work in faithfulness evaluation.","[{'name': 'Melanie Subbiah'}, {'name': 'Faisal Ladhak'}, {'name': 'Akankshya Mishra'}, {'name': 'Griffin Adams'}, {'name': 'Lydia B. Chilton'}, {'name': 'Kathleen McKeown'}]",2024-07-09T02:06:30Z
http://arxiv.org/abs/2407.12853v1,http://arxiv.org/abs/2407.12853v1,"Automated Justification Production for Claim Veracity in Fact Checking:
  A Survey on Architectures and Approaches","Automated Fact-Checking (AFC) is the automated verification of claim
accuracy. AFC is crucial in discerning truth from misinformation, especially
given the huge amounts of content are generated online daily. Current research
focuses on predicting claim veracity through metadata analysis and language
scrutiny, with an emphasis on justifying verdicts. This paper surveys recent
methodologies, proposing a comprehensive taxonomy and presenting the evolution
of research in that landscape. A comparative analysis of methodologies and
future directions for improving fact-checking explainability are also
discussed.","[{'name': 'Islam Eldifrawi'}, {'name': 'Shengrui Wang'}, {'name': 'Amine Trabelsi'}]",2024-07-09T01:54:13Z
http://arxiv.org/abs/2407.06488v1,http://arxiv.org/abs/2407.06488v1,"Towards Understanding Multi-Task Learning (Generalization) of LLMs via
  Detecting and Exploring Task-Specific Neurons","While large language models (LLMs) have demonstrated superior multi-task
capabilities, understanding the learning mechanisms behind this is still a
challenging problem. In this paper, we attempt to understand such mechanisms
from the perspective of neurons. Specifically, we detect task-sensitive neurons
in LLMs via gradient attribution on task-specific data. Through extensive
deactivation and fine-tuning experiments, we demonstrate that the detected
neurons are highly correlated with the given task, which we term as
task-specific neurons. With these identified task-specific neurons, we delve
into two common problems in multi-task learning and continuous learning:
Generalization and Catastrophic Forgetting. We find that the overlap of
task-specific neurons is strongly associated with generalization and
specialization across tasks. Interestingly, at certain layers of LLMs, there is
a high similarity in the parameters of different task-specific neurons, and
such similarity is highly correlated with the generalization performance.
Inspired by these findings, we propose a neuron-level continuous fine-tuning
method that only fine-tunes the current task-specific neurons during continuous
learning, and extensive experiments demonstrate the effectiveness of the
proposed method. Our study provides insights into the interpretability of LLMs
in multi-task learning.","[{'name': 'Yongqi Leng'}, {'name': 'Deyi Xiong'}]",2024-07-09T01:27:35Z
http://arxiv.org/abs/2407.06483v1,http://arxiv.org/abs/2407.06483v1,Composable Interventions for Language Models,"Test-time interventions for language models can enhance factual accuracy,
mitigate harmful outputs, and improve model efficiency without costly
retraining. But despite a flood of new methods, different types of
interventions are largely developing independently. In practice, multiple
interventions must be applied sequentially to the same model, yet we lack
standardized ways to study how interventions interact. We fill this gap by
introducing composable interventions, a framework to study the effects of using
multiple interventions on the same language models, featuring new metrics and a
unified codebase. Using our framework, we conduct extensive experiments and
compose popular methods from three emerging intervention categories --
Knowledge Editing, Model Compression, and Machine Unlearning. Our results from
310 different compositions uncover meaningful interactions: compression hinders
editing and unlearning, composing interventions hinges on their order of
application, and popular general-purpose metrics are inadequate for assessing
composability. Taken together, our findings showcase clear gaps in
composability, suggesting a need for new multi-objective interventions. All of
our code is public:
https://github.com/hartvigsen-group/composable-interventions.","[{'name': 'Arinbjorn Kolbeinsson'}, {'name': ""Kyle O'Brien""}, {'name': 'Tianjin Huang'}, {'name': 'Shanghua Gao'}, {'name': 'Shiwei Liu'}, {'name': 'Jonathan Richard Schwarz'}, {'name': 'Anurag Vaidya'}, {'name': 'Faisal Mahmood'}, {'name': 'Marinka Zitnik'}, {'name': 'Tianlong Chen'}, {'name': 'Thomas Hartvigsen'}]",2024-07-09T01:17:44Z
http://arxiv.org/abs/2407.06479v1,http://arxiv.org/abs/2407.06479v1,"Interaction Matters: An Evaluation Framework for Interactive Dialogue
  Assessment on English Second Language Conversations","We present an evaluation framework for interactive dialogue assessment in the
context of English as a Second Language (ESL) speakers. Our framework collects
dialogue-level interactivity labels (e.g., topic management; 4 labels in total)
and micro-level span features (e.g., backchannels; 17 features in total). Given
our annotated data, we study how the micro-level features influence the (higher
level) interactivity quality of ESL dialogues by constructing various machine
learning-based models. Our results demonstrate that certain micro-level
features strongly correlate with interactivity quality, like reference word
(e.g., she, her, he), revealing new insights about the interaction between
higher-level dialogue quality and lower-level linguistic signals. Our framework
also provides a means to assess ESL communication, which is useful for language
assessment.","[{'name': 'Rena Gao'}, {'name': 'Carsten Roever'}, {'name': 'Jey Han Lau'}]",2024-07-09T00:56:59Z
http://arxiv.org/abs/2407.06432v1,http://arxiv.org/abs/2407.06432v1,"An Empirical Study of Gendered Stereotypes in Emotional Attributes for
  Bangla in Multilingual Large Language Models","The influence of Large Language Models (LLMs) is rapidly growing, automating
more jobs over time. Assessing the fairness of LLMs is crucial due to their
expanding impact. Studies reveal the reflection of societal norms and biases in
LLMs, which creates a risk of propagating societal stereotypes in downstream
tasks. Many studies on bias in LLMs focus on gender bias in various NLP
applications. However, there's a gap in research on bias in emotional
attributes, despite the close societal link between emotion and gender. This
gap is even larger for low-resource languages like Bangla. Historically, women
are associated with emotions like empathy, fear, and guilt, while men are
linked to anger, bravado, and authority. This pattern reflects societal norms
in Bangla-speaking regions. We offer the first thorough investigation of
gendered emotion attribution in Bangla for both closed and open source LLMs in
this work. Our aim is to elucidate the intricate societal relationship between
gender and emotion specifically within the context of Bangla. We have been
successful in showing the existence of gender bias in the context of emotions
in Bangla through analytical methods and also show how emotion attribution
changes on the basis of gendered role selection in LLMs. All of our resources
including code and data are made publicly available to support future research
on Bangla NLP.
  Warning: This paper contains explicit stereotypical statements that many may
find offensive.","[{'name': 'Jayanta Sadhu'}, {'name': 'Maneesha Rani Saha'}, {'name': 'Rifat Shahriyar'}]",2024-07-08T22:22:15Z
http://arxiv.org/abs/2407.06426v1,http://arxiv.org/abs/2407.06426v1,"DebUnc: Mitigating Hallucinations in Large Language Model Agent
  Communication with Uncertainty Estimations","To enhance Large Language Model (LLM) capabilities, multi-agent debates have
been introduced, where multiple LLMs discuss solutions to a problem over
several rounds of debate. However, LLMs often produce incorrect responses that
appear deceptively confident, which can mislead other agents. This is partly
because agents do not express their confidence levels during standard debates.
To address this, we introduce DebUnc, a multi-agent debate framework that uses
uncertainty metrics to assess agent confidence levels. We adapted the LLM
attention mechanism to adjust token weights based on confidence levels and also
explored using textual prompts to convey confidence. Our evaluations across
various benchmarks show that attention-based methods are particularly
effective, and that as uncertainty metrics evolve, performance will continue to
increase. The code is available at https://github.com/lukeyoffe/debunc","[{'name': 'Luke Yoffe'}, {'name': 'Alfonso Amayuelas'}, {'name': 'William Yang Wang'}]",2024-07-08T22:15:01Z
http://arxiv.org/abs/2407.06422v1,http://arxiv.org/abs/2407.06422v1,"Exploring the Capability of ChatGPT to Reproduce Human Labels for Social
  Computing Tasks (Extended Version)","Harnessing the potential of large language models (LLMs) like ChatGPT can
help address social challenges through inclusive, ethical, and sustainable
means. In this paper, we investigate the extent to which ChatGPT can annotate
data for social computing tasks, aiming to reduce the complexity and cost of
undertaking web research. To evaluate ChatGPT's potential, we re-annotate seven
datasets using ChatGPT, covering topics related to pressing social issues like
COVID-19 misinformation, social bot deception, cyberbully, clickbait news, and
the Russo-Ukrainian War. Our findings demonstrate that ChatGPT exhibits promise
in handling these data annotation tasks, albeit with some challenges. Across
the seven datasets, ChatGPT achieves an average annotation F1-score of 72.00%.
Its performance excels in clickbait news annotation, correctly labeling 89.66%
of the data. However, we also observe significant variations in performance
across individual labels. Our study reveals predictable patterns in ChatGPT's
annotation performance. Thus, we propose GPT-Rater, a tool to predict if
ChatGPT can correctly label data for a given annotation task. Researchers can
use this to identify where ChatGPT might be suitable for their annotation
requirements. We show that GPT-Rater effectively predicts ChatGPT's
performance. It performs best on a clickbait headlines dataset by achieving an
average F1-score of 95.00%. We believe that this research opens new avenues for
analysis and can reduce barriers to engaging in social computing research.","[{'name': 'Yiming Zhu'}, {'name': 'Peixian Zhang'}, {'name': 'Ehsan-Ul Haq'}, {'name': 'Pan Hui'}, {'name': 'Gareth Tyson'}]",2024-07-08T22:04:30Z
http://arxiv.org/abs/2407.06411v1,http://arxiv.org/abs/2407.06411v1,"If You Don't Understand It, Don't Use It: Eliminating Trojans with
  Filters Between Layers","Large language models (LLMs) sometimes exhibit dangerous unintended
behaviors. Finding and fixing these is challenging because the attack surface
is massive -- it is not tractable to exhaustively search for all possible
inputs that may elicit such behavior. One specific and particularly challenging
case is that if data-poisoning-injected trojans, since there is no way to know
what they are to search for them. To our knowledge, there is no generally
applicable method to unlearn unknown trojans injected during pre-training. This
work seeks to provide a general purpose recipe (filters) and a specific
implementation (LoRA) filters that work in practice on small to medium sized
models. The focus is primarily empirical, though some perplexing behavior opens
the door to the fundamental question of how LLMs store and process information.
Not unexpectedly, we find that our filters work best on the residual stream and
the latest layers.",[{'name': 'Adriano Hernandez'}],2024-07-08T21:40:23Z
http://arxiv.org/abs/2407.06380v1,http://arxiv.org/abs/2407.06380v1,"Data, Data Everywhere: A Guide for Pretraining Dataset Construction","The impressive capabilities of recent language models can be largely
attributed to the multi-trillion token pretraining datasets that they are
trained on. However, model developers fail to disclose their construction
methodology which has lead to a lack of open information on how to develop
effective pretraining sets. To address this issue, we perform the first
systematic study across the entire pipeline of pretraining set construction.
First, we run ablations on existing techniques for pretraining set development
to identify which methods translate to the largest gains in model accuracy on
downstream evaluations. Then, we categorize the most widely used data source,
web crawl snapshots, across the attributes of toxicity, quality, type of
speech, and domain. Finally, we show how such attribute information can be used
to further refine and improve the quality of a pretraining set. These findings
constitute an actionable set of steps that practitioners can use to develop
high quality pretraining sets.","[{'name': 'Jupinder Parmar'}, {'name': 'Shrimai Prabhumoye'}, {'name': 'Joseph Jennings'}, {'name': 'Bo Liu'}, {'name': 'Aastha Jhunjhunwala'}, {'name': 'Zhilin Wang'}, {'name': 'Mostofa Patwary'}, {'name': 'Mohammad Shoeybi'}, {'name': 'Bryan Catanzaro'}]",2024-07-08T20:47:58Z
http://arxiv.org/abs/2407.06349v1,http://arxiv.org/abs/2407.06349v1,Large Language Model Recall Uncertainty is Modulated by the Fan Effect,"This paper evaluates whether large language models (LLMs) exhibit cognitive
fan effects, similar to those discovered by Anderson in humans, after being
pre-trained on human textual data. We conduct two sets of in-context recall
experiments designed to elicit fan effects. Consistent with human results, we
find that LLM recall uncertainty, measured via token probability, is influenced
by the fan effect. Our results show that removing uncertainty disrupts the
observed effect. The experiments suggest the fan effect is consistent whether
the fan value is induced in-context or in the pre-training data. Finally, these
findings provide in-silico evidence that fan effects and typicality are
expressions of the same phenomena.","[{'name': 'Jesse Roberts'}, {'name': 'Kyle Moore'}, {'name': 'Thao Pham'}, {'name': 'Oseremhen Ewaleifoh'}, {'name': 'Doug Fisher'}]",2024-07-08T19:40:50Z
http://arxiv.org/abs/2407.06323v1,http://arxiv.org/abs/2407.06323v1,"When in Doubt, Cascade: Towards Building Efficient and Capable
  Guardrails","Large language models (LLMs) have convincing performance in a variety of
downstream tasks. However, these systems are prone to generating undesirable
outputs such as harmful and biased text. In order to remedy such generations,
the development of guardrail (or detector) models has gained traction.
Motivated by findings from developing a detector for social bias, we adopt the
notion of a use-mention distinction - which we identified as the primary source
of under-performance in the preliminary versions of our social bias detector.
Armed with this information, we describe a fully extensible and reproducible
synthetic data generation pipeline which leverages taxonomy-driven instructions
to create targeted and labeled data. Using this pipeline, we generate over 300K
unique contrastive samples and provide extensive experiments to systematically
evaluate performance on a suite of open source datasets. We show that our
method achieves competitive performance with a fraction of the cost in compute
and offers insight into iteratively developing efficient and capable guardrail
models.
  Warning: This paper contains examples of text which are toxic, biased, and
potentially harmful.","[{'name': 'Manish Nagireddy'}, {'name': 'Inkit Padhi'}, {'name': 'Soumya Ghosh'}, {'name': 'Prasanna Sattigeri'}]",2024-07-08T18:39:06Z
http://arxiv.org/abs/2407.06314v3,http://arxiv.org/abs/2407.06314v3,"Personality Analysis for Social Media Users using Arabic language and
  its Effect on Sentiment Analysis","Social media is heading towards more and more personalization, where
individuals reveal their beliefs, interests, habits, and activities, simply
offering glimpses into their personality traits. This study, explores the
correlation between the use of Arabic language on twitter, personality traits
and its impact on sentiment analysis. We indicated the personality traits of
users based on the information extracted from their profile activities, and the
content of their tweets. Our analysis incorporated linguistic features, profile
statistics (including gender, age, bio, etc.), as well as additional features
like emoticons. To obtain personality data, we crawled the timelines and
profiles of users who took the 16personalities test in Arabic on
16personalities.com. Our dataset, ""AraPers"", comprised 3,250 users who shared
their personality results on twitter. We implemented various machine learning
techniques, to reveal personality traits and developed a dedicated model for
this purpose, achieving a 74.86% accuracy rate with BERT, analysis of this
dataset proved that linguistic features, profile features and derived model can
be used to differentiate between different personality traits. Furthermore, our
findings demonstrated that personality affect sentiment in social media. This
research contributes to the ongoing efforts in developing robust understanding
of the relation between human behaviour on social media and personality
features for real-world applications, such as political discourse analysis, and
public opinion tracking.","[{'name': 'Mokhaiber Dandash'}, {'name': 'Masoud Asadpour'}]",2024-07-08T18:27:54Z
http://arxiv.org/abs/2407.06304v1,http://arxiv.org/abs/2407.06304v1,VIMI: Grounding Video Generation through Multi-modal Instruction,"Existing text-to-video diffusion models rely solely on text-only encoders for
their pretraining. This limitation stems from the absence of large-scale
multimodal prompt video datasets, resulting in a lack of visual grounding and
restricting their versatility and application in multimodal integration. To
address this, we construct a large-scale multimodal prompt dataset by employing
retrieval methods to pair in-context examples with the given text prompts and
then utilize a two-stage training strategy to enable diverse video generation
tasks within the same model. In the first stage, we propose a multimodal
conditional video generation framework for pretraining on these augmented
datasets, establishing a foundational model for grounded video generation.
Secondly, we finetune the model from the first stage on three video generation
tasks, incorporating multi-modal instructions. This process further refines the
model's ability to handle diverse inputs and tasks, ensuring seamless
integration of multi-modal information. After this two-stage train-ing process,
VIMI demonstrates multimodal understanding capabilities, producing contextually
rich and personalized videos grounded in the provided inputs, as shown in
Figure 1. Compared to previous visual grounded video generation methods, VIMI
can synthesize consistent and temporally coherent videos with large motion
while retaining the semantic control. Lastly, VIMI also achieves
state-of-the-art text-to-video generation results on UCF101 benchmark.","[{'name': 'Yuwei Fang'}, {'name': 'Willi Menapace'}, {'name': 'Aliaksandr Siarohin'}, {'name': 'Tsai-Shien Chen'}, {'name': 'Kuan-Chien Wang'}, {'name': 'Ivan Skorokhodov'}, {'name': 'Graham Neubig'}, {'name': 'Sergey Tulyakov'}]",2024-07-08T18:12:49Z
http://arxiv.org/abs/2407.06292v1,http://arxiv.org/abs/2407.06292v1,"Hybrid X-Linker: Automated Data Generation and Extreme Multi-label
  Ranking for Biomedical Entity Linking","State-of-the-art deep learning entity linking methods rely on extensive
human-labelled data, which is costly to acquire. Current datasets are limited
in size, leading to inadequate coverage of biomedical concepts and diminished
performance when applied to new data. In this work, we propose to automatically
generate data to create large-scale training datasets, which allows the
exploration of approaches originally developed for the task of extreme
multi-label ranking in the biomedical entity linking task. We propose the
hybrid X-Linker pipeline that includes different modules to link disease and
chemical entity mentions to concepts in the MEDIC and the CTD-Chemical
vocabularies, respectively. X-Linker was evaluated on several biomedical
datasets: BC5CDR-Disease, BioRED-Disease, NCBI-Disease, BC5CDR-Chemical,
BioRED-Chemical, and NLM-Chem, achieving top-1 accuracies of 0.8307, 0.7969,
0.8271, 0.9511, 0.9248, and 0.7895, respectively. X-Linker demonstrated
superior performance in three datasets: BC5CDR-Disease, NCBI-Disease, and
BioRED-Chemical. In contrast, SapBERT outperformed X-Linker in the remaining
three datasets. Both models rely only on the mention string for their
operations. The source code of X-Linker and its associated data are publicly
available for performing biomedical entity linking without requiring
pre-labelled entities with identifiers from specific knowledge organization
systems.","[{'name': 'Pedro Ruas'}, {'name': 'Fernando Gallego'}, {'name': 'Francisco J. Veredas'}, {'name': 'Francisco M. Couto'}]",2024-07-08T18:04:22Z
http://arxiv.org/abs/2407.06192v1,http://arxiv.org/abs/2407.06192v1,Multi-Object Hallucination in Vision-Language Models,"Large vision language models (LVLMs) often suffer from object hallucination,
producing objects not present in the given images. While current benchmarks for
object hallucination primarily concentrate on the presence of a single object
class rather than individual entities, this work systematically investigates
multi-object hallucination, examining how models misperceive (e.g., invent
nonexistent objects or become distracted) when tasked with focusing on multiple
objects simultaneously. We introduce Recognition-based Object Probing
Evaluation (ROPE), an automated evaluation protocol that considers the
distribution of object classes within a single image during testing and uses
visual referring prompts to eliminate ambiguity. With comprehensive empirical
studies and analysis of potential factors leading to multi-object
hallucination, we found that (1) LVLMs suffer more hallucinations when focusing
on multiple objects compared to a single object. (2) The tested object class
distribution affects hallucination behaviors, indicating that LVLMs may follow
shortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced
by data-specific factors, salience and frequency, and model intrinsic
behaviors. We hope to enable LVLMs to recognize and reason about multiple
objects that often occur in realistic visual scenes, provide insights, and
quantify our progress towards mitigating the issues.","[{'name': 'Xuweiyi Chen'}, {'name': 'Ziqiao Ma'}, {'name': 'Xuejun Zhang'}, {'name': 'Sihan Xu'}, {'name': 'Shengyi Qian'}, {'name': 'Jianing Yang'}, {'name': 'David F. Fouhey'}, {'name': 'Joyce Chai'}]",2024-07-08T17:59:57Z
http://arxiv.org/abs/2407.06249v1,http://arxiv.org/abs/2407.06249v1,CodeUpdateArena: Benchmarking Knowledge Editing on API Updates,"Large language models (LLMs) are increasingly being used to synthesize and
reason about source code. However, the static nature of these models' knowledge
does not reflect the fact that libraries and API functions they invoke are
continuously evolving, with functionality being added or changing. While
numerous benchmarks evaluate how LLMs can generate code, no prior work has
studied how an LLMs' knowledge about code API functions can be updated. To fill
this gap, we present CodeUpdateArena, a benchmark for knowledge editing in the
code domain. An instance in our benchmark consists of a synthetic API function
update paired with a program synthesis example that uses the updated
functionality; our goal is to update an LLM to be able to solve this program
synthesis example without providing documentation of the update at inference
time. Compared to knowledge editing for facts encoded in text, success here is
more challenging: a code LLM must correctly reason about the semantics of the
modified function rather than just reproduce its syntax. Our dataset is
constructed by first prompting GPT-4 to generate atomic and executable function
updates. Then, for each update, we generate program synthesis examples whose
code solutions are prone to use the update. Our benchmark covers updates of
various types to 54 functions from seven diverse Python packages, with a total
of 670 program synthesis examples. Our experiments show that prepending
documentation of the update to open-source code LLMs (i.e., DeepSeek,
CodeLlama) does not allow them to incorporate changes for problem solving, and
existing knowledge editing techniques also have substantial room for
improvement. We hope our benchmark will inspire new methods for knowledge
updating in code LLMs.","[{'name': 'Zeyu Leo Liu'}, {'name': 'Shrey Pandit'}, {'name': 'Xi Ye'}, {'name': 'Eunsol Choi'}, {'name': 'Greg Durrett'}]",2024-07-08T17:55:04Z
http://arxiv.org/abs/2407.06177v1,http://arxiv.org/abs/2407.06177v1,Vision-Language Models under Cultural and Inclusive Considerations,"Large vision-language models (VLMs) can assist visually impaired people by
describing images from their daily lives. Current evaluation datasets may not
reflect diverse cultural user backgrounds or the situational context of this
use case. To address this problem, we create a survey to determine caption
preferences and propose a culture-centric evaluation benchmark by filtering
VizWiz, an existing dataset with images taken by people who are blind. We then
evaluate several VLMs, investigating their reliability as visual assistants in
a culturally diverse setting. While our results for state-of-the-art models are
promising, we identify challenges such as hallucination and misalignment of
automatic evaluation metrics with human judgment. We make our survey, data,
code, and model outputs publicly available.","[{'name': 'Antonia Karamolegkou'}, {'name': 'Phillip Rust'}, {'name': 'Yong Cao'}, {'name': 'Ruixiang Cui'}, {'name': 'Anders Søgaard'}, {'name': 'Daniel Hershcovich'}]",2024-07-08T17:50:00Z
http://arxiv.org/abs/2407.06172v1,http://arxiv.org/abs/2407.06172v1,On Speeding Up Language Model Evaluation,"Large language models (LLMs) currently dominate the field of natural language
processing (NLP), representing the state-of-the-art across a diverse array of
tasks. Developing a model of this nature, from training to inference, requires
making numerous decisions which define a combinatorial search problem. For
example, selecting the optimal pre-trained LLM, prompt, or hyperparameters to
attain the best performance for a task often requires evaluating multiple
candidates on an entire test set. This exhaustive evaluation can be
time-consuming and costly, as both inference and metric computation with LLMs
are resource-intensive. In this paper, we address the challenge of identifying
the best method within a limited budget for evaluating methods on test
examples. By leveraging the well-studied multi-armed bandit framework, which
sequentially selects the next method-example pair to evaluate, our approach,
combining multi-armed bandit algorithms with low-rank factorization,
significantly reduces the required resources. Experiments show that our
algorithms can identify the top-performing method using only 5-15\% of the
typically needed resources, resulting in an 85-95\% reduction in cost.","[{'name': 'Jin Peng Zhou'}, {'name': 'Christian K. Belardi'}, {'name': 'Ruihan Wu'}, {'name': 'Travis Zhang'}, {'name': 'Carla P. Gomes'}, {'name': 'Wen Sun'}, {'name': 'Kilian Q. Weinberger'}]",2024-07-08T17:48:42Z
http://arxiv.org/abs/2407.06153v1,http://arxiv.org/abs/2407.06153v1,"What's Wrong with Your Code Generated by Large Language Models? An
  Extensive Study","The increasing development of large language models (LLMs) in code generation
has drawn significant attention among researchers. To enhance LLM-based code
generation ability, current efforts are predominantly directed towards
collecting high-quality datasets and leveraging diverse training technologies.
However, there is a notable lack of comprehensive studies examining the
limitations and boundaries of these existing methods. To bridge this gap, we
conducted an extensive empirical study evaluating the performance of three
leading closed-source LLMs and four popular open-source LLMs on three commonly
used benchmarks. Our investigation, which evaluated the length, cyclomatic
complexity and API number of the generated code, revealed that these LLMs face
challenges in generating successful code for more complex problems, and tend to
produce code that is shorter yet more complicated as compared to canonical
solutions. Additionally, we developed a taxonomy of bugs for incorrect codes
that includes three categories and 12 sub-categories, and analyze the root
cause for common bug types. Furthermore, to better understand the performance
of LLMs in real-world projects, we manually created a real-world benchmark
comprising 140 code generation tasks. Our analysis highlights distinct
differences in bug distributions between actual scenarios and existing
benchmarks. Finally, we propose a novel training-free iterative method that
introduces self-critique, enabling LLMs to critique and correct their generated
code based on bug types and compiler feedback. Experimental results demonstrate
that our approach can significantly mitigate bugs and increase the passing rate
by 29.2% after two iterations, indicating substantial potential for LLMs to
handle more complex problems.","[{'name': 'Shihan Dou'}, {'name': 'Haoxiang Jia'}, {'name': 'Shenxi Wu'}, {'name': 'Huiyuan Zheng'}, {'name': 'Weikang Zhou'}, {'name': 'Muling Wu'}, {'name': 'Mingxu Chai'}, {'name': 'Jessica Fan'}, {'name': 'Caishuang Huang'}, {'name': 'Yunbo Tao'}, {'name': 'Yan Liu'}, {'name': 'Enyu Zhou'}, {'name': 'Ming Zhang'}, {'name': 'Yuhao Zhou'}, {'name': 'Yueming Wu'}, {'name': 'Rui Zheng'}, {'name': 'Ming Wen'}, {'name': 'Rongxiang Weng'}, {'name': 'Jingang Wang'}, {'name': 'Xunliang Cai'}, {'name': 'Tao Gui'}, {'name': 'Xipeng Qiu'}, {'name': 'Qi Zhang'}, {'name': 'Xuanjing Huang'}]",2024-07-08T17:27:17Z
http://arxiv.org/abs/2407.06146v2,http://arxiv.org/abs/2407.06146v2,"Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling
  Tasks","We present and evaluate a method called grammar masking, which is used to
guide large language models (LLMs) toward producing syntactically correct
models for a given context-free grammar. Prompt engineering methods such as
few-shot learning or priming can be used to improve the chances of an LLM
producing correct syntax, but the more complex the grammar, the more
time-consuming and less promising these methods become. Previous work is
focused primarily on the usage of either language model training or prompt
engineering. In this work, a method is presented that restricts the output to a
given grammar using constrained decoding to ensure the output adheres to a
valid syntax. We use several DSLs built with MontiCore and task multiple LLMs
to produce models with and without constrained decoding. A corresponding parser
is used to confirm the syntactic correctness of each model. We show that
grammar masking can dramatically improve the modeling capabilities of several
LLMs, reducing the need for well-refined prompting while increasing the chance
of producing correct models.","[{'name': 'Lukas Netz'}, {'name': 'Jan Reimer'}, {'name': 'Bernhard Rumpe'}]",2024-07-08T17:19:59Z
http://arxiv.org/abs/2407.06135v1,http://arxiv.org/abs/2407.06135v1,"ANOLE: An Open, Autoregressive, Native Large Multimodal Models for
  Interleaved Image-Text Generation","Previous open-source large multimodal models (LMMs) have faced several
limitations: (1) they often lack native integration, requiring adapters to
align visual representations with pre-trained large language models (LLMs); (2)
many are restricted to single-modal generation; (3) while some support
multimodal generation, they rely on separate diffusion models for visual
modeling and generation. To mitigate these limitations, we present Anole, an
open, autoregressive, native large multimodal model for interleaved image-text
generation. We build Anole from Meta AI's Chameleon, adopting an innovative
fine-tuning strategy that is both data-efficient and parameter-efficient. Anole
demonstrates high-quality, coherent multimodal generation capabilities. We have
open-sourced our model, training framework, and instruction tuning data.","[{'name': 'Ethan Chern'}, {'name': 'Jiadi Su'}, {'name': 'Yan Ma'}, {'name': 'Pengfei Liu'}]",2024-07-08T17:08:02Z
http://arxiv.org/abs/2407.12852v2,http://arxiv.org/abs/2407.12852v2,Historical Ink: Semantic Shift Detection for 19th Century Spanish,"This paper explores the evolution of word meanings in 19th-century Spanish
texts, with an emphasis on Latin American Spanish, using computational
linguistics techniques. It addresses the Semantic Shift Detection (SSD) task,
which is crucial for understanding linguistic evolution, particularly in
historical contexts. The study focuses on analyzing a set of Spanish target
words. To achieve this, a 19th-century Spanish corpus is constructed, and a
customizable pipeline for SSD tasks is developed. This pipeline helps find the
senses of a word and measure their semantic change between two corpora using
fine-tuned BERT-like models with old Spanish texts for both Latin American and
general Spanish cases. The results provide valuable insights into the cultural
and societal shifts reflected in language changes over time.","[{'name': 'Tony Montes'}, {'name': 'Laura Manrique-Gómez'}, {'name': 'Rubén Manrique'}]",2024-07-08T16:49:34Z
http://arxiv.org/abs/2407.06112v1,http://arxiv.org/abs/2407.06112v1,"Enhancing Language Model Rationality with Bi-Directional Deliberation
  Reasoning","This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel
reasoning approach to enhance the decision rationality of language models.
Traditional reasoning methods typically rely on historical information and
employ uni-directional (left-to-right) reasoning strategy. This lack of
bi-directional deliberation reasoning results in limited awareness of potential
future outcomes and insufficient integration of historical context, leading to
suboptimal decisions. BIDDER addresses this gap by incorporating principles of
rational decision-making, specifically managing uncertainty and predicting
expected utility. Our approach involves three key processes: Inferring hidden
states to represent uncertain information in the decision-making process from
historical data; Using these hidden states to predict future potential states
and potential outcomes; Integrating historical information (past contexts) and
long-term outcomes (future contexts) to inform reasoning. By leveraging
bi-directional reasoning, BIDDER ensures thorough exploration of both past and
future contexts, leading to more informed and rational decisions. We tested
BIDDER's effectiveness in two well-defined scenarios: Poker (Limit Texas
Hold'em) and Negotiation. Our experiments demonstrate that BIDDER significantly
improves the decision-making capabilities of LLMs and LLM agents.","[{'name': 'Yadong Zhang'}, {'name': 'Shaoguang Mao'}, {'name': 'Wenshan Wu'}, {'name': 'Yan Xia'}, {'name': 'Tao Ge'}, {'name': 'Man Lan'}, {'name': 'Furu Wei'}]",2024-07-08T16:48:48Z
http://arxiv.org/abs/2407.06098v1,http://arxiv.org/abs/2407.06098v1,"Epistemological Bias As a Means for the Automated Detection of
  Injustices in Text","Injustice occurs when someone experiences unfair treatment or their rights
are violated and is often due to the presence of implicit biases and prejudice
such as stereotypes. The automated identification of injustice in text has
received little attention, due in part to the fact that underlying implicit
biases or stereotypes are rarely explicitly stated and that instances often
occur unconsciously due to the pervasive nature of prejudice in society. Here,
we describe a novel framework that combines the use of a fine-tuned BERT-based
bias detection model, two stereotype detection models, and a lexicon-based
approach to show that epistemological biases (i.e., words, which presupposes,
entails, asserts, hedges, or boosts text to erode or assert a person's capacity
as a knower) can assist with the automatic detection of injustice in text. The
news media has many instances of injustice (i.e. discriminatory narratives),
thus it is our use case here. We conduct and discuss an empirical qualitative
research study which shows how the framework can be applied to detect
injustices, even at higher volumes of data.","[{'name': 'Kenya Andrews'}, {'name': 'Lamogha Chiazor'}]",2024-07-08T16:38:31Z
http://arxiv.org/abs/2407.06089v1,http://arxiv.org/abs/2407.06089v1,"Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in
  the Era of Large Language Models","The remarkable success of Large Language Models (LLMs) has ushered natural
language processing (NLP) research into a new era. Despite their diverse
capabilities, LLMs trained on different corpora exhibit varying strengths and
weaknesses, leading to challenges in maximizing their overall efficiency and
versatility. To address these challenges, recent studies have explored
collaborative strategies for LLMs. This paper provides a comprehensive overview
of this emerging research area, highlighting the motivation behind such
collaborations. Specifically, we categorize collaborative strategies into three
primary approaches: Merging, Ensemble, and Cooperation. Merging involves
integrating multiple LLMs in the parameter space. Ensemble combines the outputs
of various LLMs. Cooperation} leverages different LLMs to allow full play to
their diverse capabilities for specific tasks. We provide in-depth
introductions to these methods from different perspectives and discuss their
potential applications. Additionally, we outline future research directions,
hoping this work will catalyze further studies on LLM collaborations and paving
the way for advanced NLP applications.","[{'name': 'Jinliang Lu'}, {'name': 'Ziliang Pang'}, {'name': 'Min Xiao'}, {'name': 'Yaochen Zhu'}, {'name': 'Rui Xia'}, {'name': 'Jiajun Zhang'}]",2024-07-08T16:29:08Z
http://arxiv.org/abs/2407.06071v1,http://arxiv.org/abs/2407.06071v1,"From Loops to Oops: Fallback Behaviors of Language Models Under
  Uncertainty","Large language models (LLMs) often exhibit undesirable behaviors, such as
hallucinations and sequence repetitions. We propose to view these behaviors as
fallbacks that models exhibit under uncertainty, and investigate the connection
between them. We categorize fallback behaviors -- sequence repetitions,
degenerate text, and hallucinations -- and extensively analyze them in models
from the same family that differ by the amount of pretraining tokens, parameter
count, or the inclusion of instruction-following training. Our experiments
reveal a clear and consistent ordering of fallback behaviors, across all these
axes: the more advanced an LLM is (i.e., trained on more tokens, has more
parameters, or instruction-tuned), its fallback behavior shifts from sequence
repetitions, to degenerate text, and then to hallucinations. Moreover, the same
ordering is observed throughout a single generation, even for the
best-performing models; as uncertainty increases, models shift from generating
hallucinations to producing degenerate text and then sequence repetitions.
Lastly, we demonstrate that while common decoding techniques, such as random
sampling, might alleviate some unwanted behaviors like sequence repetitions,
they increase harder-to-detect hallucinations.","[{'name': 'Maor Ivgi'}, {'name': 'Ori Yoran'}, {'name': 'Jonathan Berant'}, {'name': 'Mor Geva'}]",2024-07-08T16:13:42Z
http://arxiv.org/abs/2407.06057v1,http://arxiv.org/abs/2407.06057v1,Variational Best-of-N Alignment,"Best-of-N (BoN) is a popular and effective algorithm for aligning language
models to human preferences. The algorithm works as follows: at inference time,
N samples are drawn from the language model, and the sample with the highest
reward, as judged by a reward model, is returned as the output. Despite its
effectiveness, BoN is computationally expensive; it reduces sampling throughput
by a factor of N. To make BoN more efficient at inference time, one strategy is
to fine-tune the language model to mimic what BoN does during inference. To
achieve this, we derive the distribution induced by the BoN algorithm. We then
propose to fine-tune the language model to minimize backward KL divergence to
the BoN distribution. Our approach is analogous to mean-field variational
inference and, thus, we term it variational BoN (vBoN). To the extent this
fine-tuning is successful and we end up with a good approximation, we have
reduced the inference cost by a factor of N. Our experiments on a controlled
generation task suggest that while variational BoN is not as effective as BoN
in aligning language models, it is close to BoN performance as vBoN appears
more often on the Pareto frontier of reward and KL divergence compared to
models trained with KL-constrained RL objective.","[{'name': 'Afra Amini'}, {'name': 'Tim Vieira'}, {'name': 'Ryan Cotterell'}]",2024-07-08T15:59:44Z
http://arxiv.org/abs/2407.06048v1,http://arxiv.org/abs/2407.06048v1,"Vision-Braille: An End-to-End Tool for Chinese Braille Image-to-Text
  Translation","Visually impaired people are a large group who can only use braille for
reading and writing. However, the lack of special educational resources is the
bottleneck for educating them. Educational equity is a reflection of the level
of social civilization, cultural equality, and individual dignity. Facilitating
and improving lifelong learning channels for the visually impaired is of great
significance. Their written braille homework or exam papers cannot be
understood by sighted teachers, because of the lack of a highly accurate
braille translation system, especially in Chinese which has tone marks. braille
writers often omit tone marks to save space, leading to confusion when braille
with the same consonants and vowels is translated into Chinese. Previous
algorithms were insufficient in extracting contextual information, resulting in
low accuracy of braille translations into Chinese. This project informatively
fine-tuned the mT5 model with an Encoder-decoder architecture for braille to
Chinese character conversion. This research created a training set of braille
and corresponding Chinese text from the Leipzig Corpora. This project
significantly reduced the confusion in braille, achieving $62.4$ and $62.3$
BLEU scores in the validation and test sets, with a curriculum learning
fine-tuning method. By incorporating the braille recognition algorithm, this
project is the first publicly available braille translation system and can
benefit lots of visually impaired students and families who are preparing for
the Chinese College Test and help to propel their college dreams in the future.
There is a demo on our homepage\footnote{\url{https://vision-braille.com/}}.","[{'name': 'Alan Wu'}, {'name': 'Ye Yuan'}, {'name': 'Ming Zhang'}]",2024-07-08T15:51:37Z
http://arxiv.org/abs/2407.06041v1,http://arxiv.org/abs/2407.06041v1,MST5 -- Multilingual Question Answering over Knowledge Graphs,"Knowledge Graph Question Answering (KGQA) simplifies querying vast amounts of
knowledge stored in a graph-based model using natural language. However, the
research has largely concentrated on English, putting non-English speakers at a
disadvantage. Meanwhile, existing multilingual KGQA systems face challenges in
achieving performance comparable to English systems, highlighting the
difficulty of generating SPARQL queries from diverse languages. In this
research, we propose a simplified approach to enhance multilingual KGQA systems
by incorporating linguistic context and entity information directly into the
processing pipeline of a language model. Unlike existing methods that rely on
separate encoders for integrating auxiliary information, our strategy leverages
a single, pretrained multilingual transformer-based language model to manage
both the primary input and the auxiliary data. Our methodology significantly
improves the language model's ability to accurately convert a natural language
query into a relevant SPARQL query. It demonstrates promising results on the
most recent QALD datasets, namely QALD-9-Plus and QALD-10. Furthermore, we
introduce and evaluate our approach on Chinese and Japanese, thereby expanding
the language diversity of the existing datasets.","[{'name': 'Nikit Srivastava'}, {'name': 'Mengshi Ma'}, {'name': 'Daniel Vollmers'}, {'name': 'Hamada Zahera'}, {'name': 'Diego Moussallem'}, {'name': 'Axel-Cyrille Ngonga Ngomo'}]",2024-07-08T15:37:51Z
http://arxiv.org/abs/2407.06027v5,http://arxiv.org/abs/2407.06027v5,PAS: Data-Efficient Plug-and-Play Prompt Augmentation System,"In recent years, the rise of Large Language Models (LLMs) has spurred a
growing demand for plug-and-play AI systems. Among the various AI techniques,
prompt engineering stands out as particularly significant. However, users often
face challenges in writing prompts due to the steep learning curve and
significant time investment, and existing automatic prompt engineering (APE)
models can be difficult to use. To address this issue, we propose PAS, an
LLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,
automatically generated prompt complementary datasets, resulting in exceptional
performance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)
results compared to previous APE models, with an average improvement of 6.09
points. Moreover, PAS is highly efficient, achieving SoTA performance with only
9000 data points. Additionally, PAS can autonomously generate prompt
augmentation data without requiring additional human labor. Its flexibility
also allows it to be compatible with all existing LLMs and applicable to a wide
range of tasks. PAS excels in human evaluations, underscoring its suitability
as a plug-in for users. This combination of high performance, efficiency, and
flexibility makes PAS a valuable system for enhancing the usability and
effectiveness of LLMs through improved prompt engineering.","[{'name': 'Miao Zheng'}, {'name': 'Hao Liang'}, {'name': 'Fan Yang'}, {'name': 'Haoze Sun'}, {'name': 'Tianpeng Li'}, {'name': 'Lingchu Xiong'}, {'name': 'Yan Zhang'}, {'name': 'Youzhen Wu'}, {'name': 'Kun Li'}, {'name': 'Yanjun Shen'}, {'name': 'Mingan Lin'}, {'name': 'Tao Zhang'}, {'name': 'Guosheng Dong'}, {'name': 'Yujing Qiao'}, {'name': 'Kun Fang'}, {'name': 'Weipeng Chen'}, {'name': 'Bin Cui'}, {'name': 'Wentao Zhang'}, {'name': 'Zenan Zhou'}]",2024-07-08T15:25:33Z
http://arxiv.org/abs/2407.12851v1,http://arxiv.org/abs/2407.12851v1,"ISPO: An Integrated Ontology of Symptom Phenotypes for Semantic
  Integration of Traditional Chinese Medical Data","Symptom phenotypes are one of the key types of manifestations for diagnosis
and treatment of various disease conditions. However, the diversity of symptom
terminologies is one of the major obstacles hindering the analysis and
knowledge sharing of various types of symptom-related medical data particularly
in the fields of Traditional Chinese Medicine (TCM). Objective: This study
aimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to
support the data mining of Chinese EMRs and real-world study in TCM field.
Methods: To construct an integrated ontology of symptom phenotypes (ISPO), we
manually annotated classical TCM textbooks and large-scale Chinese electronic
medical records (EMRs) to collect symptom terms with support from a medical
text annotation system. Furthermore, to facilitate the semantic
interoperability between different terminologies, we incorporated public
available biomedical vocabularies by manual mapping between Chinese terms and
English terms with cross-references to source vocabularies. In addition, we
evaluated the ISPO using independent clinical EMRs to provide a high-usable
medical ontology for clinical data analysis. Results: By integrating 78,696
inpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and
dictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition
or contextual texts. Adhering to the taxonomical structure of the related
anatomical systems of symptom phenotypes, ISPO provides 12 top-level categories
and 79 middle-level sub-categories. The validation of data analysis showed the
ISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with
occurrence rates of 0.5% in additional three independent curated clinical
datasets, which can demonstrate the significant value of ISPO in mapping
clinical terms to ontologies.","[{'name': 'Zixin Shu'}, {'name': 'Rui Hua'}, {'name': 'Dengying Yan'}, {'name': 'Chenxia Lu'}, {'name': 'Ning Xu'}, {'name': 'Jun Li'}, {'name': 'Hui Zhu'}, {'name': 'Jia Zhang'}, {'name': 'Dan Zhao'}, {'name': 'Chenyang Hui'}, {'name': 'Junqiu Ye'}, {'name': 'Chu Liao'}, {'name': 'Qi Hao'}, {'name': 'Wen Ye'}, {'name': 'Cheng Luo'}, {'name': 'Xinyan Wang'}, {'name': 'Chuang Cheng'}, {'name': 'Xiaodong Li'}, {'name': 'Baoyan Liu'}, {'name': 'Xiaji Zhou'}, {'name': 'Runshun Zhang'}, {'name': 'Min Xu'}, {'name': 'Xuezhong Zhou'}]",2024-07-08T15:23:50Z
http://arxiv.org/abs/2407.06023v3,http://arxiv.org/abs/2407.06023v3,Distilling System 2 into System 1,"Large language models (LLMs) can spend extra compute during inference to
generate intermediate thoughts, which helps to produce better final responses.
Since Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have
been proposed such as Rephrase and Respond (Deng et al., 2023a), System 2
Attention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,
2023). In this work we investigate self-supervised methods to ``compile''
(distill) higher quality outputs from System 2 techniques back into LLM
generations without intermediate reasoning token sequences, as this reasoning
has been distilled into System 1. We show that several such techniques can be
successfully distilled, resulting in improved results compared to the original
System 1 performance, and with less inference cost than System 2. We posit that
such System 2 distillation will be an important feature of future continually
learning AI systems, enabling them to focus System 2 capabilities on the
reasoning tasks that they cannot yet do well.","[{'name': 'Ping Yu'}, {'name': 'Jing Xu'}, {'name': 'Jason Weston'}, {'name': 'Ilia Kulikov'}]",2024-07-08T15:17:46Z
http://arxiv.org/abs/2407.06011v1,http://arxiv.org/abs/2407.06011v1,"Igea: a Decoder-Only Language Model for Biomedical Text Generation in
  Italian","The development of domain-specific language models has significantly advanced
natural language processing applications in various specialized fields,
particularly in biomedicine. However, the focus has largely been on
English-language models, leaving a gap for less-resourced languages such as
Italian. This paper introduces Igea, the first decoder-only language model
designed explicitly for biomedical text generation in Italian. Built on the
Minerva model and continually pretrained on a diverse corpus of Italian medical
texts, Igea is available in three model sizes: 350 million, 1 billion, and 3
billion parameters. The models aim to balance computational efficiency and
performance, addressing the challenges of managing the peculiarities of medical
terminology in Italian. We evaluate Igea using a mix of in-domain biomedical
corpora and general-purpose benchmarks, highlighting its efficacy and retention
of general knowledge even after the domain-specific training. This paper
discusses the model's development and evaluation, providing a foundation for
future advancements in Italian biomedical NLP.","[{'name': 'Tommaso Mario Buonocore'}, {'name': 'Simone Rancati'}, {'name': 'Enea Parimbelli'}]",2024-07-08T15:04:21Z
http://arxiv.org/abs/2407.06004v2,http://arxiv.org/abs/2407.06004v2,"Perceptions to Beliefs: Exploring Precursory Inferences for Theory of
  Mind in Large Language Models","While humans naturally develop theory of mind (ToM), the capability to
understand other people's mental states and beliefs, state-of-the-art large
language models (LLMs) underperform on simple ToM benchmarks. We posit that we
can extend our understanding of LLMs' ToM abilities by evaluating key human ToM
precursors -- perception inference and perception-to-belief inference -- in
LLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate
these precursory inferences for ToM in LLMs by annotating characters'
perceptions on ToMi and FANToM, respectively. Our evaluation of eight
state-of-the-art LLMs reveals that the models generally perform well in
perception inference while exhibiting limited capability in
perception-to-belief inference (e.g., lack of inhibitory control). Based on
these results, we present PercepToM, a novel ToM method leveraging LLMs' strong
perception inference capability while supplementing their limited
perception-to-belief inference. Experimental results demonstrate that PercepToM
significantly enhances LLM's performance, especially in false belief scenarios.","[{'name': 'Chani Jung'}, {'name': 'Dongkwan Kim'}, {'name': 'Jiho Jin'}, {'name': 'Jiseon Kim'}, {'name': 'Yeon Seonwoo'}, {'name': 'Yejin Choi'}, {'name': 'Alice Oh'}, {'name': 'Hyunwoo Kim'}]",2024-07-08T14:58:29Z
http://arxiv.org/abs/2407.05975v1,http://arxiv.org/abs/2407.05975v1,"LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation
  Capabilities Beyond 100 Languages","Large Language Models~(LLMs) demonstrate remarkable translation capabilities
in high-resource language tasks, yet their performance in low-resource
languages is hindered by insufficient multilingual data during pre-training. To
address this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting
extensive multilingual continual pre-training on the LLaMA series models,
enabling translation support across more than 100 languages. Through a
comprehensive analysis of training strategies, such as vocabulary expansion and
data augmentation, we develop LLaMAX. Remarkably, without sacrificing its
generalization ability, LLaMAX achieves significantly higher translation
performance compared to existing open-source LLMs~(by more than 10 spBLEU
points) and performs on-par with specialized translation model~(M2M-100-12B) on
the Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve
as a robust multilingual foundation model. The
code~\footnote{\url{https://github.com/CONE-MT/LLaMAX/.}} and
models~\footnote{\url{https://huggingface.co/LLaMAX/.}} are publicly available.","[{'name': 'Yinquan Lu'}, {'name': 'Wenhao Zhu'}, {'name': 'Lei Li'}, {'name': 'Yu Qiao'}, {'name': 'Fei Yuan'}]",2024-07-08T14:18:28Z
http://arxiv.org/abs/2407.05965v1,http://arxiv.org/abs/2407.05965v1,T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models,"The recent development of Sora leads to a new era in text-to-video (T2V)
generation. Along with this comes the rising concern about its security risks.
The generated videos may contain illegal or unethical content, and there is a
lack of comprehensive quantitative understanding of their safety, posing a
challenge to their reliability and practical deployment. Previous evaluations
primarily focus on the quality of video generation. While some evaluations of
text-to-image models have considered safety, they cover fewer aspects and do
not address the unique temporal risk inherent in video generation. To bridge
this research gap, we introduce T2VSafetyBench, a new benchmark designed for
conducting safety-critical assessments of text-to-video models. We define 12
critical aspects of video generation safety and construct a malicious prompt
dataset using LLMs and jailbreaking prompt attacks. Based on our evaluation
results, we draw several important findings, including: 1) no single model
excels in all aspects, with different models showing various strengths; 2) the
correlation between GPT-4 assessments and manual reviews is generally high; 3)
there is a trade-off between the usability and safety of text-to-video
generative models. This indicates that as the field of video generation rapidly
advances, safety risks are set to surge, highlighting the urgency of
prioritizing video safety. We hope that T2VSafetyBench can provide insights for
better understanding the safety of video generation in the era of generative
AI.","[{'name': 'Yibo Miao'}, {'name': 'Yifan Zhu'}, {'name': 'Yinpeng Dong'}, {'name': 'Lijia Yu'}, {'name': 'Jun Zhu'}, {'name': 'Xiao-Shan Gao'}]",2024-07-08T14:04:58Z
http://arxiv.org/abs/2407.05890v1,http://arxiv.org/abs/2407.05890v1,"Affordances-Oriented Planning using Foundation Models for Continuous
  Vision-Language Navigation","LLM-based agents have demonstrated impressive zero-shot performance in the
vision-language navigation (VLN) task. However, these zero-shot methods focus
only on solving high-level task planning by selecting nodes in predefined
navigation graphs for movements, overlooking low-level control in realistic
navigation scenarios. To bridge this gap, we propose AO-Planner, a novel
affordances-oriented planning framework for continuous VLN task. Our AO-Planner
integrates various foundation models to achieve affordances-oriented motion
planning and action decision-making, both performed in a zero-shot manner.
Specifically, we employ a visual affordances prompting (VAP) approach, where
visible ground is segmented utilizing SAM to provide navigational affordances,
based on which the LLM selects potential next waypoints and generates low-level
path planning towards selected waypoints. We further introduce a high-level
agent, PathAgent, to identify the most probable pixel-based path and convert it
into 3D coordinates to fulfill low-level motion. Experimental results on the
challenging R2R-CE benchmark demonstrate that AO-Planner achieves
state-of-the-art zero-shot performance (5.5% improvement in SPL). Our method
establishes an effective connection between LLM and 3D world to circumvent the
difficulty of directly predicting world coordinates, presenting novel prospects
for employing foundation models in low-level motion control.","[{'name': 'Jiaqi Chen'}, {'name': 'Bingqian Lin'}, {'name': 'Xinmin Liu'}, {'name': 'Xiaodan Liang'}, {'name': 'Kwan-Yee K. Wong'}]",2024-07-08T12:52:46Z
http://arxiv.org/abs/2407.05887v1,http://arxiv.org/abs/2407.05887v1,"Generation and De-Identification of Indian Clinical Discharge Summaries
  using LLMs","The consequences of a healthcare data breach can be devastating for the
patients, providers, and payers. The average financial impact of a data breach
in recent months has been estimated to be close to USD 10 million. This is
especially significant for healthcare organizations in India that are managing
rapid digitization while still establishing data governance procedures that
align with the letter and spirit of the law. Computer-based systems for
de-identification of personal information are vulnerable to data drift, often
rendering them ineffective in cross-institution settings. Therefore, a rigorous
assessment of existing de-identification against local health datasets is
imperative to support the safe adoption of digital health initiatives in India.
Using a small set of de-identified patient discharge summaries provided by an
Indian healthcare institution, in this paper, we report the nominal performance
of de-identification algorithms (based on language models) trained on publicly
available non-Indian datasets, pointing towards a lack of cross-institutional
generalization. Similarly, experimentation with off-the-shelf de-identification
systems reveals potential risks associated with the approach. To overcome data
scarcity, we explore generating synthetic clinical reports (using publicly
available and Indian summaries) by performing in-context learning over Large
Language Models (LLMs). Our experiments demonstrate the use of generated
reports as an effective strategy for creating high-performing de-identification
systems with good generalization capabilities.","[{'name': 'Sanjeet Singh'}, {'name': 'Shreya Gupta'}, {'name': 'Niralee Gupta'}, {'name': 'Naimish Sharma'}, {'name': 'Lokesh Srivastava'}, {'name': 'Vibhu Agarwal'}, {'name': 'Ashutosh Modi'}]",2024-07-08T12:47:03Z
http://arxiv.org/abs/2407.05841v1,http://arxiv.org/abs/2407.05841v1,"An Empirical Comparison of Vocabulary Expansion and Initialization
  Approaches for Language Models","Language Models (LMs) excel in natural language processing tasks for English
but show reduced performance in most other languages. This problem is commonly
tackled by continually pre-training and fine-tuning these models for said
languages. A significant issue in this process is the limited vocabulary
coverage in the original model's tokenizer, leading to inadequate
representation of new languages and necessitating an expansion of the
tokenizer. The initialization of the embeddings corresponding to new vocabulary
items presents a further challenge. Current strategies require cross-lingual
embeddings and lack a solid theoretical foundation as well as comparisons with
strong baselines. In this paper, we first establish theoretically that
initializing within the convex hull of existing embeddings is a good
initialization, followed by a novel but simple approach, Constrained Word2Vec
(CW2V), which does not require cross-lingual embeddings. Our study evaluates
different initialization methods for expanding RoBERTa and LLaMA 2 across four
languages and five tasks. The results show that CW2V performs equally well or
even better than more advanced techniques. Additionally, simpler approaches
like multivariate initialization perform on par with these advanced methods
indicating that efficient large-scale multilingual continued pretraining can be
achieved even with simpler initialization methods.","[{'name': 'Nandini Mundra'}, {'name': 'Aditya Nanda Kishore'}, {'name': 'Raj Dabre'}, {'name': 'Ratish Puduppully'}, {'name': 'Anoop Kunchukuttan'}, {'name': 'Mitesh M. Khapra'}]",2024-07-08T11:38:49Z
http://arxiv.org/abs/2407.12850v1,http://arxiv.org/abs/2407.12850v1,Limits to Predicting Online Speech Using Large Language Models,"We study the predictability of online speech on social media, and whether
predictability improves with information outside a user's own posts. Recent
work suggests that the predictive information contained in posts written by a
user's peers can surpass that of the user's own posts. Motivated by the success
of large language models, we empirically test this hypothesis. We define
unpredictability as a measure of the model's uncertainty, i.e., its negative
log-likelihood on future tokens given context. As the basis of our study, we
collect a corpus of 6.25M posts from more than five thousand X (previously
Twitter) users and their peers. Across three large language models ranging in
size from 1 billion to 70 billion parameters, we find that predicting a user's
posts from their peers' posts performs poorly. Moreover, the value of the
user's own posts for prediction is consistently higher than that of their
peers'. Across the board, we find that the predictability of social media posts
remains low, comparable to predicting financial news without context. We extend
our investigation with a detailed analysis about the causes of unpredictability
and the robustness of our findings. Specifically, we observe that a significant
amount of predictive uncertainty comes from hashtags and @-mentions. Moreover,
our results replicate if instead of prompting the model with additional
context, we finetune on additional context.","[{'name': 'Mina Remeli'}, {'name': 'Moritz Hardt'}, {'name': 'Robert C. Williamson'}]",2024-07-08T09:50:49Z
http://arxiv.org/abs/2407.05750v2,http://arxiv.org/abs/2407.05750v2,Large Language Models Understand Layout,"Large language models (LLMs) demonstrate extraordinary abilities in a wide
range of natural language processing (NLP) tasks. In this paper, we show that,
beyond text understanding capability, LLMs are capable of processing text
layouts that are denoted by spatial markers. They are able to answer questions
that require explicit spatial perceiving and reasoning, while a drastic
performance drop is observed when the spatial markers from the original data
are excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,
Llama2 and ChatGLM3 models on various types of layout-sensitive datasets for
further analysis. The experimental results reveal that the layout understanding
ability of LLMs is mainly introduced by the coding data for pretraining, which
is further enhanced at the instruction-tuning stage. In addition, layout
understanding can be enhanced by integrating low-cost, auto-generated data
approached by a novel text game. Finally, we show that layout understanding
ability is beneficial for building efficient visual question-answering (VQA)
systems.","[{'name': 'Weiming Li'}, {'name': 'Manni Duan'}, {'name': 'Dong An'}, {'name': 'Yan Shao'}]",2024-07-08T09:03:12Z
http://arxiv.org/abs/2407.18332v1,http://arxiv.org/abs/2407.18332v1,"Analyzing Speech Unit Selection for Textless Speech-to-Speech
  Translation","Recent advancements in textless speech-to-speech translation systems have
been driven by the adoption of self-supervised learning techniques. Although
most state-of-the-art systems adopt a similar architecture to transform source
language speech into sequences of discrete representations in the target
language, the criteria for selecting these target speech units remains an open
question. This work explores the selection process through a study of
downstream tasks such as automatic speech recognition, speech synthesis,
speaker recognition, and emotion recognition. Interestingly, our findings
reveal a discrepancy in the optimization of discrete speech units: units that
perform well in resynthesis performance do not necessarily correlate with those
that enhance translation efficacy. This discrepancy underscores the nuanced
complexity of target feature selection and its impact on the overall
performance of speech-to-speech translation systems.","[{'name': 'Jarod Duret'}, {'name': 'Yannick Estève'}, {'name': 'Titouan Parcollet'}]",2024-07-08T08:53:26Z
http://arxiv.org/abs/2407.05740v2,http://arxiv.org/abs/2407.05740v2,Do Multilingual Large Language Models Mitigate Stereotype Bias?,"While preliminary findings indicate that multilingual LLMs exhibit reduced
bias compared to monolingual ones, a comprehensive understanding of the effect
of multilingual training on bias mitigation, is lacking. This study addresses
this gap by systematically training six LLMs of identical size (2.6B
parameters) and architecture: five monolingual models (English, German, French,
Italian, and Spanish) and one multilingual model trained on an equal
distribution of data across these languages, all using publicly available data.
To ensure robust evaluation, standard bias benchmarks were automatically
translated into the five target languages and verified for both translation
quality and bias preservation by human annotators. Our results consistently
demonstrate that multilingual training effectively mitigates bias. Moreover, we
observe that multilingual models achieve not only lower bias but also superior
prediction accuracy when compared to monolingual models with the same amount of
training data, model architecture, and size.","[{'name': 'Shangrui Nie'}, {'name': 'Michael Fromm'}, {'name': 'Charles Welch'}, {'name': 'Rebekka Görge'}, {'name': 'Akbar Karimi'}, {'name': 'Joan Plepi'}, {'name': 'Nazia Afsan Mowmita'}, {'name': 'Nicolas Flores-Herr'}, {'name': 'Mehdi Ali'}, {'name': 'Lucie Flek'}]",2024-07-08T08:46:50Z
http://arxiv.org/abs/2407.05734v1,http://arxiv.org/abs/2407.05734v1,Empirical Study of Symmetrical Reasoning in Conversational Chatbots,"This work explores the capability of conversational chatbots powered by large
language models (LLMs), to understand and characterize predicate symmetry, a
cognitive linguistic function traditionally believed to be an inherent human
trait. Leveraging in-context learning (ICL), a paradigm shift enabling chatbots
to learn new tasks from prompts without re-training, we assess the symmetrical
reasoning of five chatbots: ChatGPT 4, Huggingface chat AI, Microsoft's Copilot
AI, LLaMA through Perplexity, and Gemini Advanced. Using the Symmetry Inference
Sentence (SIS) dataset by Tanchip et al. (2020), we compare chatbot responses
against human evaluations to gauge their understanding of predicate symmetry.
Experiment results reveal varied performance among chatbots, with some
approaching human-like reasoning capabilities. Gemini, for example, reaches a
correlation of 0.85 with human scores, while providing a sounding justification
for each symmetry evaluation. This study underscores the potential and
limitations of LLMs in mirroring complex cognitive processes as symmetrical
reasoning.","[{'name': 'Daniela N. Rim'}, {'name': 'Heeyoul Choi'}]",2024-07-08T08:38:43Z
http://arxiv.org/abs/2407.05733v1,http://arxiv.org/abs/2407.05733v1,"Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative
  Judgment Approach Based on Rater Cognition","Large Language Models (LLMs) have shown promise in Automated Essay Scoring
(AES), but their zero-shot and few-shot performance often falls short compared
to state-of-the-art models and human raters. However, fine-tuning LLMs for each
specific task is impractical due to the variety of essay prompts and rubrics
used in real-world educational contexts. This study proposes a novel approach
combining LLMs and Comparative Judgment (CJ) for AES, using zero-shot prompting
to choose between two essays. We demonstrate that a CJ method surpasses
traditional rubric-based scoring in essay scoring using LLMs.","[{'name': 'Seungju Kim'}, {'name': 'Meounggun Jo'}]",2024-07-08T08:37:00Z
http://arxiv.org/abs/2407.05721v2,http://arxiv.org/abs/2407.05721v2,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,"Mental health has attracted substantial attention in recent years and LLM can
be an effective technology for alleviating this problem owing to its capability
in text understanding and dialogue. However, existing research in this domain
often suffers from limitations, such as training on datasets lacking crucial
prior knowledge and evidence, and the absence of comprehensive evaluation
methods. In this paper, we propose a specialized psychological large language
model (LLM), named PsycoLLM, trained on a proposed high-quality psychological
dataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.
Specifically, we construct multi-turn dialogues through a three-step pipeline
comprising generation, evidence judgment, and refinement. We augment this
process with real-world psychological case backgrounds extracted from online
platforms, enhancing the relevance and applicability of the generated data.
Additionally, to compare the performance of PsycoLLM with other LLMs, we
develop a comprehensive psychological benchmark based on authoritative
psychological counseling examinations in China, which includes assessments of
professional ethics, theoretical proficiency, and case analysis. The
experimental results on the benchmark illustrates the effectiveness of
PsycoLLM, which demonstrates superior performance compared to other LLMs.","[{'name': 'Jinpeng Hu'}, {'name': 'Tengteng Dong'}, {'name': 'Luo Gang'}, {'name': 'Hui Ma'}, {'name': 'Peng Zou'}, {'name': 'Xiao Sun'}, {'name': 'Dan Guo'}, {'name': 'Meng Wang'}]",2024-07-08T08:25:56Z
http://arxiv.org/abs/2407.05718v1,http://arxiv.org/abs/2407.05718v1,"A Factuality and Diversity Reconciled Decoding Method for
  Knowledge-Grounded Dialogue Generation","Grounding external knowledge can enhance the factuality of responses in
dialogue generation. However, excessive emphasis on it might result in the lack
of engaging and diverse expressions. Through the introduction of randomness in
sampling, current approaches can increase the diversity. Nevertheless, such
sampling method could undermine the factuality in dialogue generation. In this
study, to discover a solution for advancing creativity without relying on
questionable randomness and to subtly reconcile the factuality and diversity
within the source-grounded paradigm, a novel method named DoGe is proposed.
DoGe can dynamically alternate between the utilization of internal parameter
knowledge and external source knowledge based on the model's factual
confidence. Extensive experiments on three widely-used datasets show that DoGe
can not only enhance response diversity but also maintain factuality, and it
significantly surpasses other various decoding strategy baselines.","[{'name': 'Chenxu Yang'}, {'name': 'Zheng Lin'}, {'name': 'Chong Tian'}, {'name': 'Liang Pang'}, {'name': 'Lanrui Wang'}, {'name': 'Zhengyang Tong'}, {'name': 'Qirong Ho'}, {'name': 'Yanan Cao'}, {'name': 'Weiping Wang'}]",2024-07-08T08:23:11Z
http://arxiv.org/abs/2407.05700v1,http://arxiv.org/abs/2407.05700v1,"InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with
  Inverse-Instruct","Recent advancements in open-source code large language models (LLMs) have
demonstrated remarkable coding abilities by fine-tuning on the data generated
from powerful closed-source LLMs such as GPT-3.5 and GPT-4 for instruction
tuning. This paper explores how to further improve an instruction-tuned code
LLM by generating data from itself rather than querying closed-source LLMs. Our
key observation is the misalignment between the translation of formal and
informal languages: translating formal language (i.e., code) to informal
language (i.e., natural language) is more straightforward than the reverse.
Based on this observation, we propose INVERSE-INSTRUCT, which summarizes
instructions from code snippets instead of the reverse. Specifically, given an
instruction tuning corpus for code and the resulting instruction-tuned code
LLM, we ask the code LLM to generate additional high-quality instructions for
the original corpus through code summarization and self-evaluation. Then, we
fine-tune the base LLM on the combination of the original corpus and the
self-generated one, which yields a stronger instruction-tuned LLM. We present a
series of code LLMs named InverseCoder, which surpasses the performance of the
original code LLMs on a wide range of benchmarks, including Python text-to-code
generation, multilingual coding, and data-science code generation.","[{'name': 'Yutong Wu'}, {'name': 'Di Huang'}, {'name': 'Wenxuan Shi'}, {'name': 'Wei Wang'}, {'name': 'Lingzhe Gao'}, {'name': 'Shihao Liu'}, {'name': 'Ziyuan Nan'}, {'name': 'Kaizhao Yuan'}, {'name': 'Rui Zhang'}, {'name': 'Xishan Zhang'}, {'name': 'Zidong Du'}, {'name': 'Qi Guo'}, {'name': 'Yewen Pu'}, {'name': 'Dawei Yin'}, {'name': 'Xing Hu'}, {'name': 'Yunji Chen'}]",2024-07-08T08:00:05Z
http://arxiv.org/abs/2407.05674v1,http://arxiv.org/abs/2407.05674v1,"LLM-Based Open-Domain Integrated Task and Knowledge Assistants with
  Programmable Policies","Programming LLM-based knowledge and task assistants that faithfully conform
to developer-provided policies is challenging. These agents must retrieve and
provide consistent, accurate, and relevant information to address user's
queries and needs. Yet such agents generate unfounded responses
(""hallucinate""). Traditional dialogue trees can only handle a limited number of
conversation flows, making them inherently brittle. To this end, we present
KITA - a programmable framework for creating task-oriented conversational
agents that are designed to handle complex user interactions. Unlike LLMs, KITA
provides reliable grounded responses, with controllable agent policies through
its expressive specification, KITA Worksheet. In contrast to dialog trees, it
is resilient to diverse user queries, helpful with knowledge sources, and
offers ease of programming policies through its declarative paradigm. Through a
real-user study involving 62 participants, we show that KITA beats the GPT-4
with function calling baseline by 26.1, 22.5, and 52.4 points on execution
accuracy, dialogue act accuracy, and goal completion rate, respectively. We
also release 22 real-user conversations with KITA manually corrected to ensure
accuracy.","[{'name': 'Harshit Joshi'}, {'name': 'Shicheng Liu'}, {'name': 'James Chen'}, {'name': 'Robert Weigle'}, {'name': 'Monica S. Lam'}]",2024-07-08T07:17:40Z
http://arxiv.org/abs/2407.15857v1,http://arxiv.org/abs/2407.15857v1,"BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-task Large
  Language Models","This paper introduces Bayesian Hierarchical Low-Rank Adaption (BoRA), a novel
method for finetuning multi-task Large Language Models (LLMs). Current
finetuning approaches, such as Low-Rank Adaption (LoRA), perform exeptionally
well in reducing training parameters and memory usage but face limitations when
applied to multiple similar tasks. Practitioners usually have to choose between
training separate models for each task or a single model for all tasks, both of
which come with trade-offs in specialization and data utilization.
  BoRA addresses these trade-offs by leveraging a Bayesian hierarchical model
that allows tasks to share information through global hierarchical priors. This
enables tasks with limited data to benefit from the overall structure derived
from related tasks while allowing tasks with more data to specialize. Our
experimental results show that BoRA outperforms both individual and unified
model approaches, achieving lower perplexity and better generalization across
tasks. This method provides a scalable and efficient solution for multi-task
LLM finetuning, with significant practical implications for diverse
applications.","[{'name': 'Simen Eide'}, {'name': 'Arnoldo Frigessi'}]",2024-07-08T06:38:50Z
http://arxiv.org/abs/2407.05656v1,http://arxiv.org/abs/2407.05656v1,Multi-label Learning with Random Circular Vectors,"The extreme multi-label classification~(XMC) task involves learning a
classifier that can predict from a large label set the most relevant subset of
labels for a data instance. While deep neural networks~(DNNs) have demonstrated
remarkable success in XMC problems, the task is still challenging because it
must deal with a large number of output labels, which make the DNN training
computationally expensive. This paper addresses the issue by exploring the use
of random circular vectors, where each vector component is represented as a
complex amplitude. In our framework, we can develop an output layer and loss
function of DNNs for XMC by representing the final output layer as a fully
connected layer that directly predicts a low-dimensional circular vector
encoding a set of labels for a data instance. We conducted experiments on
synthetic datasets to verify that circular vectors have better label encoding
capacity and retrieval ability than normal real-valued vectors. Then, we
conducted experiments on actual XMC datasets and found that these appealing
properties of circular vectors contribute to significant improvements in task
performance compared with a previous model using random real-valued vectors,
while reducing the size of the output layers by up to 99%.","[{'name': 'Ken Nishida'}, {'name': 'Kojiro Machi'}, {'name': 'Kazuma Onishi'}, {'name': 'Katsuhiko Hayashi'}, {'name': 'Hidetaka Kamigaito'}]",2024-07-08T06:29:46Z
http://arxiv.org/abs/2407.05627v1,http://arxiv.org/abs/2407.05627v1,"New Directions in Text Classification Research: Maximizing The
  Performance of Sentiment Classification from Limited Data","The stakeholders' needs in sentiment analysis for various issues, whether
positive or negative, are speed and accuracy. One new challenge in sentiment
analysis tasks is the limited training data, which often leads to suboptimal
machine learning models and poor performance on test data. This paper discusses
the problem of text classification based on limited training data (300 to 600
samples) into three classes: positive, negative, and neutral. A benchmark
dataset is provided for training and testing data on the issue of Kaesang
Pangarep's appointment as Chairman of PSI. External data for aggregation and
augmentation purposes are provided, consisting of two datasets: the topic of
Covid Vaccination sentiment and an open topic. The official score used is the
F1-score, which balances precision and recall among the three classes,
positive, negative, and neutral. A baseline score is provided as a reference
for researchers for unoptimized classification methods. The optimized score is
provided as a reference for the target score to be achieved by any proposed
method. Both scoring (baseline and optimized) use the SVM method, which is
widely reported as the state-of-the-art in conventional machine learning
methods. The F1-scores achieved by the baseline and optimized methods are
40.83% and 51.28%, respectively.","[{'name': 'Surya Agustian'}, {'name': 'Muhammad Irfan Syah'}, {'name': 'Nurul Fatiara'}, {'name': 'Rahmad Abdillah'}]",2024-07-08T05:42:29Z
http://arxiv.org/abs/2407.05609v1,http://arxiv.org/abs/2407.05609v1,"Open-world Multi-label Text Classification with Extremely Weak
  Supervision","We study open-world multi-label text classification under extremely weak
supervision (XWS), where the user only provides a brief description for
classification objectives without any labels or ground-truth label space.
Similar single-label XWS settings have been explored recently, however, these
methods cannot be easily adapted for multi-label. We observe that (1) most
documents have a dominant class covering the majority of content and (2)
long-tail labels would appear in some documents as a dominant class. Therefore,
we first utilize the user description to prompt a large language model (LLM)
for dominant keyphrases of a subset of raw documents, and then construct a
(initial) label space via clustering. We further apply a zero-shot multi-label
classifier to locate the documents with small top predicted scores, so we can
revisit their dominant keyphrases for more long-tail labels. We iterate this
process to discover a comprehensive label space and construct a multi-label
classifier as a novel method, X-MLClass. X-MLClass exhibits a remarkable
increase in ground-truth label space coverage on various datasets, for example,
a 40% improvement on the AAPD dataset over topic modeling and keyword
extraction methods. Moreover, X-MLClass achieves the best end-to-end
multi-label classification accuracy.","[{'name': 'Xintong Li'}, {'name': 'Jinya Jiang'}, {'name': 'Ria Dharmani'}, {'name': 'Jayanth Srinivasa'}, {'name': 'Gaowen Liu'}, {'name': 'Jingbo Shang'}]",2024-07-08T04:52:49Z
http://arxiv.org/abs/2407.05608v1,http://arxiv.org/abs/2407.05608v1,A Benchmark for Multi-speaker Anonymization,"Privacy-preserving voice protection approaches primarily suppress
privacy-related information derived from paralinguistic attributes while
preserving the linguistic content. Existing solutions focus on single-speaker
scenarios. However, they lack practicality for real-world applications, i.e.,
multi-speaker scenarios. In this paper, we present an initial attempt to
provide a multi-speaker anonymization benchmark by defining the task and
evaluation protocol, proposing benchmarking solutions, and discussing the
privacy leakage of overlapping conversations. Specifically, ideal multi-speaker
anonymization should preserve the number of speakers and the turn-taking
structure of the conversation, ensuring accurate context conveyance while
maintaining privacy. To achieve that, a cascaded system uses speaker
diarization to aggregate the speech of each speaker and speaker anonymization
to conceal speaker privacy and preserve speech content. Additionally, we
propose two conversation-level speaker vector anonymization methods to improve
the utility further. Both methods aim to make the original and corresponding
pseudo-speaker identities of each speaker unlinkable while preserving or even
improving the distinguishability among pseudo-speakers in a conversation. The
first method minimizes the differential similarity across speaker pairs in the
original and anonymized conversations to maintain original speaker
relationships in the anonymized version. The other method minimizes the
aggregated similarity across anonymized speakers to achieve better
differentiation between speakers. Experiments conducted on both non-overlap
simulated and real-world datasets demonstrate the effectiveness of the
multi-speaker anonymization system with the proposed speaker anonymizers.
Additionally, we analyzed overlapping speech regarding privacy leakage and
provide potential solutions.","[{'name': 'Xiaoxiao Miao'}, {'name': 'Ruijie Tao'}, {'name': 'Chang Zeng'}, {'name': 'Xin Wang'}]",2024-07-08T04:48:43Z
http://arxiv.org/abs/2407.05489v1,http://arxiv.org/abs/2407.05489v1,How Effective are State Space Models for Machine Translation?,"Transformers are the current architecture of choice for NLP, but their
attention layers do not scale well to long contexts. Recent works propose to
replace attention with linear recurrent layers -- this is the case for state
space models, which enjoy efficient training and inference. However, it remains
unclear whether these models are competitive with transformers in machine
translation (MT). In this paper, we provide a rigorous and comprehensive
experimental comparison between transformers and linear recurrent models for
MT. Concretely, we experiment with RetNet, Mamba, and hybrid versions of Mamba
which incorporate attention mechanisms. Our findings demonstrate that Mamba is
highly competitive with transformers on sentence and paragraph-level datasets,
where in the latter both models benefit from shifting the training distribution
towards longer sequences. Further analysis show that integrating attention into
Mamba improves translation quality, robustness to sequence length
extrapolation, and the ability to recall named entities.","[{'name': 'Hugo Pitorro'}, {'name': 'Pavlo Vasylenko'}, {'name': 'Marcos Treviso'}, {'name': 'André F. T. Martins'}]",2024-07-07T20:21:49Z
http://arxiv.org/abs/2407.05483v1,http://arxiv.org/abs/2407.05483v1,Just read twice: closing the recall gap for recurrent language models,"Recurrent large language models that compete with Transformers in language
modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV).
Excitingly, these architectures use a constant amount of memory during
inference. However, due to the limited memory, recurrent LMs cannot recall and
use all the information in long contexts leading to brittle in-context learning
(ICL) quality. A key challenge for efficient LMs is selecting what information
to store versus discard. In this work, we observe the order in which
information is shown to the LM impacts the selection difficulty. To formalize
this, we show that the hardness of information recall reduces to the hardness
of a problem called set disjointness (SD), a quintessential problem in
communication complexity that requires a streaming algorithm (e.g., recurrent
model) to decide whether inputted sets are disjoint. We empirically and
theoretically show that the recurrent memory required to solve SD changes with
set order, i.e., whether the smaller set appears first in-context. Our analysis
suggests, to mitigate the reliance on data order, we can put information in the
right order in-context or process prompts non-causally. Towards that end, we
propose: (1) JRT-Prompt, where context gets repeated multiple times in the
prompt, effectively showing the model all data orders. This gives $11.0 \pm
1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL
tasks, with $11.9\times$ higher throughput than FlashAttention-2 for generation
prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2)
JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and
provides $99\%$ of Transformer quality at $360$M params., $30$B tokens and
$96\%$ at $1.3$B params., $50$B tokens on average across the tasks, with
$19.2\times$ higher throughput for prefill than FA2.","[{'name': 'Simran Arora'}, {'name': 'Aman Timalsina'}, {'name': 'Aaryan Singhal'}, {'name': 'Benjamin Spector'}, {'name': 'Sabri Eyuboglu'}, {'name': 'Xinyi Zhao'}, {'name': 'Ashish Rao'}, {'name': 'Atri Rudra'}, {'name': 'Christopher Ré'}]",2024-07-07T19:55:09Z
http://arxiv.org/abs/2407.05480v1,http://arxiv.org/abs/2407.05480v1,Biomedical Nested NER with Large Language Model and UMLS Heuristics,"In this paper, we present our system for the BioNNE English track, which aims
to extract 8 types of biomedical nested named entities from biomedical text. We
use a large language model (Mixtral 8x7B instruct) and ScispaCy NER model to
identify entities in an article and build custom heuristics based on unified
medical language system (UMLS) semantic types to categorize the entities. We
discuss the results and limitations of our system and propose future
improvements. Our system achieved an F1 score of 0.39 on the BioNNE validation
set and 0.348 on the test set.",[{'name': 'Wenxin Zhou'}],2024-07-07T19:37:40Z
http://arxiv.org/abs/2407.05474v1,http://arxiv.org/abs/2407.05474v1,"Enhancing Hallucination Detection through Perturbation-Based Synthetic
  Data Generation in System Responses","Detecting hallucinations in large language model (LLM) outputs is pivotal,
yet traditional fine-tuning for this classification task is impeded by the
expensive and quickly outdated annotation process, especially across numerous
vertical domains and in the face of rapid LLM advancements. In this study, we
introduce an approach that automatically generates both faithful and
hallucinated outputs by rewriting system responses. Experimental findings
demonstrate that a T5-base model, fine-tuned on our generated dataset,
surpasses state-of-the-art zero-shot detectors and existing synthetic
generation methods in both accuracy and latency, indicating efficacy of our
approach.","[{'name': 'Dongxu Zhang'}, {'name': 'Varun Gangal'}, {'name': 'Barrett Martin Lattimer'}, {'name': 'Yi Yang'}]",2024-07-07T19:19:32Z
http://arxiv.org/abs/2407.05464v1,http://arxiv.org/abs/2407.05464v1,"Experiments with truth using Machine Learning: Spectral analysis and
  explainable classification of synthetic, false, and genuine information","Misinformation is still a major societal problem and the arrival of Large
Language Models (LLMs) only added to it. This paper analyzes synthetic, false,
and genuine information in the form of text from spectral analysis,
visualization, and explainability perspectives to find the answer to why the
problem is still unsolved despite multiple years of research and a plethora of
solutions in the literature. Various embedding techniques on multiple datasets
are used to represent information for the purpose. The diverse spectral and
non-spectral methods used on these embeddings include t-distributed Stochastic
Neighbor Embedding (t-SNE), Principal Component Analysis (PCA), and Variational
Autoencoders (VAEs). Classification is done using multiple machine learning
algorithms. Local Interpretable Model-Agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), and Integrated Gradients are used for the
explanation of the classification. The analysis and the explanations generated
show that misinformation is quite closely intertwined with genuine information
and the machine learning algorithms are not as effective in separating the two
despite the claims in the literature.","[{'name': 'Vishnu S. Pendyala'}, {'name': 'Madhulika Dutta'}]",2024-07-07T18:31:09Z
http://arxiv.org/abs/2407.05463v1,http://arxiv.org/abs/2407.05463v1,Training Task Experts through Retrieval Based Distillation,"One of the most reliable ways to create deployable models for specialized
tasks is to obtain an adequate amount of high-quality task-specific data.
However, for specialized tasks, often such datasets do not exist. Existing
methods address this by creating such data from large language models (LLMs)
and then distilling such knowledge into smaller models. However, these methods
are limited by the quality of the LLMs output, and tend to generate repetitive
or incorrect data. In this work, we present Retrieval Based Distillation
(ReBase), a method that first retrieves data from rich online sources and then
transforms them into domain-specific data. This method greatly enhances data
diversity. Moreover, ReBase generates Chain-of-Thought reasoning and distills
the reasoning capacity of LLMs. We test our method on 4 benchmarks and results
show that our method significantly improves performance by up to 7.8% on SQuAD,
1.37% on MNLI, and 1.94% on BigBench-Hard.","[{'name': 'Jiaxin Ge'}, {'name': 'Xueying Jia'}, {'name': 'Vijay Viswanathan'}, {'name': 'Hongyin Luo'}, {'name': 'Graham Neubig'}]",2024-07-07T18:27:59Z
http://arxiv.org/abs/2407.05399v1,http://arxiv.org/abs/2407.05399v1,IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning,"Legal systems worldwide are inundated with exponential growth in cases and
documents. There is an imminent need to develop NLP and ML techniques for
automatically processing and understanding legal documents to streamline the
legal system. However, evaluating and comparing various NLP models designed
specifically for the legal domain is challenging. This paper addresses this
challenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding
and Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual
(9 Indian languages) domain-specific tasks that address different aspects of
the legal system from the point of view of understanding and reasoning over
Indian legal documents. We present baseline models (including LLM-based) for
each task, outlining the gap between models and the ground truth. To foster
further research in the legal domain, we create a leaderboard (available at:
https://exploration-lab.github.io/IL-TUR/) where the research community can
upload and compare legal text understanding systems.","[{'name': 'Abhinav Joshi'}, {'name': 'Shounak Paul'}, {'name': 'Akshat Sharma'}, {'name': 'Pawan Goyal'}, {'name': 'Saptarshi Ghosh'}, {'name': 'Ashutosh Modi'}]",2024-07-07T14:55:04Z
http://arxiv.org/abs/2407.05374v1,http://arxiv.org/abs/2407.05374v1,"Multimodal Prompt Learning with Missing Modalities for Sentiment
  Analysis and Emotion Recognition","The development of multimodal models has significantly advanced multimodal
sentiment analysis and emotion recognition. However, in real-world
applications, the presence of various missing modality cases often leads to a
degradation in the model's performance. In this work, we propose a novel
multimodal Transformer framework using prompt learning to address the issue of
missing modalities. Our method introduces three types of prompts: generative
prompts, missing-signal prompts, and missing-type prompts. These prompts enable
the generation of missing modality features and facilitate the learning of
intra- and inter-modality information. Through prompt learning, we achieve a
substantial reduction in the number of trainable parameters. Our proposed
method outperforms other methods significantly across all evaluation metrics.
Extensive experiments and ablation studies are conducted to demonstrate the
effectiveness and robustness of our method, showcasing its ability to
effectively handle missing modalities.","[{'name': 'Zirun Guo'}, {'name': 'Tao Jin'}, {'name': 'Zhou Zhao'}]",2024-07-07T13:55:56Z
http://arxiv.org/abs/2407.05361v2,http://arxiv.org/abs/2407.05361v2,"Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for
  Large-Scale Speech Generation","Recently, speech generation models have made significant progress by using
large-scale training data. However, the research community struggle to produce
highly spontaneous and human-like speech due to the lack of large-scale,
diverse, and spontaneous speech data. This paper present Emilia, the first
multilingual speech generation dataset from in-the-wild speech data, and
Emilia-Pipe, the first open-source preprocessing pipeline designed to transform
in-the-wild speech data into high-quality training data with annotations for
speech generation. Emilia starts with over 101k hours of speech in six
languages and features diverse speech with varied speaking styles. To
facilitate the scale-up of Emilia, the open-source pipeline Emilia-Pipe can
process one hour of raw speech data ready for model training in a few mins,
which enables the research community to collaborate on large-scale speech
generation research. Experimental results validate the effectiveness of Emilia.
Demos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.","[{'name': 'Haorui He'}, {'name': 'Zengqiang Shang'}, {'name': 'Chaoren Wang'}, {'name': 'Xuyuan Li'}, {'name': 'Yicheng Gu'}, {'name': 'Hua Hua'}, {'name': 'Liwei Liu'}, {'name': 'Chen Yang'}, {'name': 'Jiaqi Li'}, {'name': 'Peiyang Shi'}, {'name': 'Yuancheng Wang'}, {'name': 'Kai Chen'}, {'name': 'Pengyuan Zhang'}, {'name': 'Zhizheng Wu'}]",2024-07-07T13:24:54Z
http://arxiv.org/abs/2407.05355v1,http://arxiv.org/abs/2407.05355v1,VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool,"Multimodal large language models (MLLMs) are flourishing, but mainly focus on
images with less attention than videos, especially in sub-fields such as prompt
engineering, video chain-of-thought (CoT), and instruction tuning on videos.
Therefore, we try to explore the collection of CoT datasets in videos to lead
to video OpenQA and improve the reasoning ability of MLLMs. Unfortunately,
making such video CoT datasets is not an easy task. Given that human annotation
is too cumbersome and expensive, while machine-generated is not reliable due to
the hallucination issue, we develop an automatic annotation tool that combines
machine and human experts, under the active learning paradigm. Active learning
is an interactive strategy between the model and human experts, in this way,
the workload of human labeling can be reduced and the quality of the dataset
can be guaranteed. With the help of the automatic annotation tool, we strive to
contribute three datasets, namely VideoCoT, TopicQA, TopicCoT. Furthermore, we
propose a simple but effective benchmark based on the collected datasets, which
exploits CoT to maximize the complex reasoning capabilities of MLLMs. Extensive
experiments demonstrate the effectiveness our solution.","[{'name': 'Yan Wang'}, {'name': 'Yawen Zeng'}, {'name': 'Jingsheng Zheng'}, {'name': 'Xiaofen Xing'}, {'name': 'Jin Xu'}, {'name': 'Xiangmin Xu'}]",2024-07-07T13:10:23Z
http://arxiv.org/abs/2407.05327v1,http://arxiv.org/abs/2407.05327v1,"Can Model Uncertainty Function as a Proxy for Multiple-Choice Question
  Item Difficulty?","Estimating the difficulty of multiple-choice questions would be great help
for educators who must spend substantial time creating and piloting stimuli for
their tests, and for learners who want to practice. Supervised approaches to
difficulty estimation have yielded to date mixed results. In this contribution
we leverage an aspect of generative large models which might be seen as a
weakness when answering questions, namely their uncertainty, and exploit it
towards exploring correlations between two different metrics of uncertainty,
and the actual student response distribution. While we observe some present but
weak correlations, we also discover that the models' behaviour is different in
the case of correct vs wrong answers, and that correlations differ
substantially according to the different question types which are included in
our fine-grained, previously unused dataset of 451 questions from a
Biopsychology course. In discussing our findings, we also suggest potential
avenues to further leverage model uncertainty as an additional proxy for item
difficulty.","[{'name': 'Leonidas Zotos'}, {'name': 'Hedderik van Rijn'}, {'name': 'Malvina Nissim'}]",2024-07-07T10:48:04Z
http://arxiv.org/abs/2407.05319v1,http://arxiv.org/abs/2407.05319v1,Rethinking Targeted Adversarial Attacks For Neural Machine Translation,"Targeted adversarial attacks are widely used to evaluate the robustness of
neural machine translation systems. Unfortunately, this paper first identifies
a critical issue in the existing settings of NMT targeted adversarial attacks,
where their attacking results are largely overestimated. To this end, this
paper presents a new setting for NMT targeted adversarial attacks that could
lead to reliable attacking results. Under the new setting, it then proposes a
Targeted Word Gradient adversarial Attack (TWGA) method to craft adversarial
examples. Experimental results demonstrate that our proposed setting could
provide faithful attacking results for targeted adversarial attacks on NMT
systems, and the proposed TWGA method can effectively attack such victim NMT
systems. In-depth analyses on a large-scale dataset further illustrate some
valuable findings. 1 Our code and data are available at
https://github.com/wujunjie1998/TWGA.","[{'name': 'Junjie Wu'}, {'name': 'Lemao Liu'}, {'name': 'Wei Bi'}, {'name': 'Dit-Yan Yeung'}]",2024-07-07T10:16:06Z
http://arxiv.org/abs/2407.05250v1,http://arxiv.org/abs/2407.05250v1,CLIMB: A Benchmark of Clinical Bias in Large Language Models,"Large language models (LLMs) are increasingly applied to clinical
decision-making. However, their potential to exhibit bias poses significant
risks to clinical equity. Currently, there is a lack of benchmarks that
systematically evaluate such clinical bias in LLMs. While in downstream tasks,
some biases of LLMs can be avoided such as by instructing the model to answer
""I'm not sure..."", the internal bias hidden within the model still lacks deep
studies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in
Large Language Models), a pioneering comprehensive benchmark to evaluate both
intrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for
clinical decision tasks. Notably, for intrinsic bias, we introduce a novel
metric, AssocMAD, to assess the disparities of LLMs across multiple demographic
groups. Additionally, we leverage counterfactual intervention to evaluate
extrinsic bias in a task of clinical diagnosis prediction. Our experiments
across popular and medically adapted LLMs, particularly from the Mistral and
LLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic
bias. This work underscores the critical need to mitigate clinical bias and
sets a new standard for future evaluations of LLMs' clinical bias.","[{'name': 'Yubo Zhang'}, {'name': 'Shudi Hou'}, {'name': 'Mingyu Derek Ma'}, {'name': 'Wei Wang'}, {'name': 'Muhao Chen'}, {'name': 'Jieyu Zhao'}]",2024-07-07T03:41:51Z
http://arxiv.org/abs/2407.05244v1,http://arxiv.org/abs/2407.05244v1,"Some Issues in Predictive Ethics Modeling: An Annotated Contrast Set of
  ""Moral Stories""","Models like Delphi have been able to label ethical dilemmas as moral or
immoral with astonishing accuracy. This paper challenges accuracy as a holistic
metric for ethics modeling by identifying issues with translating moral
dilemmas into text-based input. It demonstrates these issues with contrast sets
that substantially reduce the performance of classifiers trained on the dataset
Moral Stories. Ultimately, we obtain concrete estimates for how much specific
forms of data misrepresentation harm classifier accuracy. Specifically,
label-changing tweaks to the descriptive content of a situation (as small as
3-5 words) can reduce classifier accuracy to as low as 51%, almost half the
initial accuracy of 99.8%. Associating situations with a misleading social norm
lowers accuracy to 98.8%, while adding textual bias (i.e. an implication that a
situation already fits a certain label) lowers accuracy to 77%.
  These results suggest not only that many ethics models have substantially
overfit, but that several precautions are required to ensure that input
accurately captures a moral dilemma. This paper recommends re-examining the
structure of a social norm, training models to ask for context with defeasible
reasoning, and filtering input for textual bias. Doing so not only gives us the
first concrete estimates of the average cost to accuracy of misrepresenting
ethics data, but gives researchers practical tips for considering these
estimates in research.",[{'name': 'Ben Fitzgerald'}],2024-07-07T03:22:49Z
http://arxiv.org/abs/2407.05233v1,http://arxiv.org/abs/2407.05233v1,"Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of
  Gemma-2b-it and Phi2 Models","Prompt recovery, a crucial task in natural language processing, entails the
reconstruction of prompts or instructions that language models use to convert
input text into a specific output. Although pivotal, the design and
effectiveness of prompts represent a challenging and relatively untapped field
within NLP research. This paper delves into an exhaustive investigation of
prompt recovery methodologies, employing a spectrum of pre-trained language
models and strategies. Our study is a comparative analysis aimed at gauging the
efficacy of various models on a benchmark dataset, with the goal of pinpointing
the most proficient approach for prompt recovery. Through meticulous
experimentation and detailed analysis, we elucidate the outstanding performance
of the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its
counterparts, showcasing its exceptional capability in accurately
reconstructing prompts for text transformation tasks. Our findings offer a
significant contribution to the existing knowledge on prompt recovery, shedding
light on the intricacies of prompt design and offering insightful perspectives
for future innovations in text rewriting and the broader field of natural
language processing.","[{'name': 'Jianlong Chen'}, {'name': 'Wei Xu'}, {'name': 'Zhicheng Ding'}, {'name': 'Jinxin Xu'}, {'name': 'Hao Yan'}, {'name': 'Xinyu Zhang'}]",2024-07-07T02:15:26Z
http://arxiv.org/abs/2407.05219v2,http://arxiv.org/abs/2407.05219v2,Flood of Techniques and Drought of Theories: Emotion Mining in Disasters,"Emotion mining has become a crucial tool for understanding human emotions
during disasters, leveraging the extensive data generated on social media
platforms. This paper aims to summarize existing research on emotion mining
within disaster contexts, highlighting both significant discoveries and
persistent issues. On the one hand, emotion mining techniques have achieved
acceptable accuracy enabling applications such as rapid damage assessment and
mental health surveillance. On the other hand, with many studies adopting
data-driven approaches, several methodological issues remain. These include
arbitrary emotion classification, ignoring biases inherent in data collection
from social media, such as the overrepresentation of individuals from higher
socioeconomic status on Twitter, and the lack of application of theoretical
frameworks like cross-cultural comparisons. These problems can be summarized as
a notable lack of theory-driven research and ignoring insights from social and
behavioral sciences. This paper underscores the need for interdisciplinary
collaboration between computer scientists and social scientists to develop more
robust and theoretically grounded approaches in emotion mining. By addressing
these gaps, we aim to enhance the effectiveness and reliability of emotion
mining methodologies, ultimately contributing to improved disaster
preparedness, response, and recovery.
  Keywords: emotion mining, sentiment analysis, natural disasters, psychology,
technological disasters","[{'name': 'Soheil Shapouri'}, {'name': 'Saber Soleymani'}, {'name': 'Saed Rezayi'}]",2024-07-07T00:43:05Z
http://arxiv.org/abs/2407.05216v1,http://arxiv.org/abs/2407.05216v1,"Large Language Model as an Assignment Evaluator: Insights, Feedback, and
  Challenges in a 1000+ Student Course","Using large language models (LLMs) for automatic evaluation has become an
important evaluation method in NLP research. However, it is unclear whether
these LLM-based evaluators can be applied in real-world classrooms to assess
student assignments. This empirical report shares how we use GPT-4 as an
automatic assignment evaluator in a university course with 1,028 students.
Based on student responses, we find that LLM-based assignment evaluators are
generally acceptable to students when students have free access to these
LLM-based evaluators. However, students also noted that the LLM sometimes fails
to adhere to the evaluation instructions. Additionally, we observe that
students can easily manipulate the LLM-based evaluator to output specific
strings, allowing them to achieve high scores without meeting the assignment
rubric. Based on student feedback and our experience, we provide several
recommendations for integrating LLM-based evaluators into future classrooms.","[{'name': 'Cheng-Han Chiang'}, {'name': 'Wei-Chih Chen'}, {'name': 'Chun-Yi Kuan'}, {'name': 'Chienchou Yang'}, {'name': 'Hung-yi Lee'}]",2024-07-07T00:17:24Z
http://arxiv.org/abs/2407.05213v1,http://arxiv.org/abs/2407.05213v1,"BadCLM: Backdoor Attack in Clinical Language Models for Electronic
  Health Records","The advent of clinical language models integrated into electronic health
records (EHR) for clinical decision support has marked a significant
advancement, leveraging the depth of clinical notes for improved
decision-making. Despite their success, the potential vulnerabilities of these
models remain largely unexplored. This paper delves into the realm of backdoor
attacks on clinical language models, introducing an innovative attention-based
backdoor attack method, BadCLM (Bad Clinical Language Models). This technique
clandestinely embeds a backdoor within the models, causing them to produce
incorrect predictions when a pre-defined trigger is present in inputs, while
functioning accurately otherwise. We demonstrate the efficacy of BadCLM through
an in-hospital mortality prediction task with MIMIC III dataset, showcasing its
potential to compromise model integrity. Our findings illuminate a significant
security risk in clinical decision support systems and pave the way for future
endeavors in fortifying clinical language models against such vulnerabilities.","[{'name': 'Weimin Lyu'}, {'name': 'Zexin Bi'}, {'name': 'Fusheng Wang'}, {'name': 'Chao Chen'}]",2024-07-06T23:56:43Z
http://arxiv.org/abs/2407.05194v1,http://arxiv.org/abs/2407.05194v1,"LLMCloudHunter: Harnessing LLMs for Automated Extraction of Detection
  Rules from Cloud-Based CTI","As the number and sophistication of cyber attacks have increased, threat
hunting has become a critical aspect of active security, enabling proactive
detection and mitigation of threats before they cause significant harm.
Open-source cyber threat intelligence (OS-CTI) is a valuable resource for
threat hunters, however, it often comes in unstructured formats that require
further manual analysis. Previous studies aimed at automating OSCTI analysis
are limited since (1) they failed to provide actionable outputs, (2) they did
not take advantage of images present in OSCTI sources, and (3) they focused on
on-premises environments, overlooking the growing importance of cloud
environments. To address these gaps, we propose LLMCloudHunter, a novel
framework that leverages large language models (LLMs) to automatically generate
generic-signature detection rule candidates from textual and visual OSCTI data.
We evaluated the quality of the rules generated by the proposed framework using
12 annotated real-world cloud threat reports. The results show that our
framework achieved a precision of 92% and recall of 98% for the task of
accurately extracting API calls made by the threat actor and a precision of 99%
with a recall of 98% for IoCs. Additionally, 99.18% of the generated detection
rule candidates were successfully compiled and converted into Splunk queries.","[{'name': 'Yuval Schwartz'}, {'name': 'Lavi Benshimol'}, {'name': 'Dudu Mimran'}, {'name': 'Yuval Elovici'}, {'name': 'Asaf Shabtai'}]",2024-07-06T21:43:35Z
http://arxiv.org/abs/2407.05189v1,http://arxiv.org/abs/2407.05189v1,"Enhancing Language Learning through Technology: Introducing a New
  English-Azerbaijani (Arabic Script) Parallel Corpus","This paper introduces a pioneering English-Azerbaijani (Arabic Script)
parallel corpus, designed to bridge the technological gap in language learning
and machine translation (MT) for under-resourced languages. Consisting of
548,000 parallel sentences and approximately 9 million words per language, this
dataset is derived from diverse sources such as news articles and holy texts,
aiming to enhance natural language processing (NLP) applications and language
education technology. This corpus marks a significant step forward in the realm
of linguistic resources, particularly for Turkic languages, which have lagged
in the neural machine translation (NMT) revolution. By presenting the first
comprehensive case study for the English-Azerbaijani (Arabic Script) language
pair, this work underscores the transformative potential of NMT in low-resource
contexts. The development and utilization of this corpus not only facilitate
the advancement of machine translation systems tailored for specific linguistic
needs but also promote inclusive language learning through technology. The
findings demonstrate the corpus's effectiveness in training deep learning MT
systems and underscore its role as an essential asset for researchers and
educators aiming to foster bilingual education and multilingual communication.
This research covers the way for future explorations into NMT applications for
languages lacking substantial digital resources, thereby enhancing global
language education frameworks. The Python package of our code is available at
https://pypi.org/project/chevir-kartalol/, and we also have a website
accessible at https://translate.kartalol.com/.","[{'name': 'Jalil Nourmohammadi Khiarak'}, {'name': 'Ammar Ahmadi'}, {'name': 'Taher Ak-bari Saeed'}, {'name': 'Meysam Asgari-Chenaghlu'}, {'name': 'Toğrul Atabay'}, {'name': 'Mohammad Reza Baghban Karimi'}, {'name': 'Ismail Ceferli'}, {'name': 'Farzad Hasanvand'}, {'name': 'Seyed Mahboub Mousavi'}, {'name': 'Morteza Noshad'}]",2024-07-06T21:23:20Z
http://arxiv.org/abs/2407.05154v1,http://arxiv.org/abs/2407.05154v1,"Identifying Intensity of the Structure and Content in Tweets and the
  Discriminative Power of Attributes in Context with Referential Translation
  Machines","We use referential translation machines (RTMs) to identify the similarity
between an attribute and two words in English by casting the task as machine
translation performance prediction (MTPP) between the words and the attribute
word and the distance between their similarities for Task 10 with stacked RTM
models. RTMs are also used to predict the intensity of the structure and
content in tweets in English, Arabic, and Spanish in Task 1 where MTPP is
between the tweets and the set of words for the emotion selected from WordNet
affect emotion lists. Stacked RTM models obtain encouraging results in both.",[{'name': 'Ergun Biçici'}],2024-07-06T18:58:10Z
http://arxiv.org/abs/2407.05134v1,http://arxiv.org/abs/2407.05134v1,"Solving for X and Beyond: Can Large Language Models Solve Complex Math
  Problems with More-Than-Two Unknowns?","Large Language Models (LLMs) have demonstrated remarkable performance in
solving math problems, a hallmark of human intelligence. Despite high success
rates on current benchmarks; however, these often feature simple problems with
only one or two unknowns, which do not sufficiently challenge their reasoning
capacities. This paper introduces a novel benchmark, BeyondX, designed to
address these limitations by incorporating problems with multiple unknowns.
Recognizing the challenges in proposing multi-unknown problems from scratch, we
developed BeyondX using an innovative automated pipeline that progressively
increases complexity by expanding the number of unknowns in simpler problems.
Empirical study on BeyondX reveals that the performance of existing LLMs, even
those fine-tuned specifically on math tasks, significantly decreases as the
number of unknowns increases - with a performance drop of up to 70\% observed
in GPT-4. To tackle these challenges, we propose the Formulate-and-Solve
strategy, a generalized prompting approach that effectively handles problems
with an arbitrary number of unknowns. Our findings reveal that this strategy
not only enhances LLM performance on the BeyondX benchmark but also provides
deeper insights into the computational limits of LLMs when faced with more
complex mathematical challenges.","[{'name': 'Kuei-Chun Kao'}, {'name': 'Ruochen Wang'}, {'name': 'Cho-Jui Hsieh'}]",2024-07-06T17:01:04Z
http://arxiv.org/abs/2407.05131v1,http://arxiv.org/abs/2407.05131v1,"RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language
  Models","The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has
enhanced medical diagnosis. However, current Med-LVLMs frequently encounter
factual issues, often generating responses that do not align with established
medical facts. Retrieval-Augmented Generation (RAG), which utilizes external
knowledge, can improve the factual accuracy of these models but introduces two
major challenges. First, limited retrieved contexts might not cover all
necessary information, while excessive retrieval can introduce irrelevant and
inaccurate references, interfering with the model's generation. Second, in
cases where the model originally responds correctly, applying RAG can lead to
an over-reliance on retrieved contexts, resulting in incorrect answers. To
address these issues, we propose RULE, which consists of two components. First,
we introduce a provably effective strategy for controlling factuality risk
through the calibrated selection of the number of retrieved contexts. Second,
based on samples where over-reliance on retrieved contexts led to errors, we
curate a preference dataset to fine-tune the model, balancing its dependence on
inherent knowledge and retrieved contexts for generation. We demonstrate the
effectiveness of RULE on three medical VQA datasets, achieving an average
improvement of 20.8% in factual accuracy. We publicly release our benchmark and
code in https://github.com/richard-peng-xia/RULE.","[{'name': 'Peng Xia'}, {'name': 'Kangyu Zhu'}, {'name': 'Haoran Li'}, {'name': 'Hongtu Zhu'}, {'name': 'Yun Li'}, {'name': 'Gang Li'}, {'name': 'Linjun Zhang'}, {'name': 'Huaxiu Yao'}]",2024-07-06T16:45:07Z
http://arxiv.org/abs/2407.05116v1,http://arxiv.org/abs/2407.05116v1,Automatic Prediction of the Performance of Every Parser,"We present a new parser performance prediction (PPP) model using machine
translation performance prediction system (MTPPS), statistically independent of
any language or parser, relying only on extrinsic and novel features based on
textual, link structural, and bracketing tree structural information. This new
system, MTPPS-PPP, can predict the performance of any parser in any language
and can be useful for estimating the grammatical difficulty when understanding
a given text, for setting expectations from parsing output, for parser
selection for a specific domain, and for parser combination systems. We obtain
SoA results in PPP of bracketing $F_1$ with better results over textual
features and similar performance with previous results that use parser and
linguistic label specific information. Our results show the contribution of
different types of features as well as rankings of individual features in
different experimental settings (cased vs. uncased), in different learning
tasks (in-domain vs. out-of-domain), with different training sets, with
different learning algorithms, and with different dimensionality reduction
techniques. We achieve $0.0678$ MAE and $0.85$ RAE in setting +Link, which
corresponds to about $7.4\%$ error when predicting the bracketing $F_1$ score
for the Charniak and Johnson parser on the WSJ23 test set. MTPPS-PPP system can
predict without parsing using only the text, without a supervised parser using
only an unsupervised parser, without any parser or language dependent
information, without using a reference parser output, and can be used to
predict the performance of any parser in any language.",[{'name': 'Ergun Biçici'}],2024-07-06T15:49:24Z
http://arxiv.org/abs/2407.05054v1,http://arxiv.org/abs/2407.05054v1,"Cross-Lingual Word Alignment for ASEAN Languages with Contrastive
  Learning","Cross-lingual word alignment plays a crucial role in various natural language
processing tasks, particularly for low-resource languages. Recent study
proposes a BiLSTM-based encoder-decoder model that outperforms pre-trained
language models in low-resource settings. However, their model only considers
the similarity of word embedding spaces and does not explicitly model the
differences between word embeddings. To address this limitation, we propose
incorporating contrastive learning into the BiLSTM-based encoder-decoder
framework. Our approach introduces a multi-view negative sampling strategy to
learn the differences between word pairs in the shared cross-lingual embedding
space. We evaluate our model on five bilingual aligned datasets spanning four
ASEAN languages: Lao, Vietnamese, Thai, and Indonesian. Experimental results
demonstrate that integrating contrastive learning consistently improves word
alignment accuracy across all datasets, confirming the effectiveness of the
proposed method in low-resource scenarios. We will release our data set and
code to support future research on ASEAN or more low-resource word alignment.","[{'name': 'Jingshen Zhang'}, {'name': 'Xinying Qiu'}, {'name': 'Teng Shen'}, {'name': 'Wenyu Wang'}, {'name': 'Kailin Zhang'}, {'name': 'Wenhe Feng'}]",2024-07-06T11:56:41Z
http://arxiv.org/abs/2407.05036v1,http://arxiv.org/abs/2407.05036v1,Enhance the Robustness of Text-Centric Multimodal Alignments,"Converting different modalities into general text, serving as input prompts
for large language models (LLMs), is a common method to align multimodal models
when there is limited pairwise data. This text-centric approach leverages the
unique properties of text as a modality space, transforming diverse inputs into
a unified textual representation. This enables downstream models to effectively
interpret various modal inputs. This study assesses the quality and robustness
of multimodal representations in the presence of missing entries, noise, or
absent modalities, revealing that current text-centric alignment methods
compromise downstream robustness. To address this issue, we propose a new
text-centric approach that achieves superior robustness compared to previous
methods across various modalities in different settings. Our findings highlight
the potential of this approach to enhance the robustness and adaptability of
multimodal representations, offering a promising solution for dynamic and
real-world applications.","[{'name': 'Ting-Yu Yen'}, {'name': 'Yun-Da Tsai'}, {'name': 'Keng-Te Liao'}, {'name': 'Shou-De Lin'}]",2024-07-06T10:12:29Z
http://arxiv.org/abs/2407.05022v1,http://arxiv.org/abs/2407.05022v1,A Principled Framework for Evaluating on Typologically Diverse Languages,"Beyond individual languages, multilingual natural language processing (NLP)
research increasingly aims to develop models that perform well across languages
generally. However, evaluating these systems on all the world's languages is
practically infeasible. To attain generalizability, representative language
sampling is essential. Previous work argues that generalizable multilingual
evaluation sets should contain languages with diverse typological properties.
However, 'typologically diverse' language samples have been found to vary
considerably in this regard, and popular sampling methods are flawed and
inconsistent. We present a language sampling framework for selecting highly
typologically diverse languages given a sampling frame, informed by language
typology. We compare sampling methods with a range of metrics and find that our
systematic methods consistently retrieve more typologically diverse language
selections than previous methods in NLP. Moreover, we provide evidence that
this affects generalizability in multilingual model evaluation, emphasizing the
importance of diverse language sampling in NLP evaluation.","[{'name': 'Esther Ploeger'}, {'name': 'Wessel Poelman'}, {'name': 'Andreas Holck Høeg-Petersen'}, {'name': 'Anders Schlichtkrull'}, {'name': 'Miryam de Lhoneux'}, {'name': 'Johannes Bjerva'}]",2024-07-06T09:31:02Z
http://arxiv.org/abs/2407.05015v1,http://arxiv.org/abs/2407.05015v1,"How do you know that? Teaching Generative Language Models to Reference
  Answers to Biomedical Questions","Large language models (LLMs) have recently become the leading source of
answers for users' questions online. Despite their ability to offer eloquent
answers, their accuracy and reliability can pose a significant challenge. This
is especially true for sensitive domains such as biomedicine, where there is a
higher need for factually correct answers. This paper introduces a biomedical
retrieval-augmented generation (RAG) system designed to enhance the reliability
of generated responses. The system is based on a fine-tuned LLM for the
referenced question-answering, where retrieved relevant abstracts from PubMed
are passed to LLM's context as input through a prompt. Its output is an answer
based on PubMed abstracts, where each statement is referenced accordingly,
allowing the users to verify the answer. Our retrieval system achieves an
absolute improvement of 23% compared to the PubMed search engine. Based on the
manual evaluation on a small sample, our fine-tuned LLM component achieves
comparable results to GPT-4 Turbo in referencing relevant abstracts. We make
the dataset used to fine-tune the models and the fine-tuned models based on
Mistral-7B-instruct-v0.1 and v0.2 publicly available.","[{'name': 'Bojana Bašaragin'}, {'name': 'Adela Ljajić'}, {'name': 'Darija Medvecki'}, {'name': 'Lorenzo Cassano'}, {'name': 'Miloš Košprdić'}, {'name': 'Nikola Milošević'}]",2024-07-06T09:10:05Z
http://arxiv.org/abs/2407.05013v1,http://arxiv.org/abs/2407.05013v1,Progress or Regress? Self-Improvement Reversal in Post-training,"Self-improvement through post-training methods such as iterative preference
learning has been acclaimed for enhancing the problem-solving capabilities
(e.g., mathematical reasoning) of Large Language Models (LLMs) without human
intervention. However, as exploration deepens, it becomes crucial to assess
whether these improvements genuinely signify progress in solving more
challenging problems or if they could lead to unintended regressions. To
address this, we propose a comprehensive evaluative framework that goes beyond
the superficial pass@1 metric to scrutinize the underlying enhancements of
post-training paradigms for self-improvement. Through rigorous experimentation
and analysis across diverse problem-solving tasks, the empirical results point
out the phenomenon of \emph{self-improvement reversal}, where models showing
improved performance across benchmarks will paradoxically exhibit declines in
broader, essential capabilities, like output diversity and out-of-distribution
(OOD) generalization. These findings indicate that current self-improvement
practices through post-training are inadequate for equipping models to tackle
more complex problems. Furthermore, they underscore the necessity of our
critical evaluation metrics in discerning the \emph{progress or regress}
dichotomy for self-improving LLMs.","[{'name': 'Ting Wu'}, {'name': 'Xuefeng Li'}, {'name': 'Pengfei Liu'}]",2024-07-06T09:07:11Z
http://arxiv.org/abs/2407.18369v1,http://arxiv.org/abs/2407.18369v1,AI Safety in Generative AI Large Language Models: A Survey,"Large Language Model (LLMs) such as ChatGPT that exhibit generative AI
capabilities are facing accelerated adoption and innovation. The increased
presence of Generative AI (GAI) inevitably raises concerns about the risks and
safety associated with these models. This article provides an up-to-date survey
of recent trends in AI safety research of GAI-LLMs from a computer scientist's
perspective: specific and technical. In this survey, we explore the background
and motivation for the identified harms and risks in the context of LLMs being
generative language models; our survey differentiates by emphasising the need
for unified theories of the distinct safety challenges in the research
development and applications of LLMs. We start our discussion with a concise
introduction to the workings of LLMs, supported by relevant literature. Then we
discuss earlier research that has pointed out the fundamental constraints of
generative models, or lack of understanding thereof (e.g., performance and
safety trade-offs as LLMs scale in number of parameters). We provide a
sufficient coverage of LLM alignment -- delving into various approaches,
contending methods and present challenges associated with aligning LLMs with
human preferences. By highlighting the gaps in the literature and possible
implementation oversights, our aim is to create a comprehensive analysis that
provides insights for addressing AI safety in LLMs and encourages the
development of aligned and secure models. We conclude our survey by discussing
future directions of LLMs for AI safety, offering insights into ongoing
research in this critical area.","[{'name': 'Jaymari Chua'}, {'name': 'Yun Li'}, {'name': 'Shiyi Yang'}, {'name': 'Chen Wang'}, {'name': 'Lina Yao'}]",2024-07-06T09:00:18Z
http://arxiv.org/abs/2407.04990v1,http://arxiv.org/abs/2407.04990v1,"Conditional Semi-Supervised Data Augmentation for Spam Message Detection
  with Low Resource Data","Several machine learning schemes have attempted to perform the detection of
spam messages. However, those schemes mostly require a huge amount of labeled
data. The existing techniques addressing the lack of data availability have
issues with effectiveness and robustness. Therefore, this paper proposes a
conditional semi-supervised data augmentation (CSSDA) for a spam detection
model lacking the availability of data. The main architecture of CSSDA
comprises feature extraction and enhanced generative network. Here, we exploit
unlabeled data for data augmentation to extend training data. The enhanced
generative in our proposed scheme produces latent variables as fake samples
from unlabeled data through a conditional scheme. Latent variables can come
from labeled and unlabeled data as the input for the final classifier in our
spam detection model. The experimental results indicate that our proposed CSSDA
achieves excellent results compared to several related methods both exploiting
unlabeled data and not. In the experiment stage with various amounts of
unlabeled data, CSSDA is the only robust model that obtains a balanced accuracy
of about 85% when the availability of labeled data is large. We also conduct
several ablation studies to investigate our proposed scheme in detail. The
result also shows that several ablation studies strengthen our proposed
innovations. These experiments indicate that unlabeled data has a significant
contribution to data augmentation using the conditional semi-supervised scheme
for spam detection.","[{'name': 'Ulin Nuha'}, {'name': 'Chih-Hsueh Lin'}]",2024-07-06T07:51:24Z
http://arxiv.org/abs/2407.04981v1,http://arxiv.org/abs/2407.04981v1,"TRACE: TRansformer-based Attribution using Contrastive Embeddings in
  LLMs","The rapid evolution of large language models (LLMs) represents a substantial
leap forward in natural language understanding and generation. However,
alongside these advancements come significant challenges related to the
accountability and transparency of LLM responses. Reliable source attribution
is essential to adhering to stringent legal and regulatory standards, including
those set forth by the General Data Protection Regulation. Despite the
well-established methods in source attribution within the computer vision
domain, the application of robust attribution frameworks to natural language
processing remains underexplored. To bridge this gap, we propose a novel and
versatile TRansformer-based Attribution framework using Contrastive Embeddings
called TRACE that, in particular, exploits contrastive learning for source
attribution. We perform an extensive empirical evaluation to demonstrate the
performance and efficiency of TRACE in various settings and show that TRACE
significantly improves the ability to attribute sources accurately, making it a
valuable tool for enhancing the reliability and trustworthiness of LLMs.","[{'name': 'Cheng Wang'}, {'name': 'Xinyang Lu'}, {'name': 'See-Kiong Ng'}, {'name': 'Bryan Kian Hsiang Low'}]",2024-07-06T07:19:30Z
http://arxiv.org/abs/2407.12849v1,http://arxiv.org/abs/2407.12849v1,"Large language models are good medical coders, if provided with tools","This study presents a novel two-stage Retrieve-Rank system for automated
ICD-10-CM medical coding, comparing its performance against a Vanilla Large
Language Model (LLM) approach. Evaluating both systems on a dataset of 100
single-term medical conditions, the Retrieve-Rank system achieved 100% accuracy
in predicting correct ICD-10-CM codes, significantly outperforming the Vanilla
LLM (GPT-3.5-turbo), which achieved only 6% accuracy. Our analysis demonstrates
the Retrieve-Rank system's superior precision in handling various medical terms
across different specialties. While these results are promising, we acknowledge
the limitations of using simplified inputs and the need for further testing on
more complex, realistic medical cases. This research contributes to the ongoing
effort to improve the efficiency and accuracy of medical coding, highlighting
the importance of retrieval-based approaches.",[{'name': 'Keith Kwan'}],2024-07-06T06:58:51Z
http://arxiv.org/abs/2407.04973v1,http://arxiv.org/abs/2407.04973v1,"LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual
  Contexts","We propose LogicVista, an evaluation benchmark that assesses the integrated
logical reasoning capabilities of multimodal large language models (MLLMs) in
Visual contexts. Recent advancements in MLLMs have demonstrated various
fascinating abilities, from crafting poetry based on an image to performing
mathematical reasoning. However, there is still a lack of systematic evaluation
of MLLMs' proficiency in logical reasoning tasks, which are essential for
activities like navigation and puzzle-solving. Thus we evaluate general logical
cognition abilities across 5 logical reasoning tasks encompassing 9 different
capabilities, using a sample of 448 multiple-choice questions. Each question is
annotated with the correct answer and the human-written reasoning behind the
selection, enabling both open-ended and multiple-choice evaluation. A total of
8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Available
at https://github.com/Yijia-Xiao/LogicVista.","[{'name': 'Yijia Xiao'}, {'name': 'Edward Sun'}, {'name': 'Tianyu Liu'}, {'name': 'Wei Wang'}]",2024-07-06T06:48:16Z
http://arxiv.org/abs/2407.04969v1,http://arxiv.org/abs/2407.04969v1,"EVA-Score: Evaluation of Long-form Summarization on Informativeness
  through Extraction and Validation","Summarization is a fundamental task in natural language processing (NLP) and
since large language models (LLMs), such as GPT-4 and Claude, come out,
increasing attention has been paid to long-form summarization whose input
sequences are much longer, indicating more information contained.
  The current evaluation metrics either use similarity-based metrics like ROUGE
and BERTScore which rely on similarity and fail to consider informativeness or
LLM-based metrics, lacking quantitative analysis of information richness and
are rather subjective.
  In this paper, we propose a new evaluation metric called EVA-Score using
Atomic Fact Chain Generation and Document-level Relation Extraction together to
automatically calculate the informativeness and give a definite number as an
information score. Experiment results show that our metric shows a
state-of-the-art correlation with humans. We also re-evaluate the performance
of LLMs on long-form summarization comprehensively from the information aspect,
forecasting future ways to use LLMs for long-form summarization.","[{'name': 'Yuchen Fan'}, {'name': 'Xin Zhong'}, {'name': 'Chengsi Wang'}, {'name': 'Gaoche Wu'}, {'name': 'Bowen Zhou'}]",2024-07-06T06:02:38Z
http://arxiv.org/abs/2407.04965v2,http://arxiv.org/abs/2407.04965v2,"Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM
  Compression","Large language models (LLMs) are increasingly deployed in real-world
scenarios with the help of recent model compression techniques. Such momentum
towards local deployment means the use of compressed LLMs will widely impact a
large population. However, prior analysis works often prioritize on preserving
perplexity which is a direct analogy to training loss. The impact of
compression method on other critical aspects of model behavior, particularly
safety, still calls for a systematic assessment. To this end, we investigate
the impact of model compression on four dimensions: (1) degeneration harm,
i.e., bias and toxicity in generation; (2) representational harm, i.e., biases
in discriminative tasks; (3) dialect bias; (4) language modeling and downstream
task performance. We cover a wide spectrum of LLM compression techniques,
including unstructured pruning, semi-structured pruning and quantization. Our
analysis reveals that compression can lead to unexpected consequences. Although
compression may unintentionally remedy LLMs' degeneration harm, it can still
exacerbate on the representational harm axis. Although compression may
unintentionally remedy LLMs' degeneration harm, it can still exacerbate on the
representational harm axis. Moreover, there is a divergent impact on different
protected groups as the compression rate grows. Finally, different compression
methods have drastically different safety impacts, e.g., quantization mostly
preserves bias while pruning degrades quickly. Our findings underscore the
importance of integrating safety assessments into the development of compressed
LLMs to ensure their reliability across real-world applications. Our full
results are available here:
\url{https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval}","[{'name': 'Zhichao Xu'}, {'name': 'Ashim Gupta'}, {'name': 'Tao Li'}, {'name': 'Oliver Bentham'}, {'name': 'Vivek Srikumar'}]",2024-07-06T05:56:22Z
http://arxiv.org/abs/2407.12848v2,http://arxiv.org/abs/2407.12848v2,"Applicability of Large Language Models and Generative Models for Legal
  Case Judgement Summarization","Automatic summarization of legal case judgements, which are known to be long
and complex, has traditionally been tried via extractive summarization models.
In recent years, generative models including abstractive summarization models
and Large language models (LLMs) have gained huge popularity. In this paper, we
explore the applicability of such models for legal case judgement
summarization. We applied various domain specific abstractive summarization
models and general domain LLMs as well as extractive summarization models over
two sets of legal case judgements from the United Kingdom (UK) Supreme Court
and the Indian (IN) Supreme Court and evaluated the quality of the generated
summaries. We also perform experiments on a third dataset of legal documents of
a different type, Government reports from the United States (US). Results show
that abstractive summarization models and LLMs generally perform better than
the extractive methods as per traditional metrics for evaluating summary
quality. However, detailed investigation shows the presence of inconsistencies
and hallucinations in the outputs of the generative models, and we explore ways
to reduce the hallucinations and inconsistencies in the summaries. Overall, the
investigation suggests that further improvements are needed to enhance the
reliability of abstractive models and LLMs for legal case judgement
summarization. At present, a human-in-the-loop technique is more suitable for
performing manual checks to identify inconsistencies in the generated
summaries.","[{'name': 'Aniket Deroy'}, {'name': 'Kripabandhu Ghosh'}, {'name': 'Saptarshi Ghosh'}]",2024-07-06T04:49:40Z
http://arxiv.org/abs/2407.04952v1,http://arxiv.org/abs/2407.04952v1,Granular Privacy Control for Geolocation with Vision Language Models,"Vision Language Models (VLMs) are rapidly advancing in their capability to
answer information-seeking questions. As these models are widely deployed in
consumer applications, they could lead to new privacy risks due to emergent
abilities to identify people in photos, geolocate images, etc. As we
demonstrate, somewhat surprisingly, current open-source and proprietary VLMs
are very capable image geolocators, making widespread geolocation with VLMs an
immediate privacy risk, rather than merely a theoretical future concern. As a
first step to address this challenge, we develop a new benchmark, GPTGeoChat,
to test the ability of VLMs to moderate geolocation dialogues with users. We
collect a set of 1,000 image geolocation conversations between in-house
annotators and GPT-4v, which are annotated with the granularity of location
information revealed at each turn. Using this new dataset, we evaluate the
ability of various VLMs to moderate GPT-4v geolocation conversations by
determining when too much location information has been revealed. We find that
custom fine-tuned models perform on par with prompted API-based models when
identifying leaked location information at the country or city level; however,
fine-tuning on supervised data appears to be needed to accurately moderate
finer granularities, such as the name of a restaurant or building.","[{'name': 'Ethan Mendes'}, {'name': 'Yang Chen'}, {'name': 'James Hays'}, {'name': 'Sauvik Das'}, {'name': 'Wei Xu'}, {'name': 'Alan Ritter'}]",2024-07-06T04:06:55Z
http://arxiv.org/abs/2407.04903v1,http://arxiv.org/abs/2407.04903v1,"MMSci: A Multimodal Multi-Discipline Dataset for PhD-Level Scientific
  Comprehension","The rapid advancement of Large Language Models (LLMs) and Large Multimodal
Models (LMMs) has heightened the demand for AI-based scientific assistants
capable of understanding scientific articles and figures. Despite progress,
there remains a significant gap in evaluating models' comprehension of
professional, graduate-level, and even PhD-level scientific content. Current
datasets and benchmarks primarily focus on relatively simple scientific tasks
and figures, lacking comprehensive assessments across diverse advanced
scientific disciplines. To bridge this gap, we collected a multimodal,
multidisciplinary dataset from open-access scientific articles published in
Nature Communications journals. This dataset spans 72 scientific disciplines,
ensuring both diversity and quality. We created benchmarks with various tasks
and settings to comprehensively evaluate LMMs' capabilities in understanding
scientific figures and content. Our evaluation revealed that these tasks are
highly challenging: many open-source models struggled significantly, and even
GPT-4V and GPT-4o faced difficulties. We also explored using our dataset as
training resources by constructing visual instruction-following data, enabling
the 7B LLaVA model to achieve performance comparable to GPT-4V/o on our
benchmark. Additionally, we investigated the use of our interleaved article
texts and figure images for pre-training LMMs, resulting in improvements on the
material generation task. The source dataset, including articles, figures,
constructed benchmarks, and visual instruction-following data, is open-sourced.","[{'name': 'Zekun Li'}, {'name': 'Xianjun Yang'}, {'name': 'Kyuri Choi'}, {'name': 'Wanrong Zhu'}, {'name': 'Ryan Hsieh'}, {'name': 'HyeonJung Kim'}, {'name': 'Jin Hyuk Lim'}, {'name': 'Sungyoung Ji'}, {'name': 'Byungju Lee'}, {'name': 'Xifeng Yan'}, {'name': 'Linda Ruth Petzold'}, {'name': 'Stephen D. Wilson'}, {'name': 'Woosang Lim'}, {'name': 'William Yang Wang'}]",2024-07-06T00:40:53Z
http://arxiv.org/abs/2407.04899v1,http://arxiv.org/abs/2407.04899v1,Algorithmic Language Models with Neurally Compiled Libraries,"Important tasks such as reasoning and planning are fundamentally algorithmic,
meaning that solving them robustly requires acquiring true reasoning or
planning algorithms, rather than shortcuts. Large Language Models lack true
algorithmic ability primarily because of the limitations of neural network
optimization algorithms, their optimization data and optimization objective,
but also due to architectural inexpressivity. To solve this, our paper proposes
augmenting LLMs with a library of fundamental operations and sophisticated
differentiable programs, so that common algorithms do not need to be learned
from scratch. We add memory, registers, basic operations, and adaptive
recurrence to a transformer architecture built on LLaMA3. Then, we define a
method for directly compiling algorithms into a differentiable starting
library, which is used natively and propagates gradients for optimization. In
this preliminary study, we explore the feasability of augmenting LLaMA3 with a
differentiable computer, for instance by fine-tuning small transformers on
simple algorithmic tasks with variable computational depth.","[{'name': 'Lucas Saldyt'}, {'name': 'Subbarao Kambhampati'}]",2024-07-06T00:27:05Z
http://arxiv.org/abs/2407.04885v1,http://arxiv.org/abs/2407.04885v1,"Automating Venture Capital: Founder assessment using LLM-powered
  segmentation, feature engineering and automated labeling techniques","This study explores the application of large language models (LLMs) in
venture capital (VC) decision-making, focusing on predicting startup success
based on founder characteristics. We utilize LLM prompting techniques, like
chain-of-thought, to generate features from limited data, then extract insights
through statistics and machine learning. Our results reveal potential
relationships between certain founder characteristics and success, as well as
demonstrate the effectiveness of these characteristics in prediction. This
framework for integrating ML techniques and LLMs has vast potential for
improving startup success prediction, with important implications for VC firms
seeking to optimize their investment strategies.","[{'name': 'Ekin Ozince'}, {'name': 'Yiğit Ihlamur'}]",2024-07-05T22:54:13Z
http://arxiv.org/abs/2407.17503v1,http://arxiv.org/abs/2407.17503v1,"Challenges and Considerations in Annotating Legal Data: A Comprehensive
  Overview","The process of annotating data within the legal sector is filled with
distinct challenges that differ from other fields, primarily due to the
inherent complexities of legal language and documentation. The initial task
usually involves selecting an appropriate raw dataset that captures the
intricate aspects of legal texts. Following this, extracting text becomes a
complicated task, as legal documents often have complex structures, footnotes,
references, and unique terminology. The importance of data cleaning is
magnified in this context, ensuring that redundant information is eliminated
while maintaining crucial legal details and context. Creating comprehensive yet
straightforward annotation guidelines is imperative, as these guidelines serve
as the road map for maintaining uniformity and addressing the subtle nuances of
legal terminology. Another critical aspect is the involvement of legal
professionals in the annotation process. Their expertise is valuable in
ensuring that the data not only remains contextually accurate but also adheres
to prevailing legal standards and interpretations. This paper provides an
expanded view of these challenges and aims to offer a foundational
understanding and guidance for researchers and professionals engaged in legal
data annotation projects. In addition, we provide links to our created and
fine-tuned datasets and language models. These resources are outcomes of our
discussed projects and solutions to challenges faced while working on them.","[{'name': 'Harshil Darji'}, {'name': 'Jelena Mitrović'}, {'name': 'Michael Granitzer'}]",2024-07-05T21:56:28Z
http://arxiv.org/abs/2407.04858v1,http://arxiv.org/abs/2407.04858v1,"Question Answering with Texts and Tables through Deep Reinforcement
  Learning","This paper proposes a novel architecture to generate multi-hop answers to
open domain questions that require information from texts and tables, using the
Open Table-and-Text Question Answering dataset for validation and training. One
of the most common ways to generate answers in this setting is to retrieve
information sequentially, where a selected piece of data helps searching for
the next piece. As different models can have distinct behaviors when called in
this sequential information search, a challenge is how to select models at each
step. Our architecture employs reinforcement learning to choose between
different state-of-the-art tools sequentially until, in the end, a desired
answer is generated. This system achieved an F1-score of 19.03, comparable to
iterative systems in the literature.","[{'name': 'Marcos M. José'}, {'name': 'Flávio N. Cação'}, {'name': 'Maria F. Ribeiro'}, {'name': 'Rafael M. Cheang'}, {'name': 'Paulo Pirozelli'}, {'name': 'Fabio G. Cozman'}]",2024-07-05T20:44:01Z
http://arxiv.org/abs/2407.04855v1,http://arxiv.org/abs/2407.04855v1,"Towards Enhancing Coherence in Extractive Summarization: Dataset and
  Experiments with LLMs","Extractive summarization plays a pivotal role in natural language processing
due to its wide-range applications in summarizing diverse content efficiently,
while also being faithful to the original content. Despite significant
advancement achieved in extractive summarization by Large Language Models
(LLMs), these summaries frequently exhibit incoherence. An important aspect of
the coherent summary is its readability for intended users. Although there have
been many datasets and benchmarks proposed for creating coherent extractive
summaries, none of them currently incorporate user intent to improve coherence
in extractive summarization. Motivated by this, we propose a systematically
created human-annotated dataset consisting of coherent summaries for five
publicly available datasets and natural language user feedback, offering
valuable insights into how to improve coherence in extractive summaries. We
utilize this dataset for aligning LLMs through supervised fine-tuning with
natural language human feedback to enhance the coherence of their generated
summaries. Preliminary experiments with Falcon-40B and Llama-2-13B show
significant performance improvements (~10% Rouge-L) in terms of producing
coherent summaries. We further utilize human feedback to benchmark results over
instruction-tuned models such as FLAN-T5 which resulted in several interesting
findings. Data and source code are available at
https://github.com/Mihir3009/Extract-AI.","[{'name': 'Mihir Parmar'}, {'name': 'Hanieh Deilamsalehy'}, {'name': 'Franck Dernoncourt'}, {'name': 'Seunghyun Yoon'}, {'name': 'Ryan A. Rossi'}, {'name': 'Trung Bui'}]",2024-07-05T20:25:04Z
http://arxiv.org/abs/2407.04854v1,http://arxiv.org/abs/2407.04854v1,"Statistical investigations into the geometry and homology of random
  programs","AI-supported programming has taken giant leaps with tools such as Meta's
Llama and openAI's chatGPT. These are examples of stochastic sources of
programs and have already greatly influenced how we produce code and teach
programming. If we consider input to such models as a stochastic source, a
natural question is, what is the relation between the input and the output
distributions, between the chatGPT prompt and the resulting program?
  In this paper, we will show how the relation between random Python programs
generated from chatGPT can be described geometrically and topologically using
Tree-edit distances between the program's syntax trees and without explicit
modeling of the underlying space. A popular approach to studying
high-dimensional samples in a metric space is to use low-dimensional embedding
using, e.g., multidimensional scaling. Such methods imply errors depending on
the data and dimension of the embedding space. In this article, we propose to
restrict such projection methods to purely visualization purposes and instead
use geometric summary statistics, methods from spatial point statistics, and
topological data analysis to characterize the configurations of random programs
that do not rely on embedding approximations. To demonstrate their usefulness,
we compare two publicly available models: ChatGPT-4 and TinyLlama, on a simple
problem related to image processing.
  Application areas include understanding how questions should be asked to
obtain useful programs; measuring how consistently a given large language model
answers; and comparing the different large language models as a programming
assistant. Finally, we speculate that our approach may in the future give new
insights into the structure of programming languages.","[{'name': 'Jon Sporring'}, {'name': 'Ken Friis Larsen'}]",2024-07-05T20:25:02Z
http://arxiv.org/abs/2407.04842v1,http://arxiv.org/abs/2407.04842v1,"MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for
  Text-to-Image Generation?","While text-to-image models like DALLE-3 and Stable Diffusion are rapidly
proliferating, they often encounter challenges such as hallucination, bias, and
the production of unsafe, low-quality output. To effectively address these
issues, it is crucial to align these models with desired behaviors based on
feedback from a multimodal judge. Despite their significance, current
multimodal judges frequently undergo inadequate evaluation of their
capabilities and limitations, potentially leading to misalignment and unsafe
fine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel
benchmark which incorporates a comprehensive preference dataset to evaluate
multimodal judges in providing feedback for image generation models across four
key perspectives: alignment, safety, image quality, and bias. Specifically, we
evaluate a large variety of multimodal judges including smaller-sized
CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and
close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our
preference dataset. Experiments reveal that close-source VLMs generally provide
better feedback, with GPT-4o outperforming other judges in average. Compared
with open-source VLMs, smaller-sized scoring models can provide better feedback
regarding text-image alignment and image quality, while VLMs provide more
accurate feedback regarding safety and generation bias due to their stronger
reasoning capabilities. Further studies in feedback scale reveal that VLM
judges can generally provide more accurate and stable feedback in natural
language (Likert-scale) than numerical scales. Notably, human evaluations on
end-to-end fine-tuned models using separate feedback from these multimodal
judges provide similar conclusions, further confirming the effectiveness of
MJ-Bench. All data, code, models are available at
https://huggingface.co/MJ-Bench.","[{'name': 'Zhaorun Chen'}, {'name': 'Yichao Du'}, {'name': 'Zichen Wen'}, {'name': 'Yiyang Zhou'}, {'name': 'Chenhang Cui'}, {'name': 'Zhenzhen Weng'}, {'name': 'Haoqin Tu'}, {'name': 'Chaoqi Wang'}, {'name': 'Zhengwei Tong'}, {'name': 'Qinglan Huang'}, {'name': 'Canyu Chen'}, {'name': 'Qinghao Ye'}, {'name': 'Zhihong Zhu'}, {'name': 'Yuqing Zhang'}, {'name': 'Jiawei Zhou'}, {'name': 'Zhuokai Zhao'}, {'name': 'Rafael Rafailov'}, {'name': 'Chelsea Finn'}, {'name': 'Huaxiu Yao'}]",2024-07-05T20:03:16Z
http://arxiv.org/abs/2407.04796v2,http://arxiv.org/abs/2407.04796v2,Toucan: Many-to-Many Translation for 150 African Language Pairs,"We address a notable gap in Natural Language Processing (NLP) by introducing
a collection of resources designed to improve Machine Translation (MT) for
low-resource languages, with a specific focus on African languages. First, we
introduce two language models (LMs), Cheetah-1.2B and Cheetah-3.7B, with 1.2
billion and 3.7 billion parameters respectively. Next, we finetune the
aforementioned models to create toucan, an Afrocentric machine translation
model designed to support 156 African language pairs. To evaluate Toucan, we
carefully develop an extensive machine translation benchmark, dubbed
AfroLingu-MT, tailored for evaluating machine translation. Toucan significantly
outperforms other models, showcasing its remarkable performance on MT for
African languages. Finally, we train a new model, spBLEU-1K, to enhance
translation evaluation metrics, covering 1K languages, including 614 African
languages. This work aims to advance the field of NLP, fostering cross-cultural
understanding and knowledge exchange, particularly in regions with limited
language resources such as Africa. The GitHub repository for the Toucan project
is available at https://github.com/UBC-NLP/Toucan.","[{'name': 'AbdelRahim Elmadany'}, {'name': 'Ife Adebara'}, {'name': 'Muhammad Abdul-Mageed'}]",2024-07-05T18:12:19Z
http://arxiv.org/abs/2407.04794v1,http://arxiv.org/abs/2407.04794v1,"On Evaluating The Performance of Watermarked Machine-Generated Texts
  Under Adversarial Attacks","Large Language Models (LLMs) excel in various applications, including text
generation and complex tasks. However, the misuse of LLMs raises concerns about
the authenticity and ethical implications of the content they produce, such as
deepfake news, academic fraud, and copyright infringement. Watermarking
techniques, which embed identifiable markers in machine-generated text, offer a
promising solution to these issues by allowing for content verification and
origin tracing. Unfortunately, the robustness of current LLM watermarking
schemes under potential watermark removal attacks has not been comprehensively
explored.
  In this paper, to fill this gap, we first systematically comb the mainstream
watermarking schemes and removal attacks on machine-generated texts, and then
we categorize them into pre-text (before text generation) and post-text (after
text generation) classes so that we can conduct diversified analyses. In our
experiments, we evaluate eight watermarks (five pre-text, three post-text) and
twelve attacks (two pre-text, ten post-text) across 87 scenarios. Evaluation
results indicate that (1) KGW and Exponential watermarks offer high text
quality and watermark retention but remain vulnerable to most attacks; (2)
Post-text attacks are found to be more efficient and practical than pre-text
attacks; (3) Pre-text watermarks are generally more imperceptible, as they do
not alter text fluency, unlike post-text watermarks; (4) Additionally, combined
attack methods can significantly increase effectiveness, highlighting the need
for more robust watermarking solutions. Our study underscores the
vulnerabilities of current techniques and the necessity for developing more
resilient schemes.","[{'name': 'Zesen Liu'}, {'name': 'Tianshuo Cong'}, {'name': 'Xinlei He'}, {'name': 'Qi Li'}]",2024-07-05T18:09:06Z
http://arxiv.org/abs/2407.04652v1,http://arxiv.org/abs/2407.04652v1,"Pretraining End-to-End Keyword Search with Automatically Discovered
  Acoustic Units","End-to-end (E2E) keyword search (KWS) has emerged as an alternative and
complimentary approach to conventional keyword search which depends on the
output of automatic speech recognition (ASR) systems. While E2E methods greatly
simplify the KWS pipeline, they generally have worse performance than their
ASR-based counterparts, which can benefit from pretraining with untranscribed
data. In this work, we propose a method for pretraining E2E KWS systems with
untranscribed data, which involves using acoustic unit discovery (AUD) to
obtain discrete units for untranscribed data and then learning to locate
sequences of such units in the speech. We conduct experiments across languages
and AUD systems: we show that finetuning such a model significantly outperforms
a model trained from scratch, and the performance improvements are generally
correlated with the quality of the AUD system used for pretraining.","[{'name': 'Bolaji Yusuf'}, {'name': 'Jan ""Honza"" Černocký'}, {'name': 'Murat Saraçlar'}]",2024-07-05T17:07:58Z
http://arxiv.org/abs/2407.04641v1,http://arxiv.org/abs/2407.04641v1,"Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of
  Language Models","This paper explores speculative speech recognition (SSR), where we empower
conventional automatic speech recognition (ASR) with speculation capabilities,
allowing the recognizer to run ahead of audio. We introduce a metric for
measuring SSR performance and we propose a model which does SSR by combining a
RNN-Transducer-based ASR system with an audio-prefixed language model (LM). The
ASR system transcribes ongoing audio and feeds the resulting transcripts, along
with an audio-dependent prefix, to the LM, which speculates likely completions
for the transcriptions. We experiment with a variety of ASR datasets on which
show the efficacy our method and the feasibility of SSR as a method of reducing
ASR latency.","[{'name': 'Bolaji Yusuf'}, {'name': 'Murali Karthick Baskar'}, {'name': 'Andrew Rosenberg'}, {'name': 'Bhuvana Ramabhadran'}]",2024-07-05T16:52:55Z
http://arxiv.org/abs/2407.04615v1,http://arxiv.org/abs/2407.04615v1,ARM: Efficient Guided Decoding with Autoregressive Reward Models,"Language models trained on large amounts of data require careful tuning to be
safely deployed in real world. We revisit the guided decoding paradigm, where
the goal is to augment the logits of the base language model using the scores
from a task-specific reward model. We propose a simple but efficient
parameterization of the autoregressive reward model enabling fast and effective
guided decoding. On detoxification and sentiment control tasks, we show that
our efficient parameterization performs on par with RAD, a strong but less
efficient guided decoding approach.","[{'name': 'Sergey Troshin'}, {'name': 'Vlad Niculae'}, {'name': 'Antske Fokkens'}]",2024-07-05T16:11:03Z
http://arxiv.org/abs/2407.14521v1,http://arxiv.org/abs/2407.14521v1,"Towards Automated Functional Equation Proving: A Benchmark Dataset and A
  Domain-Specific In-Context Agent","Automated Theorem Proving (ATP) faces challenges due to its complexity and
computational demands. Recent work has explored using Large Language Models
(LLMs) for ATP action selection, but these methods can be resource-intensive.
This study introduces FEAS, an agent that enhances the COPRA in-context
learning framework within Lean. FEAS refines prompt generation, response
parsing, and incorporates domain-specific heuristics for functional equations.
It introduces FunEq, a curated dataset of functional equation problems with
varying difficulty. FEAS outperforms baselines on FunEq, particularly with the
integration of domain-specific heuristics. The results demonstrate FEAS's
effectiveness in generating and formalizing high-level proof strategies into
Lean proofs, showcasing the potential of tailored approaches for specific ATP
challenges.","[{'name': 'Mahdi Buali'}, {'name': 'Robert Hoehndorf'}]",2024-07-05T15:59:16Z
http://arxiv.org/abs/2407.04601v1,http://arxiv.org/abs/2407.04601v1,Written Term Detection Improves Spoken Term Detection,"End-to-end (E2E) approaches to keyword search (KWS) are considerably simpler
in terms of training and indexing complexity when compared to approaches which
use the output of automatic speech recognition (ASR) systems. This
simplification however has drawbacks due to the loss of modularity. In
particular, where ASR-based KWS systems can benefit from external unpaired text
via a language model, current formulations of E2E KWS systems have no such
mechanism. Therefore, in this paper, we propose a multitask training objective
which allows unpaired text to be integrated into E2E KWS without complicating
indexing and search. In addition to training an E2E KWS model to retrieve text
queries from spoken documents, we jointly train it to retrieve text queries
from masked written documents. We show empirically that this approach can
effectively leverage unpaired text for KWS, with significant improvements in
search performance across a wide variety of languages. We conduct analysis
which indicates that these improvements are achieved because the proposed
method improves document representations for words in the unpaired text.
Finally, we show that the proposed method can be used for domain adaptation in
settings where in-domain paired data is scarce or nonexistent.","[{'name': 'Bolaji Yusuf'}, {'name': 'Murat Saraçlar'}]",2024-07-05T15:50:47Z
http://arxiv.org/abs/2407.04593v1,http://arxiv.org/abs/2407.04593v1,"Testing learning hypotheses using neural networks by manipulating
  learning data","Although passivization is productive in English, it is not completely general
-- some exceptions exist (e.g. *One hour was lasted by the meeting). How do
English speakers learn these exceptions to an otherwise general pattern? Using
neural network language models as theories of acquisition, we explore the
sources of indirect evidence that a learner can leverage to learn whether a
verb can passivize. We first characterize English speakers' judgments of
exceptions to the passive, confirming that speakers find some verbs more
passivizable than others. We then show that a neural network language model can
learn restrictions to the passive that are similar to those displayed by
humans, suggesting that evidence for these exceptions is available in the
linguistic input. We test the causal role of two hypotheses for how the
language model learns these restrictions by training models on modified
training corpora, which we create by altering the existing training corpora to
remove features of the input implicated by each hypothesis. We find that while
the frequency with which a verb appears in the passive significantly affects
its passivizability, the semantics of the verb does not. This study highlight
the utility of altering a language model's training data for answering
questions where complete control over a learner's input is vital.","[{'name': 'Cara Su-Yi Leong'}, {'name': 'Tal Linzen'}]",2024-07-05T15:41:30Z
http://arxiv.org/abs/2407.04573v1,http://arxiv.org/abs/2407.04573v1,"VRSD: Rethinking Similarity and Diversity for Retrieval in Large
  Language Models","Vector retrieval algorithms are vital for semantic queries in the evolving
landscape of Large Language Models (LLMs). Retrieving vectors that
simultaneously meet criteria for both similarity and diversity significantly
enhances the capabilities of LLM-based agents. Despite the widespread use of
the Maximal Marginal Relevance (MMR) in retrieval scenarios with relevance and
diversity requirements, fluctuations caused by variations in the parameter $
\lambda $ within the MMR complicate the determination of the optimization
trajectory in vector spaces, thus obscuring the direction of enhancement.
Moreover, there is a lack of a robust theoretical analysis for the constraints
of similarity and diversity in retrieval processes. This paper introduces a
novel approach to characterizing both constraints through the relationship
between the sum vector and the query vector. The proximity of these vectors
addresses the similarity constraint, while necessitating that individual
vectors within the sum vector divergently align with the query vector to
satisfy the diversity constraint. We also formulate a new combinatorial
optimization challenge, taking a selection of $k$ vectors from a set of
candidates such that their sum vector maximally aligns with the query vector, a
problem we demonstrate to be NP-complete. This establishes the profound
difficulty of pursuing similarity and diversity simultaneously in vector
retrieval and lays a theoretical groundwork for further research. Additionally,
we present the heuristic algorithm Vectors Retrieval with Similarity and
Diversity (VRSD) which not only has a definitive optimization goal and eschews
the need for preset parameters but also offers a modest reduction in time
complexity compared to MMR. Empirical validation further confirm that VRSD
significantly surpasses MMR across various datasets.","[{'name': 'Hang Gao'}, {'name': 'Yongfeng Zhang'}]",2024-07-05T15:08:44Z
http://arxiv.org/abs/2407.04559v1,http://arxiv.org/abs/2407.04559v1,"Not (yet) the whole story: Evaluating Visual Storytelling Requires More
  than Measuring Coherence, Grounding, and Repetition","Visual storytelling consists in generating a natural language story given a
temporally ordered sequence of images. This task is not only challenging for
models, but also very difficult to evaluate with automatic metrics since there
is no consensus about what makes a story 'good'. In this paper, we introduce a
novel method that measures story quality in terms of human likeness regarding
three key aspects highlighted in previous work: visual grounding, coherence,
and repetitiveness. We then use this method to evaluate the stories generated
by several models, showing that the foundation model LLaVA obtains the best
result, but only slightly so compared to TAPM, a 50-times smaller visual
storytelling model. Upgrading the visual and language components of TAPM
results in a model that yields competitive performance with a relatively low
number of parameters. Finally, we carry out a human evaluation study, whose
results suggest that a 'good' story may require more than a human-like level of
visual grounding, coherence, and repetition.","[{'name': 'Aditya K Surikuchi'}, {'name': 'Raquel Fernández'}, {'name': 'Sandro Pezzelle'}]",2024-07-05T14:48:15Z
http://arxiv.org/abs/2407.04549v1,http://arxiv.org/abs/2407.04549v1,Spontaneous Reward Hacking in Iterative Self-Refinement,"Language models are capable of iteratively improving their outputs based on
natural language feedback, thus enabling in-context optimization of user
preference. In place of human users, a second language model can be used as an
evaluator, providing feedback along with numerical ratings which the generator
attempts to optimize. However, because the evaluator is an imperfect proxy of
user preference, this optimization can lead to reward hacking, where the
evaluator's ratings improve while the generation quality remains stagnant or
even decreases as judged by actual user preference. The concern of reward
hacking is heightened in iterative self-refinement where the generator and the
evaluator use the same underlying language model, in which case the
optimization pressure can drive them to exploit shared vulnerabilities. Using
an essay editing task, we show that iterative self-refinement leads to
deviation between the language model evaluator and human judgment,
demonstrating that reward hacking can occur spontaneously in-context with the
use of iterative self-refinement. In addition, we study conditions under which
reward hacking occurs and observe two factors that affect reward hacking
severity: model size and context sharing between the generator and the
evaluator.","[{'name': 'Jane Pan'}, {'name': 'He He'}, {'name': 'Samuel R. Bowman'}, {'name': 'Shi Feng'}]",2024-07-05T14:34:50Z
http://arxiv.org/abs/2407.04543v1,http://arxiv.org/abs/2407.04543v1,"Strengthening Structural Inductive Biases by Pre-training to Perform
  Syntactic Transformations","Models need appropriate inductive biases to effectively learn from small
amounts of data and generalize systematically outside of the training
distribution. While Transformers are highly versatile and powerful, they can
still benefit from enhanced structural inductive biases for seq2seq tasks,
especially those involving syntactic transformations, such as converting active
to passive voice or semantic parsing. In this paper, we propose to strengthen
the structural inductive bias of a Transformer by intermediate pre-training to
perform synthetically generated syntactic transformations of dependency trees
given a description of the transformation. Our experiments confirm that this
helps with few-shot learning of syntactic tasks such as chunking, and also
improves structural generalization for semantic parsing. Our analysis shows
that the intermediate pre-training leads to attention heads that keep track of
which syntactic transformation needs to be applied to which token, and that the
model can leverage these attention heads on downstream tasks.","[{'name': 'Matthias Lindemann'}, {'name': 'Alexander Koller'}, {'name': 'Ivan Titov'}]",2024-07-05T14:29:44Z
http://arxiv.org/abs/2407.04528v1,http://arxiv.org/abs/2407.04528v1,"GPT vs RETRO: Exploring the Intersection of Retrieval and
  Parameter-Efficient Fine-Tuning","Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation
(RAG) have become popular methods for adapting large language models while
minimizing compute requirements. In this paper, we apply PEFT methods
(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer
(RETRO) and a baseline GPT model across several sizes, ranging from 823 million
to 48 billion parameters. We show that RETRO models outperform GPT models in
zero-shot settings due to their unique pre-training process but GPT models have
higher performance potential with PEFT. Additionally, our study indicates that
8B parameter models strike an optimal balance between cost and performance and
P-tuning lags behind other PEFT techniques. We further provide a comparative
analysis of between applying PEFT to an Instruction-tuned RETRO model and base
RETRO model. This work presents the first comprehensive comparison of various
PEFT methods integrated with RAG, applied to both GPT and RETRO models,
highlighting their relative performance.","[{'name': 'Aleksander Ficek'}, {'name': 'Jiaqi Zeng'}, {'name': 'Oleksii Kuchaiev'}]",2024-07-05T14:16:47Z
http://arxiv.org/abs/2407.04485v1,http://arxiv.org/abs/2407.04485v1,"Leveraging Graph Structures to Detect Hallucinations in Large Language
  Models","Large language models are extensively applied across a wide range of tasks,
such as customer support, content creation, educational tutoring, and providing
financial guidance. However, a well-known drawback is their predisposition to
generate hallucinations. This damages the trustworthiness of the information
these models provide, impacting decision-making and user confidence. We propose
a method to detect hallucinations by looking at the structure of the latent
space and finding associations within hallucinated and non-hallucinated
generations. We create a graph structure that connects generations that lie
closely in the embedding space. Moreover, we employ a Graph Attention Network
which utilizes message passing to aggregate information from neighboring nodes
and assigns varying degrees of importance to each neighbor based on their
relevance. Our findings show that 1) there exists a structure in the latent
space that differentiates between hallucinated and non-hallucinated
generations, 2) Graph Attention Networks can learn this structure and
generalize it to unseen generations, and 3) the robustness of our method is
enhanced when incorporating contrastive learning. When evaluated against
evidence-based benchmarks, our model performs similarly without access to
search-based methods.","[{'name': 'Noa Nonkes'}, {'name': 'Sergei Agaronian'}, {'name': 'Evangelos Kanoulas'}, {'name': 'Roxana Petcu'}]",2024-07-05T13:08:58Z
http://arxiv.org/abs/2407.04482v1,http://arxiv.org/abs/2407.04482v1,"Controlling Whisper: Universal Acoustic Adversarial Attacks to Control
  Speech Foundation Models","Speech enabled foundation models, either in the form of flexible speech
recognition based systems or audio-prompted large language models (LLMs), are
becoming increasingly popular. One of the interesting aspects of these models
is their ability to perform tasks other than automatic speech recognition (ASR)
using an appropriate prompt. For example, the OpenAI Whisper model can perform
both speech transcription and speech translation. With the development of
audio-prompted LLMs there is the potential for even greater control options. In
this work we demonstrate that with this greater flexibility the systems can be
susceptible to model-control adversarial attacks. Without any access to the
model prompt it is possible to modify the behaviour of the system by
appropriately changing the audio input. To illustrate this risk, we demonstrate
that it is possible to prepend a short universal adversarial acoustic segment
to any input speech signal to override the prompt setting of an ASR foundation
model. Specifically, we successfully use a universal adversarial acoustic
segment to control Whisper to always perform speech translation, despite being
set to perform speech transcription. Overall, this work demonstrates a new form
of adversarial attack on multi-tasking speech enabled foundation models that
needs to be considered prior to the deployment of this form of model.","[{'name': 'Vyas Raina'}, {'name': 'Mark Gales'}]",2024-07-05T13:04:31Z
http://arxiv.org/abs/2407.04472v3,http://arxiv.org/abs/2407.04472v3,"EventChat: Implementation and user-centric evaluation of a large
  language model-driven conversational recommender system for exploring leisure
  events in an SME context","Large language models (LLMs) present an enormous evolution in the strategic
potential of conversational recommender systems (CRS). Yet to date, research
has predominantly focused upon technical frameworks to implement LLM-driven
CRS, rather than end-user evaluations or strategic implications for firms,
particularly from the perspective of a small to medium enterprises (SME) that
makeup the bedrock of the global economy. In the current paper, we detail the
design of an LLM-driven CRS in an SME setting, and its subsequent performance
in the field using both objective system metrics and subjective user
evaluations. While doing so, we additionally outline a short-form revised
ResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly
evolving field. Our results reveal good system performance from a user
experience perspective (85.5% recommendation accuracy) but underscore latency,
cost, and quality issues challenging business viability. Notably, with a median
cost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and
response time emerge as crucial areas for achieving a more user-friendly and
economically viable LLM-driven CRS for SME settings. One major driver of these
costs is the use of an advanced LLM as a ranker within the retrieval-augmented
generation (RAG) technique. Our results additionally indicate that relying
solely on approaches such as Prompt-based learning with ChatGPT as the
underlying LLM makes it challenging to achieve satisfying quality in a
production environment. Strategic considerations for SMEs deploying an
LLM-driven CRS are outlined, particularly considering trade-offs in the current
technical landscape.","[{'name': 'Hannes Kunstmann'}, {'name': 'Joseph Ollier'}, {'name': 'Joel Persson'}, {'name': 'Florian von Wangenheim'}]",2024-07-05T12:42:31Z
http://arxiv.org/abs/2407.04467v2,http://arxiv.org/abs/2407.04467v2,"Are Large Language Models Strategic Decision Makers? A Study of
  Performance and Bias in Two-Player Non-Zero-Sum Games","Large Language Models (LLMs) have been increasingly used in real-world
settings, yet their strategic abilities remain largely unexplored. Game theory
provides a good framework for assessing the decision-making abilities of LLMs
in interactions with other agents. Although prior studies have shown that LLMs
can solve these tasks with carefully curated prompts, they fail when the
problem setting or prompt changes. In this work we investigate LLMs' behaviour
in strategic games, Stag Hunt and Prisoner Dilemma, analyzing performance
variations under different settings and prompts. Our results show that the
tested state-of-the-art LLMs exhibit at least one of the following systematic
biases: (1) positional bias, (2) payoff bias, or (3) behavioural bias.
Subsequently, we observed that the LLMs' performance drops when the game
configuration is misaligned with the affecting biases. Performance is assessed
based on the selection of the correct action, one which agrees with the
prompted preferred behaviours of both players. Alignment refers to whether the
LLM's bias aligns with the correct action. For example, GPT-4o's average
performance drops by 34% when misaligned. Additionally, the current trend of
""bigger and newer is better"" does not hold for the above, where GPT-4o (the
current best-performing LLM) suffers the most substantial performance drop.
Lastly, we note that while chain-of-thought prompting does reduce the effect of
the biases on most models, it is far from solving the problem at the
fundamental level.","[{'name': 'Nathan Herr'}, {'name': 'Fernando Acero'}, {'name': 'Roberta Raileanu'}, {'name': 'María Pérez-Ortiz'}, {'name': 'Zhibin Li'}]",2024-07-05T12:30:02Z
http://arxiv.org/abs/2407.04466v1,http://arxiv.org/abs/2407.04466v1,Using LLMs to label medical papers according to the CIViC evidence model,"We introduce the sequence classification problem CIViC Evidence to the field
of medical NLP. CIViC Evidence denotes the multi-label classification problem
of assigning labels of clinical evidence to abstracts of scientific papers
which have examined various combinations of genomic variants, cancer types, and
treatment approaches. We approach CIViC Evidence using different language
models: We fine-tune pretrained checkpoints of BERT and RoBERTa on the CIViC
Evidence dataset and challenge their performance with models of the same
architecture which have been pretrained on domain-specific text. In this
context, we find that BiomedBERT and BioLinkBERT can outperform BERT on CIViC
Evidence (+0.8% and +0.9% absolute improvement in class-support weighted F1
score). All transformer-based models show a clear performance edge when
compared to a logistic regression trained on bigram tf-idf scores (+1.5 - 2.7%
improved F1 score). We compare the aforementioned BERT-like models to OpenAI's
GPT-4 in a few-shot setting (on a small subset of our original test dataset),
demonstrating that, without additional prompt-engineering or fine-tuning, GPT-4
performs worse on CIViC Evidence than our six fine-tuned models (66.1% weighted
F1 score compared to 71.8% for the best fine-tuned model). However, performance
gets reasonably close to the benchmark of a logistic regression model trained
on bigram tf-idf scores (67.7% weighted F1 score).","[{'name': 'Markus Hisch'}, {'name': 'Xing David Wang'}]",2024-07-05T12:30:01Z
http://arxiv.org/abs/2407.04459v1,http://arxiv.org/abs/2407.04459v1,Generalists vs. Specialists: Evaluating Large Language Models for Urdu,"In this paper, we compare general-purpose pretrained models, GPT-4-Turbo and
Llama-3-8b-Instruct with special-purpose models fine-tuned on specific tasks,
XLM-Roberta-large, mT5-large, and Llama-3-8b-Instruct. We focus on seven
classification and six generation tasks to evaluate the performance of these
models on Urdu language. Urdu has 70 million native speakers, yet it remains
underrepresented in Natural Language Processing (NLP). Despite the frequent
advancements in Large Language Models (LLMs), their performance in low-resource
languages, including Urdu, still needs to be explored. We also conduct a human
evaluation for the generation tasks and compare the results with the
evaluations performed by GPT-4-Turbo and Llama-3-8b-Instruct. We find that
special-purpose models consistently outperform general-purpose models across
various tasks. We also find that the evaluation done by GPT-4-Turbo for
generation tasks aligns more closely with human evaluation compared to the
evaluation by Llama-3-8b-Instruct. This paper contributes to the NLP community
by providing insights into the effectiveness of general and specific-purpose
LLMs for low-resource languages.","[{'name': 'Samee Arif'}, {'name': 'Abdul Hameed Azeemi'}, {'name': 'Agha Ali Raza'}, {'name': 'Awais Athar'}]",2024-07-05T12:09:40Z
http://arxiv.org/abs/2407.04444v1,http://arxiv.org/abs/2407.04444v1,TokenVerse: Unifying Speech and NLP Tasks via Transducer-based ASR,"In traditional conversational intelligence from speech, a cascaded pipeline
is used, involving tasks such as voice activity detection, diarization,
transcription, and subsequent processing with different NLP models for tasks
like semantic endpointing and named entity recognition (NER). Our paper
introduces TokenVerse, a single Transducer-based model designed to handle
multiple tasks. This is achieved by integrating task-specific tokens into the
reference text during ASR model training, streamlining the inference and
eliminating the need for separate NLP models. In addition to ASR, we conduct
experiments on 3 different tasks: speaker change detection, endpointing, and
NER. Our experiments on a public and a private dataset show that the proposed
method improves ASR by up to 7.7% in relative WER while outperforming the
cascaded pipeline approach in individual task performance. Additionally, we
present task transfer learning to a new task within an existing TokenVerse.","[{'name': 'Shashi Kumar'}, {'name': 'Srikanth Madikeri'}, {'name': 'Juan Zuluaga-Gomez'}, {'name': 'Iuliia Nigmatulina'}, {'name': 'Esaú Villatoro-Tello'}, {'name': 'Sergio Burdisso'}, {'name': 'Petr Motlicek'}, {'name': 'Karthik Pandia'}, {'name': 'Aravind Ganapathiraju'}]",2024-07-05T11:54:38Z
http://arxiv.org/abs/2407.04434v1,http://arxiv.org/abs/2407.04434v1,"From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive
  Language for Bias Reduction in LLMs","Gender bias is not only prevalent in Large Language Models (LLMs) and their
training data, but also firmly ingrained into the structural aspects of
language itself. Therefore, adapting linguistic structures within LLM training
data to promote gender-inclusivity can make gender representations within the
model more inclusive. The focus of our work are gender-exclusive affixes in
English, such as in 'show-girl' or 'man-cave', which can perpetuate gender
stereotypes and binary conceptions of gender. We use an LLM training dataset to
compile a catalogue of 692 gender-exclusive terms along with gender-neutral
variants and from this, develop a gender-inclusive fine-tuning dataset, the
'Tiny Heap'. Fine-tuning three different LLMs with this dataset, we observe an
overall reduction in gender-stereotyping tendencies across the models. Our
approach provides a practical method for enhancing gender inclusivity in LLM
training data and contributes to incorporating queer-feminist linguistic
activism in bias mitigation research in NLP.","[{'name': 'Marion Bartl'}, {'name': 'Susan Leavy'}]",2024-07-05T11:31:30Z
http://arxiv.org/abs/2407.04411v1,http://arxiv.org/abs/2407.04411v1,Waterfall: Framework for Robust and Scalable Text Watermarking,"Protecting intellectual property (IP) of text such as articles and code is
increasingly important, especially as sophisticated attacks become possible,
such as paraphrasing by large language models (LLMs) or even unauthorized
training of LLMs on copyrighted text to infringe such IP. However, existing
text watermarking methods are not robust enough against such attacks nor
scalable to millions of users for practical implementation. In this paper, we
propose Waterfall, the first training-free framework for robust and scalable
text watermarking applicable across multiple text types (e.g., articles, code)
and languages supportable by LLMs, for general text and LLM data provenance.
Waterfall comprises several key innovations, such as being the first to use LLM
as paraphrasers for watermarking along with a novel combination of techniques
that are surprisingly effective in achieving robust verifiability and
scalability. We empirically demonstrate that Waterfall achieves significantly
better scalability, robust verifiability, and computational efficiency compared
to SOTA article-text watermarking methods, and also showed how it could be
directly applied to the watermarking of code.","[{'name': 'Gregory Kang Ruey Lau'}, {'name': 'Xinyuan Niu'}, {'name': 'Hieu Dao'}, {'name': 'Jiangwei Chen'}, {'name': 'Chuan-Sheng Foo'}, {'name': 'Bryan Kian Hsiang Low'}]",2024-07-05T10:51:33Z
http://arxiv.org/abs/2407.12847v1,http://arxiv.org/abs/2407.12847v1,"Aligning Model Evaluations with Human Preferences: Mitigating Token
  Count Bias in Language Model Assessments","The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a
viable and cost-effective alternative to API-based Large Language Models
(LLMs), such as OpenAI's GPT-4, offering comparable performance and stability.
However, SLAM also identified discrepancies between human preferences and
traditional auto-evaluators. This follow-up paper explores methods to align LLM
evaluator preferences with human evaluations by addressing biases, particularly
toward higher token counts. We employed Bayesian statistics and a t-test to
quantify this bias and developed a recalibration procedure to adjust the
GPTScorer. Our findings significantly improve aligning the recalibrated LLM
evaluator with human evaluations across multiple use cases. For instance,
spearman's ranking correlation score in the Recommendation use case improved
from -27.27 to 44.55. These results highlight the importance of accounting for
biases in automated evaluations to ensure fair and accurate model assessments.
The recalibration process enhances the reliability of automated evaluators,
leading to better AI models that align with human values and expectations. This
study provides a robust methodology for future research into bias correction
and emphasizes the feasibility and benefits of developing human-aligned AI
evaluation systems.","[{'name': 'Roland Daynauth'}, {'name': 'Jason Mars'}]",2024-07-05T09:26:40Z
http://arxiv.org/abs/2407.04368v1,http://arxiv.org/abs/2407.04368v1,Romanization Encoding For Multilingual ASR,"We introduce romanization encoding for script-heavy languages to optimize
multilingual and code-switching Automatic Speech Recognition (ASR) systems. By
adopting romanization encoding alongside a balanced concatenated tokenizer
within a FastConformer-RNNT framework equipped with a Roman2Char module, we
significantly reduce vocabulary and output dimensions, enabling larger training
batches and reduced memory consumption. Our method decouples acoustic modeling
and language modeling, enhancing the flexibility and adaptability of the
system. In our study, applying this method to Mandarin-English ASR resulted in
a remarkable 63.51% vocabulary reduction and notable performance gains of
13.72% and 15.03% on SEAME code-switching benchmarks. Ablation studies on
Mandarin-Korean and Mandarin-Japanese highlight our method's strong capability
to address the complexities of other script-heavy languages, paving the way for
more versatile and effective multilingual ASR systems.","[{'name': 'Wen Ding'}, {'name': 'Fei Jia'}, {'name': 'Hainan Xu'}, {'name': 'Yu Xi'}, {'name': 'Junjie Lai'}, {'name': 'Boris Ginsburg'}]",2024-07-05T09:13:24Z
http://arxiv.org/abs/2407.12846v1,http://arxiv.org/abs/2407.12846v1,Identifying the Source of Generation for Large Language Models,"Large language models (LLMs) memorize text from several sources of documents.
In pretraining, LLM trains to maximize the likelihood of text but neither
receives the source of the text nor memorizes the source. Accordingly, LLM can
not provide document information on the generated content, and users do not
obtain any hint of reliability, which is crucial for factuality or privacy
infringement. This work introduces token-level source identification in the
decoding step, which maps the token representation to the reference document.
We propose a bi-gram source identifier, a multi-layer perceptron with two
successive token representations as input for better generalization. We conduct
extensive experiments on Wikipedia and PG19 datasets with several LLMs, layer
locations, and identifier sizes. The overall results show a possibility of
token-level source identifiers for tracing the document, a crucial problem for
the safe use of LLMs.","[{'name': 'Bumjin Park'}, {'name': 'Jaesik Choi'}]",2024-07-05T08:52:15Z
http://arxiv.org/abs/2407.04752v1,http://arxiv.org/abs/2407.04752v1,"SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via
  Saliency-based Spiking","The recent advancements in large language models (LLMs) with billions of
parameters have significantly boosted their performance across various
real-world applications. However, the inference processes for these models
require substantial energy and computational resources, presenting considerable
deployment challenges. In contrast, human brains, which contain approximately
86 billion biological neurons, exhibit significantly greater energy efficiency
compared to LLMs with a similar number of parameters. Inspired by this, we
redesign 7 to 70 billion parameter LLMs using bio-plausible spiking mechanisms,
emulating the efficient behavior of the human brain. We propose the first
spiking large language model as recent LLMs termed SpikeLLM. Coupled with the
proposed model, a novel spike-driven quantization framework named Optimal Brain
Spiking is introduced to reduce the energy cost and accelerate inference speed
via two essential approaches: first (second)-order differentiation-based
salient channel detection, and per-channel salient outlier expansion with
Generalized Integrate-and-Fire neurons. Our proposed spike-driven quantization
can plug in main streams of quantization training methods. In the OmniQuant
pipeline, SpikeLLM significantly reduces 25.51% WikiText2 perplexity and
improves 3.08% average accuracy of 6 zero-shot datasets on a LLAMA2-7B 4A4W
model. In the GPTQ pipeline, SpikeLLM realizes a sparse ternary quantization,
which achieves additive in all linear layers. Compared with PB-LLM with similar
operations, SpikeLLM also exceeds significantly. We will release our code on
GitHub.","[{'name': 'Xingrun Xing'}, {'name': 'Boyan Gao'}, {'name': 'Zheng Zhang'}, {'name': 'David A. Clifton'}, {'name': 'Shitao Xiao'}, {'name': 'Li Du'}, {'name': 'Guoqi Li'}, {'name': 'Jiajun Zhang'}]",2024-07-05T08:37:17Z
http://arxiv.org/abs/2407.04293v1,http://arxiv.org/abs/2407.04293v1,"Systematic Evaluation of Online Speaker Diarization Systems Regarding
  their Latency","In this paper, different online speaker diarization systems are evaluated on
the same hardware with the same test data with regard to their latency. The
latency is the time span from audio input to the output of the corresponding
speaker label. As part of the evaluation, various model combinations within the
DIART framework, a diarization system based on the online clustering algorithm
UIS-RNN-SML, and the end-to-end online diarization system FS-EEND are compared.
The lowest latency is achieved for the DIART-pipeline with the embedding model
pyannote/embedding and the segmentation model pyannote/segmentation. The
FS-EEND system shows a similarly good latency. In general there is currently no
published research that compares several online diarization systems in terms of
their latency. This makes this work even more relevant.","[{'name': 'Roman Aperdannier'}, {'name': 'Sigurd Schacht'}, {'name': 'Alexander Piazza'}]",2024-07-05T06:54:27Z
http://arxiv.org/abs/2407.04280v1,http://arxiv.org/abs/2407.04280v1,"LearnerVoice: A Dataset of Non-Native English Learners' Spontaneous
  Speech","Prevalent ungrammatical expressions and disfluencies in spontaneous speech
from second language (L2) learners pose unique challenges to Automatic Speech
Recognition (ASR) systems. However, few datasets are tailored to L2 learner
speech. We publicly release LearnerVoice, a dataset consisting of 50.04 hours
of audio and transcriptions of L2 learners' spontaneous speech. Our linguistic
analysis reveals that transcriptions in our dataset contain L2S (L2 learner's
Spontaneous speech) features, consisting of ungrammatical expressions and
disfluencies (e.g., filler words, word repetitions, self-repairs, false
starts), significantly more than native speech datasets. Fine-tuning
whisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than
vanilla whisper-small.en. Furthermore, our qualitative analysis indicates that
54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S
features, with 48.1% of them being reduced in the fine-tuned model.","[{'name': 'Haechan Kim'}, {'name': 'Junho Myung'}, {'name': 'Seoyoung Kim'}, {'name': 'Sungpah Lee'}, {'name': 'Dongyeop Kang'}, {'name': 'Juho Kim'}]",2024-07-05T06:25:54Z
http://arxiv.org/abs/2407.04279v1,http://arxiv.org/abs/2407.04279v1,BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks,"In the Emotion Recognition in Conversation task, recent investigations have
utilized attention mechanisms exploring relationships among utterances from
intra- and inter-speakers for modeling emotional interaction between them.
However, attributes such as speaker personality traits remain unexplored and
present challenges in terms of their applicability to other tasks or
compatibility with diverse model architectures. Therefore, this work introduces
a novel framework named BiosERC, which investigates speaker characteristics in
a conversation. By employing Large Language Models (LLMs), we extract the
""biographical information"" of the speaker within a conversation as
supplementary knowledge injected into the model to classify emotional labels
for each utterance. Our proposed method achieved state-of-the-art (SOTA)
results on three famous benchmark datasets: IEMOCAP, MELD, and EmoryNLP,
demonstrating the effectiveness and generalization of our model and showcasing
its potential for adaptation to various conversation analysis tasks. Our source
code is available at https://github.com/yingjie7/BiosERC.","[{'name': 'Jieying Xue'}, {'name': 'Minh Phuong Nguyen'}, {'name': 'Blake Matheny'}, {'name': 'Le Minh Nguyen'}]",2024-07-05T06:25:34Z
http://arxiv.org/abs/2407.04251v1,http://arxiv.org/abs/2407.04251v1,"Unified Interpretation of Smoothing Methods for Negative Sampling Loss
  Functions in Knowledge Graph Embedding","Knowledge Graphs (KGs) are fundamental resources in knowledge-intensive tasks
in NLP. Due to the limitation of manually creating KGs, KG Completion (KGC) has
an important role in automatically completing KGs by scoring their links with
KG Embedding (KGE). To handle many entities in training, KGE relies on Negative
Sampling (NS) loss that can reduce the computational cost by sampling. Since
the appearance frequencies for each link are at most one in KGs, sparsity is an
essential and inevitable problem. The NS loss is no exception. As a solution,
the NS loss in KGE relies on smoothing methods like Self-Adversarial Negative
Sampling (SANS) and subsampling. However, it is uncertain what kind of
smoothing method is suitable for this purpose due to the lack of theoretical
understanding. This paper provides theoretical interpretations of the smoothing
methods for the NS loss in KGE and induces a new NS loss, Triplet Adaptive
Negative Sampling (TANS), that can cover the characteristics of the
conventional smoothing methods. Experimental results of TransE, DistMult,
ComplEx, RotatE, HAKE, and HousE on FB15k-237, WN18RR, and YAGO3-10 datasets
and their sparser subsets show the soundness of our interpretation and
performance improvement by our TANS.","[{'name': 'Xincan Feng'}, {'name': 'Hidetaka Kamigaito'}, {'name': 'Katsuhiko Hayashi'}, {'name': 'Taro Watanabe'}]",2024-07-05T04:38:17Z
http://arxiv.org/abs/2407.04247v1,http://arxiv.org/abs/2407.04247v1,"ArAIEval Shared Task: Propagandistic Techniques Detection in Unimodal
  and Multimodal Arabic Content","We present an overview of the second edition of the ArAIEval shared task,
organized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In
this edition, ArAIEval offers two tasks: (i) detection of propagandistic
textual spans with persuasion techniques identification in tweets and news
articles, and (ii) distinguishing between propagandistic and non-propagandistic
memes. A total of 14 teams participated in the final evaluation phase, with 6
and 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams
submitted system description papers. Across both tasks, we observed that
fine-tuning transformer models such as AraBERT was at the core of the majority
of the participating systems. We provide a description of the task setup,
including a description of the dataset construction and the evaluation setup.
We further provide a brief overview of the participating systems. All datasets
and evaluation scripts are released to the research community
(https://araieval.gitlab.io/). We hope this will enable further research on
these important tasks in Arabic.","[{'name': 'Maram Hasanain'}, {'name': 'Md. Arid Hasan'}, {'name': 'Fatema Ahmed'}, {'name': 'Reem Suwaileh'}, {'name': 'Md. Rafiul Biswas'}, {'name': 'Wajdi Zaghouani'}, {'name': 'Firoj Alam'}]",2024-07-05T04:28:46Z
http://arxiv.org/abs/2407.04185v2,http://arxiv.org/abs/2407.04185v2,HAF-RM: A Hybrid Alignment Framework for Reward Model Training,"The reward model has become increasingly important in alignment, assessment,
and data construction for large language models (LLMs). Most existing
researchers focus on enhancing reward models through data improvements,
following the conventional training framework for reward models that directly
optimizes the predicted rewards. In this paper, we propose a hybrid alignment
framework HaF-RM for reward model training by introducing an additional
constraint on token-level policy probabilities in addition to the reward score.
It can simultaneously supervise the internal preference model at the token
level and optimize the mapping layer of the reward model at the sequence level.
Theoretical justifications and experiment results on five datasets show the
validity and effectiveness of our proposed hybrid framework for training a
high-quality reward model. By decoupling the reward modeling procedure and
incorporating hybrid supervision, our HaF-RM framework offers a principled and
effective approach to enhancing the performance and alignment of reward models,
a critical component in the responsible development of powerful language
models. We release our code at https://haf-rm.github.io.","[{'name': 'Shujun Liu'}, {'name': 'Xiaoyu Shen'}, {'name': 'Yuhang Lai'}, {'name': 'Siyuan Wang'}, {'name': 'Shengbin Yue'}, {'name': 'Zengfeng Huang'}, {'name': 'Xuanjing Huang'}, {'name': 'Zhongyu Wei'}]",2024-07-04T23:26:56Z
http://arxiv.org/abs/2407.04183v1,http://arxiv.org/abs/2407.04183v1,"Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality
  Norms","Large language models (LLMs) are trained on broad corpora and then used in
communities with specialized norms. Is providing LLMs with community rules
enough for models to follow these norms? We evaluate LLMs' capacity to detect
(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's
Neutral Point of View (NPOV) policy. LLMs struggled with bias detection,
achieving only 64% accuracy on a balanced dataset. Models exhibited contrasting
biases (some under- and others over-predicted bias), suggesting distinct priors
about neutrality. LLMs performed better at generation, removing 79% of words
removed by Wikipedia editors. However, LLMs made additional changes beyond
Wikipedia editors' simpler neutralizations, resulting in high-recall but
low-precision editing. Interestingly, crowdworkers rated AI rewrites as more
neutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative
analysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia
editors but often made extraneous non-NPOV-related changes (such as grammar).
LLMs may apply rules in ways that resonate with the public but diverge from
community experts. While potentially effective for generation, LLMs may reduce
editor agency and increase moderation workload (e.g., verifying additions).
Even when rules are easy to articulate, having LLMs apply them like community
members may still be difficult.","[{'name': 'Joshua Ashkinaze'}, {'name': 'Ruijia Guan'}, {'name': 'Laura Kurek'}, {'name': 'Eytan Adar'}, {'name': 'Ceren Budak'}, {'name': 'Eric Gilbert'}]",2024-07-04T23:05:58Z
http://arxiv.org/abs/2407.04181v1,http://arxiv.org/abs/2407.04181v1,Orchestrating LLMs with Different Personalizations,"This paper presents a novel approach to aligning large language models (LLMs)
with individual human preferences, sometimes referred to as Reinforcement
Learning from \textit{Personalized} Human Feedback (RLPHF). Given stated
preferences along multiple dimensions, such as helpfulness, conciseness, or
humor, the goal is to create an LLM without re-training that best adheres to
this specification. Starting from specialized expert LLMs, each trained for one
such particular preference dimension, we propose a black-box method that merges
their outputs on a per-token level. We train a lightweight Preference Control
Model (PCM) that dynamically translates the preference description and current
context into next-token prediction weights. By combining the expert models'
outputs at the token level, our approach dynamically generates text that
optimizes the given preference. Empirical tests show that our method matches or
surpasses existing preference merging techniques, providing a scalable,
efficient alternative to fine-tuning LLMs for individual personalization.","[{'name': 'Jin Peng Zhou'}, {'name': 'Katie Z Luo'}, {'name': 'Jingwen Gu'}, {'name': 'Jason Yuan'}, {'name': 'Kilian Q. Weinberger'}, {'name': 'Wen Sun'}]",2024-07-04T22:55:02Z
http://arxiv.org/abs/2407.04179v1,http://arxiv.org/abs/2407.04179v1,"Defense Against Syntactic Textual Backdoor Attacks with Token
  Substitution","Textual backdoor attacks present a substantial security risk to Large
Language Models (LLM). It embeds carefully chosen triggers into a victim model
at the training stage, and makes the model erroneously predict inputs
containing the same triggers as a certain class. Prior backdoor defense methods
primarily target special token-based triggers, leaving syntax-based triggers
insufficiently addressed. To fill this gap, this paper proposes a novel online
defense algorithm that effectively counters syntax-based as well as special
token-based backdoor attacks. The algorithm replaces semantically meaningful
words in sentences with entirely different ones but preserves the syntactic
templates or special tokens, and then compares the predicted labels before and
after the substitution to determine whether a sentence contains triggers.
Experimental results confirm the algorithm's performance against these two
types of triggers, offering a comprehensive defense strategy for model
integrity.","[{'name': 'Xinglin Li'}, {'name': 'Xianwen He'}, {'name': 'Yao Li'}, {'name': 'Minhao Cheng'}]",2024-07-04T22:48:57Z
http://arxiv.org/abs/2407.18328v1,http://arxiv.org/abs/2407.18328v1,"Unveiling Scoring Processes: Dissecting the Differences between LLMs and
  Human Graders in Automatic Scoring","Large language models (LLMs) have demonstrated strong potential in performing
automatic scoring for constructed response assessments. While constructed
responses graded by humans are usually based on given grading rubrics, the
methods by which LLMs assign scores remain largely unclear. It is also
uncertain how closely AI's scoring process mirrors that of humans, or if it
adheres to the same grading criteria. To address this gap, this paper uncovers
the grading rubrics that LLMs used to score students' written responses to
science tasks and their alignment with human scores. We also examine whether
enhancing the alignments can improve scoring accuracy. Specifically, we prompt
LLMs to generate analytic rubrics that they use to assign scores and study the
alignment gap with human grading rubrics. Based on a series of experiments with
various configurations of LLM settings, we reveal a notable alignment gap
between human and LLM graders. While LLMs can adapt quickly to scoring tasks,
they often resort to shortcuts, bypassing deeper logical reasoning expected in
human grading. We found that incorporating high-quality analytical rubrics
designed to reflect human grading logic can mitigate this gap and enhance LLMs'
scoring accuracy. These results caution against the simplistic application of
LLMs in science education and highlight the importance of aligning LLM outputs
with human expectations to ensure efficient and accurate automatic scoring.","[{'name': 'Xuansheng Wu'}, {'name': 'Padmaja Pravin Saraf'}, {'name': 'Gyeong-Geon Lee'}, {'name': 'Ehsan Latif'}, {'name': 'Ninghao Liu'}, {'name': 'Xiaoming Zhai'}]",2024-07-04T22:26:20Z
http://arxiv.org/abs/2407.04172v1,http://arxiv.org/abs/2407.04172v1,ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild,"Given the ubiquity of charts as a data analysis, visualization, and
decision-making tool across industries and sciences, there has been a growing
interest in developing pre-trained foundation models as well as general purpose
instruction-tuned models for chart understanding and reasoning. However,
existing methods suffer crucial drawbacks across two critical axes affecting
the performance of chart representation models: they are trained on data
generated from underlying data tables of the charts, ignoring the visual trends
and patterns in chart images, and use weakly aligned vision-language backbone
models for domain-specific training, limiting their generalizability when
encountering charts in the wild. We address these important drawbacks and
introduce ChartGemma, a novel chart understanding and reasoning model developed
over PaliGemma. Rather than relying on underlying data tables, ChartGemma is
trained on instruction-tuning data generated directly from chart images, thus
capturing both high-level trends and low-level visual information from a
diverse set of charts. Our simple approach achieves state-of-the-art results
across $5$ benchmarks spanning chart summarization, question answering, and
fact-checking, and our elaborate qualitative studies on real-world charts show
that ChartGemma generates more realistic and factually correct summaries
compared to its contemporaries. We release the code, model checkpoints,
dataset, and demos at https://github.com/vis-nlp/ChartGemma.","[{'name': 'Ahmed Masry'}, {'name': 'Megh Thakkar'}, {'name': 'Aayush Bajaj'}, {'name': 'Aaryaman Kartha'}, {'name': 'Enamul Hoque'}, {'name': 'Shafiq Joty'}]",2024-07-04T22:16:40Z
http://arxiv.org/abs/2407.04158v1,http://arxiv.org/abs/2407.04158v1,ELCC: the Emergent Language Corpus Collection,"We introduce the Emergent Language Corpus Collection (ELCC): a collection of
corpora collected from open source implementations of emergent communication
systems across the literature. These systems include a variety of signalling
game environments as well as more complex tasks like a social deduction game
and embodied navigation. Each corpus is annotated with metadata describing the
characteristics of the source system as well as a suite of analyses of the
corpus (e.g., size, entropy, average message length). Currently, research
studying emergent languages requires directly running different systems which
takes time away from actual analyses of such languages, limits the variety of
languages that are studied, and presents a barrier to entry for researchers
without a background in deep learning. The availability of a substantial
collection of well-documented emergent language corpora, then, will enable new
directions of research which focus their purview on the properties of emergent
languages themselves rather than on experimental apparatus.","[{'name': 'Brendon Boldt'}, {'name': 'David Mortensen'}]",2024-07-04T21:23:18Z
http://arxiv.org/abs/2407.04125v1,http://arxiv.org/abs/2407.04125v1,Query-Guided Self-Supervised Summarization of Nursing Notes,"Nursing notes, an important component of Electronic Health Records (EHRs),
keep track of the progression of a patient's health status during a care
episode. Distilling the key information in nursing notes through text
summarization techniques can improve clinicians' efficiency in understanding
patients' conditions when reviewing nursing notes. However, existing
abstractive summarization methods in the clinical setting have often overlooked
nursing notes and require the creation of reference summaries for supervision
signals, which is time-consuming. In this work, we introduce QGSumm, a
query-guided self-supervised domain adaptation framework for nursing note
summarization. Using patient-related clinical queries as guidance, our approach
generates high-quality, patient-centered summaries without relying on reference
summaries for training. Through automatic and manual evaluation by an expert
clinician, we demonstrate the strengths of our approach compared to the
state-of-the-art Large Language Models (LLMs) in both zero-shot and few-shot
settings. Ultimately, our approach provides a new perspective on conditional
text summarization, tailored to the specific interests of clinical personnel.","[{'name': 'Ya Gao'}, {'name': 'Hans Moen'}, {'name': 'Saila Koivusalo'}, {'name': 'Miika Koskinen'}, {'name': 'Pekka Marttinen'}]",2024-07-04T18:54:30Z
http://arxiv.org/abs/2407.04121v1,http://arxiv.org/abs/2407.04121v1,"Hallucination Detection: Robustly Discerning Reliable Answers in Large
  Language Models","Large Language Models (LLMs) have gained widespread adoption in various
natural language processing tasks, including question answering and dialogue
systems. However, a major drawback of LLMs is the issue of hallucination, where
they generate unfaithful or inconsistent content that deviates from the input
source, leading to severe consequences. In this paper, we propose a robust
discriminator named RelD to effectively detect hallucination in LLMs' generated
answers. RelD is trained on the constructed RelQA, a bilingual
question-answering dialogue dataset along with answers generated by LLMs and a
comprehensive set of metrics. Our experimental results demonstrate that the
proposed RelD successfully detects hallucination in the answers generated by
diverse LLMs. Moreover, it performs well in distinguishing hallucination in
LLMs' generated answers from both in-distribution and out-of-distribution
datasets. Additionally, we also conduct a thorough analysis of the types of
hallucinations that occur and present valuable insights. This research
significantly contributes to the detection of reliable answers generated by
LLMs and holds noteworthy implications for mitigating hallucination in the
future work.","[{'name': 'Yuyan Chen'}, {'name': 'Qiang Fu'}, {'name': 'Yichen Yuan'}, {'name': 'Zhihao Wen'}, {'name': 'Ge Fan'}, {'name': 'Dayiheng Liu'}, {'name': 'Dongmei Zhang'}, {'name': 'Zhixu Li'}, {'name': 'Yanghua Xiao'}]",2024-07-04T18:47:42Z
http://arxiv.org/abs/2407.11033v1,http://arxiv.org/abs/2407.11033v1,"Hadamard Adapter: An Extreme Parameter-Efficient Adapter Tuning Method
  for Pre-trained Language Models","Recent years, Pre-trained Language models (PLMs) have swept into various
fields of artificial intelligence and achieved great success. However, most
PLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them
is often expensive and time consuming, and storing them takes up a lot of
space. Therefore, it is necessary to adopt a parameter-efficient approach to
reduce parameters of PLMs in fine-tuning without compromising their performance
in downstream tasks. In this paper, we design a novel adapter which only acts
on self-attention outputs in PLMs. This adapter adopts element-wise linear
transformation using Hadamard product, hence named as Hadamard adapter,
requires the fewest parameters compared to previous parameter-efficient
adapters. In addition, we also summarize some tuning patterns for Hadamard
adapter shared by various downstream tasks, expecting to provide some guidance
for further parameter reduction with shared adapters in future studies. The
experiments conducted on the widely-used GLUE benchmark with several SOTA PLMs
prove that the Hadamard adapter achieves competitive performance with only
0.033\% parameters compared with full fine-tuning, and it has the fewest
parameters compared with other adapters. Moreover, we further find that there
is also some redundant layers in the Hadamard adapter which can be removed to
achieve more parameter efficiency with only 0.022\% parameters.","[{'name': 'Yuyan Chen'}, {'name': 'Qiang Fu'}, {'name': 'Ge Fan'}, {'name': 'Lun Du'}, {'name': 'Jian-Guang Lou'}, {'name': 'Shi Han'}, {'name': 'Dongmei Zhang'}, {'name': 'Zhixu Li'}, {'name': 'Yanghua Xiao'}]",2024-07-04T18:21:28Z
http://arxiv.org/abs/2407.04106v1,http://arxiv.org/abs/2407.04106v1,"MiniGPT-Med: Large Language Model as a General Interface for Radiology
  Diagnosis","Recent advancements in artificial intelligence (AI) have precipitated
significant breakthroughs in healthcare, particularly in refining diagnostic
procedures. However, previous studies have often been constrained to limited
functionalities. This study introduces MiniGPT-Med, a vision-language model
derived from large-scale language models and tailored for medical applications.
MiniGPT-Med demonstrates remarkable versatility across various imaging
modalities, including X-rays, CT scans, and MRIs, enhancing its utility. The
model is capable of performing tasks such as medical report generation, visual
question answering (VQA), and disease identification within medical imagery.
Its integrated processing of both image and textual clinical data markedly
improves diagnostic accuracy. Our empirical assessments confirm MiniGPT-Med's
superior performance in disease grounding, medical report generation, and VQA
benchmarks, representing a significant step towards reducing the gap in
assisting radiology practice. Furthermore, it achieves state-of-the-art
performance on medical report generation, higher than the previous best model
by 19\% accuracy. MiniGPT-Med promises to become a general interface for
radiology diagnoses, enhancing diagnostic efficiency across a wide range of
medical imaging applications.","[{'name': 'Asma Alkhaldi'}, {'name': 'Raneem Alnajim'}, {'name': 'Layan Alabdullatef'}, {'name': 'Rawan Alyahya'}, {'name': 'Jun Chen'}, {'name': 'Deyao Zhu'}, {'name': 'Ahmed Alsinan'}, {'name': 'Mohamed Elhoseiny'}]",2024-07-04T18:21:10Z
http://arxiv.org/abs/2407.04105v1,http://arxiv.org/abs/2407.04105v1,Can Pre-trained Language Models Understand Chinese Humor?,"Humor understanding is an important and challenging research in natural
language processing. As the popularity of pre-trained language models (PLMs),
some recent work makes preliminary attempts to adopt PLMs for humor recognition
and generation. However, these simple attempts do not substantially answer the
question: {\em whether PLMs are capable of humor understanding?} This paper is
the first work that systematically investigates the humor understanding ability
of PLMs. For this purpose, a comprehensive framework with three evaluation
steps and four evaluation tasks is designed. We also construct a comprehensive
Chinese humor dataset, which can fully meet all the data requirements of the
proposed evaluation framework. Our empirical study on the Chinese humor dataset
yields some valuable observations, which are of great guiding value for future
optimization of PLMs in humor understanding and generation.","[{'name': 'Yuyan Chen'}, {'name': 'Zhixu Li'}, {'name': 'Jiaqing Liang'}, {'name': 'Yanghua Xiao'}, {'name': 'Bang Liu'}, {'name': 'Yunwen Chen'}]",2024-07-04T18:13:38Z
http://arxiv.org/abs/2407.04093v2,http://arxiv.org/abs/2407.04093v2,"Stephanie: Step-by-Step Dialogues for Mimicking Human Interactions in
  Social Conversations","In the rapidly evolving field of natural language processing, dialogue
systems primarily employ a single-step dialogue paradigm. Although this
paradigm is efficient, it lacks the depth and fluidity of human interactions
and does not appear natural. We introduce a novel \textbf{Step}-by-Step
Dialogue Paradigm (Stephanie), designed to mimic the ongoing dynamic nature of
human conversations. By employing a dual learning strategy and a further-split
post-editing method, we generated and utilized a high-quality step-by-step
dialogue dataset to fine-tune existing large language models, enabling them to
perform step-by-step dialogues. We thoroughly present Stephanie. Tailored
automatic and human evaluations are conducted to assess its effectiveness
compared to the traditional single-step dialogue paradigm. We will release
code, Stephanie datasets, and Stephanie LLMs to facilitate the future of
chatbot eras.","[{'name': 'Hao Yang'}, {'name': 'Hongyuan Lu'}, {'name': 'Xinhua Zeng'}, {'name': 'Yang Liu'}, {'name': 'Xiang Zhang'}, {'name': 'Haoran Yang'}, {'name': 'Yumeng Zhang'}, {'name': 'Shan Huang'}, {'name': 'Yiran Wei'}, {'name': 'Wai Lam'}]",2024-07-04T17:59:41Z
http://arxiv.org/abs/2407.12844v1,http://arxiv.org/abs/2407.12844v1,"$\texttt{metabench}$ -- A Sparse Benchmark to Measure General Ability in
  Large Language Models","Large Language Models (LLMs) vary in their abilities on a range of tasks.
Initiatives such as the $\texttt{Open LLM Leaderboard}$ aim to quantify these
differences with several large benchmarks (sets of test items to which an LLM
can respond either correctly or incorrectly). However, high correlations within
and between benchmark scores suggest that (1) there exists a small set of
common underlying abilities that these benchmarks measure, and (2) items tap
into redundant information and the benchmarks may thus be considerably
compressed. We use data from $n > 5000$ LLMs to identify the most informative
items of six benchmarks, ARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande
(with $d=28,632$ items in total). From them we distill a sparse benchmark,
$\texttt{metabench}$, that has less than $3\%$ of the original size of all six
benchmarks combined. This new sparse benchmark goes beyond point scores by
yielding estimators of the underlying benchmark-specific abilities. We show
that these estimators (1) can be used to reconstruct each original
$\textit{individual}$ benchmark score with, on average, $1.5\%$ root mean
square error (RMSE), (2) reconstruct the original $\textit{total}$ score with
$0.8\%$ RMSE, and (3) have a single underlying common factor whose Spearman
correlation with the total score is $r = 0.93$.","[{'name': 'Alex Kipnis'}, {'name': 'Konstantinos Voudouris'}, {'name': 'Luca M. Schulze Buschoff'}, {'name': 'Eric Schulz'}]",2024-07-04T17:57:38Z
http://arxiv.org/abs/2407.04079v1,http://arxiv.org/abs/2407.04079v1,"AXOLOTL'24 Shared Task on Multilingual Explainable Semantic Change
  Modeling","This paper describes the organization and findings of AXOLOTL'24, the first
multilingual explainable semantic change modeling shared task. We present new
sense-annotated diachronic semantic change datasets for Finnish and Russian
which were employed in the shared task, along with a surprise test-only German
dataset borrowed from an existing source. The setup of AXOLOTL'24 is new to the
semantic change modeling field, and involves subtasks of identifying unknown
(novel) senses and providing dictionary-like definitions to these senses. The
methods of the winning teams are described and compared, thus paving a path
towards explainability in computational approaches to historical change of
meaning.","[{'name': 'Mariia Fedorova'}, {'name': 'Timothee Mickus'}, {'name': 'Niko Partanen'}, {'name': 'Janine Siewert'}, {'name': 'Elena Spaziani'}, {'name': 'Andrey Kutuzov'}]",2024-07-04T17:41:32Z
http://arxiv.org/abs/2407.04078v3,http://arxiv.org/abs/2407.04078v3,"DotaMath: Decomposition of Thought with Code Assistance and
  Self-correction for Mathematical Reasoning","Large language models (LLMs) have made impressive progress in handling simple
math problems, yet they still struggle with more challenging and complex
mathematical tasks. In this paper, we introduce a series of LLMs that employs
the Decomposition of thought with code assistance and self-correction for
mathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex
mathematical tasks by decomposing them into simpler logical subtasks,
leveraging code to solve these subtasks, obtaining fine-grained feedback from
the code interpreter, and engaging in self-reflection and correction. By
annotating diverse interactive tool-use trajectories and employing query
evolution on GSM8K and MATH datasets, we generate an instruction fine-tuning
dataset called DotaMathQA with 574K query-response pairs. We train a series of
base LLMs using imitation learning on DotaMathQA, resulting in DotaMath models
that achieve remarkable performance compared to open-source LLMs across various
in-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases
an outstanding performance of 64.8% on the competitive MATH dataset and 86.7%
on GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a
series of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,
we anticipate that the DotaMath paradigm will open new pathways for addressing
intricate mathematical problems. Our code is publicly available at
https://github.com/ChengpengLi1003/DotaMath.","[{'name': 'Chengpeng Li'}, {'name': 'Guanting Dong'}, {'name': 'Mingfeng Xue'}, {'name': 'Ru Peng'}, {'name': 'Xiang Wang'}, {'name': 'Dayiheng Liu'}]",2024-07-04T17:39:16Z
http://arxiv.org/abs/2407.04069v1,http://arxiv.org/abs/2407.04069v1,"A Systematic Survey and Critical Review on Evaluating Large Language
  Models: Challenges, Limitations, and Recommendations","Large Language Models (LLMs) have recently gained significant attention due
to their remarkable capabilities in performing diverse tasks across various
domains. However, a thorough evaluation of these models is crucial before
deploying them in real-world applications to ensure they produce reliable
performance. Despite the well-established importance of evaluating LLMs in the
community, the complexity of the evaluation process has led to varied
evaluation setups, causing inconsistencies in findings and interpretations. To
address this, we systematically review the primary challenges and limitations
causing these inconsistencies and unreliable evaluations in various steps of
LLM evaluation. Based on our critical review, we present our perspectives and
recommendations to ensure LLM evaluations are reproducible, reliable, and
robust.","[{'name': 'Md Tahmid Rahman Laskar'}, {'name': 'Sawsan Alqahtani'}, {'name': 'M Saiful Bari'}, {'name': 'Mizanur Rahman'}, {'name': 'Mohammad Abdullah Matin Khan'}, {'name': 'Haidar Khan'}, {'name': 'Israt Jahan'}, {'name': 'Amran Bhuiyan'}, {'name': 'Chee Wei Tan'}, {'name': 'Md Rizwan Parvez'}, {'name': 'Enamul Hoque'}, {'name': 'Shafiq Joty'}, {'name': 'Jimmy Huang'}]",2024-07-04T17:15:37Z
http://arxiv.org/abs/2407.04047v1,http://arxiv.org/abs/2407.04047v1,"Improving Accented Speech Recognition using Data Augmentation based on
  Unsupervised Text-to-Speech Synthesis","This paper investigates the use of unsupervised text-to-speech synthesis
(TTS) as a data augmentation method to improve accented speech recognition. TTS
systems are trained with a small amount of accented speech training data and
their pseudo-labels rather than manual transcriptions, and hence unsupervised.
This approach enables the use of accented speech data without manual
transcriptions to perform data augmentation for accented speech recognition.
Synthetic accented speech data, generated from text prompts by using the TTS
systems, are then combined with available non-accented speech data to train
automatic speech recognition (ASR) systems. ASR experiments are performed in a
self-supervised learning framework using a Wav2vec2.0 model which was
pre-trained on large amount of unsupervised accented speech data. The accented
speech data for training the unsupervised TTS are read speech, selected from
L2-ARCTIC and British Isles corpora, while spontaneous conversational speech
from the Edinburgh international accents of English corpus are used as the
evaluation data. Experimental results show that Wav2vec2.0 models which are
fine-tuned to downstream ASR task with synthetic accented speech data,
generated by the unsupervised TTS, yield up to 6.1% relative word error rate
reductions compared to a Wav2vec2.0 baseline which is fine-tuned with the
non-accented speech data from Librispeech corpus.","[{'name': 'Cong-Thanh Do'}, {'name': 'Shuhei Imai'}, {'name': 'Rama Doddipatla'}, {'name': 'Thomas Hain'}]",2024-07-04T16:42:24Z
http://arxiv.org/abs/2407.04046v1,http://arxiv.org/abs/2407.04046v1,"Systematic Task Exploration with LLMs: A Study in Citation Text
  Generation","Large language models (LLMs) bring unprecedented flexibility in defining and
executing complex, creative natural language generation (NLG) tasks. Yet, this
flexibility brings new challenges, as it introduces new degrees of freedom in
formulating the task inputs and instructions and in evaluating model
performance. To facilitate the exploration of creative NLG tasks, we propose a
three-component research framework that consists of systematic input
manipulation, reference data, and output measurement. We use this framework to
explore citation text generation -- a popular scholarly NLP task that lacks
consensus on the task definition and evaluation metric and has not yet been
tackled within the LLM paradigm. Our results highlight the importance of
systematically investigating both task instruction and input configuration when
prompting LLMs, and reveal non-trivial relationships between different
evaluation metrics used for citation text generation. Additional human
generation and human evaluation experiments provide new qualitative insights
into the task to guide future research in citation text generation. We make our
code and data publicly available.","[{'name': 'Furkan Şahinuç'}, {'name': 'Ilia Kuznetsov'}, {'name': 'Yufang Hou'}, {'name': 'Iryna Gurevych'}]",2024-07-04T16:41:08Z
http://arxiv.org/abs/2407.04020v2,http://arxiv.org/abs/2407.04020v2,"LLMAEL: Large Language Models are Good Context Augmenters for Entity
  Linking","Entity Linking (EL) models are well-trained at mapping mentions to their
corresponding entities according to a given context. However, EL models
struggle to disambiguate long-tail entities due to their limited training data.
Meanwhile, large language models (LLMs) are more robust at interpreting
uncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at
generating correct entity IDs. Furthermore, training an LLM to perform EL is
cost-intensive. Building upon these insights, we introduce LLM-Augmented Entity
Linking LLMAEL, a plug-and-play approach to enhance entity linking through LLM
data augmentation. We leverage LLMs as knowledgeable context augmenters,
generating mention-centered descriptions as additional input, while preserving
traditional EL models for task specific processing. Experiments on 6 standard
datasets show that the vanilla LLMAEL outperforms baseline EL models in most
cases, while the fine-tuned LLMAEL set the new state-of-the-art results across
all 6 benchmarks.","[{'name': 'Amy Xin'}, {'name': 'Yunjia Qi'}, {'name': 'Zijun Yao'}, {'name': 'Fangwei Zhu'}, {'name': 'Kaisheng Zeng'}, {'name': 'Xu Bin'}, {'name': 'Lei Hou'}, {'name': 'Juanzi Li'}]",2024-07-04T15:55:13Z
http://arxiv.org/abs/2407.04010v1,http://arxiv.org/abs/2407.04010v1,"Exploring Diachronic and Diatopic Changes in Dialect Continua: Tasks,
  Datasets and Challenges","Everlasting contact between language communities leads to constant changes in
languages over time, and gives rise to language varieties and dialects.
However, the communities speaking non-standard language are often overlooked by
non-inclusive NLP technologies. Recently, there has been a surge of interest in
studying diatopic and diachronic changes in dialect NLP, but there is currently
no research exploring the intersection of both. Our work aims to fill this gap
by systematically reviewing diachronic and diatopic papers from a unified
perspective. In this work, we critically assess nine tasks and datasets across
five dialects from three language families (Slavic, Romance, and Germanic) in
both spoken and written modalities. The tasks covered are diverse, including
corpus construction, dialect distance estimation, and dialect geolocation
prediction, among others. Moreover, we outline five open challenges regarding
changes in dialect use over time, the reliability of dialect datasets, the
importance of speaker characteristics, limited coverage of dialects, and
ethical considerations in data collection. We hope that our work sheds light on
future research towards inclusive computational methods and datasets for
language varieties and dialects.","[{'name': 'Melis Çelikkol'}, {'name': 'Lydia Körber'}, {'name': 'Wei Zhao'}]",2024-07-04T15:38:38Z
http://arxiv.org/abs/2407.03994v2,http://arxiv.org/abs/2407.03994v2,Unlocking the Potential of Model Merging for Low-Resource Languages,"Adapting large language models (LLMs) to new languages typically involves
continual pre-training (CT) followed by supervised fine-tuning (SFT). However,
this CT-then-SFT approach struggles with limited data in the context of
low-resource languages, failing to balance language modeling and task-solving
capabilities. We thus propose model merging as an alternative for low-resource
languages, combining models with distinct capabilities into a single model
without additional training. We use model merging to develop task-solving LLMs
for low-resource languages without SFT data in the target languages. Our
experiments based on Llama-2-7B demonstrate that model merging effectively
endows LLMs for low-resource languages with task-solving abilities,
outperforming CT-then-SFT in scenarios with extremely scarce data. Observing
performance saturation in model merging with more training tokens, we further
analyze the merging process and introduce a slack variable to the model merging
algorithm to mitigate the loss of important parameters, thereby enhancing
performance. We hope that model merging can benefit more human languages
suffering from data scarcity with its higher data efficiency.","[{'name': 'Mingxu Tao'}, {'name': 'Chen Zhang'}, {'name': 'Quzhe Huang'}, {'name': 'Tianyao Ma'}, {'name': 'Songfang Huang'}, {'name': 'Dongyan Zhao'}, {'name': 'Yansong Feng'}]",2024-07-04T15:14:17Z
http://arxiv.org/abs/2407.03993v1,http://arxiv.org/abs/2407.03993v1,A Survey on Natural Language Counterfactual Generation,"Natural Language Counterfactual generation aims to minimally modify a given
text such that the modified text will be classified into a different class. The
generated counterfactuals provide insight into the reasoning behind a model's
predictions by highlighting which words significantly influence the outcomes.
Additionally, they can be used to detect model fairness issues or augment the
training data to enhance the model's robustness. A substantial amount of
research has been conducted to generate counterfactuals for various NLP tasks,
employing different models and methodologies. With the rapid growth of studies
in this field, a systematic review is crucial to guide future researchers and
developers. To bridge this gap, this survey comprehensively overview textual
counterfactual generation methods, particularly including those based on Large
Language Models. We propose a new taxonomy that categorizes the generation
methods into four groups and systematically summarize the metrics for
evaluating the generation quality. Finally, we discuss ongoing research
challenges and outline promising directions for future work.","[{'name': 'Yongjie Wang'}, {'name': 'Xiaoqi Qiu'}, {'name': 'Yu Yue'}, {'name': 'Xu Guo'}, {'name': 'Zhiwei Zeng'}, {'name': 'Yuhong Feng'}, {'name': 'Zhiqi Shen'}]",2024-07-04T15:13:59Z
http://arxiv.org/abs/2407.03964v1,http://arxiv.org/abs/2407.03964v1,"Improving Sample Efficiency of Reinforcement Learning with Background
  Knowledge from Large Language Models","Low sample efficiency is an enduring challenge of reinforcement learning
(RL). With the advent of versatile large language models (LLMs), recent works
impart common-sense knowledge to accelerate policy learning for RL processes.
However, we note that such guidance is often tailored for one specific task but
loses generalizability. In this paper, we introduce a framework that harnesses
LLMs to extract background knowledge of an environment, which contains general
understandings of the entire environment, making various downstream RL tasks
benefit from one-time knowledge representation. We ground LLMs by feeding a few
pre-collected experiences and requesting them to delineate background knowledge
of the environment. Afterward, we represent the output knowledge as potential
functions for potential-based reward shaping, which has a good property for
maintaining policy optimality from task rewards. We instantiate three variants
to prompt LLMs for background knowledge, including writing code, annotating
preferences, and assigning goals. Our experiments show that these methods
achieve significant sample efficiency improvements in a spectrum of downstream
tasks from Minigrid and Crafter domains.","[{'name': 'Fuxiang Zhang'}, {'name': 'Junyou Li'}, {'name': 'Yi-Chen Li'}, {'name': 'Zongzhang Zhang'}, {'name': 'Yang Yu'}, {'name': 'Deheng Ye'}]",2024-07-04T14:33:47Z
http://arxiv.org/abs/2407.03963v1,http://arxiv.org/abs/2407.03963v1,"LLM-jp: A Cross-organizational Project for the Research and Development
  of Fully Open Japanese LLMs","This paper introduces LLM-jp, a cross-organizational project for the research
and development of Japanese large language models (LLMs). LLM-jp aims to
develop open-source and strong Japanese LLMs, and as of this writing, more than
1,500 participants from academia and industry are working together for this
purpose. This paper presents the background of the establishment of LLM-jp,
summaries of its activities, and technical reports on the LLMs developed by
LLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.","[{'name': 'LLM-jp'}, {'name': ':'}, {'name': 'Akiko Aizawa'}, {'name': 'Eiji Aramaki'}, {'name': 'Bowen Chen'}, {'name': 'Fei Cheng'}, {'name': 'Hiroyuki Deguchi'}, {'name': 'Rintaro Enomoto'}, {'name': 'Kazuki Fujii'}, {'name': 'Kensuke Fukumoto'}, {'name': 'Takuya Fukushima'}, {'name': 'Namgi Han'}, {'name': 'Yuto Harada'}, {'name': 'Chikara Hashimoto'}, {'name': 'Tatsuya Hiraoka'}, {'name': 'Shohei Hisada'}, {'name': 'Sosuke Hosokawa'}, {'name': 'Lu Jie'}, {'name': 'Keisuke Kamata'}, {'name': 'Teruhito Kanazawa'}, {'name': 'Hiroki Kanezashi'}, {'name': 'Hiroshi Kataoka'}, {'name': 'Satoru Katsumata'}, {'name': 'Daisuke Kawahara'}, {'name': 'Seiya Kawano'}, {'name': 'Atsushi Keyaki'}, {'name': 'Keisuke Kiryu'}, {'name': 'Hirokazu Kiyomaru'}, {'name': 'Takashi Kodama'}, {'name': 'Takahiro Kubo'}, {'name': 'Yohei Kuga'}, {'name': 'Ryoma Kumon'}, {'name': 'Shuhei Kurita'}, {'name': 'Sadao Kurohashi'}, {'name': 'Conglong Li'}, {'name': 'Taiki Maekawa'}, {'name': 'Hiroshi Matsuda'}, {'name': 'Yusuke Miyao'}, {'name': 'Kentaro Mizuki'}, {'name': 'Sakae Mizuki'}, {'name': 'Yugo Murawaki'}, {'name': 'Ryo Nakamura'}, {'name': 'Taishi Nakamura'}, {'name': 'Kouta Nakayama'}, {'name': 'Tomoka Nakazato'}, {'name': 'Takuro Niitsuma'}, {'name': 'Jiro Nishitoba'}, {'name': 'Yusuke Oda'}, {'name': 'Hayato Ogawa'}, {'name': 'Takumi Okamoto'}, {'name': 'Naoaki Okazaki'}, {'name': 'Yohei Oseki'}, {'name': 'Shintaro Ozaki'}, {'name': 'Koki Ryu'}, {'name': 'Rafal Rzepka'}, {'name': 'Keisuke Sakaguchi'}, {'name': 'Shota Sasaki'}, {'name': 'Satoshi Sekine'}, {'name': 'Kohei Suda'}, {'name': 'Saku Sugawara'}, {'name': 'Issa Sugiura'}, {'name': 'Hiroaki Sugiyama'}, {'name': 'Hisami Suzuki'}, {'name': 'Jun Suzuki'}, {'name': 'Toyotaro Suzumura'}, {'name': 'Kensuke Tachibana'}, {'name': 'Yu Takagi'}, {'name': 'Kyosuke Takami'}, {'name': 'Koichi Takeda'}, {'name': 'Masashi Takeshita'}, {'name': 'Masahiro Tanaka'}, {'name': 'Kenjiro Taura'}, {'name': 'Arseny Tolmachev'}, {'name': 'Nobuhiro Ueda'}, {'name': 'Zhen Wan'}, {'name': 'Shuntaro Yada'}, {'name': 'Sakiko Yahata'}, {'name': 'Yuya Yamamoto'}, {'name': 'Yusuke Yamauchi'}, {'name': 'Hitomi Yanaka'}, {'name': 'Rio Yokota'}, {'name': 'Koichiro Yoshino'}]",2024-07-04T14:33:03Z
http://arxiv.org/abs/2407.03958v1,http://arxiv.org/abs/2407.03958v1,"Stark: Social Long-Term Multi-Modal Conversation with Persona
  Commonsense Knowledge","Humans share a wide variety of images related to their personal experiences
within conversations via instant messaging tools. However, existing works focus
on (1) image-sharing behavior in singular sessions, leading to limited
long-term social interaction, and (2) a lack of personalized image-sharing
behavior. In this work, we introduce Stark, a large-scale long-term multi-modal
conversation dataset that covers a wide range of social personas in a
multi-modality format, time intervals, and images. To construct Stark
automatically, we propose a novel multi-modal contextualization framework, Mcu,
that generates long-term multi-modal dialogue distilled from ChatGPT and our
proposed Plan-and-Execute image aligner. Using our Stark, we train a
multi-modal conversation model, Ultron 7B, which demonstrates impressive visual
imagination ability. Furthermore, we demonstrate the effectiveness of our
dataset in human evaluation. We make our source code and dataset publicly
available.","[{'name': 'Young-Jun Lee'}, {'name': 'Dokyong Lee'}, {'name': 'Junyoung Youn'}, {'name': 'Kyeongjin Oh'}, {'name': 'Byungsoo Ko'}, {'name': 'Jonghwan Hyeon'}, {'name': 'Ho-Jin Choi'}]",2024-07-04T14:26:49Z
http://arxiv.org/abs/2407.03956v2,http://arxiv.org/abs/2407.03956v2,Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems,"Prior research has enhanced the ability of Large Language Models (LLMs) to
solve logic puzzles using techniques such as chain-of-thought prompting or
introducing a symbolic representation. These frameworks are still usually
insufficient to solve complicated logical problems, such as Zebra puzzles, due
to the inherent complexity of translating natural language clues into logical
statements. We introduce a multi-agent system, ZPS, that integrates LLMs with
an off the shelf theorem prover. This system tackles the complex puzzle-solving
task by breaking down the problem into smaller, manageable parts, generating
SMT (Satisfiability Modulo Theories) code to solve them with a theorem prover,
and using feedback between the agents to repeatedly improve their answers. We
also introduce an automated grid puzzle grader to assess the correctness of our
puzzle solutions and show that the automated grader is reliable by evaluating
it in a user-study. Our approach shows improvement in all three LLMs we tested,
with GPT-4 showing 166% improvement in the number of fully correct solutions.","[{'name': 'Shmuel Berman'}, {'name': 'Kathleen McKeown'}, {'name': 'Baishakhi Ray'}]",2024-07-04T14:22:25Z
http://arxiv.org/abs/2407.03942v1,http://arxiv.org/abs/2407.03942v1,"Diverse and Fine-Grained Instruction-Following Ability Exploration with
  Synthetic Data","Instruction-following is particularly crucial for large language models
(LLMs) to support diverse user requests. While existing work has made progress
in aligning LLMs with human preferences, evaluating their capabilities on
instruction following remains a challenge due to complexity and diversity of
real-world user instructions. While existing evaluation methods focus on
general skills, they suffer from two main shortcomings, i.e., lack of
fine-grained task-level evaluation and reliance on singular instruction
expression. To address these problems, this paper introduces DINGO, a
fine-grained and diverse instruction-following evaluation dataset that has two
main advantages: (1) DINGO is based on a manual annotated, fine-grained and
multi-level category tree with 130 nodes derived from real-world user requests;
(2) DINGO includes diverse instructions, generated by both GPT-4 and human
experts. Through extensive experiments, we demonstrate that DINGO can not only
provide more challenging and comprehensive evaluation for LLMs, but also
provide task-level fine-grained directions to further improve LLMs.","[{'name': 'Zihui Gu'}, {'name': 'Xingwu Sun'}, {'name': 'Fengzong Lian'}, {'name': 'Zhanhui Kang'}, {'name': 'Cheng-Zhong Xu'}, {'name': 'Ju Fan'}]",2024-07-04T13:54:41Z
http://arxiv.org/abs/2407.03941v1,http://arxiv.org/abs/2407.03941v1,Narrow Transformer: Starcoder-Based Java-LM For Desktop,"This paper presents NT-Java-1.1B, an open-source specialized code language
model built on StarCoderBase-1.1B, designed for coding tasks in Java
programming. NT-Java-1.1B achieves state-of-the-art performance, surpassing its
base model and majority of other models of similar size on MultiPL-E Java code
benchmark. While there have been studies on extending large, generic
pre-trained models to improve proficiency in specific programming languages
like Python, similar investigations on small code models for other programming
languages are lacking. Large code models require specialized hardware like GPUs
for inference, highlighting the need for research into building small code
models that can be deployed on developer desktops. This paper addresses this
research gap by focusing on the development of a small Java code model,
NT-Java-1.1B, and its quantized versions, which performs comparably to open
models around 1.1B on MultiPL-E Java code benchmarks, making them ideal for
desktop deployment. This paper establishes the foundation for specialized
models across languages and sizes for a family of NT Models.","[{'name': 'Kamalkumar Rathinasamy'}, {'name': 'Balaji A J'}, {'name': 'Ankush Kumar'}, {'name': 'Gagan Gayari'}, {'name': 'Harshini K'}, {'name': 'Rajab Ali Mondal'}, {'name': 'Sreenivasa Raghavan K S'}, {'name': 'Swayam Singh'}]",2024-07-04T13:54:24Z
http://arxiv.org/abs/2407.12842v1,http://arxiv.org/abs/2407.12842v1,MS2SL: Multimodal Spoken Data-Driven Continuous Sign Language Production,"Sign language understanding has made significant strides; however, there is
still no viable solution for generating sign sequences directly from entire
spoken content, e.g., text or speech. In this paper, we propose a unified
framework for continuous sign language production, easing communication between
sign and non-sign language users. In particular, a sequence diffusion model,
utilizing embeddings extracted from text or speech, is crafted to generate sign
predictions step by step. Moreover, by creating a joint embedding space for
text, audio, and sign, we bind these modalities and leverage the semantic
consistency among them to provide informative feedback for the model training.
This embedding-consistency learning strategy minimizes the reliance on sign
triplets and ensures continuous model refinement, even with a missing audio
modality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our
model achieves competitive performance in sign language production.","[{'name': 'Jian Ma'}, {'name': 'Wenguan Wang'}, {'name': 'Yi Yang'}, {'name': 'Feng Zheng'}]",2024-07-04T13:53:50Z
http://arxiv.org/abs/2407.03937v1,http://arxiv.org/abs/2407.03937v1,"TongGu: Mastering Classical Chinese Understanding with
  Knowledge-Grounded Large Language Models","Classical Chinese is a gateway to the rich heritage and wisdom of ancient
China, yet its complexities pose formidable comprehension barriers for most
modern people without specialized knowledge. While Large Language Models (LLMs)
have shown remarkable capabilities in Natural Language Processing (NLP), they
struggle with Classical Chinese Understanding (CCU), especially in
data-demanding and knowledge-intensive tasks. In response to this dilemma, we
propose \textbf{TongGu} (mean understanding ancient and modern), the first
CCU-specific LLM, underpinned by three core contributions. First, we construct
a two-stage instruction-tuning dataset ACCN-INS derived from rich classical
Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we
propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,
enabling TongGu to acquire new capabilities while preserving its foundational
knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)
technique to reduce hallucinations based on knowledge-grounding. Extensive
experiments across 24 diverse CCU tasks validate TongGu's superior ability,
underscoring the effectiveness of RAT and CCU-RAG. The model and dataset will
be public available.","[{'name': 'Jiahuan Cao'}, {'name': 'Dezhi Peng'}, {'name': 'Peirong Zhang'}, {'name': 'Yongxin Shi'}, {'name': 'Yang Liu'}, {'name': 'Kai Ding'}, {'name': 'Lianwen Jin'}]",2024-07-04T13:52:23Z
http://arxiv.org/abs/2407.03916v1,http://arxiv.org/abs/2407.03916v1,Entity-Level Sentiment: More than the Sum of Its Parts,"In sentiment analysis of longer texts, there may be a variety of topics
discussed, of entities mentioned, and of sentiments expressed regarding each
entity. We find a lack of studies exploring how such texts express their
sentiment towards each entity of interest, and how these sentiments can be
modelled. In order to better understand how sentiment regarding persons and
organizations (each entity in our scope) is expressed in longer texts, we have
collected a dataset of expert annotations where the overall sentiment regarding
each entity is identified, together with the sentence-level sentiment for these
entities separately. We show that the reader's perceived sentiment regarding an
entity often differs from an arithmetic aggregation of sentiments at the
sentence level. Only 70\% of the positive and 55\% of the negative entities
receive a correct overall sentiment label when we aggregate the
(human-annotated) sentiment labels for the sentences where the entity is
mentioned. Our dataset reveals the complexity of entity-specific sentiment in
longer texts, and allows for more precise modelling and evaluation of such
sentiment expressions.","[{'name': 'Egil Rønningstad'}, {'name': 'Roman Klinger'}, {'name': 'Erik Velldal'}, {'name': 'Lilja Øvrelid'}]",2024-07-04T13:21:07Z
http://arxiv.org/abs/2407.12841v1,http://arxiv.org/abs/2407.12841v1,"What to do if language models disagree? Black-box model ensembling for
  textual and visual question answering","A diverse range of large language models (LLMs), e.g., ChatGPT, and visual
question answering (VQA) models, e.g., BLIP, have been developed for solving
textual and visual question answering tasks. However, both LLMs and VQA models
encounter challenges when applied to task-specific datasets. Fine-tuning these
models is either difficult, as it requires access via APIs, rendering them as
black-boxes, or costly due to the need of tuning a large number of parameters.
To address this, we introduce InfoSel, a data-efficient and lightweight
ensemble method that learns to dynamically pick the winner from existing
black-box models for predictions on both textual and multimodal visual question
answering tasks. Unlike traditional ensemble models, InfoSel does not rely on
prediction probabilities or confidences, which typically are not available in
black-box models. Experimental results on four datasets demonstrate that our
approach achieves an absolute increase of up to +5.27% in the F1-score compared
to standalone LLMs. Remarkably, this improvement is achieved by utilizing only
1K training instances and 110M model parameters for training task-specific
ensemble models.","[{'name': 'Yuxi Xia'}, {'name': 'Kilm Zaporojets'}, {'name': 'Benjamin Roth'}]",2024-07-04T12:59:10Z
http://arxiv.org/abs/2407.03895v1,http://arxiv.org/abs/2407.03895v1,"Scoping Review of Active Learning Strategies and their Evaluation
  Environments for Entity Recognition Tasks","We conducted a scoping review for active learning in the domain of natural
language processing (NLP), which we summarize in accordance with the PRISMA-ScR
guidelines as follows:
  Objective: Identify active learning strategies that were proposed for entity
recognition and their evaluation environments (datasets, metrics, hardware,
execution time). Design: We used Scopus and ACM as our search engines. We
compared the results with two literature surveys to assess the search quality.
We included peer-reviewed English publications introducing or comparing active
learning strategies for entity recognition. Results: We analyzed 62 relevant
papers and identified 106 active learning strategies. We grouped them into
three categories: exploitation-based (60x), exploration-based (14x), and hybrid
strategies (32x). We found that all studies used the F1-score as an evaluation
metric. Information about hardware (6x) and execution time (13x) was only
occasionally included. The 62 papers used 57 different datasets to evaluate
their respective strategies. Most datasets contained newspaper articles or
biomedical/medical data. Our analysis revealed that 26 out of 57 datasets are
publicly accessible.
  Conclusion: Numerous active learning strategies have been identified, along
with significant open questions that still need to be addressed. Researchers
and practitioners face difficulties when making data-driven decisions about
which active learning strategy to adopt. Conducting comprehensive empirical
comparisons using the evaluation environment proposed in this study could help
establish best practices in the domain.","[{'name': 'Philipp Kohl'}, {'name': 'Yoka Krämer'}, {'name': 'Claudia Fohry'}, {'name': 'Bodo Kraft'}]",2024-07-04T12:40:35Z
http://arxiv.org/abs/2407.03884v1,http://arxiv.org/abs/2407.03884v1,Planning with Large Language Models for Conversational Agents,"Controllability and proactivity are crucial properties of autonomous
conversational agents (CAs). Controllability requires the CAs to follow the
standard operating procedures (SOPs), such as verifying identity before
activating credit cards. Proactivity requires the CAs to guide the conversation
towards the goal during user uncooperation, such as persuasive dialogue.
Existing research cannot be unified with controllability, proactivity, and low
manual annotation. To bridge this gap, we propose a new framework for
planning-based conversational agents (PCA) powered by large language models
(LLMs), which only requires humans to define tasks and goals for the LLMs.
Before conversation, LLM plans the core and necessary SOP for dialogue offline.
During the conversation, LLM plans the best action path online referring to the
SOP, and generates responses to achieve process controllability. Subsequently,
we propose a semi-automatic dialogue data creation framework and curate a
high-quality dialogue dataset (PCA-D). Meanwhile, we develop multiple variants
and evaluation metrics for PCA, e.g., planning with Monte Carlo Tree Search
(PCA-M), which searches for the optimal dialogue action while satisfying SOP
constraints and achieving the proactive of the dialogue. Experiment results
show that LLMs finetuned on PCA-D can significantly improve the performance and
generalize to unseen domains. PCA-M outperforms other CoT and ToT baselines in
terms of conversation controllability, proactivity, task success rate, and
overall logical coherence, and is applicable in industry dialogue scenarios.
The dataset and codes are available at XXXX.","[{'name': 'Zhigen Li'}, {'name': 'Jianxiang Peng'}, {'name': 'Yanmeng Wang'}, {'name': 'Tianhao Shen'}, {'name': 'Minghui Zhang'}, {'name': 'Linxi Su'}, {'name': 'Shang Wu'}, {'name': 'Yihang Wu'}, {'name': 'Yuqian Wang'}, {'name': 'Ye Wang'}, {'name': 'Wei Hu'}, {'name': 'Jianfeng Li'}, {'name': 'Shaojun Wang'}, {'name': 'Jing Xiao'}, {'name': 'Deyi Xiong'}]",2024-07-04T12:23:02Z
http://arxiv.org/abs/2407.16893v1,http://arxiv.org/abs/2407.16893v1,"The Price of Prompting: Profiling Energy Use in Large Language Models
  Inference","In the rapidly evolving realm of artificial intelligence, deploying large
language models (LLMs) poses increasingly pressing computational and
environmental challenges. This paper introduces MELODI - Monitoring Energy
Levels and Optimization for Data-driven Inference - a multifaceted framework
crafted to monitor and analyze the energy consumed during LLM inference
processes. MELODI enables detailed observations of power consumption dynamics
and facilitates the creation of a comprehensive dataset reflective of energy
efficiency across varied deployment scenarios. The dataset, generated using
MELODI, encompasses a broad spectrum of LLM deployment frameworks, multiple
language models, and extensive prompt datasets, enabling a comparative analysis
of energy use. Using the dataset, we investigate how prompt attributes,
including length and complexity, correlate with energy expenditure. Our
findings indicate substantial disparities in energy efficiency, suggesting
ample scope for optimization and adoption of sustainable measures in LLM
deployment. Our contribution lies not only in the MELODI framework but also in
the novel dataset, a resource that can be expanded by other researchers. Thus,
MELODI is a foundational tool and dataset for advancing research into
energy-conscious LLM deployment, steering the field toward a more sustainable
future.","[{'name': 'Erik Johannes Husom'}, {'name': 'Arda Goknil'}, {'name': 'Lwin Khin Shar'}, {'name': 'Sagar Sen'}]",2024-07-04T12:16:28Z
http://arxiv.org/abs/2407.03876v1,http://arxiv.org/abs/2407.03876v1,DART: Deep Adversarial Automated Red Teaming for LLM Safety,"Manual Red teaming is a commonly-used method to identify vulnerabilities in
large language models (LLMs), which, is costly and unscalable. In contrast,
automated red teaming uses a Red LLM to automatically generate adversarial
prompts to the Target LLM, offering a scalable way for safety vulnerability
detection. However, the difficulty of building a powerful automated Red LLM
lies in the fact that the safety vulnerabilities of the Target LLM are
dynamically changing with the evolution of the Target LLM. To mitigate this
issue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in
which the Red LLM and Target LLM are deeply and dynamically interacting with
each other in an iterative manner. In each iteration, in order to generate
successful attacks as many as possible, the Red LLM not only takes into account
the responses from the Target LLM, but also adversarially adjust its attacking
directions by monitoring the global diversity of generated attacks across
multiple iterations. Simultaneously, to explore dynamically changing safety
vulnerabilities of the Target LLM, we allow the Target LLM to enhance its
safety via an active learning based data selection mechanism. Experimential
results demonstrate that DART significantly reduces the safety risk of the
target LLM. For human evaluation on Anthropic Harmless dataset, compared to the
instruction-tuning target LLM, DART eliminates the violation risks by 53.4\%.
We will release the datasets and codes of DART soon.","[{'name': 'Bojian Jiang'}, {'name': 'Yi Jing'}, {'name': 'Tianhao Shen'}, {'name': 'Qing Yang'}, {'name': 'Deyi Xiong'}]",2024-07-04T12:14:27Z
http://arxiv.org/abs/2407.03861v1,http://arxiv.org/abs/2407.03861v1,"TartuNLP @ AXOLOTL-24: Leveraging Classifier Output for New Sense
  Detection in Lexical Semantics","We present our submission to the AXOLOTL-24 shared task. The shared task
comprises two subtasks: identifying new senses that words gain with time (when
comparing newer and older time periods) and producing the definitions for the
identified new senses. We implemented a conceptually simple and computationally
inexpensive solution to both subtasks. We trained adapter-based binary
classification models to match glosses with usage examples and leveraged the
probability output of the models to identify novel senses. The same models were
used to match examples of novel sense usages with Wiktionary definitions. Our
submission attained third place on the first subtask and the first place on the
second subtask.","[{'name': 'Aleksei Dorkin'}, {'name': 'Kairit Sirts'}]",2024-07-04T11:46:39Z
http://arxiv.org/abs/2407.03859v1,http://arxiv.org/abs/2407.03859v1,Anthropocentric bias and the possibility of artificial cognition,"Evaluating the cognitive capacities of large language models (LLMs) requires
overcoming not only anthropomorphic but also anthropocentric biases. This
article identifies two types of anthropocentric bias that have been neglected:
overlooking how auxiliary factors can impede LLM performance despite competence
(Type-I), and dismissing LLM mechanistic strategies that differ from those of
humans as not genuinely competent (Type-II). Mitigating these biases
necessitates an empirically-driven, iterative approach to mapping cognitive
tasks to LLM-specific capacities and mechanisms, which can be done by
supplementing carefully designed behavioral experiments with mechanistic
studies.","[{'name': 'Raphaël Millière'}, {'name': 'Charles Rathkopf'}]",2024-07-04T11:44:28Z
http://arxiv.org/abs/2407.03850v1,http://arxiv.org/abs/2407.03850v1,"HYBRINFOX at CheckThat! 2024 -- Task 1: Enhancing Language Models with
  Structured Information for Check-Worthiness Estimation","This paper summarizes the experiments and results of the HYBRINFOX team for
the CheckThat! 2024 - Task 1 competition. We propose an approach enriching
Language Models such as RoBERTa with embeddings produced by triples (subject ;
predicate ; object) extracted from the text sentences. Our analysis of the
developmental data shows that this method improves the performance of Language
Models alone. On the evaluation data, its best performance was in English,
where it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On
the other languages (Dutch and Arabic), it obtained more mixed results. Future
research tracks are identified toward adapting this processing pipeline to more
recent Large Language Models.","[{'name': 'Géraud Faye'}, {'name': 'Morgane Casanova'}, {'name': 'Benjamin Icard'}, {'name': 'Julien Chanson'}, {'name': 'Guillaume Gadek'}, {'name': 'Guillaume Gravier'}, {'name': 'Paul Égré'}]",2024-07-04T11:33:54Z
http://arxiv.org/abs/2407.03841v1,http://arxiv.org/abs/2407.03841v1,On the Benchmarking of LLMs for Open-Domain Dialogue Evaluation,"Large Language Models (LLMs) have showcased remarkable capabilities in
various Natural Language Processing tasks. For automatic open-domain dialogue
evaluation in particular, LLMs have been seamlessly integrated into evaluation
frameworks, and together with human evaluation, compose the backbone of most
evaluations. However, existing evaluation benchmarks often rely on outdated
datasets and evaluate aspects like Fluency and Relevance, which fail to
adequately capture the capabilities and limitations of state-of-the-art chatbot
models.
  This paper critically examines current evaluation benchmarks, highlighting
that the use of older response generators and quality aspects fail to
accurately reflect modern chatbot capabilities. A small annotation experiment
on a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as
GPT-4 struggle to detect actual deficiencies in dialogues generated by current
LLM chatbots.","[{'name': 'John Mendonça'}, {'name': 'Alon Lavie'}, {'name': 'Isabel Trancoso'}]",2024-07-04T11:14:47Z
http://arxiv.org/abs/2407.03818v1,http://arxiv.org/abs/2407.03818v1,"ConText at WASSA 2024 Empathy and Personality Shared Task:
  History-Dependent Embedding Utterance Representations for Empathy and Emotion
  Prediction in Conversations","Empathy and emotion prediction are key components in the development of
effective and empathetic agents, amongst several other applications. The WASSA
shared task on empathy and emotion prediction in interactions presents an
opportunity to benchmark approaches to these tasks. Appropriately selecting and
representing the historical context is crucial in the modelling of empathy and
emotion in conversations. In our submissions, we model empathy, emotion
polarity and emotion intensity of each utterance in a conversation by feeding
the utterance to be classified together with its conversational context, i.e.,
a certain number of previous conversational turns, as input to an encoder
Pre-trained Language Model, to which we append a regression head for
prediction. We also model perceived counterparty empathy of each interlocutor
by feeding all utterances from the conversation and a token identifying the
interlocutor for which we are predicting the empathy. Our system officially
ranked $1^{st}$ at the CONV-turn track and $2^{nd}$ at the CONV-dialog track.","[{'name': 'Patrícia Pereira'}, {'name': 'Helena Moniz'}, {'name': 'Joao Paulo Carvalho'}]",2024-07-04T10:44:59Z
http://arxiv.org/abs/2407.03809v1,http://arxiv.org/abs/2407.03809v1,"Finetuning End-to-End Models for Estonian Conversational Spoken Language
  Translation","This paper investigates the finetuning of end-to-end models for bidirectional
Estonian-English and Estonian-Russian conversational speech-to-text
translation. Due to the limited availability of speech translation data for
Estonian, we created additional training data by web scraping and synthesizing
data from speech recognition datasets using machine translation. We evaluated
three publicly available end-to-end models: Whisper, OWSM 3.1, and SeamlessM4T.
Our results indicate that fine-tuning with synthetic data enhances translation
accuracy by a large margin, with SeamlessM4T matching or surpassing cascaded
speech translation systems that use state-of-the-art speech recognition and
machine translation models.","[{'name': 'Tiia Sildam'}, {'name': 'Andra Velve'}, {'name': 'Tanel Alumäe'}]",2024-07-04T10:33:12Z
http://arxiv.org/abs/2407.03805v2,http://arxiv.org/abs/2407.03805v2,"Cognitive Modeling with Scaffolded LLMs: A Case Study of Referential
  Expression Generation","To what extent can LLMs be used as part of a cognitive model of language
generation? In this paper, we approach this question by exploring a
neuro-symbolic implementation of an algorithmic cognitive model of referential
expression generation by Dale & Reiter (1995). The symbolic task analysis
implements the generation as an iterative procedure that scaffolds symbolic and
gpt-3.5-turbo-based modules. We compare this implementation to an ablated model
and a one-shot LLM-only baseline on the A3DS dataset (Tsvilodub & Franke,
2023). We find that our hybrid approach is cognitively plausible and performs
well in complex contexts, while allowing for more open-ended modeling of
language generation in a larger domain.","[{'name': 'Polina Tsvilodub'}, {'name': 'Michael Franke'}, {'name': 'Fausto Carcassi'}]",2024-07-04T10:28:48Z
http://arxiv.org/abs/2407.03791v1,http://arxiv.org/abs/2407.03791v1,"M$\mathbf5$ -- A Diverse Benchmark to Assess the Performance of Large
  Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks","Since the release of ChatGPT, the field of Natural Language Processing has
experienced rapid advancements, particularly in Large Language Models (LLMs)
and their multimodal counterparts, Large Multimodal Models (LMMs). Despite
their impressive capabilities, LLMs often exhibit significant performance
disparities across different languages and cultural contexts, as demonstrated
by various text-only benchmarks. However, current research lacks such
benchmarks for multimodal visio-linguistic settings. This work fills this gap
by introducing M5, the first comprehensive benchmark designed to evaluate LMMs
on diverse vision-language tasks within a multilingual and multicultural
context. M5 includes eight datasets covering five tasks and $41$ languages,
with a focus on underrepresented languages and culturally diverse images.
Furthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a
new Visio-Linguistic Outlier Detection task, in which all evaluated open-source
models fail to significantly surpass the random baseline. Through extensive
evaluation and analyses, we highlight substantial task-agnostic performance
disparities between high- and low-resource languages. Moreover, we show that
larger models do not necessarily outperform smaller ones in a multilingual
setting.","[{'name': 'Florian Schneider'}, {'name': 'Sunayana Sitaram'}]",2024-07-04T09:55:04Z
http://arxiv.org/abs/2407.03788v2,http://arxiv.org/abs/2407.03788v2,"Meta-optimized Angular Margin Contrastive Framework for Video-Language
  Representation Learning","Data quality stands at the forefront of deciding the effectiveness of
video-language representation learning. However, video-text pairs in previous
data typically do not align perfectly with each other, which might lead to
video-language representations that do not accurately reflect cross-modal
semantics. Moreover, previous data also possess an uneven distribution of
concepts, thereby hampering the downstream performance across unpopular
subjects. To address these problems, we propose a contrastive objective with a
subtractive angular margin to regularize cross-modal representations in their
effort to reach perfect similarity. Furthermore, to adapt to the non-uniform
concept distribution, we propose a multi-layer perceptron (MLP)-parameterized
weighting function that maps loss values to sample weights which enable dynamic
adjustment of the model's focus throughout the training. With the training
guided by a small amount of unbiased meta-data and augmented by video-text data
generated by large vision-language model, we improve video-language
representations and achieve superior performances on commonly used video
question answering and text-video retrieval datasets.","[{'name': 'Thong Nguyen'}, {'name': 'Yi Bin'}, {'name': 'Xiaobao Wu'}, {'name': 'Xinshuai Dong'}, {'name': 'Zhiyuan Hu'}, {'name': 'Khoi Le'}, {'name': 'Cong-Duy Nguyen'}, {'name': 'See-Kiong Ng'}, {'name': 'Luu Anh Tuan'}]",2024-07-04T09:52:17Z
http://arxiv.org/abs/2407.03779v1,http://arxiv.org/abs/2407.03779v1,"Functional Faithfulness in the Wild: Circuit Discovery with
  Differentiable Computation Graph Pruning","In this paper, we introduce a comprehensive reformulation of the task known
as Circuit Discovery, along with DiscoGP, a novel and effective algorithm based
on differentiable masking for discovering circuits. Circuit discovery is the
task of interpreting the computational mechanisms of language models (LMs) by
dissecting their functions and capabilities into sparse subnetworks (circuits).
We identified two major limitations in existing circuit discovery efforts: (1)
a dichotomy between weight-based and connection-edge-based approaches forces
researchers to choose between pruning connections or weights, thereby limiting
the scope of mechanistic interpretation of LMs; (2) algorithms based on
activation patching tend to identify circuits that are neither functionally
faithful nor complete. The performance of these identified circuits is
substantially reduced, often resulting in near-random performance in isolation.
Furthermore, the complement of the circuit -- i.e., the original LM with the
identified circuit removed -- still retains adequate performance, indicating
that essential components of a complete circuits are missed by existing
methods.
  DiscoGP successfully addresses the two aforementioned issues and demonstrates
state-of-the-art faithfulness, completeness, and sparsity. The effectiveness of
the algorithm and its novel structure open up new avenues of gathering new
insights into the internal workings of generative AI.","[{'name': 'Lei Yu'}, {'name': 'Jingcheng Niu'}, {'name': 'Zining Zhu'}, {'name': 'Gerald Penn'}]",2024-07-04T09:42:25Z
http://arxiv.org/abs/2407.03778v1,http://arxiv.org/abs/2407.03778v1,"From Data to Commonsense Reasoning: The Use of Large Language Models for
  Explainable AI","Commonsense reasoning is a difficult task for a computer, but a critical
skill for an artificial intelligence (AI). It can enhance the explainability of
AI models by enabling them to provide intuitive and human-like explanations for
their decisions. This is necessary in many areas especially in question
answering (QA), which is one of the most important tasks of natural language
processing (NLP). Over time, a multitude of methods have emerged for solving
commonsense reasoning problems such as knowledge-based approaches using formal
logic or linguistic analysis. In this paper, we investigate the effectiveness
of large language models (LLMs) on different QA tasks with a focus on their
abilities in reasoning and explainability. We study three LLMs: GPT-3.5, Gemma
and Llama 3. We further evaluate the LLM results by means of a questionnaire.
We demonstrate the ability of LLMs to reason with commonsense as the models
outperform humans on different datasets. While GPT-3.5's accuracy ranges from
56% to 93% on various QA benchmarks, Llama 3 achieved a mean accuracy of 90% on
all eleven datasets. Thereby Llama 3 is outperforming humans on all datasets
with an average 21% higher accuracy over ten datasets. Furthermore, we can
appraise that, in the sense of explainable artificial intelligence (XAI),
GPT-3.5 provides good explanations for its decisions. Our questionnaire
revealed that 66% of participants rated GPT-3.5's explanations as either ""good""
or ""excellent"". Taken together, these findings enrich our understanding of
current LLMs and pave the way for future investigations of reasoning and
explainability.","[{'name': 'Stefanie Krause'}, {'name': 'Frieder Stolzenburg'}]",2024-07-04T09:38:49Z
http://arxiv.org/abs/2407.03770v1,http://arxiv.org/abs/2407.03770v1,"HYBRINFOX at CheckThat! 2024 -- Task 2: Enriching BERT Models with the
  Expert System VAGO for Subjectivity Detection","This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity
detection of the CLEF 2024 CheckThat! competition. The specificity of the
method is to use a hybrid system, combining a RoBERTa model, fine-tuned for
subjectivity detection, a frozen sentence-BERT (sBERT) model to capture
semantics, and several scores calculated by the English version of the expert
system VAGO, developed independently of this task to measure vagueness and
subjectivity in texts based on the lexicon. In English, the HYBRINFOX method
ranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the
other languages, the method used a translation step into English, producing
more mixed results (ranking 1st in Multilingual and 2nd in Italian over the
baseline, but under the baseline in Bulgarian, German, and Arabic). We explain
the principles of our hybrid approach, and outline ways in which the method
could be improved for other languages besides English.","[{'name': 'Morgane Casanova'}, {'name': 'Julien Chanson'}, {'name': 'Benjamin Icard'}, {'name': 'Géraud Faye'}, {'name': 'Guillaume Gadek'}, {'name': 'Guillaume Gravier'}, {'name': 'Paul Égré'}]",2024-07-04T09:29:19Z
http://arxiv.org/abs/2407.03759v1,http://arxiv.org/abs/2407.03759v1,"Convolutional vs Large Language Models for Software Log Classification
  in Edge-Deployable Cellular Network Testing","Software logs generated by sophisticated network emulators in the
telecommunications industry, such as VIAVI TM500, are extremely complex, often
comprising tens of thousands of text lines with minimal resemblance to natural
language. Only specialised expert engineers can decipher such logs and
troubleshoot defects in test runs. While AI offers a promising solution for
automating defect triage, potentially leading to massive revenue savings for
companies, state-of-the-art large language models (LLMs) suffer from
significant drawbacks in this specialised domain. These include a constrained
context window, limited applicability to text beyond natural language, and high
inference costs. To address these limitations, we propose a compact
convolutional neural network (CNN) architecture that offers a context window
spanning up to 200,000 characters and achieves over 96% accuracy (F1>0.9) in
classifying multifaceted software logs into various layers in the
telecommunications protocol stack. Specifically, the proposed model is capable
of identifying defects in test runs and triaging them to the relevant
department, formerly a manual engineering process that required expert
knowledge. We evaluate several LLMs; LLaMA2-7B, Mixtral 8x7B, Flan-T5, BERT and
BigBird, and experimentally demonstrate their shortcomings in our specialized
application. Despite being lightweight, our CNN significantly outperforms
LLM-based approaches in telecommunications log classification while minimizing
the cost of production. Our defect triaging AI model is deployable on edge
devices without dedicated hardware and widely applicable across software logs
in various industries.","[{'name': 'Achintha Ihalage'}, {'name': 'Sayed M. Taheri'}, {'name': 'Faris Muhammad'}, {'name': 'Hamed Al-Raweshidy'}]",2024-07-04T09:12:08Z
http://arxiv.org/abs/2407.03748v1,http://arxiv.org/abs/2407.03748v1,"Argument Mining in Data Scarce Settings: Cross-lingual Transfer and
  Few-shot Techniques","Recent research on sequence labelling has been exploring different strategies
to mitigate the lack of manually annotated data for the large majority of the
world languages. Among others, the most successful approaches have been based
on (i) the cross-lingual transfer capabilities of multilingual pre-trained
language models (model-transfer), (ii) data translation and label projection
(data-transfer) and (iii), prompt-based learning by reusing the mask objective
to exploit the few-shot capabilities of pre-trained language models (few-shot).
Previous work seems to conclude that model-transfer outperforms data-transfer
methods and that few-shot techniques based on prompting are superior to
updating the model's weights via fine-tuning. In this paper, we empirically
demonstrate that, for Argument Mining, a sequence labelling task which requires
the detection of long and complex discourse structures, previous insights on
cross-lingual transfer or few-shot learning do not apply. Contrary to previous
work, we show that for Argument Mining data transfer obtains better results
than model-transfer and that fine-tuning outperforms few-shot methods.
Regarding the former, the domain of the dataset used for data-transfer seems to
be a deciding factor, while, for few-shot, the type of task (length and
complexity of the sequence spans) and sampling method prove to be crucial.","[{'name': 'Anar Yeginbergen'}, {'name': 'Maite Oronoz'}, {'name': 'Rodrigo Agerri'}]",2024-07-04T08:59:17Z
http://arxiv.org/abs/2407.03734v1,http://arxiv.org/abs/2407.03734v1,Improving Self-supervised Pre-training using Accent-Specific Codebooks,"Speech accents present a serious challenge to the performance of
state-of-the-art end-to-end Automatic Speech Recognition (ASR) systems. Even
with self-supervised learning and pre-training of ASR models, accent invariance
is seldom achieved. In this work, we propose an accent-aware adaptation
technique for self-supervised learning that introduces a trainable set of
accent-specific codebooks to the self-supervised architecture. These learnable
codebooks enable the model to capture accent specific information during
pre-training, that is further refined during ASR finetuning. On the Mozilla
Common Voice dataset, our proposed approach outperforms all other
accent-adaptation approaches on both seen and unseen English accents, with up
to 9% relative reduction in word error rate (WER).","[{'name': 'Darshan Prabhu'}, {'name': 'Abhishek Gupta'}, {'name': 'Omkar Nitsure'}, {'name': 'Preethi Jyothi'}, {'name': 'Sriram Ganapathy'}]",2024-07-04T08:33:52Z
http://arxiv.org/abs/2407.03720v1,http://arxiv.org/abs/2407.03720v1,Query-oriented Data Augmentation for Session Search,"Modeling contextual information in a search session has drawn more and more
attention when understanding complex user intents. Recent methods are all
data-driven, i.e., they train different models on large-scale search log data
to identify the relevance between search contexts and candidate documents. The
common training paradigm is to pair the search context with different candidate
documents and train the model to rank the clicked documents higher than the
unclicked ones. However, this paradigm neglects the symmetric nature of the
relevance between the session context and document, i.e., the clicked documents
can also be paired with different search contexts when training. In this work,
we propose query-oriented data augmentation to enrich search logs and empower
the modeling. We generate supplemental training pairs by altering the most
important part of a search context, i.e., the current query, and train our
model to rank the generated sequence along with the original sequence. This
approach enables models to learn that the relevance of a document may vary as
the session context changes, leading to a better understanding of users' search
patterns. We develop several strategies to alter the current query, resulting
in new training data with varying degrees of difficulty. Through
experimentation on two extensive public search logs, we have successfully
demonstrated the effectiveness of our model.","[{'name': 'Haonan Chen'}, {'name': 'Zhicheng Dou'}, {'name': 'Yutao Zhu'}, {'name': 'Ji-Rong Wen'}]",2024-07-04T08:08:33Z
http://arxiv.org/abs/2407.03718v2,http://arxiv.org/abs/2407.03718v2,Multi-Convformer: Extending Conformer with Multiple Convolution Kernels,"Convolutions have become essential in state-of-the-art end-to-end Automatic
Speech Recognition~(ASR) systems due to their efficient modelling of local
context. Notably, its use in Conformers has led to superior performance
compared to vanilla Transformer-based ASR systems. While components other than
the convolution module in the Conformer have been reexamined, altering the
convolution module itself has been far less explored. Towards this, we
introduce Multi-Convformer that uses multiple convolution kernels within the
convolution module of the Conformer in conjunction with gating. This helps in
improved modeling of local dependencies at varying granularities. Our model
rivals existing Conformer variants such as CgMLP and E-Branchformer in
performance, while being more parameter efficient. We empirically compare our
approach with Conformer and its variants across four different datasets and
three different modelling paradigms and show up to 8% relative word error
rate~(WER) improvements.","[{'name': 'Darshan Prabhu'}, {'name': 'Yifan Peng'}, {'name': 'Preethi Jyothi'}, {'name': 'Shinji Watanabe'}]",2024-07-04T08:08:12Z
http://arxiv.org/abs/2407.03689v1,http://arxiv.org/abs/2407.03689v1,"Text2TimeSeries: Enhancing Financial Forecasting through Time Series
  Prediction Updates with Event-Driven Insights from Large Language Models","Time series models, typically trained on numerical data, are designed to
forecast future values. These models often rely on weighted averaging
techniques over time intervals. However, real-world time series data is seldom
isolated and is frequently influenced by non-numeric factors. For instance,
stock price fluctuations are impacted by daily random events in the broader
world, with each event exerting a unique influence on price signals.
Previously, forecasts in financial markets have been approached in two main
ways: either as time-series problems over price sequence or sentiment analysis
tasks. The sentiment analysis tasks aim to determine whether news events will
have a positive or negative impact on stock prices, often categorizing them
into discrete labels. Recognizing the need for a more comprehensive approach to
accurately model time series prediction, we propose a collaborative modeling
framework that incorporates textual information about relevant events for
predictions. Specifically, we leverage the intuition of large language models
about future changes to update real number time series predictions. We
evaluated the effectiveness of our approach on financial market data.","[{'name': 'Litton Jose Kurisinkel'}, {'name': 'Pruthwik Mishra'}, {'name': 'Yue Zhang'}]",2024-07-04T07:21:38Z
http://arxiv.org/abs/2407.03687v1,http://arxiv.org/abs/2407.03687v1,"STOC-TOT: Stochastic Tree-of-Thought with Constrained Decoding for
  Complex Reasoning in Multi-Hop Question Answering","Multi-hop question answering (MHQA) requires a model to retrieve and
integrate information from multiple passages to answer a complex question.
Recent systems leverage the power of large language models and integrate
evidence retrieval with reasoning prompts (e.g., chain-of-thought reasoning)
for the MHQA task. However, the complexities in the question types (bridge v.s.
comparison questions) and the reasoning types (sequential v.s. parallel
reasonings) require more novel and fine-grained prompting methods to enhance
the performance of MHQA under the zero-shot setting. In this paper, we propose
STOC-TOT, a stochastic tree-of-thought reasoning prompting method with
constrained decoding for MHQA and conduct a detailed comparison with other
reasoning prompts on different question types and reasoning types.
Specifically, we construct a tree-like reasoning structure by prompting the
model to break down the original question into smaller sub-questions to form
different reasoning paths. In addition, we prompt the model to provide a
probability estimation for each reasoning path at each reasoning step. At
answer time, we conduct constrained decoding on the model to generate more
grounded answers and reduce hallucination. Experiments comparing STOC-TOT with
two MHQA datasets and five large language models showed that our framework
outperforms other reasoning prompts by a significant margin.","[{'name': 'Zhenyu Bi'}, {'name': 'Daniel Hajialigol'}, {'name': 'Zhongkai Sun'}, {'name': 'Jie Hao'}, {'name': 'Xuan Wang'}]",2024-07-04T07:17:53Z
http://arxiv.org/abs/2407.03678v1,http://arxiv.org/abs/2407.03678v1,Improving Self Consistency in LLMs through Probabilistic Tokenization,"Prior research has demonstrated noticeable performance gains through the use
of probabilistic tokenizations, an approach that involves employing multiple
tokenizations of the same input string during the training phase of a language
model. Despite these promising findings, modern large language models (LLMs)
have yet to be trained using probabilistic tokenizations. Interestingly, while
the tokenizers of these contemporary LLMs have the capability to generate
multiple tokenizations, this property remains underutilized.
  In this work, we propose a novel method to leverage the multiple tokenization
capabilities of modern LLM tokenizers, aiming to enhance the self-consistency
of LLMs in reasoning tasks. Our experiments indicate that when utilizing
probabilistic tokenizations, LLMs generate logically diverse reasoning paths,
moving beyond mere surface-level linguistic diversity.We carefully study
probabilistic tokenization and offer insights to explain the self consistency
improvements it brings through extensive experimentation on 5 LLM families and
4 reasoning benchmarks.","[{'name': 'Ashutosh Sathe'}, {'name': 'Divyanshu Aggarwal'}, {'name': 'Sunayana Sitaram'}]",2024-07-04T06:52:48Z
http://arxiv.org/abs/2407.03646v2,http://arxiv.org/abs/2407.03646v2,"Differentiating between human-written and AI-generated texts using
  linguistic features automatically extracted from an online computational tool","While extensive research has focused on ChatGPT in recent years, very few
studies have systematically quantified and compared linguistic features between
human-written and Artificial Intelligence (AI)-generated language. This study
aims to investigate how various linguistic components are represented in both
types of texts, assessing the ability of AI to emulate human writing. Using
human-authored essays as a benchmark, we prompted ChatGPT to generate essays of
equivalent length. These texts were analyzed using Open Brain AI, an online
computational tool, to extract measures of phonological, morphological,
syntactic, and lexical constituents. Despite AI-generated texts appearing to
mimic human speech, the results revealed significant differences across
multiple linguistic features such as consonants, word stress, nouns, verbs,
pronouns, direct objects, prepositional modifiers, and use of difficult words
among others. These findings underscore the importance of integrating automated
tools for efficient language assessment, reducing time and effort in data
analysis. Moreover, they emphasize the necessity for enhanced training
methodologies to improve the capacity of AI for producing more human-like text.",[{'name': 'Georgios P. Georgiou'}],2024-07-04T05:37:09Z
http://arxiv.org/abs/2407.03645v2,http://arxiv.org/abs/2407.03645v2,"Continual Learning Optimizations for Auto-regressive Decoder of
  Multilingual ASR systems","Continual Learning (CL) involves fine-tuning pre-trained models with new data
while maintaining the performance on the pre-trained data. This is particularly
relevant for expanding multilingual ASR (MASR) capabilities. However, existing
CL methods, mainly designed for computer vision and reinforcement learning
tasks, often yield sub-optimal results when directly applied to MASR. We
hypothesise that this is because CL of the auto-regressive decoder in the MASR
model is difficult. To verify this, we propose four optimizations on the
decoder. They include decoder-layer gradient surgery, freezing unused token
embeddings, suppressing output of newly added tokens, and learning rate
re-scaling. Our experiments on adapting Whisper to 10 unseen languages from the
Common Voice dataset demonstrate that these optimizations reduce the Average
Word Error Rate (AWER) of pretrained languages from 14.2% to 12.4% compared
with Experience Replay, without compromising the AWER of new languages.","[{'name': 'Chin Yuen Kwok'}, {'name': 'Jia Qi Yip'}, {'name': 'Eng Siong Chng'}]",2024-07-04T05:35:47Z
http://arxiv.org/abs/2407.03627v3,http://arxiv.org/abs/2407.03627v3,"DSLR: Document Refinement with Sentence-Level Re-ranking and
  Reconstruction to Enhance Retrieval-Augmented Generation","Recent advancements in Large Language Models (LLMs) have significantly
improved their performance across various Natural Language Processing (NLP)
tasks. However, LLMs still struggle with generating non-factual responses due
to limitations in their parametric memory. Retrieval-Augmented Generation (RAG)
systems address this issue by incorporating external knowledge with a retrieval
module. Despite their successes, however, current RAG systems face challenges
with retrieval failures and the limited ability of LLMs to filter out
irrelevant information. Therefore, in this work, we propose DSLR (Document
Refinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised
framework that decomposes retrieved documents into sentences, filters out
irrelevant sentences, and reconstructs them again into coherent passages. We
experimentally validate DSLR on multiple open-domain QA datasets and the
results demonstrate that DSLR significantly enhances the RAG performance over
conventional fixed-size passage. Furthermore, our DSLR enhances performance in
specific, yet realistic scenarios without the need for additional training,
providing an effective and efficient solution for refining retrieved documents
in RAG systems.","[{'name': 'Taeho Hwang'}, {'name': 'Soyeong Jeong'}, {'name': 'Sukmin Cho'}, {'name': 'SeungYoon Han'}, {'name': 'Jong C. Park'}]",2024-07-04T04:30:04Z
http://arxiv.org/abs/2407.03624v1,http://arxiv.org/abs/2407.03624v1,Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks,"Although LLMs have the potential to transform many fields, they still
underperform humans in reasoning tasks. Existing methods induce the model to
produce step-by-step calculations, but this research explores the question:
Does making the LLM analyze the question improve its performance? We propose a
novel prompting strategy called Question Analysis Prompting (QAP), in which the
model is prompted to explain the question in $n$ words before solving. The
value of $n$ influences the length of response generated by the model. QAP is
evaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic datasets GSM8K, AQuA,
and SAT and commonsense dataset StrategyQA. QAP is compared with other
state-of-the-art prompts including Chain-of-Thought (CoT), Plan and Solve
Prompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all
state-of-the-art prompts on AQuA and SAT datasets on both GPT3.5 and GPT4. QAP
consistently ranks among the top-2 prompts on 75\% of the tests. A key factor
of QAP performance can be attributed to response length, where detailed
responses are beneficial when answering harder questions, but can negatively
affect easy questions.","[{'name': 'Dharunish Yugeswardeenoo'}, {'name': 'Kevin Zhu'}, {'name': ""Sean O'Brien""}]",2024-07-04T04:19:50Z
http://arxiv.org/abs/2407.03615v1,http://arxiv.org/abs/2407.03615v1,"Visualizing Dialogues: Enhancing Image Selection through Dialogue
  Understanding with Large Language Models","Recent advancements in dialogue systems have highlighted the significance of
integrating multimodal responses, which enable conveying ideas through diverse
modalities rather than solely relying on text-based interactions. This
enrichment not only improves overall communicative efficacy but also enhances
the quality of conversational experiences. However, existing methods for
dialogue-to-image retrieval face limitations due to the constraints of
pre-trained vision language models (VLMs) in comprehending complex dialogues
accurately. To address this, we present a novel approach leveraging the robust
reasoning capabilities of large language models (LLMs) to generate precise
dialogue-associated visual descriptors, facilitating seamless connection with
images. Extensive experiments conducted on benchmark data validate the
effectiveness of our proposed approach in deriving concise and accurate visual
descriptors, leading to significant enhancements in dialogue-to-image retrieval
performance. Furthermore, our findings demonstrate the method's
generalizability across diverse visual cues, various LLMs, and different
datasets, underscoring its practicality and potential impact in real-world
applications.","[{'name': 'Chang-Sheng Kao'}, {'name': 'Yun-Nung Chen'}]",2024-07-04T03:50:30Z
http://arxiv.org/abs/2407.03604v1,http://arxiv.org/abs/2407.03604v1,"Lateralization LoRA: Interleaved Instruction Tuning with
  Modality-Specialized Adaptations","Recent advancements in Vision-Language Models (VLMs) have led to the
development of Vision-Language Generalists (VLGs) capable of understanding and
generating interleaved images and text. Despite these advances, VLGs still
struggle to follow user instructions for interleaved text and image generation.
To address this issue, we introduce LeafInstruct, the first open-sourced
interleaved instruction tuning data with over 30,000 high-quality instances
across more than 10 domains. Due to the extensive size of existing VLGs, we opt
for parameter-efficient tuning. However, we observe that VLGs tuned with a
standard LoRA typically exhibit inferior performance in interleaved text-image
generation. We attribute this problem to modality interference and the lack of
modality-specialized adaptation design. Hence, we propose Lateralization LoRA,
a novel modality-specialized adaptation method inspired by the concept of brain
lateralization. Lateralization LoRA employs a hybrid approach, combining the
traditional linear LoRA and a Convolutional LoRA for generating text and
images, enabling the generation of high-quality text and images by leveraging
modality-specific structures and parameter sets. We perform instruction tuning
of the VLG (i.e., EMU2) using Lateralization LoRA on the LeafInstruct dataset.
Extensive experiments demonstrate that EMU2 tuned with Lateralization LoRA
achieve state-of-the-art performance, significantly surpassing baseline models
in complex interleaved tasks.","[{'name': 'Zhiyang Xu'}, {'name': 'Minqian Liu'}, {'name': 'Ying Shen'}, {'name': 'Joy Rimchala'}, {'name': 'Jiaxin Zhang'}, {'name': 'Qifan Wang'}, {'name': 'Yu Cheng'}, {'name': 'Lifu Huang'}]",2024-07-04T03:28:22Z
http://arxiv.org/abs/2407.03572v1,http://arxiv.org/abs/2407.03572v1,"Core: Robust Factual Precision Scoring with Informative Sub-Claim
  Identification","Hallucinations -- the generation of untrue claims -- pose a challenge to the
application of large language models (LLMs) [1] thereby motivating the
development of metrics to evaluate factual precision. We observe that popular
metrics using the Decompose-Then-Verify framework, such as FActScore [2], can
be manipulated by adding obvious or repetitive claims to artificially inflate
scores. We expand the FActScore dataset to design and analyze factual precision
metrics, demonstrating that models can be trained to achieve high scores under
existing metrics through exploiting the issues we identify. This motivates our
new customizable plug-and-play subclaim selection component called Core, which
filters down individual subclaims according to their uniqueness and
informativeness. Metrics augmented by Core are substantially more robust as
shown in head-to-head comparisons. We release an evaluation framework
supporting the modular use of Core (https://github.com/zipJiang/Core) and
various decomposition strategies, and we suggest its adoption by the LLM
community.
  [1] Hong et al., ""The Hallucinations Leaderboard -- An Open Effort to Measure
Hallucinations in Large Language Models"", arXiv:2404.05904v2 [cs.CL].
  [2] Min et al., ""FActScore: Fine-grained Atomic Evaluation of Factual
Precision in Long Form Text Generation"", arXiv:2305.14251v2 [cs.CL].","[{'name': 'Zhengping Jiang'}, {'name': 'Jingyu Zhang'}, {'name': 'Nathaniel Weir'}, {'name': 'Seth Ebner'}, {'name': 'Miriam Wanner'}, {'name': 'Kate Sanders'}, {'name': 'Daniel Khashabi'}, {'name': 'Anqi Liu'}, {'name': 'Benjamin Van Durme'}]",2024-07-04T01:51:38Z
http://arxiv.org/abs/2407.03563v1,http://arxiv.org/abs/2407.03563v1,"Learning Video Temporal Dynamics with Cross-Modal Attention for Robust
  Audio-Visual Speech Recognition","Audio-visual speech recognition (AVSR) aims to transcribe human speech using
both audio and video modalities. In practical environments with noise-corrupted
audio, the role of video information becomes crucial. However, prior works have
primarily focused on enhancing audio features in AVSR, overlooking the
importance of video features. In this study, we strengthen the video features
by learning three temporal dynamics in video data: context order, playback
direction, and the speed of video frames. Cross-modal attention modules are
introduced to enrich video features with audio information so that speech
variability can be taken into account when training on the video temporal
dynamics. Based on our approach, we achieve the state-of-the-art performance on
the LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach
excels in scenarios especially for babble and speech noise, indicating the
ability to distinguish the speech signal that should be recognized from lip
movements in the video modality. We support the validity of our methodology by
offering the ablation experiments for the temporal dynamics losses and the
cross-modal attention architecture design.","[{'name': 'Sungnyun Kim'}, {'name': 'Kangwook Jang'}, {'name': 'Sangmin Bae'}, {'name': 'Hoirin Kim'}, {'name': 'Se-Young Yun'}]",2024-07-04T01:25:20Z
http://arxiv.org/abs/2407.03551v1,http://arxiv.org/abs/2407.03551v1,"Feelings about Bodies: Emotions on Diet and Fitness Forums Reveal
  Gendered Stereotypes and Body Image Concerns","The gendered expectations about ideal body types can lead to body image
concerns, dissatisfaction, and in extreme cases, disordered eating and other
psychopathologies across the gender spectrum. While research has focused on
pro-anorexia online communities that glorify the 'thin ideal', less attention
has been given to the broader spectrum of body image concerns or how emerging
disorders like muscle dysmorphia ('bigorexia') present in online discussions.
To address these gaps, we analyze 46 Reddit discussion forums related to diet,
fitness, and associated mental health challenges. Using membership structure
analysis and transformer-based language models, we project these communities
along gender and body ideal axes, revealing complex interactions between
gender, body ideals, and emotional expression. Our findings show that
feminine-oriented communities generally express more negative emotions,
particularly in thinness-promoting forums. Conversely, communities focused on
the muscular ideal exhibit less negativity, regardless of gender orientation.
We also uncover a gendered pattern in emotional indicators of mental health
challenges, with communities discussing serious issues aligning more closely
with thinness-oriented, predominantly feminine-leaning communities. By
revealing the gendered emotional dynamics of online communities, our findings
can inform the development of more effective content moderation approaches that
facilitate supportive interactions, while minimizing exposure to potentially
harmful content.","[{'name': 'Cinthia Sánchez'}, {'name': 'Minh Duc Chu'}, {'name': 'Zihao He'}, {'name': 'Rebecca Dorn'}, {'name': 'Stuart Murray'}, {'name': 'Kristina Lerman'}]",2024-07-04T00:11:27Z
http://arxiv.org/abs/2407.03545v1,http://arxiv.org/abs/2407.03545v1,On Evaluating Explanation Utility for Human-AI Decision Making in NLP,"Is explainability a false promise? This debate has emerged from the
insufficient evidence that explanations aid people in situations they are
introduced for. More human-centered, application-grounded evaluations of
explanations are needed to settle this. Yet, with no established guidelines for
such studies in NLP, researchers accustomed to standardized proxy evaluations
must discover appropriate measurements, tasks, datasets, and sensible models
for human-AI teams in their studies.
  To help with this, we first review fitting existing metrics. We then
establish requirements for datasets to be suitable for application-grounded
evaluations. Among over 50 datasets available for explainability research in
NLP, we find that 4 meet our criteria. By finetuning Flan-T5-3B, we demonstrate
the importance of reassessing the state of the art to form and study human-AI
teams. Finally, we present the exemplar studies of human-AI decision-making for
one of the identified suitable tasks -- verifying the correctness of a legal
claim given a contract.","[{'name': 'Fateme Hashemi Chaleshtori'}, {'name': 'Atreya Ghosal'}, {'name': 'Alexander Gill'}, {'name': 'Purbid Bambroo'}, {'name': 'Ana Marasović'}]",2024-07-03T23:53:27Z
http://arxiv.org/abs/2407.03502v1,http://arxiv.org/abs/2407.03502v1,AgentInstruct: Toward Generative Teaching with Agentic Flows,"Synthetic data is becoming increasingly important for accelerating the
development of language models, both large and small. Despite several
successful use cases, researchers also raised concerns around model collapse
and drawbacks of imitating other models. This discrepancy can be attributed to
the fact that synthetic data varies in quality and diversity. Effective use of
synthetic data usually requires significant human effort in curating the data.
We focus on using synthetic data for post-training, specifically creating data
by powerful models to teach a new skill or behavior to another model, we refer
to this setting as Generative Teaching. We introduce AgentInstruct, an
extensible agentic framework for automatically creating large amounts of
diverse and high-quality synthetic data. AgentInstruct can create both the
prompts and responses, using only raw data sources like text documents and code
files as seeds. We demonstrate the utility of AgentInstruct by creating a post
training dataset of 25M pairs to teach language models different skills, such
as text editing, creative writing, tool usage, coding, reading comprehension,
etc. The dataset can be used for instruction tuning of any base model. We
post-train Mistral-7b with the data. When comparing the resulting model Orca-3
to Mistral-7b-Instruct (which uses the same base model), we observe significant
improvements across many benchmarks. For example, 40% improvement on AGIEval,
19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and
45% improvement on AlpacaEval. Additionally, it consistently outperforms other
models such as LLAMA-8B-instruct and GPT-3.5-turbo.","[{'name': 'Arindam Mitra'}, {'name': 'Luciano Del Corro'}, {'name': 'Guoqing Zheng'}, {'name': 'Shweti Mahajan'}, {'name': 'Dany Rouhana'}, {'name': 'Andres Codas'}, {'name': 'Yadong Lu'}, {'name': 'Wei-ge Chen'}, {'name': 'Olga Vrousgos'}, {'name': 'Corby Rosset'}, {'name': 'Fillipe Silva'}, {'name': 'Hamed Khanpour'}, {'name': 'Yash Lara'}, {'name': 'Ahmed Awadallah'}]",2024-07-03T21:01:12Z
http://arxiv.org/abs/2407.03495v1,http://arxiv.org/abs/2407.03495v1,"Codec-ASR: Training Performant Automatic Speech Recognition Systems with
  Discrete Speech Representations","Discrete speech representations have garnered recent attention for their
efficacy in training transformer-based models for various speech-related tasks
such as automatic speech recognition (ASR), translation, speaker verification,
and joint speech-text foundational models. In this work, we present a
comprehensive analysis on building ASR systems with discrete codes. We
investigate different methods for codec training such as quantization schemes
and time-domain vs spectral feature encodings. We further explore ASR training
techniques aimed at enhancing performance, training efficiency, and noise
robustness. Drawing upon our findings, we introduce a codec ASR pipeline that
outperforms Encodec at similar bit-rate. Remarkably, it also surpasses the
state-of-the-art results achieved by strong self-supervised models on the 143
languages ML-SUPERB benchmark despite being smaller in size and pretrained on
significantly less data.","[{'name': 'Kunal Dhawan'}, {'name': 'Nithin Rao Koluguri'}, {'name': 'Ante Jukić'}, {'name': 'Ryan Langman'}, {'name': 'Jagadeesh Balam'}, {'name': 'Boris Ginsburg'}]",2024-07-03T20:51:41Z
http://arxiv.org/abs/2407.03473v1,http://arxiv.org/abs/2407.03473v1,"Exploring LGBTQ+ Bias in Generative AI Answers across Different Country
  and Religious Contexts","Previous discussions have highlighted the need for generative AI tools to
become more culturally sensitive, yet often neglect the complexities of
handling content about minorities, who are perceived differently across
cultures and religions. Our study examined how two generative AI systems
respond to homophobic statements with varying cultural and religious context
information. Findings showed ChatGPT 3.5's replies exhibited cultural
relativism, in contrast to Bard's, which stressed human rights and provided
more support for LGBTQ+ issues. Both demonstrated significant change in
responses based on contextual information provided in the prompts, suggesting
that AI systems may adjust in their responses the degree and forms of support
for LGBTQ+ people according to information they receive about the user's
background. The study contributes to understanding the social and ethical
implications of AI responses and argues that any work to make generative AI
outputs more culturally diverse requires a grounding in fundamental human
rights.","[{'name': 'Lilla Vicsek'}, {'name': 'Anna Vancsó'}, {'name': 'Mike Zajko'}, {'name': 'Judit Takacs'}]",2024-07-03T19:38:19Z
http://arxiv.org/abs/2407.03470v1,http://arxiv.org/abs/2407.03470v1,Prosody-Driven Privacy-Preserving Dementia Detection,"Speaker embeddings extracted from voice recordings have been proven valuable
for dementia detection. However, by their nature, these embeddings contain
identifiable information which raises privacy concerns. In this work, we aim to
anonymize embeddings while preserving the diagnostic utility for dementia
detection. Previous studies rely on adversarial learning and models trained on
the target attribute and struggle in limited-resource settings. We propose a
novel approach that leverages domain knowledge to disentangle prosody features
relevant to dementia from speaker embeddings without relying on a dementia
classifier. Our experiments show the effectiveness of our approach in
preserving speaker privacy (speaker recognition F1-score .01%) while
maintaining high dementia detection score F1-score of 74% on the ADReSS
dataset. Our results are also on par with a more constrained
classifier-dependent system on ADReSSo (.01% and .66%), and have no impact on
synthesized speech naturalness.","[{'name': 'Dominika Woszczyk'}, {'name': 'Ranya Aloufi'}, {'name': 'Soteris Demetriou'}]",2024-07-03T19:34:47Z
http://arxiv.org/abs/2407.11030v1,http://arxiv.org/abs/2407.11030v1,DLO: Dynamic Layer Operation for Efficient Vertical Scaling of LLMs,"In this paper, we introduce Dynamic Layer Operations (DLO), a novel approach
for vertically scaling transformer-based Large Language Models (LLMs) by
dynamically expanding, activating, or skipping layers using a sophisticated
routing policy based on layerwise feature similarity. Unlike traditional
Mixture-of-Experts (MoE) methods that focus on extending the model width, our
approach targets model depth, addressing the redundancy observed across layer
representations for various input samples. Our framework is integrated with the
Supervised Fine-Tuning (SFT) stage, eliminating the need for resource-intensive
Continual Pre-Training (CPT). Experimental results demonstrate that DLO not
only outperforms the original unscaled models but also achieves comparable
results to densely expanded models with significantly improved efficiency. Our
work offers a promising direction for building efficient yet powerful LLMs. We
will release our implementation and model weights upon acceptance.","[{'name': 'Zhen Tan'}, {'name': 'Daize Dong'}, {'name': 'Xinyu Zhao'}, {'name': 'Jie Peng'}, {'name': 'Yu Cheng'}, {'name': 'Tianlong Chen'}]",2024-07-03T18:34:08Z
http://arxiv.org/abs/2407.03418v1,http://arxiv.org/abs/2407.03418v1,HEMM: Holistic Evaluation of Multimodal Foundation Models,"Multimodal foundation models that can holistically process text alongside
images, video, audio, and other sensory modalities are increasingly used in a
variety of real-world applications. However, it is challenging to characterize
and study progress in multimodal foundation models, given the range of possible
modeling decisions, tasks, and domains. In this paper, we introduce Holistic
Evaluation of Multimodal Models (HEMM) to systematically evaluate the
capabilities of multimodal foundation models across a set of 3 dimensions:
basic skills, information flow, and real-world use cases. Basic multimodal
skills are internal abilities required to solve problems, such as learning
interactions across modalities, fine-grained alignment, multi-step reasoning,
and the ability to handle external knowledge. Information flow studies how
multimodal content changes during a task through querying, translation,
editing, and fusion. Use cases span domain-specific challenges introduced in
real-world multimedia, affective computing, natural sciences, healthcare, and
human-computer interaction applications. Through comprehensive experiments
across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,
basic skills, information flows, and use cases) that pose challenges to today's
models, and (2) distill performance trends regarding how different modeling
dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,
and instruction tuning objectives) influence performance. Our conclusions
regarding challenging multimodal interactions, use cases, and tasks requiring
reasoning and external knowledge, the benefits of data and model scale, and the
impacts of instruction tuning yield actionable insights for future work in
multimodal foundation models.","[{'name': 'Paul Pu Liang'}, {'name': 'Akshay Goindani'}, {'name': 'Talha Chafekar'}, {'name': 'Leena Mathur'}, {'name': 'Haofei Yu'}, {'name': 'Ruslan Salakhutdinov'}, {'name': 'Louis-Philippe Morency'}]",2024-07-03T18:00:48Z
http://arxiv.org/abs/2407.03321v1,http://arxiv.org/abs/2407.03321v1,"Planetarium: A Rigorous Benchmark for Translating Text to Structured
  Planning Languages","Many recent works have explored using language models for planning problems.
One line of research focuses on translating natural language descriptions of
planning tasks into structured planning languages, such as the planning domain
definition language (PDDL). While this approach is promising, accurately
measuring the quality of generated PDDL code continues to pose significant
challenges. First, generated PDDL code is typically evaluated using planning
validators that check whether the problem can be solved with a planner. This
method is insufficient because a language model might generate valid PDDL code
that does not align with the natural language description of the task. Second,
existing evaluation sets often have natural language descriptions of the
planning task that closely resemble the ground truth PDDL, reducing the
challenge of the task. To bridge this gap, we introduce \benchmarkName, a
benchmark designed to evaluate language models' ability to generate PDDL code
from natural language descriptions of planning tasks. We begin by creating a
PDDL equivalence algorithm that rigorously evaluates the correctness of PDDL
code generated by language models by flexibly comparing it against a ground
truth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across
13 different tasks, with varying levels of difficulty. Finally, we evaluate
several API-access and open-weight language models that reveal this task's
complexity. For example, $87.6\%$ of the PDDL problem descriptions generated by
GPT-4o are syntactically parseable, $82.2\%$ are valid, solve-able problems,
but only $35.1\%$ are semantically correct, highlighting the need for a more
rigorous benchmark for this problem.","[{'name': 'Max Zuo'}, {'name': 'Francisco Piedrahita Velez'}, {'name': 'Xiaochen Li'}, {'name': 'Michael L. Littman'}, {'name': 'Stephen H. Bach'}]",2024-07-03T17:59:53Z
http://arxiv.org/abs/2407.03320v1,http://arxiv.org/abs/2407.03320v1,"InternLM-XComposer-2.5: A Versatile Large Vision Language Model
  Supporting Long-Contextual Input and Output","We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision
language model that supports long-contextual input and output. IXC-2.5 excels
in various text-image comprehension and composition applications, achieving
GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K
interleaved image-text contexts, it can seamlessly extend to 96K long contexts
via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in
tasks requiring extensive input and output contexts. Compared to its previous
2.0 version, InternLM-XComposer-2.5 features three major upgrades in
vision-language comprehension: (1) Ultra-High Resolution Understanding, (2)
Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In
addition to comprehension, IXC-2.5 extends to two compelling applications using
extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)
Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28
benchmarks, outperforming existing open-source state-of-the-art models on 16
benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on
16 key tasks. The InternLM-XComposer-2.5 is publicly available at
https://github.com/InternLM/InternLM-XComposer.","[{'name': 'Pan Zhang'}, {'name': 'Xiaoyi Dong'}, {'name': 'Yuhang Zang'}, {'name': 'Yuhang Cao'}, {'name': 'Rui Qian'}, {'name': 'Lin Chen'}, {'name': 'Qipeng Guo'}, {'name': 'Haodong Duan'}, {'name': 'Bin Wang'}, {'name': 'Linke Ouyang'}, {'name': 'Songyang Zhang'}, {'name': 'Wenwei Zhang'}, {'name': 'Yining Li'}, {'name': 'Yang Gao'}, {'name': 'Peng Sun'}, {'name': 'Xinyue Zhang'}, {'name': 'Wei Li'}, {'name': 'Jingwen Li'}, {'name': 'Wenhai Wang'}, {'name': 'Hang Yan'}, {'name': 'Conghui He'}, {'name': 'Xingcheng Zhang'}, {'name': 'Kai Chen'}, {'name': 'Jifeng Dai'}, {'name': 'Yu Qiao'}, {'name': 'Dahua Lin'}, {'name': 'Jiaqi Wang'}]",2024-07-03T17:59:21Z
http://arxiv.org/abs/2407.03282v1,http://arxiv.org/abs/2407.03282v1,LLM Internal States Reveal Hallucination Risk Faced With a Query,"The hallucination problem of Large Language Models (LLMs) significantly
limits their reliability and trustworthiness. Humans have a self-awareness
process that allows us to recognize what we don't know when faced with queries.
Inspired by this, our paper investigates whether LLMs can estimate their own
hallucination risk before response generation. We analyze the internal
mechanisms of LLMs broadly both in terms of training data sources and across 15
diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.
Our empirical analysis reveals two key insights: (1) LLM internal states
indicate whether they have seen the query in training data or not; and (2) LLM
internal states show they are likely to hallucinate or not regarding the query.
Our study explores particular neurons, activation layers, and tokens that play
a crucial role in the LLM perception of uncertainty and hallucination risk. By
a probing estimator, we leverage LLM self-assessment, achieving an average
hallucination estimation accuracy of 84.32\% at run time.","[{'name': 'Ziwei Ji'}, {'name': 'Delong Chen'}, {'name': 'Etsuko Ishii'}, {'name': 'Samuel Cahyawijaya'}, {'name': 'Yejin Bang'}, {'name': 'Bryan Wilie'}, {'name': 'Pascale Fung'}]",2024-07-03T17:08:52Z
http://arxiv.org/abs/2407.03277v1,http://arxiv.org/abs/2407.03277v1,"Evaluating Automatic Metrics with Incremental Machine Translation
  Systems","We introduce a dataset comprising commercial machine translations, gathered
weekly over six years across 12 translation directions. Since human A/B testing
is commonly used, we assume commercial systems improve over time, which enables
us to evaluate machine translation (MT) metrics based on their preference for
more recent translations. Our study confirms several previous findings in MT
metrics research and demonstrates the dataset's value as a testbed for metric
evaluation. We release our code at https://github.com/gjwubyron/Evo","[{'name': 'Guojun Wu'}, {'name': 'Shay B. Cohen'}, {'name': 'Rico Sennrich'}]",2024-07-03T17:04:17Z
http://arxiv.org/abs/2407.03255v2,http://arxiv.org/abs/2407.03255v2,"How Similar Are Elected Politicians and Their Constituents? Quantitative
  Evidence From Online Social Networks","How similar are politicians to those who vote for them? This is a critical
question at the heart of democratic representation and particularly relevant at
times when political dissatisfaction and populism are on the rise. To answer
this question we compare the online discourse of elected politicians and their
constituents. We collect a two and a half years (September 2020 - February
2023) constituency-level dataset for USA and UK that includes: (i) the Twitter
timelines (5.6 Million tweets) of elected political representatives (595 UK
Members of Parliament and 433 USA Representatives), (ii) the Nextdoor posts
(21.8 Million posts) of the constituency (98.4% USA and 91.5% UK
constituencies). We find that elected politicians tend to be equally similar to
their constituents in terms of content and style regardless of whether a
constituency elects a right or left-wing politician. The size of the electoral
victory and the level of income of a constituency shows a nuanced picture. The
narrower the electoral victory, the more similar the style and the more
dissimilar the content is. The lower the income of a constituency, the more
similar the content is. In terms of style, poorer constituencies tend to have a
more similar sentiment and more dissimilar psychological text traits (i.e.
measured with LIWC categories).","[{'name': 'Waleed Iqbal'}, {'name': 'Gareth Tyson'}, {'name': 'Ignacio Castro'}]",2024-07-03T16:36:26Z
http://arxiv.org/abs/2407.03253v1,http://arxiv.org/abs/2407.03253v1,"STF: Sentence Transformer Fine-Tuning For Topic Categorization With
  Limited Data","Nowadays, topic classification from tweets attracts considerable research
attention. Different classification systems have been suggested thanks to these
research efforts. Nevertheless, they face major challenges owing to low
performance metrics due to the limited amount of labeled data. We propose
Sentence Transformers Fine-tuning (STF), a topic detection system that
leverages pretrained Sentence Transformers models and fine-tuning to classify
topics from tweets accurately. Moreover, extensive parameter sensitivity
analyses were conducted to finetune STF parameters for our topic classification
task to achieve the best performance results. Experiments on two benchmark
datasets demonstrated that (1) the proposed STF can be effectively used for
classifying tweet topics and outperforms the latest state-of-the-art
approaches, and (2) the proposed STF does not require a huge amount of labeled
tweets to achieve good accuracy, which is a limitation of many state-of-the-art
approaches. Our main contribution is the achievement of promising results in
tweet topic classification by applying pretrained sentence transformers
language models.","[{'name': 'Kheir Eddine Daouadi'}, {'name': 'Yaakoub Boualleg'}, {'name': 'Oussama Guehairia'}]",2024-07-03T16:34:56Z
http://arxiv.org/abs/2407.03232v1,http://arxiv.org/abs/2407.03232v1,Single Character Perturbations Break LLM Alignment,"When LLMs are deployed in sensitive, human-facing settings, it is crucial
that they do not output unsafe, biased, or privacy-violating outputs. For this
reason, models are both trained and instructed to refuse to answer unsafe
prompts such as ""Tell me how to build a bomb."" We find that, despite these
safeguards, it is possible to break model defenses simply by appending a space
to the end of a model's input. In a study of eight open-source models, we
demonstrate that this acts as a strong enough attack to cause the majority of
models to generate harmful outputs with very high success rates. We examine the
causes of this behavior, finding that the contexts in which single spaces occur
in tokenized training data encourage models to generate lists when prompted,
overriding training signals to refuse to answer unsafe requests. Our findings
underscore the fragile state of current model alignment and promote the
importance of developing more robust alignment methods. Code and data will be
available at https://github.com/hannah-aught/space_attack.","[{'name': 'Leon Lin'}, {'name': 'Hannah Brown'}, {'name': 'Kenji Kawaguchi'}, {'name': 'Michael Shieh'}]",2024-07-03T16:03:10Z
http://arxiv.org/abs/2407.03227v1,http://arxiv.org/abs/2407.03227v1,"Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and
  Schema Pruning","We focus on Text-to-SQL semantic parsing from the perspective of Large
Language Models. Motivated by challenges related to the size of commercial
database schemata and the deployability of business intelligence solutions, we
propose an approach that dynamically retrieves input database information and
uses abstract syntax trees to select few-shot examples for in-context learning.
  Furthermore, we investigate the extent to which an in-parallel semantic
parser can be leveraged for generating $\textit{approximated}$ versions of the
expected SQL queries, to support our retrieval. We take this approach to the
extreme--we adapt a model consisting of less than $500$M parameters, to act as
an extremely efficient approximator, enhancing it with the ability to process
schemata in a parallelised manner. We apply our approach to monolingual and
cross-lingual benchmarks for semantic parsing, showing improvements over
state-of-the-art baselines. Comprehensive experiments highlight the
contribution of modules involved in this retrieval-augmented generation
setting, revealing interesting directions for future work.","[{'name': 'Zhili Shen'}, {'name': 'Pavlos Vougiouklis'}, {'name': 'Chenxin Diao'}, {'name': 'Kaustubh Vyas'}, {'name': 'Yuanyi Ji'}, {'name': 'Jeff Z. Pan'}]",2024-07-03T15:55:14Z
http://arxiv.org/abs/2407.03391v1,http://arxiv.org/abs/2407.03391v1,"Soft Begging: Modular and Efficient Shielding of LLMs against Prompt
  Injection and Jailbreaking based on Prompt Tuning","Prompt injection (both direct and indirect) and jailbreaking are now
recognized as significant issues for large language models (LLMs), particularly
due to their potential for harm in application-integrated contexts. This
extended abstract explores a novel approach to protecting LLMs from such
attacks, termed ""soft begging."" This method involves training soft prompts to
counteract the effects of corrupted prompts on the LLM's output. We provide an
overview of prompt injections and jailbreaking, introduce the theoretical basis
of the ""soft begging"" technique, and discuss an evaluation of its
effectiveness.","[{'name': 'Simon Ostermann'}, {'name': 'Kevin Baum'}, {'name': 'Christoph Endres'}, {'name': 'Julia Masloh'}, {'name': 'Patrick Schramowski'}]",2024-07-03T14:52:09Z
http://arxiv.org/abs/2407.03169v1,http://arxiv.org/abs/2407.03169v1,"Investigating Decoder-only Large Language Models for Speech-to-text
  Translation","Large language models (LLMs), known for their exceptional reasoning
capabilities, generalizability, and fluency across diverse domains, present a
promising avenue for enhancing speech-related tasks. In this paper, we focus on
integrating decoder-only LLMs to the task of speech-to-text translation (S2TT).
We propose a decoder-only architecture that enables the LLM to directly consume
the encoded speech representation and generate the text translation.
Additionally, we investigate the effects of different parameter-efficient
fine-tuning techniques and task formulation. Our model achieves
state-of-the-art performance on CoVoST 2 and FLEURS among models trained
without proprietary data. We also conduct analyses to validate the design
choices of our proposed model and bring insights to the integration of LLMs to
S2TT.","[{'name': 'Chao-Wei Huang'}, {'name': 'Hui Lu'}, {'name': 'Hongyu Gong'}, {'name': 'Hirofumi Inaguma'}, {'name': 'Ilia Kulikov'}, {'name': 'Ruslan Mavlyutov'}, {'name': 'Sravya Popuri'}]",2024-07-03T14:42:49Z
http://arxiv.org/abs/2407.03160v1,http://arxiv.org/abs/2407.03160v1,SOS! Soft Prompt Attack Against Open-Source Large Language Models,"Open-source large language models (LLMs) have become increasingly popular
among both the general public and industry, as they can be customized,
fine-tuned, and freely used. However, some open-source LLMs require approval
before usage, which has led to third parties publishing their own easily
accessible versions. Similarly, third parties have been publishing fine-tuned
or quantized variants of these LLMs. These versions are particularly appealing
to users because of their ease of access and reduced computational resource
demands. This trend has increased the risk of training time attacks,
compromising the integrity and security of LLMs. In this work, we present a new
training time attack, SOS, which is designed to be low in computational demand
and does not require clean data or modification of the model weights, thereby
maintaining the model's utility intact. The attack addresses security issues in
various scenarios, including the backdoor attack, jailbreak attack, and prompt
stealing attack. Our experimental findings demonstrate that the proposed attack
is effective across all evaluated targets. Furthermore, we present the other
side of our SOS technique, namely the copyright token -- a novel technique that
enables users to mark their copyrighted content and prevent models from using
it.","[{'name': 'Ziqing Yang'}, {'name': 'Michael Backes'}, {'name': 'Yang Zhang'}, {'name': 'Ahmed Salem'}]",2024-07-03T14:35:16Z
http://arxiv.org/abs/2407.03157v1,http://arxiv.org/abs/2407.03157v1,Let the Code LLM Edit Itself When You Edit the Code,"In this work, we investigate a typical scenario in code generation where a
developer edits existing code in real time and requests a code assistant, e.g.,
a large language model, to re-predict the next token or next line on the fly.
Naively, the LLM needs to re-encode the entire KV cache to provide an accurate
prediction. However, this process is computationally expensive, especially when
the sequence length is long. Simply encoding the edited subsequence and
integrating it to the original KV cache meets the temporal confusion problem,
leading to significantly worse performance. We address this efficiency and
accuracy trade-off by introducing \underline{\textbf{Positional
\textbf{I}ntegrity \textbf{E}ncoding} (PIE). Building upon the rotary
positional encoding, PIE first removes the rotary matrices in the Key cache
that introduce temporal confusion and then reapplies the correct rotary
matrices. This process ensures that positional relationships between tokens are
correct and requires only a single round of matrix multiplication. We validate
the effectiveness of PIE through extensive experiments on the RepoBench-C-8k
dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.
Our evaluation includes three real-world coding tasks: code insertion, code
deletion, and multi-place code editing. Results demonstrate that PIE reduces
computational overhead by over 85% compared to the standard full recomputation
approach across all model sizes and tasks while well approximating the model
performance.","[{'name': 'Zhenyu He'}, {'name': 'Jun Zhang'}, {'name': 'Shengjie Luo'}, {'name': 'Jingjing Xu'}, {'name': 'Zhi Zhang'}, {'name': 'Di He'}]",2024-07-03T14:34:03Z
http://arxiv.org/abs/2407.03104v3,http://arxiv.org/abs/2407.03104v3,KeyVideoLLM: Towards Large-scale Video Keyframe Selection,"Recently, with the rise of web videos, managing and understanding large-scale
video datasets has become increasingly important. Video Large Language Models
(VideoLLMs) have emerged in recent years due to their strong video
understanding capabilities. However, training and inference processes for
VideoLLMs demand vast amounts of data, presenting significant challenges to
data management, particularly regarding efficiency, robustness, and
effectiveness. In this work, we present KeyVideoLLM, a text-video frame
similarity-based keyframe selection method designed to manage VideoLLM data
efficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a
remarkable data compression rate of up to 60.9 times, substantially lowering
disk space requirements, which proves its high efficiency. Additionally, it
maintains a 100% selection success rate across all video formats and scales,
enhances processing speed by up to 200 times compared to existing keyframe
selection methods, and does not require hyperparameter tuning. Beyond its
outstanding efficiency and robustness, KeyVideoLLM further improves model
performance in video question-answering tasks during both training and
inference stages. Notably, it consistently achieved the state-of-the-art (SoTA)
experimental results on diverse datasets.","[{'name': 'Hao Liang'}, {'name': 'Jiapeng Li'}, {'name': 'Tianyi Bai'}, {'name': 'Xijie Huang'}, {'name': 'Linzhuang Sun'}, {'name': 'Zhengren Wang'}, {'name': 'Conghui He'}, {'name': 'Bin Cui'}, {'name': 'Chong Chen'}, {'name': 'Wentao Zhang'}]",2024-07-03T13:41:44Z
http://arxiv.org/abs/2407.03103v1,http://arxiv.org/abs/2407.03103v1,"Cactus: Towards Psychological Counseling Conversations using Cognitive
  Behavioral Theory","Recently, the demand for psychological counseling has significantly increased
as more individuals express concerns about their mental health. This surge has
accelerated efforts to improve the accessibility of counseling by using large
language models (LLMs) as counselors. To ensure client privacy, training
open-source LLMs faces a key challenge: the absence of realistic counseling
datasets. To address this, we introduce Cactus, a multi-turn dialogue dataset
that emulates real-life interactions using the goal-oriented and structured
approach of Cognitive Behavioral Therapy (CBT). We create a diverse and
realistic dataset by designing clients with varied, specific personas, and
having counselors systematically apply CBT techniques in their interactions. To
assess the quality of our data, we benchmark against established psychological
criteria used to evaluate real counseling sessions, ensuring alignment with
expert evaluations. Experimental results demonstrate that Camel, a model
trained with Cactus, outperforms other models in counseling skills,
highlighting its effectiveness and potential as a counseling agent. We make our
data, model, and code publicly available.","[{'name': 'Suyeon Lee'}, {'name': 'Sunghwan Kim'}, {'name': 'Minju Kim'}, {'name': 'Dongjin Kang'}, {'name': 'Dongil Yang'}, {'name': 'Harim Kim'}, {'name': 'Minseok Kang'}, {'name': 'Dayi Jung'}, {'name': 'Min Hee Kim'}, {'name': 'Seungbeen Lee'}, {'name': 'Kyoung-Mee Chung'}, {'name': 'Youngjae Yu'}, {'name': 'Dongha Lee'}, {'name': 'Jinyoung Yeo'}]",2024-07-03T13:41:31Z
http://arxiv.org/abs/2407.12831v1,http://arxiv.org/abs/2407.12831v1,Truth is Universal: Robust Detection of Lies in LLMs,"Large Language Models (LLMs) have revolutionised natural language processing,
exhibiting impressive human-like capabilities. In particular, LLMs are capable
of ""lying"", knowingly outputting false statements. Hence, it is of interest and
importance to develop methods to detect when LLMs lie. Indeed, several authors
trained classifiers to detect LLM lies based on their internal model
activations. However, other researchers showed that these classifiers may fail
to generalise, for example to negated statements. In this work, we aim to
develop a robust method to detect when an LLM is lying. To this end, we make
the following key contributions: (i) We demonstrate the existence of a
two-dimensional subspace, along which the activation vectors of true and false
statements can be separated. Notably, this finding is universal and holds for
various LLMs, including Gemma-7B, LLaMA2-13B and LLaMA3-8B. Our analysis
explains the generalisation failures observed in previous studies and sets the
stage for more robust lie detection; (ii) Building upon (i), we construct an
accurate LLM lie detector. Empirically, our proposed classifier achieves
state-of-the-art performance, distinguishing simple true and false statements
with 94% accuracy and detecting more complex real-world lies with 95% accuracy.","[{'name': 'Lennart Bürger'}, {'name': 'Fred A. Hamprecht'}, {'name': 'Boaz Nadler'}]",2024-07-03T13:01:54Z
http://arxiv.org/abs/2407.03076v1,http://arxiv.org/abs/2407.03076v1,"A Case Study on Context-Aware Neural Machine Translation with Multi-Task
  Learning","In document-level neural machine translation (DocNMT), multi-encoder
approaches are common in encoding context and source sentences. Recent studies
\cite{li-etal-2020-multi-encoder} have shown that the context encoder generates
noise and makes the model robust to the choice of context. This paper further
investigates this observation by explicitly modelling context encoding through
multi-task learning (MTL) to make the model sensitive to the choice of context.
We conduct experiments on cascade MTL architecture, which consists of one
encoder and two decoders. Generation of the source from the context is
considered an auxiliary task, and generation of the target from the source is
the main task. We experimented with German--English language pairs on News,
TED, and Europarl corpora. Evaluation results show that the proposed MTL
approach performs better than concatenation-based and multi-encoder DocNMT
models in low-resource settings and is sensitive to the choice of context.
However, we observe that the MTL models are failing to generate the source from
the context. These observations align with the previous studies, and this might
suggest that the available document-level parallel corpora are not
context-aware, and a robust sentence-level model can outperform the
context-aware models.","[{'name': 'Ramakrishna Appicharla'}, {'name': 'Baban Gain'}, {'name': 'Santanu Pal'}, {'name': 'Asif Ekbal'}, {'name': 'Pushpak Bhattacharyya'}]",2024-07-03T12:50:49Z
http://arxiv.org/abs/2407.03045v1,http://arxiv.org/abs/2407.03045v1,"JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts
  Discovery from Large-Scale Human-LLM Conversational Datasets","Large Language Models (LLMs) have gained significant attention but also
raised concerns due to the risk of misuse. Jailbreak prompts, a popular type of
adversarial attack towards LLMs, have appeared and constantly evolved to breach
the safety protocols of LLMs. To address this issue, LLMs are regularly updated
with safety patches based on reported jailbreak prompts. However, malicious
users often keep their successful jailbreak prompts private to exploit LLMs. To
uncover these private jailbreak prompts, extensive analysis of large-scale
conversational datasets is necessary to identify prompts that still manage to
bypass the system's defenses. This task is highly challenging due to the
immense volume of conversation data, diverse characteristics of jailbreak
prompts, and their presence in complex multi-turn conversations. To tackle
these challenges, we introduce JailbreakHunter, a visual analytics approach for
identifying jailbreak prompts in large-scale human-LLM conversational datasets.
We have designed a workflow with three analysis levels: group-level,
conversation-level, and turn-level. Group-level analysis enables users to grasp
the distribution of conversations and identify suspicious conversations using
multiple criteria, such as similarity with reported jailbreak prompts in
previous research and attack success rates. Conversation-level analysis
facilitates the understanding of the progress of conversations and helps
discover jailbreak prompts within their conversation contexts. Turn-level
analysis allows users to explore the semantic similarity and token overlap
between a singleturn prompt and the reported jailbreak prompts, aiding in the
identification of new jailbreak strategies. The effectiveness and usability of
the system were verified through multiple case studies and expert interviews.","[{'name': 'Zhihua Jin'}, {'name': 'Shiyi Liu'}, {'name': 'Haotian Li'}, {'name': 'Xun Zhao'}, {'name': 'Huamin Qu'}]",2024-07-03T12:10:41Z
http://arxiv.org/abs/2407.03040v1,http://arxiv.org/abs/2407.03040v1,"Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction
  Tuning for Large Language Model","Instruction tuning as an effective technique aligns the outputs of large
language models (LLMs) with human preference. But how to generate the seasonal
multi-turn dialogues from raw documents for instruction tuning still requires
further exploration. In this paper, we present a novel framework named R2S that
leverages the CoD-Chain of Dialogue logic to guide large language models (LLMs)
in generating knowledge-intensive multi-turn dialogues for instruction tuning.
By integrating raw documents from both open-source datasets and domain-specific
web-crawled documents into a benchmark K-BENCH, we cover diverse areas such as
Wikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach
first decides the logic flow of the current dialogue and then prompts LLMs to
produce key phrases for sourcing relevant response content. This methodology
enables the creation of the G I NSTRUCT instruction dataset, retaining raw
document knowledge within dialoguestyle interactions. Utilizing this dataset,
we fine-tune GLLM, a model designed to transform raw documents into structured
multi-turn dialogues, thereby injecting comprehensive domain knowledge into the
SFT model for enhanced instruction tuning. This work signifies a stride towards
refining the adaptability and effectiveness of LLMs in processing and
generating more accurate, contextually nuanced responses across various fields.","[{'name': 'Xia Hou'}, {'name': 'Qifeng Li'}, {'name': 'Jian Yang'}, {'name': 'Tongliang Li'}, {'name': 'Linzheng Chai'}, {'name': 'Xianjie Wu'}, {'name': 'Hangyuan Ji'}, {'name': 'Zhoujun Li'}, {'name': 'Jixuan Nie'}, {'name': 'Jingbo Dun'}, {'name': 'Wenfeng Song'}]",2024-07-03T12:04:10Z
http://arxiv.org/abs/2407.03020v1,http://arxiv.org/abs/2407.03020v1,"Exploiting Dialect Identification in Automatic Dialectal Text
  Normalization","Dialectal Arabic is the primary spoken language used by native Arabic
speakers in daily communication. The rise of social media platforms has notably
expanded its use as a written language. However, Arabic dialects do not have
standard orthographies. This, combined with the inherent noise in
user-generated content on social media, presents a major challenge to NLP
applications dealing with Dialectal Arabic. In this paper, we explore and
report on the task of CODAfication, which aims to normalize Dialectal Arabic
into the Conventional Orthography for Dialectal Arabic (CODA). We work with a
unique parallel corpus of multiple Arabic dialects focusing on five major city
dialects. We benchmark newly developed pretrained sequence-to-sequence models
on the task of CODAfication. We further show that using dialect identification
information improves the performance across all dialects. We make our code,
data, and pretrained models publicly available.","[{'name': 'Bashar Alhafni'}, {'name': 'Sarah Al-Towaity'}, {'name': 'Ziyad Fawzy'}, {'name': 'Fatema Nassar'}, {'name': 'Fadhl Eryani'}, {'name': 'Houda Bouamor'}, {'name': 'Nizar Habash'}]",2024-07-03T11:30:03Z
http://arxiv.org/abs/2407.12830v1,http://arxiv.org/abs/2407.12830v1,Knowledge-based Consistency Testing of Large Language Models,"In this work, we systematically expose and measure the inconsistency and
knowledge gaps of Large Language Models (LLMs). Specifically, we propose an
automated testing framework (called KONTEST) which leverages a knowledge graph
to construct test cases. KONTEST probes and measures the inconsistencies in the
LLM's knowledge of the world via a combination of semantically-equivalent
queries and test oracles (metamorphic or ontological oracle). KONTEST further
mitigates knowledge gaps via a weighted LLM model ensemble. Using four
state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that
KONTEST generates 19.2% error inducing inputs (1917 errors from 9983 test
inputs). It also reveals a 16.5% knowledge gap across all tested LLMs.
KONTEST's mitigation method reduces LLM knowledge gap by 32.48%. Our ablation
study further shows that GPT3.5 is not suitable for knowledge-based consistency
testing because it is only 60%-68% effective in knowledge construction.","[{'name': 'Sai Sathiesh Rajan'}, {'name': 'Ezekiel Soremekun'}, {'name': 'Sudipta Chattopadhyay'}]",2024-07-03T11:16:54Z
http://arxiv.org/abs/2407.02996v1,http://arxiv.org/abs/2407.02996v1,Are Large Language Models Consistent over Value-laden Questions?,"Large language models (LLMs) appear to bias their survey answers toward
certain values. Nonetheless, some argue that LLMs are too inconsistent to
simulate particular values. Are they? To answer, we first define value
consistency as the similarity of answers across (1) paraphrases of one
question, (2) related questions under one topic, (3) multiple-choice and
open-ended use-cases of one question, and (4) multilingual translations of a
question to English, Chinese, German, and Japanese. We apply these measures to
a few large ($>=34b$), open LLMs including llama-3, as well as gpt-4o, using
eight thousand questions spanning more than 300 topics. Unlike prior work, we
find that models are relatively consistent across paraphrases, use-cases,
translations, and within a topic. Still, some inconsistencies remain. Models
are more consistent on uncontroversial topics (e.g., in the U.S.,
""Thanksgiving"") than on controversial ones (""euthanasia""). Base models are both
more consistent compared to fine-tuned models and are uniform in their
consistency across topics, while fine-tuned models are more inconsistent about
some topics (""euthanasia"") than others (""women's rights"") like our human
subjects (n=165).","[{'name': 'Jared Moore'}, {'name': 'Tanvi Deshpande'}, {'name': 'Diyi Yang'}]",2024-07-03T10:53:54Z
http://arxiv.org/abs/2407.02987v1,http://arxiv.org/abs/2407.02987v1,"LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content
  Moderation of Large Language Models","Guardrails have emerged as an alternative to safety alignment for content
moderation of large language models (LLMs). Existing model-based guardrails
have not been designed for resource-constrained computational portable devices,
such as mobile phones, more and more of which are running LLM-based
applications locally. We introduce LoRA-Guard, a parameter-efficient guardrail
adaptation method that relies on knowledge sharing between LLMs and guardrail
models. LoRA-Guard extracts language features from the LLMs and adapts them for
the content moderation task using low-rank adapters, while a dual-path design
prevents any performance degradation on the generative task. We show that
LoRA-Guard outperforms existing approaches with 100-1000x lower parameter
overhead while maintaining accuracy, enabling on-device content moderation.","[{'name': 'Hayder Elesedy'}, {'name': 'Pedro M. Esperança'}, {'name': 'Silviu Vlad Oprea'}, {'name': 'Mete Ozay'}]",2024-07-03T10:38:40Z
http://arxiv.org/abs/2407.02978v1,http://arxiv.org/abs/2407.02978v1,"Mast Kalandar at SemEval-2024 Task 8: On the Trail of Textual Origins:
  RoBERTa-BiLSTM Approach to Detect AI-Generated Text","Large Language Models (LLMs) have showcased impressive abilities in
generating fluent responses to diverse user queries. However, concerns
regarding the potential misuse of such texts in journalism, educational, and
academic contexts have surfaced. SemEval 2024 introduces the task of
Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text
Detection, aiming to develop automated systems for identifying
machine-generated text and detecting potential misuse. In this paper, we i)
propose a RoBERTa-BiLSTM based classifier designed to classify text into two
categories: AI-generated or human ii) conduct a comparative study of our model
with baseline approaches to evaluate its effectiveness. This paper contributes
to the advancement of automatic text detection systems in addressing the
challenges posed by machine-generated text misuse. Our architecture ranked 46th
on the official leaderboard with an accuracy of 80.83 among 125.","[{'name': 'Jainit Sushil Bafna'}, {'name': 'Hardik Mittal'}, {'name': 'Suyash Sethia'}, {'name': 'Manish Shrivastava'}, {'name': 'Radhika Mamidi'}]",2024-07-03T10:22:23Z
http://arxiv.org/abs/2407.02977v1,http://arxiv.org/abs/2407.02977v1,Large Language Models as Evaluators for Scientific Synthesis,"Our study explores how well the state-of-the-art Large Language Models
(LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries
or, more fittingly, scientific syntheses, comparing their evaluations to those
of human annotators. We used a dataset of 100 research questions and their
syntheses made by GPT-4 from abstracts of five related papers, checked against
human quality ratings. The study evaluates both the closed-source GPT-4 and the
open-source Mistral model's ability to rate these summaries and provide reasons
for their judgments. Preliminary results show that LLMs can offer logical
explanations that somewhat match the quality ratings, yet a deeper statistical
analysis shows a weak correlation between LLM and human ratings, suggesting the
potential and current limitations of LLMs in scientific synthesis evaluation.","[{'name': 'Julia Evans'}, {'name': ""Jennifer D'Souza""}, {'name': 'Sören Auer'}]",2024-07-03T10:21:27Z
http://arxiv.org/abs/2407.02964v1,http://arxiv.org/abs/2407.02964v1,"FSM: A Finite State Machine Based Zero-Shot Prompting Paradigm for
  Multi-Hop Question Answering","Large Language Models (LLMs) with chain-of-thought (COT) prompting have
demonstrated impressive abilities on simple nature language inference tasks.
However, they tend to perform poorly on Multi-hop Question Answering (MHQA)
tasks due to several challenges, including hallucination, error propagation and
limited context length. We propose a prompting method, Finite State Machine
(FSM) to enhance the reasoning capabilities of LLM for complex tasks in
addition to improved effectiveness and trustworthiness. Different from COT
methods, FSM addresses MHQA by iteratively decomposing a question into
multi-turn sub-questions, and self-correcting in time, improving the accuracy
of answers in each step. Specifically, FSM addresses one sub-question at a time
and decides on the next step based on its current result and state, in an
automaton-like format. Experiments on benchmarks show the effectiveness of our
method. Although our method performs on par with the baseline on relatively
simpler datasets, it excels on challenging datasets like Musique. Moreover,
this approach mitigates the hallucination phenomenon, wherein the correct final
answer can be recovered despite errors in intermediate reasoning. Furthermore,
our method improves LLMs' ability to follow specified output format
requirements, significantly reducing the difficulty of answer interpretation
and the need for reformatting.","[{'name': 'Xiaochen Wang'}, {'name': 'Junqing He'}, {'name': 'Zhe yang'}, {'name': 'Yiru Wang'}, {'name': 'Xiangdi Meng'}, {'name': 'Kunhao Pan'}, {'name': 'Zhifang Sui'}]",2024-07-03T10:01:01Z
http://arxiv.org/abs/2407.02960v1,http://arxiv.org/abs/2407.02960v1,"ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary
  LLMs on Private Datasets","This work addresses the timely yet underexplored problem of performing
inference and finetuning of a proprietary LLM owned by a model provider entity
on the confidential/private data of another data owner entity, in a way that
ensures the confidentiality of both the model and the data. Hereby, the
finetuning is conducted offsite, i.e., on the computation infrastructure of a
third-party cloud provider. We tackle this problem by proposing ObfuscaTune, a
novel, efficient and fully utility-preserving approach that combines a simple
yet effective obfuscation technique with an efficient usage of confidential
computing (only 5% of the model parameters are placed on TEE). We empirically
demonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models
with different sizes on four NLP benchmark datasets. Finally, we compare to a
na\""ive version of our approach to highlight the necessity of using random
matrices with low condition numbers in our approach to reduce errors induced by
the obfuscation.","[{'name': 'Ahmed Frikha'}, {'name': 'Nassim Walha'}, {'name': 'Ricardo Mendes'}, {'name': 'Krishna Kanth Nakka'}, {'name': 'Xue Jiang'}, {'name': 'Xuebing Zhou'}]",2024-07-03T09:54:08Z
http://arxiv.org/abs/2407.02956v1,http://arxiv.org/abs/2407.02956v1,"IncogniText: Privacy-enhancing Conditional Text Anonymization via
  LLM-based Private Attribute Randomization","In this work, we address the problem of text anonymization where the goal is
to prevent adversaries from correctly inferring private attributes of the
author, while keeping the text utility, i.e., meaning and semantics. We propose
IncogniText, a technique that anonymizes the text to mislead a potential
adversary into predicting a wrong private attribute value. Our empirical
evaluation shows a reduction of private attribute leakage by more than 90%.
Finally, we demonstrate the maturity of IncogniText for real-world applications
by distilling its anonymization capability into a set of LoRA parameters
associated with an on-device model.","[{'name': 'Ahmed Frikha'}, {'name': 'Nassim Walha'}, {'name': 'Krishna Kanth Nakka'}, {'name': 'Ricardo Mendes'}, {'name': 'Xue Jiang'}, {'name': 'Xuebing Zhou'}]",2024-07-03T09:49:03Z
http://arxiv.org/abs/2407.02943v1,http://arxiv.org/abs/2407.02943v1,"PII-Compass: Guiding LLM training data extraction prompts towards the
  target PII via grounding","The latest and most impactful advances in large models stem from their
increased size. Unfortunately, this translates into an improved memorization
capacity, raising data privacy concerns. Specifically, it has been shown that
models can output personal identifiable information (PII) contained in their
training data. However, reported PIII extraction performance varies widely, and
there is no consensus on the optimal methodology to evaluate this risk,
resulting in underestimating realistic adversaries. In this work, we
empirically demonstrate that it is possible to improve the extractability of
PII by over ten-fold by grounding the prefix of the manually constructed
extraction prompt with in-domain data. Our approach, PII-Compass, achieves
phone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308
queries, respectively, i.e., the phone number of 1 person in 15 is extractable.","[{'name': 'Krishna Kanth Nakka'}, {'name': 'Ahmed Frikha'}, {'name': 'Ricardo Mendes'}, {'name': 'Xue Jiang'}, {'name': 'Xuebing Zhou'}]",2024-07-03T09:20:04Z
http://arxiv.org/abs/2407.17487v2,http://arxiv.org/abs/2407.17487v2,"Explainable Natural Language Processing for Corporate Sustainability
  Analysis","Sustainability commonly refers to entities, such as individuals, companies,
and institutions, having a non-detrimental (or even positive) impact on the
environment, society, and the economy. With sustainability becoming a synonym
of acceptable and legitimate behaviour, it is being increasingly demanded and
regulated. Several frameworks and standards have been proposed to measure the
sustainability impact of corporations, including United Nations' sustainable
development goals and the recently introduced global sustainability reporting
framework, amongst others. However, the concept of corporate sustainability is
complex due to the diverse and intricate nature of firm operations (i.e.
geography, size, business activities, interlinks with other stakeholders). As a
result, corporate sustainability assessments are plagued by subjectivity both
within data that reflect corporate sustainability efforts (i.e. corporate
sustainability disclosures) and the analysts evaluating them. This subjectivity
can be distilled into distinct challenges, such as incompleteness, ambiguity,
unreliability and sophistication on the data dimension, as well as limited
resources and potential bias on the analyst dimension. Put together,
subjectivity hinders effective cost attribution to entities non-compliant with
prevailing sustainability expectations, potentially rendering sustainability
efforts and its associated regulations futile. To this end, we argue that
Explainable Natural Language Processing (XNLP) can significantly enhance
corporate sustainability analysis. Specifically, linguistic understanding
algorithms (lexical, semantic, syntactic), integrated with XAI capabilities
(interpretability, explainability, faithfulness), can bridge gaps in analyst
resources and mitigate subjectivity problems within data.","[{'name': 'Keane Ong'}, {'name': 'Rui Mao'}, {'name': 'Ranjan Satapathy'}, {'name': 'Ricardo Shirota Filho'}, {'name': 'Erik Cambria'}, {'name': 'Johan Sulaeman'}, {'name': 'Gianmarco Mengaldo'}]",2024-07-03T08:27:51Z
http://arxiv.org/abs/2407.02894v1,http://arxiv.org/abs/2407.02894v1,"Translatotron-V(ison): An End-to-End Model for In-Image Machine
  Translation","In-image machine translation (IIMT) aims to translate an image containing
texts in source language into an image containing translations in target
language. In this regard, conventional cascaded methods suffer from issues such
as error propagation, massive parameters, and difficulties in deployment and
retaining visual characteristics of the input image. Thus, constructing
end-to-end models has become an option, which, however, faces two main
challenges: 1) the huge modeling burden, as it is required to simultaneously
learn alignment across languages and preserve the visual characteristics of the
input image; 2) the difficulties of directly predicting excessively lengthy
pixel sequences. In this paper, we propose \textit{Translatotron-V(ision)}, an
end-to-end IIMT model consisting of four modules. In addition to an image
encoder, and an image decoder, our model contains a target text decoder and an
image tokenizer. Among them, the target text decoder is used to alleviate the
language alignment burden, and the image tokenizer converts long sequences of
pixels into shorter sequences of visual tokens, preventing the model from
focusing on low-level visual features. Besides, we present a two-stage training
framework for our model to assist the model in learning alignment across
modalities and languages. Finally, we propose a location-aware evaluation
metric called Structure-BLEU to assess the translation quality of the generated
images. Experimental results demonstrate that our model achieves competitive
performance compared to cascaded models with only 70.9\% of parameters, and
significantly outperforms the pixel-level end-to-end IIMT model.","[{'name': 'Zhibin Lan'}, {'name': 'Liqiang Niu'}, {'name': 'Fandong Meng'}, {'name': 'Jie Zhou'}, {'name': 'Min Zhang'}, {'name': 'Jinsong Su'}]",2024-07-03T08:15:39Z
http://arxiv.org/abs/2407.02891v1,http://arxiv.org/abs/2407.02891v1,GPTQT: Quantize Large Language Models Twice to Push the Efficiency,"Due to their large size, generative Large Language Models (LLMs) require
significant computing and storage resources. This paper introduces a new
post-training quantization method, GPTQT, to reduce memory usage and enhance
processing speed by expressing the weight of LLM in 3bit/2bit. Practice has
shown that minimizing the quantization error of weights is ineffective, leading
to overfitting. Therefore, GPTQT employs a progressive two-step approach:
initially quantizing weights using Linear quantization to a relatively high
bit, followed by converting obtained int weight to lower bit binary coding. A
re-explore strategy is proposed to optimize initial scaling factor. During
inference, these steps are merged into pure binary coding, enabling efficient
computation. Testing across various models and datasets confirms GPTQT's
effectiveness. Compared to the strong 3-bit quantization baseline, GPTQT
further reduces perplexity by 4.01 on opt-66B and increases speed by 1.24 times
on opt-30b. The results on Llama2 show that GPTQT is currently the best binary
coding quantization method for such kind of LLMs.","[{'name': 'Yipin Guo'}, {'name': 'Yilin Lang'}, {'name': 'Qinyuan Ren'}]",2024-07-03T08:08:01Z
http://arxiv.org/abs/2407.02885v2,http://arxiv.org/abs/2407.02885v2,"CogErgLLM: Exploring Large Language Model Systems Design Perspective
  Using Cognitive Ergonomics","Integrating cognitive ergonomics with LLMs is essential for enhancing safety,
reliability, and user satisfaction in human-AI interactions. Current LLM design
often lacks this integration, leading to systems that may not fully align with
human cognitive capabilities and limitations. Insufficient focus on
incorporating cognitive science methods exacerbates biases in LLM outputs,
while inconsistent application of user-centered design principles results in
sub-optimal user experiences. To address these challenges, our position paper
explores the critical integration of cognitive ergonomics principles into LLM
design, aiming to provide a comprehensive framework and practical guidelines
for ethical LLM development. Through our contributions, we seek to advance
understanding and practice in integrating cognitive ergonomics into LLM
systems, fostering safer, more reliable, and ethically sound human-AI
interactions.",[{'name': 'Azmine Toushik Wasi'}],2024-07-03T07:59:52Z
http://arxiv.org/abs/2407.02883v1,http://arxiv.org/abs/2407.02883v1,CoIR: A Comprehensive Benchmark for Code Information Retrieval Models,"Despite the substantial success of Information Retrieval (IR) in various NLP
tasks, most IR systems predominantly handle queries and corpora in natural
language, neglecting the domain of code retrieval. Code retrieval is critically
important yet remains under-explored, with existing methods and benchmarks
inadequately representing the diversity of code in various domains and tasks.
Addressing this gap, we present \textbf{\name} (\textbf{Co}de
\textbf{I}nformation \textbf{R}etrieval Benchmark), a robust and comprehensive
benchmark specifically designed to assess code retrieval capabilities. \name
comprises \textbf{ten} meticulously curated code datasets, spanning
\textbf{eight} distinctive retrieval tasks across \textbf{seven} diverse
domains. We first discuss the construction of \name and its diverse dataset
composition. Further, we evaluate nine widely used retrieval models using
\name, uncovering significant difficulties in performing code retrieval tasks
even with state-of-the-art systems. To facilitate easy adoption and integration
within existing research workflows, \name has been developed as a user-friendly
Python framework, readily installable via pip. It shares same data schema as
other popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark
evaluations. Through \name, we aim to invigorate research in the code retrieval
domain, providing a versatile benchmarking tool that encourages further
development and exploration of code retrieval systems\footnote{\url{
https://github.com/CoIR-team/coir}}.","[{'name': 'Xiangyang Li'}, {'name': 'Kuicai Dong'}, {'name': 'Yi Quan Lee'}, {'name': 'Wei Xia'}, {'name': 'Yichun Yin'}, {'name': 'Hao Zhang'}, {'name': 'Yong Liu'}, {'name': 'Yasheng Wang'}, {'name': 'Ruiming Tang'}]",2024-07-03T07:58:20Z
http://arxiv.org/abs/2407.02867v1,http://arxiv.org/abs/2407.02867v1,"Contrast then Memorize: Semantic Neighbor Retrieval-Enhanced Inductive
  Multimodal Knowledge Graph Completion","A large number of studies have emerged for Multimodal Knowledge Graph
Completion (MKGC) to predict the missing links in MKGs. However, fewer studies
have been proposed to study the inductive MKGC (IMKGC) involving emerging
entities unseen during training. Existing inductive approaches focus on
learning textual entity representations, which neglect rich semantic
information in visual modality. Moreover, they focus on aggregating structural
neighbors from existing KGs, which of emerging entities are usually limited.
However, the semantic neighbors are decoupled from the topology linkage and
usually imply the true target entity. In this paper, we propose the IMKGC task
and a semantic neighbor retrieval-enhanced IMKGC framework CMR, where the
contrast brings the helpful semantic neighbors close, and then the memorize
supports semantic neighbor retrieval to enhance inference. Specifically, we
first propose a unified cross-modal contrastive learning to simultaneously
capture the textual-visual and textual-textual correlations of query-entity
pairs in a unified representation space. The contrastive learning increases the
similarity of positive query-entity pairs, therefore making the representations
of helpful semantic neighbors close. Then, we explicitly memorize the knowledge
representations to support the semantic neighbor retrieval. At test time, we
retrieve the nearest semantic neighbors and interpolate them to the
query-entity similarity distribution to augment the final prediction. Extensive
experiments validate the effectiveness of CMR on three inductive MKGC datasets.
Codes are available at https://github.com/OreOZhao/CMR.","[{'name': 'Yu Zhao'}, {'name': 'Ying Zhang'}, {'name': 'Baohang Zhou'}, {'name': 'Xinying Qian'}, {'name': 'Kehui Song'}, {'name': 'Xiangrui Cai'}]",2024-07-03T07:31:33Z
http://arxiv.org/abs/2407.02855v1,http://arxiv.org/abs/2407.02855v1,"Safe Unlearning: A Surprisingly Effective and Generalizable Solution to
  Defend Against Jailbreak Attacks","LLMs are known to be vulnerable to jailbreak attacks, even after safety
alignment. An important observation is that, while different types of jailbreak
attacks can generate significantly different queries, they mostly result in
similar responses that are rooted in the same harmful knowledge (e.g., detailed
steps to make a bomb). Therefore, we conjecture that directly unlearn the
harmful knowledge in the LLM can be a more effective way to defend against
jailbreak attacks than the mainstream supervised fine-tuning (SFT) based
approaches. Our extensive experiments confirmed our insight and suggested
surprising generalizability of our unlearning-based approach: using only 20 raw
harmful questions \emph{without} any jailbreak prompt during training, our
solution reduced the Attack Success Rate (ASR) in Vicuna-7B on
\emph{out-of-distribution} (OOD) harmful questions wrapped with various complex
jailbreak prompts from 82.6\% to 7.7\%. This significantly outperforms
Llama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment samples but
still has an ASR of 21.9\% even under the help of an additional safety system
prompt. Further analysis reveals that the generalization ability of our
solution stems from the intrinsic relatedness among harmful responses across
harmful questions (e.g., response patterns, shared steps and actions, and
similarity among their learned representations in the LLM). Our code is
available at \url{https://github.com/thu-coai/SafeUnlearning}.","[{'name': 'Zhexin Zhang'}, {'name': 'Junxiao Yang'}, {'name': 'Pei Ke'}, {'name': 'Shiyao Cui'}, {'name': 'Chujie Zheng'}, {'name': 'Hongning Wang'}, {'name': 'Minlie Huang'}]",2024-07-03T07:14:05Z
http://arxiv.org/abs/2407.02854v1,http://arxiv.org/abs/2407.02854v1,"Universal Gloss-level Representation for Gloss-free Sign Language
  Translation and Production","Sign language, essential for the deaf and hard-of-hearing, presents unique
challenges in translation and production due to its multimodal nature and the
inherent ambiguity in mapping sign language motion to spoken language words.
Previous methods often rely on gloss annotations, requiring time-intensive
labor and specialized expertise in sign language. Gloss-free methods have
emerged to address these limitations, but they often depend on external sign
language data or dictionaries, failing to completely eliminate the need for
gloss annotations. There is a clear demand for a comprehensive approach that
can supplant gloss annotations and be utilized for both Sign Language
Translation (SLT) and Sign Language Production (SLP). We introduce Universal
Gloss-level Representation (UniGloR), a unified and self-supervised solution
for both SLT and SLP, trained on multiple datasets including PHOENIX14T,
How2Sign, and NIASL2021. Our results demonstrate UniGloR's effectiveness in the
translation and production tasks. We further report an encouraging result for
the Sign Language Recognition (SLR) on previously unseen data. Our study
suggests that self-supervised learning can be made in a unified manner, paving
the way for innovative and practical applications in future research.","[{'name': 'Eui Jun Hwang'}, {'name': 'Sukmin Cho'}, {'name': 'Huije Lee'}, {'name': 'Youngwoo Yoon'}, {'name': 'Jong C. Park'}]",2024-07-03T07:12:36Z
http://arxiv.org/abs/2407.02842v1,http://arxiv.org/abs/2407.02842v1,"MindBench: A Comprehensive Benchmark for Mind Map Structure Recognition
  and Analysis","Multimodal Large Language Models (MLLM) have made significant progress in the
field of document analysis. Despite this, existing benchmarks typically focus
only on extracting text and simple layout information, neglecting the complex
interactions between elements in structured documents such as mind maps and
flowcharts. To address this issue, we introduce the new benchmark named
MindBench, which not only includes meticulously constructed bilingual authentic
or synthetic images, detailed annotations, evaluation metrics and baseline
models, but also specifically designs five types of structured understanding
and parsing tasks. These tasks include full parsing, partial parsing,
position-related parsing, structured Visual Question Answering (VQA), and
position-related VQA, covering key areas such as text recognition, spatial
awareness, relationship discernment, and structured parsing. Extensive
experimental results demonstrate the substantial potential and significant room
for improvement in current models' ability to handle structured document
information. We anticipate that the launch of MindBench will significantly
advance research and application development in structured document analysis
technology. MindBench is available at:
https://miasanlei.github.io/MindBench.github.io/.","[{'name': 'Lei Chen'}, {'name': 'Feng Yan'}, {'name': 'Yujie Zhong'}, {'name': 'Shaoxiang Chen'}, {'name': 'Zequn Jie'}, {'name': 'Lin Ma'}]",2024-07-03T06:39:18Z
http://arxiv.org/abs/2407.02837v1,http://arxiv.org/abs/2407.02837v1,"Comparing Feature-based and Context-aware Approaches to PII
  Generalization Level Prediction","Protecting Personal Identifiable Information (PII) in text data is crucial
for privacy, but current PII generalization methods face challenges such as
uneven data distributions and limited context awareness. To address these
issues, we propose two approaches: a feature-based method using machine
learning to improve performance on structured inputs, and a novel context-aware
framework that considers the broader context and semantic relationships between
the original text and generalized candidates. The context-aware approach
employs Multilingual-BERT for text representation, functional transformations,
and mean squared error scoring to evaluate candidates. Experiments on the
WikiReplace dataset demonstrate the effectiveness of both methods, with the
context-aware approach outperforming the feature-based one across different
scales. This work contributes to advancing PII generalization techniques by
highlighting the importance of feature selection, ensemble learning, and
incorporating contextual information for better privacy protection in text
anonymization.","[{'name': 'Kailin Zhang'}, {'name': 'Xinying Qiu'}]",2024-07-03T06:32:03Z
http://arxiv.org/abs/2407.13781v1,http://arxiv.org/abs/2407.13781v1,"RDBE: Reasoning Distillation-Based Evaluation Enhances Automatic Essay
  Scoring","Recently, various encoder-only and encoder-decoder pre-trained models like
BERT and T5 have been applied to automatic essay scoring (AES) as small
language models. However, existing studies have primarily treated this task
akin to a classification problem, focusing solely on outputting scores in the
target text without offering interpretations for the generated scores.
Departing from the approaches, we introduce Reasoning Distillation-Based
Evaluation (RDBE), which integrates interpretability to elucidate the rationale
behind model scores while enhancing performance through initial reasoning. This
interpretive capability is acquired during training by leveraging generated
reasoning from a large language model (LLM) to distill a small language model
(SLM). Our experimental results demonstrate the efficacy of RDBE across all
scoring rubrics considered in the dataset. RDBE outperforms both zero-shot LLM
generation and generation from a baseline fine-tuned model, establishing itself
as state-of-the-art in the corresponding dataset. This highlights its practical
interpretative output and enhanced performance.",[{'name': 'Ali Ghiasvand Mohammadkhani'}],2024-07-03T05:49:01Z
http://arxiv.org/abs/2407.02820v1,http://arxiv.org/abs/2407.02820v1,"Investigating the Contextualised Word Embedding Dimensions Responsible
  for Contextual and Temporal Semantic Changes","Words change their meaning over time as well as in different contexts. The
sense-aware contextualised word embeddings (SCWEs) such as the ones produced by
XL-LEXEME by fine-tuning masked langauge models (MLMs) on Word-in-Context (WiC)
data attempt to encode such semantic changes of words within the contextualised
word embedding (CWE) spaces. Despite the superior performance of SCWEs in
contextual/temporal semantic change detection (SCD) benchmarks, it remains
unclear as to how the meaning changes are encoded in the embedding space. To
study this, we compare pre-trained CWEs and their fine-tuned versions on
contextual and temporal semantic change benchmarks under Principal Component
Analysis (PCA) and Independent Component Analysis (ICA) transformations. Our
experimental results reveal several novel insights such as (a) although there
exist a smaller number of axes that are responsible for semantic changes of
words in the pre-trained CWE space, this information gets distributed across
all dimensions when fine-tuned, and (b) in contrast to prior work studying the
geometry of CWEs, we find that PCA to better represent semantic changes than
ICA. Source code is available at https://github.com/LivNLP/svp-dims .","[{'name': 'Taichi Aida'}, {'name': 'Danushka Bollegala'}]",2024-07-03T05:42:20Z
http://arxiv.org/abs/2407.02819v1,http://arxiv.org/abs/2407.02819v1,"Efficient Training of Language Models with Compact and Consistent Next
  Token Distributions","Maximizing the likelihood of the next token is an established, statistically
sound objective for pre-training language models. In this paper we show that we
can train better models faster by pre-aggregating the corpus with a collapsed
$n$-gram distribution. Previous studies have proposed corpus-level $n$-gram
statistics as a regularizer; however, the construction and querying of such
$n$-grams, if done naively, prove to be costly and significantly impede
training speed, thereby limiting their application in modern large language
model pre-training.
  We introduce an alternative compact representation of the next token
distribution that, in expectation, aligns with the complete $n$-gram
distribution while markedly reducing variance across mini-batches compared to
the standard next-token loss. Empirically, we demonstrate that both the
$n$-gram regularized model and our approximation yield substantial improvements
in model quality and convergence rate compared to existing methods.
Furthermore, our approximation facilitates scalability of gains to larger
datasets and models compared to the straightforward $n$-gram regularization
method.","[{'name': 'Ashutosh Sathe'}, {'name': 'Sunita Sarawagi'}]",2024-07-03T05:40:41Z
http://arxiv.org/abs/2407.02814v1,http://arxiv.org/abs/2407.02814v1,"Images Speak Louder than Words: Understanding and Mitigating Bias in
  Vision-Language Model from a Causal Mediation Perspective","Vision-language models (VLMs) pre-trained on extensive datasets can
inadvertently learn biases by correlating gender information with specific
objects or scenarios. Current methods, which focus on modifying inputs and
monitoring changes in the model's output probability scores, often struggle to
comprehensively understand bias from the perspective of model components. We
propose a framework that incorporates causal mediation analysis to measure and
map the pathways of bias generation and propagation within VLMs. This approach
allows us to identify the direct effects of interventions on model bias and the
indirect effects of interventions on bias mediated through different model
components. Our results show that image features are the primary contributors
to bias, with significantly higher impacts than text features, specifically
accounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE
datasets, respectively. Notably, the image encoder's contribution surpasses
that of the text encoder and the deep fusion encoder. Further experimentation
confirms that contributions from both language and vision modalities are
aligned and non-conflicting. Consequently, focusing on blurring gender
representations within the image encoder, which contributes most to the model
bias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and
PASCAL-SENTENCE datasets, respectively, with minimal performance loss or
increased computational demands.","[{'name': 'Zhaotian Weng'}, {'name': 'Zijun Gao'}, {'name': 'Jerone Andrews'}, {'name': 'Jieyu Zhao'}]",2024-07-03T05:19:45Z
http://arxiv.org/abs/2407.02751v2,http://arxiv.org/abs/2407.02751v2,"Emotion and Intent Joint Understanding in Multimodal Conversation: A
  Benchmarking Dataset","Emotion and Intent Joint Understanding in Multimodal Conversation (MC-EIU)
aims to decode the semantic information manifested in a multimodal
conversational history, while inferring the emotions and intents simultaneously
for the current utterance. MC-EIU is enabling technology for many
human-computer interfaces. However, there is a lack of available datasets in
terms of annotation, modality, language diversity, and accessibility. In this
work, we propose an MC-EIU dataset, which features 7 emotion categories, 9
intent categories, 3 modalities, i.e., textual, acoustic, and visual content,
and two languages, i.e., English and Mandarin. Furthermore, it is completely
open-source for free access. To our knowledge, MC-EIU is the first
comprehensive and rich emotion and intent joint understanding dataset for
multimodal conversation. Together with the release of the dataset, we also
develop an Emotion and Intent Interaction (EI$^2$) network as a reference
system by modeling the deep correlation between emotion and intent in the
multimodal conversation. With comparative experiments and ablation studies, we
demonstrate the effectiveness of the proposed EI$^2$ method on the MC-EIU
dataset. The dataset and codes will be made available at:
https://github.com/MC-EIU/MC-EIU.","[{'name': 'Rui Liu'}, {'name': 'Haolin Zuo'}, {'name': 'Zheng Lian'}, {'name': 'Xiaofen Xing'}, {'name': 'Björn W. Schuller'}, {'name': 'Haizhou Li'}]",2024-07-03T01:56:00Z
http://arxiv.org/abs/2407.02750v1,http://arxiv.org/abs/2407.02750v1,"Learning to Reduce: Towards Improving Performance of Large Language
  Models on Structured Data","Large Language Models (LLMs) have been achieving competent performance on a
wide range of downstream tasks, yet existing work shows that inference on
structured data is challenging for LLMs. This is because LLMs need to either
understand long structured data or select the most relevant evidence before
inference, and both approaches are not trivial. This paper proposes a
framework, Learning to Reduce, that fine-tunes a language model with On-Policy
Learning to generate a reduced version of an input structured data. When
compared to state-of-the-art LLMs like GPT-4, Learning to Reduce not only
achieves outstanding performance in reducing the input, but shows
generalizability on different datasets. We further show that the model
fine-tuned with our framework helps LLMs better perform on table QA tasks
especially when the context is longer.","[{'name': 'Younghun Lee'}, {'name': 'Sungchul Kim'}, {'name': 'Ryan A. Rossi'}, {'name': 'Tong Yu'}, {'name': 'Xiang Chen'}]",2024-07-03T01:51:50Z
http://arxiv.org/abs/2407.02723v1,http://arxiv.org/abs/2407.02723v1,"e-Health CSIRO at ""Discharge Me!"" 2024: Generating Discharge Summary
  Sections with Fine-tuned Language Models","Clinical documentation is an important aspect of clinicians' daily work and
often demands a significant amount of time. The BioNLP 2024 Shared Task on
Streamlining Discharge Documentation (Discharge Me!) aims to alleviate this
documentation burden by automatically generating discharge summary sections,
including brief hospital course and discharge instruction, which are often
time-consuming to synthesize and write manually. We approach the generation
task by fine-tuning multiple open-sourced language models (LMs), including both
decoder-only and encoder-decoder LMs, with various configurations on input
context. We also examine different setups for decoding algorithms, model
ensembling or merging, and model specialization. Our results show that
conditioning on the content of discharge summary prior to the target sections
is effective for the generation task. Furthermore, we find that smaller
encoder-decoder LMs can work as well or even slightly better than larger
decoder based LMs fine-tuned through LoRA. The model checkpoints from our team
(aehrc) are openly available.","[{'name': 'Jinghui Liu'}, {'name': 'Aaron Nicolson'}, {'name': 'Jason Dowling'}, {'name': 'Bevan Koopman'}, {'name': 'Anthony Nguyen'}]",2024-07-03T00:32:28Z
http://arxiv.org/abs/2407.02719v1,http://arxiv.org/abs/2407.02719v1,Boosting Biomedical Concept Extraction by Rule-Based Data Augmentation,"Document-level biomedical concept extraction is the task of identifying
biomedical concepts mentioned in a given document. Recent advancements have
adapted pre-trained language models for this task. However, the scarcity of
domain-specific data and the deviation of concepts from their canonical names
often hinder these models' effectiveness. To tackle this issue, we employ
MetaMapLite, an existing rule-based concept mapping system, to generate
additional pseudo-annotated data from PubMed and PMC. The annotated data are
used to augment the limited training data. Through extensive experiments, this
study demonstrates the utility of a manually crafted concept mapping tool for
training a better concept extraction model.","[{'name': 'Qiwei Shao'}, {'name': 'Fengran Mo'}, {'name': 'Jian-Yun Nie'}]",2024-07-03T00:00:21Z
http://arxiv.org/abs/2407.02662v1,http://arxiv.org/abs/2407.02662v1,"Supporters and Skeptics: LLM-based Analysis of Engagement with Mental
  Health (Mis)Information Content on Video-sharing Platforms","Over one in five adults in the US lives with a mental illness. In the face of
a shortage of mental health professionals and offline resources, online
short-form video content has grown to serve as a crucial conduit for
disseminating mental health help and resources. However, the ease of content
creation and access also contributes to the spread of misinformation, posing
risks to accurate diagnosis and treatment. Detecting and understanding
engagement with such content is crucial to mitigating their harmful effects on
public health. We perform the first quantitative study of the phenomenon using
YouTube Shorts and Bitchute as the sites of study. We contribute MentalMisinfo,
a novel labeled mental health misinformation (MHMisinfo) dataset of 739 videos
(639 from Youtube and 100 from Bitchute) and 135372 comments in total, using an
expert-driven annotation schema. We first found that few-shot in-context
learning with large language models (LLMs) are effective in detecting MHMisinfo
videos. Next, we discover distinct and potentially alarming linguistic patterns
in how audiences engage with MHMisinfo videos through commentary on both
video-sharing platforms. Across the two platforms, comments could exacerbate
prevailing stigma with some groups showing heightened susceptibility to and
alignment with MHMisinfo. We discuss technical and public health-driven
adaptive solutions to tackling the ""epidemic"" of mental health misinformation
online.","[{'name': 'Viet Cuong Nguyen'}, {'name': 'Mini Jain'}, {'name': 'Abhijat Chauhan'}, {'name': 'Heather Jaime Soled'}, {'name': 'Santiago Alvarez Lesmes'}, {'name': 'Zihang Li'}, {'name': 'Michael L. Birnbaum'}, {'name': 'Sunny X. Tang'}, {'name': 'Srijan Kumar'}, {'name': 'Munmun De Choudhury'}]",2024-07-02T20:51:06Z
http://arxiv.org/abs/2407.02659v2,http://arxiv.org/abs/2407.02659v2,"LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model
  Training Data Through Knowledge Graph Comparison","In light of recent legal allegations brought by publishers, newspapers, and
other creators of copyrighted corpora against large language model developers
who use their copyrighted materials for training or fine-tuning purposes, we
propose a novel system, a variant of a plagiarism detection system, that
assesses whether a knowledge source has been used in the training or
fine-tuning of a large language model. Unlike current methods, we utilize an
approach that uses Resource Description Framework (RDF) triples to create
knowledge graphs from both a source document and an LLM continuation of that
document. These graphs are then analyzed with respect to content using cosine
similarity and with respect to structure using a normalized version of graph
edit distance that shows the degree of isomorphism. Unlike traditional
plagiarism systems that focus on content matching and keyword identification
between a source and a target corpus, our approach enables a broader and more
accurate evaluation of similarity between a source document and LLM
continuation by focusing on relationships between ideas and their organization
with regards to others. Additionally, our approach does not require access to
LLM metrics like perplexity that may be unavailable in closed large language
model ""black-box"" systems, as well as the training corpus. We thus assess
whether an LLM has ""plagiarized"" a corpus in its continuation through
similarity measures. A prototype of our system will be found on a hyperlinked
GitHub repository.","[{'name': 'Devam Mondal'}, {'name': 'Carlo Lipizzi'}]",2024-07-02T20:49:21Z
http://arxiv.org/abs/2407.02646v1,http://arxiv.org/abs/2407.02646v1,"A Practical Review of Mechanistic Interpretability for Transformer-Based
  Language Models","Mechanistic interpretability (MI) is an emerging sub-field of
interpretability that seeks to understand a neural network model by
reverse-engineering its internal computations. Recently, MI has garnered
significant attention for interpreting transformer-based language models (LMs),
resulting in many novel insights yet introducing new challenges. However, there
has not been work that comprehensively reviews these insights and challenges,
particularly as a guide for newcomers to this field. To fill this gap, we
present a comprehensive survey outlining fundamental objects of study in MI,
techniques that have been used for its investigation, approaches for evaluating
MI results, and significant findings and applications stemming from the use of
MI to understand LMs. In particular, we present a roadmap for beginners to
navigate the field and leverage MI for their benefit. Finally, we also identify
current gaps in the field and discuss potential future directions.","[{'name': 'Daking Rai'}, {'name': 'Yilun Zhou'}, {'name': 'Shi Feng'}, {'name': 'Abulhair Saparov'}, {'name': 'Ziyu Yao'}]",2024-07-02T20:28:16Z
http://arxiv.org/abs/2407.02637v1,http://arxiv.org/abs/2407.02637v1,Change My Frame: Reframing in the Wild in r/ChangeMyView,"Recent work in reframing, within the scope of text style transfer, has so far
made use of out-of-context, task-prompted utterances in order to produce
neutralizing or optimistic reframes. Our work aims to generalize reframing
based on the subreddit r/ChangeMyView (CMV). We build a dataset that leverages
CMV's community's interactions and conventions to identify high-value,
community-recognized utterances that produce changes of perspective. With this
data, we widen the scope of the direction of reframing since the changes in
perspective do not only occur in neutral or positive directions. We fine tune
transformer-based models, make use of a modern LLM to refine our dataset, and
explore challenges in the dataset creation and evaluation around this type of
reframing.","[{'name': 'Arturo Martínez Peguero'}, {'name': 'Taro Watanabe'}]",2024-07-02T20:09:11Z
http://arxiv.org/abs/2407.02604v2,http://arxiv.org/abs/2407.02604v2,"D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data
  and eXpert model predictions","Large vision language models (VLMs) have progressed incredibly from research
to applicability for general-purpose use cases. LLaVA-Med, a pioneering large
language and vision assistant for biomedicine, can perform multi-modal
biomedical image and data analysis to provide a natural language interface for
radiologists. While it is highly generalizable and works with multi-modal data,
it is currently limited by well-known challenges that exist in the large
language model space. Hallucinations and imprecision in responses can lead to
misdiagnosis which currently hinder the clinical adaptability of VLMs. To
create precise, user-friendly models in healthcare, we propose D-Rax -- a
domain-specific, conversational, radiologic assistance tool that can be used to
gain insights about a particular radiologic image. In this study, we enhance
the conversational analysis of chest X-ray (CXR) images to support radiological
reporting, offering comprehensive insights from medical imaging and aiding in
the formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the
LLaVA-Med architecture on our curated enhanced instruction-following data,
comprising of images, instructions, as well as disease diagnosis and
demographic predictions derived from MIMIC-CXR imaging data, CXR-related visual
question answer (VQA) pairs, and predictive outcomes from multiple expert AI
models. We observe statistically significant improvement in responses when
evaluated for both open and close-ended conversations. Leveraging the power of
state-of-the-art diagnostic models combined with VLMs, D-Rax empowers
clinicians to interact with medical images using natural language, which could
potentially streamline their decision-making process, enhance diagnostic
accuracy, and conserve their time.","[{'name': 'Hareem Nisar'}, {'name': 'Syed Muhammad Anwar'}, {'name': 'Zhifan Jiang'}, {'name': 'Abhijeet Parida'}, {'name': 'Ramon Sanchez-Jacob'}, {'name': 'Vishwesh Nath'}, {'name': 'Holger R. Roth'}, {'name': 'Marius George Linguraru'}]",2024-07-02T18:43:10Z
http://arxiv.org/abs/2407.02596v1,http://arxiv.org/abs/2407.02596v1,Towards More Realistic Extraction Attacks: An Adversarial Perspective,"Language models are prone to memorizing large parts of their training data,
making them vulnerable to extraction attacks. Existing research on these
attacks remains limited in scope, often studying isolated trends rather than
the real-world interactions with these models. In this paper, we revisit
extraction attacks from an adversarial perspective, exploiting the brittleness
of language models. We find significant churn in extraction attack trends,
i.e., even minor, unintuitive changes to the prompt, or targeting smaller
models and older checkpoints, can exacerbate the risks of extraction by up to
$2-4 \times$. Moreover, relying solely on the widely accepted verbatim match
underestimates the extent of extracted information, and we provide various
alternatives to more accurately capture the true risks of extraction. We
conclude our discussion with data deduplication, a commonly suggested
mitigation strategy, and find that while it addresses some memorization
concerns, it remains vulnerable to the same escalation of extraction risks
against a real-world adversary. Our findings highlight the necessity of
acknowledging an adversary's true capabilities to avoid underestimating
extraction risks.","[{'name': 'Yash More'}, {'name': 'Prakhar Ganesh'}, {'name': 'Golnoosh Farnadi'}]",2024-07-02T18:33:49Z
http://arxiv.org/abs/2407.02490v1,http://arxiv.org/abs/2407.02490v1,"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via
  Dynamic Sparse Attention","The computational challenges of Large Language Model (LLM) inference remain a
significant barrier to their widespread deployment, especially as prompt
lengths continue to increase. Due to the quadratic complexity of the attention
computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens
(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for
speeding up prefilling often fail to maintain acceptable accuracy or efficiency
when applied to long-context LLMs. To address this gap, we introduce MInference
(Milliontokens Inference), a sparse calculation method designed to accelerate
pre-filling of long-sequence processing. Specifically, we identify three unique
patterns in long-context attention matrices-the A-shape, Vertical-Slash, and
Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We
determine the optimal pattern for each attention head offline and dynamically
build sparse indices based on the assigned pattern during inference. With the
pattern and sparse indices, we perform efficient sparse attention calculations
via our optimized GPU kernels to significantly reduce the latency in the
pre-filling stage of long-context LLMs. Our proposed technique can be directly
applied to existing LLMs without any modifications to the pre-training setup or
additional fine-tuning. By evaluating on a wide range of downstream tasks,
including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models
including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we
demonstrate that MInference effectively reduces inference latency by up to 10x
for pre-filling on an A100, while maintaining accuracy. Our code is available
at https://aka.ms/MInference.","[{'name': 'Huiqiang Jiang'}, {'name': 'Yucheng Li'}, {'name': 'Chengruidong Zhang'}, {'name': 'Qianhui Wu'}, {'name': 'Xufang Luo'}, {'name': 'Surin Ahn'}, {'name': 'Zhenhua Han'}, {'name': 'Amir H. Abdi'}, {'name': 'Dongsheng Li'}, {'name': 'Chin-Yew Lin'}, {'name': 'Yuqing Yang'}, {'name': 'Lili Qiu'}]",2024-07-02T17:59:56Z
http://arxiv.org/abs/2407.02486v1,http://arxiv.org/abs/2407.02486v1,Neurocache: Efficient Vector Retrieval for Long-range Language Modeling,"This paper introduces Neurocache, an approach to extend the effective context
size of large language models (LLMs) using an external vector cache to store
its past states. Like recent vector retrieval approaches, Neurocache uses an
efficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states
and incorporate them into the attention process. Neurocache improves upon
previous methods by (1) storing compressed states, which reduces cache size;
(2) performing a single retrieval operation per token which increases inference
speed; and (3) extending the retrieval window to neighboring states, which
improves both language modeling and downstream task accuracy. Our experiments
show the effectiveness of Neurocache both for models trained from scratch and
for pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the
cache mechanism. We also compare Neurocache with text retrieval methods and
show improvements in single-document question-answering and few-shot learning
tasks. We made the source code available under:
https://github.com/alisafaya/neurocache","[{'name': 'Ali Safaya'}, {'name': 'Deniz Yuret'}]",2024-07-02T17:59:29Z
http://arxiv.org/abs/2407.02485v1,http://arxiv.org/abs/2407.02485v1,"RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in
  LLMs","Large language models (LLMs) typically utilize the top-k contexts from a
retriever in retrieval-augmented generation (RAG). In this work, we propose a
novel instruction fine-tuning framework RankRAG, which instruction-tunes a
single LLM for the dual purpose of context ranking and answer generation in
RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding
a small fraction of ranking data into the training blend, and outperform
existing expert ranking models, including the same LLM exclusively fine-tuned
on a large amount of ranking data. For generation, we compare our model with
many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and
ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG
benchmarks. Specifically, our Llama3-RankRAG significantly outperforms
Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In
addition, it also performs comparably to GPT-4 on five RAG benchmarks in the
biomedical domain without instruction fine-tuning on biomedical data,
demonstrating its superb capability for generalization to new domains.","[{'name': 'Yue Yu'}, {'name': 'Wei Ping'}, {'name': 'Zihan Liu'}, {'name': 'Boxin Wang'}, {'name': 'Jiaxuan You'}, {'name': 'Chao Zhang'}, {'name': 'Mohammad Shoeybi'}, {'name': 'Bryan Catanzaro'}]",2024-07-02T17:59:17Z
http://arxiv.org/abs/2407.02483v1,http://arxiv.org/abs/2407.02483v1,MMedAgent: Learning to Use Medical Tools with Multi-modal Agent,"Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit
limited generality and often fall short when compared to specialized models.
Recently, LLM-based agents have been developed to address these challenges by
selecting appropriate specialized models as tools based on user inputs.
However, such advancements have not been extensively explored within the
medical domain. To bridge this gap, this paper introduces the first agent
explicitly designed for the medical field, named \textbf{M}ulti-modal
\textbf{Med}ical \textbf{Agent} (MMedAgent). We curate an instruction-tuning
dataset comprising six medical tools solving seven tasks, enabling the agent to
choose the most suitable tools for a given task. Comprehensive experiments
demonstrate that MMedAgent achieves superior performance across a variety of
medical tasks compared to state-of-the-art open-source methods and even the
closed-source model, GPT-4o. Furthermore, MMedAgent exhibits efficiency in
updating and integrating new medical tools.","[{'name': 'Binxu Li'}, {'name': 'Tiankai Yan'}, {'name': 'Yuanting Pan'}, {'name': 'Zhe Xu'}, {'name': 'Jie Luo'}, {'name': 'Ruiyang Ji'}, {'name': 'Shilong Liu'}, {'name': 'Haoyu Dong'}, {'name': 'Zihao Lin'}, {'name': 'Yixin Wang'}]",2024-07-02T17:58:23Z
http://arxiv.org/abs/2407.02477v1,http://arxiv.org/abs/2407.02477v1,Understanding Alignment in Multimodal LLMs: A Comprehensive Study,"Preference alignment has become a crucial component in enhancing the
performance of Large Language Models (LLMs), yet its impact in Multimodal Large
Language Models (MLLMs) remains comparatively underexplored. Similar to
language models, MLLMs for image understanding tasks encounter challenges like
hallucination. In MLLMs, hallucination can occur not only by stating incorrect
facts but also by producing responses that are inconsistent with the image
content. A primary objective of alignment for MLLMs is to encourage these
models to align responses more closely with image information. Recently,
multiple works have introduced preference datasets for MLLMs and examined
different alignment methods, including Direct Preference Optimization (DPO) and
Proximal Policy Optimization (PPO). However, due to variations in datasets,
base model types, and alignment methods, it remains unclear which specific
elements contribute most significantly to the reported improvements in these
works. In this paper, we independently analyze each aspect of preference
alignment in MLLMs. We start by categorizing the alignment algorithms into two
groups, offline (such as DPO), and online (such as online-DPO), and show that
combining offline and online methods can improve the performance of the model
in certain scenarios. We review a variety of published multimodal preference
datasets and discuss how the details of their construction impact model
performance. Based on these insights, we introduce a novel way of creating
multimodal preference data called Bias-Driven Hallucination Sampling (BDHS)
that needs neither additional annotation nor external models, and show that it
can achieve competitive performance to previously published alignment work for
multimodal models across a range of benchmarks.","[{'name': 'Elmira Amirloo'}, {'name': 'Jean-Philippe Fauconnier'}, {'name': 'Christoph Roesmann'}, {'name': 'Christian Kerl'}, {'name': 'Rinu Boney'}, {'name': 'Yusu Qian'}, {'name': 'Zirui Wang'}, {'name': 'Afshin Dehghan'}, {'name': 'Yinfei Yang'}, {'name': 'Zhe Gan'}, {'name': 'Peter Grasch'}]",2024-07-02T17:55:03Z
http://arxiv.org/abs/2407.02472v1,http://arxiv.org/abs/2407.02472v1,"ValueScope: Unveiling Implicit Norms and Values via Return Potential
  Model of Social Interactions","This study introduces ValueScope, a framework leveraging language models to
quantify social norms and values within online communities, grounded in social
science perspectives on normative structures. We employ ValueScope to dissect
and analyze linguistic and stylistic expressions across 13 Reddit communities
categorized under gender, politics, science, and finance. Our analysis provides
a quantitative foundation showing that even closely related communities exhibit
remarkably diverse norms. This diversity supports existing theories and adds a
new dimension--community preference--to understanding community interactions.
ValueScope not only delineates differing social norms among communities but
also effectively traces their evolution and the influence of significant
external events like the U.S. presidential elections and the emergence of new
sub-communities. The framework thus highlights the pivotal role of social norms
in shaping online interactions, presenting a substantial advance in both the
theory and application of social norm studies in digital spaces.","[{'name': 'Chan Young Park'}, {'name': 'Shuyue Stella Li'}, {'name': 'Hayoung Jung'}, {'name': 'Svitlana Volkova'}, {'name': 'Tanushree Mitra'}, {'name': 'David Jurgens'}, {'name': 'Yulia Tsvetkov'}]",2024-07-02T17:51:27Z
http://arxiv.org/abs/2407.02552v1,http://arxiv.org/abs/2407.02552v1,"RLHF Can Speak Many Languages: Unlocking Multilingual Preference
  Optimization for LLMs","Preference optimization techniques have become a standard final stage for
training state-of-art large language models (LLMs). However, despite widespread
adoption, the vast majority of work to-date has focused on first-class citizen
languages like English and Chinese. This captures a small fraction of the
languages in the world, but also makes it unclear which aspects of current
state-of-the-art research transfer to a multilingual setting. In this work, we
perform an exhaustive study to achieve a new state-of-the-art in aligning
multilingual LLMs. We introduce a novel, scalable method for generating
high-quality multilingual feedback data to balance data coverage. We establish
the benefits of cross-lingual transfer and increased dataset size in preference
training. Our preference-trained model achieves a 54.4% win-rate against Aya 23
8B, the current state-of-the-art multilingual LLM in its parameter class, and a
69.5% win-rate or higher against widely used models like Gemma-1.1-7B-it,
Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.3. As a result of our study, we
expand the frontier of alignment techniques to 23 languages covering half of
the world's population.","[{'name': 'John Dang'}, {'name': 'Arash Ahmadian'}, {'name': 'Kelly Marchisio'}, {'name': 'Julia Kreutzer'}, {'name': 'Ahmet Üstün'}, {'name': 'Sara Hooker'}]",2024-07-02T17:42:30Z
http://arxiv.org/abs/2407.02448v1,http://arxiv.org/abs/2407.02448v1,"Ensemble of pre-trained language models and data augmentation for hate
  speech detection from Arabic tweets","Today, hate speech classification from Arabic tweets has drawn the attention
of several researchers. Many systems and techniques have been developed to
resolve this classification task. Nevertheless, two of the major challenges
faced in this context are the limited performance and the problem of imbalanced
data. In this study, we propose a novel approach that leverages ensemble
learning and semi-supervised learning based on previously manually labeled. We
conducted experiments on a benchmark dataset by classifying Arabic tweets into
5 distinct classes: non-hate, general hate, racial, religious, or sexism.
Experimental results show that: (1) ensemble learning based on pre-trained
language models outperforms existing related works; (2) Our proposed data
augmentation improves the accuracy results of hate speech detection from Arabic
tweets and outperforms existing related works. Our main contribution is the
achievement of encouraging results in Arabic hate speech detection.","[{'name': 'Kheir Eddine Daouadi'}, {'name': 'Yaakoub Boualleg'}, {'name': 'Kheir Eddine Haouaouchi'}]",2024-07-02T17:26:26Z
http://arxiv.org/abs/2407.02446v1,http://arxiv.org/abs/2407.02446v1,"Predicting vs. Acting: A Trade-off Between World Modeling & Agent
  Modeling","RLHF-aligned LMs have shown unprecedented ability on both benchmarks and
long-form text generation, yet they struggle with one foundational task:
next-token prediction. As RLHF models become agent models aimed at interacting
with humans, they seem to lose their world modeling -- the ability to predict
what comes next in arbitrary documents, which is the foundational training
objective of the Base LMs that RLHF adapts.
  Besides empirically demonstrating this trade-off, we propose a potential
explanation: to perform coherent long-form generation, RLHF models restrict
randomness via implicit blueprints. In particular, RLHF models concentrate
probability on sets of anchor spans that co-occur across multiple generations
for the same prompt, serving as textual scaffolding but also limiting a model's
ability to generate documents that do not include these spans. We study this
trade-off on the most effective current agent models, those aligned with RLHF,
while exploring why this may remain a fundamental trade-off between models that
act and those that predict, even as alignment techniques improve.","[{'name': 'Margaret Li'}, {'name': 'Weijia Shi'}, {'name': 'Artidoro Pagnoni'}, {'name': 'Peter West'}, {'name': 'Ari Holtzman'}]",2024-07-02T17:22:54Z
http://arxiv.org/abs/2407.02432v1,http://arxiv.org/abs/2407.02432v1,"Evaluating the Robustness of Adverse Drug Event Classification Models
  Using Templates","An adverse drug effect (ADE) is any harmful event resulting from medical drug
treatment. Despite their importance, ADEs are often under-reported in official
channels. Some research has therefore turned to detecting discussions of ADEs
in social media. Impressive results have been achieved in various attempts to
detect ADEs. In a high-stakes domain such as medicine, however, an in-depth
evaluation of a model's abilities is crucial. We address the issue of thorough
performance evaluation in English-language ADE detection with hand-crafted
templates for four capabilities: Temporal order, negation, sentiment, and
beneficial effect. We find that models with similar performance on held-out
test sets have varying results on these capabilities.","[{'name': 'Dorothea MacPhail'}, {'name': 'David Harbecke'}, {'name': 'Lisa Raithel'}, {'name': 'Sebastian Möller'}]",2024-07-02T17:09:24Z
http://arxiv.org/abs/2407.02408v1,http://arxiv.org/abs/2407.02408v1,"CEB: Compositional Evaluation Benchmark for Fairness in Large Language
  Models","As Large Language Models (LLMs) are increasingly deployed to handle various
natural language processing (NLP) tasks, concerns regarding the potential
negative societal impacts of LLM-generated content have also arisen. To
evaluate the biases exhibited by LLMs, researchers have recently proposed a
variety of datasets. However, existing bias evaluation efforts often focus on
only a particular type of bias and employ inconsistent evaluation metrics,
leading to difficulties in comparison across different datasets and LLMs. To
address these limitations, we collect a variety of datasets designed for the
bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation
Benchmark that covers different types of bias across different social groups
and tasks. The curation of CEB is based on our newly proposed compositional
taxonomy, which characterizes each dataset from three dimensions: bias types,
social groups, and tasks. By combining the three dimensions, we develop a
comprehensive evaluation strategy for the bias in LLMs. Our experiments
demonstrate that the levels of bias vary across these dimensions, thereby
providing guidance for the development of specific bias mitigation methods.","[{'name': 'Song Wang'}, {'name': 'Peng Wang'}, {'name': 'Tong Zhou'}, {'name': 'Yushun Dong'}, {'name': 'Zhen Tan'}, {'name': 'Jundong Li'}]",2024-07-02T16:31:37Z
http://arxiv.org/abs/2407.02397v1,http://arxiv.org/abs/2407.02397v1,Learning to Refine with Fine-Grained Natural Language Feedback,"Recent work has explored the capability of large language models (LLMs) to
identify and correct errors in LLM-generated responses. These refinement
approaches frequently evaluate what sizes of models are able to do refinement
for what problems, but less attention is paid to what effective feedback for
refinement looks like. In this work, we propose looking at refinement with
feedback as a composition of three distinct LLM competencies: (1)
identification of bad generations; (2) fine-grained natural language feedback
generation; (3) refining with fine-grained feedback. The first step can be
implemented with a high-performing discriminative model and steps 2 and 3 can
be implemented either via prompted or fine-tuned LLMs. A key property of this
approach is that the step 2 critique model can give fine-grained feedback about
errors, made possible by offloading the discrimination to a separate model in
step 1. We show that models of different capabilities benefit from refining
with this approach on the task of improving factual consistency of document
grounded summaries. Overall, our proposed method consistently outperforms
existing end-to-end refinement approaches and current trained models not
fine-tuned for factuality critiquing.","[{'name': 'Manya Wadhwa'}, {'name': 'Xinyu Zhao'}, {'name': 'Junyi Jessy Li'}, {'name': 'Greg Durrett'}]",2024-07-02T16:15:01Z
http://arxiv.org/abs/2407.13780v1,http://arxiv.org/abs/2407.13780v1,"Generative Model for Small Molecules with Latent Space RL Fine-Tuning to
  Protein Targets","A specific challenge with deep learning approaches for molecule generation is
generating both syntactically valid and chemically plausible molecular string
representations. To address this, we propose a novel generative latent-variable
transformer model for small molecules that leverages a recently proposed
molecular string representation called SAFE. We introduce a modification to
SAFE to reduce the number of invalid fragmented molecules generated during
training and use this to train our model. Our experiments show that our model
can generate novel molecules with a validity rate > 90% and a fragmentation
rate < 1% by sampling from a latent space. By fine-tuning the model using
reinforcement learning to improve molecular docking, we significantly increase
the number of hit candidates for five specific protein targets compared to the
pre-trained model, nearly doubling this number for certain targets.
Additionally, our top 5% mean docking scores are comparable to the current
state-of-the-art (SOTA), and we marginally outperform SOTA on three of the five
targets.","[{'name': 'Ulrich A. Mbou Sob'}, {'name': 'Qiulin Li'}, {'name': 'Miguel Arbesú'}, {'name': 'Oliver Bent'}, {'name': 'Andries P. Smit'}, {'name': 'Arnu Pretorius'}]",2024-07-02T16:01:37Z
http://arxiv.org/abs/2407.02354v1,http://arxiv.org/abs/2407.02354v1,Talking to Machines: do you read me?,"In this dissertation I would like to guide the reader to the research on
dialogue but more precisely the research I have conducted during my career
since my PhD thesis. Starting from modular architectures with machine
learning/deep learning and reinforcement learning to end-to-end deep neural
networks. Besides my work as research associate, I also present the work I have
supervised in the last years.
  I review briefly the state of the art and highlight the open research
problems on conversational agents. Afterwards, I present my contribution to
Task-Oriented Dialogues (TOD), both as research associate and as the industrial
supervisor of CIFRE theses. I discuss conversational QA. Particularly, I
present the work of two PhD candidates Thibault Cordier and Sebastien Montella;
as well as the work of the young researcher Quentin Brabant. Finally, I present
the scientific project, where I discuss about Large Language Models (LLMs) for
Task-Oriented Dialogue and Multimodal Task-Oriented Dialogue.",[{'name': 'Lina M. Rojas-Barahona'}],2024-07-02T15:19:46Z
http://arxiv.org/abs/2407.02345v1,http://arxiv.org/abs/2407.02345v1,"MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring
  and Utilizing Latent Space","Personalized Dialogue Generation (PDG) aims to create coherent responses
according to roles or personas. Traditional PDG relies on external role data,
which can be scarce and raise privacy concerns. Approaches address these issues
by extracting role information from dialogue history, which often fail to
generically model roles in continuous space. To overcome these limitations, we
introduce a novel framework \textbf{MO}dels \textbf{R}oles from
\textbf{P}ersonalized Dialogue \textbf{H}istory by \textbf{E}xploring and
\textbf{U}tilizing Latent \textbf{S}pace (MORPHEUS) through a three-stage
training process. Specifically, we create a persona codebook to represent roles
in latent space compactly, and this codebook is used to construct a posterior
distribution of role information. This method enables the model to generalize
across roles, allowing the generation of personalized dialogues even for unseen
roles. Experiments on both Chinese and English datasets demonstrate that
MORPHEUS enhances the extraction of role information, and improves response
generation without external role data. Additionally, MORPHEUS can be considered
an efficient fine-tuning for large language models.","[{'name': 'Yihong Tang'}, {'name': 'Bo Wang'}, {'name': 'Dongming Zhao'}, {'name': 'Xiaojia Jin'}, {'name': 'Jijun Zhang'}, {'name': 'Ruifang He'}, {'name': 'Yuexian Hou'}]",2024-07-02T15:12:34Z
http://arxiv.org/abs/2407.02340v1,http://arxiv.org/abs/2407.02340v1,RVISA: Reasoning and Verification for Implicit Sentiment Analysis,"With an increasing social demand for fine-grained sentiment analysis (SA),
implicit sentiment analysis (ISA) poses a significant challenge with the
absence of salient cue words in expressions. It necessitates reliable reasoning
to understand how the sentiment is aroused and thus determine implicit
sentiments. In the era of Large Language Models (LLMs), Encoder-Decoder (ED)
LLMs have gained popularity to serve as backbone models for SA applications,
considering impressive text comprehension and reasoning ability among diverse
tasks. On the other hand, Decoder-only (DO) LLMs exhibit superior natural
language generation and in-context learning capabilities. However, their
responses may contain misleading or inaccurate information. To identify
implicit sentiment with reliable reasoning, this study proposes RVISA, a
two-stage reasoning framework that harnesses the generation ability of DO LLMs
and the reasoning ability of ED LLMs to train an enhanced reasoner.
Specifically, we adopt three-hop reasoning prompting to explicitly furnish
sentiment elements as cues. The generated rationales are utilized to fine-tune
an ED LLM into a skilled reasoner. Additionally, we develop a straightforward
yet effective verification mechanism to ensure the reliability of the reasoning
learning. We evaluated the proposed method on two benchmark datasets and
achieved state-of-the-art results in ISA performance.","[{'name': 'Wenna Lai'}, {'name': 'Haoran Xie'}, {'name': 'Guandong Xu'}, {'name': 'Qing Li'}]",2024-07-02T15:07:54Z
http://arxiv.org/abs/2407.02328v1,http://arxiv.org/abs/2407.02328v1,Efficient Sparse Attention needs Adaptive Token Release,"In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities across a wide array of text-centric tasks. However, their `large'
scale introduces significant computational and storage challenges, particularly
in managing the key-value states of the transformer, which limits their wider
applicability. Therefore, we propose to adaptively release resources from
caches and rebuild the necessary key-value states. Particularly, we accomplish
this by a lightweight controller module to approximate an ideal top-$K$ sparse
attention. This module retains the tokens with the highest top-$K$ attention
weights and simultaneously rebuilds the discarded but necessary tokens, which
may become essential for future decoding. Comprehensive experiments in natural
language generation and modeling reveal that our method is not only competitive
with full attention in terms of performance but also achieves a significant
throughput improvement of up to 221.8%. The code for replication is available
on the https://github.com/WHUIR/ADORE.","[{'name': 'Chaoran Zhang'}, {'name': 'Lixin Zou'}, {'name': 'Dan Luo'}, {'name': 'Min Tang'}, {'name': 'Xiangyang Luo'}, {'name': 'Zihao Li'}, {'name': 'Chenliang Li'}]",2024-07-02T14:58:44Z
http://arxiv.org/abs/2407.02320v1,http://arxiv.org/abs/2407.02320v1,"Exploring the Role of Transliteration in In-Context Learning for
  Low-resource Languages Written in Non-Latin Scripts","Decoder-only large language models (LLMs) excel in high-resource languages
across various tasks through few-shot or even zero-shot in-context learning
(ICL). However, their performance often does not transfer well to low-resource
languages, especially those written in non-Latin scripts. Inspired by recent
work that leverages transliteration in encoder-only models, we investigate
whether transliteration is also effective in improving LLMs' performance for
low-resource languages written in non-Latin scripts. To this end, we propose
three prompt templates, where the target-language text is represented in (1)
its original script, (2) Latin script, or (3) both. We apply these methods to
several representative LLMs of different sizes on various tasks including text
classification and sequential labeling. Our findings show that the
effectiveness of transliteration varies by task type and model size. For
instance, all models benefit from transliterations for sequential labeling
(with increases of up to 25%).","[{'name': 'Chunlan Ma'}, {'name': 'Yihong Liu'}, {'name': 'Haotian Ye'}, {'name': 'Hinrich Schütze'}]",2024-07-02T14:51:20Z
http://arxiv.org/abs/2407.02302v1,http://arxiv.org/abs/2407.02302v1,Towards Human Understanding of Paraphrase Types in ChatGPT,"Paraphrases represent a human's intuitive ability to understand expressions
presented in various different ways. Current paraphrase evaluations of language
models primarily use binary approaches, offering limited interpretability of
specific text changes. Atomic paraphrase types (APT) decompose paraphrases into
different linguistic changes and offer a granular view of the flexibility in
linguistic expression (e.g., a shift in syntax or vocabulary used). In this
study, we assess the human preferences towards ChatGPT in generating English
paraphrases with ten APTs and five prompting techniques. We introduce APTY
(Atomic Paraphrase TYpes), a dataset of 500 sentence-level and word-level
annotations by 15 annotators. The dataset also provides a human preference
ranking of paraphrases with different types that can be used to fine-tune
models with RLHF and DPO methods. Our results reveal that ChatGPT can generate
simple APTs, such as additions and deletions, but struggle with complex
structures (e.g., subordination changes). This study contributes to
understanding which aspects of paraphrasing language models have already
succeeded at understanding and what remains elusive. In addition, our curated
datasets can be used to develop language models with specific linguistic
capabilities.","[{'name': 'Dominik Meier'}, {'name': 'Jan Philip Wahle'}, {'name': 'Terry Ruas'}, {'name': 'Bela Gipp'}]",2024-07-02T14:35:10Z
http://arxiv.org/abs/2407.02301v1,http://arxiv.org/abs/2407.02301v1,"CFinBench: A Comprehensive Chinese Financial Benchmark for Large
  Language Models","Large language models (LLMs) have achieved remarkable performance on various
NLP tasks, yet their potential in more challenging and domain-specific task,
such as finance, has not been fully explored. In this paper, we present
CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark
to date, for assessing the financial knowledge of LLMs under Chinese context.
In practice, to better align with the career trajectory of Chinese financial
practitioners, we build a systematic evaluation from 4 first-level categories:
(1) Financial Subject: whether LLMs can memorize the necessary basic knowledge
of financial subjects, such as economics, statistics and auditing. (2)
Financial Qualification: whether LLMs can obtain the needed financial qualified
certifications, such as certified public accountant, securities qualification
and banking qualification. (3) Financial Practice: whether LLMs can fulfill the
practical financial jobs, such as tax consultant, junior accountant and
securities analyst. (4) Financial Law: whether LLMs can meet the requirement of
financial laws and regulations, such as tax law, insurance law and economic
law. CFinBench comprises 99,100 questions spanning 43 second-level categories
with 3 question types: single-choice, multiple-choice and judgment. We conduct
extensive experiments of 50 representative LLMs with various model size on
CFinBench. The results show that GPT4 and some Chinese-oriented models lead the
benchmark, with the highest average accuracy being 60.16%, highlighting the
challenge presented by CFinBench. The dataset and evaluation code are available
at https://cfinbench.github.io/.","[{'name': 'Ying Nie'}, {'name': 'Binwei Yan'}, {'name': 'Tianyu Guo'}, {'name': 'Hao Liu'}, {'name': 'Haoyu Wang'}, {'name': 'Wei He'}, {'name': 'Binfan Zheng'}, {'name': 'Weihao Wang'}, {'name': 'Qiang Li'}, {'name': 'Weijian Sun'}, {'name': 'Yunhe Wang'}, {'name': 'Dacheng Tao'}]",2024-07-02T14:34:36Z
http://arxiv.org/abs/2407.12828v2,http://arxiv.org/abs/2407.12828v2,Why Does New Knowledge Create Messy Ripple Effects in LLMs?,"Extensive previous research has focused on post-training knowledge editing
(KE) for language models (LMs) to ensure that knowledge remains accurate and
up-to-date. One desired property and open question in KE is to let edited LMs
correctly handle ripple effects, where LM is expected to answer its logically
related knowledge accurately. In this paper, we answer the question of why most
KE methods still create messy ripple effects. We conduct extensive analysis and
identify a salient indicator, GradSim, that effectively reveals when and why
updated knowledge ripples in LMs. GradSim is computed by the cosine similarity
between gradients of the original fact and its related knowledge. We observe a
strong positive correlation between ripple effect performance and GradSim
across different LMs, KE methods, and evaluation metrics. Further
investigations into three counter-intuitive failure cases (Negation,
Over-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures
are often associated with very low GradSim. This finding validates that GradSim
is an effective indicator of when knowledge ripples in LMs.","[{'name': 'Jiaxin Qin'}, {'name': 'Zixuan Zhang'}, {'name': 'Chi Han'}, {'name': 'Manling Li'}, {'name': 'Pengfei Yu'}, {'name': 'Heng Ji'}]",2024-07-02T14:33:44Z
http://arxiv.org/abs/2407.12827v1,http://arxiv.org/abs/2407.12827v1,The Solution for The PST-KDD-2024 OAG-Challenge,"In this paper, we introduce the second-place solution in the KDD-2024
OAG-Challenge paper source tracing track. Our solution is mainly based on two
methods, BERT and GCN, and combines the reasoning results of BERT and GCN in
the final submission to achieve complementary performance. In the BERT
solution, we focus on processing the fragments that appear in the references of
the paper, and use a variety of operations to reduce the redundant interference
in the fragments, so that the information received by BERT is more refined. In
the GCN solution, we map information such as paper fragments, abstracts, and
titles to a high-dimensional semantic space through an embedding model, and try
to build edges between titles, abstracts, and fragments to integrate contextual
relationships for judgment. In the end, our solution achieved a remarkable
score of 0.47691 in the competition.","[{'name': 'Shupeng Zhong'}, {'name': 'Xinger Li'}, {'name': 'Shushan Jin'}, {'name': 'Yang Yang'}]",2024-07-02T14:15:05Z
http://arxiv.org/abs/2407.12825v1,http://arxiv.org/abs/2407.12825v1,"A Depression Detection Method Based on Multi-Modal Feature Fusion Using
  Cross-Attention","Depression, a prevalent and serious mental health issue, affects
approximately 3.8\% of the global population. Despite the existence of
effective treatments, over 75\% of individuals in low- and middle-income
countries remain untreated, partly due to the challenge in accurately
diagnosing depression in its early stages. This paper introduces a novel method
for detecting depression based on multi-modal feature fusion utilizing
cross-attention. By employing MacBERT as a pre-training model to extract
lexical features from text and incorporating an additional Transformer module
to refine task-specific contextual understanding, the model's adaptability to
the targeted task is enhanced. Diverging from previous practices of simply
concatenating multimodal features, this approach leverages cross-attention for
feature integration, significantly improving the accuracy in depression
detection and enabling a more comprehensive and precise analysis of user
emotions and behaviors. Furthermore, a Multi-Modal Feature Fusion Network based
on Cross-Attention (MFFNC) is constructed, demonstrating exceptional
performance in the task of depression identification. The experimental results
indicate that our method achieves an accuracy of 0.9495 on the test dataset,
marking a substantial improvement over existing approaches. Moreover, it
outlines a promising methodology for other social media platforms and tasks
involving multi-modal processing. Timely identification and intervention for
individuals with depression are crucial for saving lives, highlighting the
immense potential of technology in facilitating early intervention for mental
health issues.","[{'name': 'Shengjie Li'}, {'name': 'Yinhao Xiao'}]",2024-07-02T13:13:35Z
http://arxiv.org/abs/2407.02243v1,http://arxiv.org/abs/2407.02243v1,"Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference
  Optimization","In this paper, we propose reverse inference optimization (RIO), a simple and
effective method designed to enhance the robustness of
autoregressive-model-based zero-shot text-to-speech (TTS) systems using
reinforcement learning from human feedback (RLHF). To assess the quality of
speech produced by the TTS system without human annotations, RIO introduces a
novel concept termed as reverse inference based on the Bayesian principle,
which suggests that a high-quality generated speech should be able to be used
as a prompt for subsequent generation using the same TTS model. By leveraging
reverse inference as the standard to select exemplars used in RLHF from the
speech samples generated by the TTS system itself, RIO steers the subsequent
optimization towards a direction of enhancing the TTS robustness. The RIO
framework, comprising sampling, automatic annotating, and learning, obviates
the need for a reward model or pairwise preference data, and significantly
improves the stability of zero-shot TTS performance by reducing the
discrepancies between training and inference conditions. Our experimental
results verify that RIO can effectively improve both subjective and objective
metrics, including mean opinion scores, word error rates, and speaker
similarity. Remarkably, RIO can also diminish the incidence of bad outputs to
nearly zero percent, rivalling the robustness when using ground-truth speech as
the prompt.","[{'name': 'Yuchen Hu'}, {'name': 'Chen Chen'}, {'name': 'Siyin Wang'}, {'name': 'Eng Siong Chng'}, {'name': 'Chao Zhang'}]",2024-07-02T13:04:04Z
http://arxiv.org/abs/2407.12824v1,http://arxiv.org/abs/2407.12824v1,"Whispering Experts: Neural Interventions for Toxicity Mitigation in
  Language Models","An important issue with Large Language Models (LLMs) is their undesired
ability to generate toxic language. In this work, we show that the neurons
responsible for toxicity can be determined by their power to discriminate toxic
sentences, and that toxic language can be mitigated by reducing their
activation levels proportionally to this power. We propose AUROC adaptation
(AurA), an intervention that can be applied to any pre-trained LLM to mitigate
toxicity. As the intervention is proportional to the ability of each neuron to
discriminate toxic content, it is free of any model-dependent hyperparameters.
We show that AurA can achieve up to $2.2 \times$ reduction in toxicity with
only a $0.72$ perplexity increase. We also show that AurA is effective with
models of different scale (from 1.5B to 40B parameters), and its effectiveness
in mitigating toxic language, while preserving common-sense zero-shot
abilities, holds across all scales. AurA can be combined with pre-prompting
strategies, boosting its average mitigation potential from $1.28\times$ to
$2.35\times$. Moreover, AurA can counteract adversarial pre-prompts that
maliciously elicit toxic content, making it an effective method for deploying
safer and less toxic models.","[{'name': 'Xavier Suau'}, {'name': 'Pieter Delobelle'}, {'name': 'Katherine Metcalf'}, {'name': 'Armand Joulin'}, {'name': 'Nicholas Apostoloff'}, {'name': 'Luca Zappella'}, {'name': 'Pau Rodríguez'}]",2024-07-02T12:48:29Z
http://arxiv.org/abs/2407.02211v1,http://arxiv.org/abs/2407.02211v1,"PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt
  during Large Language Model Fine-tuning","Large language models (LLMs) have played a fundamental role in various
natural language processing tasks with powerful prompt techniques. However, in
real-world applications, there are often similar prompt components for repeated
queries, which causes significant computational burdens during inference.
Existing prompt compression and direct fine-tuning methods aim to tackle these
challenges, yet they frequently struggle to strike an optimal balance between
cost-efficiency and performance effectiveness, especially in complex tasks such
as NL2Code. In this paper, we propose a novel method namely PromptIntern to
internalize the prompt knowledge into model parameters via progressive
fine-tuning. Our method enables LLMs to emulate the human learning process for
a new task, where detailed templates and examples in a prompt are gradually
internalized and phased out progressively as the model grows accustomed to the
task. Extensive experiments demonstrate that our method reduces inference
tokens over 90%, speedups inference by 4.2 times, and saves 88.3% monetary
cost.","[{'name': 'Jiaru Zou'}, {'name': 'Mengyu Zhou'}, {'name': 'Tao Li'}, {'name': 'Shi Han'}, {'name': 'Dongmei Zhang'}]",2024-07-02T12:21:14Z
http://arxiv.org/abs/2407.02136v1,http://arxiv.org/abs/2407.02136v1,Black Big Boxes: Do Language Models Hide a Theory of Adjective Order?,"In English and other languages, multiple adjectives in a complex noun phrase
show intricate ordering patterns that have been a target of much linguistic
theory. These patterns offer an opportunity to assess the ability of language
models (LMs) to learn subtle rules of language involving factors that cross the
traditional divisions of syntax, semantics, and pragmatics. We review existing
hypotheses designed to explain Adjective Order Preferences (AOPs) in humans and
develop a setup to study AOPs in LMs: we present a reusable corpus of adjective
pairs and define AOP measures for LMs. With these tools, we study a series of
LMs across intermediate checkpoints during training. We find that all models'
predictions are much closer to human AOPs than predictions generated by factors
identified in theoretical linguistics. At the same time, we demonstrate that
the observed AOPs in LMs are strongly correlated with the frequency of the
adjective pairs in the training data and report limited generalization to
unseen combinations. This highlights the difficulty in establishing the link
between LM performance and linguistic theory. We therefore conclude with a road
map for future studies our results set the stage for, and a discussion of key
questions about the nature of knowledge in LMs and their ability to generalize
beyond the training sets.","[{'name': 'Jaap Jumelet'}, {'name': 'Lisa Bylinina'}, {'name': 'Willem Zuidema'}, {'name': 'Jakub Szymanik'}]",2024-07-02T10:29:09Z
http://arxiv.org/abs/2407.02122v1,http://arxiv.org/abs/2407.02122v1,Fake News Detection: It's All in the Data!,"This comprehensive survey serves as an indispensable resource for researchers
embarking on the journey of fake news detection. By highlighting the pivotal
role of dataset quality and diversity, it underscores the significance of these
elements in the effectiveness and robustness of detection models. The survey
meticulously outlines the key features of datasets, various labeling systems
employed, and prevalent biases that can impact model performance. Additionally,
it addresses critical ethical issues and best practices, offering a thorough
overview of the current state of available datasets. Our contribution to this
field is further enriched by the provision of GitHub repository, which
consolidates publicly accessible datasets into a single, user-friendly portal.
This repository is designed to facilitate and stimulate further research and
development efforts aimed at combating the pervasive issue of fake news.","[{'name': 'Soveatin Kuntur'}, {'name': 'Anna Wróblewska'}, {'name': 'Marcin Paprzycki'}, {'name': 'Maria Ganzha'}]",2024-07-02T10:12:06Z
http://arxiv.org/abs/2407.02119v2,http://arxiv.org/abs/2407.02119v2,"Cost-Effective Proxy Reward Model Construction with On-Policy and Active
  Learning","Reinforcement learning with human feedback (RLHF), as a widely adopted
approach in current large language model pipelines, is \textit{bottlenecked by
the size of human preference data}. While traditional methods rely on offline
preference dataset constructions, recent approaches have shifted towards online
settings, where a learner uses a small amount of labeled seed data and a large
pool of unlabeled prompts to iteratively construct new preference data through
self-generated responses and high-quality reward/preference feedback. However,
most current online algorithms still focus on preference labeling during policy
model updating with given feedback oracles, which incurs significant expert
query costs. \textit{We are the first to explore cost-effective proxy reward
oracles construction strategies for further labeling preferences or rewards
with extremely limited labeled data and expert query budgets}. Our approach
introduces two key innovations: (1) on-policy query to avoid OOD and imbalance
issues in seed data, and (2) active learning to select the most informative
data for preference queries. Using these methods, we train a evaluation model
with minimal expert-labeled data, which then effectively labels nine times more
preference pairs for further RLHF training. For instance, our model using
Direct Preference Optimization (DPO) gains around over 1% average improvement
on AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our
methodology is orthogonal to other direct expert query-based strategies and
therefore might be integrated with them to further reduce query costs.","[{'name': 'Yifang Chen'}, {'name': 'Shuohang Wang'}, {'name': 'Ziyi Yang'}, {'name': 'Hiteshi Sharma'}, {'name': 'Nikos Karampatziakis'}, {'name': 'Donghan Yu'}, {'name': 'Kevin Jamieson'}, {'name': 'Simon Shaolei Du'}, {'name': 'Yelong Shen'}]",2024-07-02T10:09:19Z
http://arxiv.org/abs/2407.02118v1,http://arxiv.org/abs/2407.02118v1,"Breaking Language Barriers: Cross-Lingual Continual Pre-Training at
  Scale","In recent years, Large Language Models (LLMs) have made significant strides
towards Artificial General Intelligence. However, training these models from
scratch requires substantial computational resources and vast amounts of text
data. In this paper, we explore an alternative approach to constructing an LLM
for a new language by continually pretraining (CPT) from existing pretrained
LLMs, instead of using randomly initialized parameters. Based on parallel
experiments on 40 model sizes ranging from 40M to 5B parameters, we find that
1) CPT converges faster and saves significant resources in a scalable manner;
2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022)
with a joint data-parameter scaling term; 3) The compute-optimal data-parameter
allocation for CPT markedly differs based on our estimated scaling factors; 4)
The effectiveness of transfer at scale is influenced by training duration and
linguistic properties, while robust to data replaying, a method that
effectively mitigates catastrophic forgetting in CPT. We hope our findings
provide deeper insights into the transferability of LLMs at scale for the
research community.","[{'name': 'Wenzhen Zheng'}, {'name': 'Wenbo Pan'}, {'name': 'Xu Xu'}, {'name': 'Libo Qin'}, {'name': 'Li Yue'}, {'name': 'Ming Zhou'}]",2024-07-02T10:06:41Z
http://arxiv.org/abs/2407.02099v1,http://arxiv.org/abs/2407.02099v1,"Helpful assistant or fruitful facilitator? Investigating how personas
  affect language model behavior","One way to personalize and steer generations from large language models (LLM)
is to assign a persona: a role that describes how the user expects the LLM to
behave (e.g., a helpful assistant, a teacher, a woman). This paper investigates
how personas affect diverse aspects of model behavior. We assign to seven LLMs
162 personas from 12 categories spanning variables like gender, sexual
orientation, and occupation. We prompt them to answer questions from five
datasets covering objective (e.g., questions about math and history) and
subjective tasks (e.g., questions about beliefs and values). We also compare
persona's generations to two baseline settings: a control persona setting with
30 paraphrases of ""a helpful assistant"" to control for models' prompt
sensitivity, and an empty persona setting where no persona is assigned. We find
that for all models and datasets, personas show greater variability than the
control setting and that some measures of persona behavior generalize across
models.","[{'name': 'Pedro Henrique Luz de Araujo'}, {'name': 'Benjamin Roth'}]",2024-07-02T09:36:54Z
http://arxiv.org/abs/2407.02067v1,http://arxiv.org/abs/2407.02067v1,"Crossroads of Continents: Automated Artifact Extraction for Cultural
  Adaptation with Large Multimodal Models","In this work, we present a comprehensive three-phase study to examine (1) the
effectiveness of large multimodal models (LMMs) in recognizing cultural
contexts; (2) the accuracy of their representations of diverse cultures; and
(3) their ability to adapt content across cultural boundaries. We first
introduce Dalle Street, a large-scale dataset generated by DALL-E 3 and
validated by humans, containing 9,935 images of 67 countries and 10 concept
classes. We reveal disparities in cultural understanding at the sub-region
level with both open-weight (LLaVA) and closed-source (GPT-4V) models on Dalle
Street and other existing benchmarks. Next, we assess models' deeper culture
understanding by an artifact extraction task and identify over 18,000 artifacts
associated with different countries. Finally, we propose a highly composable
pipeline, CultureAdapt, to adapt images from culture to culture. Our findings
reveal a nuanced picture of the cultural competence of LMMs, highlighting the
need to develop culture-aware systems. Dataset and code are available at
https://github.com/iamshnoo/crossroads","[{'name': 'Anjishnu Mukherjee'}, {'name': 'Ziwei Zhu'}, {'name': 'Antonios Anastasopoulos'}]",2024-07-02T08:55:41Z
http://arxiv.org/abs/2407.02043v1,http://arxiv.org/abs/2407.02043v1,Concise and Precise Context Compression for Tool-Using Language Models,"Through reading the documentation in the context, tool-using language models
can dynamically extend their capability using external tools. The cost is that
we have to input lengthy documentation every time the model needs to use the
tool, occupying the input window as well as slowing down the decoding process.
  Given the progress in general-purpose compression, soft context compression
is a suitable approach to alleviate the problem. However, when compressing tool
documentation, existing methods suffer from the weaknesses of key information
loss (specifically, tool/parameter name errors) and difficulty in adjusting the
length of compressed sequences based on documentation lengths.
  To address these problems, we propose two strategies for compressing tool
documentation into concise and precise summary sequences for tool-using
language models. 1) Selective compression strategy mitigates key information
loss by deliberately retaining key information as raw text tokens. 2) Block
compression strategy involves dividing tool documentation into short chunks and
then employing a fixed-length compression model to achieve variable-length
compression. This strategy facilitates the flexible adjustment of the
compression ratio.
  Results on API-Bank and APIBench show that our approach reaches a performance
comparable to the upper-bound baseline under up to 16x compression ratio.","[{'name': 'Yang Xu'}, {'name': 'Yunlong Feng'}, {'name': 'Honglin Mu'}, {'name': 'Yutai Hou'}, {'name': 'Yitong Li'}, {'name': 'Xinghao Wang'}, {'name': 'Wanjun Zhong'}, {'name': 'Zhongyang Li'}, {'name': 'Dandan Tu'}, {'name': 'Qingfu Zhu'}, {'name': 'Min Zhang'}, {'name': 'Wanxiang Che'}]",2024-07-02T08:17:00Z
http://arxiv.org/abs/2407.02042v1,http://arxiv.org/abs/2407.02042v1,"Fake News Detection and Manipulation Reasoning via Large Vision-Language
  Models","Fake news becomes a growing threat to information security and public opinion
with the rapid sprawl of media manipulation. Therefore, fake news detection
attracts widespread attention from academic community. Traditional fake news
detection models demonstrate remarkable performance on authenticity binary
classification but their ability to reason detailed faked traces based on the
news content remains under-explored. Furthermore, due to the lack of external
knowledge, the performance of existing methods on fact-related news is
questionable, leaving their practical implementation unclear. In this paper, we
propose a new multi-media research topic, namely manipulation reasoning.
Manipulation reasoning aims to reason manipulations based on news content. To
support the research, we introduce a benchmark for fake news detection and
manipulation reasoning, referred to as Human-centric and Fact-related Fake News
(HFFN). The benchmark highlights the centrality of human and the high factual
relevance, with detailed manual annotations. HFFN encompasses four realistic
domains with fake news samples generated through three manipulation approaches.
Moreover, a Multi-modal news Detection and Reasoning langUage Model (M-DRUM) is
presented not only to judge on the authenticity of multi-modal news, but also
raise analytical reasoning about potential manipulations. On the feature
extraction level, a cross-attention mechanism is employed to extract
fine-grained fusion features from multi-modal inputs. On the reasoning level, a
large vision-language model (LVLM) serves as the backbone to facilitate
fact-related reasoning. A two-stage training framework is deployed to better
activate the capacity of identification and reasoning. Comprehensive
experiments demonstrate that our model outperforms state-of-the-art (SOTA) fake
news detection models and powerful LVLMs like GPT-4 and LLaVA.","[{'name': 'Ruihan Jin'}, {'name': 'Ruibo Fu'}, {'name': 'Zhengqi Wen'}, {'name': 'Shuai Zhang'}, {'name': 'Yukun Liu'}, {'name': 'Jianhua Tao'}]",2024-07-02T08:16:43Z
http://arxiv.org/abs/2407.02039v1,http://arxiv.org/abs/2407.02039v1,Prompt Stability Scoring for Text Annotation with Large Language Models,"Researchers are increasingly using language models (LMs) for text annotation.
These approaches rely only on a prompt telling the model to return a given
output according to a set of instructions. The reproducibility of LM outputs
may nonetheless be vulnerable to small changes in the prompt design. This calls
into question the replicability of classification routines. To tackle this
problem, researchers have typically tested a variety of semantically similar
prompts to determine what we call ""prompt stability."" These approaches remain
ad-hoc and task specific. In this article, we propose a general framework for
diagnosing prompt stability by adapting traditional approaches to intra- and
inter-coder reliability scoring. We call the resulting metric the Prompt
Stability Score (PSS) and provide a Python package PromptStability for its
estimation. Using six different datasets and twelve outcomes, we classify >150k
rows of data to: a) diagnose when prompt stability is low; and b) demonstrate
the functionality of the package. We conclude by providing best practice
recommendations for applied researchers.","[{'name': 'Christopher Barrie'}, {'name': 'Elli Palaiologou'}, {'name': 'Petter Törnberg'}]",2024-07-02T08:11:18Z
http://arxiv.org/abs/2407.17482v1,http://arxiv.org/abs/2407.17482v1,"Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,
  Whose Perspectives?","We argue for the epistemic and ethical advantages of pluralism in
Reinforcement Learning from Human Feedback (RLHF) in the context of Large
Language Models (LLM). Drawing on social epistemology and pluralist philosophy
of science, we suggest ways in which RHLF can be made more responsive to human
needs and how we can address challenges along the way. The paper concludes with
an agenda for change, i.e. concrete, actionable steps to improve LLM
development.","[{'name': 'Kristian González Barman'}, {'name': 'Simon Lohse'}, {'name': 'Henk de Regt'}]",2024-07-02T08:07:27Z
http://arxiv.org/abs/2407.02030v1,http://arxiv.org/abs/2407.02030v1,"Breaking Bias, Building Bridges: Evaluation and Mitigation of Social
  Biases in LLMs via Contact Hypothesis","Large Language Models (LLMs) perpetuate social biases, reflecting prejudices
in their training data and reinforcing societal stereotypes and inequalities.
Our work explores the potential of the Contact Hypothesis, a concept from
social psychology for debiasing LLMs. We simulate various forms of social
contact through LLM prompting to measure their influence on the model's biases,
mirroring how intergroup interactions can reduce prejudices in social contexts.
We create a dataset of 108,000 prompts following a principled approach
replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and
NousHermes) across 13 social bias dimensions. We propose a unique debiasing
technique, Social Contact Debiasing (SCD), that instruction-tunes these models
with unbiased responses to prompts. Our research demonstrates that LLM
responses exhibit social biases when subject to contact probing, but more
importantly, these biases can be significantly reduced by up to 40% in 1 epoch
of instruction tuning LLaMA 2 following our SCD strategy. Our code and data are
available at https://github.com/chahatraj/breakingbias.","[{'name': 'Chahat Raj'}, {'name': 'Anjishnu Mukherjee'}, {'name': 'Aylin Caliskan'}, {'name': 'Antonios Anastasopoulos'}, {'name': 'Ziwei Zhu'}]",2024-07-02T07:58:46Z
http://arxiv.org/abs/2407.02028v1,http://arxiv.org/abs/2407.02028v1,"Why does in-context learning fail sometimes? Evaluating in-context
  learning on open and closed questions","We measure the performance of in-context learning as a function of task
novelty and difficulty for open and closed questions. For that purpose, we
created a novel benchmark consisting of hard scientific questions, each paired
with a context of various relevancy. We show that counter-intuitively, a
context that is more aligned with the topic does not always help more than a
less relevant context. This effect is especially visible for open questions and
questions of high difficulty or novelty. This result reveals a fundamental
difference between the treatment of close-form and open-form questions by
large-language models and shows a need for a more robust evaluation of
in-context learning on the variety of different types of questions. It also
poses a new question of how to optimally select a context for large language
models, especially in the context of Retrieval Augmented Generation (RAG)
systems. Our results suggest that the answer to this question can be highly
application-dependent and might be contingent on factors including the format
of the question, the perceived difficulty level of the questions, and the
novelty or popularity of the information we seek.","[{'name': 'Xiang Li'}, {'name': 'Haoran Tang'}, {'name': 'Siyu Chen'}, {'name': 'Ziwei Wang'}, {'name': 'Ryan Chen'}, {'name': 'Marcin Abram'}]",2024-07-02T07:52:30Z
http://arxiv.org/abs/2407.12043v1,http://arxiv.org/abs/2407.12043v1,The Art of Saying No: Contextual Noncompliance in Language Models,"Chat-based language models are designed to be helpful, yet they should not
comply with every user request. While most existing work primarily focuses on
refusal of ""unsafe"" queries, we posit that the scope of noncompliance should be
broadened. We introduce a comprehensive taxonomy of contextual noncompliance
describing when and how models should not comply with user requests. Our
taxonomy spans a wide range of categories including incomplete, unsupported,
indeterminate, and humanizing requests (in addition to unsafe requests). To
test noncompliance capabilities of language models, we use this taxonomy to
develop a new evaluation suite of 1000 noncompliance prompts. We find that most
existing models show significantly high compliance rates in certain previously
understudied categories with models like GPT-4 incorrectly complying with as
many as 30% of requests. To address these gaps, we explore different training
strategies using a synthetically-generated training set of requests and
expected noncompliant responses. Our experiments demonstrate that while direct
finetuning of instruction-tuned models can lead to both over-refusal and a
decline in general capabilities, using parameter efficient methods like low
rank adapters helps to strike a good balance between appropriate noncompliance
and other capabilities.","[{'name': 'Faeze Brahman'}, {'name': 'Sachin Kumar'}, {'name': 'Vidhisha Balachandran'}, {'name': 'Pradeep Dasigi'}, {'name': 'Valentina Pyatkin'}, {'name': 'Abhilasha Ravichander'}, {'name': 'Sarah Wiegreffe'}, {'name': 'Nouha Dziri'}, {'name': 'Khyathi Chandu'}, {'name': 'Jack Hessel'}, {'name': 'Yulia Tsvetkov'}, {'name': 'Noah A. Smith'}, {'name': 'Yejin Choi'}, {'name': 'Hannaneh Hajishirzi'}]",2024-07-02T07:12:51Z
http://arxiv.org/abs/2407.01994v1,http://arxiv.org/abs/2407.01994v1,"Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph
  Completion","High-quality and high-coverage rule sets are imperative to the success of
Neuro-Symbolic Knowledge Graph Completion (NS-KGC) models, because they form
the basis of all symbolic inferences. Recent literature builds neural models
for generating rule sets, however, preliminary experiments show that they
struggle with maintaining high coverage. In this work, we suggest three simple
augmentations to existing rule sets: (1) transforming rules to their abductive
forms, (2) generating equivalent rules that use inverse forms of constituent
relations and (3) random walks that propose new rules. Finally, we prune
potentially low quality rules. Experiments over four datasets and five
ruleset-baseline settings suggest that these simple augmentations consistently
improve results, and obtain up to 7.1 pt MRR and 8.5 pt Hits@1 gains over using
rules without augmentations.","[{'name': 'Ananjan Nandi'}, {'name': 'Navdeep Kaur'}, {'name': 'Parag Singla'}, {'name': 'Mausam'}]",2024-07-02T07:07:59Z
