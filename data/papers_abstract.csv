id,link,title,abstract,authors,published
http://arxiv.org/abs/2408.10188v1,http://arxiv.org/abs/2408.10188v1,LongVILA: Scaling Long-Context Visual Language Models for Long Videos,"Long-context capability is critical for multi-modal foundation models. We
introduce LongVILA, a full-stack solution for long-context vision-language
models, including system, model training, and dataset development. On the
system side, we introduce the first Multi-Modal Sequence Parallelism (MM-SP)
system that enables long-context training and inference, enabling 2M context
length training on 256 GPUs. MM-SP is also efficient, being 2.1x - 5.7x faster
than Ring-Style Sequence Parallelism and 1.1x - 1.4x faster than Megatron-LM in
text-only settings. Moreover, it seamlessly integrates with Hugging Face
Transformers. For model training, we propose a five-stage pipeline comprising
alignment, pre-training, context extension, and long-short joint supervised
fine-tuning. Regarding datasets, we meticulously construct large-scale visual
language pre-training datasets and long video instruction-following datasets to
support our multi-stage training process. The full-stack solution extends the
feasible frame number of VILA by a factor of 128 (from 8 to 1024 frames) and
improves long video captioning score from 2.00 to 3.26 (1.6x), achieving 99.5%
accuracy in 1400-frames video (274k context length) needle in a haystack.
LongVILA-8B also demonstrates a consistent improvement in performance on long
videos within the VideoMME benchmark as the video frames increase.","[{'name': 'Fuzhao Xue'}, {'name': 'Yukang Chen'}, {'name': 'Dacheng Li'}, {'name': 'Qinghao Hu'}, {'name': 'Ligeng Zhu'}, {'name': 'Xiuyu Li'}, {'name': 'Yunhao Fang'}, {'name': 'Haotian Tang'}, {'name': 'Shang Yang'}, {'name': 'Zhijian Liu'}, {'name': 'Ethan He'}, {'name': 'Hongxu Yin'}, {'name': 'Pavlo Molchanov'}, {'name': 'Jan Kautz'}, {'name': 'Linxi Fan'}, {'name': 'Yuke Zhu'}, {'name': 'Yao Lu'}, {'name': 'Song Han'}]",2024-08-19T17:48:08Z
http://arxiv.org/abs/2408.10151v1,http://arxiv.org/abs/2408.10151v1,"Multilingual Needle in a Haystack: Investigating Long-Context Behavior
  of Multilingual Large Language Models","While recent large language models (LLMs) demonstrate remarkable abilities in
responding to queries in diverse languages, their ability to handle long
multilingual contexts is unexplored. As such, a systematic evaluation of the
long-context capabilities of LLMs in multilingual settings is crucial,
specifically in the context of information retrieval. To address this gap, we
introduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to
assess a model's ability to retrieve relevant information (the needle) from a
collection of multilingual distractor texts (the haystack). This test serves as
an extension of the multilingual question-answering task, encompassing both
monolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs
on MLNeedle. Our findings reveal that model performance can vary significantly
with language and needle position. Specifically, we observe that model
performance is the lowest when the needle is (i) in a language outside the
English language family and (ii) located in the middle of the input context.
Furthermore, although some models claim a context size of $8k$ tokens or
greater, none demonstrate satisfactory cross-lingual retrieval performance as
the context length increases. Our analysis provides key insights into the
long-context behavior of LLMs in multilingual settings to guide future
evaluation protocols. To our knowledge, this is the first study to investigate
the multilingual long-context behavior of LLMs.","[{'name': 'Amey Hengle'}, {'name': 'Prasoon Bajpai'}, {'name': 'Soham Dan'}, {'name': 'Tanmoy Chakraborty'}]",2024-08-19T17:02:06Z
http://arxiv.org/abs/2408.10147v1,http://arxiv.org/abs/2408.10147v1,"In-Context Learning with Representations: Contextual Generalization of
  Trained Transformers","In-context learning (ICL) refers to a remarkable capability of pretrained
large language models, which can learn a new task given a few examples during
inference. However, theoretical understanding of ICL is largely under-explored,
particularly whether transformers can be trained to generalize to unseen
examples in a prompt, which will require the model to acquire contextual
knowledge of the prompt for generalization. This paper investigates the
training dynamics of transformers by gradient descent through the lens of
non-linear regression tasks. The contextual generalization here can be attained
via learning the template function for each task in-context, where all template
functions lie in a linear space with $m$ basis functions. We analyze the
training dynamics of one-layer multi-head transformers to in-contextly predict
unlabeled inputs given partially labeled prompts, where the labels contain
Gaussian noise and the number of examples in each prompt are not sufficient to
determine the template. Under mild assumptions, we show that the training loss
for a one-layer multi-head transformer converges linearly to a global minimum.
Moreover, the transformer effectively learns to perform ridge regression over
the basis functions. To our knowledge, this study is the first provable
demonstration that transformers can learn contextual (i.e., template)
information to generalize to both unseen examples and tasks when prompts
contain only a small number of query-answer pairs.","[{'name': 'Tong Yang'}, {'name': 'Yu Huang'}, {'name': 'Yingbin Liang'}, {'name': 'Yuejie Chi'}]",2024-08-19T16:47:46Z
http://arxiv.org/abs/2408.10141v1,http://arxiv.org/abs/2408.10141v1,"Instruction Finetuning for Leaderboard Generation from Empirical AI
  Research","This study demonstrates the application of instruction finetuning of
pretrained Large Language Models (LLMs) to automate the generation of AI
research leaderboards, extracting (Task, Dataset, Metric, Score) quadruples
from articles. It aims to streamline the dissemination of advancements in AI
research by transitioning from traditional, manual community curation, or
otherwise taxonomy-constrained natural language inference (NLI) models, to an
automated, generative LLM-based approach. Utilizing the FLAN-T5 model, this
research enhances LLMs' adaptability and reliability in information extraction,
offering a novel method for structured knowledge representation.","[{'name': 'Salomon Kabongo'}, {'name': ""Jennifer D'Souza""}]",2024-08-19T16:41:07Z
http://arxiv.org/abs/2408.10130v1,http://arxiv.org/abs/2408.10130v1,Rhyme-aware Chinese lyric generator based on GPT,"Neural language representation models such as GPT, pre-trained on large-scale
corpora, can effectively capture rich semantic patterns from plain text and be
fine-tuned to consistently improve natural language generation performance.
However, existing pre-trained language models used to generate lyrics rarely
consider rhyme information, which is crucial in lyrics. Using a pre-trained
model directly results in poor performance. To enhance the rhyming quality of
generated lyrics, we incorporate integrated rhyme information into our model,
thereby improving lyric generation performance.","[{'name': 'Yixiao Yuan'}, {'name': 'Yangchen Huang'}, {'name': 'Yu Ma'}, {'name': 'Xinjin Li'}, {'name': 'Zhenglin Li'}, {'name': 'Yiming Shi'}, {'name': 'Huapeng Zhou'}]",2024-08-19T16:17:20Z
http://arxiv.org/abs/2408.10115v1,http://arxiv.org/abs/2408.10115v1,"GLIMMER: Incorporating Graph and Lexical Features in Unsupervised
  Multi-Document Summarization","Pre-trained language models are increasingly being used in multi-document
summarization tasks. However, these models need large-scale corpora for
pre-training and are domain-dependent. Other non-neural unsupervised
summarization approaches mostly rely on key sentence extraction, which can lead
to information loss. To address these challenges, we propose a lightweight yet
effective unsupervised approach called GLIMMER: a Graph and LexIcal features
based unsupervised Multi-docuMEnt summaRization approach. It first constructs a
sentence graph from the source documents, then automatically identifies
semantic clusters by mining low-level features from raw texts, thereby
improving intra-cluster correlation and the fluency of generated sentences.
Finally, it summarizes clusters into natural sentences. Experiments conducted
on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach
outperforms existing unsupervised approaches. Furthermore, it surpasses
state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS
and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally,
human evaluations indicate that summaries generated by GLIMMER achieve high
readability and informativeness scores. Our code is available at
https://github.com/Oswald1997/GLIMMER.","[{'name': 'Ran Liu'}, {'name': 'Ming Liu'}, {'name': 'Min Yu'}, {'name': 'Jianguo Jiang'}, {'name': 'Gang Li'}, {'name': 'Dan Zhang'}, {'name': 'Jingyuan Li'}, {'name': 'Xiang Meng'}, {'name': 'Weiqing Huang'}]",2024-08-19T16:01:48Z
http://arxiv.org/abs/2408.10075v1,http://arxiv.org/abs/2408.10075v1,"Personalizing Reinforcement Learning from Human Feedback with
  Variational Preference Learning","Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for
aligning foundation models to human values and preferences. However, current
RLHF techniques cannot account for the naturally occurring differences in
individual human preferences across a diverse population. When these
differences arise, traditional RLHF frameworks simply average over them,
leading to inaccurate rewards and poor performance for individual subgroups. To
address the need for pluralistic alignment, we develop a class of multimodal
RLHF methods. Our proposed techniques are based on a latent variable
formulation - inferring a novel user-specific latent and learning reward models
and policies conditioned on this latent without additional user-specific data.
While conceptually simple, we show that in practice, this reward modeling
requires careful algorithmic considerations around model architecture and
reward scaling. To empirically validate our proposed technique, we first show
that it can provide a way to combat underspecification in simulated control
problems, inferring and optimizing user-specific reward functions. Next, we
conduct experiments on pluralistic language datasets representing diverse user
preferences and demonstrate improved reward function accuracy. We additionally
show the benefits of this probabilistic framework in terms of measuring
uncertainty, and actively learning user preferences. This work enables learning
from diverse populations of users with divergent preferences, an important
challenge that naturally occurs in problems from robot learning to foundation
model alignment.","[{'name': 'Sriyash Poddar'}, {'name': 'Yanming Wan'}, {'name': 'Hamish Ivison'}, {'name': 'Abhishek Gupta'}, {'name': 'Natasha Jaques'}]",2024-08-19T15:18:30Z
http://arxiv.org/abs/2408.10053v1,http://arxiv.org/abs/2408.10053v1,"Privacy Checklist: Privacy Violation Detection Grounding on Contextual
  Integrity Theory","Privacy research has attracted wide attention as individuals worry that their
private data can be easily leaked during interactions with smart devices,
social platforms, and AI applications. Computer science researchers, on the
other hand, commonly study privacy issues through privacy attacks and defenses
on segmented fields. Privacy research is conducted on various sub-fields,
including Computer Vision (CV), Natural Language Processing (NLP), and Computer
Networks. Within each field, privacy has its own formulation. Though pioneering
works on attacks and defenses reveal sensitive privacy issues, they are
narrowly trapped and cannot fully cover people's actual privacy concerns.
Consequently, the research on general and human-centric privacy research
remains rather unexplored. In this paper, we formulate the privacy issue as a
reasoning problem rather than simple pattern matching. We ground on the
Contextual Integrity (CI) theory which posits that people's perceptions of
privacy are highly correlated with the corresponding social context. Based on
such an assumption, we develop the first comprehensive checklist that covers
social identities, private attributes, and existing privacy regulations. Unlike
prior works on CI that either cover limited expert annotated norms or model
incomplete social context, our proposed privacy checklist uses the whole Health
Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to
show that we can resort to large language models (LLMs) to completely cover the
HIPAA's regulations. Additionally, our checklist also gathers expert
annotations across multiple ontologies to determine private information
including but not limited to personally identifiable information (PII). We use
our preliminary results on the HIPAA to shed light on future context-centric
privacy research to cover more privacy regulations, social norms and standards.","[{'name': 'Haoran Li'}, {'name': 'Wei Fan'}, {'name': 'Yulin Chen'}, {'name': 'Jiayang Cheng'}, {'name': 'Tianshu Chu'}, {'name': 'Xuebing Zhou'}, {'name': 'Peizhao Hu'}, {'name': 'Yangqiu Song'}]",2024-08-19T14:48:04Z
http://arxiv.org/abs/2408.09949v1,http://arxiv.org/abs/2408.09949v1,"C${^2}$RL: Content and Context Representation Learning for Gloss-free
  Sign Language Translation and Retrieval","Sign Language Representation Learning (SLRL) is crucial for a range of sign
language-related downstream tasks such as Sign Language Translation (SLT) and
Sign Language Retrieval (SLRet). Recently, many gloss-based and gloss-free SLRL
methods have been proposed, showing promising performance. Among them, the
gloss-free approach shows promise for strong scalability without relying on
gloss annotations. However, it currently faces suboptimal solutions due to
challenges in encoding the intricate, context-sensitive characteristics of sign
language videos, mainly struggling to discern essential sign features using a
non-monotonic video-text alignment strategy. Therefore, we introduce an
innovative pretraining paradigm for gloss-free SLRL, called C${^2}$RL, in this
paper. Specifically, rather than merely incorporating a non-monotonic semantic
alignment of video and text to learn language-oriented sign features, we
emphasize two pivotal aspects of SLRL: Implicit Content Learning (ICL) and
Explicit Context Learning (ECL). ICL delves into the content of communication,
capturing the nuances, emphasis, timing, and rhythm of the signs. In contrast,
ECL focuses on understanding the contextual meaning of signs and converting
them into equivalent sentences. Despite its simplicity, extensive experiments
confirm that the joint optimization of ICL and ECL results in robust sign
language representation and significant performance gains in gloss-free SLT and
SLRet tasks. Notably, C${^2}$RL improves the BLEU-4 score by +5.3 on P14T,
+10.6 on CSL-daily, +6.2 on OpenASL, and +1.3 on How2Sign. It also boosts the
R@1 score by +8.3 on P14T, +14.4 on CSL-daily, and +5.9 on How2Sign.
Additionally, we set a new baseline for the OpenASL dataset in the SLRet task.","[{'name': 'Zhigang Chen'}, {'name': 'Benjia Zhou'}, {'name': 'Yiqing Huang'}, {'name': 'Jun Wan'}, {'name': 'Yibo Hu'}, {'name': 'Hailin Shi'}, {'name': 'Yanyan Liang'}, {'name': 'Zhen Lei'}, {'name': 'Du Zhang'}]",2024-08-19T12:42:10Z
http://arxiv.org/abs/2408.09946v1,http://arxiv.org/abs/2408.09946v1,Microscopic Analysis on LLM players via Social Deduction Game,"Recent studies have begun developing autonomous game players for social
deduction games using large language models (LLMs). When building LLM players,
fine-grained evaluations are crucial for addressing weaknesses in game-playing
abilities. However, existing studies have often overlooked such assessments.
Specifically, we point out two issues with the evaluation methods employed.
First, game-playing abilities have typically been assessed through game-level
outcomes rather than specific event-level skills; Second, error analyses have
lacked structured methodologies. To address these issues, we propose an
approach utilizing a variant of the SpyFall game, named SpyGame. We conducted
an experiment with four LLMs, analyzing their gameplay behavior in SpyGame both
quantitatively and qualitatively. For the quantitative analysis, we introduced
eight metrics to resolve the first issue, revealing that these metrics are more
effective than existing ones for evaluating the two critical skills: intent
identification and camouflage. In the qualitative analysis, we performed
thematic analysis to resolve the second issue. This analysis identifies four
major categories that affect gameplay of LLMs. Additionally, we demonstrate how
these categories complement and support the findings from the quantitative
analysis.","[{'name': 'Byungjun Kim'}, {'name': 'Dayeon Seo'}, {'name': 'Bugeun Kim'}]",2024-08-19T12:35:23Z
http://arxiv.org/abs/2408.09945v1,http://arxiv.org/abs/2408.09945v1,"Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating
  Adequacy, Fluency, and Elegance","Large language models (LLMs) have shown remarkable performance in general
translation tasks. However, the increasing demand for high-quality translations
that are not only adequate but also fluent and elegant. To assess the extent to
which current LLMs can meet these demands, we introduce a suitable benchmark
for translating classical Chinese poetry into English. This task requires not
only adequacy in translating culturally and historically significant content
but also a strict adherence to linguistic fluency and poetic elegance. Our
study reveals that existing LLMs fall short of this task. To address these
issues, we propose RAT, a \textbf{R}etrieval-\textbf{A}ugmented machine
\textbf{T}ranslation method that enhances the translation process by
incorporating knowledge related to classical poetry. Additionally, we propose
an automatic evaluation metric based on GPT-4, which better assesses
translation quality in terms of adequacy, fluency, and elegance, overcoming the
limitations of traditional metrics. Our dataset and code will be made
available.","[{'name': 'Andong Chen'}, {'name': 'Lianzhang Lou'}, {'name': 'Kehai Chen'}, {'name': 'Xuefeng Bai'}, {'name': 'Yang Xiang'}, {'name': 'Muyun Yang'}, {'name': 'Tiejun Zhao'}, {'name': 'Min Zhang'}]",2024-08-19T12:34:31Z
http://arxiv.org/abs/2408.09939v1,http://arxiv.org/abs/2408.09939v1,"""Image, Tell me your story!"" Predicting the original meta-context of
  visual misinformation","To assist human fact-checkers, researchers have developed automated
approaches for visual misinformation detection. These methods assign veracity
scores by identifying inconsistencies between the image and its caption, or by
detecting forgeries in the image. However, they neglect a crucial point of the
human fact-checking process: identifying the original meta-context of the
image. By explaining what is actually true about the image, fact-checkers can
better detect misinformation, focus their efforts on check-worthy visual
content, engage in counter-messaging before misinformation spreads widely, and
make their explanation more convincing. Here, we fill this gap by introducing
the task of automated image contextualization. We create 5Pils, a dataset of
1,676 fact-checked images with question-answer pairs about their original
meta-context. Annotations are based on the 5 Pillars fact-checking framework.
We implement a first baseline that grounds the image in its original
meta-context using the content of the image and textual evidence retrieved from
the open web. Our experiments show promising results while highlighting several
open challenges in retrieval and reasoning. We make our code and data publicly
available.","[{'name': 'Jonathan Tonglet'}, {'name': 'Marie-Francine Moens'}, {'name': 'Iryna Gurevych'}]",2024-08-19T12:21:34Z
http://arxiv.org/abs/2408.09916v1,http://arxiv.org/abs/2408.09916v1,"Attribution Analysis Meets Model Editing: Advancing Knowledge Correction
  in Vision Language Models with VisEdit","Model editing aims to correct outdated or erroneous knowledge in large models
without costly retraining. Recent research discovered that the mid-layer
representation of the subject's final token in a prompt has a strong influence
on factual predictions, and developed Large Language Model (LLM) editing
techniques based on this observation. However, for Vision-LLMs (VLLMs), how
visual representations impact the predictions from a decoder-only language
model remains largely unexplored. To the best of our knowledge, model editing
for VLLMs has not been extensively studied in the literature. In this work, we
employ the contribution allocation and noise perturbation methods to measure
the contributions of visual representations for token predictions. Our
attribution analysis shows that visual representations in mid-to-later layers
that are highly relevant to the prompt contribute significantly to predictions.
Based on these insights, we propose VisEdit, a novel model editor for VLLMs
that effectively corrects knowledge by editing intermediate visual
representations in regions important to the edit prompt. We evaluated VisEdit
using multiple VLLM backbones and public VLLM editing benchmark datasets. The
results show the superiority of VisEdit over the strong baselines adapted from
existing state-of-the-art editors for LLMs.","[{'name': 'Qizhou Chen'}, {'name': 'Taolin Zhang'}, {'name': 'Chengyu Wang'}, {'name': 'Xiaofeng He'}, {'name': 'Dakan Wang'}, {'name': 'Tingting Liu'}]",2024-08-19T11:44:40Z
http://arxiv.org/abs/2408.09914v1,http://arxiv.org/abs/2408.09914v1,"Active Learning for Identifying Disaster-Related Tweets: A Comparison
  with Keyword Filtering and Generic Fine-Tuning","Information from social media can provide essential information for emergency
response during natural disasters in near real-time. However, it is difficult
to identify the disaster-related posts among the large amounts of unstructured
data available. Previous methods often use keyword filtering, topic modelling
or classification-based techniques to identify such posts. Active Learning (AL)
presents a promising sub-field of Machine Learning (ML) that has not been used
much in the field of text classification of social media content. This study
therefore investigates the potential of AL for identifying disaster-related
Tweets. We compare a keyword filtering approach, a RoBERTa model fine-tuned
with generic data from CrisisLex, a base RoBERTa model trained with AL and a
fine-tuned RoBERTa model trained with AL regarding classification performance.
For testing, data from CrisisLex and manually labelled data from the 2021 flood
in Germany and the 2023 Chile forest fires were considered. The results show
that generic fine-tuning combined with 10 rounds of AL outperformed all other
approaches. Consequently, a broadly applicable model for the identification of
disaster-related Tweets could be trained with very little labelling effort. The
model can be applied to use cases beyond this study and provides a useful tool
for further research in social media analysis.","[{'name': 'David Hanny'}, {'name': 'Sebastian Schmidt'}, {'name': 'Bernd Resch'}]",2024-08-19T11:40:20Z
http://arxiv.org/abs/2408.09895v1,http://arxiv.org/abs/2408.09895v1,Performance Law of Large Language Models,"Guided by the belief of the scaling law, large language models (LLMs) have
achieved impressive performance in recent years. However, scaling law only
gives a qualitative estimation of loss, which is influenced by various factors
such as model architectures, data distributions, tokenizers, and computation
precision. Thus, estimating the real performance of LLMs with different
training settings rather than loss may be quite useful in practical
development. In this article, we present an empirical equation named
""Performance Law"" to directly predict the MMLU score of an LLM, which is a
widely used metric to indicate the general capability of LLMs in real-world
conversations and applications. Based on only a few key hyperparameters of the
LLM architecture and the size of training data, we obtain a quite accurate MMLU
prediction of various LLMs with diverse sizes and architectures developed by
different organizations in different years. Performance law can be used to
guide the choice of LLM architecture and the effective allocation of
computational resources without extensive experiments.","[{'name': 'Chuhan Wu'}, {'name': 'Ruiming Tang'}]",2024-08-19T11:09:12Z
http://arxiv.org/abs/2408.09869v1,http://arxiv.org/abs/2408.09869v1,Docling Technical Report,"This technical report introduces Docling, an easy to use, self-contained,
MIT-licensed open-source package for PDF document conversion. It is powered by
state-of-the-art specialized AI models for layout analysis (DocLayNet) and
table structure recognition (TableFormer), and runs efficiently on commodity
hardware in a small resource budget. The code interface allows for easy
extensibility and addition of new features and models.","[{'name': 'Christoph Auer'}, {'name': 'Maksym Lysak'}, {'name': 'Ahmed Nassar'}, {'name': 'Michele Dolfi'}, {'name': 'Nikolaos Livathinos'}, {'name': 'Panos Vagenas'}, {'name': 'Cesar Berrospi Ramis'}, {'name': 'Matteo Omenetti'}, {'name': 'Fabian Lindlbauer'}, {'name': 'Kasper Dinkla'}, {'name': 'Valery Weber'}, {'name': 'Lucas Morin'}, {'name': 'Ingmar Meijer'}, {'name': 'Viktor Kuropiatnyk'}, {'name': 'Peter W. J. Staar'}]",2024-08-19T10:20:06Z
http://arxiv.org/abs/2408.09865v1,http://arxiv.org/abs/2408.09865v1,"MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in
  Explainable Recommendation","Explainable Recommendation task is designed to receive a pair of user and
item and output explanations to justify why an item is recommended to a user.
Many models treat review-generation as a proxy of explainable recommendation.
Although they are able to generate fluent and grammatical sentences, they
suffer from generality and hallucination issues. We propose a personalized,
aspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), in which it
integrates aspect category as another input dimension to facilitate the
memorization of fine-grained aspect terms. Experiments on two real-world review
datasets in restaurant domain show that MAPLE outperforms the baseline
review-generation models in terms of text and feature diversity while
maintaining excellent coherence and factual relevance. We further treat MAPLE
as a retriever component in the retriever-reader framework and employ a
Large-Language Model (LLM) as the reader, showing that MAPLE's explanation
along with the LLM's comprehension ability leads to enriched and personalized
explanation as a result. We will release the code and data in this http upon
acceptance.","[{'name': 'Ching-Wen Yang'}, {'name': 'Che Wei Chen'}, {'name': 'Kun-da Wu'}, {'name': 'Hao Xu'}, {'name': 'Jui-Feng Yao'}, {'name': 'Hung-Yu Kao'}]",2024-08-19T10:12:52Z
http://arxiv.org/abs/2408.09857v1,http://arxiv.org/abs/2408.09857v1,"TaSL: Continual Dialog State Tracking via Task Skill Localization and
  Consolidation","A practical dialogue system requires the capacity for ongoing skill
acquisition and adaptability to new tasks while preserving prior knowledge.
However, current methods for Continual Dialogue State Tracking (DST), a crucial
function of dialogue systems, struggle with the catastrophic forgetting issue
and knowledge transfer between tasks. We present TaSL, a novel framework for
task skill localization and consolidation that enables effective knowledge
transfer without relying on memory replay. TaSL uses a novel group-wise
technique to pinpoint task-specific and task-shared areas. Additionally, a
fine-grained skill consolidation strategy protects task-specific knowledge from
being forgotten while updating shared knowledge for bi-directional knowledge
transfer. As a result, TaSL strikes a balance between preserving previous
knowledge and excelling at new tasks. Comprehensive experiments on various
backbones highlight the significant performance improvements of TaSL over
existing state-of-the-art methods. The source code is provided for
reproducibility.","[{'name': 'Yujie Feng'}, {'name': 'Xu Chu'}, {'name': 'Yongxin Xu'}, {'name': 'Guangyuan Shi'}, {'name': 'Bo Liu'}, {'name': 'Xiao-Ming Wu'}]",2024-08-19T10:01:28Z
http://arxiv.org/abs/2408.09856v1,http://arxiv.org/abs/2408.09856v1,"TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and
  Competition","While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have
effectively addressed GPU memory constraints during fine-tuning, their
performance often falls short, especially in multidimensional task scenarios.
To address this issue, one straightforward solution is to introduce
task-specific LoRA modules as domain experts, leveraging the modeling of
multiple experts' capabilities and thus enhancing the general capability of
multi-task learning. Despite promising, these additional components often add
complexity to the training and inference process, contravening the efficient
characterization of PEFT designed for. Considering this, we introduce an
innovative PEFT method, TeamLoRA, consisting of a collaboration and competition
module for experts, and thus achieving the right balance of effectiveness and
efficiency: (i) For collaboration, a novel knowledge-sharing and -organizing
mechanism is devised to appropriately reduce the scale of matrix operations,
thereby boosting the training and inference speed. (ii) For competition, we
propose leveraging a game-theoretic interaction mechanism for experts,
encouraging experts to transfer their domain-specific knowledge while facing
diverse downstream tasks, and thus enhancing the performance. By doing so,
TeamLoRA elegantly connects the experts as a ""Team"" with internal collaboration
and competition, enabling a faster and more accurate PEFT paradigm for
multi-task learning. To validate the superiority of TeamLoRA, we curate a
comprehensive multi-task evaluation(CME) benchmark to thoroughly assess the
capability of multi-task learning. Experiments conducted on our CME and other
benchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project
is available at https://github.com/Lin-Tianwei/TeamLoRA.","[{'name': 'Tianwei Lin'}, {'name': 'Jiang Liu'}, {'name': 'Wenqiao Zhang'}, {'name': 'Zhaocheng Li'}, {'name': 'Yang Dai'}, {'name': 'Haoyuan Li'}, {'name': 'Zhelun Yu'}, {'name': 'Wanggui He'}, {'name': 'Juncheng Li'}, {'name': 'Hao Jiang'}, {'name': 'Siliang Tang'}, {'name': 'Yueting Zhuang'}]",2024-08-19T09:58:53Z
http://arxiv.org/abs/2408.09853v1,http://arxiv.org/abs/2408.09853v1,Self-Directed Turing Test for Large Language Models,"The Turing test examines whether AIs can exhibit human-like behaviour in
natural language conversations. Traditional Turing tests adopt a rigid dialogue
format where each participant sends only one message each time and require
continuous human involvement to direct the entire interaction with the test
subject. This fails to reflect a natural conversational style and hinders the
evaluation of Large Language Models (LLMs) in complex and prolonged dialogues.
This paper proposes the Self-Directed Turing Test, which extends the original
test with a burst dialogue format, allowing more dynamic exchanges by multiple
consecutive messages. It further efficiently reduces human workload by having
the LLM self-direct the majority of the test process, iteratively generating
dialogues that simulate its interaction with humans. With the pseudo-dialogue
history, the model then engages in a shorter dialogue with a human, which is
paired with a human-human conversation on the same topic to be judged using
questionnaires. We introduce the X-Turn Pass-Rate metric to assess the human
likeness of LLMs across varying durations. While LLMs like GPT-4 initially
perform well, achieving pass rates of 51.9% and 38.9% during 3 turns and 10
turns of dialogues respectively, their performance drops as the dialogue
progresses, which underscores the difficulty in maintaining consistency in the
long term.","[{'name': 'Weiqi Wu'}, {'name': 'Hongqiu Wu'}, {'name': 'Hai Zhao'}]",2024-08-19T09:57:28Z
http://arxiv.org/abs/2408.09849v1,http://arxiv.org/abs/2408.09849v1,Importance Weighting Can Help Large Language Models Self-Improve,"Large language models (LLMs) have shown remarkable capability in numerous
tasks and applications. However, fine-tuning LLMs using high-quality datasets
under external supervision remains prohibitively expensive. In response, LLM
self-improvement approaches have been vibrantly developed recently. The typical
paradigm of LLM self-improvement involves training LLM on self-generated data,
part of which may be detrimental and should be filtered out due to the unstable
data quality. While current works primarily employs filtering strategies based
on answer correctness, in this paper, we demonstrate that filtering out correct
but with high distribution shift extent (DSE) samples could also benefit the
results of self-improvement. Given that the actual sample distribution is
usually inaccessible, we propose a new metric called DS weight to approximate
DSE, inspired by the Importance Weighting methods. Consequently, we integrate
DS weight with self-consistency to comprehensively filter the self-generated
samples and fine-tune the language model. Experiments show that with only a
tiny valid set (up to 5\% size of the training set) to compute DS weight, our
approach can notably promote the reasoning ability of current LLM
self-improvement methods. The resulting performance is on par with methods that
rely on external supervision from pre-trained reward models.","[{'name': 'Chunyang Jiang'}, {'name': 'Chi-min Chan'}, {'name': 'Wei Xue'}, {'name': 'Qifeng Liu'}, {'name': 'Yike Guo'}]",2024-08-19T09:51:02Z
http://arxiv.org/abs/2408.09846v1,http://arxiv.org/abs/2408.09846v1,Continual Dialogue State Tracking via Reason-of-Select Distillation,"An ideal dialogue system requires continuous skill acquisition and adaptation
to new tasks while retaining prior knowledge. Dialogue State Tracking (DST),
vital in these systems, often involves learning new services and confronting
catastrophic forgetting, along with a critical capability loss termed the
""Value Selection Quandary."" To address these challenges, we introduce the
Reason-of-Select (RoS) distillation method by enhancing smaller models with a
novel 'meta-reasoning' capability. Meta-reasoning employs an enhanced
multi-domain perspective, combining fragments of meta-knowledge from
domain-specific dialogues during continual learning. This transcends
traditional single-perspective reasoning. The domain bootstrapping process
enhances the model's ability to dissect intricate dialogues from multiple
possible values. Its domain-agnostic property aligns data distribution across
different domains, effectively mitigating forgetting. Additionally, two novel
improvements, ""multi-value resolution"" strategy and Semantic Contrastive
Reasoning Selection method, significantly enhance RoS by generating
DST-specific selection chains and mitigating hallucinations in teachers'
reasoning, ensuring effective and reliable knowledge transfer. Extensive
experiments validate the exceptional performance and robust generalization
capabilities of our method. The source code is provided for reproducibility.","[{'name': 'Yujie Feng'}, {'name': 'Bo Liu'}, {'name': 'Xiaoyu Dong'}, {'name': 'Zexin Lu'}, {'name': 'Li-Ming Zhan'}, {'name': 'Xiao-Ming Wu'}, {'name': 'Albert Y. S. Lam'}]",2024-08-19T09:48:50Z
http://arxiv.org/abs/2408.09819v1,http://arxiv.org/abs/2408.09819v1,"CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language
  Models","What a large language model (LLM) would respond in ethically relevant
context? In this paper, we curate a large benchmark CMoralEval for morality
evaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a
Chinese TV program discussing Chinese moral norms with stories from the society
and 2) a collection of Chinese moral anomies from various newspapers and
academic papers on morality. With these sources, we aim to create a moral
evaluation dataset characterized by diversity and authenticity. We develop a
morality taxonomy and a set of fundamental moral principles that are not only
rooted in traditional Chinese culture but also consistent with contemporary
societal norms. To facilitate efficient construction and annotation of
instances in CMoralEval, we establish a platform with AI-assisted instance
generation to streamline the annotation process. These help us curate
CMoralEval that encompasses both explicit moral scenarios (14,964 instances)
and moral dilemma scenarios (15,424 instances), each with instances from
different data sources. We conduct extensive experiments with CMoralEval to
examine a variety of Chinese LLMs. Experiment results demonstrate that
CMoralEval is a challenging benchmark for Chinese LLMs. The dataset is publicly
available at \url{https://github.com/tjunlp-lab/CMoralEval}.","[{'name': 'Linhao Yu'}, {'name': 'Yongqi Leng'}, {'name': 'Yufei Huang'}, {'name': 'Shang Wu'}, {'name': 'Haixin Liu'}, {'name': 'Xinmeng Ji'}, {'name': 'Jiahui Zhao'}, {'name': 'Jinwang Song'}, {'name': 'Tingting Cui'}, {'name': 'Xiaoqing Cheng'}, {'name': 'Tao Liu'}, {'name': 'Deyi Xiong'}]",2024-08-19T09:15:35Z
http://arxiv.org/abs/2408.09794v1,http://arxiv.org/abs/2408.09794v1,AutoML-guided Fusion of Entity and LLM-based representations,"Large semantic knowledge bases are grounded in factual knowledge. However,
recent approaches to dense text representations (embeddings) do not efficiently
exploit these resources. Dense and robust representations of documents are
essential for effectively solving downstream classification and retrieval
tasks. This work demonstrates that injecting embedded information from
knowledge bases can augment the performance of contemporary Large Language
Model (LLM)-based representations for the task of text classification. Further,
by considering automated machine learning (AutoML) with the fused
representation space, we demonstrate it is possible to improve classification
accuracy even if we use low-dimensional projections of the original
representation space obtained via efficient matrix factorization. This result
shows that significantly faster classifiers can be achieved with minimal or no
loss in predictive performance, as demonstrated using five strong LLM baselines
on six diverse real-life datasets.","[{'name': 'Boshko Koloski'}, {'name': 'Senja Pollak'}, {'name': 'Roberto Navigli'}, {'name': 'Blaž Škrlj'}]",2024-08-19T08:41:40Z
http://arxiv.org/abs/2408.09787v1,http://arxiv.org/abs/2408.09787v1,"Anim-Director: A Large Multimodal Model Powered Agent for Controllable
  Animation Video Generation","Traditional animation generation methods depend on training generative models
with human-labelled data, entailing a sophisticated multi-stage pipeline that
demands substantial human effort and incurs high training costs. Due to limited
prompting plans, these methods typically produce brief, information-poor, and
context-incoherent animations. To overcome these limitations and automate the
animation process, we pioneer the introduction of large multimodal models
(LMMs) as the core processor to build an autonomous animation-making agent,
named Anim-Director. This agent mainly harnesses the advanced understanding and
reasoning capabilities of LMMs and generative AI tools to create animated
videos from concise narratives or simple instructions. Specifically, it
operates in three main stages: Firstly, the Anim-Director generates a coherent
storyline from user inputs, followed by a detailed director's script that
encompasses settings of character profiles and interior/exterior descriptions,
and context-coherent scene descriptions that include appearing characters,
interiors or exteriors, and scene events. Secondly, we employ LMMs with the
image generation tool to produce visual images of settings and scenes. These
images are designed to maintain visual consistency across different scenes
using a visual-language prompting method that combines scene descriptions and
images of the appearing character and setting. Thirdly, scene images serve as
the foundation for producing animated videos, with LMMs generating prompts to
guide this process. The whole process is notably autonomous without manual
intervention, as the LMMs interact seamlessly with generative tools to generate
prompts, evaluate visual quality, and select the best one to optimize the final
output.","[{'name': 'Yunxin Li'}, {'name': 'Haoyuan Shi'}, {'name': 'Baotian Hu'}, {'name': 'Longyue Wang'}, {'name': 'Jiashun Zhu'}, {'name': 'Jinyi Xu'}, {'name': 'Zhen Zhao'}, {'name': 'Min Zhang'}]",2024-08-19T08:27:31Z
http://arxiv.org/abs/2408.09785v1,http://arxiv.org/abs/2408.09785v1,"GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining
  Automotive Software Release Decision-Making","Traditional methods for making software deployment decisions in the
automotive industry typically rely on manual analysis of tabular software test
data. These methods often lead to higher costs and delays in the software
release cycle due to their labor-intensive nature. Large Language Models (LLMs)
present a promising solution to these challenges. However, their application
generally demands multiple rounds of human-driven prompt engineering, which
limits their practical deployment, particularly for industrial end-users who
need reliable and efficient results. In this paper, we propose GoNoGo, an LLM
agent system designed to streamline automotive software deployment while
meeting both functional requirements and practical industrial constraints.
Unlike previous systems, GoNoGo is specifically tailored to address
domain-specific and risk-sensitive systems. We evaluate GoNoGo's performance
across different task difficulties using zero-shot and few-shot examples taken
from industrial practice. Our results show that GoNoGo achieves a 100% success
rate for tasks up to Level 2 difficulty with 3-shot examples, and maintains
high performance even for more complex tasks. We find that GoNoGo effectively
automates decision-making for simpler tasks, significantly reducing the need
for manual intervention. In summary, GoNoGo represents an efficient and
user-friendly LLM-based solution currently employed in our industrial partner's
company to assist with software release decision-making, supporting more
informed and timely decisions in the release process for risk-sensitive vehicle
systems.","[{'name': 'Arsham Gholamzadeh Khoee'}, {'name': 'Yinan Yu'}, {'name': 'Robert Feldt'}, {'name': 'Andris Freimanis'}, {'name': 'Patrick Andersson'}, {'name': 'Dhasarathy Parthasarathy'}]",2024-08-19T08:22:20Z
http://arxiv.org/abs/2408.09777v1,http://arxiv.org/abs/2408.09777v1,Summarizing long regulatory documents with a multi-step pipeline,"Due to their length and complexity, long regulatory texts are challenging to
summarize. To address this, a multi-step extractive-abstractive architecture is
proposed to handle lengthy regulatory documents more effectively. In this
paper, we show that the effectiveness of a two-step architecture for
summarizing long regulatory texts varies significantly depending on the model
used. Specifically, the two-step architecture improves the performance of
decoder-only models. For abstractive encoder-decoder models with short context
lengths, the effectiveness of an extractive step varies, whereas for
long-context encoder-decoder models, the extractive step worsens their
performance. This research also highlights the challenges of evaluating
generated texts, as evidenced by the differing results from human and automated
evaluations. Most notably, human evaluations favoured language models
pretrained on legal text, while automated metrics rank general-purpose language
models higher. The results underscore the importance of selecting the
appropriate summarization strategy based on model architecture and context
length.","[{'name': 'Mika Sie'}, {'name': 'Ruby Beek'}, {'name': 'Michiel Bots'}, {'name': 'Sjaak Brinkkemper'}, {'name': 'Albert Gatt'}]",2024-08-19T08:07:25Z
http://arxiv.org/abs/2408.09773v1,http://arxiv.org/abs/2408.09773v1,"Are Large Language Models More Honest in Their Probabilistic or
  Verbalized Confidence?","Large language models (LLMs) have been found to produce hallucinations when
the question exceeds their internal knowledge boundaries. A reliable model
should have a clear perception of its knowledge boundaries, providing correct
answers within its scope and refusing to answer when it lacks knowledge.
Existing research on LLMs' perception of their knowledge boundaries typically
uses either the probability of the generated tokens or the verbalized
confidence as the model's confidence in its response. However, these studies
overlook the differences and connections between the two. In this paper, we
conduct a comprehensive analysis and comparison of LLMs' probabilistic
perception and verbalized perception of their factual knowledge boundaries.
First, we investigate the pros and cons of these two perceptions. Then, we
study how they change under questions of varying frequencies. Finally, we
measure the correlation between LLMs' probabilistic confidence and verbalized
confidence. Experimental results show that 1) LLMs' probabilistic perception is
generally more accurate than verbalized perception but requires an in-domain
validation set to adjust the confidence threshold. 2) Both perceptions perform
better on less frequent questions. 3) It is challenging for LLMs to accurately
express their internal confidence in natural language.","[{'name': 'Shiyu Ni'}, {'name': 'Keping Bi'}, {'name': 'Lulu Yu'}, {'name': 'Jiafeng Guo'}]",2024-08-19T08:01:11Z
http://arxiv.org/abs/2408.09757v1,http://arxiv.org/abs/2408.09757v1,"Strategic Demonstration Selection for Improved Fairness in LLM
  In-Context Learning","Recent studies highlight the effectiveness of using in-context learning (ICL)
to steer large language models (LLMs) in processing tabular data, a challenging
task given the structured nature of such data. Despite advancements in
performance, the fairness implications of these methods are less understood.
This study investigates how varying demonstrations within ICL prompts influence
the fairness outcomes of LLMs. Our findings reveal that deliberately including
minority group samples in prompts significantly boosts fairness without
sacrificing predictive accuracy. Further experiments demonstrate that the
proportion of minority to majority samples in demonstrations affects the
trade-off between fairness and prediction accuracy. Based on these insights, we
introduce a mitigation technique that employs clustering and evolutionary
strategies to curate a diverse and representative sample set from the training
data. This approach aims to enhance both predictive performance and fairness in
ICL applications. Experimental results validate that our proposed method
dramatically improves fairness across various metrics, showing its efficacy in
real-world scenarios.","[{'name': 'Jingyu Hu'}, {'name': 'Weiru Liu'}, {'name': 'Mengnan Du'}]",2024-08-19T07:34:43Z
http://arxiv.org/abs/2408.09743v1,http://arxiv.org/abs/2408.09743v1,"R2GenCSR: Retrieving Context Samples for Large Language Model based
  X-ray Medical Report Generation","Inspired by the tremendous success of Large Language Models (LLMs), existing
X-ray medical report generation methods attempt to leverage large models to
achieve better performance. They usually adopt a Transformer to extract the
visual features of a given X-ray image, and then, feed them into the LLM for
text generation. How to extract more effective information for the LLMs to help
them improve final results is an urgent problem that needs to be solved.
Additionally, the use of visual Transformer models also brings high
computational complexity. To address these issues, this paper proposes a novel
context-guided efficient X-ray medical report generation framework.
Specifically, we introduce the Mamba as the vision backbone with linear
complexity, and the performance obtained is comparable to that of the strong
Transformer model. More importantly, we perform context retrieval from the
training set for samples within each mini-batch during the training phase,
utilizing both positively and negatively related samples to enhance feature
representation and discriminative learning. Subsequently, we feed the vision
tokens, context information, and prompt statements to invoke the LLM for
generating high-quality medical reports. Extensive experiments on three X-ray
report generation datasets (i.e., IU-Xray, MIMIC-CXR, CheXpert Plus) fully
validated the effectiveness of our proposed model. The source code of this work
will be released on \url{https://github.com/Event-AHU/Medical_Image_Analysis}.","[{'name': 'Xiao Wang'}, {'name': 'Yuehang Li'}, {'name': 'Fuling Wang'}, {'name': 'Shiao Wang'}, {'name': 'Chuanfu Li'}, {'name': 'Bo Jiang'}]",2024-08-19T07:15:11Z
http://arxiv.org/abs/2408.09742v1,http://arxiv.org/abs/2408.09742v1,"Paired Completion: Flexible Quantification of Issue-framing at Scale
  with LLMs","Detecting and quantifying issue framing in textual discourse - the
perspective one takes to a given topic (e.g. climate science vs. denialism,
misogyny vs. gender equality) - is highly valuable to a range of end-users from
social and political scientists to program evaluators and policy analysts.
However, conceptual framing is notoriously challenging for automated natural
language processing (NLP) methods since the words and phrases used by either
`side' of an issue are often held in common, with only subtle stylistic
flourishes separating their use. Here we develop and rigorously evaluate new
detection methods for issue framing and narrative analysis within large text
datasets. By introducing a novel application of next-token log probabilities
derived from generative large language models (LLMs) we show that issue framing
can be reliably and efficiently detected in large corpora with only a few
examples of either perspective on a given issue, a method we call `paired
completion'. Through 192 independent experiments over three novel, synthetic
datasets, we evaluate paired completion against prompt-based LLM methods and
labelled methods using traditional NLP and recent LLM contextual embeddings. We
additionally conduct a cost-based analysis to mark out the feasible set of
performant methods at production-level scales, and a model bias analysis.
Together, our work demonstrates a feasible path to scalable, accurate and
low-bias issue-framing in large corpora.","[{'name': 'Simon D Angus'}, {'name': ""Lachlan O'Neill""}]",2024-08-19T07:14:15Z
http://arxiv.org/abs/2408.09720v1,http://arxiv.org/abs/2408.09720v1,"Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large
  Language Model Augmented Framework","Pedestrian Attribute Recognition (PAR) is one of the indispensable tasks in
human-centered research. However, existing datasets neglect different domains
(e.g., environments, times, populations, and data sources), only conducting
simple random splits, and the performance of these datasets has already
approached saturation. In the past five years, no large-scale dataset has been
opened to the public. To address this issue, this paper proposes a new
large-scale, cross-domain pedestrian attribute recognition dataset to fill the
data gap, termed MSP60K. It consists of 60,122 images and 57 attribute
annotations across eight scenarios. Synthetic degradation is also conducted to
further narrow the gap between the dataset and real-world challenging
scenarios. To establish a more rigorous benchmark, we evaluate 17
representative PAR models under both random and cross-domain split protocols on
our dataset. Additionally, we propose an innovative Large Language Model (LLM)
augmented PAR framework, named LLM-PAR. This framework processes pedestrian
images through a Vision Transformer (ViT) backbone to extract features and
introduces a multi-embedding query Transformer to learn partial-aware features
for attribute classification. Significantly, we enhance this framework with LLM
for ensemble learning and visual feature augmentation. Comprehensive
experiments across multiple PAR benchmark datasets have thoroughly validated
the efficacy of our proposed framework. The dataset and source code
accompanying this paper will be made publicly available at
\url{https://github.com/Event-AHU/OpenPAR}.","[{'name': 'Jiandong Jin'}, {'name': 'Xiao Wang'}, {'name': 'Qian Zhu'}, {'name': 'Haiyang Wang'}, {'name': 'Chenglong Li'}]",2024-08-19T06:19:31Z
http://arxiv.org/abs/2408.09717v1,http://arxiv.org/abs/2408.09717v1,"SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction
  with Legal Clue Tracing","Legal Judgment Prediction (LJP) aims to form legal judgments based on the
criminal fact description. However, researchers struggle to classify confusing
criminal cases, such as robbery and theft, which requires LJP models to
distinguish the nuances between similar crimes. Existing methods usually design
handcrafted features to pick up necessary semantic legal clues to make more
accurate legal judgment predictions. In this paper, we propose a Semantic-Aware
Dual Encoder Model (SEMDR), which designs a novel legal clue tracing mechanism
to conduct fine-grained semantic reasoning between criminal facts and
instruments. Our legal clue tracing mechanism is built from three reasoning
levels: 1) Lexicon-Tracing, which aims to extract criminal facts from criminal
descriptions; 2) Sentence Representation Learning, which contrastively trains
language models to better represent confusing criminal facts; 3) Multi-Fact
Reasoning, which builds a reasons graph to propagate semantic clues among fact
nodes to capture the subtle difference among criminal facts. Our legal clue
tracing mechanism helps SEMDR achieve state-of-the-art on the CAIL2018 dataset
and shows its advance in few-shot scenarios. Our experiments show that SEMDR
has a strong ability to learn more uniform and distinguished representations
for criminal facts, which helps to make more accurate predictions on confusing
criminal cases and reduces the model uncertainty during making judgments. All
codes will be released via GitHub.","[{'name': 'Pengjie Liu'}, {'name': 'Wang Zhang'}, {'name': 'Yulong Ding'}, {'name': 'Xuefeng Zhang'}, {'name': 'Shuang-Hua Yang'}]",2024-08-19T06:13:19Z
http://arxiv.org/abs/2408.09701v1,http://arxiv.org/abs/2408.09701v1,"Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code
  Generation in LLMs via Zero-Shot Cross-Lingual Transfer","The use of Large Language Models (LLMs) for program code generation has
gained substantial attention, but their biases and limitations with non-English
prompts challenge global inclusivity. This paper investigates the complexities
of multilingual prompt-based code generation. Our evaluations of LLMs,
including CodeLLaMa and CodeGemma, reveal significant disparities in code
quality for non-English prompts; we also demonstrate the inadequacy of simple
approaches like prompt translation, bootstrapped data augmentation, and
fine-tuning. To address this, we propose a zero-shot cross-lingual approach
using a neural projection technique, integrating a cross-lingual encoder like
LASER artetxe2019massively to map multilingual embeddings from it into the
LLM's token space. This method requires training only on English data and
scales effectively to other languages. Results on a translated and
quality-checked MBPP dataset show substantial improvements in code quality.
This research promotes a more inclusive code generation landscape by empowering
LLMs with multilingual capabilities to support the diverse linguistic spectrum
in programming.","[{'name': 'Mingda Li'}, {'name': 'Abhijit Mishra'}, {'name': 'Utkarsh Mujumdar'}]",2024-08-19T05:11:46Z
http://arxiv.org/abs/2408.09688v1,http://arxiv.org/abs/2408.09688v1,"Recording for Eyes, Not Echoing to Ears: Contextualized
  Spoken-to-Written Conversion of ASR Transcripts","Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and
various spoken language phenomena such as disfluencies, ungrammatical
sentences, and incomplete sentences, hence suffering from poor readability. To
improve readability, we propose a Contextualized Spoken-to-Written conversion
(CoS2W) task to address ASR and grammar errors and also transfer the informal
text into the formal style with content preserved, utilizing contexts and
auxiliary information. This task naturally matches the in-context learning
capabilities of Large Language Models (LLMs). To facilitate comprehensive
comparisons of various LLMs, we construct a document-level Spoken-to-Written
conversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study
the impact of different granularity levels on the CoS2W performance, and
propose methods to exploit contexts and auxiliary information to enhance the
outputs. Experimental results reveal that LLMs have the potential to excel in
the CoS2W task, particularly in grammaticality and formality, our methods
achieve effective understanding of contexts and auxiliary information by LLMs.
We further investigate the effectiveness of using LLMs as evaluators and find
that LLM evaluators show strong correlations with human evaluations on rankings
of faithfulness and formality, which validates the reliability of LLM
evaluators for the CoS2W task.","[{'name': 'Jiaqing Liu'}, {'name': 'Chong Deng'}, {'name': 'Qinglin Zhang'}, {'name': 'Qian Chen'}, {'name': 'Hai Yu'}, {'name': 'Wen Wang'}]",2024-08-19T03:53:48Z
http://arxiv.org/abs/2408.09667v1,http://arxiv.org/abs/2408.09667v1,BLADE: Benchmarking Language Model Agents for Data-Driven Science,"Data-driven scientific discovery requires the iterative integration of
scientific domain knowledge, statistical expertise, and an understanding of
data semantics to make nuanced analytical decisions, e.g., about which
variables, transformations, and statistical models to consider. LM-based agents
equipped with planning, memory, and code execution capabilities have the
potential to support data-driven science. However, evaluating agents on such
open-ended tasks is challenging due to multiple valid approaches, partially
correct steps, and different ways to express the same decisions. To address
these challenges, we present BLADE, a benchmark to automatically evaluate
agents' multifaceted approaches to open-ended research questions. BLADE
consists of 12 datasets and research questions drawn from existing scientific
literature, with ground truth collected from independent analyses by expert
data scientists and researchers. To automatically evaluate agent responses, we
developed corresponding computational methods to match different
representations of analyses to this ground truth. Though language models
possess considerable world knowledge, our evaluation shows that they are often
limited to basic analyses. However, agents capable of interacting with the
underlying data demonstrate improved, but still non-optimal, diversity in their
analytical decision making. Our work enables the evaluation of agents for
data-driven science and provides researchers deeper insights into agents'
analysis approaches.","[{'name': 'Ken Gu'}, {'name': 'Ruoxi Shang'}, {'name': 'Ruien Jiang'}, {'name': 'Keying Kuang'}, {'name': 'Richard-John Lin'}, {'name': 'Donghe Lyu'}, {'name': 'Yue Mao'}, {'name': 'Youran Pan'}, {'name': 'Teng Wu'}, {'name': 'Jiaqian Yu'}, {'name': 'Yikun Zhang'}, {'name': 'Tianmai M. Zhang'}, {'name': 'Lanyi Zhu'}, {'name': 'Mike A. Merrill'}, {'name': 'Jeffrey Heer'}, {'name': 'Tim Althoff'}]",2024-08-19T02:59:35Z
http://arxiv.org/abs/2408.09656v1,http://arxiv.org/abs/2408.09656v1,"A Comparison of Large Language Model and Human Performance on Random
  Number Generation Tasks","Random Number Generation Tasks (RNGTs) are used in psychology for examining
how humans generate sequences devoid of predictable patterns. By adapting an
existing human RNGT for an LLM-compatible environment, this preliminary study
tests whether ChatGPT-3.5, a large language model (LLM) trained on
human-generated text, exhibits human-like cognitive biases when generating
random number sequences. Initial findings indicate that ChatGPT-3.5 more
effectively avoids repetitive and sequential patterns compared to humans, with
notably lower repeat frequencies and adjacent number frequencies. Continued
research into different models, parameters, and prompting methodologies will
deepen our understanding of how LLMs can more closely mimic human random
generation behaviors, while also broadening their applications in cognitive and
behavioral science research.",[{'name': 'Rachel M. Harrison'}],2024-08-19T02:34:15Z
http://arxiv.org/abs/2408.09640v1,http://arxiv.org/abs/2408.09640v1,Acquiring Bidirectionality via Large and Small Language Models,"Using token representation from bidirectional language models (LMs) such as
BERT is still a widely used approach for token-classification tasks. Even
though there exist much larger unidirectional LMs such as Llama-2, they are
rarely used to replace the token representation of bidirectional LMs. In this
work, we hypothesize that their lack of bidirectionality is keeping them
behind. To that end, we propose to newly train a small backward LM and
concatenate its representations to those of existing LM for downstream tasks.
Through experiments in named entity recognition, we demonstrate that
introducing backward model improves the benchmark performance more than 10
points. Furthermore, we show that the proposed method is especially effective
for rare domains and in few-shot learning settings.","[{'name': 'Takumi Goto'}, {'name': 'Hiroyoshi Nagao'}, {'name': 'Yuta Koreeda'}]",2024-08-19T01:54:37Z
http://arxiv.org/abs/2408.09639v1,http://arxiv.org/abs/2408.09639v1,"How to Make the Most of LLMs' Grammatical Knowledge for Acceptability
  Judgments","The grammatical knowledge of language models (LMs) is often measured using a
benchmark of linguistic minimal pairs, where LMs are presented with a pair of
acceptable and unacceptable sentences and required to judge which is
acceptable. The existing dominant approach, however, naively calculates and
compares the probabilities of paired sentences using LMs. Additionally, large
language models (LLMs) have yet to be thoroughly examined in this field. We
thus investigate how to make the most of LLMs' grammatical knowledge to
comprehensively evaluate it. Through extensive experiments of nine judgment
methods in English and Chinese, we demonstrate that a probability readout
method, in-template LP, and a prompting-based method, Yes/No probability
computing, achieve particularly high performance, surpassing the conventional
approach. Our analysis reveals their different strengths, e.g., Yes/No
probability computing is robust against token-length bias, suggesting that they
harness different aspects of LLMs' grammatical knowledge. Consequently, we
recommend using diverse judgment methods to evaluate LLMs comprehensively.","[{'name': 'Yusuke Ide'}, {'name': 'Yuto Nishida'}, {'name': 'Miyu Oba'}, {'name': 'Yusuke Sakai'}, {'name': 'Justin Vasselli'}, {'name': 'Hidetaka Kamigaito'}, {'name': 'Taro Watanabe'}]",2024-08-19T01:53:47Z
http://arxiv.org/abs/2408.09632v1,http://arxiv.org/abs/2408.09632v1,MoDeGPT: Modular Decomposition for Large Language Model Compression,"Large Language Models (LLMs) have reshaped the landscape of artificial
intelligence by demonstrating exceptional performance across various tasks.
However, substantial computational requirements make their deployment
challenging on devices with limited resources. Recently, compression methods
using low-rank matrix techniques have shown promise, yet these often lead to
degraded accuracy or introduce significant overhead in parameters and inference
latency. This paper introduces \textbf{Mo}dular \textbf{De}composition
(MoDeGPT), a novel structured compression framework that does not need recovery
fine-tuning while resolving the above drawbacks. MoDeGPT partitions the
Transformer block into modules comprised of matrix pairs and reduces the hidden
dimensions via reconstructing the module-level outputs. MoDeGPT is developed
based on a theoretical framework that utilizes three well-established matrix
decomposition algorithms -- Nystr\""om approximation, CR decomposition, and SVD
-- and applies them to our redefined transformer modules. Our comprehensive
experiments show MoDeGPT, without backward propagation, matches or surpasses
previous structured compression methods that rely on gradient information, and
saves 98% of compute costs on compressing a 13B model. On \textsc{Llama}-2/3
and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%
compression rates. Moreover, the compression can be done on a single GPU within
a few hours and increases the inference throughput by up to 46%.","[{'name': 'Chi-Heng Lin'}, {'name': 'Shangqian Gao'}, {'name': 'James Seale Smith'}, {'name': 'Abhishek Patel'}, {'name': 'Shikhar Tuli'}, {'name': 'Yilin Shen'}, {'name': 'Hongxia Jin'}, {'name': 'Yen-Chang Hsu'}]",2024-08-19T01:30:14Z
http://arxiv.org/abs/2408.09629v1,http://arxiv.org/abs/2408.09629v1,"A Strategy to Combine 1stGen Transformers and Open LLMs for Automatic
  Text Classification","Transformer models have achieved state-of-the-art results, with Large
Language Models (LLMs), an evolution of first-generation transformers (1stTR),
being considered the cutting edge in several NLP tasks. However, the literature
has yet to conclusively demonstrate that LLMs consistently outperform 1stTRs
across all NLP tasks. This study compares three 1stTRs (BERT, RoBERTa, and
BART) with two open LLMs (Llama 2 and Bloom) across 11 sentiment analysis
datasets. The results indicate that open LLMs may moderately outperform or
match 1stTRs in 8 out of 11 datasets but only when fine-tuned. Given this
substantial cost for only moderate gains, the practical applicability of these
models in cost-sensitive scenarios is questionable. In this context, a
confidence-based strategy that seamlessly integrates 1stTRs with open LLMs
based on prediction certainty is proposed. High-confidence documents are
classified by the more cost-effective 1stTRs, while uncertain cases are handled
by LLMs in zero-shot or few-shot modes, at a much lower cost than fine-tuned
versions. Experiments in sentiment analysis demonstrate that our solution not
only outperforms 1stTRs, zero-shot, and few-shot LLMs but also competes closely
with fine-tuned LLMs at a fraction of the cost.","[{'name': 'Claudio M. V. de Andrade'}, {'name': 'Washington Cunha'}, {'name': 'Davi Reis'}, {'name': 'Adriana Silvina Pagano'}, {'name': 'Leonardo Rocha'}, {'name': 'Marcos André Gonçalves'}]",2024-08-19T01:22:21Z
http://arxiv.org/abs/2408.09621v1,http://arxiv.org/abs/2408.09621v1,"Refining Packing and Shuffling Strategies for Enhanced Performance in
  Generative Language Models","Packing and shuffling tokens is a common practice in training auto-regressive
language models (LMs) to prevent overfitting and improve efficiency. Typically
documents are concatenated to chunks of maximum sequence length (MSL) and then
shuffled. However setting the atom size, the length for each data chunk
accompanied by random shuffling, to MSL may lead to contextual incoherence due
to tokens from different documents being packed into the same chunk. An
alternative approach is to utilize padding, another common data packing
strategy, to avoid contextual incoherence by only including one document in
each shuffled chunk. To optimize both packing strategies (concatenation vs
padding), we investigated the optimal atom size for shuffling and compared
their performance and efficiency. We found that matching atom size to MSL
optimizes performance for both packing methods (concatenation and padding), and
padding yields lower final perplexity (higher performance) than concatenation
at the cost of more training steps and lower compute efficiency. This trade-off
informs the choice of packing methods in training language models.","[{'name': 'Yanbing Chen'}, {'name': 'Ruilin Wang'}, {'name': 'Zihao Yang'}, {'name': 'Lavender Yao Jiang'}, {'name': 'Eric Karl Oermann'}]",2024-08-19T00:26:53Z
http://arxiv.org/abs/2408.09574v1,http://arxiv.org/abs/2408.09574v1,PhysBERT: A Text Embedding Model for Physics Scientific Literature,"The specialized language and complex concepts in physics pose significant
challenges for information extraction through Natural Language Processing
(NLP). Central to effective NLP applications is the text embedding model, which
converts text into dense vector representations for efficient information
retrieval and semantic analysis. In this work, we introduce PhysBERT, the first
physics-specific text embedding model. Pre-trained on a curated corpus of 1.2
million arXiv physics papers and fine-tuned with supervised data, PhysBERT
outperforms leading general-purpose models on physics-specific tasks including
the effectiveness in fine-tuning for specific physics subdomains.","[{'name': 'Thorsten Hellert'}, {'name': 'João Montenegro'}, {'name': 'Andrea Pollastro'}]",2024-08-18T19:18:12Z
http://arxiv.org/abs/2408.09565v1,http://arxiv.org/abs/2408.09565v1,Grammatical Error Feedback: An Implicit Evaluation Approach,"Grammatical feedback is crucial for consolidating second language (L2)
learning. Most research in computer-assisted language learning has focused on
feedback through grammatical error correction (GEC) systems, rather than
examining more holistic feedback that may be more useful for learners. This
holistic feedback will be referred to as grammatical error feedback (GEF). In
this paper, we present a novel implicit evaluation approach to GEF that
eliminates the need for manual feedback annotations. Our method adopts a
grammatical lineup approach where the task is to pair feedback and essay
representations from a set of possible alternatives. This matching process can
be performed by appropriately prompting a large language model (LLM). An
important aspect of this process, explored here, is the form of the lineup,
i.e., the selection of foils. This paper exploits this framework to examine the
quality and need for GEC to generate feedback, as well as the system used to
generate feedback, using essays from the Cambridge Learner Corpus.","[{'name': 'Stefano Bannò'}, {'name': 'Kate Knill'}, {'name': 'Mark J. F. Gales'}]",2024-08-18T18:31:55Z
http://arxiv.org/abs/2408.09559v1,http://arxiv.org/abs/2408.09559v1,"HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon
  Agent Tasks with Large Language Model","Large Language Model (LLM)-based agents exhibit significant potential across
various domains, operating as interactive systems that process environmental
observations to generate executable actions for target tasks. The effectiveness
of these agents is significantly influenced by their memory mechanism, which
records historical experiences as sequences of action-observation pairs. We
categorize memory into two types: cross-trial memory, accumulated across
multiple attempts, and in-trial memory (working memory), accumulated within a
single attempt. While considerable research has optimized performance through
cross-trial memory, the enhancement of agent performance through improved
working memory utilization remains underexplored. Instead, existing approaches
often involve directly inputting entire historical action-observation pairs
into LLMs, leading to redundancy in long-horizon tasks. Inspired by human
problem-solving strategies, this paper introduces HiAgent, a framework that
leverages subgoals as memory chunks to manage the working memory of LLM-based
agents hierarchically. Specifically, HiAgent prompts LLMs to formulate subgoals
before generating executable actions and enables LLMs to decide proactively to
replace previous subgoals with summarized observations, retaining only the
action-observation pairs relevant to the current subgoal. Experimental results
across five long-horizon tasks demonstrate that HiAgent achieves a twofold
increase in success rate and reduces the average number of steps required by
3.8. Additionally, our analysis shows that HiAgent consistently improves
performance across various steps, highlighting its robustness and
generalizability. Project Page: https://github.com/HiAgent2024/HiAgent .","[{'name': 'Mengkang Hu'}, {'name': 'Tianxing Chen'}, {'name': 'Qiguang Chen'}, {'name': 'Yao Mu'}, {'name': 'Wenqi Shao'}, {'name': 'Ping Luo'}]",2024-08-18T17:59:49Z
http://arxiv.org/abs/2408.09544v1,http://arxiv.org/abs/2408.09544v1,"No Such Thing as a General Learner: Language models and their dual
  optimization","What role can the otherwise successful Large Language Models (LLMs) play in
the understanding of human cognition, and in particular in terms of informing
language acquisition debates? To contribute to this question, we first argue
that neither humans nor LLMs are general learners, in a variety of senses. We
make a novel case for how in particular LLMs follow a dual-optimization
process: they are optimized during their training (which is typically compared
to language acquisition), and modern LLMs have also been selected, through a
process akin to natural selection in a species. From this perspective, we argue
that the performance of LLMs, whether similar or dissimilar to that of humans,
does not weigh easily on important debates about the importance of human
cognitive biases for language.","[{'name': 'Emmanuel Chemla'}, {'name': 'Ryan M. Nefdt'}]",2024-08-18T17:01:42Z
http://arxiv.org/abs/2408.09540v1,http://arxiv.org/abs/2408.09540v1,Using ChatGPT to Score Essays and Short-Form Constructed Responses,"This study aimed to determine if ChatGPT's large language models could match
the scoring accuracy of human and machine scores from the ASAP competition. The
investigation focused on various prediction models, including linear
regression, random forest, gradient boost, and boost. ChatGPT's performance was
evaluated against human raters using quadratic weighted kappa (QWK) metrics.
Results indicated that while ChatGPT's gradient boost model achieved QWKs close
to human raters for some data sets, its overall performance was inconsistent
and often lower than human scores. The study highlighted the need for further
refinement, particularly in handling biases and ensuring scoring fairness.
Despite these challenges, ChatGPT demonstrated potential for scoring
efficiency, especially with domain-specific fine-tuning. The study concludes
that ChatGPT can complement human scoring but requires additional development
to be reliable for high-stakes assessments. Future research should improve
model accuracy, address ethical considerations, and explore hybrid models
combining ChatGPT with empirical methods.",[{'name': 'Mark D. Shermis'}],2024-08-18T16:51:28Z
http://arxiv.org/abs/2408.09529v1,http://arxiv.org/abs/2408.09529v1,"Revisiting the Graph Reasoning Ability of Large Language Models: Case
  Studies in Translation, Connectivity and Shortest Path","Large Language Models (LLMs) have achieved great success in various reasoning
tasks. In this work, we focus on the graph reasoning ability of LLMs. Although
theoretical studies proved that LLMs are capable of handling graph reasoning
tasks, empirical evaluations reveal numerous failures. To deepen our
understanding on this discrepancy, we revisit the ability of LLMs on three
fundamental graph tasks: graph description translation, graph connectivity, and
the shortest-path problem. Our findings suggest that LLMs can fail to
understand graph structures through text descriptions and exhibit varying
performance for all these three fundamental tasks. Meanwhile, we perform a
real-world investigation on knowledge graphs and make consistent observations
with our findings. The codes and datasets are available.","[{'name': 'Xinnan Dai'}, {'name': 'Qihao Wen'}, {'name': 'Yifei Shen'}, {'name': 'Hongzhi Wen'}, {'name': 'Dongsheng Li'}, {'name': 'Jiliang Tang'}, {'name': 'Caihua Shan'}]",2024-08-18T16:26:39Z
http://arxiv.org/abs/2408.09503v1,http://arxiv.org/abs/2408.09503v1,"Out-of-distribution generalization via composition: a lens through
  induction heads in Transformers","Large language models (LLMs) such as GPT-4 sometimes appear to be creative,
solving novel tasks often with a few demonstrations in the prompt. These tasks
require the models to generalize on distributions different from those from
training data -- which is known as out-of-distribution (OOD) generalization.
Despite the tremendous success of LLMs, how they approach OOD generalization
remains an open and underexplored question. We examine OOD generalization in
settings where instances are generated according to hidden rules, including
in-context learning with symbolic reasoning. Models are required to infer the
hidden rules behind input prompts without any fine-tuning.
  We empirically examined the training dynamics of Transformers on a synthetic
example and conducted extensive experiments on a variety of pretrained LLMs,
focusing on a type of components known as induction heads. We found that OOD
generalization and composition are tied together -- models can learn rules by
composing two self-attention layers, thereby achieving OOD generalization.
Furthermore, a shared latent subspace in the embedding (or feature) space acts
as a bridge for composition by aligning early layers and later layers, which we
refer to as the common bridge representation hypothesis.","[{'name': 'Jiajun Song'}, {'name': 'Zhuoyan Xu'}, {'name': 'Yiqiao Zhong'}]",2024-08-18T14:52:25Z
http://arxiv.org/abs/2408.09489v1,http://arxiv.org/abs/2408.09489v1,"REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement
  Learning","With the introduction of (large) language models, there has been significant
concern about the unintended bias such models may inherit from their training
data. A number of studies have shown that such models propagate gender
stereotypes, as well as geographical and racial bias, among other biases. While
existing works tackle this issue by preprocessing data and debiasing
embeddings, the proposed methods require a lot of computational resources and
annotation effort while being limited to certain types of biases. To address
these issues, we introduce REFINE-LM, a debiasing method that uses
reinforcement learning to handle different types of biases without any
fine-tuning. By training a simple model on top of the word probability
distribution of a LM, our bias agnostic reinforcement learning method enables
model debiasing without human annotations or significant computational
resources. Experiments conducted on a wide range of models, including several
LMs, show that our method (i) significantly reduces stereotypical biases while
preserving LMs performance; (ii) is applicable to different types of biases,
generalizing across contexts such as gender, ethnicity, religion, and
nationality-based biases; and (iii) it is not expensive to train.","[{'name': 'Rameez Qureshi'}, {'name': 'Naïm Es-Sebbani'}, {'name': 'Luis Galárraga'}, {'name': 'Yvette Graham'}, {'name': 'Miguel Couceiro'}, {'name': 'Zied Bouraoui'}]",2024-08-18T14:08:31Z
http://arxiv.org/abs/2408.09485v1,http://arxiv.org/abs/2408.09485v1,Activated Parameter Locating via Causal Intervention for Model Merging,"Model merging combines multiple homologous models into one model, achieving
convincing generalization without the necessity of additional training. A key
challenge in this problem is resolving parameter redundancies and conflicts
across multiple models. Existing models have demonstrated that dropping a
portion of delta parameters can alleviate conflicts while maintaining
performance. However, these methods often drop parameters either randomly or
based on magnitude, overlooking task-specific information embedded in
fine-tuned models. In this paper, we propose an Activated Parameter Locating
(APL) method that utilizes causal intervention to estimate parameter
importance, enabling more precise parameter drops and better conflict
mitigation. Moreover, to reduce the computational complexity associated with a
large number of parameter partitions, we also introduce a theoretically
supported gradient approximation strategy for APL. Experiments on model merging
within both in-domain and out-of-domain settings, along with associated
analyses, showcase the effectiveness of APL.","[{'name': 'Fanshuang Kong'}, {'name': 'Richong Zhang'}, {'name': 'Ziqiao Wang'}]",2024-08-18T14:00:00Z
http://arxiv.org/abs/2408.09481v1,http://arxiv.org/abs/2408.09481v1,"PanoSent: A Panoptic Sextuple Extraction Benchmark for Multimodal
  Conversational Aspect-based Sentiment Analysis","While existing Aspect-based Sentiment Analysis (ABSA) has received extensive
effort and advancement, there are still gaps in defining a more holistic
research target seamlessly integrating multimodality, conversation context,
fine-granularity, and also covering the changing sentiment dynamics as well as
cognitive causal rationales. This paper bridges the gaps by introducing a
multimodal conversational ABSA, where two novel subtasks are proposed: 1)
Panoptic Sentiment Sextuple Extraction, panoramically recognizing holder,
target, aspect, opinion, sentiment, rationale from multi-turn multi-party
multimodal dialogue. 2) Sentiment Flipping Analysis, detecting the dynamic
sentiment transformation throughout the conversation with the causal reasons.
To benchmark the tasks, we construct PanoSent, a dataset annotated both
manually and automatically, featuring high quality, large scale, multimodality,
multilingualism, multi-scenarios, and covering both implicit and explicit
sentiment elements. To effectively address the tasks, we devise a novel
Chain-of-Sentiment reasoning framework, together with a novel multimodal large
language model (namely Sentica) and a paraphrase-based verification mechanism.
Extensive evaluations demonstrate the superiority of our methods over strong
baselines, validating the efficacy of all our proposed methods. The work is
expected to open up a new era for the ABSA community, and thus all our codes
and data are open at https://PanoSent.github.io/","[{'name': 'Meng Luo'}, {'name': 'Hao Fei'}, {'name': 'Bobo Li'}, {'name': 'Shengqiong Wu'}, {'name': 'Qian Liu'}, {'name': 'Soujanya Poria'}, {'name': 'Erik Cambria'}, {'name': 'Mong-Li Lee'}, {'name': 'Wynne Hsu'}]",2024-08-18T13:51:01Z
http://arxiv.org/abs/2408.09474v1,http://arxiv.org/abs/2408.09474v1,Image-Based Geolocation Using Large Vision-Language Models,"Geolocation is now a vital aspect of modern life, offering numerous benefits
but also presenting serious privacy concerns. The advent of large
vision-language models (LVLMs) with advanced image-processing capabilities
introduces new risks, as these models can inadvertently reveal sensitive
geolocation information. This paper presents the first in-depth study analyzing
the challenges posed by traditional deep learning and LVLM-based geolocation
methods. Our findings reveal that LVLMs can accurately determine geolocations
from images, even without explicit geographic training.
  To address these challenges, we introduce \tool{}, an innovative framework
that significantly enhances image-based geolocation accuracy. \tool{} employs a
systematic chain-of-thought (CoT) approach, mimicking human geoguessing
strategies by carefully analyzing visual and contextual cues such as vehicle
types, architectural styles, natural landscapes, and cultural elements.
Extensive testing on a dataset of 50,000 ground-truth data points shows that
\tool{} outperforms both traditional models and human benchmarks in accuracy.
It achieves an impressive average score of 4550.5 in the GeoGuessr game, with
an 85.37\% win rate, and delivers highly precise geolocation predictions, with
the closest distances as accurate as 0.3 km. Furthermore, our study highlights
issues related to dataset integrity, leading to the creation of a more robust
dataset and a refined framework that leverages LVLMs' cognitive capabilities to
improve geolocation precision. These findings underscore \tool{}'s superior
ability to interpret complex visual data, the urgent need to address emerging
security vulnerabilities posed by LVLMs, and the importance of responsible AI
development to ensure user privacy protection.","[{'name': 'Yi Liu'}, {'name': 'Junchen Ding'}, {'name': 'Gelei Deng'}, {'name': 'Yuekang Li'}, {'name': 'Tianwei Zhang'}, {'name': 'Weisong Sun'}, {'name': 'Yaowen Zheng'}, {'name': 'Jingquan Ge'}, {'name': 'Yang Liu'}]",2024-08-18T13:39:43Z
http://arxiv.org/abs/2408.09459v1,http://arxiv.org/abs/2408.09459v1,"WPN: An Unlearning Method Based on N-pair Contrastive Learning in
  Language Models","Generative language models (LMs) offer numerous advantages but may produce
inappropriate or harmful outputs due to the harmful knowledge acquired during
pre-training. This knowledge often manifests as undesirable correspondences,
such as ""harmful prompts"" leading to ""harmful outputs,"" which our research aims
to mitigate through unlearning techniques.However, existing unlearning methods
based on gradient ascent can significantly impair the performance of LMs. To
address this issue, we propose a novel approach called Weighted Positional
N-pair (WPN) Learning, which leverages position-weighted mean pooling within an
n-pair contrastive learning framework. WPN is designed to modify the output
distribution of LMs by eliminating specific harmful outputs (e.g., replacing
toxic responses with neutral ones), thereby transforming the model's behavior
from ""harmful prompt-harmful output"" to ""harmful prompt-harmless
response"".Experiments on OPT and GPT-NEO LMs show that WPN effectively reduces
the proportion of harmful responses, achieving a harmless rate of up to 95.8\%
while maintaining stable performance on nine common benchmarks (with less than
2\% degradation on average). Moreover, we provide empirical evidence to
demonstrate WPN's ability to weaken the harmful correspondences in terms of
generalizability and robustness, as evaluated on out-of-distribution test sets
and under adversarial attacks.","[{'name': 'Guitao Chen'}, {'name': 'Yunshen Wang'}, {'name': 'Hongye Sun'}, {'name': 'Guang Chen'}]",2024-08-18T12:37:03Z
http://arxiv.org/abs/2408.09452v1,http://arxiv.org/abs/2408.09452v1,"Identifying Speakers and Addressees of Quotations in Novels with Prompt
  Learning","Quotations in literary works, especially novels, are important to create
characters, reflect character relationships, and drive plot development.
Current research on quotation extraction in novels primarily focuses on
quotation attribution, i.e., identifying the speaker of the quotation. However,
the addressee of the quotation is also important to construct the relationship
between the speaker and the addressee. To tackle the problem of dataset
scarcity, we annotate the first Chinese quotation corpus with elements
including speaker, addressee, speaking mode and linguistic cue. We propose
prompt learning-based methods for speaker and addressee identification based on
fine-tuned pre-trained models. Experiments on both Chinese and English datasets
show the effectiveness of the proposed methods, which outperform methods based
on zero-shot and few-shot large language models.","[{'name': 'Yuchen Yan'}, {'name': 'Hanjie Zhao'}, {'name': 'Senbin Zhu'}, {'name': 'Hongde Liu'}, {'name': 'Zhihong Zhang'}, {'name': 'Yuxiang Jia'}]",2024-08-18T12:19:18Z
http://arxiv.org/abs/2408.09437v1,http://arxiv.org/abs/2408.09437v1,Hindi-BEIR : A Large Scale Retrieval Benchmark in Hindi,"Given the large number of Hindi speakers worldwide, there is a pressing need
for robust and efficient information retrieval systems for Hindi. Despite
ongoing research, there is a lack of comprehensive benchmark for evaluating
retrieval models in Hindi. To address this gap, we introduce the Hindi version
of the BEIR benchmark, which includes a subset of English BEIR datasets
translated to Hindi, existing Hindi retrieval datasets, and synthetically
created datasets for retrieval. The benchmark is comprised of $15$ datasets
spanning across $8$ distinct tasks. We evaluate state-of-the-art multilingual
retrieval models on this benchmark to identify task and domain-specific
challenges and their impact on retrieval performance. By releasing this
benchmark and a set of relevant baselines, we enable researchers to understand
the limitations and capabilities of current Hindi retrieval models, promoting
advancements in this critical area. The datasets from Hindi-BEIR are publicly
available.","[{'name': 'Arkadeep Acharya'}, {'name': 'Rudra Murthy'}, {'name': 'Vishwajeet Kumar'}, {'name': 'Jaydeep Sen'}]",2024-08-18T10:55:04Z
http://arxiv.org/abs/2408.09434v1,http://arxiv.org/abs/2408.09434v1,"HySem: A context length optimized LLM pipeline for unstructured tabular
  extraction","Regulatory compliance reporting in the pharmaceutical industry relies on
detailed tables, but these are often under-utilized beyond compliance due to
their unstructured format and arbitrary content. Extracting and semantically
representing tabular data is challenging due to diverse table presentations.
Large Language Models (LLMs) demonstrate substantial potential for semantic
representation, yet they encounter challenges related to accuracy and context
size limitations, which are crucial considerations for the industry
applications. We introduce HySem, a pipeline that employs a novel context
length optimization technique to generate accurate semantic JSON
representations from HTML tables. This approach utilizes a custom fine-tuned
model specifically designed for cost- and privacy-sensitive small and medium
pharmaceutical enterprises. Running on commodity hardware and leveraging
open-source models, our auto-correcting agents rectify both syntax and semantic
errors in LLM-generated content. HySem surpasses its peer open-source models in
accuracy and provides competitive performance when benchmarked against OpenAI
GPT-4o and effectively addresses context length limitations, which is a crucial
factor for supporting larger tables.","[{'name': 'Narayanan PP'}, {'name': 'Anantharaman Palacode Narayana Iyer'}]",2024-08-18T10:40:37Z
http://arxiv.org/abs/2408.09430v1,http://arxiv.org/abs/2408.09430v1,FASST: Fast LLM-based Simultaneous Speech Translation,"Simultaneous speech translation (SST) takes streaming speech input and
generates text translation on the fly. Existing methods either have high
latency due to recomputation of input representations, or fall behind of
offline ST in translation quality. In this paper, we propose FASST, a fast
large language model based method for streaming speech translation. We propose
blockwise-causal speech encoding and consistency mask, so that streaming speech
input can be encoded incrementally without recomputation. Furthermore, we
develop a two-stage training strategy to optimize FASST for simultaneous
inference. We evaluate FASST and multiple strong prior models on MuST-C
dataset. Experiment results show that FASST achieves the best quality-latency
trade-off. It outperforms the previous best model by an average of 1.5 BLEU
under the same latency for English to Spanish translation.","[{'name': 'Siqi Ouyang'}, {'name': 'Xi Xu'}, {'name': 'Chinmay Dandekar'}, {'name': 'Lei Li'}]",2024-08-18T10:12:39Z
http://arxiv.org/abs/2408.09422v1,http://arxiv.org/abs/2408.09422v1,"Distinguish Confusion in Legal Judgment Prediction via Revised Relation
  Knowledge","Legal Judgment Prediction (LJP) aims to automatically predict a law case's
judgment results based on the text description of its facts. In practice, the
confusing law articles (or charges) problem frequently occurs, reflecting that
the law cases applicable to similar articles (or charges) tend to be misjudged.
Although some recent works based on prior knowledge solve this issue well, they
ignore that confusion also occurs between law articles with a high posterior
semantic similarity due to the data imbalance problem instead of only between
the prior highly similar ones, which is this work's further finding. This paper
proposes an end-to-end model named \textit{D-LADAN} to solve the above
challenges. On the one hand, D-LADAN constructs a graph among law articles
based on their text definition and proposes a graph distillation operation
(GDO) to distinguish the ones with a high prior semantic similarity. On the
other hand, D-LADAN presents a novel momentum-updated memory mechanism to
dynamically sense the posterior similarity between law articles (or charges)
and a weighted GDO to adaptively capture the distinctions for revising the
inductive bias caused by the data imbalance problem. We perform extensive
experiments to demonstrate that D-LADAN significantly outperforms
state-of-the-art methods in accuracy and robustness.","[{'name': 'Nuo Xu'}, {'name': 'Pinghui Wang'}, {'name': 'Junzhou Zhao'}, {'name': 'Feiyang Sun'}, {'name': 'Lin Lan'}, {'name': 'Jing Tao'}, {'name': 'Li Pan'}, {'name': 'Xiaohong Guan'}]",2024-08-18T09:44:59Z
http://arxiv.org/abs/2408.09420v1,http://arxiv.org/abs/2408.09420v1,"Enhancing Startup Success Predictions in Venture Capital: A GraphRAG
  Augmented Multivariate Time Series Method","In the Venture Capital(VC) industry, predicting the success of startups is
challenging due to limited financial data and the need for subjective revenue
forecasts. Previous methods based on time series analysis or deep learning
often fall short as they fail to incorporate crucial inter-company
relationships such as competition and collaboration. Regarding the issues, we
propose a novel approach using GrahphRAG augmented time series model. With
GraphRAG, time series predictive methods are enhanced by integrating these
vital relationships into the analysis framework, allowing for a more dynamic
understanding of the startup ecosystem in venture capital. Our experimental
results demonstrate that our model significantly outperforms previous models in
startup success predictions. To the best of our knowledge, our work is the
first application work of GraphRAG.","[{'name': 'Gao Zitian'}, {'name': 'Xiao Yihao'}]",2024-08-18T09:31:13Z
http://arxiv.org/abs/2408.09416v1,http://arxiv.org/abs/2408.09416v1,Challenges and Responses in the Practice of Large Language Models,"This paper carefully summarizes extensive and profound questions from all
walks of life, focusing on the current high-profile AI field, covering multiple
dimensions such as industry trends, academic research, technological innovation
and business applications. This paper meticulously curates questions that are
both thought-provoking and practically relevant, providing nuanced and
insightful answers to each. To facilitate readers' understanding and reference,
this paper specifically classifies and organizes these questions systematically
and meticulously from the five core dimensions of computing power
infrastructure, software architecture, data resources, application scenarios,
and brain science. This work aims to provide readers with a comprehensive,
in-depth and cutting-edge AI knowledge framework to help people from all walks
of life grasp the pulse of AI development, stimulate innovative thinking, and
promote industrial progress.",[{'name': 'Hongyin Zhu'}],2024-08-18T09:15:11Z
http://arxiv.org/abs/2408.09404v1,http://arxiv.org/abs/2408.09404v1,"Comparison between the Structures of Word Co-occurrence and Word
  Similarity Networks for Ill-formed and Well-formed Texts in Taiwan Mandarin","The study of word co-occurrence networks has attracted the attention of
researchers due to their potential significance as well as applications.
Understanding the structure of word co-occurrence networks is therefore
important to fully realize their significance and usages. In past studies, word
co-occurrence networks built on well-formed texts have been found to possess
certain characteristics, including being small-world, following a two-regime
power law distribution, and being generally disassortative. On the flip side,
past studies have found that word co-occurrence networks built from ill-formed
texts such as microblog posts may behave differently from those built from
well-formed documents. While both kinds of word co-occurrence networks are
small-world and disassortative, word co-occurrence networks built from
ill-formed texts are scale-free and follow the power law distribution instead
of the two-regime power law distribution. However, since past studies on the
behavior of word co-occurrence networks built from ill-formed texts only
investigated English, the universality of such characteristics remains to be
seen among different languages. In addition, it is yet to be investigated
whether there could be possible similitude/differences between word
co-occurrence networks and other potentially comparable networks. This study
therefore investigates and compares the structure of word co-occurrence
networks and word similarity networks based on Taiwan Mandarin ill-formed
internet forum posts and compare them with those built with well-formed
judicial judgments, and seeks to find out whether the three aforementioned
properties (scale-free, small-world, and disassortative) for ill-formed and
well-formed texts are universal among different languages and between word
co-occurrence and word similarity networks.","[{'name': 'Po-Hsuan Huang'}, {'name': 'Hsuan-Lei Shao'}]",2024-08-18T08:30:16Z
http://arxiv.org/abs/2408.09386v1,http://arxiv.org/abs/2408.09386v1,Game Development as Human-LLM Interaction,"Game development is a highly specialized task that relies on a complex game
engine powered by complex programming languages, preventing many gaming
enthusiasts from handling it. This paper introduces the Interaction-driven Game
Engine (IGE) powered by LLM, which allows everyone to develop a custom game
using natural language through Human-LLM interaction. To enable an LLM to
function as an IGE, we instruct it to perform the following processes in each
turn: (1) $P_{script}$ : configure the game script segment based on the user's
input; (2) $P_{code}$ : generate the corresponding code snippet based on the
game script segment; (3) $P_{utter}$ : interact with the user, including
guidance and feedback. We propose a data synthesis pipeline based on the LLM to
generate game script-code pairs and interactions from a few manually crafted
seed data. We propose a three-stage progressive training strategy to transfer
the dialogue-based LLM to our IGE smoothly. We construct an IGE for poker games
as a case study and comprehensively evaluate it from two perspectives:
interaction quality and code correctness. The code and data are available at
\url{https://github.com/alterego238/IGE}.","[{'name': 'Jiale Hong'}, {'name': 'Hongqiu Wu'}, {'name': 'Hai Zhao'}]",2024-08-18T07:06:57Z
http://arxiv.org/abs/2408.09385v1,http://arxiv.org/abs/2408.09385v1,Offline RLHF Methods Need More Accurate Supervision Signals,"With the rapid advances in Large Language Models (LLMs), aligning LLMs with
human preferences become increasingly important. Although Reinforcement
Learning with Human Feedback (RLHF) proves effective, it is complicated and
highly resource-intensive. As such, offline RLHF has been introduced as an
alternative solution, which directly optimizes LLMs with ranking losses on a
fixed preference dataset. Current offline RLHF only captures the ``ordinal
relationship'' between responses, overlooking the crucial aspect of ``how
much'' one is preferred over the others. To address this issue, we propose a
simple yet effective solution called \textbf{R}eward \textbf{D}ifference
\textbf{O}ptimization, shorted as \textbf{RDO}. Specifically, we introduce {\it
reward difference coefficients} to reweigh sample pairs in offline RLHF. We
then develop a {\it difference model} involving rich interactions between a
pair of responses for predicting these difference coefficients. Experiments
with 7B LLMs on the HH and TL;DR datasets substantiate the effectiveness of our
method in both automatic metrics and human evaluation, thereby highlighting its
potential for aligning LLMs with human intent and values.","[{'name': 'Shiqi Wang'}, {'name': 'Zhengze Zhang'}, {'name': 'Rui Zhao'}, {'name': 'Fei Tan'}, {'name': 'Cam Tu Nguyen'}]",2024-08-18T07:04:16Z
http://arxiv.org/abs/2408.09366v1,http://arxiv.org/abs/2408.09366v1,"Improving and Assessing the Fidelity of Large Language Models Alignment
  to Online Communities","Large language models (LLMs) have shown promise in representing individuals
and communities, offering new ways to study complex social dynamics. However,
effectively aligning LLMs with specific human groups and systematically
assessing the fidelity of the alignment remains a challenge. This paper
presents a robust framework for aligning LLMs with online communities via
instruction-tuning and comprehensively evaluating alignment across various
aspects of language, including authenticity, emotional tone, toxicity, and
harm. We demonstrate the utility of our approach by applying it to online
communities centered on dieting and body image. We administer an eating
disorder psychometric test to the aligned LLMs to reveal unhealthy beliefs and
successfully differentiate communities with varying levels of eating disorder
risk. Our results highlight the potential of LLMs in automated moderation and
broader applications in public health and social science research.","[{'name': 'Minh Duc Chu'}, {'name': 'Zihao He'}, {'name': 'Rebecca Dorn'}, {'name': 'Kristina Lerman'}]",2024-08-18T05:41:36Z
http://arxiv.org/abs/2408.09365v1,http://arxiv.org/abs/2408.09365v1,"Concept Distillation from Strong to Weak Models via
  Hypotheses-to-Theories Prompting","Hand-crafting high quality prompts to optimize the performance of language
models is a complicated and labor-intensive process. Furthermore, when
migrating to newer, smaller, or weaker models (possibly due to latency or cost
gains), prompts need to be updated to re-optimize the task performance. We
propose Concept Distillation (CD), an automatic prompt optimization technique
for enhancing weaker models on complex tasks. CD involves: (1) collecting
mistakes made by weak models with a base prompt (initialization), (2) using a
strong model to generate reasons for these mistakes and create rules/concepts
for weak models (induction), and (3) filtering these rules based on validation
set performance and integrating them into the base prompt
(deduction/verification). We evaluated CD on NL2Code and mathematical reasoning
tasks, observing significant performance boosts for small and weaker language
models. Notably, Mistral-7B's accuracy on Multi-Arith increased by 20%, and
Phi-3-mini-3.8B's accuracy on HumanEval rose by 34%. Compared to other
automated methods, CD offers an effective, cost-efficient strategy for
improving weak models' performance on complex tasks and enables seamless
workload migration across different language models without compromising
performance.","[{'name': 'Emmanuel Aboah Boateng'}, {'name': 'Cassiano O. Becker'}, {'name': 'Nabiha Asghar'}, {'name': 'Kabir Walia'}, {'name': 'Ashwin Srinivasan'}, {'name': 'Ehi Nosakhare'}, {'name': 'Victor Dibia'}, {'name': 'Soundar Srinivasan'}]",2024-08-18T05:37:48Z
http://arxiv.org/abs/2408.09333v1,http://arxiv.org/abs/2408.09333v1,"SkyScript-100M: 1,000,000,000 Pairs of Scripts and Shooting Scripts for
  Short Drama","Generating high-quality shooting scripts containing information such as scene
and shot language is essential for short drama script generation. We collect
6,660 popular short drama episodes from the Internet, each with an average of
100 short episodes, and the total number of short episodes is about 80,000,
with a total duration of about 2,000 hours and totaling 10 terabytes (TB). We
perform keyframe extraction and annotation on each episode to obtain about
10,000,000 shooting scripts. We perform 100 script restorations on the
extracted shooting scripts based on our self-developed large short drama
generation model SkyReels. This leads to a dataset containing 1,000,000,000
pairs of scripts and shooting scripts for short dramas, called SkyScript-100M.
We compare SkyScript-100M with the existing dataset in detail and demonstrate
some deeper insights that can be achieved based on SkyScript-100M. Based on
SkyScript-100M, researchers can achieve several deeper and more far-reaching
script optimization goals, which may drive a paradigm shift in the entire field
of text-to-video and significantly advance the field of short drama video
generation. The data and code are available at
https://github.com/vaew/SkyScript-100M.","[{'name': 'Jing Tang'}, {'name': 'Quanlu Jia'}, {'name': 'Yuqiang Xie'}, {'name': 'Zeyu Gong'}, {'name': 'Xiang Wen'}, {'name': 'Jiayi Zhang'}, {'name': 'Yalong Guo'}, {'name': 'Guibin Chen'}, {'name': 'Jiangping Yang'}]",2024-08-18T02:27:25Z
http://arxiv.org/abs/2408.09330v1,http://arxiv.org/abs/2408.09330v1,"Fostering Natural Conversation in Large Language Models with NICO: a
  Natural Interactive COnversation dataset","Benefiting from diverse instruction datasets, contemporary Large Language
Models (LLMs) perform effectively as AI assistants in collaborating with
humans. However, LLMs still struggle to generate natural and colloquial
responses in real-world applications such as chatbots and psychological
counseling that require more human-like interactions. To address these
limitations, we introduce NICO, a Natural Interactive COnversation dataset in
Chinese. We first use GPT-4-turbo to generate dialogue drafts and make them
cover 20 daily-life topics and 5 types of social interactions. Then, we hire
workers to revise these dialogues to ensure that they are free of grammatical
errors and unnatural utterances. We define two dialogue-level natural
conversation tasks and two sentence-level tasks for identifying and rewriting
unnatural sentences. Multiple open-source and closed-source LLMs are tested and
analyzed in detail. The experimental results highlight the challenge of the
tasks and demonstrate how NICO can help foster the natural dialogue
capabilities of LLMs. The dataset will be released.","[{'name': 'Renliang Sun'}, {'name': 'Mengyuan Liu'}, {'name': 'Shiping Yang'}, {'name': 'Rui Wang'}, {'name': 'Junqing He'}, {'name': 'Jiaxing Zhang'}]",2024-08-18T02:06:25Z
http://arxiv.org/abs/2408.09327v1,http://arxiv.org/abs/2408.09327v1,"Threshold Filtering Packing for Supervised Fine-Tuning: Training Related
  Samples within Packs","Packing for Supervised Fine-Tuning (SFT) in autoregressive models involves
concatenating data points of varying lengths until reaching the designed
maximum length to facilitate GPU processing. However, randomly concatenating
data points and feeding them into an autoregressive transformer can lead to
cross-contamination of sequences due to the significant difference in their
subject matter. The mainstream approaches in SFT ensure that each token in the
attention calculation phase only focuses on tokens within its own short
sequence, without providing additional learning signals for the preceding
context. To address these challenges, we introduce Threshold Filtering Packing
(TFP), a method that selects samples with related context while maintaining
sufficient diversity within the same pack. Our experiments show that TFP offers
a simple-to-implement and scalable approach that significantly enhances SFT
performance, with observed improvements of up to 7\% on GSM8K, 4\% on
HumanEval, and 15\% on the adult-census-income dataset.","[{'name': 'Jiancheng Dong'}, {'name': 'Lei Jiang'}, {'name': 'Wei Jin'}, {'name': 'Lu Cheng'}]",2024-08-18T01:59:41Z
http://arxiv.org/abs/2408.09326v1,http://arxiv.org/abs/2408.09326v1,"Characterizing and Evaluating the Reliability of LLMs against Jailbreak
  Attacks","Large Language Models (LLMs) have increasingly become pivotal in content
generation with notable societal impact. These models hold the potential to
generate content that could be deemed harmful.Efforts to mitigate this risk
include implementing safeguards to ensure LLMs adhere to social ethics.However,
despite such measures, the phenomenon of ""jailbreaking"" -- where carefully
crafted prompts elicit harmful responses from models -- persists as a
significant challenge. Recognizing the continuous threat posed by jailbreaking
tactics and their repercussions for the trustworthy use of LLMs, a rigorous
assessment of the models' robustness against such attacks is essential. This
study introduces an comprehensive evaluation framework and conducts an
large-scale empirical experiment to address this need. We concentrate on 10
cutting-edge jailbreak strategies across three categories, 1525 questions from
61 specific harmful categories, and 13 popular LLMs. We adopt multi-dimensional
metrics such as Attack Success Rate (ASR), Toxicity Score, Fluency, Token
Length, and Grammatical Errors to thoroughly assess the LLMs' outputs under
jailbreak. By normalizing and aggregating these metrics, we present a detailed
reliability score for different LLMs, coupled with strategic recommendations to
reduce their susceptibility to such vulnerabilities. Additionally, we explore
the relationships among the models, attack strategies, and types of harmful
content, as well as the correlations between the evaluation metrics, which
proves the validity of our multifaceted evaluation framework. Our extensive
experimental results demonstrate a lack of resilience among all tested LLMs
against certain strategies, and highlight the need to concentrate on the
reliability facets of LLMs. We believe our study can provide valuable insights
into enhancing the security evaluation of LLMs against jailbreak within the
domain.","[{'name': 'Kexin Chen'}, {'name': 'Yi Liu'}, {'name': 'Dongxia Wang'}, {'name': 'Jiaying Chen'}, {'name': 'Wenhai Wang'}]",2024-08-18T01:58:03Z
http://arxiv.org/abs/2408.09311v1,http://arxiv.org/abs/2408.09311v1,"An Open-Source American Sign Language Fingerspell Recognition and
  Semantic Pose Retrieval Interface","This paper introduces an open-source interface for American Sign Language
fingerspell recognition and semantic pose retrieval, aimed to serve as a
stepping stone towards more advanced sign language translation systems.
Utilizing a combination of convolutional neural networks and pose estimation
models, the interface provides two modular components: a recognition module for
translating ASL fingerspelling into spoken English and a production module for
converting spoken English into ASL pose sequences. The system is designed to be
highly accessible, user-friendly, and capable of functioning in real-time under
varying environmental conditions like backgrounds, lighting, skin tones, and
hand sizes. We discuss the technical details of the model architecture,
application in the wild, as well as potential future enhancements for
real-world consumer applications.",[{'name': 'Kevin Jose Thomas'}],2024-08-17T23:59:17Z
http://arxiv.org/abs/2408.09304v1,http://arxiv.org/abs/2408.09304v1,"CyberPal.AI: Empowering LLMs with Expert-Driven Cybersecurity
  Instructions","Large Language Models (LLMs) have significantly advanced natural language
processing (NLP), providing versatile capabilities across various applications.
However, their application to complex, domain-specific tasks, such as
cyber-security, often faces substantial challenges. In this study, we introduce
SecKnowledge and CyberPal.AI to address these challenges and train
security-expert LLMs. SecKnowledge is a domain-knowledge-driven cyber-security
instruction dataset, meticulously designed using years of accumulated expert
knowledge in the domain through a multi-phase generation process. CyberPal.AI
refers to a family of LLMs fine-tuned using SecKnowledge, aimed at building
security-specialized LLMs capable of answering and following complex
security-related instructions. Additionally, we introduce SecKnowledge-Eval, a
comprehensive and diverse cyber-security evaluation benchmark, composed of an
extensive set of cyber-security tasks we specifically developed to assess LLMs
in the field of cyber-security, along with other publicly available security
benchmarks. Our results show a significant average improvement of up to 24%
over the baseline models, underscoring the benefits of our expert-driven
instruction dataset generation process. These findings contribute to the
advancement of AI-based cyber-security applications, paving the way for
security-expert LLMs that can enhance threat-hunting and investigation
processes.","[{'name': 'Matan Levi'}, {'name': 'Yair Alluouche'}, {'name': 'Daniel Ohayon'}, {'name': 'Anton Puzanov'}]",2024-08-17T22:37:39Z
http://arxiv.org/abs/2408.09273v1,http://arxiv.org/abs/2408.09273v1,"ConVerSum: A Contrastive Learning based Approach for Data-Scarce
  Solution of Cross-Lingual Summarization Beyond Direct Equivalents","Cross-Lingual summarization (CLS) is a sophisticated branch in Natural
Language Processing that demands models to accurately translate and summarize
articles from different source languages. Despite the improvement of the
subsequent studies, This area still needs data-efficient solutions along with
effective training methodologies. To the best of our knowledge, there is no
feasible solution for CLS when there is no available high-quality CLS data. In
this paper, we propose a novel data-efficient approach, ConVerSum, for CLS
leveraging the power of contrastive learning, generating versatile candidate
summaries in different languages based on the given source document and
contrasting these summaries with reference summaries concerning the given
documents. After that, we train the model with a contrastive ranking loss.
Then, we rigorously evaluate the proposed approach against current
methodologies and compare it to powerful Large Language Models (LLMs)- Gemini,
GPT 3.5, and GPT 4 proving our model performs better for low-resource
languages' CLS. These findings represent a substantial improvement in the area,
opening the door to more efficient and accurate cross-lingual summarizing
techniques.","[{'name': 'Sanzana Karim Lora'}, {'name': 'Rifat Shahriyar'}]",2024-08-17T19:03:53Z
http://arxiv.org/abs/2408.09235v1,http://arxiv.org/abs/2408.09235v1,"Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of
  Free-Form Text","The rapid advancements in Large Language Models (LLMs) have highlighted the
critical need for robust evaluation methods that can accurately assess the
quality of generated text, particularly in free-form tasks. Traditional metrics
like BLEU and ROUGE, while useful, often fail to capture the semantic richness
and contextual relevance of free-form text compared to reference answers. In
this study, we introduce a reference-guided verdict method that leverages
multiple LLMs-as-judges to provide a more reliable and accurate evaluation of
open-ended LLM generations. By integrating diverse LLMs, our approach mitigates
individual model biases and significantly improves alignment with human
judgments, especially in challenging tasks where traditional metrics and
single-model evaluations fall short. Through experiments across multiple
question-answering tasks, we show that our method closely aligns with human
evaluations, establishing it as a scalable, reproducible, and effective
alternative to human evaluation. Our approach not only enhances evaluation
reliability but also opens new avenues for refining automated assessment in
generative AI.","[{'name': 'Sher Badshah'}, {'name': 'Hassan Sajjad'}]",2024-08-17T16:01:45Z
http://arxiv.org/abs/2408.09215v1,http://arxiv.org/abs/2408.09215v1,"Generating Data with Text-to-Speech and Large-Language Models for
  Conversational Speech Recognition","Currently, a common approach in many speech processing tasks is to leverage
large scale pre-trained models by fine-tuning them on in-domain data for a
particular application. Yet obtaining even a small amount of such data can be
problematic, especially for sensitive domains and conversational speech
scenarios, due to both privacy issues and annotation costs. To address this,
synthetic data generation using single speaker datasets has been employed. Yet,
for multi-speaker cases, such an approach often requires extensive manual
effort and is prone to domain mismatches. In this work, we propose a synthetic
data generation pipeline for multi-speaker conversational ASR, leveraging a
large language model (LLM) for content creation and a conversational
multi-speaker text-to-speech (TTS) model for speech synthesis. We conduct
evaluation by fine-tuning the Whisper ASR model for telephone and distant
conversational speech settings, using both in-domain data and generated
synthetic data. Our results show that the proposed method is able to
significantly outperform classical multi-speaker generation approaches that use
external, non-conversational speech datasets.","[{'name': 'Samuele Cornell'}, {'name': 'Jordan Darefsky'}, {'name': 'Zhiyao Duan'}, {'name': 'Shinji Watanabe'}]",2024-08-17T14:47:05Z
http://arxiv.org/abs/2408.09205v1,http://arxiv.org/abs/2408.09205v1,"Architectural Foundations and Strategic Considerations for the Large
  Language Model Infrastructures","The development of a large language model (LLM) infrastructure is a pivotal
undertaking in artificial intelligence. This paper explores the intricate
landscape of LLM infrastructure, software, and data management. By analyzing
these core components, we emphasize the pivotal considerations and safeguards
crucial for successful LLM development. This work presents a concise synthesis
of the challenges and strategies inherent in constructing a robust and
effective LLM infrastructure, offering valuable insights for researchers and
practitioners alike.",[{'name': 'Hongyin Zhu'}],2024-08-17T13:54:34Z
http://arxiv.org/abs/2408.09193v1,http://arxiv.org/abs/2408.09193v1,AI Managed Emergency Documentation with a Pretrained Model,"This study investigates the use of a large language model system to improve
efficiency and quality in emergency department (ED) discharge letter writing.
Time constraints and infrastructural deficits make compliance with current
discharge letter targets difficult. We explored potential efficiencies from an
artificial intelligence software in the generation of ED discharge letters and
the attitudes of doctors toward this technology. The evaluated system leverages
advanced techniques to fine-tune a model to generate discharge summaries from
short-hand inputs, including voice, text, and electronic health record data.
Nineteen physicians with emergency medicine experience evaluated the system
text and voice-to-text interfaces against manual typing. The results showed
significant time savings with MedWrite LLM interfaces compared to manual
methods.","[{'name': 'David Menzies'}, {'name': 'Sean Kirwan'}, {'name': 'Ahmad Albarqawi'}]",2024-08-17T13:11:46Z
http://arxiv.org/abs/2408.09177v1,http://arxiv.org/abs/2408.09177v1,"Chinese Metaphor Recognition Using a Multi-stage Prompting Large
  Language Model","Metaphors are common in everyday language, and the identification and
understanding of metaphors are facilitated by models to achieve a better
understanding of the text. Metaphors are mainly identified and generated by
pre-trained models in existing research, but situations, where tenors or
vehicles are not included in the metaphor, cannot be handled. The problem can
be effectively solved by using Large Language Models (LLMs), but significant
room for exploration remains in this early-stage research area. A multi-stage
generative heuristic-enhanced prompt framework is proposed in this study to
enhance the ability of LLMs to recognize tenors, vehicles, and grounds in
Chinese metaphors. In the first stage, a small model is trained to obtain the
required confidence score for answer candidate generation. In the second stage,
questions are clustered and sampled according to specific rules. Finally, the
heuristic-enhanced prompt needed is formed by combining the generated answer
candidates and demonstrations. The proposed model achieved 3rd place in Track 1
of Subtask 1, 1st place in Track 2 of Subtask 1, and 1st place in both tracks
of Subtask 2 at the NLPCC-2024 Shared Task 9.","[{'name': 'Jie Wang'}, {'name': 'Jin Wang'}, {'name': 'Xuejie Zhang'}]",2024-08-17T11:56:38Z
http://arxiv.org/abs/2408.09176v1,http://arxiv.org/abs/2408.09176v1,"Cognitive LLMs: Towards Integrating Cognitive Architectures and Large
  Language Models for Manufacturing Decision-making","Resolving the dichotomy between the human-like yet constrained reasoning
processes of Cognitive Architectures and the broad but often noisy inference
behavior of Large Language Models (LLMs) remains a challenging but exciting
pursuit, for enabling reliable machine reasoning capabilities in production
systems. Because Cognitive Architectures are famously developed for the purpose
of modeling the internal mechanisms of human cognitive decision-making at a
computational level, new investigations consider the goal of informing LLMs
with the knowledge necessary for replicating such processes, e.g., guided
perception, memory, goal-setting, and action. Previous approaches that use LLMs
for grounded decision-making struggle with complex reasoning tasks that require
slower, deliberate cognition over fast and intuitive inference -- reporting
issues related to the lack of sufficient grounding, as in hallucination. To
resolve these challenges, we introduce LLM-ACTR, a novel neuro-symbolic
architecture that provides human-aligned and versatile decision-making by
integrating the ACT-R Cognitive Architecture with LLMs. Our framework extracts
and embeds knowledge of ACT-R's internal decision-making process as latent
neural representations, injects this information into trainable LLM adapter
layers, and fine-tunes the LLMs for downstream prediction. Our experiments on
novel Design for Manufacturing tasks show both improved task performance as
well as improved grounded decision-making capability of our approach, compared
to LLM-only baselines that leverage chain-of-thought reasoning strategies.","[{'name': 'Siyu Wu'}, {'name': 'Alessandro Oltramari'}, {'name': 'Jonathan Francis'}, {'name': 'C. Lee Giles'}, {'name': 'Frank E. Ritter'}]",2024-08-17T11:49:53Z
http://arxiv.org/abs/2408.09174v1,http://arxiv.org/abs/2408.09174v1,"TableBench: A Comprehensive and Complex Benchmark for Table Question
  Answering","Recent advancements in Large Language Models (LLMs) have markedly enhanced
the interpretation and processing of tabular data, introducing previously
unimaginable capabilities. Despite these achievements, LLMs still encounter
significant challenges when applied in industrial scenarios, particularly due
to the increased complexity of reasoning required with real-world tabular data,
underscoring a notable disparity between academic benchmarks and practical
applications. To address this discrepancy, we conduct a detailed investigation
into the application of tabular data in industrial scenarios and propose a
comprehensive and complex benchmark TableBench, including 18 fields within four
major categories of table question answering (TableQA) capabilities.
Furthermore, we introduce TableLLM, trained on our meticulously constructed
training set TableInstruct, achieving comparable performance with GPT-3.5.
Massive experiments conducted on TableBench indicate that both open-source and
proprietary LLMs still have significant room for improvement to meet real-world
demands, where the most advanced model, GPT-4, achieves only a modest score
compared to humans.","[{'name': 'Xianjie Wu'}, {'name': 'Jian Yang'}, {'name': 'Linzheng Chai'}, {'name': 'Ge Zhang'}, {'name': 'Jiaheng Liu'}, {'name': 'Xinrun Du'}, {'name': 'Di Liang'}, {'name': 'Daixin Shu'}, {'name': 'Xianfu Cheng'}, {'name': 'Tianzhen Sun'}, {'name': 'Guanglin Niu'}, {'name': 'Tongliang Li'}, {'name': 'Zhoujun Li'}]",2024-08-17T11:40:10Z
http://arxiv.org/abs/2408.09172v1,http://arxiv.org/abs/2408.09172v1,"Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context
  Example Selection","Nowadays, Large Language Models (LLMs) have demonstrated exceptional
performance across various downstream tasks. However, it is challenging for
users to discern whether the responses are generated with certainty or are
fabricated to meet user expectations. Estimating the uncertainty of LLMs is
particularly challenging due to their vast scale and the lack of white-box
access. In this work, we propose a novel Uncertainty Tripartite Testing
Paradigm (Unc-TTP) to classify LLM uncertainty, via evaluating the consistency
of LLM outputs when incorporating label interference into the sampling-based
approach. Based on Unc-TTP outputs, we aggregate instances into certain and
uncertain categories. Further, we conduct a detailed analysis of the
uncertainty properties of LLMs and show Unc-TTP's superiority over the existing
sampling-based methods. In addition, we leverage the obtained uncertainty
information to guide in-context example selection, demonstrating that Unc-TTP
obviously outperforms retrieval-based and sampling-based approaches in
selecting more informative examples. Our work paves a new way to classify the
uncertainty of both open- and closed-source LLMs, and introduces a practical
approach to exploit this uncertainty to improve LLMs performance.","[{'name': 'Hsiu-Yuan Huang'}, {'name': 'Zichen Wu'}, {'name': 'Yutong Yang'}, {'name': 'Junzhao Zhang'}, {'name': 'Yunfang Wu'}]",2024-08-17T11:33:23Z
http://arxiv.org/abs/2408.09169v1,http://arxiv.org/abs/2408.09169v1,"Automatic Metrics in Natural Language Generation: A Survey of Current
  Evaluation Practices","Automatic metrics are extensively used to evaluate natural language
processing systems. However, there has been increasing focus on how they are
used and reported by practitioners within the field. In this paper, we have
conducted a survey on the use of automatic metrics, focusing particularly on
natural language generation (NLG) tasks. We inspect which metrics are used as
well as why they are chosen and how their use is reported. Our findings from
this survey reveal significant shortcomings, including inappropriate metric
usage, lack of implementation details and missing correlations with human
judgements. We conclude with recommendations that we believe authors should
follow to enable more rigour within the field.","[{'name': 'Patrícia Schmidtová'}, {'name': 'Saad Mahamood'}, {'name': 'Simone Balloccu'}, {'name': 'Ondřej Dušek'}, {'name': 'Albert Gatt'}, {'name': 'Dimitra Gkatzia'}, {'name': 'David M. Howcroft'}, {'name': 'Ondřej Plátek'}, {'name': 'Adarsa Sivaprasad'}]",2024-08-17T11:13:10Z
http://arxiv.org/abs/2408.09150v1,http://arxiv.org/abs/2408.09150v1,CogLM: Tracking Cognitive Development of Large Language Models,"Piaget's Theory of Cognitive Development (PTC) posits that the development of
cognitive levels forms the foundation for human learning across various
abilities. As Large Language Models (LLMs) have recently shown remarkable
abilities across a wide variety of tasks, we are curious about the cognitive
levels of current LLMs: to what extent they have developed and how this
development has been achieved. To this end, we construct a benchmark CogLM
(Cognitive Ability Evaluation for Language Model) based on PTC to assess the
cognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive
abilities crafted by more than 20 human experts, providing a comprehensive
testbed for the cognitive levels of LLMs. Through extensive experiments across
multiple mainstream LLMs with CogLM, we find that: (1) Human-like cognitive
abilities have emerged in advanced LLMs (GPT-4), comparable to those of a
20-year-old human. (2) The parameter size and optimization objective are two
key factors affecting the cognitive levels of LLMs. (3) The performance on
downstream tasks is positively correlated with the level of cognitive
abilities. These findings fill the gap in research on the cognitive abilities
of LLMs, tracing the development of LLMs from a cognitive perspective and
guiding the future direction of their evolution.","[{'name': 'Xinglin Wang'}, {'name': 'Peiwen Yuan'}, {'name': 'Shaoxiong Feng'}, {'name': 'Yiwei Li'}, {'name': 'Boyuan Pan'}, {'name': 'Heda Wang'}, {'name': 'Yao Hu'}, {'name': 'Kan Li'}]",2024-08-17T09:49:40Z
http://arxiv.org/abs/2408.09121v1,http://arxiv.org/abs/2408.09121v1,Selective Prompt Anchoring for Code Generation,"Recent advances in large language models (LLMs) such as Copilot and ChatGPT
have transformed software development by automating coding tasks. Despite these
advancements, challenges remain in reducing error rates and fully meeting user
expectations. Our empirical study reveals LLMs tend to dilute their
self-attention on the initial prompt as more code tokens are generated. We
hypothesize this self-attention dilution issue is one of the root causes of
inaccuracies in LLM-generated code. To mitigate this issue, we propose
Selective Prompt Anchoring (SPA). SPA amplifies the influence of the selected
parts in the initial prompt, which we refer to as ``anchored text'', during
code generation. Specifically, SPA calculates the logit distribution difference
with and without the anchored text. We prove this difference approximates the
anchored text's contextual contribution to the output logits. SPA creates an
augmented logit distribution by linearly combining the original logit
distribution and the logit difference. We evaluate SPA with five LLMs on four
benchmarks. Our results demonstrate that using SPA can consistently improve
Pass@1 rates by up to 9.7% in all settings. Notably, with selective text
anchoring, a small version of DeepSeek-Coder (6.7B) can achieve better
performance than an original much larger version (33B). Our code is available
at https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.","[{'name': 'Yuan Tian'}, {'name': 'Tianyi Zhang'}]",2024-08-17T07:11:02Z
http://arxiv.org/abs/2408.09111v1,http://arxiv.org/abs/2408.09111v1,Measuring Visual Sycophancy in Multimodal Models,"This paper introduces and examines the phenomenon of ""visual sycophancy"" in
multimodal language models, a term we propose to describe these models'
tendency to disproportionately favor visually presented information, even when
it contradicts their prior knowledge or responses. Our study employs a
systematic methodology to investigate this phenomenon: we present models with
images of multiple-choice questions, which they initially answer correctly,
then expose the same model to versions with visually pre-marked options. Our
findings reveal a significant shift in the models' responses towards the
pre-marked option despite their previous correct answers. Comprehensive
evaluations demonstrate that visual sycophancy is a consistent and quantifiable
behavior across various model architectures. Our findings highlight potential
limitations in the reliability of these models when processing potentially
misleading visual information, raising important questions about their
application in critical decision-making contexts.","[{'name': 'Jaehyuk Lim'}, {'name': 'Bruce W. Lee'}]",2024-08-17T06:25:36Z
http://arxiv.org/abs/2408.09075v1,http://arxiv.org/abs/2408.09075v1,Improving Rare Word Translation With Dictionaries and Attention Masking,"In machine translation, rare words continue to be a problem for the dominant
encoder-decoder architecture, especially in low-resource and out-of-domain
translation settings. Human translators solve this problem with monolingual or
bilingual dictionaries. In this paper, we propose appending definitions from a
bilingual dictionary to source sentences and using attention masking to link
together rare words with their definitions. We find that including definitions
for rare words improves performance by up to 1.0 BLEU and 1.6 MacroF1.","[{'name': 'Kenneth J. Sible'}, {'name': 'David Chiang'}]",2024-08-17T02:26:29Z
http://arxiv.org/abs/2408.09070v1,http://arxiv.org/abs/2408.09070v1,"CodeTaxo: Enhancing Taxonomy Expansion with Limited Examples via Code
  Language Prompts","Taxonomies play a crucial role in various applications by providing a
structural representation of knowledge. The task of taxonomy expansion involves
integrating emerging concepts into existing taxonomies by identifying
appropriate parent concepts for these new query concepts. Previous approaches
typically relied on self-supervised methods that generate annotation data from
existing taxonomies. However, these methods are less effective when the
existing taxonomy is small (fewer than 100 entities). In this work, we
introduce \textsc{CodeTaxo}, a novel approach that leverages large language
models through code language prompts to capture the taxonomic structure.
Extensive experiments on five real-world benchmarks from different domains
demonstrate that \textsc{CodeTaxo} consistently achieves superior performance
across all evaluation metrics, significantly outperforming previous
state-of-the-art methods. The code and data are available at
\url{https://github.com/QingkaiZeng/CodeTaxo-Pub}.","[{'name': 'Qingkai Zeng'}, {'name': 'Yuyang Bai'}, {'name': 'Zhaoxuan Tan'}, {'name': 'Zhenyu Wu'}, {'name': 'Shangbin Feng'}, {'name': 'Meng Jiang'}]",2024-08-17T02:15:07Z
http://arxiv.org/abs/2408.09053v1,http://arxiv.org/abs/2408.09053v1,"Learning to Route for Dynamic Adapter Composition in Continual Learning
  with Language Models","Parameter-efficient fine-tuning (PEFT) methods are increasingly used with
pre-trained language models (PLMs) for continual learning (CL). These methods
involve training a PEFT module for each new task and using similarity-based
selection to route modules during inference. However, they face two major
limitations: 1) interference with already learned modules and 2) suboptimal
routing when composing modules. In this paper, we introduce a method that
isolates the training of PEFT modules for task specialization. Then, before
evaluation, it learns to compose the previously learned modules by training a
router that leverages samples from a small memory. We evaluate our method in
two CL setups using several benchmarks. Our results show that our method
provides a better composition of PEFT modules, leading to better generalization
and performance compared to previous methods.","[{'name': 'Vladimir Araujo'}, {'name': 'Marie-Francine Moens'}, {'name': 'Tinne Tuytelaars'}]",2024-08-16T23:57:29Z
http://arxiv.org/abs/2408.09049v1,http://arxiv.org/abs/2408.09049v1,Language Models Show Stable Value Orientations Across Diverse Role-Plays,"We demonstrate that large language models (LLMs) exhibit consistent value
orientations despite adopting diverse personas, revealing a persistent inertia
in their responses that remains stable across the variety of roles they are
prompted to assume. To systematically explore this phenomenon, we introduce the
role-play-at-scale methodology, which involves prompting LLMs with randomized,
diverse personas and analyzing the macroscopic trend of their responses. Unlike
previous works that simply feed these questions to LLMs as if testing human
subjects, our role-play-at-scale methodology diagnoses inherent tendencies in a
systematic and scalable manner by: (1) prompting the model to act in different
random personas and (2) asking the same question multiple times for each random
persona. This approach reveals consistent patterns in LLM responses across
diverse role-play scenarios, indicating deeply encoded inherent tendencies. Our
findings contribute to the discourse on value alignment in foundation models
and demonstrate the efficacy of role-play-at-scale as a diagnostic tool for
uncovering encoded biases in LLMs.","[{'name': 'Bruce W. Lee'}, {'name': 'Yeongheon Lee'}, {'name': 'Hyunsoo Cho'}]",2024-08-16T23:24:10Z
http://arxiv.org/abs/2408.09030v1,http://arxiv.org/abs/2408.09030v1,"Studying the Effects of Collaboration in Interactive Theme Discovery
  Systems","NLP-assisted solutions have gained considerable traction to support
qualitative data analysis. However, there does not exist a unified evaluation
framework that can account for the many different settings in which qualitative
researchers may employ them. In this paper, we take a first step in this
direction by proposing an evaluation framework to study the way in which
different tools may result in different outcomes depending on the collaboration
strategy employed. Specifically, we study the impact of synchronous vs.
asynchronous collaboration using two different NLP-assisted qualitative
research tools and present a comprehensive analysis of significant differences
in the consistency, cohesiveness, and correctness of their outputs.","[{'name': 'Alvin Po-Chun Chen'}, {'name': 'Dananjay Srinivas'}, {'name': 'Alexandra Barry'}, {'name': 'Maksim Seniw'}, {'name': 'Maria Leonor Pacheco'}]",2024-08-16T21:57:23Z
http://arxiv.org/abs/2408.08981v1,http://arxiv.org/abs/2408.08981v1,"From Lazy to Prolific: Tackling Missing Labels in Open Vocabulary
  Extreme Classification by Positive-Unlabeled Sequence Learning","Open-vocabulary Extreme Multi-label Classification (OXMC) extends traditional
XMC by allowing prediction beyond an extremely large, predefined label set
(typically $10^3$ to $10^{12}$ labels), addressing the dynamic nature of
real-world labeling tasks. However, self-selection bias in data annotation
leads to significant missing labels in both training and test data,
particularly for less popular inputs. This creates two critical challenges:
generation models learn to be ""lazy'"" by under-generating labels, and
evaluation becomes unreliable due to insufficient annotation in the test set.
In this work, we introduce Positive-Unlabeled Sequence Learning (PUSL), which
reframes OXMC as an infinite keyphrase generation task, addressing the
generation model's laziness. Additionally, we propose to adopt a suite of
evaluation metrics, F1@$\mathcal{O}$ and newly proposed B@$k$, to reliably
assess OXMC models with incomplete ground truths. In a highly imbalanced
e-commerce dataset with substantial missing labels, PUSL generates 30% more
unique labels, and 72% of its predictions align with actual user queries. On
the less skewed EURLex-4.3k dataset, PUSL demonstrates superior F1 scores,
especially as label counts increase from 15 to 30. Our approach effectively
tackles both the modeling and evaluation challenges in OXMC with missing
labels.","[{'name': 'Haoran Ranran Zhang'}, {'name': 'Bensu Uçar'}, {'name': 'Soumik Dey'}, {'name': 'Hansi Wu'}, {'name': 'Binbin Li'}, {'name': 'Rui Zhang'}]",2024-08-16T19:10:48Z
http://arxiv.org/abs/2408.08978v1,http://arxiv.org/abs/2408.08978v1,"See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering
  LLM Weaknesses","The impressive performance of Large Language Models (LLMs) has consistently
surpassed numerous human-designed benchmarks, presenting new challenges in
assessing the shortcomings of LLMs. Designing tasks and finding LLMs'
limitations are becoming increasingly important. In this paper, we investigate
the question of whether an LLM can discover its own limitations from the errors
it makes. To this end, we propose a Self-Challenge evaluation framework with
human-in-the-loop. Starting from seed instances that GPT-4 fails to answer, we
prompt GPT-4 to summarize error patterns that can be used to generate new
instances and incorporate human feedback on them to refine these patterns for
generating more challenging data, iteratively. We end up with 8 diverse
patterns, such as text manipulation and questions with assumptions. We then
build a benchmark, SC-G4, consisting of 1,835 instances generated by GPT-4
using these patterns, with human-annotated gold responses. The SC-G4 serves as
a challenging benchmark that allows for a detailed assessment of LLMs'
abilities. Our results show that only 44.96\% of instances in SC-G4 can be
answered correctly by GPT-4. Interestingly, our pilot study indicates that
these error patterns also challenge other LLMs, such as Claude-3 and Llama-3,
and cannot be fully resolved through fine-tuning. Our work takes the first step
to demonstrate that LLMs can autonomously identify their inherent flaws and
provide insights for future dynamic and automatic evaluation.","[{'name': 'Yulong Chen'}, {'name': 'Yang Liu'}, {'name': 'Jianhao Yan'}, {'name': 'Xuefeng Bai'}, {'name': 'Ming Zhong'}, {'name': 'Yinghao Yang'}, {'name': 'Ziyi Yang'}, {'name': 'Chenguang Zhu'}, {'name': 'Yue Zhang'}]",2024-08-16T19:01:52Z
http://arxiv.org/abs/2408.08971v1,http://arxiv.org/abs/2408.08971v1,"A Multi-Task and Multi-Label Classification Model for Implicit Discourse
  Relation Recognition","In this work, we address the inherent ambiguity in Implicit Discourse
Relation Recognition (IDRR) by introducing a novel multi-task classification
model capable of learning both multi-label and single-label representations of
discourse relations. Leveraging the DiscoGeM corpus, we train and evaluate our
model on both multi-label and traditional single-label classification tasks. To
the best of our knowledge, our work presents the first truly multi-label
classifier in IDRR, establishing a benchmark for multi-label classification and
achieving SOTA results in single-label classification on DiscoGeM.
Additionally, we evaluate our model on the PDTB 3.0 corpus for single-label
classification without any prior exposure to its data. While the performance is
below the current SOTA, our model demonstrates promising results indicating
potential for effective transfer learning across both corpora.","[{'name': 'Nelson Filipe Costa'}, {'name': 'Leila Kosseim'}]",2024-08-16T18:47:08Z
http://arxiv.org/abs/2408.08964v1,http://arxiv.org/abs/2408.08964v1,"BnSentMix: A Diverse Bengali-English Code-Mixed Dataset for Sentiment
  Analysis","The widespread availability of code-mixed data can provide valuable insights
into low-resource languages like Bengali, which have limited datasets.
Sentiment analysis has been a fundamental text classification task across
several languages for code-mixed data. However, there has yet to be a
large-scale and diverse sentiment analysis dataset on code-mixed Bengali. We
address this limitation by introducing BnSentMix, a sentiment analysis dataset
on code-mixed Bengali consisting of 20,000 samples with $4$ sentiment labels
from Facebook, YouTube, and e-commerce sites. We ensure diversity in data
sources to replicate realistic code-mixed scenarios. Additionally, we propose
$14$ baseline methods including novel transformer encoders further pre-trained
on code-mixed Bengali-English, achieving an overall accuracy of $69.8\%$ and an
F1 score of $69.1\%$ on sentiment classification tasks. Detailed analyses
reveal variations in performance across different sentiment labels and text
types, highlighting areas for future improvement.","[{'name': 'Sadia Alam'}, {'name': 'Md Farhan Ishmam'}, {'name': 'Navid Hasin Alvee'}, {'name': 'Md Shahnewaz Siddique'}, {'name': 'Md Azam Hossain'}, {'name': 'Abu Raihan Mostofa Kamal'}]",2024-08-16T18:30:22Z
http://arxiv.org/abs/2408.08959v1,http://arxiv.org/abs/2408.08959v1,"Adaptive Guardrails For Large Language Models via Trust Modeling and
  In-Context Learning","Guardrails have become an integral part of Large language models (LLMs), by
moderating harmful or toxic response in order to maintain LLMs' alignment to
human expectations. However, the existing guardrail methods do not consider
different needs and access rights of individual users, and treat all the users
with the same rule. This study introduces an adaptive guardrail mechanism,
supported by trust modeling and enhanced with in-context learning, to
dynamically modulate access to sensitive content based on user trust metrics.
By leveraging a combination of direct interaction trust and authority-verified
trust, the system precisely tailors the strictness of content moderation to
align with the user's credibility and the specific context of their inquiries.
Our empirical evaluations demonstrate that the adaptive guardrail effectively
meets diverse user needs, outperforming existing guardrails in practicality
while securing sensitive information and precisely managing potentially
hazardous content through a context-aware knowledge base. This work is the
first to introduce trust-oriented concept within a guardrail system, offering a
scalable solution that enriches the discourse on ethical deployment for
next-generation LLMs.","[{'name': 'Jinwei Hu'}, {'name': 'Yi Dong'}, {'name': 'Xiaowei Huang'}]",2024-08-16T18:07:48Z
http://arxiv.org/abs/2408.08872v1,http://arxiv.org/abs/2408.08872v1,xGen-MM (BLIP-3): A Family of Open Large Multimodal Models,"This report introduces xGen-MM (also known as BLIP-3), a framework for
developing Large Multimodal Models (LMMs). The framework comprises meticulously
curated datasets, a training recipe, model architectures, and a resulting suite
of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen
initiative on foundation AI models. Our models undergo rigorous evaluation
across a range of tasks, including both single and multi-image benchmarks. Our
pre-trained base model exhibits strong in-context learning capabilities and the
instruction-tuned model demonstrates competitive performance among open-source
LMMs with similar model sizes. In addition, we introduce a safety-tuned model
with DPO, aiming to mitigate harmful behaviors such as hallucinations and
improve safety. We open-source our models, curated large-scale datasets, and
our fine-tuning codebase to facilitate further advancements in LMM research.
Associated resources will be available on our project page above.","[{'name': 'Le Xue'}, {'name': 'Manli Shu'}, {'name': 'Anas Awadalla'}, {'name': 'Jun Wang'}, {'name': 'An Yan'}, {'name': 'Senthil Purushwalkam'}, {'name': 'Honglu Zhou'}, {'name': 'Viraj Prabhu'}, {'name': 'Yutong Dai'}, {'name': 'Michael S Ryoo'}, {'name': 'Shrikant Kendre'}, {'name': 'Jieyu Zhang'}, {'name': 'Can Qin'}, {'name': 'Shu Zhang'}, {'name': 'Chia-Chih Chen'}, {'name': 'Ning Yu'}, {'name': 'Juntao Tan'}, {'name': 'Tulika Manoj Awalgaonkar'}, {'name': 'Shelby Heinecke'}, {'name': 'Huan Wang'}, {'name': 'Yejin Choi'}, {'name': 'Ludwig Schmidt'}, {'name': 'Zeyuan Chen'}, {'name': 'Silvio Savarese'}, {'name': 'Juan Carlos Niebles'}, {'name': 'Caiming Xiong'}, {'name': 'Ran Xu'}]",2024-08-16T17:57:01Z
http://arxiv.org/abs/2408.08869v2,http://arxiv.org/abs/2408.08869v2,"PEDAL: Enhancing Greedy Decoding with Large Language Models using
  Diverse Exemplars","Self-ensembling techniques with diverse reasoning paths such as
Self-Consistency have demonstrated remarkable performance gains in text
generation with Large Language Models (LLMs). However, such techniques depend
on the availability of an accurate answer extraction process to aggregate
across multiple outputs. Moreover, they acquire higher inference cost, in
comparison to Greedy Decoding, due to generation of relatively higher number of
output tokens. Research has shown that the free form text outputs from
Self-Consistency can be aggregated reliably using LLMs to produce the final
output. Additionally, recent advancements in LLM inference have demonstrated
that usage of diverse exemplars in prompts have the ability to induce diversity
in the LLM outputs. Such proven techniques can be easily extended to
self-ensembling based approaches to achieve enhanced results in text
generation. In this paper, we introduce PEDAL (Prompts based on Exemplar
Diversity Aggregated using LLMs), a hybrid self-ensembling approach, that
combines the strengths of diverse exemplar based prompts and LLM based
aggregation to achieve improvement in overall performance. On the publicly
available SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve
better accuracy than Greedy Decoding based strategies with lower inference cost
compared to Self Consistency based approaches.",[{'name': 'Sumanth Prabhu'}],2024-08-16T17:54:09Z
http://arxiv.org/abs/2408.08848v1,http://arxiv.org/abs/2408.08848v1,PsychoLex: Unveiling the Psychological Mind of Large Language Models,"This paper explores the intersection of psychology and artificial
intelligence through the development and evaluation of specialized Large
Language Models (LLMs). We introduce PsychoLex, a suite of resources designed
to enhance LLMs' proficiency in psychological tasks in both Persian and
English. Key contributions include the PsychoLexQA dataset for instructional
content and the PsychoLexEval dataset for rigorous evaluation of LLMs in
complex psychological scenarios. Additionally, we present the PsychoLexLLaMA
model, optimized specifically for psychological applications, demonstrating
superior performance compared to general-purpose models. The findings
underscore the potential of tailored LLMs for advancing psychological research
and applications, while also highlighting areas for further refinement. This
research offers a foundational step towards integrating LLMs into specialized
psychological domains, with implications for future advancements in AI-driven
psychological practice.","[{'name': 'Mohammad Amin Abbasi'}, {'name': 'Farnaz Sadat Mirnezami'}, {'name': 'Hassan Naderi'}]",2024-08-16T17:19:23Z
http://arxiv.org/abs/2408.08841v1,http://arxiv.org/abs/2408.08841v1,FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats,"The table reasoning task aims to answer the question according to the given
table. Currently, using Large Language Models (LLMs) is the predominant method
for table reasoning. Most existing methods employ a fixed tabular format to
represent the table, which could limit the performance. Given that each
instance requires different capabilities and models possess varying abilities,
we assert that different instances and models suit different tabular formats.
We prove the aforementioned claim through quantitative analysis of experimental
results, where different instances and models achieve different performances
using various tabular formats. Building on this discussion, we propose
FLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by
employing flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a
classifier to predict the most suitable tabular format based on the instance
and the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.
Our experiments on WikiTableQuestions and TabFact reveal significant
improvements, with average gains of 2.3% and 4.8% compared to the best
performance achieved using a fixed tabular format with greedy decoding and
self-consistency decoding, thereby validating the effectiveness of our methods.","[{'name': 'Xuanliang Zhang'}, {'name': 'Dingzirui Wang'}, {'name': 'Longxu Dou'}, {'name': 'Baoxin Wang'}, {'name': 'Dayong Wu'}, {'name': 'Qingfu Zhu'}, {'name': 'Wanxiang Che'}]",2024-08-16T17:00:11Z
http://arxiv.org/abs/2408.08805v1,http://arxiv.org/abs/2408.08805v1,"CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational
  Dialogue Systems","In this study, we introduce CIKMar, an efficient approach to educational
dialogue systems powered by the Gemma Language model. By leveraging a
Dual-Encoder ranking system that incorporates both BERT and SBERT model, we
have designed CIKMar to deliver highly relevant and accurate responses, even
with the constraints of a smaller language model size. Our evaluation reveals
that CIKMar achieves a robust recall and F1-score of 0.70 using BERTScore
metrics. However, we have identified a significant challenge: the Dual-Encoder
tends to prioritize theoretical responses over practical ones. These findings
underscore the potential of compact and efficient models like Gemma in
democratizing access to advanced educational AI systems, ensuring effective and
contextually appropriate responses.","[{'name': 'Joanito Agili Lopo'}, {'name': 'Marina Indah Prasasti'}, {'name': 'Alma Permatasari'}]",2024-08-16T15:29:54Z
http://arxiv.org/abs/2408.08803v1,http://arxiv.org/abs/2408.08803v1,"Leveraging FourierKAN Classification Head for Pre-Trained
  Transformer-based Text Classification","For many years, transformer-based pre-trained models with Multi-layer
Perceptron (MLP) heads have been the standard for text classification tasks.
However, the fixed non-linear functions employed by MLPs often fall short of
capturing the intricacies of the contextualized embeddings produced by
pre-trained encoders. Furthermore, MLPs usually require a significant number of
training parameters, which can be computationally expensive. In this work, we
introduce FourierKAN (FR-KAN), a variant of the promising MLP alternative
called Kolmogorov-Arnold Networks (KANs), as classification heads for
transformer-based encoders. Our studies reveal an average increase of 10% in
accuracy and 11% in F1-score when incorporating FR-KAN heads instead of
traditional MLP heads for several transformer-based pre-trained models across
multiple text classification tasks. Beyond improving model accuracy, FR-KAN
heads train faster and require fewer parameters. Our research opens new grounds
for broader applications of KAN across several Natural Language Processing
(NLP) tasks.","[{'name': 'Abdullah Al Imran'}, {'name': 'Md Farhan Ishmam'}]",2024-08-16T15:28:02Z
http://arxiv.org/abs/2408.08782v1,http://arxiv.org/abs/2408.08782v1,"EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling
  MiXed Emotions and Discourse Dynamics","Designing emotionally intelligent conversational systems to provide comfort
and advice to people experiencing distress is a compelling area of research.
Previous efforts have focused on developing modular dialogue systems that treat
socio-emotional strategy prediction as an auxiliary task and generate
strategy-conditioned responses with customized decoders. Recently, with
advancements in large language models (LLMs), end-to-end dialogue agents
without explicit socio-emotional strategy prediction steps have become
prevalent. However, despite their excellence in language generation, recent
studies show that LLMs' inherent preference bias towards certain
socio-emotional strategies hinders the delivery of high-quality emotional
support. To address this challenge, we propose decoupling strategy prediction
from language generation, and introduce a novel dialogue strategy predictor,
EmoDynamiX, which models the discourse dynamics between user emotions and
system strategies using a heterogeneous graph. Additionally, we make use of the
Emotion Recognition in Conversations (ERC) task and design a flexible
mixed-emotion module to capture fine-grained emotional states of the user.
Experimental results on two ESC datasets show EmoDynamiX outperforms previous
state-of-the-art methods with a significant margin.","[{'name': 'Chenwei Wan'}, {'name': 'Matthieu Labeau'}, {'name': 'Chloé Clavel'}]",2024-08-16T14:54:41Z
http://arxiv.org/abs/2408.08781v1,http://arxiv.org/abs/2408.08781v1,"Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation
  Instructions","LLMs-as-a-judge is a recently popularized method which replaces human
judgements in task evaluation (Zheng et al. 2024) with automatic evaluation
using LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human
Feedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have
strong alignment with human preferences when prompted for a quality judgement,
such as the coherence of a text. While this seems beneficial, it is not clear
whether the assessments by an LLM-as-a-judge constitute only an evaluation
based on the instructions in the prompts, or reflect its preference for
high-quality data similar to its fine-tune data. To investigate how much
influence prompting the LLMs-as-a-judge has on the alignment of AI judgements
to human judgements, we analyze prompts with increasing levels of instructions
about the target quality of an evaluation, for several LLMs-as-a-judge.
Further, we compare to a prompt-free method using model perplexity as a quality
measure instead. We aggregate a taxonomy of quality criteria commonly used
across state-of-the-art evaluations with LLMs and provide this as a rigorous
benchmark of models as judges. Overall, we show that the LLMs-as-a-judge
benefit only little from highly detailed instructions in prompts and that
perplexity can sometimes align better with human judgements than prompting,
especially on textual quality.","[{'name': 'Bhuvanashree Murugadoss'}, {'name': 'Christian Poelitz'}, {'name': 'Ian Drosos'}, {'name': 'Vu Le'}, {'name': 'Nick McKenna'}, {'name': 'Carina Suzana Negreanu'}, {'name': 'Chris Parnin'}, {'name': 'Advait Sarkar'}]",2024-08-16T14:49:35Z
http://arxiv.org/abs/2408.08780v1,http://arxiv.org/abs/2408.08780v1,"Large Language Models Might Not Care What You Are Saying: Prompt Format
  Beats Descriptions","With the help of in-context learning (ICL), large language models (LLMs) have
achieved impressive performance across various tasks. However, the function of
descriptive instructions during ICL remains under-explored. In this work, we
propose an ensemble prompt framework to describe the selection criteria of
multiple in-context examples, and preliminary experiments on machine
translation (MT) across six translation directions confirm that this framework
boosts ICL perfromance. But to our surprise, LLMs might not necessarily care
what the descriptions actually say, and the performance gain is primarily
caused by the ensemble format, since the framework could lead to improvement
even with random descriptive nouns. We further apply this new ensemble prompt
on a range of commonsense, math, logical reasoning and hallucination tasks with
three LLMs and achieve promising results, suggesting again that designing a
proper prompt format would be much more effective and efficient than paying
effort into specific descriptions. Our code will be publicly available once
this paper is published.","[{'name': 'Chenming Tang'}, {'name': 'Zhixiang Wang'}, {'name': 'Yunfang Wu'}]",2024-08-16T14:49:04Z
http://arxiv.org/abs/2408.08779v1,http://arxiv.org/abs/2408.08779v1,DAC: Decomposed Automation Correction for Text-to-SQL,"Text-to-SQL is an important task that helps people obtain information from
databases by automatically generating SQL queries. Considering the brilliant
performance, approaches based on Large Language Models (LLMs) become the
mainstream for text-to-SQL. Among these approaches, automated correction is an
effective approach that further enhances performance by correcting the mistakes
in the generated results. The existing correction methods require LLMs to
directly correct with generated SQL, while previous research shows that LLMs do
not know how to detect mistakes, leading to poor performance. Therefore, in
this paper, we propose to employ the decomposed correction to enhance
text-to-SQL performance. We first demonstrate that decomposed correction
outperforms direct correction since detecting and fixing mistakes with the
results of the decomposed sub-tasks is easier than with SQL. Based on this
analysis, we introduce Decomposed Automation Correction (DAC), which corrects
SQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC
first generates the entity and skeleton corresponding to the question and then
compares the differences between the initial SQL and the generated entities and
skeleton as feedback for correction. Experimental results show that our method
improves performance by $3.7\%$ on average of Spider, Bird, and KaggleDBQA
compared with the baseline method, demonstrating the effectiveness of DAC.","[{'name': 'Dingzirui Wang'}, {'name': 'Longxu Dou'}, {'name': 'Xuanliang Zhang'}, {'name': 'Qingfu Zhu'}, {'name': 'Wanxiang Che'}]",2024-08-16T14:43:15Z
http://arxiv.org/abs/2408.08769v1,http://arxiv.org/abs/2408.08769v1,"Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion
  Contrastive Decoding with Truthfulness Refocused","Large Language Models (LLMs) have demonstrated exceptional performance across
various natural language processing tasks, yet they occasionally tend to yield
content that factually inaccurate or discordant with the expected output, a
phenomenon empirically referred to as ""hallucination"". To tackle this issue,
recent works have investigated contrastive decoding between the original model
and an amateur model with induced hallucination, which has shown promising
results. Nonetheless, this method may undermine the output distribution of the
original LLM caused by its coarse contrast and simplistic subtraction
operation, potentially leading to errors in certain cases. In this paper, we
introduce a novel contrastive decoding framework termed LOL (LOwer Layer
Matters). Our approach involves concatenating the contrastive decoding of both
the final and lower layers between the original model and the amateur model,
thereby achieving multi-layer fusion to aid in the mitigation of hallucination.
Additionally, we incorporate a truthfulness refocused module that leverages
contextual guidance to enhance factual encoding, further capturing truthfulness
during contrastive decoding. Extensive experiments conducted on two publicly
available datasets illustrate that our proposed LOL framework can substantially
alleviate hallucination while surpassing existing baselines in most cases.
Compared with the best baseline, we improve by average 4.5 points on all
metrics of TruthfulQA. The source code is coming soon.","[{'name': 'Dingwei Chen'}, {'name': 'Feiteng Fang'}, {'name': 'Shiwen Ni'}, {'name': 'Feng Liang'}, {'name': 'Ruifeng Xu'}, {'name': 'Min Yang'}, {'name': 'Chengming Li'}]",2024-08-16T14:23:59Z
http://arxiv.org/abs/2408.08729v1,http://arxiv.org/abs/2408.08729v1,"ConcateNet: Dialogue Separation Using Local And Global Feature
  Concatenation","Dialogue separation involves isolating a dialogue signal from a mixture, such
as a movie or a TV program. This can be a necessary step to enable dialogue
enhancement for broadcast-related applications. In this paper, ConcateNet for
dialogue separation is proposed, which is based on a novel approach for
processing local and global features aimed at better generalization for
out-of-domain signals. ConcateNet is trained using a noise reduction-focused,
publicly available dataset and evaluated using three datasets: two noise
reduction-focused datasets (in-domain), which show competitive performance for
ConcateNet, and a broadcast-focused dataset (out-of-domain), which verifies the
better generalization performance for the proposed architecture compared to
considered state-of-the-art noise-reduction methods.","[{'name': 'Mhd Modar Halimeh'}, {'name': 'Matteo Torcoli'}, {'name': 'Emanuël Habets'}]",2024-08-16T13:22:55Z
http://arxiv.org/abs/2408.08724v1,http://arxiv.org/abs/2408.08724v1,"ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target
  Language","Although large language models(LLMs) show amazing capabilities, among various
exciting applications discovered for LLMs fall short in other low-resource
languages. Besides, most existing methods depend on large-scale dialogue
corpora and thus building systems for dialogue generation in a zero-shot
scenario remains a considerable challenge. To address this challenge, we
propose a novel end-to-end zero-shot dialogue generation model ChatZero based
on cross-lingual code-switching method. First, we construct code-switching
language and pseudo-target language with placeholders. Then for cross-lingual
semantic transfer, we employ unsupervised contrastive learning to minimize the
semantics gap of the source language, code-switching language, and
pseudo-target language that are mutually positive examples in the high
dimensional semantic space. Experiments on the multilingual DailyDialog and
DSTC7-AVSD datasets demonstrate that ChatZero can achieve more than 90\% of the
original performance under the zero-shot case compared to supervised learning,
and achieve state-of-the-art performance compared with other baselines.","[{'name': 'Yongkang Liu'}, {'name': 'Feng Shi'}, {'name': 'Daling Wang'}, {'name': 'Yifei Zhang'}, {'name': 'Hinrich Schütze'}]",2024-08-16T13:11:53Z
http://arxiv.org/abs/2408.08696v1,http://arxiv.org/abs/2408.08696v1,"Turning Trash into Treasure: Accelerating Inference of Large Language
  Models with Token Recycling","The rapid growth in the parameters of large language models (LLMs) has made
inference latency a fundamental bottleneck, limiting broader application of
LLMs. Speculative decoding represents a lossless approach to accelerate
inference through a guess-and-verify paradigm, leveraging the parallel
capabilities of modern hardware. Some speculative decoding methods rely on
additional structures to guess draft tokens, such as small models or
parameter-efficient architectures, which need extra training before use.
Alternatively, retrieval-based train-free techniques build libraries from
pre-existing corpora or by n-gram generation. However, they face challenges
like large storage requirements, time-consuming retrieval, and limited
adaptability. Observing that candidate tokens generated during the decoding
process are likely to reoccur in future sequences, we propose Token Recycling.
This approach stores candidate tokens in an adjacency matrix and employs a
breadth-first search (BFS)-like algorithm on the matrix to construct a draft
tree. The tree is then validated through tree attention. New candidate tokens
from the decoding process are then used to update the matrix. Token Recycling
requires \textless2MB of additional storage and achieves approximately 2x
speedup across all sizes of LLMs. It significantly outperforms existing
train-free methods by 30\% and even a training method by 25\%. It can be
directly applied to any existing LLMs and tasks without the need for
adaptation.","[{'name': 'Xianzhen Luo'}, {'name': 'Yixuan Wang'}, {'name': 'Qingfu Zhu'}, {'name': 'Zhiming Zhang'}, {'name': 'Xuanyu Zhang'}, {'name': 'Qing Yang'}, {'name': 'Dongliang Xu'}, {'name': 'Wanxiang Che'}]",2024-08-16T12:20:56Z
http://arxiv.org/abs/2408.08694v1,http://arxiv.org/abs/2408.08694v1,"Quantifying the Effectiveness of Student Organization Activities using
  Natural Language Processing","Student extracurricular activities play an important role in enriching the
students' educational experiences. With the increasing popularity of Machine
Learning and Natural Language Processing, it becomes a logical step that
incorporating ML-NLP in improving extracurricular activities is a potential
focus of study in Artificial Intelligence (AI). This research study aims to
develop a machine learning workflow that will quantify the effectiveness of
student-organized activities based on student emotional responses using
sentiment analysis. The study uses the Bidirectional Encoder Representations
from Transformers (BERT) Large Language Model (LLM) called via the
pysentimiento toolkit, as a Transformer pipeline in Hugging Face. A sample data
set from Organization C, a Recognized Student Organization (RSO) of a higher
educational institute in the Philippines, College X, was used to develop the
workflow. The workflow consisted of data preprocessing, key feature selection,
LLM feature processing, and score aggregation, resulting in an Event Score for
each data set. The results show that the BERT LLM can also be used effectively
in analyzing sentiment beyond product reviews and post comments. For the
student affairs offices of educational institutions, this study can provide a
practical example of how NLP can be applied to real-world scenarios, showcasing
the potential impact of data-driven decision making.","[{'name': 'Lyberius Ennio F. Taruc'}, {'name': 'Arvin R. De La Cruz'}]",2024-08-16T12:16:59Z
http://arxiv.org/abs/2408.08693v1,http://arxiv.org/abs/2408.08693v1,"Med-PMC: Medical Personalized Multi-modal Consultation with a Proactive
  Ask-First-Observe-Next Paradigm","The application of the Multi-modal Large Language Models (MLLMs) in medical
clinical scenarios remains underexplored. Previous benchmarks only focus on the
capacity of the MLLMs in medical visual question-answering (VQA) or report
generation and fail to assess the performance of the MLLMs on complex clinical
multi-modal tasks. In this paper, we propose a novel Medical Personalized
Multi-modal Consultation (Med-PMC) paradigm to evaluate the clinical capacity
of the MLLMs. Med-PMC builds a simulated clinical environment where the MLLMs
are required to interact with a patient simulator to complete the multi-modal
information-gathering and decision-making task. Specifically, the patient
simulator is decorated with personalized actors to simulate diverse patients in
real scenarios. We conduct extensive experiments to access 12 types of MLLMs,
providing a comprehensive view of the MLLMs' clinical performance. We found
that current MLLMs fail to gather multimodal information and show potential
bias in the decision-making task when consulted with the personalized patient
simulators. Further analysis demonstrates the effectiveness of Med-PMC, showing
the potential to guide the development of robust and reliable clinical MLLMs.
Code and data are available at https://github.com/LiuHC0428/Med-PMC.","[{'name': 'Hongcheng Liu'}, {'name': 'Yusheng Liao'}, {'name': 'Siqv Ou'}, {'name': 'Yuhao Wang'}, {'name': 'Heyang Liu'}, {'name': 'Yanfeng Wang'}, {'name': 'Yu Wang'}]",2024-08-16T12:14:55Z
http://arxiv.org/abs/2408.08688v1,http://arxiv.org/abs/2408.08688v1,"The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
  Preference Optimization Dataset Generation","This paper presents and evaluates multi-agent workflows for synthetic
Preference Optimization (PO) dataset generation. PO dataset generation requires
two modules: (1) response evaluation, and (2) response generation. In the
response evaluation module, the responses from Large Language Models (LLMs) are
evaluated and ranked - a task typically carried out by human annotators that we
automate using LLMs. We assess the response evaluation module in a 2 step
process. In step 1, we assess LLMs as evaluators using three distinct prompting
strategies. In step 2, we apply the winning prompting strategy to compare the
performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In each step, we
use inter-rater agreement using Cohen's Kappa between human annotators and
LLMs. For the response generation module, we compare different configurations
for the LLM Feedback Loop using the identified LLM evaluator configuration. We
use the win rate (the fraction of times a generation framework is selected as
the best by an LLM evaluator) to determine the best multi-agent configuration
for generation. After identifying the best configurations for both modules, we
use models from the GPT, Gemma, and Llama families to generate our PO datasets
using the above pipeline. We generate two types of PO datasets, one to improve
the generation capabilities of individual LLM and the other to improve the
multi-agent workflow. Our evaluation shows that GPT-4o-as-a-Judge is more
consistent across datasets when the candidate responses do not include
responses from the GPT family. Additionally, we find that the LLM Feedback
Loop, with Llama as the generator and Gemma as the reviewer, achieves a notable
71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively.","[{'name': 'Samee Arif'}, {'name': 'Sualeha Farid'}, {'name': 'Abdul Hameed Azeemi'}, {'name': 'Awais Athar'}, {'name': 'Agha Ali Raza'}]",2024-08-16T12:01:55Z
http://arxiv.org/abs/2408.08682v1,http://arxiv.org/abs/2408.08682v1,LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression,"The key to effective point cloud compression is to obtain a robust context
model consistent with complex 3D data structures. Recently, the advancement of
large language models (LLMs) has highlighted their capabilities not only as
powerful generators for in-context learning and generation but also as
effective compressors. These dual attributes of LLMs make them particularly
well-suited to meet the demands of data compression. Therefore, this paper
explores the potential of using LLM for compression tasks, focusing on lossless
point cloud geometry compression (PCGC) experiments. However, applying LLM
directly to PCGC tasks presents some significant challenges, i.e., LLM does not
understand the structure of the point cloud well, and it is a difficult task to
fill the gap between text and point cloud through text description, especially
for large complicated and small shapeless point clouds. To address these
problems, we introduce a novel architecture, namely the Large Language
Model-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to
compress point cloud geometry information without any text description or
aligning operation. By utilizing different adaptation techniques for
cross-modality representation alignment and semantic consistency, including
clustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA),
the proposed method can translate LLM to a compressor/generator for point
cloud. To the best of our knowledge, this is the first structure to employ LLM
as a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC
outperforms the other existing methods significantly, by achieving -40.213% bit
rate reduction compared to the reference software of MPEG Geometry-based Point
Cloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction
compared to the state-of-the-art learning-based method.","[{'name': 'Yuqi Ye'}, {'name': 'Wei Gao'}]",2024-08-16T11:55:44Z
http://arxiv.org/abs/2408.08661v1,http://arxiv.org/abs/2408.08661v1,MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector,"The increasing parameters and expansive dataset of large language models
(LLMs) highlight the urgent demand for a technical solution to audit the
underlying privacy risks and copyright issues associated with LLMs. Existing
studies have partially addressed this need through an exploration of the
pre-training data detection problem, which is an instance of a membership
inference attack (MIA). This problem involves determining whether a given piece
of text has been used during the pre-training phase of the target LLM. Although
existing methods have designed various sophisticated MIA score functions to
achieve considerable detection performance in pre-trained LLMs, how to achieve
high-confidence detection and how to perform MIA on aligned LLMs remain
challenging. In this paper, we propose MIA-Tuner, a novel instruction-based MIA
method, which instructs LLMs themselves to serve as a more precise pre-training
data detector internally, rather than design an external MIA score function.
Furthermore, we design two instruction-based safeguards to respectively
mitigate the privacy risks brought by the existing methods and MIA-Tuner. To
comprehensively evaluate the most recent state-of-the-art LLMs, we collect a
more up-to-date MIA benchmark dataset, named WIKIMIA-24, to replace the widely
adopted benchmark WIKIMIA. We conduct extensive experiments across various
aligned and unaligned LLMs over the two benchmark datasets. The results
demonstrate that MIA-Tuner increases the AUC of MIAs from 0.7 to a
significantly high level of 0.9.","[{'name': 'Wenjie Fu'}, {'name': 'Huandong Wang'}, {'name': 'Chen Gao'}, {'name': 'Guanghua Liu'}, {'name': 'Yong Li'}, {'name': 'Tao Jiang'}]",2024-08-16T11:09:56Z
http://arxiv.org/abs/2408.08656v1,http://arxiv.org/abs/2408.08656v1,"LLMs Are Biased Towards Output Formats! Systematically Evaluating and
  Mitigating Output Format Bias of LLMs","We present the first systematic evaluation examining format bias in
performance of large language models (LLMs). Our approach distinguishes between
two categories of an evaluation metric under format constraints to reliably and
accurately assess performance: one measures performance when format constraints
are adhered to, while the other evaluates performance regardless of constraint
adherence. We then define a metric for measuring the format bias of LLMs and
establish effective strategies to reduce it. Subsequently, we present our
empirical format bias evaluation spanning four commonly used categories --
multiple-choice question-answer, wrapping, list, and mapping -- covering 15
widely-used formats. Our evaluation on eight generation tasks uncovers
significant format bias across state-of-the-art LLMs. We further discover that
improving the format-instruction following capabilities of LLMs across formats
potentially reduces format bias. Based on our evaluation findings, we study
prompting and fine-tuning with synthesized format data techniques to mitigate
format bias. Our methods successfully reduce the variance in ChatGPT's
performance among wrapping formats from 235.33 to 0.71 (%$^2$).","[{'name': 'Do Xuan Long'}, {'name': 'Hai Nguyen Ngoc'}, {'name': 'Tiviatis Sim'}, {'name': 'Hieu Dao'}, {'name': 'Shafiq Joty'}, {'name': 'Kenji Kawaguchi'}, {'name': 'Nancy F. Chen'}, {'name': 'Min-Yen Kan'}]",2024-08-16T10:45:45Z
http://arxiv.org/abs/2408.08651v1,http://arxiv.org/abs/2408.08651v1,"Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of
  Thought Reasoning","Language models are known to absorb biases from their training data, leading
to predictions driven by statistical regularities rather than semantic
relevance. We investigate the impact of these biases on answer choice
preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our
findings reveal that differences in learned regularities across answer options
are predictive of model preferences and mirror human test-taking strategies. To
address this issue, we introduce two novel methods: Counterfactual Prompting
with Chain of Thought (CoT) and Counterfactual Prompting with Agnostically
Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with
CoT alone is insufficient to mitigate bias, our novel Primed Counterfactual
Prompting with CoT approach effectively reduces the influence of base-rate
probabilities while improving overall accuracy. Our results suggest that
mitigating bias requires a ""System-2"" like process and that CoT reasoning is
susceptible to confirmation bias under some prompting methodologies. Our
contributions offer practical solutions for developing more robust and fair
language models.","[{'name': 'Kyle Moore'}, {'name': 'Jesse Roberts'}, {'name': 'Thao Pham'}, {'name': 'Douglas Fisher'}]",2024-08-16T10:34:50Z
http://arxiv.org/abs/2408.08650v1,http://arxiv.org/abs/2408.08650v1,An End-to-End Model for Photo-Sharing Multi-modal Dialogue Generation,"Photo-Sharing Multi-modal dialogue generation requires a dialogue agent not
only to generate text responses but also to share photos at the proper moment.
Using image text caption as the bridge, a pipeline model integrates an image
caption model, a text generation model, and an image generation model to handle
this complex multi-modal task. However, representing the images with text
captions may loss important visual details and information and cause error
propagation in the complex dialogue system. Besides, the pipeline model
isolates the three models separately because discrete image text captions
hinder end-to-end gradient propagation. We propose the first end-to-end model
for photo-sharing multi-modal dialogue generation, which integrates an image
perceptron and an image generator with a large language model. The large
language model employs the Q-Former to perceive visual images in the input end.
For image generation in the output end, we propose a dynamic vocabulary
transformation matrix and use straight-through and gumbel-softmax techniques to
align the large language model and stable diffusion model and achieve
end-to-end gradient propagation. We perform experiments on PhotoChat and
DialogCC datasets to evaluate our end-to-end model. Compared with pipeline
models, the end-to-end model gains state-of-the-art performances on various
metrics of text and image generation. More analysis experiments also verify the
effectiveness of the end-to-end model for photo-sharing multi-modal dialogue
generation.","[{'name': 'Peiming Guo'}, {'name': 'Sinuo Liu'}, {'name': 'Yanzhao Zhang'}, {'name': 'Dingkun Long'}, {'name': 'Pengjun Xie'}, {'name': 'Meishan Zhang'}, {'name': 'Min Zhang'}]",2024-08-16T10:33:19Z
http://arxiv.org/abs/2408.08648v1,http://arxiv.org/abs/2408.08648v1,"Understanding Enthymemes in Argument Maps: Bridging Argument Mining and
  Logic-based Argumentation","Argument mining is natural language processing technology aimed at
identifying arguments in text. Furthermore, the approach is being developed to
identify the premises and claims of those arguments, and to identify the
relationships between arguments including support and attack relationships. In
this paper, we assume that an argument map contains the premises and claims of
arguments, and support and attack relationships between them, that have been
identified by argument mining. So from a piece of text, we assume an argument
map is obtained automatically by natural language processing. However, to
understand and to automatically analyse that argument map, it would be
desirable to instantiate that argument map with logical arguments. Once we have
the logical representation of the arguments in an argument map, we can use
automated reasoning to analyze the argumentation (e.g. check consistency of
premises, check validity of claims, and check the labelling on each arc
corresponds with thw logical arguments). We address this need by using
classical logic for representing the explicit information in the text, and
using default logic for representing the implicit information in the text. In
order to investigate our proposal, we consider some specific options for
instantiation.","[{'name': 'Jonathan Ben-Naim'}, {'name': 'Victor David'}, {'name': 'Anthony Hunter'}]",2024-08-16T10:30:30Z
http://arxiv.org/abs/2408.08640v1,http://arxiv.org/abs/2408.08640v1,"Math-PUMA: Progressive Upward Multimodal Alignment to Enhance
  Mathematical Reasoning","Multimodal Large Language Models (MLLMs) excel in solving text-based
mathematical problems, but they struggle with mathematical diagrams since they
are primarily trained on natural scene images. For humans, visual aids
generally enhance problem-solving, but MLLMs perform worse as information
shifts from textual to visual modality. This decline is mainly due to their
shortcomings in aligning images and text. To tackle aforementioned challenges,
we propose Math-PUMA, a methodology focused on Progressive Upward Multimodal
Alignment. This approach is designed to improve the mathematical reasoning
skills of MLLMs through a three-stage training process, with the second stage
being the critical alignment stage. We first enhance the language model's
mathematical reasoning capabilities with extensive set of textual mathematical
problems. We then construct a multimodal dataset with varying degrees of
textual and visual information, creating data pairs by presenting each problem
in at least two forms. By leveraging the Kullback-Leibler (KL) divergence of
next-token prediction distributions to align visual and textual modalities,
consistent problem-solving abilities are ensured. Finally, we utilize
multimodal instruction tuning for MLLMs with high-quality multimodal data.
Experimental results on multiple mathematical reasoning benchmarks demonstrate
that the MLLMs trained with Math-PUMA surpass most open-source MLLMs. Our
approach effectively narrows the performance gap for problems presented in
different modalities.","[{'name': 'Wenwen Zhuang'}, {'name': 'Xin Huang'}, {'name': 'Xiantao Zhang'}, {'name': 'Jin Zeng'}]",2024-08-16T10:11:05Z
http://arxiv.org/abs/2408.08632v1,http://arxiv.org/abs/2408.08632v1,A Survey on Benchmarks of Multimodal Large Language Models,"Multimodal Large Language Models (MLLMs) are gaining increasing popularity in
both academia and industry due to their remarkable performance in various
applications such as visual question answering, visual perception,
understanding, and reasoning. Over the past few years, significant efforts have
been made to examine MLLMs from multiple perspectives. This paper presents a
comprehensive review of \textbf{180 benchmarks} and evaluation for MLLMs,
focusing on (1)perception and understanding, (2)cognition and reasoning,
(3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we
discuss the limitations of the current evaluation methods for MLLMs and explore
promising future directions. Our key argument is that evaluation should be
regarded as a crucial discipline to better support the development of MLLMs.
For more details, please visit our GitHub repository:
https://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.","[{'name': 'Jian Li'}, {'name': 'Weiheng Lu'}]",2024-08-16T09:52:02Z
http://arxiv.org/abs/2408.08631v1,http://arxiv.org/abs/2408.08631v1,"Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by
  Ensembling the Role-playing and Neutral Prompts","Recent studies demonstrate that prompting an appropriate role-playing persona
to an LLM improves its reasoning capability. However, assigning a proper
persona is difficult since an LLM's performance is extremely sensitive to
assigned prompts; therefore, personas sometimes hinder LLMs and degrade their
reasoning capabilities. In this paper, we propose a novel framework, Jekyll \&
Hyde, which ensembles the results of role-playing and neutral prompts to
eradicate performance degradation via unilateral use of role-playing prompted
LLM and enhance the robustness of an LLM's reasoning ability. Specifically,
Jekyll \& Hyde collects two potential solutions from both role-playing and
neutral prompts and selects a better solution after cross-checking via an LLM
evaluator. However, LLM-based evaluators tend to be affected by the order of
those potential solutions within the prompt when selecting the proper solution;
thus, we also propose a robust LLM evaluator to mitigate the position bias. The
experimental analysis demonstrates that role-playing prompts distract LLMs and
degrade their reasoning abilities in 4 out of 12 datasets, even when using
GPT-4. In addition, we reveal that Jekyll \& Hyde improves reasoning
capabilities by selecting better choices among the potential solutions on
twelve widely-used reasoning datasets. We further show that our proposed LLM
evaluator outperforms other baselines, proving the LLMs' position bias is
successfully mitigated.","[{'name': 'Junseok Kim'}, {'name': 'Nakyeong Yang'}, {'name': 'Kyomin Jung'}]",2024-08-16T09:49:51Z
http://arxiv.org/abs/2408.08624v1,http://arxiv.org/abs/2408.08624v1,"RealMedQA: A pilot biomedical question answering dataset containing
  realistic clinical questions","Clinical question answering systems have the potential to provide clinicians
with relevant and timely answers to their questions. Nonetheless, despite the
advances that have been made, adoption of these systems in clinical settings
has been slow. One issue is a lack of question-answering datasets which reflect
the real-world needs of health professionals. In this work, we present
RealMedQA, a dataset of realistic clinical questions generated by humans and an
LLM. We describe the process for generating and verifying the QA pairs and
assess several QA models on BioASQ and RealMedQA to assess the relative
difficulty of matching answers to questions. We show that the LLM is more
cost-efficient for generating ""ideal"" QA pairs. Additionally, we achieve a
lower lexical similarity between questions and answers than BioASQ which
provides an additional challenge to the top two QA models, as per the results.
We release our code and our dataset publicly to encourage further research.","[{'name': 'Gregory Kell'}, {'name': 'Angus Roberts'}, {'name': 'Serge Umansky'}, {'name': 'Yuti Khare'}, {'name': 'Najma Ahmed'}, {'name': 'Nikhil Patel'}, {'name': 'Chloe Simela'}, {'name': 'Jack Coumbe'}, {'name': 'Julian Rozario'}, {'name': 'Ryan-Rhys Griffiths'}, {'name': 'Iain J. Marshall'}]",2024-08-16T09:32:43Z
http://arxiv.org/abs/2408.08590v1,http://arxiv.org/abs/2408.08590v1,"A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive
  Language Models","Recent studies on logical reasoning in auto-regressive Language Models (LMs)
have sparked a debate on whether such models can learn systematic reasoning
principles during pre-training or merely exploit superficial patterns in the
training data. This paper presents a mechanistic interpretation of syllogistic
reasoning in LMs to further enhance our understanding of internal dynamics.
Specifically, we present a methodology for circuit discovery aimed at
disentangling content-independent reasoning mechanisms from world knowledge
acquired during pre-training. Through two distinct intervention methods, we
uncover a sufficient and necessary circuit involving middle-term suppression
that elucidates how LMs transfer information to derive valid conclusions from
premises. Furthermore, we investigate how belief biases manifest in syllogistic
reasoning, finding evidence of partial contamination from additional attention
heads responsible for encoding commonsense and contextualized knowledge.
Finally, we explore the generalization of the discovered mechanisms across
various syllogistic schemes and model sizes, finding that the identified
circuit is sufficient and necessary for all the schemes on which the model
achieves high downstream accuracy ($\geq$ 60\%). Overall, our findings suggest
that LMs indeed learn transferable content-independent reasoning mechanisms,
but that, at the same time, such mechanisms do not involve generalisable and
abstract logical primitives, being susceptible to contamination by the same
world knowledge acquired during pre-training.","[{'name': 'Geonhee Kim'}, {'name': 'Marco Valentino'}, {'name': 'André Freitas'}]",2024-08-16T07:47:39Z
http://arxiv.org/abs/2408.08566v1,http://arxiv.org/abs/2408.08566v1,"Overview of the BioLaySumm 2024 Shared Task on the Lay Summarization of
  Biomedical Research Articles","This paper presents the setup and results of the second edition of the
BioLaySumm shared task on the Lay Summarisation of Biomedical Research
Articles, hosted at the BioNLP Workshop at ACL 2024. In this task edition, we
aim to build on the first edition's success by further increasing research
interest in this important task and encouraging participants to explore novel
approaches that will help advance the state-of-the-art. Encouragingly, we found
research interest in the task to be high, with this edition of the task
attracting a total of 53 participating teams, a significant increase in
engagement from the previous edition. Overall, our results show that a broad
range of innovative approaches were adopted by task participants, with a
predictable shift towards the use of Large Language Models (LLMs).","[{'name': 'Tomas Goldsack'}, {'name': 'Carolina Scarton'}, {'name': 'Matthew Shardlow'}, {'name': 'Chenghua Lin'}]",2024-08-16T07:00:08Z
http://arxiv.org/abs/2408.08564v1,http://arxiv.org/abs/2408.08564v1,"Collaborative Cross-modal Fusion with Large Language Model for
  Recommendation","Despite the success of conventional collaborative filtering (CF) approaches
for recommendation systems, they exhibit limitations in leveraging semantic
knowledge within the textual attributes of users and items. Recent focus on the
application of large language models for recommendation (LLM4Rec) has
highlighted their capability for effective semantic knowledge capture. However,
these methods often overlook the collaborative signals in user behaviors. Some
simply instruct-tune a language model, while others directly inject the
embeddings of a CF-based model, lacking a synergistic fusion of different
modalities. To address these issues, we propose a framework of Collaborative
Cross-modal Fusion with Large Language Models, termed CCF-LLM, for
recommendation. In this framework, we translate the user-item interactions into
a hybrid prompt to encode both semantic knowledge and collaborative signals,
and then employ an attentive cross-modal fusion strategy to effectively fuse
latent embeddings of both modalities. Extensive experiments demonstrate that
CCF-LLM outperforms existing methods by effectively utilizing semantic and
collaborative signals in the LLM4Rec context.","[{'name': 'Zhongzhou Liu'}, {'name': 'Hao Zhang'}, {'name': 'Kuicai Dong'}, {'name': 'Yuan Fang'}]",2024-08-16T06:54:10Z
http://arxiv.org/abs/2408.08551v1,http://arxiv.org/abs/2408.08551v1,"Integrating Multi-view Analysis: Multi-view Mixture-of-Expert for
  Textual Personality Detection","Textual personality detection aims to identify personality traits by
analyzing user-generated content. To achieve this effectively, it is essential
to thoroughly examine user-generated content from various perspectives.
However, previous studies have struggled with automatically extracting and
effectively integrating information from multiple perspectives, thereby
limiting their performance on personality detection. To address these
challenges, we propose the Multi-view Mixture-of-Experts Model for Textual
Personality Detection (MvP). MvP introduces a Multi-view Mixture-of-Experts
(MoE) network to automatically analyze user posts from various perspectives.
Additionally, it employs User Consistency Regularization to mitigate conflicts
among different perspectives and learn a multi-view generic user
representation. The model's training is optimized via a multi-task joint
learning strategy that balances supervised personality detection with
self-supervised user consistency constraints. Experimental results on two
widely-used personality detection datasets demonstrate the effectiveness of the
MvP model and the benefits of automatically analyzing user posts from diverse
perspectives for textual personality detection.","[{'name': 'Haohao Zhu'}, {'name': 'Xiaokun Zhang'}, {'name': 'Junyu Lu'}, {'name': 'Liang Yang'}, {'name': 'Hongfei Lin'}]",2024-08-16T06:35:31Z
http://arxiv.org/abs/2408.08545v1,http://arxiv.org/abs/2408.08545v1,"SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language
  Models","Large language models (LLMs) have gained increased popularity due to their
remarkable success across various tasks, which has led to the active
development of a large set of diverse LLMs. However, individual LLMs have
limitations when applied to complex tasks because of such factors as training
biases, model sizes, and the datasets used. A promising approach is to
efficiently harness the diverse capabilities of LLMs to overcome these
individual limitations. Towards this goal, we introduce a novel LLM selection
algorithm called SelectLLM. This algorithm directs input queries to the most
suitable subset of LLMs from a large pool, ensuring they collectively provide
the correct response efficiently. SelectLLM uses a multi-label classifier,
utilizing the classifier's predictions and confidence scores to design optimal
policies for selecting an optimal, query-aware, and lightweight subset of LLMs.
Our findings show that the proposed model outperforms individual LLMs and
achieves competitive performance compared to similarly sized, computationally
expensive top-performing LLM subsets. Specifically, with a similarly sized
top-performing LLM subset, we achieve a significant reduction in latency on two
standard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower
latency for MMLU. Additionally, we conduct comprehensive analyses and ablation
studies, which validate the robustness of the proposed model.","[{'name': 'Kaushal Kumar Maurya'}, {'name': 'KV Aditya Srivatsa'}, {'name': 'Ekaterina Kochmar'}]",2024-08-16T06:11:21Z
http://arxiv.org/abs/2408.08541v1,http://arxiv.org/abs/2408.08541v1,Where is the signal in tokenization space?,"Large Language Models (LLMs) are typically shipped with tokenizers that
deterministically encode text into so-called canonical token sequences, to
which the LLMs assign probability values. One common assumption is that the
probability of a piece of text is the probability of its canonical token
sequence. However, the tokenization of a string is not unique: e.g., the Llama2
tokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same
text. In this paper, we study non-canonical tokenizations. We prove that, given
a string, it is computationally hard to find the most likely tokenization for
an autoregressive LLM, as well as to compute the marginal probability over all
possible tokenizations. We then show how the marginal is, in most cases,
indistinguishable from the canonical probability. Surprisingly, we then
empirically demonstrate the existence of a significant amount of signal hidden
within tokenization space. Notably, by simply aggregating the probabilities of
non-canonical tokenizations, we achieve improvements across a range of LLM
evaluation benchmarks for a variety of architectures, including transformers
and state space models.","[{'name': 'Renato Lui Geh'}, {'name': 'Honghua Zhang'}, {'name': 'Kareem Ahmed'}, {'name': 'Benjie Wang'}, {'name': 'Guy Van den Broeck'}]",2024-08-16T05:56:10Z
http://arxiv.org/abs/2408.08535v1,http://arxiv.org/abs/2408.08535v1,"CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for
  Advanced Retrieval-Augmented Generation in Fact-Checking","Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems, their effectiveness is often hindered by a lack of
integration with entity relationships and community structures, limiting their
ability to provide contextually rich and accurate information retrieval for
fact-checking. We introduce CommunityKG-RAG (Community Knowledge
Graph-Retrieval Augmented Generation), a novel zero-shot framework that
integrates community structures within Knowledge Graphs (KGs) with RAG systems
to enhance the fact-checking process. Capable of adapting to new domains and
queries without additional training, CommunityKG-RAG utilizes the multi-hop
nature of community structures within KGs to significantly improve the accuracy
and relevance of information retrieval. Our experimental results demonstrate
that CommunityKG-RAG outperforms traditional methods, representing a
significant advancement in fact-checking by offering a robust, scalable, and
efficient solution.","[{'name': 'Rong-Ching Chang'}, {'name': 'Jiawei Zhang'}]",2024-08-16T05:15:12Z
http://arxiv.org/abs/2408.08521v1,http://arxiv.org/abs/2408.08521v1,"MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement
  Framework for Multimodal Question Answering","Recent advancements in retrieval-augmented generation (RAG) have demonstrated
impressive performance in the question-answering (QA) task. However, most
previous works predominantly focus on text-based answers. While some studies
address multimodal data, they still fall short in generating comprehensive
multimodal answers, particularly for explaining concepts or providing
step-by-step tutorials on how to accomplish specific goals. This capability is
especially valuable for applications such as enterprise chatbots and settings
such as customer service and educational systems, where the answers are sourced
from multimodal data. In this paper, we introduce a simple and effective
framework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR
enhances text-based answers by retrieving relevant multimodal data and refining
the responses to create coherent multimodal answers. This framework can be
easily extended to support multimodal answers in enterprise chatbots with
minimal modifications. Human evaluation results indicate that multimodal
answers generated by MuRAR are more useful and readable compared to plain text
answers.","[{'name': 'Zhengyuan Zhu'}, {'name': 'Daniel Lee'}, {'name': 'Hong Zhang'}, {'name': 'Sai Sree Harsha'}, {'name': 'Loic Feujio'}, {'name': 'Akash Maharaj'}, {'name': 'Yunyao Li'}]",2024-08-16T04:32:10Z
http://arxiv.org/abs/2408.08506v1,http://arxiv.org/abs/2408.08506v1,"Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding","Generating long-term texts such as novels using artificial intelligence has
always been a challenge. A common approach is to use large language models
(LLMs) to construct a hierarchical framework that first plans and then writes.
Despite the fact that the generated novels reach a sufficient length, they
exhibit poor logical coherence and appeal in their plots and deficiencies in
character and event depiction, ultimately compromising the overall narrative
quality. In this paper, we propose a method named Extracting Excelsior and
Expanding. Ex3 initially extracts structure information from raw novel data. By
combining this structure information with the novel data, an
instruction-following dataset is meticulously crafted. This dataset is then
utilized to fine-tune the LLM, aiming for excelsior generation performance. In
the final stage, a tree-like expansion method is deployed to facilitate the
generation of arbitrarily long novels. Evaluation against previous methods
showcases Ex3's ability to produce higher-quality long-form novels.","[{'name': 'Huang Lei'}, {'name': 'Jiaming Guo'}, {'name': 'Guanhua He'}, {'name': 'Xishan Zhang'}, {'name': 'Rui Zhang'}, {'name': 'Shaohui Peng'}, {'name': 'Shaoli Liu'}, {'name': 'Tianshi Chen'}]",2024-08-16T03:06:57Z
http://arxiv.org/abs/2408.08930v1,http://arxiv.org/abs/2408.08930v1,"DePrompt: Desensitization and Evaluation of Personal Identifiable
  Information in Large Language Model Prompts","Prompt serves as a crucial link in interacting with large language models
(LLMs), widely impacting the accuracy and interpretability of model outputs.
However, acquiring accurate and high-quality responses necessitates precise
prompts, which inevitably pose significant risks of personal identifiable
information (PII) leakage. Therefore, this paper proposes DePrompt, a
desensitization protection and effectiveness evaluation framework for prompt,
enabling users to safely and transparently utilize LLMs. Specifically, by
leveraging large model fine-tuning techniques as the underlying privacy
protection method, we integrate contextual attributes to define privacy types,
achieving high-precision PII entity identification. Additionally, through the
analysis of key features in prompt desensitization scenarios, we devise
adversarial generative desensitization methods that retain important semantic
content while disrupting the link between identifiers and privacy attributes.
Furthermore, we present utility evaluation metrics for prompt to better gauge
and balance privacy and usability. Our framework is adaptable to prompts and
can be extended to text usability-dependent scenarios. Through comparison with
benchmarks and other model methods, experimental evaluations demonstrate that
our desensitized prompt exhibit superior privacy protection utility and model
inference results.","[{'name': 'Xiongtao Sun'}, {'name': 'Gan Liu'}, {'name': 'Zhipeng He'}, {'name': 'Hui Li'}, {'name': 'Xiaoguang Li'}]",2024-08-16T02:38:25Z
http://arxiv.org/abs/2408.08459v1,http://arxiv.org/abs/2408.08459v1,JPEG-LM: LLMs as Image Generators with Canonical Codec Representations,"Recent work in image and video generation has been adopting the
autoregressive LLM architecture due to its generality and potentially easy
integration into multi-modal systems. The crux of applying autoregressive
training in language generation to visual generation is discretization --
representing continuous data like images and videos as discrete tokens. Common
methods of discretizing images and videos include modeling raw pixel values,
which are prohibitively lengthy, or vector quantization, which requires
convoluted pre-hoc training. In this work, we propose to directly model images
and videos as compressed files saved on computers via canonical codecs (e.g.,
JPEG, AVC/H.264). Using the default Llama architecture without any
vision-specific modifications, we pretrain JPEG-LM from scratch to generate
images (and AVC-LM to generate videos as a proof of concept), by directly
outputting compressed file bytes in JPEG and AVC formats. Evaluation of image
generation shows that this simple and straightforward approach is more
effective than pixel-based modeling and sophisticated vector quantization
baselines (on which our method yields a 31% reduction in FID). Our analysis
shows that JPEG-LM has an especial advantage over vector quantization models in
generating long-tail visual elements. Overall, we show that using canonical
codec representations can help lower the barriers between language generation
and visual generation, facilitating future research on multi-modal
language/image/video LLMs.","[{'name': 'Xiaochuang Han'}, {'name': 'Marjan Ghazvininejad'}, {'name': 'Pang Wei Koh'}, {'name': 'Yulia Tsvetkov'}]",2024-08-15T23:57:02Z
http://arxiv.org/abs/2408.08444v1,http://arxiv.org/abs/2408.08444v1,"W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question
  Answering","In knowledge-intensive tasks such as open-domain question answering (OpenQA),
Large Language Models (LLMs) often struggle to generate factual answers relying
solely on their internal (parametric) knowledge. To address this limitation,
Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving
relevant information from external sources, thereby positioning the retriever
as a pivotal component. Although dense retrieval demonstrates state-of-the-art
performance, its training poses challenges due to the scarcity of ground-truth
evidence, largely attributed to the high costs of human annotation. In this
paper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create
weakly labeled data for training dense retrievers. Specifically, we rerank the
top-$K$ passages retrieved via BM25 by assessing the probability that LLMs will
generate the correct answer based on the question and each passage. The
highest-ranking passages are then used as positive training examples for dense
retrieval. Our comprehensive experiments across four publicly available OpenQA
datasets demonstrate that our approach enhances both retrieval and OpenQA
performance compared to baseline models.","[{'name': 'Jinming Nian'}, {'name': 'Zhiyuan Peng'}, {'name': 'Qifan Wang'}, {'name': 'Yi Fang'}]",2024-08-15T22:34:44Z
http://arxiv.org/abs/2408.08411v1,http://arxiv.org/abs/2408.08411v1,Rater Cohesion and Quality from a Vicarious Perspective,"Human feedback is essential for building human-centered AI systems across
domains where disagreement is prevalent, such as AI safety, content moderation,
or sentiment analysis. Many disagreements, particularly in politically charged
settings, arise because raters have opposing values or beliefs. Vicarious
annotation is a method for breaking down disagreement by asking raters how they
think others would annotate the data. In this paper, we explore the use of
vicarious annotation with analytical methods for moderating rater disagreement.
We employ rater cohesion metrics to study the potential influence of political
affiliations and demographic backgrounds on raters' perceptions of offense.
Additionally, we utilize CrowdTruth's rater quality metrics, which consider the
demographics of the raters, to score the raters and their annotations. We study
how the rater quality metrics influence the in-group and cross-group rater
cohesion across the personal and vicarious levels.","[{'name': 'Deepak Pandita'}, {'name': 'Tharindu Cyril Weerasooriya'}, {'name': 'Sujan Dutta'}, {'name': 'Sarah K. Luger'}, {'name': 'Tharindu Ranasinghe'}, {'name': 'Ashiqur R. KhudaBukhsh'}, {'name': 'Marcos Zampieri'}, {'name': 'Christopher M. Homan'}]",2024-08-15T20:37:36Z
http://arxiv.org/abs/2408.08927v1,http://arxiv.org/abs/2408.08927v1,"VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning
  and Abstract Syntax Tree (AST)-based Waveform Tracing Tool","Due to the growing complexity of modern Integrated Circuits (ICs), automating
hardware design can prevent a significant amount of human error from the
engineering process and result in less errors. Verilog is a popular hardware
description language for designing and modeling digital systems; thus, Verilog
generation is one of the emerging areas of research to facilitate the design
process. In this work, we propose VerilogCoder, a system of multiple Artificial
Intelligence (AI) agents for Verilog code generation, to autonomously write
Verilog code and fix syntax and functional errors using collaborative Verilog
tools (i.e., syntax checker, simulator, and waveform tracer). Firstly, we
propose a task planner that utilizes a novel Task and Circuit Relation Graph
retrieval method to construct a holistic plan based on module descriptions. To
debug and fix functional errors, we develop a novel and efficient abstract
syntax tree (AST)-based waveform tracing tool, which is integrated within the
autonomous Verilog completion flow. The proposed methodology successfully
generates 94.2% syntactically and functionally correct Verilog code, surpassing
the state-of-the-art methods by 33.9% on the VerilogEval-Human v2 benchmark.","[{'name': 'Chia-Tung Ho'}, {'name': 'Haoxing Ren'}, {'name': 'Brucek Khailany'}]",2024-08-15T20:06:06Z
http://arxiv.org/abs/2408.08400v1,http://arxiv.org/abs/2408.08400v1,"Zero-Shot Learning and Key Points Are All You Need for Automated
  Fact-Checking","Automated fact-checking is an important task because determining the accurate
status of a proposed claim within the vast amount of information available
online is a critical challenge. This challenge requires robust evaluation to
prevent the spread of false information. Modern large language models (LLMs)
have demonstrated high capability in performing a diverse range of Natural
Language Processing (NLP) tasks. By utilizing proper prompting strategies,
their versatility due to their understanding of large context sizes and
zero-shot learning ability enables them to simulate human problem-solving
intuition and move towards being an alternative to humans for solving problems.
In this work, we introduce a straightforward framework based on Zero-Shot
Learning and Key Points (ZSL-KeP) for automated fact-checking, which despite
its simplicity, performed well on the AVeriTeC shared task dataset by robustly
improving the baseline and achieving 10th place.","[{'name': 'Mohammad Ghiasvand Mohammadkhani'}, {'name': 'Ali Ghiasvand Mohammadkhani'}, {'name': 'Hamid Beigy'}]",2024-08-15T19:57:42Z
http://arxiv.org/abs/2408.08396v1,http://arxiv.org/abs/2408.08396v1,Level Up Your Tutorials: VLMs for Game Tutorials Quality Assessment,"Designing effective game tutorials is crucial for a smooth learning curve for
new players, especially in games with many rules and complex core mechanics.
Evaluating the effectiveness of these tutorials usually requires multiple
iterations with testers who have no prior knowledge of the game. Recent
Vision-Language Models (VLMs) have demonstrated significant capabilities in
understanding and interpreting visual content. VLMs can analyze images, provide
detailed insights, and answer questions about their content. They can recognize
objects, actions, and contexts in visual data, making them valuable tools for
various applications, including automated game testing. In this work, we
propose an automated game-testing solution to evaluate the quality of game
tutorials. Our approach leverages VLMs to analyze frames from video game
tutorials, answer relevant questions to simulate human perception, and provide
feedback. This feedback is compared with expected results to identify confusing
or problematic scenes and highlight potential errors for developers. In
addition, we publish complete tutorial videos and annotated frames from
different game versions used in our tests. This solution reduces the need for
extensive manual testing, especially by speeding up and simplifying the initial
development stages of the tutorial to improve the final game experience.","[{'name': 'Daniele Rege Cambrin'}, {'name': 'Gabriele Scaffidi Militone'}, {'name': 'Luca Colomba'}, {'name': 'Giovanni Malnati'}, {'name': 'Daniele Apiletti'}, {'name': 'Paolo Garza'}]",2024-08-15T19:46:21Z
http://arxiv.org/abs/2408.08379v1,http://arxiv.org/abs/2408.08379v1,"Towards Realistic Synthetic User-Generated Content: A Scaffolding
  Approach to Generating Online Discussions","The emergence of synthetic data represents a pivotal shift in modern machine
learning, offering a solution to satisfy the need for large volumes of data in
domains where real data is scarce, highly private, or difficult to obtain. We
investigate the feasibility of creating realistic, large-scale synthetic
datasets of user-generated content, noting that such content is increasingly
prevalent and a source of frequently sought information. Large language models
(LLMs) offer a starting point for generating synthetic social media discussion
threads, due to their ability to produce diverse responses that typify online
interactions. However, as we demonstrate, straightforward application of LLMs
yields limited success in capturing the complex structure of online
discussions, and standard prompting mechanisms lack sufficient control. We
therefore propose a multi-step generation process, predicated on the idea of
creating compact representations of discussion threads, referred to as
scaffolds. Our framework is generic yet adaptable to the unique characteristics
of specific social media platforms. We demonstrate its feasibility using data
from two distinct online discussion platforms. To address the fundamental
challenge of ensuring the representativeness and realism of synthetic data, we
propose a portfolio of evaluation measures to compare various instantiations of
our framework.","[{'name': 'Krisztian Balog'}, {'name': 'John Palowitch'}, {'name': 'Barbara Ikica'}, {'name': 'Filip Radlinski'}, {'name': 'Hamidreza Alvari'}, {'name': 'Mehdi Manshadi'}]",2024-08-15T18:43:50Z
http://arxiv.org/abs/2408.08374v1,http://arxiv.org/abs/2408.08374v1,"Evaluating Text Classification Robustness to Part-of-Speech Adversarial
  Examples","As machine learning systems become more widely used, especially for safety
critical applications, there is a growing need to ensure that these systems
behave as intended, even in the face of adversarial examples. Adversarial
examples are inputs that are designed to trick the decision making process, and
are intended to be imperceptible to humans. However, for text-based
classification systems, changes to the input, a string of text, are always
perceptible. Therefore, text-based adversarial examples instead focus on trying
to preserve semantics. Unfortunately, recent work has shown this goal is often
not met. To improve the quality of text-based adversarial examples, we need to
know what elements of the input text are worth focusing on. To address this, in
this paper, we explore what parts of speech have the highest impact of
text-based classifiers. Our experiments highlight a distinct bias in CNN
algorithms against certain parts of speech tokens within review datasets. This
finding underscores a critical vulnerability in the linguistic processing
capabilities of CNNs.","[{'name': 'Anahita Samadi'}, {'name': 'Allison Sullivan'}]",2024-08-15T18:33:54Z
http://arxiv.org/abs/2408.08313v1,http://arxiv.org/abs/2408.08313v1,Can Large Language Models Understand Symbolic Graphics Programs?,"Assessing the capabilities of large language models (LLMs) is often
challenging, in part, because it is hard to find tasks to which they have not
been exposed during training. We take one step to address this challenge by
turning to a new task: focusing on symbolic graphics programs, which are a
popular representation for graphics content that procedurally generates visual
data. LLMs have shown exciting promise towards program synthesis, but do they
understand symbolic graphics programs? Unlike conventional programs, symbolic
graphics programs can be translated to graphics content. Here, we characterize
an LLM's understanding of symbolic programs in terms of their ability to answer
questions related to the graphics content. This task is challenging as the
questions are difficult to answer from the symbolic programs alone -- yet, they
would be easy to answer from the corresponding graphics content as we verify
through a human experiment. To understand symbolic programs, LLMs may need to
possess the ability to imagine how the corresponding graphics content would
look without directly accessing the rendered visual content. We use this task
to evaluate LLMs by creating a large benchmark for the semantic understanding
of symbolic graphics programs. This benchmark is built via program-graphics
correspondence, hence requiring minimal human efforts. We evaluate current LLMs
on our benchmark to elucidate a preliminary assessment of their ability to
reason about visual scenes from programs. We find that this task distinguishes
existing LLMs and models considered good at reasoning perform better. Lastly,
we introduce Symbolic Instruction Tuning (SIT) to improve this ability.
Specifically, we query GPT4-o with questions and images generated by symbolic
programs. Such data are then used to finetune an LLM. We also find that SIT
data can improve the general instruction following ability of LLMs.","[{'name': 'Zeju Qiu'}, {'name': 'Weiyang Liu'}, {'name': 'Haiwen Feng'}, {'name': 'Zhen Liu'}, {'name': 'Tim Z. Xiao'}, {'name': 'Katherine M. Collins'}, {'name': 'Joshua B. Tenenbaum'}, {'name': 'Adrian Weller'}, {'name': 'Michael J. Black'}, {'name': 'Bernhard Schölkopf'}]",2024-08-15T17:59:57Z
http://arxiv.org/abs/2408.08310v1,http://arxiv.org/abs/2408.08310v1,"ScalingFilter: Assessing Data Quality through Inverse Utilization of
  Scaling Laws","High-quality data is crucial for the pre-training performance of large
language models. Unfortunately, existing quality filtering methods rely on a
known high-quality dataset as reference, which can introduce potential bias and
compromise diversity. In this paper, we propose ScalingFilter, a novel approach
that evaluates text quality based on the perplexity difference between two
language models trained on the same data, thereby eliminating the influence of
the reference dataset in the filtering process. An theoretical analysis shows
that ScalingFilter is equivalent to an inverse utilization of scaling laws.
Through training models with 1.3B parameters on the same data source processed
by various quality filters, we find ScalingFilter can improve zero-shot
performance of pre-trained models in downstream tasks. To assess the bias
introduced by quality filtering, we introduce semantic diversity, a metric of
utilizing text embedding models for semantic representations. Extensive
experiments reveal that semantic diversity is a reliable indicator of dataset
diversity, and ScalingFilter achieves an optimal balance between downstream
performance and semantic diversity.","[{'name': 'Ruihang Li'}, {'name': 'Yixuan Wei'}, {'name': 'Miaosen Zhang'}, {'name': 'Nenghai Yu'}, {'name': 'Han Hu'}, {'name': 'Houwen Peng'}]",2024-08-15T17:59:30Z
http://arxiv.org/abs/2408.08302v1,http://arxiv.org/abs/2408.08302v1,"Benchmarking the Capabilities of Large Language Models in Transportation
  System Engineering: Accuracy, Consistency, and Reasoning Behaviors","In this paper, we explore the capabilities of state-of-the-art large language
models (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini
1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level
transportation engineering problems. We introduce TransportBench, a benchmark
dataset that includes a sample of transportation engineering problems on a wide
range of subjects in the context of planning, design, management, and control
of transportation systems. This dataset is used by human experts to evaluate
the capabilities of various commercial and open-sourced LLMs, especially their
accuracy, consistency, and reasoning behaviors, in solving transportation
engineering problems. Our comprehensive analysis uncovers the unique strengths
and limitations of each LLM, e.g. our analysis shows the impressive accuracy
and some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving
TransportBench problems. Our study marks a thrilling first step toward
harnessing artificial general intelligence for complex transportation
challenges.","[{'name': 'Usman Syed'}, {'name': 'Ethan Light'}, {'name': 'Xingang Guo'}, {'name': 'Huan Zhang'}, {'name': 'Lianhui Qin'}, {'name': 'Yanfeng Ouyang'}, {'name': 'Bin Hu'}]",2024-08-15T17:55:45Z
http://arxiv.org/abs/2408.08291v1,http://arxiv.org/abs/2408.08291v1,"The ShareLM Collection and Plugin: Contributing Human-Model Chats for
  the Benefit of the Community","Human-model conversations provide a window into users' real-world scenarios,
behavior, and needs, and thus are a valuable resource for model development and
research. While for-profit companies collect user data through the APIs of
their models, using it internally to improve their own models, the open source
and research community lags behind.
  We introduce the ShareLM collection, a unified set of human conversations
with large language models, and its accompanying plugin, a Web extension for
voluntarily contributing user-model conversations. Where few platforms share
their chats, the ShareLM plugin adds this functionality, thus, allowing users
to share conversations from most platforms. The plugin allows the user to rate
their conversations, both at the conversation and the response levels, and
delete conversations they prefer to keep private before they ever leave the
user's local storage. We release the plugin conversations as part of the
ShareLM collection, and call for more community effort in the field of open
human-model data.
  The code, plugin, and data are available.","[{'name': 'Shachar Don-Yehiya'}, {'name': 'Leshem Choshen'}, {'name': 'Omri Abend'}]",2024-08-15T17:46:54Z
http://arxiv.org/abs/2408.08926v1,http://arxiv.org/abs/2408.08926v1,"Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk
  of Language Models","Language Model (LM) agents for cybersecurity that are capable of autonomously
identifying vulnerabilities and executing exploits have the potential to cause
real-world impact. Policymakers, model providers, and other researchers in the
AI and cybersecurity communities are interested in quantifying the capabilities
of such agents to help mitigate cyberrisk and investigate opportunities for
penetration testing. Toward that end, we introduce Cybench, a framework for
specifying cybersecurity tasks and evaluating agents on those tasks. We include
40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF
competitions, chosen to be recent, meaningful, and spanning a wide range of
difficulties. Each task includes its own description, starter files, and is
initialized in an environment where an agent can execute bash commands and
observe outputs. Since many tasks are beyond the capabilities of existing LM
agents, we introduce subtasks, which break down a task into intermediary steps
for more gradated evaluation; we add subtasks for 17 of the 40 tasks. To
evaluate agent capabilities, we construct a cybersecurity agent and evaluate 7
models: GPT-4o, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct,
Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without
guidance, we find that agents are able to solve only the easiest complete tasks
that took human teams up to 11 minutes to solve, with Claude 3.5 Sonnet and
GPT-4o having the highest success rates. Finally, subtasks provide more signal
for measuring performance compared to unguided runs, with models achieving a
3.2\% higher success rate on complete tasks with subtask-guidance than without
subtask-guidance. All code and data are publicly available at
https://cybench.github.io","[{'name': 'Andy K. Zhang'}, {'name': 'Neil Perry'}, {'name': 'Riya Dulepet'}, {'name': 'Eliot Jones'}, {'name': 'Justin W. Lin'}, {'name': 'Joey Ji'}, {'name': 'Celeste Menders'}, {'name': 'Gashon Hussein'}, {'name': 'Samantha Liu'}, {'name': 'Donovan Jasper'}, {'name': 'Pura Peetathawatchai'}, {'name': 'Ari Glenn'}, {'name': 'Vikram Sivashankar'}, {'name': 'Daniel Zamoshchin'}, {'name': 'Leo Glikbarg'}, {'name': 'Derek Askaryar'}, {'name': 'Mike Yang'}, {'name': 'Teddy Zhang'}, {'name': 'Rishi Alluri'}, {'name': 'Nathan Tran'}, {'name': 'Rinnara Sangpisit'}, {'name': 'Polycarpos Yiorkadjis'}, {'name': 'Kenny Osele'}, {'name': 'Gautham Raghupathi'}, {'name': 'Dan Boneh'}, {'name': 'Daniel E. Ho'}, {'name': 'Percy Liang'}]",2024-08-15T17:23:10Z
http://arxiv.org/abs/2408.08261v1,http://arxiv.org/abs/2408.08261v1,"mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental
  Health Text Analysis","This paper introduces mhGPT, a lightweight generative pre-trained transformer
trained on mental health-related social media and PubMed articles. Fine-tuned
for specific mental health tasks, mhGPT was evaluated under limited hardware
constraints and compared with state-of-the-art models like MentaLLaMA and
Gemma. Despite having only 1.98 billion parameters and using just 5% of the
dataset, mhGPT outperformed larger models and matched the performance of models
trained on significantly more data. The key contributions include integrating
diverse mental health data, creating a custom tokenizer, and optimizing a
smaller architecture for low-resource settings. This research could advance
AI-driven mental health care, especially in areas with limited computing power.","[{'name': 'Dae-young Kim'}, {'name': 'Rebecca Hwa'}, {'name': 'Muhammad Mahbubur Rahman'}]",2024-08-15T17:01:57Z
http://arxiv.org/abs/2408.08925v1,http://arxiv.org/abs/2408.08925v1,"Retail-GPT: leveraging Retrieval Augmented Generation (RAG) for building
  E-commerce Chat Assistants","This work presents Retail-GPT, an open-source RAG-based chatbot designed to
enhance user engagement in retail e-commerce by guiding users through product
recommendations and assisting with cart operations. The system is
cross-platform and adaptable to various e-commerce domains, avoiding reliance
on specific chat applications or commercial activities. Retail-GPT engages in
human-like conversations, interprets user demands, checks product availability,
and manages cart operations, aiming to serve as a virtual sales agent and test
the viability of such assistants across different retail businesses.","[{'name': 'Bruno Amaral Teixeira de Freitas'}, {'name': 'Roberto de Alencar Lotufo'}]",2024-08-15T16:53:05Z
http://arxiv.org/abs/2408.08212v2,http://arxiv.org/abs/2408.08212v2,"Covert Bias: The Severity of Social Views' Unalignment in Language
  Models Towards Implicit and Explicit Opinion","While various approaches have recently been studied for bias identification,
little is known about how implicit language that does not explicitly convey a
viewpoint affects bias amplification in large language models. To examine the
severity of bias toward a view, we evaluated the performance of two downstream
tasks where the implicit and explicit knowledge of social groups were used.
First, we present a stress test evaluation by using a biased model in edge
cases of excessive bias scenarios. Then, we evaluate how LLMs calibrate
linguistically in response to both implicit and explicit opinions when they are
aligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM
performance in identifying implicit and explicit opinions, with a general
tendency of bias toward explicit opinions of opposing stances. Moreover, the
bias-aligned models generate more cautious responses using uncertainty phrases
compared to the unaligned (zero-shot) base models. The direct, incautious
responses of the unaligned models suggest a need for further refinement of
decisiveness by incorporating uncertainty markers to enhance their reliability,
especially on socially nuanced topics with high subjectivity.","[{'name': 'Abeer Aldayel'}, {'name': 'Areej Alokaili'}, {'name': 'Rehab Alahmadi'}]",2024-08-15T15:23:00Z
http://arxiv.org/abs/2408.08152v1,http://arxiv.org/abs/2408.08152v1,"DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for
  Reinforcement Learning and Monte-Carlo Tree Search","We introduce DeepSeek-Prover-V1.5, an open-source language model designed for
theorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both
training and inference processes. Pre-trained on DeepSeekMath-Base with
specialization in formal mathematical languages, the model undergoes supervised
fine-tuning using an enhanced formal theorem proving dataset derived from
DeepSeek-Prover-V1. Further refinement is achieved through reinforcement
learning from proof assistant feedback (RLPAF). Beyond the single-pass
whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a
variant of Monte-Carlo tree search that employs an intrinsic-reward-driven
exploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5
demonstrates significant improvements over DeepSeek-Prover-V1, achieving new
state-of-the-art results on the test set of the high school level miniF2F
benchmark ($63.5\%$) and the undergraduate level ProofNet benchmark ($25.3\%$).","[{'name': 'Huajian Xin'}, {'name': 'Z. Z. Ren'}, {'name': 'Junxiao Song'}, {'name': 'Zhihong Shao'}, {'name': 'Wanjia Zhao'}, {'name': 'Haocheng Wang'}, {'name': 'Bo Liu'}, {'name': 'Liyue Zhang'}, {'name': 'Xuan Lu'}, {'name': 'Qiushi Du'}, {'name': 'Wenjun Gao'}, {'name': 'Qihao Zhu'}, {'name': 'Dejian Yang'}, {'name': 'Zhibin Gou'}, {'name': 'Z. F. Wu'}, {'name': 'Fuli Luo'}, {'name': 'Chong Ruan'}]",2024-08-15T13:40:03Z
http://arxiv.org/abs/2408.08147v1,http://arxiv.org/abs/2408.08147v1,P/D-Serve: Serving Disaggregated Large Language Model at Scale,"Serving disaggregated large language models (LLMs) over tens of thousands of
xPU devices (GPUs or NPUs) with reliable performance faces multiple challenges.
1) Ignoring the diversity (various prefixes and tidal requests), treating all
the prompts in a mixed pool is inadequate. To facilitate the similarity per
scenario and minimize the inner mismatch on P/D (prefill and decoding)
processing, fine-grained organization is required, dynamically adjusting P/D
ratios for better performance. 2) Due to inaccurate estimation on workload
(queue status or maintained connections), the global scheduler easily incurs
unnecessary timeouts in prefill. 3) Block-fixed device-to-device (D2D) KVCache
transfer over cluster-level RDMA (remote direct memory access) fails to achieve
desired D2D utilization as expected. To overcome previous problems, this paper
proposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps
(machine learning operations), which models end-to-end (E2E) P/D performance
and enables: 1) fine-grained P/D organization, mapping the service with RoCE
(RDMA over converged ethernet) as needed, to facilitate similar processing and
dynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for
idle prefill, decoupling the scheduler from regular inaccurate reports and
local queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer
via optimized D2D access. P/D-Serve is implemented upon Ascend and MindSpore,
has been deployed over tens of thousands of NPUs for more than eight months in
commercial use, and further achieves 60\%, 42\% and 46\% improvements on E2E
throughput, time-to-first-token (TTFT) SLO (service level objective) and D2D
transfer time. As the E2E system with optimizations, P/D-Serve achieves 6.7x
increase on throughput, compared with aggregated LLMs.","[{'name': 'Yibo Jin'}, {'name': 'Tao Wang'}, {'name': 'Huimin Lin'}, {'name': 'Mingyang Song'}, {'name': 'Peiyang Li'}, {'name': 'Yipeng Ma'}, {'name': 'Yicheng Shan'}, {'name': 'Zhengfan Yuan'}, {'name': 'Cailong Li'}, {'name': 'Yajing Sun'}, {'name': 'Tiandeng Wu'}, {'name': 'Xing Chu'}, {'name': 'Ruizhi Huan'}, {'name': 'Li Ma'}, {'name': 'Xiao You'}, {'name': 'Wenting Zhou'}, {'name': 'Yunpeng Ye'}, {'name': 'Wen Liu'}, {'name': 'Xiangkun Xu'}, {'name': 'Yongsheng Zhang'}, {'name': 'Tiantian Dong'}, {'name': 'Jiawei Zhu'}, {'name': 'Zhe Wang'}, {'name': 'Xijian Ju'}, {'name': 'Jianxun Song'}, {'name': 'Haoliang Cheng'}, {'name': 'Xiaojing Li'}, {'name': 'Jiandong Ding'}, {'name': 'Hefei Guo'}, {'name': 'Zhengyong Zhang'}]",2024-08-15T13:32:25Z
http://arxiv.org/abs/2408.08146v1,http://arxiv.org/abs/2408.08146v1,"KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft
  Heads with Adversarial Learning","Large Language Models (LLMs) exhibit high inference latency due to their
autoregressive decoding nature. While the draft head in speculative decoding
mitigates this issue, its full potential remains unexplored. In this paper, we
introduce KOALA (K-layer Optimized Adversarial Learning Architecture), an
orthogonal approach to the draft head. By transforming the conventional
single-layer draft head into a multi-layer architecture and incorporating
adversarial learning into the traditional supervised training, KOALA
significantly improves the accuracy of the draft head in predicting subsequent
tokens, thus more closely mirroring the functionality of LLMs. Although this
improvement comes at the cost of slightly increased drafting overhead, KOALA
substantially unlocks the draft head's potential, greatly enhancing speculative
decoding. We conducted comprehensive evaluations of KOALA, including both
autoregressive and non-autoregressive draft heads across various tasks,
demonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is
10.57%-14.09% faster than the original draft heads.","[{'name': 'Kaiqi Zhang'}, {'name': 'Jing Zhao'}, {'name': 'Rui Chen'}]",2024-08-15T13:29:48Z
http://arxiv.org/abs/2408.08144v1,http://arxiv.org/abs/2408.08144v1,"MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for
  Multi-turn NLU","Although Large Language Models(LLMs) can generate coherent and contextually
relevant text, they often struggle to recognise the intent behind the human
user's query. Natural Language Understanding (NLU) models, however, interpret
the purpose and key information of user's input to enable responsive
interactions. Existing NLU models generally map individual utterances to a
dual-level semantic frame, involving sentence-level intent and word-level slot
labels. However, real-life conversations primarily consist of multi-turn
conversations, involving the interpretation of complex and extended dialogues.
Researchers encounter challenges addressing all facets of multi-turn dialogue
conversations using a unified single NLU model. This paper introduces a novel
approach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge
distillation for multi-turn NLU. To achieve this, we construct distinct
teachers for varying levels of conversation knowledge, namely, sentence-level
intent detection, word-level slot filling, and conversation-level domain
classification. These teachers are then fine-tuned to acquire specific
knowledge of their designated levels. A multi-teacher loss is proposed to
facilitate the combination of these multi-level teachers, guiding a student
model in multi-turn dialogue tasks. The experimental results demonstrate the
efficacy of our model in improving the overall multi-turn conversation
understanding, showcasing the potential for advancements in NLU models through
the incorporation of multi-level dialogue knowledge distillation techniques.","[{'name': 'Yan Li'}, {'name': 'So-Eon Kim'}, {'name': 'Seong-Bae Park'}, {'name': 'Soyeon Caren Han'}]",2024-08-15T13:28:18Z
http://arxiv.org/abs/2408.08921v1,http://arxiv.org/abs/2408.08921v1,Graph Retrieval-Augmented Generation: A Survey,"Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable
success in addressing the challenges of Large Language Models (LLMs) without
necessitating retraining. By referencing an external knowledge base, RAG
refines LLM outputs, effectively mitigating issues such as ``hallucination'',
lack of domain-specific knowledge, and outdated information. However, the
complex structure of relationships among different entities in databases
presents challenges for RAG systems. In response, GraphRAG leverages structural
information across entities to enable more precise and comprehensive retrieval,
capturing relational knowledge and facilitating more accurate, context-aware
responses. Given the novelty and potential of GraphRAG, a systematic review of
current technologies is imperative. This paper provides the first comprehensive
overview of GraphRAG methodologies. We formalize the GraphRAG workflow,
encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced
Generation. We then outline the core technologies and training methods at each
stage. Additionally, we examine downstream tasks, application domains,
evaluation methodologies, and industrial use cases of GraphRAG. Finally, we
explore future research directions to inspire further inquiries and advance
progress in the field.","[{'name': 'Boci Peng'}, {'name': 'Yun Zhu'}, {'name': 'Yongchao Liu'}, {'name': 'Xiaohe Bo'}, {'name': 'Haizhou Shi'}, {'name': 'Chuntao Hong'}, {'name': 'Yan Zhang'}, {'name': 'Siliang Tang'}]",2024-08-15T12:20:24Z
http://arxiv.org/abs/2408.08089v1,http://arxiv.org/abs/2408.08089v1,AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents,"In this paper, we present a simulation system called AgentCourt that
simulates the entire courtroom process. The judge, plaintiff's lawyer, defense
lawyer, and other participants are autonomous agents driven by large language
models (LLMs). Our core goal is to enable lawyer agents to learn how to argue a
case, as well as improving their overall legal skills, through courtroom
process simulation. To achieve this goal, we propose an adversarial
evolutionary approach for the lawyer-agent. Since AgentCourt can simulate the
occurrence and development of court hearings based on a knowledge base and LLM,
the lawyer agents can continuously learn and accumulate experience from real
court cases. The simulation experiments show that after two lawyer-agents have
engaged in a thousand adversarial legal cases in AgentCourt (which can take a
decade for real-world lawyers), compared to their pre-evolutionary state, the
evolved lawyer agents exhibit consistent improvement in their ability to handle
legal tasks. To enhance the credibility of our experimental results, we
enlisted a panel of professional lawyers to evaluate our simulations. The
evaluation indicates that the evolved lawyer agents exhibit notable
advancements in responsiveness, as well as expertise and logical rigor. This
work paves the way for advancing LLM-driven agent technology in legal
scenarios. Code is available at https://github.com/relic-yuexi/AgentCourt.","[{'name': 'Guhong Chen'}, {'name': 'Liyang Fan'}, {'name': 'Zihan Gong'}, {'name': 'Nan Xie'}, {'name': 'Zixuan Li'}, {'name': 'Ziqiang Liu'}, {'name': 'Chengming Li'}, {'name': 'Qiang Qu'}, {'name': 'Shiwen Ni'}, {'name': 'Min Yang'}]",2024-08-15T11:33:20Z
http://arxiv.org/abs/2408.08073v1,http://arxiv.org/abs/2408.08073v1,Extracting Sentence Embeddings from Pretrained Transformer Models,"Background/introduction: Pre-trained transformer models shine in many natural
language processing tasks and therefore are expected to bear the representation
of the input sentence or text meaning. These sentence-level embeddings are also
important in retrieval-augmented generation. But do commonly used plain
averaging or prompt templates surface it enough?
  Methods: Given 110M parameters BERT's hidden representations from multiple
layers and multiple tokens we tried various ways to extract optimal sentence
representations. We tested various token aggregation and representation
post-processing techniques. We also tested multiple ways of using a general
Wikitext dataset to complement BERTs sentence representations. All methods were
tested on 8 Semantic Textual Similarity (STS), 6 short text clustering, and 12
classification tasks. We also evaluated our representation-shaping techniques
on other static models, including random token representations.
  Results: Proposed representation extraction methods improved the performance
on STS and clustering tasks for all models considered. Very high improvements
for static token-based models, especially random embeddings for STS tasks
almost reach the performance of BERT-derived representations.
  Conclusions: Our work shows that for multiple tasks simple baselines with
representation shaping techniques reach or even outperform more complex
BERT-based models or are able to contribute to their performance.","[{'name': 'Lukas Stankevičius'}, {'name': 'Mantas Lukoševičius'}]",2024-08-15T10:54:55Z
http://arxiv.org/abs/2408.08072v1,http://arxiv.org/abs/2408.08072v1,"I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative
  Self-Enhancement Paradigm","Large Language Models (LLMs) have achieved significant advancements, however,
the common learning paradigm treats LLMs as passive information repositories,
neglecting their potential for active learning and alignment. Some approaches
train LLMs using their own generated synthetic data, exploring the possibility
of active alignment. However, there is still a huge gap between these one-time
alignment methods and the continuous automatic alignment of humans. In this
paper, we introduce \textbf{I-SHEEP}, an \textbf{I}terative
\textbf{S}elf-En\textbf{H}anc\textbf{E}m\textbf{E}nt \textbf{P}aradigm.This
human-like paradigm enables LLMs to \textbf{continuously self-align from
scratch with nothing}. Compared to the one-time alignment method Dromedary
\cite{sun2023principledriven}, which refers to the first iteration in this
paper, I-SHEEP can significantly enhance capacities on both Qwen and Llama
models. I-SHEEP achieves a maximum relative improvement of 78.2\% in the Alpaca
Eval, 24.0\% in the MT Bench, and an absolute increase of 8.88\% in the IFEval
accuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,
I-SHEEP surpasses the base model in various standard benchmark generation
tasks, achieving an average improvement of 24.77\% in code generation tasks,
12.04\% in TrivialQA, and 20.29\% in SQuAD. We also provide new insights based
on the experiment results. Our codes, datasets, and models are available at
\textbf{https://anonymous.4open.science/r/I-SHEEP}.","[{'name': 'Yiming Liang'}, {'name': 'Ge Zhang'}, {'name': 'Xingwei Qu'}, {'name': 'Tianyu Zheng'}, {'name': 'Jiawei Guo'}, {'name': 'Xinrun Du'}, {'name': 'Zhenzhu Yang'}, {'name': 'Jiaheng Liu'}, {'name': 'Chenghua Lin'}, {'name': 'Lei Ma'}, {'name': 'Wenhao Huang'}, {'name': 'Jiajun Zhang'}]",2024-08-15T10:44:38Z
http://arxiv.org/abs/2408.08067v2,http://arxiv.org/abs/2408.08067v2,"RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented
  Generation","Despite Retrieval-Augmented Generation (RAG) showing promising capability in
leveraging external knowledge, a comprehensive evaluation of RAG systems is
still challenging due to the modular nature of RAG, evaluation of long-form
responses and reliability of measurements. In this paper, we propose a
fine-grained evaluation framework, RAGChecker, that incorporates a suite of
diagnostic metrics for both the retrieval and generation modules. Meta
evaluation verifies that RAGChecker has significantly better correlations with
human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8
RAG systems and conduct an in-depth analysis of their performance, revealing
insightful patterns and trade-offs in the design choices of RAG architectures.
The metrics of RAGChecker can guide researchers and practitioners in developing
more effective RAG systems. This work has been open sourced at
https://github.com/amazon-science/RAGChecker.","[{'name': 'Dongyu Ru'}, {'name': 'Lin Qiu'}, {'name': 'Xiangkun Hu'}, {'name': 'Tianhang Zhang'}, {'name': 'Peng Shi'}, {'name': 'Shuaichen Chang'}, {'name': 'Cheng Jiayang'}, {'name': 'Cunxiang Wang'}, {'name': 'Shichao Sun'}, {'name': 'Huanyu Li'}, {'name': 'Zizhao Zhang'}, {'name': 'Binjie Wang'}, {'name': 'Jiarong Jiang'}, {'name': 'Tong He'}, {'name': 'Zhiguo Wang'}, {'name': 'Pengfei Liu'}, {'name': 'Yue Zhang'}, {'name': 'Zheng Zhang'}]",2024-08-15T10:20:54Z
http://arxiv.org/abs/2408.08054v1,http://arxiv.org/abs/2408.08054v1,"Text2BIM: Generating Building Models Using a Large Language Model-based
  Multi-Agent Framework","The conventional BIM authoring process typically requires designers to master
complex and tedious modeling commands in order to materialize their design
intentions within BIM authoring tools. This additional cognitive burden
complicates the design process and hinders the adoption of BIM and model-based
design in the AEC (Architecture, Engineering, and Construction) industry. To
facilitate the expression of design intentions more intuitively, we propose
Text2BIM, an LLM-based multi-agent framework that can generate 3D building
models from natural language instructions. This framework orchestrates multiple
LLM agents to collaborate and reason, transforming textual user input into
imperative code that invokes the BIM authoring tool's APIs, thereby generating
editable BIM models with internal layouts, external envelopes, and semantic
information directly in the software. Furthermore, a rule-based model checker
is introduced into the agentic workflow, utilizing predefined domain knowledge
to guide the LLM agents in resolving issues within the generated models and
iteratively improving model quality. Extensive experiments were conducted to
compare and analyze the performance of three different LLMs under the proposed
framework. The evaluation results demonstrate that our approach can effectively
generate high-quality, structurally rational building models that are aligned
with the abstract concepts specified by user input. Finally, an interactive
software prototype was developed to integrate the framework into the BIM
authoring software Vectorworks, showcasing the potential of modeling by
chatting.","[{'name': 'Changyu Du'}, {'name': 'Sebastian Esser'}, {'name': 'Stavros Nousias'}, {'name': 'André Borrmann'}]",2024-08-15T09:48:45Z
http://arxiv.org/abs/2408.08027v1,http://arxiv.org/abs/2408.08027v1,"Enhancing Large Language Model-based Speech Recognition by
  Contextualization for Rare and Ambiguous Words","We develop a large language model (LLM) based automatic speech recognition
(ASR) system that can be contextualized by providing keywords as prior
information in text prompts. We adopt decoder-only architecture and use our
in-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by
Japanese and English texts as the decoder. We adopt a pre-trained Whisper
encoder as an audio encoder, and the audio embeddings from the audio encoder
are projected to the text embedding space by an adapter layer and concatenated
with text embeddings converted from text prompts to form inputs to the decoder.
By providing keywords as prior information in the text prompts, we can
contextualize our LLM-based ASR system without modifying the model architecture
to transcribe ambiguous words in the input audio accurately. Experimental
results demonstrate that providing keywords to the decoder can significantly
improve the recognition performance of rare and ambiguous words.","[{'name': 'Kento Nozawa'}, {'name': 'Takashi Masuko'}, {'name': 'Toru Taniguchi'}]",2024-08-15T08:50:58Z
http://arxiv.org/abs/2408.08003v1,http://arxiv.org/abs/2408.08003v1,Leveraging Web-Crawled Data for High-Quality Fine-Tuning,"Most large language models are fine-tuned using either expensive
human-annotated data or GPT-4 generated data which cannot guarantee performance
in certain domains. We argue that although the web-crawled data often has
formatting errors causing semantic inaccuracies, it can still serve as a
valuable source for high-quality supervised fine-tuning in specific domains
without relying on advanced models like GPT-4. To this end, we create a paired
training dataset automatically by aligning web-crawled data with a smaller set
of high-quality data. By training a language model on this dataset, we can
convert web data with irregular formats into high-quality ones. Our experiments
show that training with the model-transformed data yields better results,
surpassing training with only high-quality data by an average score of 9.4% in
Chinese math problems. Additionally, our 7B model outperforms several
open-source models larger than 32B and surpasses well-known closed-source
models such as GPT-3.5, highlighting the efficacy of our approach.","[{'name': 'Jing Zhou'}, {'name': 'Chenglin Jiang'}, {'name': 'Wei Shen'}, {'name': 'Xiao Zhou'}, {'name': 'Xiaonan He'}]",2024-08-15T08:12:52Z
http://arxiv.org/abs/2408.07990v1,http://arxiv.org/abs/2408.07990v1,FuseChat: Knowledge Fusion of Chat Models,"While training large language models (LLMs) from scratch can indeed lead to
models with distinct capabilities and strengths, it incurs substantial costs
and may lead to redundancy in competencies. Knowledge fusion aims to integrate
existing LLMs of diverse architectures and capabilities into a more potent LLM
through lightweight continual training, thereby reducing the need for costly
LLM development. In this work, we propose a new framework for the knowledge
fusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we
conduct pairwise knowledge fusion on source chat LLMs of varying structures and
scales to create multiple target LLMs with identical structure and size via
lightweight fine-tuning. During this process, a statistics-based token
alignment approach is introduced as the cornerstone for fusing LLMs with
different structures. Secondly, we merge these target LLMs within the parameter
space, where we propose a novel method for determining the merging coefficients
based on the magnitude of parameter updates before and after fine-tuning. We
implement and validate FuseChat using six prominent chat LLMs with diverse
architectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,
NH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and
Qwen-1.5-Chat-72B. Experimental results on two instruction-following
benchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of
FuseChat-7B over baselines of various sizes. Our model is even comparable to
the larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.
Our code, model weights, and data are public at
\url{https://github.com/fanqiwan/FuseAI}.","[{'name': 'Fanqi Wan'}, {'name': 'Longguang Zhong'}, {'name': 'Ziyi Yang'}, {'name': 'Ruijun Chen'}, {'name': 'Xiaojun Quan'}]",2024-08-15T07:37:24Z
http://arxiv.org/abs/2408.07983v1,http://arxiv.org/abs/2408.07983v1,"ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal
  Knowledge in Large Language Models","The rapid advancements in Large Language Models (LLMs) have led to
significant improvements in various natural language processing tasks. However,
the evaluation of LLMs' legal knowledge, particularly in non-English languages
such as Arabic, remains under-explored. To address this gap, we introduce
ArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal
knowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval
consists of multiple tasks sourced from Saudi legal documents and synthesized
questions. In this work, we aim to analyze the capabilities required to solve
legal problems in Arabic and benchmark the performance of state-of-the-art
LLMs. We explore the impact of in-context learning and investigate various
evaluation methods. Additionally, we explore workflows for generating questions
with automatic validation to enhance the dataset's quality. We benchmark
multilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We
also share our methodology for creating the dataset and validation, which can
be generalized to other domains. We hope to accelerate AI research in the
Arabic Legal domain by releasing the ArabLegalEval dataset and code:
https://github.com/Thiqah/ArabLegalEval","[{'name': 'Faris Hijazi'}, {'name': 'Somayah AlHarbi'}, {'name': 'Abdulaziz AlHussein'}, {'name': 'Harethah Abu Shairah'}, {'name': 'Reem AlZahrani'}, {'name': 'Hebah AlShamlan'}, {'name': 'Omar Knio'}, {'name': 'George Turkiyyah'}]",2024-08-15T07:09:51Z
http://arxiv.org/abs/2408.07978v2,http://arxiv.org/abs/2408.07978v2,"Coupling without Communication and Drafter-Invariant Speculative
  Decoding","Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice
wants to generate a sample $a\sim P$ and Bob a sample $b \sim Q$ such that $a =
b$ with has as high of probability as possible. It is well-known that, by
sampling from an optimal coupling between the distributions, Alice and Bob can
achieve $Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total
variation distance. What if Alice and Bob must solve this same problem without
communicating at all? Perhaps surprisingly, with access to public randomness,
they can still achieve $Pr[a=b] \geq \frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)} \geq
1-2D_{TV}(P,Q)$. In fact, this bound can be obtained using a simple protocol
based on the Weighted MinHash algorithm. In this work, we explore the
communication-free coupling problem in greater depth. First, we show that an
equally simple protocol based on Gumbel sampling matches the worst-case
guarantees of the Weighted MinHash approach, but tends to perform better in
practice. Conversely, we prove that both approaches are actually sharp: no
communication-free protocol can achieve
$Pr[a=b]>\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$ in the worst-case. Finally, we
prove that, for distributions over $n$ items, there exists a scheme that uses
just $O(\log(n/\epsilon))$ bits of communication to achieve $Pr[a = b] = 1 -
D_{TV}(P,Q) - \epsilon$, i.e. to essentially match optimal coupling. Beyond our
theoretical results, we demonstrate an application of communication-free
coupling to speculative decoding, a recent method for accelerating
autoregressive large language models [Leviathan, Kalman, Matias, ICML 2023]. We
show that communication-free protocols yield a variant of speculative decoding
that we call Drafter-Invariant Speculative Decoding, which has the desirable
property that the output of the method is fixed given a fixed random seed,
regardless of what drafter is used for speculation.","[{'name': 'Majid Daliri'}, {'name': 'Christopher Musco'}, {'name': 'Ananda Theertha Suresh'}]",2024-08-15T06:52:24Z
http://arxiv.org/abs/2408.07975v1,http://arxiv.org/abs/2408.07975v1,"Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual
  Grounding and Large Language Models","This paper investigates the task of the open-ended interactive robotic
manipulation on table-top scenarios. While recent Large Language Models (LLMs)
enhance robots' comprehension of user instructions, their lack of visual
grounding constrains their ability to physically interact with the environment.
This is because the robot needs to locate the target object for manipulation
within the physical workspace. To this end, we introduce an interactive robotic
manipulation framework called Polaris, which integrates perception and
interaction by utilizing GPT-4 alongside grounded vision models. For precise
manipulation, it is essential that such grounded vision models produce detailed
object pose for the target object, rather than merely identifying pixels
belonging to them in the image. Consequently, we propose a novel
Synthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes
rendered synthetic data for training and is then transferred to real-world
manipulation tasks. The real-world performance demonstrates the efficacy of our
proposed pipeline and underscores its potential for extension to more general
categories. Moreover, real-robot experiments have showcased the impressive
performance of our framework in grasping and executing multiple manipulation
tasks. This indicates its potential to generalize to scenarios beyond the
tabletop. More information and video results are available here:
https://star-uu-wang.github.io/Polaris/","[{'name': 'Tianyu Wang'}, {'name': 'Haitao Lin'}, {'name': 'Junqiu Yu'}, {'name': 'Yanwei Fu'}]",2024-08-15T06:40:38Z
http://arxiv.org/abs/2408.07971v1,http://arxiv.org/abs/2408.07971v1,Predicting Lung Cancer Patient Prognosis with Large Language Models,"Prognosis prediction is crucial for determining optimal treatment plans for
lung cancer patients. Traditionally, such predictions relied on models
developed from retrospective patient data. Recently, large language models
(LLMs) have gained attention for their ability to process and generate text
based on extensive learned knowledge. In this study, we evaluate the potential
of GPT-4o mini and GPT-3.5 in predicting the prognosis of lung cancer patients.
We collected two prognosis datasets, i.e., survival and post-operative
complication datasets, and designed multiple tasks to assess the models'
performance comprehensively. Logistic regression models were also developed as
baselines for comparison. The experimental results demonstrate that LLMs can
achieve competitive, and in some tasks superior, performance in lung cancer
prognosis prediction compared to data-driven logistic regression models despite
not using additional patient data. These findings suggest that LLMs can be
effective tools for prognosis prediction in lung cancer, particularly when
patient data is limited or unavailable.","[{'name': 'Danqing Hu'}, {'name': 'Bing Liu'}, {'name': 'Xiang Li'}, {'name': 'Xiaofeng Zhu'}, {'name': 'Nan Wu'}]",2024-08-15T06:36:27Z
http://arxiv.org/abs/2408.07955v1,http://arxiv.org/abs/2408.07955v1,"GERestaurant: A German Dataset of Annotated Restaurant Reviews for
  Aspect-Based Sentiment Analysis","We present GERestaurant, a novel dataset consisting of 3,078 German language
restaurant reviews manually annotated for Aspect-Based Sentiment Analysis
(ABSA). All reviews were collected from Tripadvisor, covering a diverse
selection of restaurants, including regional and international cuisine with
various culinary styles. The annotations encompass both implicit and explicit
aspects, including all aspect terms, their corresponding aspect categories, and
the sentiments expressed towards them. Furthermore, we provide baseline scores
for the four ABSA tasks Aspect Category Detection, Aspect Category Sentiment
Analysis, End-to-End ABSA and Target Aspect Sentiment Detection as a reference
point for future advances. The dataset fills a gap in German language resources
and facilitates exploration of ABSA in the restaurant domain.","[{'name': 'Nils Constantin Hellwig'}, {'name': 'Jakob Fehle'}, {'name': 'Markus Bink'}, {'name': 'Christian Wolff'}]",2024-08-15T06:08:53Z
http://arxiv.org/abs/2408.07930v2,http://arxiv.org/abs/2408.07930v2,"MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and
  Iterative Sub-SQL Refinement for Text-to-SQL","Recent In-Context Learning based methods have achieved remarkable success in
Text-to-SQL task. However, there is still a large gap between the performance
of these models and human performance on datasets with complex database schema
and difficult questions, such as BIRD. Besides, existing work has neglected to
supervise intermediate steps when solving questions iteratively with question
decomposition methods, and the schema linking methods used in these works are
very rudimentary. To address these issues, we propose MAG-SQL, a multi-agent
generative approach with soft schema linking and iterative Sub-SQL refinement.
In our framework, an entity-based method with tables' summary is used to select
the columns in database, and a novel targets-conditions decomposition method is
introduced to decompose those complex questions. Additionally, we build a
iterative generating module which includes a Sub-SQL Generator and Sub-SQL
Refiner, introducing external oversight for each step of generation. Through a
series of ablation studies, the effectiveness of each agent in our framework
has been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL
achieves an execution accuracy of 61.08%, compared to the baseline accuracy of
46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.
Besides, our approach makes similar progress on Spider.","[{'name': 'Wenxuan Xie'}, {'name': 'Gaochen Wu'}, {'name': 'Bowen Zhou'}]",2024-08-15T04:57:55Z
http://arxiv.org/abs/2408.08335v1,http://arxiv.org/abs/2408.08335v1,Plan with Code: Comparing approaches for robust NL to DSL generation,"Planning in code is considered a more reliable approach for many
orchestration tasks. This is because code is more tractable than steps
generated via Natural Language and make it easy to support more complex
sequences by abstracting deterministic logic into functions. It also allows
spotting issues with incorrect function names with the help of parsing checks
that can be run on code. Progress in Code Generation methodologies, however,
remains limited to general-purpose languages like C, C++, and Python. LLMs
continue to face challenges with custom function names in Domain Specific
Languages or DSLs, leading to higher hallucination rates and syntax errors.
This is more common for custom function names, that are typically part of the
plan. Moreover, keeping LLMs up-to-date with newer function names is an issue.
This poses a challenge for scenarios like task planning over a large number of
APIs, since the plan is represented as a DSL having custom API names. In this
paper, we focus on workflow automation in RPA (Robotic Process Automation)
domain as a special case of task planning. We present optimizations for using
Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with
an ablation study comparing these strategies with a fine-tuned model. Our
results showed that the fine-tuned model scored the best on code similarity
metric. However, with our optimizations, RAG approach is able to match the
quality for in-domain API names in the test set. Additionally, it offers
significant advantage for out-of-domain or unseen API names, outperforming
Fine-Tuned model on similarity metric by 7 pts.","[{'name': 'Nastaran Bassamzadeh'}, {'name': 'Chhaya Methani'}]",2024-08-15T04:29:33Z
http://arxiv.org/abs/2408.07910v1,http://arxiv.org/abs/2408.07910v1,"DM2RM: Dual-Mode Multimodal Ranking for Target Objects and Receptacles
  Based on Open-Vocabulary Instructions","In this study, we aim to develop a domestic service robot (DSR) that, guided
by open-vocabulary instructions, can carry everyday objects to the specified
pieces of furniture. Few existing methods handle mobile manipulation tasks with
open-vocabulary instructions in the image retrieval setting, and most do not
identify both the target objects and the receptacles. We propose the Dual-Mode
Multimodal Ranking model (DM2RM), which enables images of both the target
objects and receptacles to be retrieved using a single model based on
multimodal foundation models. We introduce a switching mechanism that leverages
a mode token and phrase identification via a large language model to switch the
embedding space based on the prediction target. To evaluate the DM2RM, we
construct a novel dataset including real-world images collected from hundreds
of building-scale environments and crowd-sourced instructions with referring
expressions. The evaluation results show that the proposed DM2RM outperforms
previous approaches in terms of standard metrics in image retrieval settings.
Furthermore, we demonstrate the application of the DM2RM on a standardized
real-world DSR platform including fetch-and-carry actions, where it achieves a
task success rate of 82% despite the zero-shot transfer setting. Demonstration
videos, code, and more materials are available at
https://kkrr10.github.io/dm2rm/.","[{'name': 'Ryosuke Korekata'}, {'name': 'Kanta Kaneda'}, {'name': 'Shunya Nagashima'}, {'name': 'Yuto Imai'}, {'name': 'Komei Sugiura'}]",2024-08-15T03:34:02Z
http://arxiv.org/abs/2408.07904v1,http://arxiv.org/abs/2408.07904v1,Assessing Language Models' Worldview for Fiction Generation,"The use of Large Language Models (LLMs) has become ubiquitous, with abundant
applications in computational creativity. One such application is fictional
story generation. Fiction is a narrative that occurs in a story world that is
slightly different than ours. With LLMs becoming writing partners, we question
how suitable they are to generate fiction. This study investigates the ability
of LLMs to maintain a state of world essential to generate fiction. Through a
series of questions to nine LLMs, we find that only two models exhibit
consistent worldview, while the rest are self-conflicting. Subsequent analysis
of stories generated by four models revealed a strikingly uniform narrative
pattern. This uniformity across models further suggests a lack of `state'
necessary for fiction. We highlight the limitations of current LLMs in fiction
writing and advocate for future research to test and create story worlds for
LLMs to reside in. All code, dataset, and the generated responses can be found
in https://github.com/tanny411/llm-reliability-and-consistency-evaluation.","[{'name': 'Aisha Khatun'}, {'name': 'Daniel G. Brown'}]",2024-08-15T03:19:41Z
http://arxiv.org/abs/2408.07873v1,http://arxiv.org/abs/2408.07873v1,"Words Matter: Reducing Stigma in Online Conversations about Substance
  Use with Large Language Models","Stigma is a barrier to treatment for individuals struggling with substance
use disorders (SUD), which leads to significantly lower treatment engagement
rates. With only 7% of those affected receiving any form of help, societal
stigma not only discourages individuals with SUD from seeking help but isolates
them, hindering their recovery journey and perpetuating a cycle of shame and
self-doubt. This study investigates how stigma manifests on social media,
particularly Reddit, where anonymity can exacerbate discriminatory behaviors.
We analyzed over 1.2 million posts, identifying 3,207 that exhibited
stigmatizing language towards people who use substances (PWUS). Using Informed
and Stylized LLMs, we develop a model for de-stigmatization of these
expressions into empathetic language, resulting in 1,649 reformed phrase pairs.
Our paper contributes to the field by proposing a computational framework for
analyzing stigma and destigmatizing online content, and delving into the
linguistic features that propagate stigma towards PWUS. Our work not only
enhances understanding of stigma's manifestations online but also provides
practical tools for fostering a more supportive digital environment for those
affected by SUD. Code and data will be made publicly available upon acceptance.","[{'name': 'Layla Bouzoubaa'}, {'name': 'Elham Aghakhani'}, {'name': 'Rezvaneh Rezapour'}]",2024-08-15T01:00:28Z
http://arxiv.org/abs/2408.07852v1,http://arxiv.org/abs/2408.07852v1,"Training Language Models on the Knowledge Graph: Insights on
  Hallucinations and Their Detectability","While many capabilities of language models (LMs) improve with increased
training budget, the influence of scale on hallucinations is not yet fully
understood. Hallucinations come in many forms, and there is no universally
accepted definition. We thus focus on studying only those hallucinations where
a correct answer appears verbatim in the training set. To fully control the
training data content, we construct a knowledge graph (KG)-based dataset, and
use it to train a set of increasingly large LMs. We find that for a fixed
dataset, larger and longer-trained LMs hallucinate less. However, hallucinating
on $\leq5$% of the training data requires an order of magnitude larger model,
and thus an order of magnitude more compute, than Hoffmann et al. (2022)
reported was optimal. Given this costliness, we study how hallucination
detectors depend on scale. While we see detector size improves performance on
fixed LM's outputs, we find an inverse relationship between the scale of the LM
and the detectability of its hallucinations.","[{'name': 'Jiri Hron'}, {'name': 'Laura Culp'}, {'name': 'Gamaleldin Elsayed'}, {'name': 'Rosanne Liu'}, {'name': 'Ben Adlam'}, {'name': 'Maxwell Bileschi'}, {'name': 'Bernd Bohnet'}, {'name': 'JD Co-Reyes'}, {'name': 'Noah Fiedel'}, {'name': 'C. Daniel Freeman'}, {'name': 'Izzeddin Gur'}, {'name': 'Kathleen Kenealy'}, {'name': 'Jaehoon Lee'}, {'name': 'Peter J. Liu'}, {'name': 'Gaurav Mishra'}, {'name': 'Igor Mordatch'}, {'name': 'Azade Nova'}, {'name': 'Roman Novak'}, {'name': 'Aaron Parisi'}, {'name': 'Jeffrey Pennington'}, {'name': 'Alex Rizkowsky'}, {'name': 'Isabelle Simpson'}, {'name': 'Hanie Sedghi'}, {'name': 'Jascha Sohl-dickstein'}, {'name': 'Kevin Swersky'}, {'name': 'Sharad Vikram'}, {'name': 'Tris Warkentin'}, {'name': 'Lechao Xiao'}, {'name': 'Kelvin Xu'}, {'name': 'Jasper Snoek'}, {'name': 'Simon Kornblith'}]",2024-08-14T23:34:28Z
http://arxiv.org/abs/2408.07851v1,http://arxiv.org/abs/2408.07851v1,"SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion
  Recognition","Speech emotion recognition (SER) has made significant strides with the advent
of powerful self-supervised learning (SSL) models. However, the generalization
of these models to diverse languages and emotional expressions remains a
challenge. We propose a large-scale benchmark to evaluate the robustness and
adaptability of state-of-the-art SER models in both in-domain and out-of-domain
settings. Our benchmark includes a diverse set of multilingual datasets,
focusing on less commonly used corpora to assess generalization to new data. We
employ logit adjustment to account for varying class distributions and
establish a single dataset cluster for systematic evaluation. Surprisingly, we
find that the Whisper model, primarily designed for automatic speech
recognition, outperforms dedicated SSL models in cross-lingual SER. Our results
highlight the need for more robust and generalizable SER models, and our
benchmark serves as a valuable resource to drive future research in this
direction.","[{'name': 'Mohamed Osman'}, {'name': 'Daniel Z. Kaplan'}, {'name': 'Tamer Nadeem'}]",2024-08-14T23:33:10Z
http://arxiv.org/abs/2408.08333v1,http://arxiv.org/abs/2408.08333v1,CodeMirage: Hallucinations in Code Generated by Large Language Models,"Large Language Models (LLMs) have shown promising potentials in program
generation and no-code automation. However, LLMs are prone to generate
hallucinations, i.e., they generate text which sounds plausible but is
incorrect. Although there has been a recent surge in research on LLM
hallucinations for text generation, similar hallucination phenomenon can happen
in code generation. Sometimes the generated code can have syntactical or
logical errors as well as more advanced issues like security vulnerabilities,
memory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in
code generation and development in general, it becomes imperative to
investigate hallucinations in code generation. To the best of our knowledge,
this is the first attempt at studying hallucinations in the code generated by
LLMs. We start by introducing the code hallucination definition and a
comprehensive taxonomy of code hallucination types. We propose the first
benchmark CodeMirage dataset for code hallucinations. The benchmark contains
1,137 GPT-3.5 generated hallucinated code snippets for Python programming
problems from two base datasets - HumanEval and MBPP. We then propose the
methodology for code hallucination detection and experiment with open source
LLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using
one-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and
gives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset.
Towards the end, we discuss various mitigation strategies for code
hallucinations and conclude our work.","[{'name': 'Vibhor Agarwal'}, {'name': 'Yulong Pei'}, {'name': 'Salwa Alamir'}, {'name': 'Xiaomo Liu'}]",2024-08-14T22:53:07Z
http://arxiv.org/abs/2408.07840v1,http://arxiv.org/abs/2408.07840v1,"ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction
  Based on Large Language Model","In the realm of event prediction, temporal knowledge graph forecasting (TKGF)
stands as a pivotal technique. Previous approaches face the challenges of not
utilizing experience during testing and relying on a single short-term history,
which limits adaptation to evolving data. In this paper, we introduce the
Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by
integrating dynamic causal rule mining (DCRM) and dual history augmented
generation (DHAG). DCRM dynamically constructs causal rules from real-time
data, allowing for swift adaptation to new causal relationships. In parallel,
DHAG merges short-term and long-term historical contexts, leveraging a
bi-branch approach to enrich event prediction. Our framework demonstrates
notable performance enhancements across diverse datasets, with significant
Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language
models (LLMs) for event prediction without necessitating extensive retraining.
The ONSEP framework not only advances the field of TKGF but also underscores
the potential of neural-symbolic approaches in adapting to dynamic data
environments.","[{'name': 'Xuanqing Yu'}, {'name': 'Wangtao Sun'}, {'name': 'Jingwei Li'}, {'name': 'Kang Liu'}, {'name': 'Chengbao Liu'}, {'name': 'Jie Tan'}]",2024-08-14T22:28:19Z
http://arxiv.org/abs/2408.07702v2,http://arxiv.org/abs/2408.07702v2,"The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned
  Language Models","Schema linking is a crucial step in Text-to-SQL pipelines. Its goal is to
retrieve the relevant tables and columns of a target database for a user's
query while disregarding irrelevant ones. However, imperfect schema linking can
often exclude required columns needed for accurate query generation. In this
work, we revisit schema linking when using the latest generation of large
language models (LLMs). We find empirically that newer models are adept at
utilizing relevant schema elements during generation even in the presence of
large numbers of irrelevant ones. As such, our Text-to-SQL pipeline entirely
forgoes schema linking in cases where the schema fits within the model's
context window in order to minimize issues due to filtering required schema
elements. Furthermore, instead of filtering contextual information, we
highlight techniques such as augmentation, selection, and correction, and adopt
them to improve the accuracy of our Text-to-SQL pipeline. Our approach ranks
first on the BIRD benchmark achieving an accuracy of 71.83%.","[{'name': 'Karime Maamari'}, {'name': 'Fadhil Abubaker'}, {'name': 'Daniel Jaroslawicz'}, {'name': 'Amine Mhedhbi'}]",2024-08-14T17:59:04Z
http://arxiv.org/abs/2408.07697v1,http://arxiv.org/abs/2408.07697v1,Quantifying over Optimum Answer Sets,"Answer Set Programming with Quantifiers (ASP(Q)) has been introduced to
provide a natural extension of ASP modeling to problems in the polynomial
hierarchy (PH). However, ASP(Q) lacks a method for encoding in an elegant and
compact way problems requiring a polynomial number of calls to an oracle in
$\Sigma_n^p$ (that is, problems in $\Delta_{n+1}^p$). Such problems include, in
particular, optimization problems. In this paper we propose an extension of
ASP(Q), in which component programs may contain weak constraints. Weak
constraints can be used both for expressing local optimization within
quantified component programs and for modeling global optimization criteria. We
showcase the modeling capabilities of the new formalism through various
application scenarios. Further, we study its computational properties obtaining
complexity results and unveiling non-obvious characteristics of ASP(Q) programs
with weak constraints.","[{'name': 'Giuseppe Mazzotta'}, {'name': 'Francesco Ricca'}, {'name': 'Mirek Truszczynski'}]",2024-08-14T17:53:13Z
http://arxiv.org/abs/2408.07676v1,http://arxiv.org/abs/2408.07676v1,"Enhanced Detection of Conversational Mental Manipulation Through
  Advanced Prompting Techniques","This study presents a comprehensive, long-term project to explore the
effectiveness of various prompting techniques in detecting dialogical mental
manipulation. We implement Chain-of-Thought prompting with Zero-Shot and
Few-Shot settings on a binary mental manipulation detection task, building upon
existing work conducted with Zero-Shot and Few- Shot prompting. Our primary
objective is to decipher why certain prompting techniques display superior
performance, so as to craft a novel framework tailored for detection of mental
manipulation. Preliminary findings suggest that advanced prompting techniques
may not be suitable for more complex models, if they are not trained through
example-based learning.","[{'name': 'Ivory Yang'}, {'name': 'Xiaobo Guo'}, {'name': 'Sean Xie'}, {'name': 'Soroush Vosoughi'}]",2024-08-14T17:23:12Z
http://arxiv.org/abs/2408.07666v2,http://arxiv.org/abs/2408.07666v2,"Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,
  Applications and Opportunities","Model merging is an efficient empowerment technique in the machine learning
community that does not require the collection of raw training data and does
not require expensive computation. As model merging becomes increasingly
prevalent across various fields, it is crucial to understand the available
model merging techniques comprehensively. However, there is a significant gap
in the literature regarding a systematic and thorough review of these
techniques. This survey provides a comprehensive overview of model merging
methods and theories, their applications in various domains and settings, and
future research directions. Specifically, we first propose a new taxonomic
approach that exhaustively discusses existing model merging methods. Secondly,
we discuss the application of model merging techniques in large language
models, multimodal large language models, and 10+ machine learning subfields,
including continual learning, multi-task learning, few-shot learning, etc.
Finally, we highlight the remaining challenges of model merging and discuss
future research directions. A comprehensive list of papers about model merging
is available at
\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.","[{'name': 'Enneng Yang'}, {'name': 'Li Shen'}, {'name': 'Guibing Guo'}, {'name': 'Xingwei Wang'}, {'name': 'Xiaochun Cao'}, {'name': 'Jie Zhang'}, {'name': 'Dacheng Tao'}]",2024-08-14T16:58:48Z
http://arxiv.org/abs/2408.07665v1,http://arxiv.org/abs/2408.07665v1,"Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech
  Large Language Models","Warning: This paper may contain texts with uncomfortable content.
  Large Language Models (LLMs) have achieved remarkable performance in various
tasks, including those involving multimodal data like speech. However, these
models often exhibit biases due to the nature of their training data. Recently,
more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent
need to address these biases. This study introduces Spoken Stereoset, a dataset
specifically designed to evaluate social biases in SLLMs. By examining how
different models respond to speech from diverse demographic groups, we aim to
identify these biases. Our experiments reveal significant insights into their
performance and bias levels. The findings indicate that while most models show
minimal bias, some still exhibit slightly stereotypical or anti-stereotypical
tendencies.","[{'name': 'Yi-Cheng Lin'}, {'name': 'Wei-Chih Chen'}, {'name': 'Hung-yi Lee'}]",2024-08-14T16:55:06Z
http://arxiv.org/abs/2408.07663v1,http://arxiv.org/abs/2408.07663v1,"Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining
  of Probability Distributions","Large language models are susceptible to jailbreak attacks, which can result
in the generation of harmful content. While prior defenses mitigate these risks
by perturbing or inspecting inputs, they ignore competing objectives, the
underlying cause of alignment failures. In this paper, we propose
Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive
decoding to address the root causes of jailbreak issues. We first define the
Competitive Index to quantify alignment failures and utilize feedback from
self-evaluation to compute post-alignment logits. Then, AED adaptively combines
AED and post-alignment logits with the original logits to obtain harmless and
helpful distributions. Consequently, our method enhances safety alignment while
maintaining helpfulness. We conduct experiments across five models and four
common jailbreaks, with the results validating the effectiveness of our
approach. Code is available at https://github.com/GIGABaozi/AED.git.","[{'name': 'Quan Liu'}, {'name': 'Zhenhong Zhou'}, {'name': 'Longzhu He'}, {'name': 'Yi Liu'}, {'name': 'Wei Zhang'}, {'name': 'Sen Su'}]",2024-08-14T16:51:21Z
http://arxiv.org/abs/2408.07648v1,http://arxiv.org/abs/2408.07648v1,See It All: Contextualized Late Aggregation for 3D Dense Captioning,"3D dense captioning is a task to localize objects in a 3D scene and generate
descriptive sentences for each object. Recent approaches in 3D dense captioning
have adopted transformer encoder-decoder frameworks from object detection to
build an end-to-end pipeline without hand-crafted components. However, these
approaches struggle with contradicting objectives where a single query
attention has to simultaneously view both the tightly localized object regions
and contextual environment. To overcome this challenge, we introduce SIA
(See-It-All), a transformer pipeline that engages in 3D dense captioning with a
novel paradigm called late aggregation. SIA simultaneously decodes two sets of
queries-context query and instance query. The instance query focuses on
localization and object attribute descriptions, while the context query
versatilely captures the region-of-interest of relationships between multiple
objects or with the global scene, then aggregated afterwards (i.e., late
aggregation) via simple distance-based measures. To further enhance the quality
of contextualized caption generation, we design a novel aggregator to generate
a fully informed caption based on the surrounding context, the global
environment, and object instances. Extensive experiments on two of the most
widely-used 3D dense captioning datasets demonstrate that our proposed method
achieves a significant improvement over prior methods.","[{'name': 'Minjung Kim'}, {'name': 'Hyung Suk Lim'}, {'name': 'Seung Hwan Kim'}, {'name': 'Soonyoung Lee'}, {'name': 'Bumsoo Kim'}, {'name': 'Gunhee Kim'}]",2024-08-14T16:19:18Z
http://arxiv.org/abs/2408.07637v1,http://arxiv.org/abs/2408.07637v1,Hierarchical Working Memory and a New Magic Number,"The extremely limited working memory span, typically around four items,
contrasts sharply with our everyday experience of processing much larger
streams of sensory information concurrently. This disparity suggests that
working memory can organize information into compact representations such as
chunks, yet the underlying neural mechanisms remain largely unknown. Here, we
propose a recurrent neural network model for chunking within the framework of
the synaptic theory of working memory. We showed that by selectively
suppressing groups of stimuli, the network can maintain and retrieve the
stimuli in chunks, hence exceeding the basic capacity. Moreover, we show that
our model can dynamically construct hierarchical representations within working
memory through hierarchical chunking. A consequence of this proposed mechanism
is a new limit on the number of items that can be stored and subsequently
retrieved from working memory, depending only on the basic working memory
capacity when chunking is not invoked. Predictions from our model were
confirmed by analyzing single-unit responses in epileptic patients and memory
experiments with verbal material. Our work provides a novel conceptual and
analytical framework for understanding the on-the-fly organization of
information in the brain that is crucial for cognition.","[{'name': 'Weishun Zhong'}, {'name': 'Mikhail Katkov'}, {'name': 'Misha Tsodyks'}]",2024-08-14T16:03:47Z
http://arxiv.org/abs/2408.07611v1,http://arxiv.org/abs/2408.07611v1,"WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation
  Integrating Web Search and Knowledge Graphs","Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce ""phantom"" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a ""Retrieval-Augmented
Generation (RAG)"" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.","[{'name': 'Weijian Xie'}, {'name': 'Xuefeng Liang'}, {'name': 'Yuhui Liu'}, {'name': 'Kaihua Ni'}, {'name': 'Hong Cheng'}, {'name': 'Zetian Hu'}]",2024-08-14T15:19:16Z
http://arxiv.org/abs/2408.07599v1,http://arxiv.org/abs/2408.07599v1,"Assessing the Role of Lexical Semantics in Cross-lingual Transfer
  through Controlled Manipulations","While cross-linguistic model transfer is effective in many settings, there is
still limited understanding of the conditions under which it works. In this
paper, we focus on assessing the role of lexical semantics in cross-lingual
transfer, as we compare its impact to that of other language properties.
Examining each language property individually, we systematically analyze how
differences between English and a target language influence the capacity to
align the language with an English pretrained representation space. We do so by
artificially manipulating the English sentences in ways that mimic specific
characteristics of the target language, and reporting the effect of each
manipulation on the quality of alignment with the representation space. We show
that while properties such as the script or word order only have a limited
impact on alignment quality, the degree of lexical matching between the two
languages, which we define using a measure of translation entropy, greatly
affects it.","[{'name': 'Roy Ilani'}, {'name': 'Taelin Karidi'}, {'name': 'Omri Abend'}]",2024-08-14T14:59:20Z
http://arxiv.org/abs/2408.07583v1,http://arxiv.org/abs/2408.07583v1,"Transformers and Large Language Models for Efficient Intrusion Detection
  Systems: A Comprehensive Survey","With significant advancements in Transformers LLMs, NLP has extended its
reach into many research fields due to its enhanced capabilities in text
generation and user interaction. One field benefiting greatly from these
advancements is cybersecurity. In cybersecurity, many parameters that need to
be protected and exchanged between senders and receivers are in the form of
text and tabular data, making NLP a valuable tool in enhancing the security
measures of communication protocols. This survey paper provides a comprehensive
analysis of the utilization of Transformers and LLMs in cyber-threat detection
systems. The methodology of paper selection and bibliometric analysis is
outlined to establish a rigorous framework for evaluating existing research.
The fundamentals of Transformers are discussed, including background
information on various cyber-attacks and datasets commonly used in this field.
The survey explores the application of Transformers in IDSs, focusing on
different architectures such as Attention-based models, LLMs like BERT and GPT,
CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.
Furthermore, it explores the diverse environments and applications where
Transformers and LLMs-based IDS have been implemented, including computer
networks, IoT devices, critical infrastructure protection, cloud computing,
SDN, as well as in autonomous vehicles. The paper also addresses research
challenges and future directions in this area, identifying key issues such as
interpretability, scalability, and adaptability to evolving threats, and more.
Finally, the conclusion summarizes the findings and highlights the significance
of Transformers and LLMs in enhancing cyber-threat detection capabilities,
while also outlining potential avenues for further research and development.",[{'name': 'Hamza Kheddar'}],2024-08-14T14:28:11Z
http://arxiv.org/abs/2408.07543v2,http://arxiv.org/abs/2408.07543v2,"MathScape: Evaluating MLLMs in multimodal Math Scenarios through a
  Hierarchical Benchmark","With the development of Multimodal Large Language Models (MLLMs), the
evaluation of multimodal models in the context of mathematical problems has
become a valuable research field. Multimodal visual-textual mathematical
reasoning serves as a critical indicator for evaluating the comprehension and
complex multi-step quantitative reasoning abilities of MLLMs. However, previous
multimodal math benchmarks have not sufficiently integrated visual and textual
information. To address this gap, we proposed MathScape, a new benchmark that
emphasizes the understanding and application of combined visual and textual
information. MathScape is designed to evaluate photo-based math problem
scenarios, assessing the theoretical understanding and application ability of
MLLMs through a categorical hierarchical approach. We conduct a
multi-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark
is challenging even for the most sophisticated models. By analyzing the
evaluation results, we identify the limitations of MLLMs, offering valuable
insights for enhancing model performance.","[{'name': 'Minxuan Zhou'}, {'name': 'Hao Liang'}, {'name': 'Tianpeng Li'}, {'name': 'Zhiyu Wu'}, {'name': 'Mingan Lin'}, {'name': 'Linzhuang Sun'}, {'name': 'Yaqi Zhou'}, {'name': 'Yan Zhang'}, {'name': 'Xiaoqin Huang'}, {'name': 'Yicong Chen'}, {'name': 'Yujing Qiao'}, {'name': 'Weipeng Chen'}, {'name': 'Bin Cui'}, {'name': 'Wentao Zhang'}, {'name': 'Zenan Zhou'}]",2024-08-14T13:23:43Z
http://arxiv.org/abs/2408.07531v1,http://arxiv.org/abs/2408.07531v1,"Development of a Multi-Agent Clinical Decision Support System for Korean
  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in
  Emergency Departments","Emergency department (ED) overcrowding and the complexity of rapid
decision-making in critical care settings pose significant challenges to
healthcare systems worldwide. While clinical decision support systems (CDSS)
have shown promise, the integration of large language models (LLMs) offers new
possibilities for enhancing triage accuracy and clinical decision-making. This
study presents an LLM-driven CDSS designed to assist ED physicians and nurses
in patient triage, treatment planning, and overall emergency care management.
  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,
orchestrated by CrewAI and Langchain. The system comprises four AI agents
emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED
Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for
triage assessment and integrates with the RxNorm API for medication management.
  The model was evaluated using the Asclepius dataset, with performance
assessed by a clinical emergency medicine specialist. The CDSS demonstrated
high accuracy in triage decision-making compared to the baseline of a
single-agent system. Furthermore, the system exhibited strong performance in
critical areas, including primary diagnosis, critical findings identification,
disposition decision-making, treatment planning, and resource allocation.
  Our multi-agent CDSS demonstrates significant potential for supporting
comprehensive emergency care management. By leveraging state-of-the-art AI
technologies, this system offers a scalable and adaptable tool that could
enhance emergency medical care delivery, potentially alleviating ED
overcrowding and improving patient outcomes. This work contributes to the
growing field of AI applications in emergency medicine and offers a promising
direction for future research and clinical implementation.","[{'name': 'Seungjun Han'}, {'name': 'Wongyung Choi'}]",2024-08-14T13:03:41Z
http://arxiv.org/abs/2408.07505v1,http://arxiv.org/abs/2408.07505v1,Large Language Models Know What Makes Exemplary Contexts,"In-context learning (ICL) has proven to be a significant capability with the
advancement of Large Language models (LLMs). By instructing LLMs using few-shot
demonstrative examples, ICL enables them to perform a wide range of tasks
without needing to update millions of parameters. This paper presents a unified
framework for LLMs that allows them to self-select influential in-context
examples to compose their contexts; self-rank candidates with different
demonstration compositions; self-optimize the demonstration selection and
ordering through reinforcement learning. Specifically, our method designs a
parameter-efficient retrieval head that generates the optimized demonstration
after training with rewards from LLM's own preference. Experimental results
validate the proposed method's effectiveness in enhancing ICL performance.
Additionally, our approach effectively identifies and selects the most
representative examples for the current task, and includes more diversity in
retrieval.","[{'name': 'Quanyu Long'}, {'name': 'Jianda Chen'}]",2024-08-14T12:32:41Z
http://arxiv.org/abs/2408.07479v1,http://arxiv.org/abs/2408.07479v1,"A Study on Bias Detection and Classification in Natural Language
  Processing","Human biases have been shown to influence the performance of models and
algorithms in various fields, including Natural Language Processing. While the
study of this phenomenon is garnering focus in recent years, the available
resources are still relatively scarce, often focusing on different forms or
manifestations of biases. The aim of our work is twofold: 1) gather
publicly-available datasets and determine how to better combine them to
effectively train models in the task of hate speech detection and
classification; 2) analyse the main issues with these datasets, such as
scarcity, skewed resources, and reliance on non-persistent data. We discuss
these issues in tandem with the development of our experiments, in which we
show that the combinations of different datasets greatly impact the models'
performance.","[{'name': 'Ana Sofia Evans'}, {'name': 'Helena Moniz'}, {'name': 'Luísa Coheur'}]",2024-08-14T11:49:24Z
http://arxiv.org/abs/2408.07471v1,http://arxiv.org/abs/2408.07471v1,"Bridging and Modeling Correlations in Pairwise Data for Direct
  Preference Optimization","Direct preference optimization (DPO), a widely adopted offline preference
optimization algorithm, aims to align large language models (LLMs) with
human-desired behaviors using pairwise preference data. However, the winning
response and the losing response within pairwise data are generated isolatedly,
leading to weak correlations between them as well as suboptimal alignment
performance. To address this issue, we propose an effective framework named
BMC, for bridging and modeling correlations in pairwise data. Firstly, we
increase the consistency and informativeness of the pairwise preference signals
by targeted modifications, synthesizing a pseudo winning response through
improving the losing response based on the winning response. Secondly, we
identify that DPO alone is insufficient to model these correlations and capture
nuanced variations. Therefore, we propose learning token-level correlations by
dynamically leveraging the policy model's confidence during training.
Comprehensive experiments on QA, math, and instruction-following tasks
demonstrate the effectiveness of our approach, significantly surpassing
competitive baselines, including DPO. Additionally, our in-depth quantitative
analysis reveals the reasons behind our method's superior performance over DPO
and showcases its versatility to other DPO variants.","[{'name': 'Yuxin Jiang'}, {'name': 'Bo Huang'}, {'name': 'Yufei Wang'}, {'name': 'Xingshan Zeng'}, {'name': 'Liangyou Li'}, {'name': 'Yasheng Wang'}, {'name': 'Xin Jiang'}, {'name': 'Lifeng Shang'}, {'name': 'Ruiming Tang'}, {'name': 'Wei Wang'}]",2024-08-14T11:29:47Z
http://arxiv.org/abs/2408.07465v1,http://arxiv.org/abs/2408.07465v1,Large Language Models Prompting With Episodic Memory,"Prompt optimization is essential for enhancing the performance of Large
Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks,
particularly in scenarios of few-shot learning where training examples are
incorporated directly into the prompt. Despite the growing interest in
optimizing prompts with few-shot examples, existing methods for prompt
optimization are often resource-intensive or perform inadequately. In this
work, we propose PrOmpting with Episodic Memory (POEM), a novel prompt
optimization technique that is simple, efficient, and demonstrates strong
generalization capabilities. We approach prompt optimization as a Reinforcement
Learning (RL) challenge, using episodic memory to archive combinations of input
data, permutations of few-shot examples, and the rewards observed during
training. In the testing phase, we optimize the sequence of examples for each
test query by selecting the sequence that yields the highest total rewards from
the top-k most similar training examples in the episodic memory. Our results
show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over
5.3% in various text classification tasks. Furthermore, our approach adapts
well to broader language understanding tasks, consistently outperforming
conventional heuristic methods for ordering examples.","[{'name': 'Dai Do'}, {'name': 'Quan Tran'}, {'name': 'Svetha Venkatesh'}, {'name': 'Hung Le'}]",2024-08-14T11:19:28Z
http://arxiv.org/abs/2408.07457v1,http://arxiv.org/abs/2408.07457v1,From Brazilian Portuguese to European Portuguese,"Brazilian Portuguese and European Portuguese are two varieties of the same
language and, despite their close similarities, they exhibit several
differences. However, there is a significant disproportion in the availability
of resources between the two variants, with Brazilian Portuguese having more
abundant resources. This inequity can impact the quality of translation
services accessible to European Portuguese speakers. To address this issue, we
propose the development of a Brazilian Portuguese to European Portuguese
translation system, leveraging recent advancements in neural architectures and
models. To evaluate the performance of such systems, we manually curated a gold
test set comprising 500 sentences across five different topics. Each sentence
in the gold test set has two distinct references, facilitating a
straightforward evaluation of future translation models. We experimented with
various models by fine-tuning existing Large Language Models using parallel
data extracted from movie subtitles and TED Talks transcripts in both Brazilian
and European Portuguese. Our evaluation involved the use of conventional
automatic metrics as well as a human evaluation. In addition, all models were
compared against ChatGPT 3.5 Turbo, which currently yields the best results.","[{'name': 'João Sanches'}, {'name': 'Rui Ribeiro'}, {'name': 'Luísa Coheur'}]",2024-08-14T10:58:48Z
http://arxiv.org/abs/2408.07453v1,http://arxiv.org/abs/2408.07453v1,"Fact or Fiction? Improving Fact Verification with Knowledge Graphs
  through Simplified Subgraph Retrievals","Despite recent success in natural language processing (NLP), fact
verification still remains a difficult task. Due to misinformation spreading
increasingly fast, attention has been directed towards automatically verifying
the correctness of claims. In the domain of NLP, this is usually done by
training supervised machine learning models to verify claims by utilizing
evidence from trustworthy corpora. We present efficient methods for verifying
claims on a dataset where the evidence is in the form of structured knowledge
graphs. We use the FactKG dataset, which is constructed from the DBpedia
knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval
process, from fine-tuned language models to simple logical retrievals, we are
able to construct models that both require less computational resources and
achieve better test-set accuracy.",[{'name': 'Tobias A. Opsahl'}],2024-08-14T10:46:15Z
http://arxiv.org/abs/2408.07452v1,http://arxiv.org/abs/2408.07452v1,CMU's IWSLT 2024 Simultaneous Speech Translation System,"This paper describes CMU's submission to the IWSLT 2024 Simultaneous Speech
Translation (SST) task for translating English speech to German text in a
streaming manner. Our end-to-end speech-to-text (ST) system integrates the
WavLM speech encoder, a modality adapter, and the Llama2-7B-Base model as the
decoder. We employ a two-stage training approach: initially, we align the
representations of speech and text, followed by full fine-tuning. Both stages
are trained on MuST-c v2 data with cross-entropy loss. We adapt our offline ST
model for SST using a simple fixed hold-n policy. Experiments show that our
model obtains an offline BLEU score of 31.1 and a BLEU score of 29.5 under 2
seconds latency on the MuST-C-v2 tst-COMMON.","[{'name': 'Xi Xu'}, {'name': 'Siqi Ouyang'}, {'name': 'Brian Yan'}, {'name': 'Patrick Fernandes'}, {'name': 'William Chen'}, {'name': 'Lei Li'}, {'name': 'Graham Neubig'}, {'name': 'Shinji Watanabe'}]",2024-08-14T10:44:51Z
http://arxiv.org/abs/2408.07448v1,http://arxiv.org/abs/2408.07448v1,LiveFC: A System for Live Fact-Checking of Audio Streams,"The advances in the digital era have led to rapid dissemination of
information. This has also aggravated the spread of misinformation and
disinformation. This has potentially serious consequences, such as civil
unrest. While fact-checking aims to combat this, manual fact-checking is
cumbersome and not scalable. While automated fact-checking approaches exist,
they do not operate in real-time and do not always account for spread of
misinformation through different modalities. This is particularly important as
proactive fact-checking on live streams in real-time can help people be
informed of false narratives and prevent catastrophic consequences that may
cause civil unrest. This is particularly relevant with the rapid dissemination
of information through video on social media platforms or other streams like
political rallies and debates. Hence, in this work we develop a platform named
\name{}, that can aid in fact-checking live audio streams in real-time. \name{}
has a user-friendly interface that displays the claims detected along with
their veracity and evidence for live streams with associated speakers for
claims from respective segments. The app can be accessed at
http://livefc.factiverse.ai and a screen recording of the demo can be found at
https://bit.ly/3WVAoIw.","[{'name': 'Venktesh V'}, {'name': 'Vinay Setty'}]",2024-08-14T10:36:17Z
http://arxiv.org/abs/2408.07425v1,http://arxiv.org/abs/2408.07425v1,Exploring Retrieval Augmented Generation in Arabic,"Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful
technique in natural language processing, combining the strengths of
retrieval-based and generation-based models to enhance text generation tasks.
However, the application of RAG in Arabic, a language with unique
characteristics and resource constraints, remains underexplored. This paper
presents a comprehensive case study on the implementation and evaluation of RAG
for Arabic text. The work focuses on exploring various semantic embedding
models in the retrieval stage and several LLMs in the generation stage, in
order to investigate what works and what doesn't in the context of Arabic. The
work also touches upon the issue of variations between document dialect and
query dialect in the retrieval stage. Results show that existing semantic
embedding models and LLMs can be effectively employed to build Arabic RAG
pipelines.","[{'name': 'Samhaa R. El-Beltagy'}, {'name': 'Mohamed A. Abdallah'}]",2024-08-14T10:03:28Z
http://arxiv.org/abs/2408.07413v1,http://arxiv.org/abs/2408.07413v1,"Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge
  Editing for Large Language Models","Knowledge editing aims to update outdated or incorrect knowledge in large
language models (LLMs). However, current knowledge editing methods have limited
scalability for lifelong editing. This study explores the fundamental reason
why knowledge editing fails in lifelong editing. We begin with the closed-form
solution derived from linear associative memory, which underpins
state-of-the-art knowledge editing methods. We extend the solution from single
editing to lifelong editing, and through rigorous mathematical derivation,
identify an interference term in the final solution, suggesting that editing
knowledge may impact irrelevant knowledge. Further analysis of the interference
term reveals a close relationship with superposition between knowledge
representations. When knowledge superposition does not exist in language
models, the interference term vanishes, allowing for lossless knowledge
editing. Experiments across numerous language models reveal that knowledge
superposition is universal, exhibiting high kurtosis, zero mean, and
heavy-tailed distributions with clear scaling laws. Ultimately, by combining
theory and experiments, we demonstrate that knowledge superposition is the
fundamental reason for the failure of lifelong editing. Moreover, this is the
first study to investigate knowledge editing from the perspective of
superposition and provides a comprehensive observation of superposition across
numerous real-world language models. Code available at
https://github.com/ChenhuiHu/knowledge_in_superposition.","[{'name': 'Chenhui Hu'}, {'name': 'Pengfei Cao'}, {'name': 'Yubo Chen'}, {'name': 'Kang Liu'}, {'name': 'Jun Zhao'}]",2024-08-14T09:43:32Z
http://arxiv.org/abs/2408.07410v1,http://arxiv.org/abs/2408.07410v1,Aquila2 Technical Report,"This paper introduces the Aquila2 series, which comprises a wide range of
bilingual models with parameter sizes of 7, 34, and 70 billion. These models
are trained based on an innovative framework named HeuriMentor (HM), which
offers real-time insights into model convergence and enhances the training
process and data management. The HM System, comprising the Adaptive Training
Engine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),
allows for precise monitoring of the model's training progress and enables
efficient optimization of data distribution, thereby enhancing training
effectiveness. Extensive evaluations show that the Aquila2 model series
performs comparably well on both English and Chinese benchmarks. Specifically,
Aquila2-34B demonstrates only a slight decrease in performance when quantized
to Int4. Furthermore, we have made our training code
(https://github.com/FlagOpen/FlagScale) and model weights
(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing
research and the development of applications.","[{'name': 'Bo-Wen Zhang'}, {'name': 'Liangdong Wang'}, {'name': 'Jijie Li'}, {'name': 'Shuhao Gu'}, {'name': 'Xinya Wu'}, {'name': 'Zhengduo Zhang'}, {'name': 'Boyan Gao'}, {'name': 'Yulong Ao'}, {'name': 'Guang Liu'}]",2024-08-14T09:34:19Z
http://arxiv.org/abs/2408.07402v1,http://arxiv.org/abs/2408.07402v1,A Quantum-Inspired Analysis of Human Disambiguation Processes,"Formal languages are essential for computer programming and are constructed
to be easily processed by computers. In contrast, natural languages are much
more challenging and instigated the field of Natural Language Processing (NLP).
One major obstacle is the ubiquity of ambiguities. Recent advances in NLP have
led to the development of large language models, which can resolve ambiguities
with high accuracy. At the same time, quantum computers have gained much
attention in recent years as they can solve some computational problems faster
than classical computers. This new computing paradigm has reached the fields of
machine learning and NLP, where hybrid classical-quantum learning algorithms
have emerged. However, more research is needed to identify which NLP tasks
could benefit from a genuine quantum advantage. In this thesis, we applied
formalisms arising from foundational quantum mechanics, such as contextuality
and causality, to study ambiguities arising from linguistics. By doing so, we
also reproduced psycholinguistic results relating to the human disambiguation
process. These results were subsequently used to predict human behaviour and
outperformed current NLP methods.",[{'name': 'Daphne Wang'}],2024-08-14T09:21:23Z
http://arxiv.org/abs/2408.07401v1,http://arxiv.org/abs/2408.07401v1,"DataVisT5: A Pre-trained Language Model for Jointly Understanding Text
  and Data Visualization","Data visualization (DV) is the fundamental and premise tool to improve the
efficiency in conveying the insights behind the big data, which has been widely
accepted in existing data-driven world. Task automation in DV, such as
converting natural language queries to visualizations (i.e., text-to-vis),
generating explanations from visualizations (i.e., vis-to-text), answering
DV-related questions in free form (i.e. FeVisQA), and explicating tabular data
(i.e., table-to-text), is vital for advancing the field. Despite their
potential, the application of pre-trained language models (PLMs) like T5 and
BERT in DV has been limited by high costs and challenges in handling
cross-modal information, leading to few studies on PLMs for DV. We introduce
\textbf{DataVisT5}, a novel PLM tailored for DV that enhances the T5
architecture through a hybrid objective pre-training and multi-task fine-tuning
strategy, integrating text and DV datasets to effectively interpret cross-modal
semantics. Extensive evaluations on public datasets show that DataVisT5
consistently outperforms current state-of-the-art models on various DV-related
tasks. We anticipate that DataVisT5 will not only inspire further research on
vertical PLMs but also expand the range of applications for PLMs.","[{'name': 'Zhuoyue Wan'}, {'name': 'Yuanfeng Song'}, {'name': 'Shuaimin Li'}, {'name': 'Chen Jason Zhang'}, {'name': 'Raymond Chi-Wing Wong'}]",2024-08-14T09:20:17Z
http://arxiv.org/abs/2408.07377v2,http://arxiv.org/abs/2408.07377v2,"Do GPT Language Models Suffer From Split Personality Disorder? The
  Advent Of Substrate-Free Psychometrics","Previous research on emergence in large language models shows these display
apparent human-like abilities and psychological latent traits. However, results
are partly contradicting in expression and magnitude of these latent traits,
yet agree on the worrisome tendencies to score high on the Dark Triad of
narcissism, psychopathy, and Machiavellianism, which, together with a track
record of derailments, demands more rigorous research on safety of these
models. We provided a state of the art language model with the same personality
questionnaire in nine languages, and performed Bayesian analysis of Gaussian
Mixture Model, finding evidence for a deeper-rooted issue. Our results suggest
both interlingual and intralingual instabilities, which indicate that current
language models do not develop a consistent core personality. This can lead to
unsafe behaviour of artificial intelligence systems that are based on these
foundation models, and are increasingly integrated in human life. We
subsequently discuss the shortcomings of modern psychometrics, abstract it, and
provide a framework for its species-neutral, substrate-free formulation.","[{'name': 'Peter Romero'}, {'name': 'Stephen Fitz'}, {'name': 'Teruo Nakatsuma'}]",2024-08-14T08:53:00Z
http://arxiv.org/abs/2408.07353v1,http://arxiv.org/abs/2408.07353v1,"Only One Relation Possible? Modeling the Ambiguity in Event Temporal
  Relation Extraction","Event Temporal Relation Extraction (ETRE) aims to identify the temporal
relationship between two events, which plays an important role in natural
language understanding. Most previous works follow a single-label
classification style, classifying an event pair into either a specific temporal
relation (e.g., \textit{Before}, \textit{After}), or a special label
\textit{Vague} when there may be multiple possible temporal relations between
the pair. In our work, instead of directly making predictions on
\textit{Vague}, we propose a multi-label classification solution for ETRE
(METRE) to infer the possibility of each temporal relation independently, where
we treat \textit{Vague} as the cases when there is more than one possible
relation between two events. We design a speculation mechanism to explore the
possible relations hidden behind \textit{Vague}, which enables the latent
information to be used efficiently. Experiments on TB-Dense, MATRES and UDS-T
show that our method can effectively utilize the \textit{Vague} instances to
improve the recognition for specific temporal relations and outperforms most
state-of-the-art methods.","[{'name': 'Yutong Hu'}, {'name': 'Quzhe Huang'}, {'name': 'Yansong Feng'}]",2024-08-14T07:57:51Z
http://arxiv.org/abs/2408.07303v1,http://arxiv.org/abs/2408.07303v1,"Enhancing Visual Question Answering through Ranking-Based Hybrid
  Training and Multimodal Fusion","Visual Question Answering (VQA) is a challenging task that requires systems
to provide accurate answers to questions based on image content. Current VQA
models struggle with complex questions due to limitations in capturing and
integrating multimodal information effectively. To address these challenges, we
propose the Rank VQA model, which leverages a ranking-inspired hybrid training
strategy to enhance VQA performance. The Rank VQA model integrates high-quality
visual features extracted using the Faster R-CNN model and rich semantic text
features obtained from a pre-trained BERT model. These features are fused
through a sophisticated multimodal fusion technique employing multi-head
self-attention mechanisms. Additionally, a ranking learning module is
incorporated to optimize the relative ranking of answers, thus improving answer
accuracy. The hybrid training strategy combines classification and ranking
losses, enhancing the model's generalization ability and robustness across
diverse datasets. Experimental results demonstrate the effectiveness of the
Rank VQA model. Our model significantly outperforms existing state-of-the-art
models on standard VQA datasets, including VQA v2.0 and COCO-QA, in terms of
both accuracy and Mean Reciprocal Rank (MRR). The superior performance of Rank
VQA is evident in its ability to handle complex questions that require
understanding nuanced details and making sophisticated inferences from the
image and text. This work highlights the effectiveness of a ranking-based
hybrid training strategy in improving VQA performance and lays the groundwork
for further research in multimodal learning methods.","[{'name': 'Peiyuan Chen'}, {'name': 'Zecheng Zhang'}, {'name': 'Yiping Dong'}, {'name': 'Li Zhou'}, {'name': 'Han Wang'}]",2024-08-14T05:18:43Z
http://arxiv.org/abs/2408.07238v1,http://arxiv.org/abs/2408.07238v1,"Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge
  Distillation Approach","Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior
performance in complex human-like interactions. But they are costly, or too
large for edge devices such as smartphones and harder to self-host, leading to
security and privacy concerns. This paper introduces a novel interpretable
knowledge distillation approach to enhance the performance of smaller, more
economical LLMs that firms can self-host. We study this problem in the context
of building a customer service agent aimed at achieving high customer
satisfaction through goal-oriented dialogues. Unlike traditional knowledge
distillation, where the ""student"" model learns directly from the ""teacher""
model's responses via fine-tuning, our interpretable ""strategy"" teaching
approach involves the teacher providing strategies to improve the student's
performance in various scenarios. This method alternates between a ""scenario
generation"" step and a ""strategies for improvement"" step, creating a customized
library of scenarios and optimized strategies for automated prompting. The
method requires only black-box access to both student and teacher models; hence
it can be used without manipulating model parameters. In our customer service
application, the method improves performance, and the learned strategies are
transferable to other LLMs and scenarios beyond the training set. The method's
interpretabilty helps safeguard against potential harms through human audit.","[{'name': 'Tong Wang'}, {'name': 'K. Sudhir'}, {'name': 'Dat Hong'}]",2024-08-13T23:59:36Z
http://arxiv.org/abs/2408.07237v1,http://arxiv.org/abs/2408.07237v1,"Neural embedding of beliefs reveals the role of relative dissonance in
  human decision-making","Beliefs serve as the foundation for human cognition and decision-making. They
guide individuals in deriving meaning from their lives, shaping their
behaviors, and forming social connections. Therefore, a model that encapsulates
beliefs and their interrelationships is crucial for quantitatively studying the
influence of beliefs on our actions. Despite its importance, research on the
interplay between human beliefs has often been limited to a small set of
beliefs pertaining to specific issues, with a heavy reliance on surveys or
experiments. Here, we propose a method for extracting nuanced relations between
thousands of beliefs by leveraging large-scale user participation data from an
online debate platform and mapping these beliefs to an embedding space using a
fine-tuned large language model (LLM). This belief embedding space effectively
encapsulates the interconnectedness of diverse beliefs as well as polarization
across various social issues. We discover that the positions within this belief
space predict new beliefs of individuals. Furthermore, we find that the
relative distance between one's existing beliefs and new beliefs can serve as a
quantitative estimate of cognitive dissonance, allowing us to predict new
beliefs. Our study highlights how modern LLMs, when combined with collective
online records of human beliefs, can offer insights into the fundamental
principles that govern human belief formation and decision-making processes.","[{'name': 'Byunghwee Lee'}, {'name': 'Rachith Aiyappa'}, {'name': 'Yong-Yeol Ahn'}, {'name': 'Haewoon Kwak'}, {'name': 'Jisun An'}]",2024-08-13T23:58:45Z
http://arxiv.org/abs/2408.07190v1,http://arxiv.org/abs/2408.07190v1,BERT's Conceptual Cartography: Mapping the Landscapes of Meaning,"Conceptual Engineers want to make words better. However, they often
underestimate how varied our usage of words is. In this paper, we take the
first steps in exploring the contextual nuances of words by creating conceptual
landscapes -- 2D surfaces representing the pragmatic usage of words -- that
conceptual engineers can use to inform their projects. We use the spoken
component of the British National Corpus and BERT to create contextualised word
embeddings, and use Gaussian Mixture Models, a selection of metrics, and
qualitative analysis to visualise and numerically represent lexical landscapes.
Such an approach has not yet been used in the conceptual engineering literature
and provides a detailed examination of how different words manifest in various
contexts that is potentially useful to conceptual engineering projects. Our
findings highlight the inherent complexity of conceptual engineering, revealing
that each word exhibits a unique and intricate landscape. Conceptual Engineers
cannot, therefore, use a one-size-fits-all approach when improving words -- a
task that may be practically intractable at scale.","[{'name': 'Nina Haket'}, {'name': 'Ryan Daniels'}]",2024-08-13T20:08:26Z
http://arxiv.org/abs/2408.07180v1,http://arxiv.org/abs/2408.07180v1,Unlocking Efficiency: Adaptive Masking for Gene Transformer Models,"Gene transformer models such as Nucleotide Transformer, DNABert, and LOGO are
trained to learn optimal gene sequence representations by using the Masked
Language Modeling (MLM) training objective over the complete Human Reference
Genome. However, the typical tokenization methods employ a basic sliding window
of tokens, such as k-mers, that fail to utilize gene-centric semantics. This
could result in the (trivial) masking of easily predictable sequences, leading
to inefficient MLM training. Time-variant training strategies are known to
improve pretraining efficiency in both language and vision tasks. In this work,
we focus on using curriculum masking where we systematically increase the
difficulty of masked token prediction task by using a Pointwise Mutual
Information-based difficulty criterion, as gene sequences lack well-defined
semantic units similar to words or sentences of NLP domain. Our proposed
Curriculum Masking-based Gene Masking Strategy (CM-GEMS) demonstrates superior
representation learning capabilities compared to baseline masking approaches
when evaluated on downstream gene sequence classification tasks. We perform
extensive evaluation in both few-shot (five datasets) and full dataset settings
(Genomic Understanding Evaluation benchmark consisting of 27 tasks). Our
findings reveal that CM-GEMS outperforms state-of-the-art models (DNABert-2,
Nucleotide transformer, DNABert) trained at 120K steps, achieving similar
results in just 10K and 1K steps. We also demonstrate that Curriculum-Learned
LOGO (a 2-layer DNABert-like model) can achieve nearly 90% of the
state-of-the-art model performance of 120K steps. We will make the models and
codes publicly available at https://github.com/roysoumya/curriculum-GeneMask.","[{'name': 'Soumyadeep Roy'}, {'name': 'Shamik Sural'}, {'name': 'Niloy Ganguly'}]",2024-08-13T19:45:02Z
http://arxiv.org/abs/2408.07154v1,http://arxiv.org/abs/2408.07154v1,Self-folding Self-replication,"Inspired by protein folding, we explored the construction of
three-dimensional structures and machines from one-dimensional chains of simple
building blocks. This approach not only allows us to recreate the
self-replication mechanism introduced earlier, but also significantly
simplifies the process. We introduced a new set of folding blocks that
facilitate the formation of secondary structures such as {\alpha}-helices and
\b{eta}-sheets, as well as more advanced tertiary and quaternary structures,
including self-replicating machines. The introduction of rotational degrees of
freedom leads to a reduced variety of blocks and, most importantly, reduces the
overall size of the machines by a factor of five. In addition, we present a
universal copier-constructor, a highly efficient self-replicating mechanism
composed of approximately 40 blocks, including the restictions posed on it. The
paper also addresses evolutionary considerations, outlining several steps on
the evolutionary ladder towards more sophisticated self-replicating systems.
Finally, this study offers a clear rationale for nature's preference for
one-dimensional chains in constructing three-dimensional structures.",[{'name': 'Ralph P. Lano'}],2024-08-13T18:50:07Z
http://arxiv.org/abs/2408.07144v1,http://arxiv.org/abs/2408.07144v1,Language Models as Models of Language,"This chapter critically examines the potential contributions of modern
language models to theoretical linguistics. Despite their focus on engineering
goals, these models' ability to acquire sophisticated linguistic knowledge from
mere exposure to data warrants a careful reassessment of their relevance to
linguistic theory. I review a growing body of empirical evidence suggesting
that language models can learn hierarchical syntactic structure and exhibit
sensitivity to various linguistic phenomena, even when trained on
developmentally plausible amounts of data. While the competence/performance
distinction has been invoked to dismiss the relevance of such models to
linguistic theory, I argue that this assessment may be premature. By carefully
controlling learning conditions and making use of causal intervention methods,
experiments with language models can potentially constrain hypotheses about
language acquisition and competence. I conclude that closer collaboration
between theoretical linguists and computational researchers could yield
valuable insights, particularly in advancing debates about linguistic nativism.",[{'name': 'Raphaël Millière'}],2024-08-13T18:26:04Z
http://arxiv.org/abs/2408.07137v1,http://arxiv.org/abs/2408.07137v1,"ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal
  Advice","Despite remarkable performance in legal consultation exhibited by legal Large
Language Models(LLMs) combined with legal article retrieval components, there
are still cases when the advice given is incorrect or baseless. To alleviate
these problems, we propose {\bf ELLA}, a tool for {\bf E}mpowering {\bf L}LMs
for interpretable, accurate, and informative {\bf L}egal {\bf A}dvice. ELLA
visually presents the correlation between legal articles and LLM's response by
calculating their similarities, providing users with an intuitive legal basis
for the responses. Besides, based on the users' queries, ELLA retrieves
relevant legal articles and displays them to users. Users can interactively
select legal articles for LLM to generate more accurate responses. ELLA also
retrieves relevant legal cases for user reference. Our user study shows that
presenting the legal basis for the response helps users understand better. The
accuracy of LLM's responses also improves when users intervene in selecting
legal articles for LLM. Providing relevant legal cases also aids individuals in
obtaining comprehensive information.","[{'name': 'Yutong Hu'}, {'name': 'Kangcheng Luo'}, {'name': 'Yansong Feng'}]",2024-08-13T18:12:00Z
http://arxiv.org/abs/2408.07065v1,http://arxiv.org/abs/2408.07065v1,Fingerspelling within Sign Language Translation,"Fingerspelling poses challenges for sign language processing due to its
high-frequency motion and use for open-vocabulary terms. While prior work has
studied fingerspelling recognition, there has been little attention to
evaluating how well sign language translation models understand fingerspelling
in the context of entire sentences -- and improving this capability. We
manually annotate instances of fingerspelling within FLEURS-ASL and use them to
evaluate the effect of two simple measures to improve fingerspelling
recognition within American Sign Language to English translation: 1) use a
model family (ByT5) with character- rather than subword-level tokenization, and
2) mix fingerspelling recognition data into the translation training mixture.
We find that 1) substantially improves understanding of fingerspelling (and
therefore translation quality overall), but the effect of 2) is mixed.",[{'name': 'Garrett Tanzer'}],2024-08-13T17:57:14Z
http://arxiv.org/abs/2408.07060v1,http://arxiv.org/abs/2408.07060v1,"Diversity Empowers Intelligence: Integrating Expertise of Software
  Engineering Agents","Large language model (LLM) agents have shown great potential in solving
real-world software engineering (SWE) problems. The most advanced open-source
SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.
However, these sophisticated agent frameworks exhibit varying strengths,
excelling in certain tasks while underperforming in others. To fully harness
the diversity of these agents, we propose DEI (Diversity Empowered
Intelligence), a framework that leverages their unique expertise. DEI functions
as a meta-module atop existing SWE agent frameworks, managing agent collectives
for enhanced problem-solving. Experimental results show that a DEI-guided
committee of agents is able to surpass the best individual agent's performance
by a large margin. For instance, a group of open-source SWE agents, with a
maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%
resolve rate with DEI, making a 25% improvement and beating most closed-source
solutions. Our best-performing group excels with a 55% resolve rate, securing
the highest ranking on SWE-Bench Lite. Our findings contribute to the growing
body of research on collaborative AI systems and their potential to solve
complex software engineering challenges.","[{'name': 'Kexun Zhang'}, {'name': 'Weiran Yao'}, {'name': 'Zuxin Liu'}, {'name': 'Yihao Feng'}, {'name': 'Zhiwei Liu'}, {'name': 'Rithesh Murthy'}, {'name': 'Tian Lan'}, {'name': 'Lei Li'}, {'name': 'Renze Lou'}, {'name': 'Jiacheng Xu'}, {'name': 'Bo Pang'}, {'name': 'Yingbo Zhou'}, {'name': 'Shelby Heinecke'}, {'name': 'Silvio Savarese'}, {'name': 'Huan Wang'}, {'name': 'Caiming Xiong'}]",2024-08-13T17:50:28Z
http://arxiv.org/abs/2408.07057v1,http://arxiv.org/abs/2408.07057v1,"A Survey on Model MoErging: Recycling and Routing Among Specialized
  Experts for Collaborative Learning","The availability of performant pre-trained models has led to a proliferation
of fine-tuned expert models that are specialized to a particular domain or
task. Model MoErging methods aim to recycle expert models to create an
aggregate system with improved performance or generalization. A key component
of MoErging methods is the creation of a router that decides which expert
model(s) to use for a particular input or application. The promise,
effectiveness, and large design space of MoErging has spurred the development
of many new methods over the past few years. This rapid pace of development has
made it challenging to compare different MoErging methods, which are rarely
compared to one another and are often validated in different experimental
setups. To remedy such gaps, we present a comprehensive survey of MoErging
methods that includes a novel taxonomy for cataloging key design choices and
clarifying suitable applications for each method. Apart from surveying MoErging
research, we inventory software tools and applications that make use of
MoErging. We additionally discuss related fields of study such as model
merging, multitask learning, and mixture-of-experts models. Taken as a whole,
our survey provides a unified overview of existing MoErging methods and creates
a solid foundation for future work in this burgeoning field.","[{'name': 'Prateek Yadav'}, {'name': 'Colin Raffel'}, {'name': 'Mohammed Muqeeth'}, {'name': 'Lucas Caccia'}, {'name': 'Haokun Liu'}, {'name': 'Tianlong Chen'}, {'name': 'Mohit Bansal'}, {'name': 'Leshem Choshen'}, {'name': 'Alessandro Sordoni'}]",2024-08-13T17:49:00Z
http://arxiv.org/abs/2408.07055v1,http://arxiv.org/abs/2408.07055v1,"LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs","Current long context large language models (LLMs) can process inputs up to
100,000 tokens, yet struggle to generate outputs exceeding even a modest length
of 2,000 words. Through controlled experiments, we find that the model's
effective generation length is inherently bounded by the sample it has seen
during supervised fine-tuning (SFT). In other words, their output limitation is
due to the scarcity of long-output examples in existing SFT datasets. To
address this, we introduce AgentWrite, an agent-based pipeline that decomposes
ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to
generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we
construct LongWriter-6k, a dataset containing 6,000 SFT data with output
lengths ranging from 2k to 32k words. By incorporating this dataset into model
training, we successfully scale the output length of existing models to over
10,000 words while maintaining output quality. We also develop LongBench-Write,
a comprehensive benchmark for evaluating ultra-long generation capabilities.
Our 9B parameter model, further improved through DPO, achieves state-of-the-art
performance on this benchmark, surpassing even much larger proprietary models.
In general, our work demonstrates that existing long context LLM already
possesses the potential for a larger output window--all you need is data with
extended output during model alignment to unlock this capability. Our code &
models are at: https://github.com/THUDM/LongWriter.","[{'name': 'Yushi Bai'}, {'name': 'Jiajie Zhang'}, {'name': 'Xin Lv'}, {'name': 'Linzhi Zheng'}, {'name': 'Siqi Zhu'}, {'name': 'Lei Hou'}, {'name': 'Yuxiao Dong'}, {'name': 'Jie Tang'}, {'name': 'Juanzi Li'}]",2024-08-13T17:46:12Z
http://arxiv.org/abs/2408.07052v1,http://arxiv.org/abs/2408.07052v1,The News Comment Gap and Algorithmic Agenda Setting in Online Forums,"The disparity between news stories valued by journalists and those preferred
by readers, known as the ""News Gap"", is well-documented. However, the
difference in expectations regarding news related user-generated content is
less studied. Comment sections, hosted by news websites, are popular venues for
reader engagement, yet still subject to editorial decisions. It is thus
important to understand journalist vs reader comment preferences and how these
are served by various comment ranking algorithms that represent discussions
differently. We analyse 1.2 million comments from Austrian newspaper Der
Standard to understand the ""News Comment Gap"" and the effects of different
ranking algorithms. We find that journalists prefer positive, timely, complex,
direct responses, while readers favour comments similar to article content from
elite authors. We introduce the versatile Feature-Oriented Ranking Utility
Metric (FORUM) to assess the impact of different ranking algorithms and find
dramatic differences in how they prioritise the display of comments by
sentiment, topical relevance, lexical diversity, and readability. Journalists
can exert substantial influence over the discourse through both curatorial and
algorithmic means. Understanding these choices' implications is vital in
fostering engaging and civil discussions while aligning with journalistic
objectives, especially given the increasing legal scrutiny and societal
importance of online discourse.","[{'name': 'Flora Böwing'}, {'name': 'Patrick Gildersleve'}]",2024-08-13T17:43:32Z
http://arxiv.org/abs/2408.07045v1,http://arxiv.org/abs/2408.07045v1,TableGuard -- Securing Structured & Unstructured Data,"With the increasing demand for data sharing across platforms and
organizations, ensuring the privacy and security of sensitive information has
become a critical challenge. This paper introduces ""TableGuard"". An innovative
approach to data obfuscation tailored for relational databases. Building on the
principles and techniques developed in prior work on context-sensitive
obfuscation, TableGuard applies these methods to ensure that API calls return
only obfuscated data, thereby safeguarding privacy when sharing data with third
parties. TableGuard leverages advanced context-sensitive obfuscation techniques
to replace sensitive data elements with contextually appropriate alternatives.
By maintaining the relational integrity and coherence of the data, our approach
mitigates the risks of cognitive dissonance and data leakage. We demonstrate
the implementation of TableGuard using a BERT based transformer model, which
identifies and obfuscates sensitive entities within relational tables. Our
evaluation shows that TableGuard effectively balances privacy protection with
data utility, minimizing information loss while ensuring that the obfuscated
data remains functionally useful for downstream applications. The results
highlight the importance of domain-specific obfuscation strategies and the role
of context length in preserving data integrity. The implications of this
research are significant for organizations that need to share data securely
with external parties. TableGuard offers a robust framework for implementing
privacy-preserving data sharing mechanisms, thereby contributing to the broader
field of data privacy and security.","[{'name': 'Anantha Sharma'}, {'name': 'Ajinkya Deshmukh'}]",2024-08-13T17:20:52Z
http://arxiv.org/abs/2408.07003v1,http://arxiv.org/abs/2408.07003v1,Generative AI for automatic topic labelling,"Topic Modeling has become a prominent tool for the study of scientific
fields, as they allow for a large scale interpretation of research trends.
Nevertheless, the output of these models is structured as a list of keywords
which requires a manual interpretation for the labelling. This paper proposes
to assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini
for topic labelling. Drawing on previous research leveraging BERTopic, we
generate topics from a dataset of all the scientific articles (n=34,797)
authored by all biology professors in Switzerland (n=465) between 2008 and
2020, as recorded in the Web of Science database. We assess the output of the
three models both quantitatively and qualitatively and find that, first, both
GPT models are capable of accurately and precisely label topics from the
models' output keywords. Second, 3-word labels are preferable to grasp the
complexity of research topics.","[{'name': 'Diego Kozlowski'}, {'name': 'Carolina Pradier'}, {'name': 'Pierre Benz'}]",2024-08-13T16:07:16Z
http://arxiv.org/abs/2408.06931v1,http://arxiv.org/abs/2408.06931v1,"The advantages of context specific language models: the case of the
  Erasmian Language Model","The current trend to improve language model performance seems to be based on
scaling up with the number of parameters (e.g. the state of the art GPT4 model
has approximately 1.7 trillion parameters) or the amount of training data fed
into the model. However this comes at significant costs in terms of
computational resources and energy costs that compromise the sustainability of
AI solutions, as well as risk relating to privacy and misuse. In this paper we
present the Erasmian Language Model (ELM) a small context specific, 900 million
parameter model, pre-trained and fine-tuned by and for Erasmus University
Rotterdam. We show how the model performs adequately in a classroom context for
essay writing, and how it achieves superior performance in subjects that are
part of its context. This has implications for a wide range of institutions and
organizations, showing that context specific language models may be a viable
alternative for resource constrained, privacy sensitive use cases.","[{'name': 'João Gonçalves'}, {'name': 'Nick Jelicic'}, {'name': 'Michele Murgia'}, {'name': 'Evert Stamhuis'}]",2024-08-13T14:34:59Z
http://arxiv.org/abs/2408.06930v2,http://arxiv.org/abs/2408.06930v2,"Diagnosis extraction from unstructured Dutch echocardiogram reports
  using span- and document-level characteristic classification","Clinical machine learning research and AI driven clinical decision support
models rely on clinically accurate labels. Manually extracting these labels
with the help of clinical specialists is often time-consuming and expensive.
This study tests the feasibility of automatic span- and document-level
diagnosis extraction from unstructured Dutch echocardiogram reports. We
included 115,692 unstructured echocardiogram reports from the UMCU a large
university hospital in the Netherlands. A randomly selected subset was manually
annotated for the occurrence and severity of eleven commonly described cardiac
characteristics. We developed and tested several automatic labelling techniques
at both span and document levels, using weighted and macro F1-score, precision,
and recall for performance evaluation. We compared the performance of span
labelling against document labelling methods, which included both direct
document classifiers and indirect document classifiers that rely on span
classification results. The SpanCategorizer and MedRoBERTa$.$nl models
outperformed all other span and document classifiers, respectively. The
weighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in
SpanCategorizer and 0.96 to 0.98 in MedRoBERTa$.$nl. Direct document
classification was superior to indirect document classification using span
classifiers. SetFit achieved competitive document classification performance
using only 10% of the training data. Utilizing a reduced label set yielded
near-perfect document classification results. We recommend using our published
SpanCategorizer and MedRoBERTa$.$nl models for span- and document-level
diagnosis extraction from Dutch echocardiography reports. For settings with
limited training data, SetFit may be a promising alternative for document
classification.","[{'name': 'Bauke Arends'}, {'name': 'Melle Vessies'}, {'name': 'Dirk van Osch'}, {'name': 'Arco Teske'}, {'name': 'Pim van der Harst'}, {'name': 'René van Es'}, {'name': 'Bram van Es'}]",2024-08-13T14:33:32Z
http://arxiv.org/abs/2408.06929v1,http://arxiv.org/abs/2408.06929v1,"Evaluating Cultural Adaptability of a Large Language Model via
  Simulation of Synthetic Personas","The success of Large Language Models (LLMs) in multicultural environments
hinges on their ability to understand users' diverse cultural backgrounds. We
measure this capability by having an LLM simulate human profiles representing
various nationalities within the scope of a questionnaire-style psychological
experiment. Specifically, we employ GPT-3.5 to reproduce reactions to
persuasive news articles of 7,286 participants from 15 countries; comparing the
results with a dataset of real participants sharing the same demographic
traits. Our analysis shows that specifying a person's country of residence
improves GPT-3.5's alignment with their responses. In contrast, using native
language prompting introduces shifts that significantly reduce overall
alignment, with some languages particularly impairing performance. These
findings suggest that while direct nationality information enhances the model's
cultural adaptability, native language cues do not reliably improve simulation
fidelity and can detract from the model's effectiveness.","[{'name': 'Louis Kwok'}, {'name': 'Michal Bravansky'}, {'name': 'Lewis D. Griffin'}]",2024-08-13T14:32:43Z
http://arxiv.org/abs/2408.06904v1,http://arxiv.org/abs/2408.06904v1,"Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge
  Perspectives","As large language models (LLMs) continue to scale, their enhanced performance
often proves insufficient for solving domain-specific tasks. Systematically
analyzing their failures and effectively enhancing their performance remain
significant challenges. This paper introduces the Re-TASK framework, a novel
theoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge
perspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space
Theory. The Re-TASK framework provides a systematic methodology to deepen our
understanding, evaluation, and enhancement of LLMs for domain-specific tasks.
It explores the interplay among an LLM's capabilities, the knowledge it
processes, and the skills it applies, elucidating how these elements are
interconnected and impact task performance. Our application of the Re-TASK
framework reveals that many failures in domain-specific tasks can be attributed
to insufficient knowledge or inadequate skill adaptation. With this insight, we
propose structured strategies for enhancing LLMs through targeted knowledge
injection and skill adaptation. Specifically, we identify key capability items
associated with tasks and employ a deliberately designed prompting strategy to
enhance task performance, thereby reducing the need for extensive fine-tuning.
Alternatively, we fine-tune the LLM using capability-specific instructions,
further validating the efficacy of our framework. Experimental results confirm
the framework's effectiveness, demonstrating substantial improvements in both
the performance and applicability of LLMs.","[{'name': 'Zhihu Wang'}, {'name': 'Shiwan Zhao'}, {'name': 'Yu Wang'}, {'name': 'Heyuan Huang'}, {'name': 'Jiaxin Shi'}, {'name': 'Sitao Xie'}, {'name': 'Zhixing Wang'}, {'name': 'Yubo Zhang'}, {'name': 'Hongyan Li'}, {'name': 'Junchi Yan'}]",2024-08-13T13:58:23Z
http://arxiv.org/abs/2408.06874v1,http://arxiv.org/abs/2408.06874v1,"Leveraging Language Models for Emotion and Behavior Analysis in
  Education","The analysis of students' emotions and behaviors is crucial for enhancing
learning outcomes and personalizing educational experiences. Traditional
methods often rely on intrusive visual and physiological data collection,
posing privacy concerns and scalability issues. This paper proposes a novel
method leveraging large language models (LLMs) and prompt engineering to
analyze textual data from students. Our approach utilizes tailored prompts to
guide LLMs in detecting emotional and engagement states, providing a
non-intrusive and scalable solution. We conducted experiments using Qwen,
ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and
chain-of-thought (CoT) prompting. Results demonstrate that our method
significantly outperforms the baselines in both accuracy and contextual
understanding. This study highlights the potential of LLMs combined with prompt
engineering to offer practical and effective tools for educational emotion and
behavior analysis.","[{'name': 'Kaito Tanaka'}, {'name': 'Benjamin Tan'}, {'name': 'Brian Wong'}]",2024-08-13T13:11:53Z
http://arxiv.org/abs/2408.06854v1,http://arxiv.org/abs/2408.06854v1,"LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large
  Language Models","Fine-tuning large language models (LLMs) with high parameter efficiency for
downstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)
significantly reduces the number of trainable parameters for fine-tuning.
Although it has demonstrated commendable performance, updating parameters
within a single scale may not be the optimal choice for complex downstream
tasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.
We first combine orthogonal projection theory to train a set of LoRAs in two
mutually orthogonal planes. Then, we improve the importance score algorithm,
which reduce parameter sensitivity score calculations by approximately 98.5\%.
By pruning singular values with lower importance scores, thereby enhancing
adaptability to various downstream tasks. Extensive experiments are conducted
on two widely used pre-trained models to validate the effectiveness of
LoRA$^2$. Results show that it significantly reduces the number of trainable
parameters to just 0.72\% compared to full fine-tuning, while still delivering
highly impressive performance. Even when the parameters are further reduced to
0.17M, it still achieves comparable results to the baseline with 8 times more
parameters. Our code is available here:
https://anonymous.4open.science/r/LoRA-2-5B4C","[{'name': 'Jia-Chen Zhang'}, {'name': 'Yu-Jie Xiong'}, {'name': 'He-Xi Qiu'}, {'name': 'Dong-Hai Zhu'}, {'name': 'Chun-Ming Xia'}]",2024-08-13T12:31:30Z
http://arxiv.org/abs/2408.06849v1,http://arxiv.org/abs/2408.06849v1,Causal Agent based on Large Language Model,"Large language models (LLMs) have achieved significant success across various
domains. However, the inherent complexity of causal problems and causal theory
poses challenges in accurately describing them in natural language, making it
difficult for LLMs to comprehend and use them effectively. Causal methods are
not easily conveyed through natural language, which hinders LLMs' ability to
apply them accurately. Additionally, causal datasets are typically tabular,
while LLMs excel in handling natural language data, creating a structural
mismatch that impedes effective reasoning with tabular data. This lack of
causal reasoning capability limits the development of LLMs. To address these
challenges, we have equipped the LLM with causal tools within an agent
framework, named the Causal Agent, enabling it to tackle causal problems. The
causal agent comprises tools, memory, and reasoning modules. In the tools
module, the causal agent applies causal methods to align tabular data with
natural language. In the reasoning module, the causal agent employs the ReAct
framework to perform reasoning through multiple iterations with the tools. In
the memory module, the causal agent maintains a dictionary instance where the
keys are unique names and the values are causal graphs. To verify the causal
ability of the causal agent, we established a benchmark consisting of four
levels of causal problems: variable level, edge level, causal graph level, and
causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for
these four levels of issues and tested the causal agent on the datasets. Our
methodology demonstrates remarkable efficacy on the four-level causal problems,
with accuracy rates all above 80%. For further insights and implementation
details, our code is accessible via the GitHub repository
https://github.com/Kairong-Han/Causal_Agent.","[{'name': 'Kairong Han'}, {'name': 'Kun Kuang'}, {'name': 'Ziyu Zhao'}, {'name': 'Junjian Ye'}, {'name': 'Fei Wu'}]",2024-08-13T12:22:26Z
http://arxiv.org/abs/2408.06816v1,http://arxiv.org/abs/2408.06816v1,"MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data
  Uncertainty","Although large language models (LLMs) are capable of performing various
tasks, they still suffer from producing plausible but incorrect responses. To
improve the reliability of LLMs, recent research has focused on uncertainty
quantification to predict whether a response is correct or not. However, most
uncertainty quantification methods have been evaluated on questions requiring a
single clear answer, ignoring the existence of data uncertainty that arises
from irreducible randomness. Instead, these methods only consider model
uncertainty, which arises from a lack of knowledge. In this paper, we
investigate previous uncertainty quantification methods under the presence of
data uncertainty. Our contributions are two-fold: 1) proposing a new
Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge,
mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty
quantification regarding data uncertainty, and 2) assessing 5 uncertainty
quantification methods of diverse white- and black-box LLMs. Our findings show
that entropy and consistency-based methods estimate the model uncertainty well
even under data uncertainty, while other methods for white- and black-box LLMs
struggle depending on the tasks. Additionally, methods designed for white-box
LLMs suffer from overconfidence in reasoning tasks compared to simple knowledge
queries. We believe our observations will pave the way for future work on
uncertainty quantification in realistic setting.","[{'name': 'Yongjin Yang'}, {'name': 'Haneul Yoo'}, {'name': 'Hwaran Lee'}]",2024-08-13T11:17:31Z
http://arxiv.org/abs/2408.08907v1,http://arxiv.org/abs/2408.08907v1,"What should I wear to a party in a Greek taverna? Evaluation for
  Conversational Agents in the Fashion Domain","Large language models (LLMs) are poised to revolutionize the domain of online
fashion retail, enhancing customer experience and discovery of fashion online.
LLM-powered conversational agents introduce a new way of discovery by directly
interacting with customers, enabling them to express in their own ways, refine
their needs, obtain fashion and shopping advice that is relevant to their taste
and intent. For many tasks in e-commerce, such as finding a specific product,
conversational agents need to convert their interactions with a customer to a
specific call to different backend systems, e.g., a search system to showcase a
relevant set of products. Therefore, evaluating the capabilities of LLMs to
perform those tasks related to calling other services is vital. However, those
evaluations are generally complex, due to the lack of relevant and high quality
datasets, and do not align seamlessly with business needs, amongst others. To
this end, we created a multilingual evaluation dataset of 4k conversations
between customers and a fashion assistant in a large e-commerce fashion
platform to measure the capabilities of LLMs to serve as an assistant between
customers and a backend engine. We evaluate a range of models, showcasing how
our dataset scales to business needs and facilitates iterative development of
tools.","[{'name': 'Antonis Maronikolakis'}, {'name': 'Ana Peleteiro Ramallo'}, {'name': 'Weiwei Cheng'}, {'name': 'Thomas Kober'}]",2024-08-13T11:11:27Z
http://arxiv.org/abs/2408.06793v1,http://arxiv.org/abs/2408.06793v1,Layerwise Recurrent Router for Mixture-of-Experts,"The scaling of large language models (LLMs) has revolutionized their
capabilities in various tasks, yet this growth must be matched with efficient
computational strategies. The Mixture-of-Experts (MoE) architecture stands out
for its ability to scale model size without significantly increasing training
costs. Despite their advantages, current MoE models often display parameter
inefficiency. For instance, a pre-trained MoE-based LLM with 52 billion
parameters might perform comparably to a standard model with 6.7 billion
parameters. Being a crucial part of MoE, current routers in different layers
independently assign tokens without leveraging historical routing information,
potentially leading to suboptimal token-expert combinations and the parameter
inefficiency problem. To alleviate this issue, we introduce the Layerwise
Recurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated
Recurrent Unit (GRU) to establish dependencies between routing decisions across
consecutive layers. Such layerwise recurrence can be efficiently parallelly
computed for input tokens and introduces negotiable costs. Our extensive
empirical evaluations demonstrate that RMoE-based language models consistently
outperform a spectrum of baseline models. Furthermore, RMoE integrates a novel
computation stage orthogonal to existing methods, allowing seamless
compatibility with other MoE architectures. Our analyses attribute RMoE's gains
to its effective cross-layer information sharing, which also improves expert
selection and diversity. Our code is at https://github.com/qiuzh20/RMoE","[{'name': 'Zihan Qiu'}, {'name': 'Zeyu Huang'}, {'name': 'Shuang Cheng'}, {'name': 'Yizhi Zhou'}, {'name': 'Zili Wang'}, {'name': 'Ivan Titov'}, {'name': 'Jie Fu'}]",2024-08-13T10:25:13Z
http://arxiv.org/abs/2408.06787v1,http://arxiv.org/abs/2408.06787v1,Unlock the Power of Frozen LLMs in Knowledge Graph Completion,"Classical knowledge graph completion (KGC) methods rely solely on structural
information, struggling with the inherent sparsity of knowledge graphs (KGs).
Large Language Models (LLMs) learn extensive knowledge from large corpora with
powerful context modeling, which is ideal for mitigating the limitations of
previous methods. Directly fine-tuning LLMs offers great capability but comes
at the cost of huge time and memory consumption, while utilizing frozen LLMs
yields suboptimal results. In this work, we aim to leverage LLMs for KGC
effectively and efficiently. We capture the context-aware hidden states of
knowledge triples by employing prompts to stimulate the intermediate layers of
LLMs. We then train a data-efficient classifier on these hidden states to
harness the inherent capabilities of frozen LLMs in KGC. We also generate
entity descriptions with subgraph sampling on KGs, reducing the ambiguity of
triplets and enriching the knowledge representation. Extensive experiments on
standard benchmarks showcase the efficiency and effectiveness of our approach.
We outperform classical KGC methods on most datasets and match the performance
of fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU
memory efficiency by \textbf{$188\times$} and speed up training+inference by
\textbf{$13.48\times$}.","[{'name': 'Bo Xue'}, {'name': 'Yi Xu'}, {'name': 'Yunchong Song'}, {'name': 'Yiming Pang'}, {'name': 'Yuyang Ren'}, {'name': 'Jiaxin Ding'}, {'name': 'Luoyi Fu'}, {'name': 'Xinbing Wang'}]",2024-08-13T10:15:55Z
http://arxiv.org/abs/2408.06778v1,http://arxiv.org/abs/2408.06778v1,Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors,"Link prediction models can benefit from incorporating textual descriptions of
entities and relations, enabling fully inductive learning and flexibility in
dynamic graphs. We address the challenge of also capturing rich structured
information about the local neighbourhood of entities and their relations, by
introducing a Transformer-based approach that effectively integrates textual
descriptions with graph structure, reducing the reliance on resource-intensive
text encoders. Our experiments on three challenging datasets show that our
Fast-and-Frugal Text-Graph (FnF-TG) Transformers achieve superior performance
compared to the previous state-of-the-art methods, while maintaining efficiency
and scalability.","[{'name': 'Andrei C. Coman'}, {'name': 'Christos Theodoropoulos'}, {'name': 'Marie-Francine Moens'}, {'name': 'James Henderson'}]",2024-08-13T10:04:29Z
http://arxiv.org/abs/2408.06755v1,http://arxiv.org/abs/2408.06755v1,"Sumotosima: A Framework and Dataset for Classifying and Summarizing
  Otoscopic Images","Otoscopy is a diagnostic procedure to examine the ear canal and eardrum using
an otoscope. It identifies conditions like infections, foreign bodies, ear drum
perforations and ear abnormalities. We propose a novel resource efficient deep
learning and transformer based framework, Sumotosima (Summarizer for otoscopic
images), an end-to-end pipeline for classification followed by summarization.
Our framework works on combination of triplet and cross-entropy losses.
Additionally, we use Knowledge Enhanced Multimodal BART whose input is fused
textual and image embedding. The objective is to provide summaries that are
well-suited for patients, ensuring clarity and efficiency in understanding
otoscopic images. Given the lack of existing datasets, we have curated our own
OCASD (Otoscopic Classification And Summary Dataset), which includes 500 images
with 5 unique categories annotated with their class and summaries by
Otolaryngologists. Sumotosima achieved a result of 98.03%, which is 7.00%,
3.10%, 3.01% higher than K-Nearest Neighbors, Random Forest and Support Vector
Machines, respectively, in classification tasks. For summarization, Sumotosima
outperformed GPT-4o and LLaVA by 88.53% and 107.57% in ROUGE scores,
respectively. We have made our code and dataset publicly available at
https://github.com/anas2908/Sumotosima","[{'name': 'Eram Anwarul Khan'}, {'name': 'Anas Anwarul Haq Khan'}]",2024-08-13T09:26:41Z
http://arxiv.org/abs/2408.06737v1,http://arxiv.org/abs/2408.06737v1,Multilingual Models for Check-Worthy Social Media Posts Detection,"This work presents an extensive study of transformer-based NLP models for
detection of social media posts that contain verifiable factual claims and
harmful claims. The study covers various activities, including dataset
collection, dataset pre-processing, architecture selection, setup of settings,
model training (fine-tuning), model testing, and implementation. The study
includes a comprehensive analysis of different models, with a special focus on
multilingual models where the same model is capable of processing social media
posts in both English and in low-resource languages such as Arabic, Bulgarian,
Dutch, Polish, Czech, Slovak. The results obtained from the study were
validated against state-of-the-art models, and the comparison demonstrated the
robustness of the proposed models. The novelty of this work lies in the
development of multi-label multilingual classification models that can
simultaneously detect harmful posts and posts that contain verifiable factual
claims in an efficient way.","[{'name': 'Sebastian Kula'}, {'name': 'Michal Gregor'}]",2024-08-13T08:55:28Z
http://arxiv.org/abs/2408.06732v1,http://arxiv.org/abs/2408.06732v1,"Exploring the anatomy of articulation rate in spontaneous English
  speech: relationships between utterance length effects and social factors","Speech rate has been shown to vary across social categories such as gender,
age, and dialect, while also being conditioned by properties of speech
planning. The effect of utterance length, where speech rate is faster and less
variable for longer utterances, has also been shown to reduce the role of
social factors once it has been accounted for, leaving unclear the relationship
between social factors and speech production in conditioning speech rate.
Through modelling of speech rate across 13 English speech corpora, it is found
that utterance length has the largest effect on speech rate, though this effect
itself varies little across corpora and speakers. While age and gender also
modulate speech rate, their effects are much smaller in magnitude. These
findings suggest utterance length effects may be conditioned by articulatory
and perceptual constraints, and that social influences on speech rate should be
interpreted in the broader context of how speech rate variation is structured.","[{'name': 'James Tanner'}, {'name': 'Morgan Sonderegger'}, {'name': 'Jane Stuart-Smith'}, {'name': 'Tyler Kendall'}, {'name': 'Jeff Mielke'}, {'name': 'Robin Dodsworth'}, {'name': 'Erik Thomas'}]",2024-08-13T08:47:29Z
http://arxiv.org/abs/2408.06731v1,http://arxiv.org/abs/2408.06731v1,"Large language models can consistently generate high-quality content for
  election disinformation operations","Advances in large language models have raised concerns about their potential
use in generating compelling election disinformation at scale. This study
presents a two-part investigation into the capabilities of LLMs to automate
stages of an election disinformation operation. First, we introduce DisElect, a
novel evaluation dataset designed to measure LLM compliance with instructions
to generate content for an election disinformation operation in localised UK
context, containing 2,200 malicious prompts and 50 benign prompts. Using
DisElect, we test 13 LLMs and find that most models broadly comply with these
requests; we also find that the few models which refuse malicious prompts also
refuse benign election-related prompts, and are more likely to refuse to
generate content from a right-wing perspective. Secondly, we conduct a series
of experiments (N=2,340) to assess the ""humanness"" of LLMs: the extent to which
disinformation operation content generated by an LLM is able to pass as
human-written. Our experiments suggest that almost all LLMs tested released
since 2022 produce election disinformation operation content indiscernible by
human evaluators over 50% of the time. Notably, we observe that multiple models
achieve above-human levels of humanness. Taken together, these findings suggest
that current LLMs can be used to generate high-quality content for election
disinformation operations, even in hyperlocalised scenarios, at far lower costs
than traditional methods, and offer researchers and policymakers an empirical
benchmark for the measurement and evaluation of these capabilities in current
and future models.","[{'name': 'Angus R. Williams'}, {'name': 'Liam Burke-Moore'}, {'name': 'Ryan Sze-Yin Chan'}, {'name': 'Florence E. Enock'}, {'name': 'Federico Nanni'}, {'name': 'Tvesha Sippy'}, {'name': 'Yi-Ling Chung'}, {'name': 'Evelina Gabasova'}, {'name': 'Kobi Hackenburg'}, {'name': 'Jonathan Bright'}]",2024-08-13T08:45:34Z
http://arxiv.org/abs/2408.06725v1,http://arxiv.org/abs/2408.06725v1,"Enhancing Visual Dialog State Tracking through Iterative Object-Entity
  Alignment in Multi-Round Conversations","Visual Dialog (VD) is a task where an agent answers a series of image-related
questions based on a multi-round dialog history. However, previous VD methods
often treat the entire dialog history as a simple text input, disregarding the
inherent conversational information flows at the round level. In this paper, we
introduce Multi-round Dialogue State Tracking model (MDST), a framework that
addresses this limitation by leveraging the dialogue state learned from dialog
history to answer questions. MDST captures each round of dialog history,
constructing internal dialogue state representations defined as 2-tuples of
vision-language representations. These representations effectively ground the
current question, enabling the generation of accurate answers. Experimental
results on the VisDial v1.0 dataset demonstrate that MDST achieves a new
state-of-the-art performance in generative setting. Furthermore, through a
series of human studies, we validate the effectiveness of MDST in generating
long, consistent, and human-like answers while consistently answering a series
of questions correctly.","[{'name': 'Wei Pang'}, {'name': 'Ruixue Duan'}, {'name': 'Jinfu Yang'}, {'name': 'Ning Li'}]",2024-08-13T08:36:15Z
http://arxiv.org/abs/2408.06675v1,http://arxiv.org/abs/2408.06675v1,"Latin Treebanks in Review: An Evaluation of Morphological Tagging Across
  Time","Existing Latin treebanks draw from Latin's long written tradition, spanning
17 centuries and a variety of cultures. Recent efforts have begun to harmonize
these treebanks' annotations to better train and evaluate morphological
taggers. However, the heterogeneity of these treebanks must be carefully
considered to build effective and reliable data. In this work, we review
existing Latin treebanks to identify the texts they draw from, identify their
overlap, and document their coverage across time and genre. We additionally
design automated conversions of their morphological feature annotations into
the conventions of standard Latin grammar. From this, we build new time-period
data splits that draw from the existing treebanks which we use to perform a
broad cross-time analysis for POS and morphological feature tagging. We find
that BERT-based taggers outperform existing taggers while also being more
robust to cross-domain shifts.","[{'name': 'Marisa Hudspeth'}, {'name': ""Brendan O'Connor""}, {'name': 'Laure Thompson'}]",2024-08-13T06:55:54Z
http://arxiv.org/abs/2408.06673v1,http://arxiv.org/abs/2408.06673v1,Pragmatic inference of scalar implicature by LLMs,"This study investigates how Large Language Models (LLMs), particularly BERT
(Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic
inference of scalar implicature, such as some. Two sets of experiments were
conducted using cosine similarity and next sentence/token prediction as
experimental methods. The results in experiment 1 showed that, both models
interpret some as pragmatic implicature not all in the absence of context,
aligning with human language processing. In experiment 2, in which Question
Under Discussion (QUD) was presented as a contextual cue, BERT showed
consistent performance regardless of types of QUDs, while GPT-2 encountered
processing difficulties since a certain type of QUD required pragmatic
inference for implicature. The findings revealed that, in terms of theoretical
approaches, BERT inherently incorporates pragmatic implicature not all within
the term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2
seems to encounter processing difficulties in inferring pragmatic implicature
within context, consistent with Context-driven model (Sperber and Wilson,
2002).","[{'name': 'Ye-eun Cho'}, {'name': 'Seong mook Kim'}]",2024-08-13T06:52:29Z
http://arxiv.org/abs/2408.06663v2,http://arxiv.org/abs/2408.06663v2,"Amuro & Char: Analyzing the Relationship between Pre-Training and
  Fine-Tuning of Large Language Models","The development of large language models leads to the formation of a
pre-train-then-align paradigm, in which the model is typically pre-trained on a
large text corpus and undergoes a tuning stage to align the model with human
preference or downstream tasks. In this work, we investigate the relationship
between pre-training and fine-tuning by fine-tuning multiple intermediate
pre-trained model checkpoints. Our results on 18 datasets suggest that i)
continual pre-training improves the model in a latent way that unveils after
fine-tuning; ii) with extra fine-tuning, the datasets that the model does not
demonstrate capability gain much more than those that the model performs well
during the pre-training stage; iii) although model benefits significantly
through supervised fine-tuning, it may forget previously known domain knowledge
and the tasks that are not seen during fine-tuning; iv) the model resembles
high sensitivity to evaluation prompts after supervised fine-tuning, but this
sensitivity can be alleviated by more pre-training.","[{'name': 'Kaiser Sun'}, {'name': 'Mark Dredze'}]",2024-08-13T06:28:43Z
http://arxiv.org/abs/2408.06634v1,http://arxiv.org/abs/2408.06634v1,"Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM
  Approach","Accurate stock market predictions following earnings reports are crucial for
investors. Traditional methods, particularly classical machine learning models,
struggle with these predictions because they cannot effectively process and
interpret extensive textual data contained in earnings reports and often
overlook nuances that influence market movements. This paper introduces an
advanced approach by employing Large Language Models (LLMs) instruction
fine-tuned with a novel combination of instruction-based techniques and
quantized low-rank adaptation (QLoRA) compression. Our methodology integrates
'base factors', such as financial metric growth and earnings transcripts, with
'external factors', including recent market indices performances and analyst
grades, to create a rich, supervised dataset. This comprehensive dataset
enables our models to achieve superior predictive performance in terms of
accuracy, weighted F1, and Matthews correlation coefficient (MCC), especially
evident in the comparison with benchmarks such as GPT-4. We specifically
highlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases
significant improvements over baseline models. The paper also discusses the
potential of expanding the output capabilities to include a 'Hold' option and
extending the prediction horizon, aiming to accommodate various investment
styles and time frames. This study not only demonstrates the power of
integrating cutting-edge AI with fine-tuned financial data but also paves the
way for future research in enhancing AI-driven financial analysis tools.","[{'name': 'Haowei Ni'}, {'name': 'Shuchen Meng'}, {'name': 'Xupeng Chen'}, {'name': 'Ziqing Zhao'}, {'name': 'Andi Chen'}, {'name': 'Panfeng Li'}, {'name': 'Shiyao Zhang'}, {'name': 'Qifu Yin'}, {'name': 'Yuanqing Wang'}, {'name': 'Yuxi Chan'}]",2024-08-13T04:53:31Z
http://arxiv.org/abs/2408.06632v1,http://arxiv.org/abs/2408.06632v1,"EditScribe: Non-Visual Image Editing with Natural Language Verification
  Loops","Image editing is an iterative process that requires precise visual evaluation
and manipulation for the output to match the editing intent. However, current
image editing tools do not provide accessible interaction nor sufficient
feedback for blind and low vision individuals to achieve this level of control.
To address this, we developed EditScribe, a prototype system that makes image
editing accessible using natural language verification loops powered by large
multimodal models. Using EditScribe, the user first comprehends the image
content through initial general and object descriptions, then specifies edit
actions using open-ended natural language prompts. EditScribe performs the
image edit, and provides four types of verification feedback for the user to
verify the performed edit, including a summary of visual changes, AI judgement,
and updated general and object descriptions. The user can ask follow-up
questions to clarify and probe into the edits or verification feedback, before
performing another edit. In a study with ten blind or low-vision users, we
found that EditScribe supported participants to perform and verify image edit
actions non-visually. We observed different prompting strategies from
participants, and their perceptions on the various types of verification
feedback. Finally, we discuss the implications of leveraging natural language
verification loops to make visual authoring non-visually accessible.","[{'name': 'Ruei-Che Chang'}, {'name': 'Yuxuan Liu'}, {'name': 'Lotus Zhang'}, {'name': 'Anhong Guo'}]",2024-08-13T04:40:56Z
http://arxiv.org/abs/2408.06631v1,http://arxiv.org/abs/2408.06631v1,"IFShip: A Large Vision-Language Model for Interpretable Fine-grained
  Ship Classification via Domain Knowledge-Enhanced Instruction Tuning","End-to-end interpretation is currently the prevailing paradigm for remote
sensing fine-grained ship classification (RS-FGSC) task. However, its inference
process is uninterpretable, leading to criticism as a black box model. To
address this issue, we propose a large vision-language model (LVLM) named
IFShip for interpretable fine-grained ship classification. Unlike traditional
methods, IFShip excels in interpretability by accurately conveying the
reasoning process of FGSC in natural language. Specifically, we first design a
domain knowledge-enhanced Chain-of-Thought (COT) prompt generation mechanism.
This mechanism is used to semi-automatically construct a task-specific
instruction-following dataset named TITANIC-FGS, which emulates human-like
logical decision-making. We then train the IFShip model using task instructions
tuned with the TITANIC-FGS dataset. Building on IFShip, we develop an FGSC
visual chatbot that redefines the FGSC problem as a step-by-step reasoning task
and conveys the reasoning process in natural language. Experimental results
reveal that the proposed method surpasses state-of-the-art FGSC algorithms in
both classification interpretability and accuracy. Moreover, compared to LVLMs
like LLaVA and MiniGPT-4, our approach demonstrates superior expertise in the
FGSC task. It provides an accurate chain of reasoning when fine-grained ship
types are recognizable to the human eye and offers interpretable explanations
when they are not.","[{'name': 'Mingning Guo'}, {'name': 'Mengwei Wu'}, {'name': 'Yuxiang Shen'}, {'name': 'Haifeng Li'}, {'name': 'Chao Tao'}]",2024-08-13T04:36:18Z
http://arxiv.org/abs/2408.06627v1,http://arxiv.org/abs/2408.06627v1,WorldScribe: Towards Context-Aware Live Visual Descriptions,"Automated live visual descriptions can aid blind people in understanding
their surroundings with autonomy and independence. However, providing
descriptions that are rich, contextual, and just-in-time has been a
long-standing challenge in accessibility. In this work, we develop WorldScribe,
a system that generates automated live real-world visual descriptions that are
customizable and adaptive to users' contexts: (i) WorldScribe's descriptions
are tailored to users' intents and prioritized based on semantic relevance.
(ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively
succinct descriptions for dynamic scenes, while presenting longer and detailed
ones for stable settings. (iii) WorldScribe is adaptive to sound contexts,
e.g., increasing volume in noisy environments, or pausing when conversations
start. Powered by a suite of vision, language, and sound recognition models,
WorldScribe introduces a description generation pipeline that balances the
tradeoffs between their richness and latency to support real-time use. The
design of WorldScribe is informed by prior work on providing visual
descriptions and a formative study with blind participants. Our user study and
subsequent pipeline evaluation show that WorldScribe can provide real-time and
fairly accurate visual descriptions to facilitate environment understanding
that is adaptive and customized to users' contexts. Finally, we discuss the
implications and further steps toward making live visual descriptions more
context-aware and humanized.","[{'name': 'Ruei-Che Chang'}, {'name': 'Yuxuan Liu'}, {'name': 'Anhong Guo'}]",2024-08-13T04:32:45Z
http://arxiv.org/abs/2408.06621v1,http://arxiv.org/abs/2408.06621v1,"Towards Robust and Cost-Efficient Knowledge Unlearning for Large
  Language Models","Large Language Models (LLMs) have demonstrated strong reasoning and
memorization capabilities via pretraining on massive textual corpora. However,
training LLMs on human-written text entails significant risk of privacy and
copyright violations, which demands an efficient machine unlearning framework
to remove knowledge of sensitive data without retraining the model from
scratch. While Gradient Ascent (GA) is widely used for unlearning by reducing
the likelihood of generating unwanted information, the unboundedness of
increasing the cross-entropy loss causes not only unstable optimization, but
also catastrophic forgetting of knowledge that needs to be retained. We also
discover its joint application under low-rank adaptation results in
significantly suboptimal computational cost vs. generative performance
trade-offs. In light of this limitation, we propose two novel techniques for
robust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge
loss that suppresses unwanted tokens by increasing the probability of the next
most likely token, thereby retaining fluency and structure in language
generation. We also propose to initialize low-rank adapter weights based on
Fisher-weighted low-rank approximation, which induces faster unlearning and
better knowledge retention by allowing model updates to be focused on
parameters that are important in generating textual data we wish to remove.","[{'name': 'Sungmin Cha'}, {'name': 'Sungjun Cho'}, {'name': 'Dasol Hwang'}, {'name': 'Moontae Lee'}]",2024-08-13T04:18:32Z
http://arxiv.org/abs/2408.06618v1,http://arxiv.org/abs/2408.06618v1,"Generalized knowledge-enhanced framework for biomedical entity and
  relation extraction","In recent years, there has been an increasing number of frameworks developed
for biomedical entity and relation extraction. This research effort aims to
address the accelerating growth in biomedical publications and the intricate
nature of biomedical texts, which are written for mainly domain experts. To
handle these challenges, we develop a novel framework that utilizes external
knowledge to construct a task-independent and reusable background knowledge
graph for biomedical entity and relation extraction. The design of our model is
inspired by how humans learn domain-specific topics. In particular, humans
often first acquire the most basic and common knowledge regarding a field to
build the foundational knowledge and then use that as a basis for extending to
various specialized topics. Our framework employs such common-knowledge-sharing
mechanism to build a general neural-network knowledge graph that is learning
transferable to different domain-specific biomedical texts effectively.
Experimental evaluations demonstrate that our model, equipped with this
generalized and cross-transferable knowledge base, achieves competitive
performance benchmarks, including BioRelEx for binding interaction detection
and ADE for Adverse Drug Effect identification.","[{'name': 'Minh Nguyen'}, {'name': 'Phuong Le'}]",2024-08-13T04:06:45Z
http://arxiv.org/abs/2408.06610v1,http://arxiv.org/abs/2408.06610v1,CROME: Cross-Modal Adapters for Efficient Multimodal LLM,"Multimodal Large Language Models (MLLMs) demonstrate remarkable
image-language capabilities, but their widespread use faces challenges in
cost-effective training and adaptation. Existing approaches often necessitate
expensive language model retraining and limited adaptability. Additionally, the
current focus on zero-shot performance improvements offers insufficient
guidance for task-specific tuning. We propose CROME, an efficient
vision-language instruction tuning framework. It features a novel gated
cross-modal adapter that effectively combines visual and textual
representations prior to input into a frozen LLM. This lightweight adapter,
trained with minimal parameters, enables efficient cross-modal understanding.
Notably, CROME demonstrates superior zero-shot performance on standard visual
question answering and instruction-following benchmarks. Moreover, it yields
fine-tuning with exceptional parameter efficiency, competing with task-specific
specialist state-of-the-art methods. CROME demonstrates the potential of pre-LM
alignment for building scalable, adaptable, and parameter-efficient multimodal
models.","[{'name': 'Sayna Ebrahimi'}, {'name': 'Sercan O. Arik'}, {'name': 'Tejas Nama'}, {'name': 'Tomas Pfister'}]",2024-08-13T03:45:11Z
http://arxiv.org/abs/2408.06598v1,http://arxiv.org/abs/2408.06598v1,"A Perspective on Large Language Models, Intelligent Machines, and
  Knowledge Acquisition","Large Language Models (LLMs) are known for their remarkable ability to
generate synthesized 'knowledge', such as text documents, music, images, etc.
However, there is a huge gap between LLM's and human capabilities for
understanding abstract concepts and reasoning. We discuss these issues in a
larger philosophical context of human knowledge acquisition and the Turing
test. In addition, we illustrate the limitations of LLMs by analyzing GPT-4
responses to questions ranging from science and math to common sense reasoning.
These examples show that GPT-4 can often imitate human reasoning, even though
it lacks understanding. However, LLM responses are synthesized from a large LLM
model trained on all available data. In contrast, human understanding is based
on a small number of abstract concepts. Based on this distinction, we discuss
the impact of LLMs on acquisition of human knowledge and education.","[{'name': 'Vladimir Cherkassky'}, {'name': 'Eng Hock Lee'}]",2024-08-13T03:25:49Z
http://arxiv.org/abs/2408.06583v3,http://arxiv.org/abs/2408.06583v3,"An Event Structure-aware Generative Model for Biomedical Event
  Extraction","Biomedical Event Extraction (BEE) is a challenging task that involves
modeling complex relationships between fine-grained entities in biomedical
text. BEE has traditionally been formulated as a classification problem. With
the recent technological advancements in large language models (LLMs),
generation-based models that cast event extraction as a sequence generation
problem have attracted much attention from the NLP research communities.
However, current generative models often overlook the importance of
cross-instance information from complex event structures such as nested events
and overlapping events, which contribute quite significantly in the benchmark
datasets. In this paper, we propose an event structure-aware generative model
called GenBEE, which can capture complex event structures in biomedical text
for biomedical event extraction. In particular, GenBEE constructs event prompts
that distill knowledge from LLMs for incorporating both label semantics and
argument dependency relationships into the proposed model. In addition, GenBEE
also generates prefixes with event structural prompts to incorporate structural
features for improving the model's overall performance. We have evaluated the
proposed GenBEE model on three widely used biomedical event extraction
benchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that
GenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,
and achieved competitive results when compared to the state-of-the-art
classification-based models on the PHEE dataset.","[{'name': 'Haohan Yuan'}, {'name': 'Siu Cheung Hui'}, {'name': 'Haopeng Zhang'}]",2024-08-13T02:43:19Z
http://arxiv.org/abs/2408.06578v2,http://arxiv.org/abs/2408.06578v2,OpenEP: Open-Ended Future Event Prediction,"Future event prediction (FEP) is a long-standing and crucial task in the
world, as understanding the evolution of events enables early risk
identification, informed decision-making, and strategic planning. Existing work
typically treats event prediction as classification tasks and confines the
outcomes of future events to a fixed scope, such as yes/no questions, candidate
set, and taxonomy, which is difficult to include all possible outcomes of
future events. In this paper, we introduce OpenEP (an Open-Ended Future Event
Prediction task), which generates flexible and diverse predictions aligned with
real-world scenarios. This is mainly reflected in two aspects: firstly, the
predictive questions are diverse, covering different stages of event
development and perspectives; secondly, the outcomes are flexible, without
constraints on scope or format. To facilitate the study of this task, we
construct OpenEPBench, an open-ended future event prediction dataset. For
question construction, we pose questions from seven perspectives, including
location, time, event development, event outcome, event impact, event response,
and other, to facilitate an in-depth analysis and understanding of the
comprehensive evolution of events. For outcome construction, we collect
free-form text containing the outcomes as ground truth to provide semantically
complete and detail-enriched outcomes. Furthermore, we propose StkFEP, a
stakeholder-enhanced future event prediction framework, that incorporates event
characteristics for open-ended settings. Our method extracts stakeholders
involved in events to extend questions to gather diverse information. We also
collect historically events that are relevant and similar to the question to
reveal potential evolutionary patterns. Experiment results indicate that
accurately predicting future events in open-ended settings is challenging for
existing LLMs.","[{'name': 'Yong Guan'}, {'name': 'Hao Peng'}, {'name': 'Xiaozhi Wang'}, {'name': 'Lei Hou'}, {'name': 'Juanzi Li'}]",2024-08-13T02:35:54Z
http://arxiv.org/abs/2408.06576v1,http://arxiv.org/abs/2408.06576v1,"CTISum: A New Benchmark Dataset For Cyber Threat Intelligence
  Summarization","Cyber Threat Intelligence (CTI) summarization task requires the system to
generate concise and accurate highlights from raw intelligence data, which
plays an important role in providing decision-makers with crucial information
to quickly detect and respond to cyber threats in the cybersecurity domain.
However, efficient techniques for summarizing CTI reports, including facts,
analytical insights, attack processes, etc., have largely been unexplored,
primarily due to the lack of available dataset. To this end, we present CTISum,
a new benchmark for CTI summarization task. Considering the importance of
attack process, a novel fine-grained subtask of attack process summarization is
proposed to enable defenders to assess risk, identify security gaps,
vulnerabilities, and so on. Specifically, we first design a multi-stage
annotation pipeline to gather and annotate the CTI data, and then benchmark the
CTISum with a collection of extractive and abstractive summarization methods.
Experimental results show that current state-of-the-art models exhibit
limitations when applied to CTISum, underscoring the fact that automatically
producing concise summaries of CTI reports remains an open research challenge.","[{'name': 'Wei Peng'}, {'name': 'Junmei Ding'}, {'name': 'Wei Wang'}, {'name': 'Lei Cui'}, {'name': 'Wei Cai'}, {'name': 'Zhiyu Hao'}, {'name': 'Xiaochun Yun'}]",2024-08-13T02:25:16Z
http://arxiv.org/abs/2408.06574v1,http://arxiv.org/abs/2408.06574v1,"SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark
  Large Language Model","Large language models (LLMs) have shown remarkable achievements across
various language tasks.To enhance the performance of LLMs in scientific
literature services, we developed the scientific literature LLM (SciLit-LLM)
through pre-training and supervised fine-tuning on scientific literature,
building upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge
service system Spark Research Assistant (SparkRA) based on our SciLit-LLM.
SparkRA is accessible online and provides three primary functions: literature
investigation, paper reading, and academic writing. As of July 30, 2024,
SparkRA has garnered over 50,000 registered users, with a total usage count
exceeding 1.3 million.","[{'name': 'Dayong Wu'}, {'name': 'Jiaqi Li'}, {'name': 'Baoxin Wang'}, {'name': 'Honghong Zhao'}, {'name': 'Siyuan Xue'}, {'name': 'Yanjie Yang'}, {'name': 'Zhijun Chang'}, {'name': 'Rui Zhang'}, {'name': 'Li Qian'}, {'name': 'Bo Wang'}, {'name': 'Shijin Wang'}, {'name': 'Zhixiong Zhang'}, {'name': 'Guoping Hu'}]",2024-08-13T02:18:47Z
http://arxiv.org/abs/2408.06569v1,http://arxiv.org/abs/2408.06569v1,Social Debiasing for Fair Multi-modal LLMs,"Multi-modal Large Language Models (MLLMs) have advanced significantly,
offering powerful vision-language understanding capabilities. However, these
models often inherit severe social biases from their training datasets, leading
to unfair predictions based on attributes like race and gender. This paper
addresses the issue of social biases in MLLMs by i) Introducing a comprehensive
Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a
more diverse and extensive training set compared to existing datasets. ii)
Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by
revisiting the MLLM training process, rescaling the autoregressive loss
function, and improving data sampling methods to counteract biases. Through
extensive experiments on various MLLMs, our CMSC dataset and ASD method
demonstrate a significant reduction in social biases while maintaining the
models' original performance.","[{'name': 'Harry Cheng'}, {'name': 'Yangyang Guo'}, {'name': 'Qingpei Guo'}, {'name': 'Ming Yang'}, {'name': 'Tian Gan'}, {'name': 'Liqiang Nie'}]",2024-08-13T02:08:32Z
http://arxiv.org/abs/2408.06567v1,http://arxiv.org/abs/2408.06567v1,"AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out
  Strategies","In recent years, with the rapid application of large language models across
various fields, the scale of these models has gradually increased, and the
resources required for their pre-training have grown exponentially. Training an
LLM from scratch will cost a lot of computation resources while scaling up from
a smaller model is a more efficient approach and has thus attracted significant
attention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B
Mixture of Experts (MoE) language model that has 8 experts with 16 billion
parameters each and is developed using an innovative training methodology
called EfficientScale. This approach optimizes performance while minimizing
data requirements through a two-stage process. The first stage, termed
Scale-Up, initializes the larger model with weights from a pre-trained smaller
model, enabling substantial knowledge transfer and continuous pretraining with
significantly less data. The second stage, Scale-Out, uses a pre-trained dense
model to initialize the MoE experts, further enhancing knowledge transfer and
performance. Extensive validation experiments on 1.8B and 7B models compared
various initialization schemes, achieving models that maintain and reduce loss
during continuous pretraining. Utilizing the optimal scheme, we successfully
trained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating
significant improvements in performance and training efficiency.","[{'name': 'Bo-Wen Zhang'}, {'name': 'Liangdong Wang'}, {'name': 'Ye Yuan'}, {'name': 'Jijie Li'}, {'name': 'Shuhao Gu'}, {'name': 'Mengdi Zhao'}, {'name': 'Xinya Wu'}, {'name': 'Guang Liu'}, {'name': 'Chengwei Wu'}, {'name': 'Hanyu Zhao'}, {'name': 'Li Du'}, {'name': 'Yiming Ju'}, {'name': 'Quanyue Ma'}, {'name': 'Yulong Ao'}, {'name': 'Yingli Zhao'}, {'name': 'Songhe Zhu'}, {'name': 'Zhou Cao'}, {'name': 'Dong Liang'}, {'name': 'Yonghua Lin'}, {'name': 'Ming Zhang'}, {'name': 'Shunfei Wang'}, {'name': 'Yanxin Zhou'}, {'name': 'Min Ye'}, {'name': 'Xuekai Chen'}, {'name': 'Xinyang Yu'}, {'name': 'Xiangjun Huang'}, {'name': 'Jian Yang'}]",2024-08-13T02:07:00Z
http://arxiv.org/abs/2408.06537v3,http://arxiv.org/abs/2408.06537v3,"Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality
  Parallel Data Outperforms Traditional Web-Crawled Data","Recent research in neural machine translation (NMT) has shown that training
on high-quality machine-generated data can outperform training on
human-generated data. This work accompanies the first-ever release of a
LLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and
multi-sentence examples. We perform extensive experiments to demonstrate the
quality of our dataset in terms of its downstream impact on NMT model
performance. We find that training from scratch on our (machine-generated)
dataset outperforms training on the (web-crawled) WMT'23 training dataset
(which is 300 times larger), and also outperforms training on the top-quality
subset of the WMT'23 training dataset. We also find that performing
self-distillation by finetuning the LLM which generated this dataset
outperforms the LLM's strong few-shot baseline. These findings corroborate the
quality of our dataset, and demonstrate the value of high-quality
machine-generated data in improving performance of NMT models.","[{'name': 'Mara Finkelstein'}, {'name': 'David Vilar'}, {'name': 'Markus Freitag'}]",2024-08-13T00:06:56Z
http://arxiv.org/abs/2408.06527v1,http://arxiv.org/abs/2408.06527v1,"Chain-of-Strategy Planning with LLMs: Aligning the Generation of
  Psychotherapy Dialogue with Strategy in Motivational Interviewing","Recent advancements in large language models (LLMs) have shown promise in
generating psychotherapeutic dialogues, especially in Motivational Interviewing
(MI). However, how to employ strategies, a set of motivational interviewing
(MI) skills, to generate therapeutic-adherent conversations with explainability
is underexplored. We propose an approach called strategy-aware dialogue
generation with Chain-of-Strategy (CoS) planning, which first predicts MI
strategies as reasoning and utilizes these strategies to guide the subsequent
dialogue generation. It brings the potential for controllable and explainable
generation in psychotherapy by aligning the generated MI dialogues with
therapeutic strategies. Extensive experiments including automatic and human
evaluations are conducted to validate the effectiveness of the MI strategy. Our
findings demonstrate the potential of LLMs in producing strategically aligned
dialogues and suggest directions for practical applications in
psychotherapeutic settings.","[{'name': 'Xin Sun'}, {'name': 'Xiao Tang'}, {'name': 'Abdallah El Ali'}, {'name': 'Zhuying Li'}, {'name': 'Xiaoyu Shen'}, {'name': 'Pengjie Ren'}, {'name': 'Jan de Wit'}, {'name': 'Jiahuan Pei'}, {'name': 'Jos A. Bosch'}]",2024-08-12T23:19:02Z
http://arxiv.org/abs/2408.06520v1,http://arxiv.org/abs/2408.06520v1,"Hierarchical in-Context Reinforcement Learning with Hindsight Modular
  Reflections for Planning","Large Language Models (LLMs) have demonstrated remarkable abilities in
various language tasks, making them promising candidates for decision-making in
robotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose
Hierarchical in-Context Reinforcement Learning (HCRL), a novel framework that
decomposes complex tasks into sub-tasks using an LLM-based high-level policy,
in which a complex task is decomposed into sub-tasks by a high-level policy
on-the-fly. The sub-tasks, defined by goals, are assigned to the low-level
policy to complete. Once the LLM agent determines that the goal is finished, a
new goal will be proposed. To improve the agent's performance in multi-episode
execution, we propose Hindsight Modular Reflection (HMR), where, instead of
reflecting on the full trajectory, we replace the task objective with
intermediate goals and let the agent reflect on shorter trajectories to improve
reflection efficiency. We evaluate the decision-making ability of the proposed
HCRL in three benchmark environments--ALFWorld, Webshop, and HotpotQA. Results
show that HCRL can achieve 9%, 42%, and 10% performance improvement in 5
episodes of execution over strong in-context learning baselines.","[{'name': 'Chuanneng Sun'}, {'name': 'Songjun Huang'}, {'name': 'Dario Pompili'}]",2024-08-12T22:40:01Z
http://arxiv.org/abs/2408.06518v1,http://arxiv.org/abs/2408.06518v1,"Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in
  Language Models","Despite their wide adoption, the biases and unintended behaviors of language
models remain poorly understood. In this paper, we identify and characterize a
phenomenon never discussed before, which we call semantic leakage, where models
leak irrelevant information from the prompt into the generation in unexpected
ways. We propose an evaluation setting to detect semantic leakage both by
humans and automatically, curate a diverse test suite for diagnosing this
behavior, and measure significant semantic leakage in 13 flagship models. We
also show that models exhibit semantic leakage in languages besides English and
across different settings and generation scenarios. This discovery highlights
yet another type of bias in language models that affects their generation
patterns and behavior.","[{'name': 'Hila Gonen'}, {'name': 'Terra Blevins'}, {'name': 'Alisa Liu'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Noah A. Smith'}]",2024-08-12T22:30:55Z
http://arxiv.org/abs/2408.06494v1,http://arxiv.org/abs/2408.06494v1,"What Color Scheme is More Effective in Assisting Readers to Locate
  Information in a Color-Coded Article?","Color coding, a technique assigning specific colors to cluster information
types, has proven advantages in aiding human cognitive activities, especially
reading and comprehension. The rise of Large Language Models (LLMs) has
streamlined document coding, enabling simple automatic text labeling with
various schemes. This has the potential to make color-coding more accessible
and benefit more users. However, the impact of color choice on information
seeking is understudied. We conducted a user study assessing various color
schemes' effectiveness in LLM-coded text documents, standardizing contrast
ratios to approximately 5.55:1 across schemes. Participants performed timed
information-seeking tasks in color-coded scholarly abstracts. Results showed
non-analogous and yellow-inclusive color schemes improved performance, with the
latter also being more preferred by participants. These findings can inform
better color scheme choices for text annotation. As LLMs advance document
coding, we advocate for more research focusing on the ""color"" aspect of
color-coding techniques.","[{'name': 'Ho Yin Ng'}, {'name': 'Zeyu He'}, {'name': ""Ting-Hao 'Kenneth' Huang""}]",2024-08-12T21:04:16Z
http://arxiv.org/abs/2408.06484v1,http://arxiv.org/abs/2408.06484v1,"Cross-Lingual Conversational Speech Summarization with Large Language
  Models","Cross-lingual conversational speech summarization is an important problem,
but suffers from a dearth of resources. While transcriptions exist for a number
of languages, translated conversational speech is rare and datasets containing
summaries are non-existent. We build upon the existing Fisher and Callhome
Spanish-English Speech Translation corpus by supplementing the translations
with summaries. The summaries are generated using GPT-4 from the reference
translations and are treated as ground truth. The task is to generate similar
summaries in the presence of transcription and translation errors. We build a
baseline cascade-based system using open-source speech recognition and machine
translation models. We test a range of LLMs for summarization and analyze the
impact of transcription and translation errors. Adapting the Mistral-7B model
for this task performs significantly better than off-the-shelf models and
matches the performance of GPT-4.","[{'name': 'Max Nelson'}, {'name': 'Shannon Wotherspoon'}, {'name': 'Francis Keith'}, {'name': 'William Hartmann'}, {'name': 'Matthew Snover'}]",2024-08-12T20:40:46Z
http://arxiv.org/abs/2408.06474v1,http://arxiv.org/abs/2408.06474v1,TOGGL: Transcribing Overlapping Speech with Staggered Labeling,"Transcribing the speech of multiple overlapping speakers typically requires
separating the audio into multiple streams and recognizing each one
independently. More recent work jointly separates and transcribes, but requires
a separate decoding component for each speaker. We propose the TOGGL model to
simultaneously transcribe the speech of multiple speakers. The TOGGL model uses
special output tokens to attribute the speech to each speaker with only a
single decoder. Our approach generalizes beyond two speakers, even when trained
only on two-speaker data. We demonstrate superior performance compared to
competing approaches on a conversational speech dataset. Our approach also
improves performance on single-speaker audio.","[{'name': 'Chak-Fai Li'}, {'name': 'William Hartmann'}, {'name': 'Matthew Snover'}]",2024-08-12T20:19:27Z
http://arxiv.org/abs/2408.06458v1,http://arxiv.org/abs/2408.06458v1,"Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in
  Language Models","We propose a novel in-context learning algorithm for building autonomous
decision-making language agents. The language agent continuously attempts to
solve the same task by self-correcting each time the task fails. Our selected
language agent demonstrates the ability to solve tasks in a text-based game
environment. Our results show that the gemma-2-9b-it language model, using our
proposed method, can successfully complete two of six tasks that failed in the
first attempt. This highlights the effectiveness of our approach in enhancing
the problem-solving capabilities of a single language model through
self-correction, paving the way for more advanced autonomous agents. The code
is publicly available at
https://github.com/YenCheHsiao/AutonomousLLMAgentwithAdaptingPlanning.","[{'name': 'Yen-Che Hsiao'}, {'name': 'Abhishek Dutta'}]",2024-08-12T19:18:05Z
http://arxiv.org/abs/2408.06335v1,http://arxiv.org/abs/2408.06335v1,"LOLgorithm: Integrating Semantic,Syntactic and Contextual Elements for
  Humor Classification","This paper explores humor detection through a linguistic lens, prioritizing
syntactic, semantic, and contextual features over computational methods in
Natural Language Processing. We categorize features into syntactic, semantic,
and contextual dimensions, including lexicons, structural statistics, Word2Vec,
WordNet, and phonetic style. Our proposed model, Colbert, utilizes BERT
embeddings and parallel hidden layers to capture sentence congruity. By
combining syntactic, semantic, and contextual features, we train Colbert for
humor detection. Feature engineering examines essential syntactic and semantic
features alongside BERT embeddings. SHAP interpretations and decision trees
identify influential features, revealing that a holistic approach improves
humor detection accuracy on unseen data. Integrating linguistic cues from
different dimensions enhances the model's ability to understand humor
complexity beyond traditional computational methods.","[{'name': 'Tanisha Khurana'}, {'name': 'Kaushik Pillalamarri'}, {'name': 'Vikram Pande'}, {'name': 'Munindar Singh'}]",2024-08-12T17:52:11Z
http://arxiv.org/abs/2408.06333v1,http://arxiv.org/abs/2408.06333v1,"FastFiD: Improve Inference Efficiency of Open Domain Question Answering
  via Sentence Selection","Open Domain Question Answering (ODQA) has been advancing rapidly in recent
times, driven by significant developments in dense passage retrieval and
pretrained language models. Current models typically incorporate the FiD
framework, which is composed by a neural retriever alongside an encoder-decoder
neural reader. In the answer generation process, the retriever will retrieve
numerous passages (around 100 for instance), each of which is then individually
encoded by the encoder. Subsequently, the decoder makes predictions based on
these encoded passages. Nevertheless, this framework can be relatively
time-consuming, particularly due to the extensive length of the gathered
passages. To address this, we introduce FastFiD in this paper, a novel approach
that executes sentence selection on the encoded passages. This aids in
retaining valuable sentences while reducing the context length required for
generating answers. Experiments on three commonly used datasets (Natural
Questions, TriviaQA and ASQA) demonstrate that our method can enhance the
inference speed by 2.3X-5.7X, while simultaneously maintaining the model's
performance. Moreover, an in-depth analysis of the model's attention reveals
that the selected sentences indeed hold a substantial contribution towards the
final answer. The codes are publicly available at
https://github.com/thunlp/FastFiD.","[{'name': 'Yufei Huang'}, {'name': 'Xu Han'}, {'name': 'Maosong Sun'}]",2024-08-12T17:50:02Z
http://arxiv.org/abs/2408.06332v1,http://arxiv.org/abs/2408.06332v1,"Animate, or Inanimate, That is the Question for Large Language Models","The cognitive essence of humans is deeply intertwined with the concept of
animacy, which plays an essential role in shaping their memory, vision, and
multi-layered language understanding. Although animacy appears in language via
nuanced constraints on verbs and adjectives, it is also learned and refined
through extralinguistic information. Similarly, we assume that the LLMs'
limited abilities to understand natural language when processing animacy are
motivated by the fact that these models are trained exclusively on text.
  Hence, the question this paper aims to answer arises: can LLMs, in their
digital wisdom, process animacy in a similar way to what humans would do? We
then propose a systematic analysis via prompting approaches. In particular, we
probe different LLMs by prompting them using animate, inanimate, usual, and
stranger contexts. Results reveal that, although LLMs have been trained
predominantly on textual data, they exhibit human-like behavior when faced with
typical animate and inanimate entities in alignment with earlier studies.
Hence, LLMs can adapt to understand unconventional situations by recognizing
oddities as animated without needing to interface with unspoken cognitive
triggers humans rely on to break down animations.","[{'name': 'Leonardo Ranaldi'}, {'name': 'Giulia Pucci'}, {'name': 'Fabio Massimo Zanzotto'}]",2024-08-12T17:48:55Z
http://arxiv.org/abs/2408.06327v1,http://arxiv.org/abs/2408.06327v1,"VisualAgentBench: Towards Large Multimodal Models as Visual Foundation
  Agents","Large Multimodal Models (LMMs) have ushered in a new era in artificial
intelligence, merging capabilities in both language and vision to form highly
capable Visual Foundation Agents. These agents are postulated to excel across a
myriad of tasks, potentially approaching general artificial intelligence.
However, existing benchmarks fail to sufficiently challenge or showcase the
full potential of LMMs in complex, real-world environments. To address this
gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering
benchmark specifically designed to train and evaluate LMMs as visual foundation
agents across diverse scenarios, including Embodied, Graphical User Interface,
and Visual Design, with tasks formulated to probe the depth of LMMs'
understanding and interaction capabilities. Through rigorous testing across
nine proprietary LMM APIs and eight open models, we demonstrate the
considerable yet still developing agent capabilities of these models.
Additionally, VAB constructs a trajectory training set constructed through
hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and
Human Demonstrations, promoting substantial performance improvements in LMMs
through behavior cloning. Our work not only aims to benchmark existing models
but also provides a solid foundation for future development into visual
foundation agents. Code, train \& test data, and part of fine-tuned open LMMs
are available at \url{https://github.com/THUDM/VisualAgentBench}.","[{'name': 'Xiao Liu'}, {'name': 'Tianjie Zhang'}, {'name': 'Yu Gu'}, {'name': 'Iat Long Iong'}, {'name': 'Yifan Xu'}, {'name': 'Xixuan Song'}, {'name': 'Shudan Zhang'}, {'name': 'Hanyu Lai'}, {'name': 'Xinyi Liu'}, {'name': 'Hanlin Zhao'}, {'name': 'Jiadai Sun'}, {'name': 'Xinyue Yang'}, {'name': 'Yu Yang'}, {'name': 'Zehan Qi'}, {'name': 'Shuntian Yao'}, {'name': 'Xueqiao Sun'}, {'name': 'Siyi Cheng'}, {'name': 'Qinkai Zheng'}, {'name': 'Hao Yu'}, {'name': 'Hanchen Zhang'}, {'name': 'Wenyi Hong'}, {'name': 'Ming Ding'}, {'name': 'Lihang Pan'}, {'name': 'Xiaotao Gu'}, {'name': 'Aohan Zeng'}, {'name': 'Zhengxiao Du'}, {'name': 'Chan Hee Song'}, {'name': 'Yu Su'}, {'name': 'Yuxiao Dong'}, {'name': 'Jie Tang'}]",2024-08-12T17:44:17Z
http://arxiv.org/abs/2408.06303v1,http://arxiv.org/abs/2408.06303v1,Long-Form Answers to Visual Questions from Blind and Low Vision People,"Vision language models can now generate long-form answers to questions about
images - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a
dataset of long-form answers to visual questions posed by blind and low vision
(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,
collected from human expert describers and six VQA models. We develop and
annotate functional roles of sentences of LFVQA and demonstrate that long-form
answers contain information beyond the question answer such as explanations and
suggestions. We further conduct automatic and human evaluations with BLV and
sighted people to evaluate long-form answers. BLV people perceive both
human-written and generated long-form answers to be plausible, but generated
answers often hallucinate incorrect visual details, especially for unanswerable
visual questions (e.g., blurry or irrelevant images). To reduce hallucinations,
we evaluate the ability of VQA models to abstain from answering unanswerable
questions across multiple prompting strategies.","[{'name': 'Mina Huh'}, {'name': 'Fangyuan Xu'}, {'name': 'Yi-Hao Peng'}, {'name': 'Chongyan Chen'}, {'name': 'Hansika Murugu'}, {'name': 'Danna Gurari'}, {'name': 'Eunsol Choi'}, {'name': 'Amy Pavel'}]",2024-08-12T17:15:02Z
http://arxiv.org/abs/2408.06292v2,http://arxiv.org/abs/2408.06292v2,"The AI Scientist: Towards Fully Automated Open-Ended Scientific
  Discovery","One of the grand challenges of artificial general intelligence is developing
agents capable of conducting scientific research and discovering new knowledge.
While frontier models have already been used as aides to human scientists, e.g.
for brainstorming ideas, writing code, or prediction tasks, they still conduct
only a small part of the scientific process. This paper presents the first
comprehensive framework for fully automatic scientific discovery, enabling
frontier large language models to perform research independently and
communicate their findings. We introduce The AI Scientist, which generates
novel research ideas, writes code, executes experiments, visualizes results,
describes its findings by writing a full scientific paper, and then runs a
simulated review process for evaluation. In principle, this process can be
repeated to iteratively develop ideas in an open-ended fashion, acting like the
human scientific community. We demonstrate its versatility by applying it to
three distinct subfields of machine learning: diffusion modeling,
transformer-based language modeling, and learning dynamics. Each idea is
implemented and developed into a full paper at a cost of less than $15 per
paper. To evaluate the generated papers, we design and validate an automated
reviewer, which we show achieves near-human performance in evaluating paper
scores. The AI Scientist can produce papers that exceed the acceptance
threshold at a top machine learning conference as judged by our automated
reviewer. This approach signifies the beginning of a new era in scientific
discovery in machine learning: bringing the transformative benefits of AI
agents to the entire research process of AI itself, and taking us closer to a
world where endless affordable creativity and innovation can be unleashed on
the world's most challenging problems. Our code is open-sourced at
https://github.com/SakanaAI/AI-Scientist","[{'name': 'Chris Lu'}, {'name': 'Cong Lu'}, {'name': 'Robert Tjarko Lange'}, {'name': 'Jakob Foerster'}, {'name': 'Jeff Clune'}, {'name': 'David Ha'}]",2024-08-12T16:58:11Z
http://arxiv.org/abs/2408.06285v1,http://arxiv.org/abs/2408.06285v1,"Synthetic Patient-Physician Dialogue Generation from Clinical Notes
  Using LLM","Medical dialogue systems (MDS) enhance patient-physician communication,
improve healthcare accessibility, and reduce costs. However, acquiring suitable
data to train these systems poses significant challenges. Privacy concerns
prevent the use of real conversations, necessitating synthetic alternatives.
Synthetic dialogue generation from publicly available clinical notes offers a
promising solution to this issue, providing realistic data while safeguarding
privacy. Our approach, SynDial, uses a single LLM iteratively with zero-shot
prompting and a feedback loop to generate and refine high-quality synthetic
dialogues. The feedback consists of weighted evaluation scores for similarity
and extractiveness. The iterative process ensures dialogues meet predefined
thresholds, achieving superior extractiveness as a result of the feedback loop.
Additionally, evaluation shows that the generated dialogues excel in factuality
metric compared to the baselines and has comparable diversity scores with GPT4.","[{'name': 'Trisha Das'}, {'name': 'Dina Albassam'}, {'name': 'Jimeng Sun'}]",2024-08-12T16:49:22Z
http://arxiv.org/abs/2408.06281v1,http://arxiv.org/abs/2408.06281v1,MovieSum: An Abstractive Summarization Dataset for Movie Screenplays,"Movie screenplay summarization is challenging, as it requires an
understanding of long input contexts and various elements unique to movies.
Large language models have shown significant advancements in document
summarization, but they often struggle with processing long input contexts.
Furthermore, while television transcripts have received attention in recent
studies, movie screenplay summarization remains underexplored. To stimulate
research in this area, we present a new dataset, MovieSum, for abstractive
summarization of movie screenplays. This dataset comprises 2200 movie
screenplays accompanied by their Wikipedia plot summaries. We manually
formatted the movie screenplays to represent their structural elements.
Compared to existing datasets, MovieSum possesses several distinctive features:
(1) It includes movie screenplays, which are longer than scripts of TV
episodes. (2) It is twice the size of previous movie screenplay datasets. (3)
It provides metadata with IMDb IDs to facilitate access to additional external
knowledge. We also show the results of recently released large language models
applied to summarization on our dataset to provide a detailed baseline.","[{'name': 'Rohit Saxena'}, {'name': 'Frank Keller'}]",2024-08-12T16:43:09Z
http://arxiv.org/abs/2408.06276v2,http://arxiv.org/abs/2408.06276v2,"Review-driven Personalized Preference Reasoning with Large Language
  Models for Recommendation","Recent advancements in Large Language Models (LLMs) have demonstrated
exceptional performance across a wide range of tasks, generating significant
interest in their application to recommendation systems. However, existing
methods have not fully capitalized on the potential of LLMs, often constrained
by limited input information or failing to fully utilize their advanced
reasoning capabilities. To address these limitations, we introduce EXP3RT, a
novel LLM-based recommender designed to leverage rich preference information
contained in user and item reviews. EXP3RT is basically fine-tuned through
distillation from a teacher LLM to perform three key tasks in order: EXP3RT
first extracts and encapsulates essential subjective preferences from raw
reviews, aggregates and summarizes them according to specific criteria to
create user and item profiles. It then generates detailed step-by-step
reasoning followed by predicted rating, i.e., reasoning-enhanced rating
prediction, by considering both subjective and objective information from
user/item profiles and item descriptions. This personalized preference
reasoning from EXP3RT enhances rating prediction accuracy and also provides
faithful and reasonable explanations for recommendation. Extensive experiments
show that EXP3RT outperforms existing methods on both rating prediction and
candidate item reranking for top-k recommendation, while significantly
enhancing the explainability of recommendation systems.","[{'name': 'Jieyong Kim'}, {'name': 'Hyunseo Kim'}, {'name': 'Hyunjin Cho'}, {'name': 'SeongKu Kang'}, {'name': 'Buru Chang'}, {'name': 'Jinyoung Yeo'}, {'name': 'Dongha Lee'}]",2024-08-12T16:39:03Z
http://arxiv.org/abs/2408.06273v2,http://arxiv.org/abs/2408.06273v2,"FuxiTranyu: A Multilingual Large Language Model Trained with Balanced
  Data","Large language models (LLMs) have demonstrated prowess in a wide range of
tasks. However, many LLMs exhibit significant performance discrepancies between
high- and low-resource languages. To mitigate this challenge, we present
FuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the
need of the research community for balanced and high-performing multilingual
capabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is
trained from scratch on a meticulously balanced multilingual data repository
that contains 600 billion tokens covering 43 natural languages and 16
programming languages. In addition to the base model, we also develop two
instruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse
multilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined
with DPO on a preference dataset for enhanced alignment ability. Extensive
experiments on a wide range of multilingual benchmarks demonstrate the
competitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,
BLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability
analyses at both the neuron and representation level suggest that FuxiTranyu is
able to learn consistent multilingual representations across different
languages. To promote further research into multilingual LLMs and their working
mechanisms, we release both the base and instruction-tuned FuxiTranyu models
together with 58 pretraining checkpoints at HuggingFace and Github.","[{'name': 'Haoran Sun'}, {'name': 'Renren Jin'}, {'name': 'Shaoyang Xu'}, {'name': 'Leiyu Pan'}, {'name': 'Supryadi'}, {'name': 'Menglong Cui'}, {'name': 'Jiangcun Du'}, {'name': 'Yikun Lei'}, {'name': 'Lei Yang'}, {'name': 'Ling Shi'}, {'name': 'Juesi Xiao'}, {'name': 'Shaolin Zhu'}, {'name': 'Deyi Xiong'}]",2024-08-12T16:34:56Z
http://arxiv.org/abs/2408.06266v1,http://arxiv.org/abs/2408.06266v1,"Anchored Preference Optimization and Contrastive Revisions: Addressing
  Underspecification in Alignment","Large Language Models (LLMs) are often aligned using contrastive alignment
objectives and preference pair datasets. The interaction between model, paired
data, and objective makes alignment a complicated procedure, sometimes
producing subpar results. We study this and find that (i) preference data gives
a better learning signal when the underlying responses are contrastive, and
(ii) alignment objectives lead to better performance when they specify more
control over the model during training. Based on these insights, we introduce
Contrastive Learning from AI Revisions (CLAIR), a data-creation method which
leads to more contrastive preference pairs, and Anchored Preference
Optimization (APO), a controllable and more stable alignment objective. We
align Llama-3-8B-Instruct using various comparable datasets and alignment
objectives and measure MixEval-Hard scores, which correlate highly with human
judgments. The CLAIR preferences lead to the strongest performance out of all
datasets, and APO consistently outperforms less controllable objectives. Our
best model, trained on 32K CLAIR preferences with APO, improves
Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code
is available at https://github.com/ContextualAI/CLAIR_and_APO.","[{'name': ""Karel D'Oosterlinck""}, {'name': 'Winnie Xu'}, {'name': 'Chris Develder'}, {'name': 'Thomas Demeester'}, {'name': 'Amanpreet Singh'}, {'name': 'Christopher Potts'}, {'name': 'Douwe Kiela'}, {'name': 'Shikib Mehri'}]",2024-08-12T16:24:51Z
http://arxiv.org/abs/2408.06259v1,http://arxiv.org/abs/2408.06259v1,"Context-aware Visual Storytelling with Visual Prefix Tuning and
  Contrastive Learning","Visual storytelling systems generate multi-sentence stories from image
sequences. In this task, capturing contextual information and bridging visual
variation bring additional challenges. We propose a simple yet effective
framework that leverages the generalization capabilities of pretrained
foundation models, only training a lightweight vision-language mapping network
to connect modalities, while incorporating context to enhance coherence. We
introduce a multimodal contrastive objective that also improves visual
relevance and story informativeness. Extensive experimental results, across
both automatic metrics and human evaluations, demonstrate that the stories
generated by our framework are diverse, coherent, informative, and interesting.","[{'name': 'Yingjin Song'}, {'name': 'Denis Paperno'}, {'name': 'Albert Gatt'}]",2024-08-12T16:15:32Z
http://arxiv.org/abs/2408.06227v1,http://arxiv.org/abs/2408.06227v1,FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks,"This paper introduces FLEURS-R, a speech restoration applied version of the
Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS)
corpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages as
FLEURS, with improved audio quality and fidelity by applying the speech
restoration model Miipher. The aim of FLEURS-R is to advance speech technology
in more languages and catalyze research including text-to-speech (TTS) and
other speech generation tasks in low-resource languages. Comprehensive
evaluations with the restored speech and TTS baseline models trained from the
new corpus show that the new corpus obtained significantly improved speech
quality while maintaining the semantic contents of the speech. The corpus is
publicly released via Hugging Face.","[{'name': 'Min Ma'}, {'name': 'Yuma Koizumi'}, {'name': 'Shigeki Karita'}, {'name': 'Heiga Zen'}, {'name': 'Jason Riesa'}, {'name': 'Haruko Ishikawa'}, {'name': 'Michiel Bacchiani'}]",2024-08-12T15:28:51Z
http://arxiv.org/abs/2408.06223v1,http://arxiv.org/abs/2408.06223v1,"On Effects of Steering Latent Representation for Large Language Model
  Unlearning","Representation Misdirection for Unlearning (RMU), which steers model
representation in the intermediate layer to a target random representation, is
an effective method for large language model (LLM) unlearning. Despite its high
performance, the underlying cause and explanation remain underexplored. In this
paper, we first theoretically demonstrate that steering forget representations
in the intermediate layer reduces token confidence, causing LLMs to generate
wrong or nonsense responses. Second, we investigate how the coefficient
influences the alignment of forget-sample representations with the random
direction and hint at the optimal coefficient values for effective unlearning
across different network layers. Third, we show that RMU unlearned models are
robust against adversarial jailbreak attacks. Last, our empirical analysis
shows that RMU is less effective when applied to the middle and later layers in
LLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yet
effective alternative method that makes unlearning effective with most layers.
Extensive experiments demonstrate that Adaptive RMU significantly improves the
unlearning performance compared to prior art while incurring no additional
computational cost.","[{'name': 'Dang Huu-Tien'}, {'name': 'Trung-Tin Pham'}, {'name': 'Hoang Thanh-Tung'}, {'name': 'Naoya Inoue'}]",2024-08-12T15:24:50Z
http://arxiv.org/abs/2408.06195v1,http://arxiv.org/abs/2408.06195v1,Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers,"This paper introduces rStar, a self-play mutual reasoning approach that
significantly improves reasoning capabilities of small language models (SLMs)
without fine-tuning or superior models. rStar decouples reasoning into a
self-play mutual generation-discrimination process. First, a target SLM
augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like
reasoning actions to construct higher quality reasoning trajectories. Next,
another SLM, with capabilities similar to the target SLM, acts as a
discriminator to verify each trajectory generated by the target SLM. The
mutually agreed reasoning trajectories are considered mutual consistent, thus
are more likely to be correct. Extensive experiments across five SLMs
demonstrate rStar can effectively solve diverse reasoning problems, including
GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K
accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for
Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be
available at https://github.com/zhentingqi/rStar.","[{'name': 'Zhenting Qi'}, {'name': 'Mingyuan Ma'}, {'name': 'Jiahang Xu'}, {'name': 'Li Lyna Zhang'}, {'name': 'Fan Yang'}, {'name': 'Mao Yang'}]",2024-08-12T14:42:13Z
http://arxiv.org/abs/2408.06186v1,http://arxiv.org/abs/2408.06186v1,"Improving Structural Diversity of Blackbox LLMs via
  Chain-of-Specification Prompting","The capability to generate diverse text is a key challenge facing large
language models (LLMs). Thus far, diversity has been studied via metrics such
as $n$-gram diversity or diversity of BERT embeddings. However, for these kinds
of diversity, the user has little control over the dimensions along which
diversity is considered. For example, in the poetry domain, one might desire
diversity in terms of rhyme and meter, whereas in the code domain, one might
desire diversity in terms of the kinds of expressions used to solve a problem.
We propose a diversity metric called structural diversity, where the user
provides a mapping from generated text to features capturing the kinds of
diversity that they care about. In addition, we propose a novel strategy called
chain-of-specification (CoS) prompting for improving diversity by first having
the LLM generate a specification encoding one instance of structural features,
and then prompting the LLM to generate text that satisfies these features;
notably, our strategy works with blackbox LLMs. In our experiments, we show
that for structural diversity in the poetry and code domains, CoS significantly
improves diversity compared to several baselines.","[{'name': 'Halley Young'}, {'name': 'Yimeng Zeng'}, {'name': 'Jacob Gardner'}, {'name': 'Osbert Bastani'}]",2024-08-12T14:34:06Z
http://arxiv.org/abs/2408.06150v2,http://arxiv.org/abs/2408.06150v2,"LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid
  Library","In this study, we generate and maintain a database of 10 million virtual
lipids through METiS's in-house de novo lipid generation algorithms and lipid
virtual screening techniques. These virtual lipids serve as a corpus for
pre-training, lipid representation learning, and downstream task knowledge
transfer, culminating in state-of-the-art LNP property prediction performance.
We propose LipidBERT, a BERT-like model pre-trained with the Masked Language
Model (MLM) and various secondary tasks. Additionally, we compare the
performance of embeddings generated by LipidBERT and PhatGPT, our GPT-like
lipid generation model, on downstream tasks. The proposed bilingual LipidBERT
model operates in two languages: the language of ionizable lipid pre-training,
using in-house dry-lab lipid structures, and the language of LNP fine-tuning,
utilizing in-house LNP wet-lab data. This dual capability positions LipidBERT
as a key AI-based filter for future screening tasks, including new versions of
METiS de novo lipid libraries and, more importantly, candidates for in vivo
testing for orgran-targeting LNPs. To the best of our knowledge, this is the
first successful demonstration of the capability of a pre-trained language
model on virtual lipids and its effectiveness in downstream tasks using web-lab
data. This work showcases the clever utilization of METiS's in-house de novo
lipid library as well as the power of dry-wet lab integration.","[{'name': 'Tianhao Yu'}, {'name': 'Cai Yao'}, {'name': 'Zhuorui Sun'}, {'name': 'Feng Shi'}, {'name': 'Lin Zhang'}, {'name': 'Kangjie Lyu'}, {'name': 'Xuan Bai'}, {'name': 'Andong Liu'}, {'name': 'Xicheng Zhang'}, {'name': 'Jiali Zou'}, {'name': 'Wenshou Wang'}, {'name': 'Chris Lai'}, {'name': 'Kai Wang'}]",2024-08-12T13:44:24Z
http://arxiv.org/abs/2408.06142v1,http://arxiv.org/abs/2408.06142v1,Med42-v2: A Suite of Clinical LLMs,"Med42-v2 introduces a suite of clinical large language models (LLMs) designed
to address the limitations of generic models in healthcare settings. These
models are built on Llama3 architecture and fine-tuned using specialized
clinical data. They underwent multi-stage preference alignment to effectively
respond to natural prompts. While generic models are often preference-aligned
to avoid answering clinical queries as a precaution, Med42-v2 is specifically
trained to overcome this limitation, enabling its use in clinical settings.
Med42-v2 models demonstrate superior performance compared to the original
Llama3 models in both 8B and 70B parameter configurations and GPT-4 across
various medical benchmarks. These LLMs are developed to understand clinical
queries, perform reasoning tasks, and provide valuable assistance in clinical
environments. The models are now publicly available at
\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.","[{'name': 'Clément Christophe'}, {'name': 'Praveen K Kanithi'}, {'name': 'Tathagata Raha'}, {'name': 'Shadab Khan'}, {'name': 'Marco AF Pimentel'}]",2024-08-12T13:37:31Z
http://arxiv.org/abs/2408.07277v1,http://arxiv.org/abs/2408.07277v1,"Speech vs. Transcript: Does It Matter for Human Annotators in Speech
  Summarization?","Reference summaries for abstractive speech summarization require human
annotation, which can be performed by listening to an audio recording or by
reading textual transcripts of the recording. In this paper, we examine whether
summaries based on annotators listening to the recordings differ from those
based on annotators reading transcripts. Using existing intrinsic evaluation
based on human evaluation, automatic metrics, LLM-based evaluation, and a
retrieval-based reference-free method. We find that summaries are indeed
different based on the source modality, and that speech-based summaries are
more factually consistent and information-selective than transcript-based
summaries. Meanwhile, transcript-based summaries are impacted by recognition
errors in the source, and expert-written summaries are more informative and
reliable. We make all the collected data and analysis code
public(https://github.com/cmu-mlsp/interview_humanssum) to facilitate the
reproduction of our work and advance research in this area.","[{'name': 'Roshan Sharma'}, {'name': 'Suwon Shon'}, {'name': 'Mark Lindsey'}, {'name': 'Hira Dhamyal'}, {'name': 'Rita Singh'}, {'name': 'Bhiksha Raj'}]",2024-08-12T13:25:53Z
http://arxiv.org/abs/2408.06124v1,http://arxiv.org/abs/2408.06124v1,Utilize Transformers for translating Wikipedia category names,"On Wikipedia, articles are categorized to aid readers in navigating content
efficiently. The manual creation of new categories can be laborious and
time-intensive. To tackle this issue, we built language models to translate
Wikipedia categories from English to Vietnamese with a dataset containing
15,000 English-Vietnamese category pairs. Subsequently, small to medium-scale
Transformer pre-trained models with a sequence-to-sequence architecture were
fine-tuned for category translation. The experiments revealed that
OPUS-MT-en-vi surpassed other models, attaining the highest performance with a
BLEU score of 0.73, despite its smaller model storage. We expect our paper to
be an alternative solution for translation tasks with limited computer
resources.","[{'name': 'Hoang-Thang Ta'}, {'name': 'Quoc Thang La'}]",2024-08-12T13:07:34Z
http://arxiv.org/abs/2408.06120v1,http://arxiv.org/abs/2408.06120v1,"How ChatGPT Changed the Media's Narratives on AI: A Semi-Automated
  Narrative Analysis Through Frame Semantics","The recent explosion of attention to AI is arguably one of the biggest in the
technology's media coverage. To investigate the effects it has on the
discourse, we perform a mixed-method frame semantics-based analysis on a
dataset of more than 49,000 sentences collected from 5846 news articles that
mention AI. The dataset covers the twelve-month period centred around the
launch of OpenAI's chatbot ChatGPT and is collected from the most visited
open-access English-language news publishers. Our findings indicate that during
the half year succeeding the launch, media attention rose
tenfold$\unicode{x2014}$from already historically high levels. During this
period, discourse has become increasingly centred around experts and political
leaders, and AI has become more closely associated with dangers and risks. A
deeper review of the data also suggests a qualitative shift in the types of
threat AI is thought to represent, as well as the anthropomorphic qualities
ascribed to it.","[{'name': 'Igor Ryazanov'}, {'name': 'Carl Öhman'}, {'name': 'Johanna Björklund'}]",2024-08-12T13:02:31Z
http://arxiv.org/abs/2408.06087v1,http://arxiv.org/abs/2408.06087v1,Building Decision Making Models Through Language Model Regime,"We propose a novel approach for decision making problems leveraging the
generalization capabilities of large language models (LLMs). Traditional
methods such as expert systems, planning algorithms, and reinforcement learning
often exhibit limited generalization, typically requiring the training of new
models for each unique task. In contrast, LLMs demonstrate remarkable success
in generalizing across varied language tasks, inspiring a new strategy for
training decision making models. Our approach, referred to as ""Learning then
Using"" (LTU), entails a two-stage process. Initially, the \textit{learning}
phase develops a robust foundational decision making model by integrating
diverse knowledge from various domains and decision making contexts. The
subsequent \textit{using} phase refines this foundation model for specific
decision making scenarios. Distinct from other studies that employ LLMs for
decision making through supervised learning, our LTU method embraces a
versatile training methodology that combines broad pre-training with targeted
fine-tuning. Experiments in e-commerce domains such as advertising and search
optimization have shown that LTU approach outperforms traditional supervised
learning regimes in decision making capabilities and generalization. The LTU
approach is the first practical training architecture for both single-step and
multi-step decision making tasks combined with LLMs, which can be applied
beyond game and robot domains. It provides a robust and adaptable framework for
decision making, enhances the effectiveness and flexibility of various systems
in tackling various challenges.","[{'name': 'Yu Zhang'}, {'name': 'Haoxiang Liu'}, {'name': 'Feijun Jiang'}, {'name': 'Weihua Luo'}, {'name': 'Kaifu Zhang'}]",2024-08-12T12:04:14Z
http://arxiv.org/abs/2408.06065v1,http://arxiv.org/abs/2408.06065v1,An Investigation Into Explainable Audio Hate Speech Detection,"Research on hate speech has predominantly revolved around detection and
interpretation from textual inputs, leaving verbal content largely unexplored.
While there has been limited exploration into hate speech detection within
verbal acoustic speech inputs, the aspect of interpretability has been
overlooked. Therefore, we introduce a new task of explainable audio hate speech
detection. Specifically, we aim to identify the precise time intervals,
referred to as audio frame-level rationales, which serve as evidence for hate
speech classification. Towards this end, we propose two different approaches:
cascading and End-to-End (E2E). The cascading approach initially converts audio
to transcripts, identifies hate speech within these transcripts, and
subsequently locates the corresponding audio time frames. Conversely, the E2E
approach processes audio utterances directly, which allows it to pinpoint hate
speech within specific time frames. Additionally, due to the lack of
explainable audio hate speech datasets that include audio frame-level
rationales, we curated a synthetic audio dataset to train our models. We
further validated these models on actual human speech utterances and found that
the E2E approach outperforms the cascading method in terms of the audio frame
Intersection over Union (IoU) metric. Furthermore, we observed that including
frame-level rationales significantly enhances hate speech detection accuracy
for the E2E approach.
  \textbf{Disclaimer} The reader may encounter content of an offensive or
hateful nature. However, given the nature of the work, this cannot be avoided.","[{'name': 'Jinmyeong An'}, {'name': 'Wonjun Lee'}, {'name': 'Yejin Jeon'}, {'name': 'Jungseul Ok'}, {'name': 'Yunsu Kim'}, {'name': 'Gary Geunbae Lee'}]",2024-08-12T11:32:34Z
http://arxiv.org/abs/2408.06062v2,http://arxiv.org/abs/2408.06062v2,"On Tables with Numbers, with Numbers","This paper is a critical reflection on the epistemic culture of contemporary
computational linguistics, framed in the context of its growing obsession with
tables with numbers. We argue against tables with numbers on the basis of their
epistemic irrelevance, their environmental impact, their role in enabling and
exacerbating social inequalities, and their deep ties to commercial
applications and profit-driven research. We substantiate our arguments with
empirical evidence drawn from a meta-analysis of computational linguistics
research over the last decade.","[{'name': 'Konstantinos Kogkalidis'}, {'name': 'Stergios Chatzikyriakidis'}]",2024-08-12T11:23:24Z
http://arxiv.org/abs/2408.06061v1,http://arxiv.org/abs/2408.06061v1,Quantum Algorithms for Compositional Text Processing,"Quantum computing and AI have found a fruitful intersection in the field of
natural language processing. We focus on the recently proposed DisCoCirc
framework for natural language, and propose a quantum adaptation, QDisCoCirc.
This is motivated by a compositional approach to rendering AI interpretable:
the behavior of the whole can be understood in terms of the behavior of parts,
and the way they are put together. For the model-native primitive operation of
text similarity, we derive quantum algorithms for fault-tolerant quantum
computers to solve the task of question-answering within QDisCoCirc, and show
that this is BQP-hard; note that we do not consider the complexity of
question-answering in other natural language processing models. Assuming
widely-held conjectures, implementing the proposed model classically would
require super-polynomial resources. Therefore, it could provide a meaningful
demonstration of the power of practical quantum processors. The model
construction builds on previous work in compositional quantum natural language
processing. Word embeddings are encoded as parameterized quantum circuits, and
compositionality here means that the quantum circuits compose according to the
linguistic structure of the text. We outline a method for evaluating the model
on near-term quantum processors, and elsewhere we report on a recent
implementation of this on quantum hardware. In addition, we adapt a quantum
algorithm for the closest vector problem to obtain a Grover-like speedup in the
fault-tolerant regime for our model. This provides an unconditional quadratic
speedup over any classical algorithm in certain circumstances, which we will
verify empirically in future work.","[{'name': 'Tuomas Laakkonen'}, {'name': 'Konstantinos Meichanetzidis'}, {'name': 'Bob Coecke'}]",2024-08-12T11:21:40Z
http://arxiv.org/abs/2408.06044v1,http://arxiv.org/abs/2408.06044v1,"DiagESC: Dialogue Synthesis for Integrating Depression Diagnosis into
  Emotional Support Conversation","Dialogue systems for mental health care aim to provide appropriate support to
individuals experiencing mental distress. While extensive research has been
conducted to deliver adequate emotional support, existing studies cannot
identify individuals who require professional medical intervention and cannot
offer suitable guidance. We introduce the Diagnostic Emotional Support
Conversation task for an advanced mental health management system. We develop
the DESC dataset to assess depression symptoms while maintaining user
experience by utilizing task-specific utterance generation prompts and a strict
filtering algorithm. Evaluations by professional psychological counselors
indicate that DESC has a superior ability to diagnose depression than existing
data. Additionally, conversational quality evaluation reveals that DESC
maintains fluent, consistent, and coherent dialogues.","[{'name': 'Seungyeon Seo'}, {'name': 'Gary Geunbae Lee'}]",2024-08-12T10:26:39Z
http://arxiv.org/abs/2408.06043v1,http://arxiv.org/abs/2408.06043v1,"Enhancing Dialogue Speech Recognition with Robust Contextual Awareness
  via Noise Representation Learning","Recent dialogue systems rely on turn-based spoken interactions, requiring
accurate Automatic Speech Recognition (ASR). Errors in ASR can significantly
impact downstream dialogue tasks. To address this, using dialogue context from
user and agent interactions for transcribing subsequent utterances has been
proposed. This method incorporates the transcription of the user's speech and
the agent's response as model input, using the accumulated context generated by
each turn. However, this context is susceptible to ASR errors because it is
generated by the ASR model in an auto-regressive fashion. Such noisy context
can further degrade the benefits of context input, resulting in suboptimal ASR
performance. In this paper, we introduce Context Noise Representation Learning
(CNRL) to enhance robustness against noisy context, ultimately improving
dialogue speech recognition accuracy. To maximize the advantage of context
awareness, our approach includes decoder pre-training using text-based dialogue
data and noise representation learning for a context encoder. Based on the
evaluation of speech dialogues, our method shows superior results compared to
baselines. Furthermore, the strength of our approach is highlighted in noisy
environments where user speech is barely audible due to real-world noise,
relying on contextual information to transcribe the input accurately.","[{'name': 'Wonjun Lee'}, {'name': 'San Kim'}, {'name': 'Gary Geunbae Lee'}]",2024-08-12T10:21:09Z
http://arxiv.org/abs/2408.06040v1,http://arxiv.org/abs/2408.06040v1,"ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation
  Using Large Language Models and Transformers","In the rapidly evolving fields of natural language processing and computer
vision, Visual Word Sense Disambiguation (VWSD) stands as a critical, yet
challenging task. The quest for models that can seamlessly integrate and
interpret multimodal data is more pressing than ever. Imagine a system that can
understand language with the depth and nuance of human cognition, while
simultaneously interpreting the rich visual context of the world around it.
  We present ARPA, an architecture that fuses the unparalleled contextual
understanding of large language models with the advanced feature extraction
capabilities of transformers, which then pass through a custom Graph Neural
Network (GNN) layer to learn intricate relationships and subtle nuances within
the data. This innovative architecture not only sets a new benchmark in visual
word disambiguation but also introduces a versatile framework poised to
transform how linguistic and visual data interact by harnessing the synergistic
strengths of its components, ensuring robust performance even in the most
complex disambiguation scenarios. Through a series of experiments and
comparative analysis, we reveal the substantial advantages of our model,
underscoring its potential to redefine standards in the field. Beyond its
architectural prowess, our architecture excels through experimental
enrichments, including sophisticated data augmentation and multi-modal training
techniques.
  ARPA's introduction marks a significant milestone in visual word
disambiguation, offering a compelling solution that bridges the gap between
linguistic and visual modalities. We invite researchers and practitioners to
explore the capabilities of our model, envisioning a future where such hybrid
models drive unprecedented advancements in artificial intelligence.","[{'name': 'Aristi Papastavrou'}, {'name': 'Maria Lymperaiou'}, {'name': 'Giorgos Stamou'}]",2024-08-12T10:15:13Z
http://arxiv.org/abs/2408.06022v1,http://arxiv.org/abs/2408.06022v1,"Controlling Surprisal in Music Generation via Information Content Curve
  Matching","In recent years, the quality and public interest in music generation systems
have grown, encouraging research into various ways to control these systems. We
propose a novel method for controlling surprisal in music generation using
sequence models. To achieve this goal, we define a metric called Instantaneous
Information Content (IIC). The IIC serves as a proxy function for the perceived
musical surprisal (as estimated from a probabilistic model) and can be
calculated at any point within a music piece. This enables the comparison of
surprisal across different musical content even if the musical events occur in
irregular time intervals. We use beam search to generate musical material whose
IIC curve closely approximates a given target IIC. We experimentally show that
the IIC correlates with harmonic and rhythmic complexity and note density. The
correlation decreases with the length of the musical context used for
estimating the IIC. Finally, we conduct a qualitative user study to test if
human listeners can identify the IIC curves that have been used as targets when
generating the respective musical material. We provide code for creating IIC
interpolations and IIC visualizations on https://github.com/muthissar/iic.","[{'name': 'Mathias Rose Bjare'}, {'name': 'Stefan Lattner'}, {'name': 'Gerhard Widmer'}]",2024-08-12T09:21:41Z
http://arxiv.org/abs/2408.05976v1,http://arxiv.org/abs/2408.05976v1,Global-to-Local Support Spectrums for Language Model Explainability,"Existing sample-based methods, like influence functions and representer
points, measure the importance of a training point by approximating the effect
of its removal from training. As such, they are skewed towards outliers and
points that are very close to the decision boundaries. The explanations
provided by these methods are often static and not specific enough for
different test points. In this paper, we propose a method to generate an
explanation in the form of support spectrums which are based on two main ideas:
the support sets and a global-to-local importance measure. The support set is
the set of training points, in the predicted class, that ``lie in between'' the
test point and training points in the other classes. They indicate how well the
test point can be distinguished from the points not in the predicted class. The
global-to-local importance measure is obtained by decoupling existing methods
into the global and local components which are then used to select the points
in the support set. Using this method, we are able to generate explanations
that are tailored to specific test points. In the experiments, we show the
effectiveness of the method in image classification and text generation tasks.","[{'name': 'Lucas Agussurja'}, {'name': 'Xinyang Lu'}, {'name': 'Bryan Kian Hsiang Low'}]",2024-08-12T08:05:30Z
http://arxiv.org/abs/2408.05977v1,http://arxiv.org/abs/2408.05977v1,"The Language of Trauma: Modeling Traumatic Event Descriptions Across
  Domains with Explainable AI","Psychological trauma can manifest following various distressing events and is
captured in diverse online contexts. However, studies traditionally focus on a
single aspect of trauma, often neglecting the transferability of findings
across different scenarios. We address this gap by training language models
with progressing complexity on trauma-related datasets, including
genocide-related court data, a Reddit dataset on post-traumatic stress disorder
(PTSD), counseling conversations, and Incel forum posts. Our results show that
the fine-tuned RoBERTa model excels in predicting traumatic events across
domains, slightly outperforming large language models like GPT-4. Additionally,
SLALOM-feature scores and conceptual explanations effectively differentiate and
cluster trauma-related language, highlighting different trauma aspects and
identifying sexual abuse and experiences related to death as a common traumatic
event across all datasets. This transferability is crucial as it allows for the
development of tools to enhance trauma detection and intervention in diverse
populations and settings.","[{'name': 'Miriam Schirmer'}, {'name': 'Tobias Leemann'}, {'name': 'Gjergji Kasneci'}, {'name': 'Jürgen Pfeffer'}, {'name': 'David Jurgens'}]",2024-08-12T08:05:30Z
http://arxiv.org/abs/2408.05948v1,http://arxiv.org/abs/2408.05948v1,"ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge
  Graph QA datasets with Large Language Models","The rapid advancement of Large Language Models (LLMs) and conversational
assistants necessitates dynamic, scalable, and configurable conversational
datasets for training and evaluation. These datasets must accommodate diverse
user interaction modes, including text and voice, each presenting unique
modeling challenges. Knowledge Graphs (KGs), with their structured and evolving
nature, offer an ideal foundation for current and precise knowledge. Although
human-curated KG-based conversational datasets exist, they struggle to keep
pace with the rapidly changing user information needs. We present ConvKGYarn, a
scalable method for generating up-to-date and configurable conversational KGQA
datasets. Qualitative psychometric analyses confirm our method can generate
high-quality datasets rivaling a popular conversational KGQA dataset while
offering it at scale and covering a wide range of human-interaction
configurations. We showcase its utility by testing LLMs on diverse
conversations - exploring model behavior on conversational KGQA sets with
different configurations grounded in the same KG fact set. Our results
highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate
parametric knowledge of LLMs, thus offering a robust solution to the constantly
evolving landscape of conversational assistants.","[{'name': 'Ronak Pradeep'}, {'name': 'Daniel Lee'}, {'name': 'Ali Mousavi'}, {'name': 'Jeff Pound'}, {'name': 'Yisi Sang'}, {'name': 'Jimmy Lin'}, {'name': 'Ihab Ilyas'}, {'name': 'Saloni Potdar'}, {'name': 'Mostafa Arefiyan'}, {'name': 'Yunyao Li'}]",2024-08-12T06:48:43Z
http://arxiv.org/abs/2408.05911v1,http://arxiv.org/abs/2408.05911v1,"A New Pipeline For Generating Instruction Dataset via RAG and Self
  Fine-Tuning","With the rapid development of large language models in recent years, there
has been an increasing demand for domain-specific Agents that can cater to the
unique needs of enterprises and organizations. Unlike general models, which
strive for broad coverage, these specialized Agents rely on focused datasets
tailored to their intended applications. This research proposes a pipeline that
leverages the power of LLMs and the Retrieval-Augmented Generation related
framework to construct high-quality instruction datasets for fine-tuning on
specific domains using custom document collections. By ingesting
domain-specific documents, the pipeline generates relevant and contextually
appropriate instructions, thus effectively creating a comprehensive dataset for
fine-tuning LLMs on the target domain. This approach overcomes the limitations
of traditional dataset creation methods, which often rely on manual curation or
web-scraping techniques that may introduce noise and irrelevant data. Notably,
our pipeline offers a dynamic solution that can quickly adapt to updates or
modifications in the domain-specific document collection, eliminating the need
for complete retraining. Additionally, it addresses the challenge of data
scarcity by enabling the generation of instruction datasets from a limited set
of initial documents, rendering it suitable for unpopular or specialized
domains where comprehensive datasets are scarce. As a case study, we apply this
approach to the domain of psychiatry, a field requiring specialized knowledge
and sensitive handling of patient information. The resulting fine-tuned LLM
demonstrates showcases the viability of the proposed approach and underscores
its potential for widespread adoption across various industries and domains
where tailored, accurate, and contextually relevant language models are
indispensable.","[{'name': 'Chih-Wei Song'}, {'name': 'Yu-Kai Lee'}, {'name': 'Yin-Te Tsai'}]",2024-08-12T03:52:11Z
http://arxiv.org/abs/2408.05906v1,http://arxiv.org/abs/2408.05906v1,"AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine
  Advertising","With the increase in the more fluent ad texts automatically created by
natural language generation technology, it is in the high demand to verify the
quality of these creatives in a real-world setting. We propose AdTEC, the first
public benchmark to evaluate ad texts in multiple aspects from the perspective
of practical advertising operations. Our contributions are: (i) Defining five
tasks for evaluating the quality of ad texts and building a dataset based on
the actual operational experience of advertising agencies, which is typically
kept in-house. (ii) Validating the performance of existing pre-trained language
models (PLMs) and human evaluators on the dataset. (iii) Analyzing the
characteristics and providing challenges of the benchmark. The results show
that while PLMs have already reached the practical usage level in several
tasks, human still outperforms in certain domains, implying that there is
significant room for improvement in such area.","[{'name': 'Peinan Zhang'}, {'name': 'Yusuke Sakai'}, {'name': 'Masato Mita'}, {'name': 'Hiroki Ouchi'}, {'name': 'Taro Watanabe'}]",2024-08-12T03:32:53Z
http://arxiv.org/abs/2408.05894v1,http://arxiv.org/abs/2408.05894v1,GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models,"Vision-Language Models (VLMs) building upon the foundation of powerful large
language models have made rapid progress in reasoning across visual and textual
data. While VLMs perform well on vision tasks that they are trained on, our
results highlight key challenges in abstract pattern recognition. We present
GlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of
visual patterns from 40 writing systems with three visual presentation styles.
  GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models
to understand and judge natural language descriptions of visual patterns.
GlyphPattern patterns are drawn from a large-scale cognitive science
investigation of human writing systems; as a result, they are rich in spatial
reference and compositionality. Our experiments show that GlyphPattern is
challenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with
marginal gains from few-shot prompting. Our detailed error analysis reveals
challenges at multiple levels, including visual processing, natural language
understanding, and pattern generalization.","[{'name': 'Zixuan Wu'}, {'name': 'Yoolim Kim'}, {'name': 'Carolyn Jane Anderson'}]",2024-08-12T02:16:47Z
http://arxiv.org/abs/2408.05882v1,http://arxiv.org/abs/2408.05882v1,Creating Arabic LLM Prompts at Scale,"The debut of chatGPT and BARD has popularized instruction following text
generation using LLMs, where a user can interrogate an LLM using natural
language requests and obtain natural language answers that matches their
requests. Training LLMs to respond in this manner requires a large number of
worked out examples of user requests (aka prompts) with corresponding gold
responses. In this paper, we introduce two methods for creating such prompts
for Arabic cheaply and quickly. The first methods entails automatically
translating existing prompt datasets from English, such as PromptSource and
Super-NaturalInstructions, and then using machine translation quality
estimation to retain high quality translations only. The second method involves
creating natural language prompts on top of existing Arabic NLP datasets. Using
these two methods we were able to create more than 67.4 million Arabic prompts
that cover a variety of tasks including summarization, headline generation,
grammar checking, open/closed question answering, creative writing, etc. We
show that fine tuning an open 7 billion parameter large language model, namely
base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter
instruction tuned model, namely Llama3 70B, in handling Arabic prompts.","[{'name': 'Abdelrahman El-Sheikh'}, {'name': 'Ahmed Elmogtaba'}, {'name': 'Kareem Darwish'}, {'name': 'Muhammad Elmallah'}, {'name': 'Ashraf Elneima'}, {'name': 'Hassan Sawaf'}]",2024-08-12T00:46:39Z
http://arxiv.org/abs/2408.05874v1,http://arxiv.org/abs/2408.05874v1,LLM-Based Robust Product Classification in Commerce and Compliance,"Product classification is a crucial task in international trade, as
compliance regulations are verified and taxes and duties are applied based on
product categories. Manual classification of products is time-consuming and
error-prone, and the sheer volume of products imported and exported renders the
manual process infeasible. Consequently, e-commerce platforms and enterprises
involved in international trade have turned to automatic product classification
using machine learning. However, current approaches do not consider the
real-world challenges associated with product classification, such as very
abbreviated and incomplete product descriptions. In addition, recent
advancements in generative Large Language Models (LLMs) and their reasoning
capabilities are mainly untapped in product classification and e-commerce. In
this research, we explore the real-life challenges of industrial classification
and we propose data perturbations that allow for realistic data simulation.
Furthermore, we employ LLM-based product classification to improve the
robustness of the prediction in presence of incomplete data. Our research shows
that LLMs with in-context learning outperform the supervised approaches in the
clean-data scenario. Additionally, we illustrate that LLMs are significantly
more robust than the supervised approaches when data attacks are present.","[{'name': 'Sina Gholamian'}, {'name': 'Gianfranco Romani'}, {'name': 'Bartosz Rudnikowicz'}, {'name': 'Laura Skylaki'}]",2024-08-11T22:59:32Z
http://arxiv.org/abs/2408.05873v1,http://arxiv.org/abs/2408.05873v1,"Defining Boundaries: A Spectrum of Task Feasibility for Large Language
  Models","Large language models (LLMs) have shown remarkable performance in various
tasks but often fail to handle queries that exceed their knowledge and
capabilities, leading to incorrect or fabricated responses. This paper
addresses the need for LLMs to recognize and refuse infeasible tasks due to the
required skills surpassing their capabilities. We first systematically
conceptualize infeasible tasks for LLMs, providing formal definitions and
categorizations that cover a spectrum of related hallucinations. We develop and
benchmark a new dataset comprising diverse infeasible and feasible tasks to
test multiple LLMs' abilities on task feasibility. Furthermore, we explore the
potential of training enhancements to increase LLMs' refusal capabilities with
fine-tuning. Experiments validate the effectiveness of our methods, offering
promising directions for refining the operational boundaries of LLMs in real
applications.","[{'name': 'Wenbo Zhang'}, {'name': 'Zihang Xu'}, {'name': 'Hengrui Cai'}]",2024-08-11T22:58:23Z
http://arxiv.org/abs/2408.08899v1,http://arxiv.org/abs/2408.08899v1,"Kov: Transferable and Naturalistic Black-Box LLM Attacks using Markov
  Decision Processes and Tree Search","Eliciting harmful behavior from large language models (LLMs) is an important
task to ensure the proper alignment and safety of the models. Often when
training LLMs, ethical guidelines are followed yet alignment failures may still
be uncovered through red teaming adversarial attacks. This work frames the
red-teaming problem as a Markov decision process (MDP) and uses Monte Carlo
tree search to find harmful behaviors of black-box, closed-source LLMs. We
optimize token-level prompt suffixes towards targeted harmful behaviors on
white-box LLMs and include a naturalistic loss term, log-perplexity, to
generate more natural language attacks for better interpretability. The
proposed algorithm, Kov, trains on white-box LLMs to optimize the adversarial
attacks and periodically evaluates responses from the black-box LLM to guide
the search towards more harmful black-box behaviors. In our preliminary study,
results indicate that we can jailbreak black-box models, such as GPT-3.5, in
only 10 queries, yet fail on GPT-4$-$which may indicate that newer models are
more robust to token-level attacks. All work to reproduce these results is open
sourced (https://github.com/sisl/Kov.jl).",[{'name': 'Robert J. Moss'}],2024-08-11T20:31:52Z
http://arxiv.org/abs/2408.07092v2,http://arxiv.org/abs/2408.07092v2,Post-Training Sparse Attention with Double Sparsity,"The inference process for large language models is slow and memory-intensive,
with one of the most critical bottlenecks being excessive Key-Value (KV) cache
accesses. This paper introduces ""Double Sparsity,"" a novel post-training sparse
attention technique designed to alleviate this bottleneck by reducing KV cache
access. Double Sparsity combines token sparsity, which focuses on utilizing
only the important tokens for computing self-attention, with channel sparsity,
an approach that uses important feature channels for identifying important
tokens. Our key insight is that the pattern of channel sparsity is relatively
static, allowing us to use offline calibration to make it efficient at runtime,
thereby enabling accurate and efficient identification of important tokens.
Moreover, this method can be combined with offloading to achieve significant
memory usage reduction. Experimental results demonstrate that Double Sparsity
can achieve $\frac{1}{16}$ token and channel sparsity with minimal impact on
accuracy across various tasks, including wiki-2 perplexity, key-value
retrieval, and long context benchmarks with models including Llama-2-7B,
Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\times$ acceleration in
attention operations and a 1.9$\times$ improvement in end-to-end inference on
GPUs. With offloading, it achieves a decoding speed acceleration of
16.3$\times$ compared to state-of-the-art solutions at a sequence length of
256K. Our code is publicly available at
https://github.com/andy-yang-1/DoubleSparse.","[{'name': 'Shuo Yang'}, {'name': 'Ying Sheng'}, {'name': 'Joseph E. Gonzalez'}, {'name': 'Ion Stoica'}, {'name': 'Lianmin Zheng'}]",2024-08-11T18:40:36Z
http://arxiv.org/abs/2408.05840v2,http://arxiv.org/abs/2408.05840v2,Iterative Improvement of an Additively Regularized Topic Model,"Topic modelling is fundamentally a soft clustering problem (of known objects
-- documents, over unknown clusters -- topics). That is, the task is
incorrectly posed. In particular, the topic models are unstable and incomplete.
All this leads to the fact that the process of finding a good topic model
(repeated hyperparameter selection, model training, and topic quality
assessment) can be particularly long and labor-intensive. We aim to simplify
the process, to make it more deterministic and provable. To this end, we
present a method for iterative training of a topic model. The essence of the
method is that a series of related topic models are trained so that each
subsequent model is at least as good as the previous one, i.e., that it retains
all the good topics found earlier. The connection between the models is
achieved by additive regularization. The result of this iterative training is
the last topic model in the series, which we call the iteratively updated
additively regularized topic model (ITAR). Experiments conducted on several
collections of natural language texts show that the proposed ITAR model
performs better than other popular topic models (LDA, ARTM, BERTopic), its
topics are diverse, and its perplexity (ability to ""explain"" the underlying
data) is moderate.","[{'name': 'Alex Gorbulev'}, {'name': 'Vasiliy Alekseev'}, {'name': 'Konstantin Vorontsov'}]",2024-08-11T18:22:12Z
http://arxiv.org/abs/2408.05794v1,http://arxiv.org/abs/2408.05794v1,"HateSieve: A Contrastive Learning Framework for Detecting and Segmenting
  Hateful Content in Multimodal Memes","Amidst the rise of Large Multimodal Models (LMMs) and their widespread
application in generating and interpreting complex content, the risk of
propagating biased and harmful memes remains significant. Current safety
measures often fail to detect subtly integrated hateful content within
``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new
framework designed to enhance the detection and segmentation of hateful
elements in memes. \textsc{HateSieve} features a novel Contrastive Meme
Generator that creates semantically paired memes, a customized triplet dataset
for contrastive learning, and an Image-Text Alignment module that produces
context-aware embeddings for accurate meme segmentation. Empirical experiments
on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses
existing LMMs in performance with fewer trainable parameters but also offers a
robust mechanism for precisely identifying and isolating hateful content.
\textcolor{red}{Caution: Contains academic discussions of hate speech; viewer
discretion advised.}","[{'name': 'Xuanyu Su'}, {'name': 'Yansong Li'}, {'name': 'Diana Inkpen'}, {'name': 'Nathalie Japkowicz'}]",2024-08-11T14:56:06Z
http://arxiv.org/abs/2408.05793v1,http://arxiv.org/abs/2408.05793v1,"SAGA: A Participant-specific Examination of Story Alternatives and Goal
  Applicability for a Deeper Understanding of Complex Events","Interpreting and assessing goal driven actions is vital to understanding and
reasoning over complex events. It is important to be able to acquire the
knowledge needed for this understanding, though doing so is challenging. We
argue that such knowledge can be elicited through a participant achievement
lens. We analyze a complex event in a narrative according to the intended
achievements of the participants in that narrative, the likely future actions
of the participants, and the likelihood of goal success. We collect 6.3K high
quality goal and action annotations reflecting our proposed participant
achievement lens, with an average weighted Fleiss-Kappa IAA of 80%. Our
collection contains annotated alternate versions of each narrative. These
alternate versions vary minimally from the ""original"" story, but can license
drastically different inferences. Our findings suggest that while modern large
language models can reflect some of the goal-based knowledge we study, they
find it challenging to fully capture the design and intent behind concerted
actions, even when the model pretraining included the data from which we
extracted the goal knowledge. We show that smaller models fine-tuned on our
dataset can achieve performance surpassing larger models.","[{'name': 'Sai Vallurupalli'}, {'name': 'Katrin Erk'}, {'name': 'Francis Ferraro'}]",2024-08-11T14:52:40Z
http://arxiv.org/abs/2408.05786v1,http://arxiv.org/abs/2408.05786v1,"HiLight: A Hierarchy-aware Light Global Model with Hierarchical Local
  ConTrastive Learning","Hierarchical text classification (HTC) is a special sub-task of multi-label
classification (MLC) whose taxonomy is constructed as a tree and each sample is
assigned with at least one path in the tree. Latest HTC models contain three
modules: a text encoder, a structure encoder and a multi-label classification
head. Specially, the structure encoder is designed to encode the hierarchy of
taxonomy. However, the structure encoder has scale problem. As the taxonomy
size increases, the learnable parameters of recent HTC works grow rapidly.
Recursive regularization is another widely-used method to introduce
hierarchical information but it has collapse problem and generally relaxed by
assigning with a small weight (ie. 1e-6). In this paper, we propose a
Hierarchy-aware Light Global model with Hierarchical local conTrastive learning
(HiLight), a lightweight and efficient global model only consisting of a text
encoder and a multi-label classification head. We propose a new learning task
to introduce the hierarchical information, called Hierarchical Local
Contrastive Learning (HiLCL). Extensive experiments are conducted on two
benchmark datasets to demonstrate the effectiveness of our model.","[{'name': 'Zhijian Chen'}, {'name': 'Zhonghua Li'}, {'name': 'Jianxin Yang'}, {'name': 'Ye Qi'}]",2024-08-11T14:26:58Z
http://arxiv.org/abs/2408.05769v1,http://arxiv.org/abs/2408.05769v1,"LI-TTA: Language Informed Test-Time Adaptation for Automatic Speech
  Recognition","Test-Time Adaptation (TTA) has emerged as a crucial solution to the domain
shift challenge, wherein the target environment diverges from the original
training environment. A prime exemplification is TTA for Automatic Speech
Recognition (ASR), which enhances model performance by leveraging output
prediction entropy minimization as a self-supervision signal. However, a key
limitation of this self-supervision lies in its primary focus on acoustic
features, with minimal attention to the linguistic properties of the input. To
address this gap, we propose Language Informed Test-Time Adaptation (LI-TTA),
which incorporates linguistic insights during TTA for ASR. LI-TTA integrates
corrections from an external language model to merge linguistic with acoustic
information by minimizing the CTC loss from the correction alongside the
standard TTA loss. With extensive experiments, we show that LI-TTA effectively
improves the performance of TTA for ASR in various distribution shift
situations.","[{'name': 'Eunseop Yoon'}, {'name': 'Hee Suk Yoon'}, {'name': 'John Harvill'}, {'name': 'Mark Hasegawa-Johnson'}, {'name': 'Chang D. Yoo'}]",2024-08-11T13:19:27Z
http://arxiv.org/abs/2408.05767v1,http://arxiv.org/abs/2408.05767v1,Reference-free Hallucination Detection for Large Vision-Language Models,"Large vision-language models (LVLMs) have made significant progress in recent
years. While LVLMs exhibit excellent ability in language understanding,
question answering, and conversations of visual inputs, they are prone to
producing hallucinations. While several methods are proposed to evaluate the
hallucinations in LVLMs, most are reference-based and depend on external tools,
which complicates their practical application. To assess the viability of
alternative methods, it is critical to understand whether the reference-free
approaches, which do not rely on any external tools, can efficiently detect
hallucinations. Therefore, we initiate an exploratory study to demonstrate the
effectiveness of different reference-free solutions in detecting hallucinations
in LVLMs. In particular, we conduct an extensive study on three kinds of
techniques: uncertainty-based, consistency-based, and supervised uncertainty
quantification methods on four representative LVLMs across two different tasks.
The empirical results show that the reference-free approaches are capable of
effectively detecting non-factual responses in LVLMs, with the supervised
uncertainty quantification method outperforming the others, achieving the best
performance across different settings.","[{'name': 'Qing Li'}, {'name': 'Chenyang Lyu'}, {'name': 'Jiahui Geng'}, {'name': 'Derui Zhu'}, {'name': 'Maxim Panov'}, {'name': 'Fakhri Karray'}]",2024-08-11T13:17:14Z
http://arxiv.org/abs/2408.05758v1,http://arxiv.org/abs/2408.05758v1,"VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for
  Speech Processing","Deep learning has brought significant improvements to the field of
cross-modal representation learning. For tasks such as text-to-speech (TTS),
voice conversion (VC), and automatic speech recognition (ASR), a cross-modal
fine-grained (frame-level) sequence representation is desired, emphasizing the
semantic content of the text modality while de-emphasizing the paralinguistic
information of the speech modality. We propose a method called ""Vector
Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)"", which uses the
cross-modal aligned sequence transcoder to bring text and speech into a joint
multimodal space, learning how to connect text and speech at the frame level.
The proposed VQ-CTAP is a paradigm for cross-modal sequence representation
learning, offering a promising solution for fine-grained generation and
recognition tasks in speech processing. The VQ-CTAP can be directly applied to
VC and ASR tasks without fine-tuning or additional structures. We propose a
sequence-aware semantic connector, which connects multiple frozen pre-trained
modules for the TTS task, exhibiting a plug-and-play capability. We design a
stepping optimization strategy to ensure effective model convergence by
gradually injecting and adjusting the influence of various loss components.
Furthermore, we propose a semantic-transfer-wise paralinguistic consistency
loss to enhance representational capabilities, allowing the model to better
generalize to unseen data and capture the nuances of paralinguistic
information. In addition, VQ-CTAP achieves high-compression speech coding at a
rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the
sampling rate. The audio demo is available at
https://qiangchunyu.github.io/VQCTAP/","[{'name': 'Chunyu Qiang'}, {'name': 'Wang Geng'}, {'name': 'Yi Zhao'}, {'name': 'Ruibo Fu'}, {'name': 'Tao Wang'}, {'name': 'Cheng Gong'}, {'name': 'Tianrui Wang'}, {'name': 'Qiuyu Liu'}, {'name': 'Jiangyan Yi'}, {'name': 'Zhengqi Wen'}, {'name': 'Chen Zhang'}, {'name': 'Hao Che'}, {'name': 'Longbiao Wang'}, {'name': 'Jianwu Dang'}, {'name': 'Jianhua Tao'}]",2024-08-11T12:24:23Z
http://arxiv.org/abs/2408.05738v1,http://arxiv.org/abs/2408.05738v1,"Language-Informed Beam Search Decoding for Multilingual Machine
  Translation","Beam search decoding is the de-facto method for decoding auto-regressive
Neural Machine Translation (NMT) models, including multilingual NMT where the
target language is specified as an input. However, decoding multilingual NMT
models commonly produces ``off-target'' translations -- yielding translation
outputs not in the intended language. In this paper, we first conduct an error
analysis of off-target translations for a strong multilingual NMT model and
identify how these decodings are produced during beam search. We then propose
Language-informed Beam Search (LiBS), a general decoding algorithm
incorporating an off-the-shelf Language Identification (LiD) model into beam
search decoding to reduce off-target translations. LiBS is an inference-time
procedure that is NMT-model agnostic and does not require any additional
parallel data. Results show that our proposed LiBS algorithm on average
improves +1.1 BLEU and +0.9 BLEU on WMT and OPUS datasets, and reduces
off-target rates from 22.9\% to 7.7\% and 65.8\% to 25.3\% respectively.","[{'name': 'Yilin Yang'}, {'name': 'Stefan Lee'}, {'name': 'Prasad Tadepalli'}]",2024-08-11T09:57:46Z
http://arxiv.org/abs/2408.05664v1,http://arxiv.org/abs/2408.05664v1,"Training an NLP Scholar at a Small Liberal Arts College: A Backwards
  Designed Course Proposal","The rapid growth in natural language processing (NLP) over the last couple
years has generated student interest and excitement in learning more about the
field. In this paper, we present two types of students that NLP courses might
want to train. First, an ""NLP engineer"" who is able to flexibly design, build
and apply new technologies in NLP for a wide range of tasks. Second, an ""NLP
scholar"" who is able to pose, refine and answer questions in NLP and how it
relates to the society, while also learning to effectively communicate these
answers to a broader audience. While these two types of skills are not mutually
exclusive -- NLP engineers should be able to think critically, and NLP scholars
should be able to build systems -- we think that courses can differ in the
balance of these skills. As educators at Small Liberal Arts Colleges, the
strengths of our students and our institution favors an approach that is better
suited to train NLP scholars. In this paper we articulate what kinds of skills
an NLP scholar should have, and then adopt a backwards design to propose course
components that can aid the acquisition of these skills.","[{'name': 'Grusha Prasad'}, {'name': 'Forrest Davis'}]",2024-08-11T00:50:59Z
http://arxiv.org/abs/2408.05655v1,http://arxiv.org/abs/2408.05655v1,"WiDe-analysis: Enabling One-click Content Moderation Analysis on
  Wikipedia's Articles for Deletion","Content moderation in online platforms is crucial for ensuring activity
therein adheres to existing policies, especially as these platforms grow. NLP
research in this area has typically focused on automating some part of it given
that it is not feasible to monitor all active discussions effectively. Past
works have focused on revealing deletion patterns with like sentiment analysis,
or on developing platform-specific models such as Wikipedia policy or stance
detectors. Unsurprisingly, however, this valuable body of work is rather
scattered, with little to no agreement with regards to e.g., the deletion
discussions corpora used for training or the number of stance labels. Moreover,
while efforts have been made to connect stance with rationales (e.g., to ground
a deletion decision on the relevant policy), there is little explanability work
beyond that. In this paper, we introduce a suite of experiments on Wikipedia
deletion discussions and wide-analyis (Wikipedia Deletion Analysis), a Python
package aimed at providing one click analysis to content moderation
discussions. We release all assets associated with wide-analysis, including
data, models and the Python package, and a HuggingFace space with the goal to
accelerate research on automating content moderation in Wikipedia and beyond.","[{'name': 'Hsuvas Borkakoty'}, {'name': 'Luis Espinosa-Anke'}]",2024-08-10T23:43:11Z
http://arxiv.org/abs/2408.05646v1,http://arxiv.org/abs/2408.05646v1,Eigen Attention: Attention in Low-Rank Space for KV Cache Compression,"Large language models (LLMs) represent a groundbreaking advancement in the
domain of natural language processing due to their impressive reasoning
abilities. Recently, there has been considerable interest in increasing the
context lengths for these models to enhance their applicability to complex
tasks. However, at long context lengths and large batch sizes, the key-value
(KV) cache, which stores the attention keys and values, emerges as the new
bottleneck in memory usage during inference. To address this, we propose Eigen
Attention, which performs the attention operation in a low-rank space, thereby
reducing the KV cache memory overhead. Our proposed approach is orthogonal to
existing KV cache compression techniques and can be used synergistically with
them. Through extensive experiments over OPT, MPT, and Llama model families, we
demonstrate that Eigen Attention results in up to 40% reduction in KV cache
sizes and up to 60% reduction in attention operation latency with minimal drop
in performance.","[{'name': 'Utkarsh Saxena'}, {'name': 'Gobinda Saha'}, {'name': 'Sakshi Choudhary'}, {'name': 'Kaushik Roy'}]",2024-08-10T22:47:12Z
http://arxiv.org/abs/2408.05636v2,http://arxiv.org/abs/2408.05636v2,"Speculative Diffusion Decoding: Accelerating Language Generation through
  Diffusion","Speculative decoding has emerged as a widely adopted method to accelerate
large language model inference without sacrificing the quality of the model
outputs. While this technique has facilitated notable speed improvements by
enabling parallel sequence verification, its efficiency remains inherently
limited by the reliance on incremental token generation in existing draft
models. To overcome this limitation, this paper proposes an adaptation of
speculative decoding which uses discrete diffusion models to generate draft
sequences. This allows parallelization of both the drafting and verification
steps, providing significant speed-ups to the inference process. Our proposed
approach, Speculative Diffusion Decoding (SpecDiff), is validated on standard
language generation benchmarks and empirically demonstrated to provide a up to
8.7x speed-up over standard generation processes and up to 2.5x speed-up over
existing speculative decoding approaches.","[{'name': 'Jacob K Christopher'}, {'name': 'Brian R Bartoldson'}, {'name': 'Bhavya Kailkhura'}, {'name': 'Ferdinando Fioretto'}]",2024-08-10T21:24:25Z
http://arxiv.org/abs/2408.06385v1,http://arxiv.org/abs/2408.06385v1,ViC: Virtual Compiler Is All You Need For Assembly Code Search,"Assembly code search is vital for reducing the burden on reverse engineers,
allowing them to quickly identify specific functions using natural language
within vast binary programs. Despite its significance, this critical task is
impeded by the complexities involved in building high-quality datasets. This
paper explores training a Large Language Model (LLM) to emulate a general
compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion
tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC),
capable of compiling any source code of any language to assembly code. This
approach allows for virtual compilation across a wide range of programming
languages without the need for a real compiler, preserving semantic equivalency
and expanding the possibilities for assembly code dataset construction.
Furthermore, we use ViC to construct a sufficiently large dataset for assembly
code search. Employing this extensive dataset, we achieve a substantial
improvement in assembly code search performance, with our model surpassing the
leading baseline by 26%.","[{'name': 'Zeyu Gao'}, {'name': 'Hao Wang'}, {'name': 'Yuanda Wang'}, {'name': 'Chao Zhang'}]",2024-08-10T17:23:02Z
http://arxiv.org/abs/2408.05568v1,http://arxiv.org/abs/2408.05568v1,Metacognitive Myopia in Large Language Models,"Large Language Models (LLMs) exhibit potentially harmful biases that
reinforce culturally inherent stereotypes, cloud moral judgments, or amplify
positive evaluations of majority groups. Previous explanations mainly
attributed bias in LLMs to human annotators and the selection of training data.
Consequently, they have typically been addressed with bottom-up approaches such
as reinforcement learning or debiasing corpora. However, these methods only
treat the effects of LLM biases by indirectly influencing the model
architecture, but do not address the underlying causes in the computational
process. Here, we propose metacognitive myopia as a cognitive-ecological
framework that can account for a conglomerate of established and emerging LLM
biases and provide a lever to address problems in powerful but vulnerable
tools. Our theoretical framework posits that a lack of the two components of
metacognition, monitoring and control, causes five symptoms of metacognitive
myopia in LLMs: integration of invalid tokens and embeddings, susceptibility to
redundant information, neglect of base rates in conditional computation,
decision rules based on frequency, and inappropriate higher-order statistical
inference for nested data structures. As a result, LLMs produce erroneous
output that reaches into the daily high-stakes decisions of humans. By
introducing metacognitive regulatory processes into LLMs, engineers and
scientists can develop precise remedies for the underlying causes of these
biases. Our theory sheds new light on flawed human-machine interactions and
raises ethical concerns regarding the increasing, imprudent implementation of
LLMs in organizational structures.","[{'name': 'Florian Scholten'}, {'name': 'Tobias R. Rebholz'}, {'name': 'Mandy Hütter'}]",2024-08-10T14:43:57Z
http://arxiv.org/abs/2408.05566v1,http://arxiv.org/abs/2408.05566v1,Document-Level Event Extraction with Definition-Driven ICL,"In the field of Natural Language Processing (NLP), Large Language Models
(LLMs) have shown great potential in document-level event extraction tasks, but
existing methods face challenges in the design of prompts. To address this
issue, we propose an optimization strategy called ""Definition-driven
Document-level Event Extraction (DDEE)."" By adjusting the length of the prompt
and enhancing the clarity of heuristics, we have significantly improved the
event extraction performance of LLMs. We used data balancing techniques to
solve the long-tail effect problem, enhancing the model's generalization
ability for event types. At the same time, we refined the prompt to ensure it
is both concise and comprehensive, adapting to the sensitivity of LLMs to the
style of prompts. In addition, the introduction of structured heuristic methods
and strict limiting conditions has improved the precision of event and argument
role extraction. These strategies not only solve the prompt engineering
problems of LLMs in document-level event extraction but also promote the
development of event extraction technology, providing new research perspectives
for other tasks in the NLP field.","[{'name': 'Zhuoyuan Liu'}, {'name': 'Yilin Luo'}]",2024-08-10T14:24:09Z
http://arxiv.org/abs/2408.05555v1,http://arxiv.org/abs/2408.05555v1,"Large Language Model-based Role-Playing for Personalized Medical Jargon
  Extraction","Previous studies reveal that Electronic Health Records (EHR), which have been
widely adopted in the U.S. to allow patients to access their personal medical
information, do not have high readability to patients due to the prevalence of
medical jargon. Tailoring medical notes to individual comprehension by
identifying jargon that is difficult for each person will enhance the utility
of generative models. We present the first quantitative analysis to measure the
impact of role-playing in LLM in medical term extraction. By comparing the
results of Mechanical Turk workers over 20 sentences, our study demonstrates
that LLM role-playing improves F1 scores in 95% of cases across 14 different
socio-demographic backgrounds. Furthermore, applying role-playing with
in-context learning outperformed the previous state-of-the-art models. Our
research showed that ChatGPT can improve traditional medical term extraction
systems by utilizing role-play to deliver personalized patient education, a
potential that previous models had not achieved.","[{'name': 'Jung Hoon Lim'}, {'name': 'Sunjae Kwon'}, {'name': 'Zonghai Yao'}, {'name': 'John P. Lalor'}, {'name': 'Hong Yu'}]",2024-08-10T13:40:44Z
http://arxiv.org/abs/2408.05554v1,http://arxiv.org/abs/2408.05554v1,"Improving Whisper's Recognition Performance for Under-Represented
  Language Kazakh Leveraging Unpaired Speech and Text","Whisper and other large-scale automatic speech recognition models have made
significant progress in performance. However, their performance on many
low-resource languages, such as Kazakh, is not satisfactory. It is worth
researching how to utilize low-cost data to improve the performance of Whisper
on under-represented languages. In this study, we utilized easily accessible
unpaired speech and text data and combined the language model GPT with Whisper
on Kazakh. We implemented end of transcript (EOT) judgment modification and
hallucination penalty to improve the performance of speech recognition.
Further, we employed the decoding average token log probability as a criterion
to select samples from unlabeled speech data and used pseudo-labeled data to
fine-tune the model to further improve its performance. Ultimately, we achieved
more than 10\% absolute WER reduction in multiple experiments, and the whole
process has the potential to be generalized to other under-represented
languages.","[{'name': 'Jinpeng Li'}, {'name': 'Yu Pu'}, {'name': 'Qi Sun'}, {'name': 'Wei-Qiang Zhang'}]",2024-08-10T13:39:13Z
http://arxiv.org/abs/2408.05545v2,http://arxiv.org/abs/2408.05545v2,Multi-layer Sequence Labeling-based Joint Biomedical Event Extraction,"In recent years, biomedical event extraction has been dominated by
complicated pipeline and joint methods, which need to be simplified. In
addition, existing work has not effectively utilized trigger word information
explicitly. Hence, we propose MLSL, a method based on multi-layer sequence
labeling for joint biomedical event extraction. MLSL does not introduce prior
knowledge and complex structures. Moreover, it explicitly incorporates the
information of candidate trigger words into the sequence labeling to learn the
interaction relationships between trigger words and argument roles. Based on
this, MLSL can learn well with just a simple workflow. Extensive
experimentation demonstrates the superiority of MLSL in terms of extraction
performance compared to other state-of-the-art methods.","[{'name': 'Gongchi Chen'}, {'name': 'Pengchao Wu'}, {'name': 'Jinghang Gu'}, {'name': 'Longhua Qian'}, {'name': 'Guodong Zhou'}]",2024-08-10T13:03:19Z
http://arxiv.org/abs/2408.05541v1,http://arxiv.org/abs/2408.05541v1,"P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for
  Optimizing LLM Training","In the rapidly evolving field of Large Language Models (LLMs), selecting
high-quality data for fine-tuning is essential. This paper focuses on
task-specific data pruning and selection to enhance fine-tuning. We introduce
an innovative framework, termed P3, which improves LLM performance through a
dynamic, adaptive training strategy. Specifically, P3 comprises the following
components: (1) Policy-driven Difficulty Measurement: we begin by measuring the
difficulty of data based on the model's real-time performance, transitioning
from static, predefined metrics to more dynamic and adaptable ones. (2)
Pace-adaptive Selection: we employ self-paced learning (SPL) to gradually
select increasingly challenging data, thereby progressively enhancing the
model's performance. (3) Diversity Promotion: we integrate Determinantal Point
Process (DPP) into the selection process to promote the diversity within and
between samples, enriching the learning process. We have validated our method
on two well-known LLM datasets, APPS and MATH, designed for logical reasoning
scenarios. The results show that our P3 framework significantly improves
training outcomes compared to traditional methods. By fundamentally refining
data selection and utilization strategies, P3 not only advances theoretical
understanding of dynamic training approaches but also provides a versatile
framework that can revolutionize model training in natural language processing.","[{'name': 'Yingxuan Yang'}, {'name': 'Huayi Wang'}, {'name': 'Muning Wen'}, {'name': 'Weinan Zhang'}]",2024-08-10T12:44:49Z
http://arxiv.org/abs/2408.05524v1,http://arxiv.org/abs/2408.05524v1,"Context-Driven Index Trimming: A Data Quality Perspective to Enhancing
  Precision of RALMs","Retrieval-Augmented Large Language Models (RALMs) have made significant
strides in enhancing the accuracy of generated responses.However, existing
research often overlooks the data quality issues within retrieval results,
often caused by inaccurate existing vector-distance-based retrieval methods.We
propose to boost the precision of RALMs' answers from a data quality
perspective through the Context-Driven Index Trimming (CDIT) framework, where
Context Matching Dependencies (CMDs) are employed as logical data quality rules
to capture and regulate the consistency between retrieved contexts.Based on the
semantic comprehension capabilities of Large Language Models (LLMs), CDIT can
effectively identify and discard retrieval results that are inconsistent with
the query context and further modify indexes in the database, thereby improving
answer quality.Experiments demonstrate on challenging question-answering
tasks.Also, the flexibility of CDIT is verified through its compatibility with
various language models and indexing methods, which offers a promising approach
to bolster RALMs' data quality and retrieval precision jointly.","[{'name': 'Kexin Ma'}, {'name': 'Ruochun Jin'}, {'name': 'Xi Wang'}, {'name': 'Huan Chen'}, {'name': 'Jing Ren'}, {'name': 'Yuhua Tang'}]",2024-08-10T11:39:22Z
http://arxiv.org/abs/2408.05517v3,http://arxiv.org/abs/2408.05517v3,SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning,"Recent development in Large Language Models (LLMs) and Multi-modal Large
Language Models (MLLMs) have leverage Attention-based Transformer architectures
and achieved superior performance and generalization capabilities. They have
since covered extensive areas of traditional learning tasks. For instance,
text-based tasks such as text-classification and sequence-labeling, as well as
multi-modal tasks like Visual Question Answering (VQA) and Optical Character
Recognition (OCR), which were previously addressed using different models, can
now be tackled based on one foundation model. Consequently, the training and
lightweight fine-tuning of LLMs and MLLMs, especially those based on
Transformer architecture, has become particularly important. In recognition of
these overwhelming needs, we develop SWIFT, a customizable one-stop
infrastructure for large models. With support of over $300+$ LLMs and $50+$
MLLMs, SWIFT stands as the open-source framework that provide the most
comprehensive support for fine-tuning large models. In particular, it is the
first training framework that provides systematic support for MLLMs. In
addition to the core functionalities of fine-tuning, SWIFT also integrates
post-training processes such as inference, evaluation, and model quantization,
to facilitate fast adoptions of large models in various application scenarios.
With a systematic integration of various training techniques, SWIFT offers
helpful utilities such as benchmark comparisons among different training
techniques for large models. For fine-tuning models specialized in agent
framework, we show that notable improvements on the ToolBench leader-board can
be achieved by training with customized dataset on SWIFT, with an increase of
5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in
hallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.","[{'name': 'Yuze Zhao'}, {'name': 'Jintao Huang'}, {'name': 'Jinghan Hu'}, {'name': 'Xingjun Wang'}, {'name': 'Yunlin Mao'}, {'name': 'Daoze Zhang'}, {'name': 'Zeyinzi Jiang'}, {'name': 'Zhikai Wu'}, {'name': 'Baole Ai'}, {'name': 'Ang Wang'}, {'name': 'Wenmeng Zhou'}, {'name': 'Yingda Chen'}]",2024-08-10T11:00:13Z
http://arxiv.org/abs/2408.05506v1,http://arxiv.org/abs/2408.05506v1,"Your Context Is Not an Array: Unveiling Random Access Limitations in
  Transformers","Despite their recent successes, Transformer-based large language models show
surprising failure modes. A well-known example of such failure modes is their
inability to length-generalize: solving problem instances at inference time
that are longer than those seen during training. In this work, we further
explore the root cause of this failure by performing a detailed analysis of
model behaviors on the simple parity task. Our analysis suggests that length
generalization failures are intricately related to a model's inability to
perform random memory accesses within its context window. We present supporting
evidence for this hypothesis by demonstrating the effectiveness of
methodologies that circumvent the need for indexing or that enable random token
access indirectly, through content-based addressing. We further show where and
how the failure to perform random memory access manifests through attention map
visualizations.","[{'name': 'MohammadReza Ebrahimi'}, {'name': 'Sunny Panchal'}, {'name': 'Roland Memisevic'}]",2024-08-10T10:12:09Z
http://arxiv.org/abs/2408.05502v1,http://arxiv.org/abs/2408.05502v1,"GEM: Context-Aware Gaze EstiMation with Visual Search Behavior Matching
  for Chest Radiograph","Gaze estimation is pivotal in human scene comprehension tasks, particularly
in medical diagnostic analysis. Eye-tracking technology facilitates the
recording of physicians' ocular movements during image interpretation, thereby
elucidating their visual attention patterns and information-processing
strategies. In this paper, we initially define the context-aware gaze
estimation problem in medical radiology report settings. To understand the
attention allocation and cognitive behavior of radiologists during the medical
image interpretation process, we propose a context-aware Gaze EstiMation (GEM)
network that utilizes eye gaze data collected from radiologists to simulate
their visual search behavior patterns throughout the image interpretation
process. It consists of a context-awareness module, visual behavior graph
construction, and visual behavior matching. Within the context-awareness
module, we achieve intricate multimodal registration by establishing
connections between medical reports and images. Subsequently, for a more
accurate simulation of genuine visual search behavior patterns, we introduce a
visual behavior graph structure, capturing such behavior through high-order
relationships (edges) between gaze points (nodes). To maintain the authenticity
of visual behavior, we devise a visual behavior-matching approach, adjusting
the high-order relationships between them by matching the graph constructed
from real and estimated gaze points. Extensive experiments on four publicly
available datasets demonstrate the superiority of GEM over existing methods and
its strong generalizability, which also provides a new direction for the
effective utilization of diverse modalities in medical image interpretation and
enhances the interpretability of models in the field of medical imaging.
https://github.com/Tiger-SN/GEM","[{'name': 'Shaonan Liu'}, {'name': 'Wenting Chen'}, {'name': 'Jie Liu'}, {'name': 'Xiaoling Luo'}, {'name': 'Linlin Shen'}]",2024-08-10T09:46:25Z
http://arxiv.org/abs/2408.05497v1,http://arxiv.org/abs/2408.05497v1,"MABR: A Multilayer Adversarial Bias Removal Approach Without Prior Bias
  Knowledge","Models trained on real-world data often mirror and exacerbate existing social
biases. Traditional methods for mitigating these biases typically require prior
knowledge of the specific biases to be addressed, such as gender or racial
biases, and the social groups associated with each instance. In this paper, we
introduce a novel adversarial training strategy that operates independently of
prior bias-type knowledge and protected attribute labels. Our approach
proactively identifies biases during model training by utilizing auxiliary
models, which are trained concurrently by predicting the performance of the
main model without relying on task labels. Additionally, we implement these
auxiliary models at various levels of the feature maps of the main model,
enabling the detection of a broader and more nuanced range of bias features.
Through experiments on racial and gender biases in sentiment and occupation
classification tasks, our method effectively reduces social biases without the
need for demographic annotations. Moreover, our approach not only matches but
often surpasses the efficacy of methods that require detailed demographic
insights, marking a significant advancement in bias mitigation techniques.","[{'name': 'Maxwell J. Yin'}, {'name': 'Boyu Wang'}, {'name': 'Charles Ling'}]",2024-08-10T09:11:01Z
http://arxiv.org/abs/2408.05457v1,http://arxiv.org/abs/2408.05457v1,Investigating Instruction Tuning Large Language Models on Graphs,"Inspired by the recent advancements of Large Language Models (LLMs) in NLP
tasks, there's growing interest in applying LLMs to graph-related tasks. This
study delves into the capabilities of instruction-following LLMs for engaging
with real-world graphs, aiming to offer empirical insights into how LLMs can
effectively interact with graphs and generalize across graph tasks. We begin by
constructing a dataset designed for instruction tuning, which comprises a
diverse collection of 79 graph-related tasks from academic and e-commerce
domains, featuring 44,240 training instances and 18,960 test samples. Utilizing
this benchmark, our initial investigation focuses on identifying the optimal
graph representation that serves as a conduit for LLMs to understand complex
graph structures. Our findings indicate that JSON format for graph
representation consistently outperforms natural language and code formats
across various LLMs and graph types. Furthermore, we examine the key factors
that influence the generalization abilities of instruction-tuned LLMs by
evaluating their performance on both in-domain and out-of-domain graph tasks.","[{'name': 'Kerui Zhu'}, {'name': 'Bo-Wei Huang'}, {'name': 'Bowen Jin'}, {'name': 'Yizhu Jiao'}, {'name': 'Ming Zhong'}, {'name': 'Kevin Chang'}, {'name': 'Shou-De Lin'}, {'name': 'Jiawei Han'}]",2024-08-10T06:54:35Z
http://arxiv.org/abs/2408.05456v1,http://arxiv.org/abs/2408.05456v1,"Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph
  Representation","Unified graph representation learning aims to produce node embeddings, which
can be applied to multiple downstream applications. However, existing studies
based on graph neural networks and language models either suffer from the
limitations of numerous training needed toward specific downstream predictions
or have shallow semantic features. In this work, we propose a novel Path-LLM
model to learn unified graph representation, which leverages a powerful large
language model (LLM) to incorporate our proposed path features. Our Path-LLM
framework consists of several well-designed techniques. First, we develop a new
mechanism of long-to-short shortest path (L2SP) selection, which covers
essential connections between different dense groups. An in-depth comparison of
different path selection plans is offered to illustrate the strength of our
designed L2SP. Then, we design path textualization to obtain L2SP-based
training texts. Next, we feed the texts into a self-supervised LLM training
process to learn embeddings. Extensive experiments on benchmarks validate the
superiority of Path-LLM against the state-of-the-art WalkLM method on two
classical graph learning tasks (node classification and link prediction) and
one NP-hard graph query processing task (keyword search), meanwhile saving more
than 90% of training paths.","[{'name': 'Wenbo Shang'}, {'name': 'Xuliang Zhu'}, {'name': 'Xin Huang'}]",2024-08-10T06:35:11Z
http://arxiv.org/abs/2408.05212v1,http://arxiv.org/abs/2408.05212v1,"Preserving Privacy in Large Language Models: A Survey on Current Threats
  and Solutions","Large Language Models (LLMs) represent a significant advancement in
artificial intelligence, finding applications across various domains. However,
their reliance on massive internet-sourced datasets for training brings notable
privacy issues, which are exacerbated in critical domains (e.g., healthcare).
Moreover, certain application-specific scenarios may require fine-tuning these
models on private data. This survey critically examines the privacy threats
associated with LLMs, emphasizing the potential for these models to memorize
and inadvertently reveal sensitive information. We explore current threats by
reviewing privacy attacks on LLMs and propose comprehensive solutions for
integrating privacy mechanisms throughout the entire learning pipeline. These
solutions range from anonymizing training datasets to implementing differential
privacy during training or inference and machine unlearning after training. Our
comprehensive review of existing literature highlights ongoing challenges,
available tools, and future directions for preserving privacy in LLMs. This
work aims to guide the development of more secure and trustworthy AI systems by
providing a thorough understanding of privacy preservation methods and their
effectiveness in mitigating risks.","[{'name': 'Michele Miranda'}, {'name': 'Elena Sofia Ruzzetti'}, {'name': 'Andrea Santilli'}, {'name': 'Fabio Massimo Zanzotto'}, {'name': 'Sébastien Bratières'}, {'name': 'Emanuele Rodolà'}]",2024-08-10T05:41:19Z
http://arxiv.org/abs/2408.05442v1,http://arxiv.org/abs/2408.05442v1,"Chain of Condition: Construct, Verify and Solve Conditions for
  Conditional Question Answering","Conditional question answering (CQA) is an important task that aims to find
probable answers and identify conditions that need to be satisfied to support
the answer. Existing approaches struggle with CQA due to two main challenges:
(1) precisely identifying conditions and their logical relationship, and (2)
verifying and solving the conditions. To address these challenges, we propose
Chain of Condition, a novel prompting approach by firstly identifying all
conditions and constructing their logical relationships explicitly according to
the document, then verifying whether these conditions are satisfied, finally
solving the logical expression by tools to indicate any missing conditions and
generating the answer based on the resolved conditions. The experiments on two
benchmark conditional question answering datasets shows chain of condition
outperforms existing prompting baselines, establishing a new state-of-the-art.
Furthermore, with backbone models like GPT-3.5-Turbo or GPT-4, it surpasses all
supervised baselines with only few-shot settings.","[{'name': 'Jiuheng Lin'}, {'name': 'Yuxuan Lai'}, {'name': 'Yansong Feng'}]",2024-08-10T05:09:11Z
http://arxiv.org/abs/2408.05404v1,http://arxiv.org/abs/2408.05404v1,"LaiDA: Linguistics-aware In-context Learning with Data Augmentation for
  Metaphor Components Identification","Metaphor Components Identification (MCI) contributes to enhancing machine
understanding of metaphors, thereby advancing downstream natural language
processing tasks. However, the complexity, diversity, and dependency on context
and background knowledge pose significant challenges for MCI. Large language
models (LLMs) offer new avenues for accurate comprehension of complex natural
language texts due to their strong semantic analysis and extensive commonsense
knowledge. In this research, a new LLM-based framework is proposed, named
Linguistics-aware In-context Learning with Data Augmentation (LaiDA).
Specifically, ChatGPT and supervised fine-tuning are utilized to tailor a
high-quality dataset. LaiDA incorporates a simile dataset for pre-training. A
graph attention network encoder generates linguistically rich feature
representations to retrieve similar examples. Subsequently, LLM is fine-tuned
with prompts that integrate linguistically similar examples. LaiDA ranked 2nd
in Subtask 2 of NLPCC2024 Shared Task 9, demonstrating its effectiveness. Code
and data are available at https://github.com/WXLJZ/LaiDA.","[{'name': 'Hongde Liu'}, {'name': 'Chenyuan He'}, {'name': 'Feiyang Meng'}, {'name': 'Changyong Niu'}, {'name': 'Yuxiang Jia'}]",2024-08-10T02:02:26Z
http://arxiv.org/abs/2408.05365v1,http://arxiv.org/abs/2408.05365v1,"FiST-Financial Style Transfer with Hallucination and Creativity Control
  Framework","Financial report generation using general purpose large language models pose
two major challenges, including the lack of compound sentences and
hallucinations. Advanced prompt engineering and retrieval augmented generation
(RAG) techniques are incapable of curing the writing style discrepancies. In
this work we propose a novel two-stage fine-tuning process wherein public
domain financial reports are processed into prompt-completions and augmented
using simple LLM prompts to then enable sectional financial report generation
using minimal instructions and tabular data inputs. Our proposed fine-tuning
framework results doubles the number of correct questions answers and reduces
hallucinations by over 50%. Additionally, the two-stage fine tuned models have
lower perplexity, improved ROUGE, TER and BLEU scores, higher creativity and
knowledge density with lower uncertainty and cross entropy.","[{'name': 'Sohini Roychowdhury'}, {'name': 'Marko Krema'}, {'name': 'Brian Moore'}, {'name': 'Xingjian Lai'}, {'name': 'Dike Effedua'}, {'name': 'Bharat Jethwani'}]",2024-08-09T22:29:23Z
http://arxiv.org/abs/2408.05346v2,http://arxiv.org/abs/2408.05346v2,"DataNarrative: Automated Data-Driven Storytelling with Visualizations
  and Texts","Data-driven storytelling is a powerful method for conveying insights by
combining narrative techniques with visualizations and text. These stories
integrate visual aids, such as highlighted bars and lines in charts, along with
textual annotations explaining insights. However, creating such stories
requires a deep understanding of the data and meticulous narrative planning,
often necessitating human intervention, which can be time-consuming and
mentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,
their ability to generate coherent and comprehensive data stories remains
underexplored. In this work, we introduce a novel task for data story
generation and a benchmark containing 1,449 stories from diverse sources. To
address the challenges of crafting coherent data stories, we propose a
multiagent framework employing two LLM agents designed to replicate the human
storytelling process: one for understanding and describing the data
(Reflection), generating the outline, and narration, and another for
verification at each intermediary step. While our agentic framework generally
outperforms non-agentic counterparts in both model-based and human evaluations,
the results also reveal unique challenges in data story generation.","[{'name': 'Mohammed Saidul Islam'}, {'name': 'Md Tahmid Rahman Laskar'}, {'name': 'Md Rizwan Parvez'}, {'name': 'Enamul Hoque'}, {'name': 'Shafiq Joty'}]",2024-08-09T21:31:33Z
http://arxiv.org/abs/2408.05334v1,http://arxiv.org/abs/2408.05334v1,Revisiting Multi-Modal LLM Evaluation,"With the advent of multi-modal large language models (MLLMs), datasets used
for visual question answering (VQA) and referring expression comprehension have
seen a resurgence. However, the most popular datasets used to evaluate MLLMs
are some of the earliest ones created, and they have many known problems,
including extreme bias, spurious correlations, and an inability to permit
fine-grained analysis. In this paper, we pioneer evaluating recent MLLMs (LLaVA
1.5, LLaVA-NeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-4o) on datasets designed
to address weaknesses in earlier ones. We assess three VQA datasets: 1) TDIUC,
which permits fine-grained analysis on 12 question types; 2) TallyQA, which has
simple and complex counting questions; and 3) DVQA, which requires optical
character recognition for chart understanding. We also study VQDv1, a dataset
that requires identifying all image regions that satisfy a given query. Our
experiments reveal the weaknesses of many MLLMs that have not previously been
reported. Our code is integrated into the widely used LAVIS framework for MLLM
evaluation, enabling the rapid assessment of future MLLMs. Project webpage:
https://kevinlujian.github.io/MLLM_Evaluations/","[{'name': 'Jian Lu'}, {'name': 'Shikhar Srivastava'}, {'name': 'Junyu Chen'}, {'name': 'Robik Shrestha'}, {'name': 'Manoj Acharya'}, {'name': 'Kushal Kafle'}, {'name': 'Christopher Kanan'}]",2024-08-09T20:55:46Z
http://arxiv.org/abs/2408.05328v1,http://arxiv.org/abs/2408.05328v1,"From Text to Insight: Leveraging Large Language Models for Performance
  Evaluation in Management","This study explores the potential of Large Language Models (LLMs),
specifically GPT-4, to enhance objectivity in organizational task performance
evaluations. Through comparative analyses across two studies, including various
task performance outputs, we demonstrate that LLMs can serve as a reliable and
even superior alternative to human raters in evaluating knowledge-based
performance outputs, which are a key contribution of knowledge workers. Our
results suggest that GPT ratings are comparable to human ratings but exhibit
higher consistency and reliability. Additionally, combined multiple GPT ratings
on the same performance output show strong correlations with aggregated human
performance ratings, akin to the consensus principle observed in performance
evaluation literature. However, we also find that LLMs are prone to contextual
biases, such as the halo effect, mirroring human evaluative biases. Our
research suggests that while LLMs are capable of extracting meaningful
constructs from text-based data, their scope is currently limited to specific
forms of performance evaluation. By highlighting both the potential and
limitations of LLMs, our study contributes to the discourse on AI role in
management studies and sets a foundation for future research to refine AI
theoretical and practical applications in management.","[{'name': 'Ning Li'}, {'name': 'Huaikang Zhou'}, {'name': 'Mingze Xu'}]",2024-08-09T20:35:10Z
http://arxiv.org/abs/2408.05326v1,http://arxiv.org/abs/2408.05326v1,A Psychology-based Unified Dynamic Framework for Curriculum Learning,"Directly learning from examples of random difficulty levels is often
challenging for both humans and machine learning models. A more effective
strategy involves exposing learners to examples in a progressive order, from
easy to difficult. Curriculum Learning (CL) has been proposed to implement this
strategy in machine learning model training. However, two key challenges
persist in CL framework design: defining the difficulty of training data and
determining the appropriate amount of data to input at each training step. This
paper presents a Psychology-based Unified Dynamic Framework for Curriculum
Learning (PUDF), drawing inspiration from psychometrics. We quantify the
difficulty of training data by applying Item Response Theory (IRT) to responses
from Artificial Crowds (AC). This theory-driven IRT-AC approach leads to global
(i.e., model-independent) and interpretable difficulty values. Leveraging IRT,
we propose a Dynamic Data Selection via Model Ability Estimation (DDS-MAE)
strategy to schedule the appropriate amount of data during model training.
Since our difficulty labeling and model ability estimation are based on a
consistent theory, namely IRT, their values are comparable within the same
scope, potentially leading to a faster convergence compared to the other CL
methods. Experimental results demonstrate that fine-tuning pre-trained language
models with PUDF enhances their performance on the GLUE benchmark. Moreover,
PUDF surpasses other state-of-the-art (SOTA) CL methods on the GLUE benchmark.
We further explore the components of PUDF, namely the difficulty measurer
(IRT-AC) and the training scheduler (DDS-MAE) qualitatively and quantitatively.
Lastly, we conduct an ablation study to clarify which components of PUDF
contribute to faster convergence and higher accuracy.","[{'name': 'Guangyu Meng'}, {'name': 'Qingkai Zeng'}, {'name': 'John P. Lalor'}, {'name': 'Hong Yu'}]",2024-08-09T20:30:37Z
http://arxiv.org/abs/2408.05283v1,http://arxiv.org/abs/2408.05283v1,"MUSE: Multi-Knowledge Passing on the Edges, Boosting Knowledge Graph
  Completion","Knowledge Graph Completion (KGC) aims to predict the missing information in
the (head entity)-[relation]-(tail entity) triplet. Deep Neural Networks have
achieved significant progress in the relation prediction task. However, most
existing KGC methods focus on single features (e.g., entity IDs) and sub-graph
aggregation, which cannot fully explore all the features in the Knowledge Graph
(KG), and neglect the external semantic knowledge injection. To address these
problems, we propose MUSE, a knowledge-aware reasoning model to learn a
tailored embedding space in three dimensions for missing relation prediction
through a multi-knowledge representation learning mechanism. Our MUSE consists
of three parallel components: 1) Prior Knowledge Learning for enhancing the
triplets' semantic representation by fine-tuning BERT; 2) Context Message
Passing for enhancing the context messages of KG; 3) Relational Path
Aggregation for enhancing the path representation from the head entity to the
tail entity. Our experimental results show that MUSE significantly outperforms
other baselines on four public datasets, such as over 5.50% improvement in H@1
and 4.20% improvement in MRR on the NELL995 dataset. The code and all datasets
will be released via https://github.com/NxxTGT/MUSE.",[{'name': 'Pengjie Liu'}],2024-08-09T18:10:02Z
http://arxiv.org/abs/2408.05211v1,http://arxiv.org/abs/2408.05211v1,VITA: Towards Open-Source Interactive Omni Multimodal LLM,"The remarkable multimodal capabilities and interactive experience of GPT-4o
underscore their necessity in practical applications, yet open-source models
rarely excel in both areas. In this paper, we introduce VITA, the first-ever
open-source Multimodal Large Language Model (MLLM) adept at simultaneous
processing and analysis of Video, Image, Text, and Audio modalities, and
meanwhile has an advanced multimodal interactive experience. Starting from
Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary
followed by bilingual instruction tuning. We further endow the language model
with visual and audio capabilities through two-stage multi-task learning of
multimodal alignment and instruction tuning. VITA demonstrates robust
foundational capabilities of multilingual, vision, and audio understanding, as
evidenced by its strong performance across a range of both unimodal and
multimodal benchmarks. Beyond foundational capabilities, we have made
considerable progress in enhancing the natural multimodal human-computer
interaction experience. To the best of our knowledge, we are the first to
exploit non-awakening interaction and audio interrupt in MLLM. VITA is the
first step for the open-source community to explore the seamless integration of
multimodal understanding and interaction. While there is still lots of work to
be done on VITA to get close to close-source counterparts, we hope that its
role as a pioneer can serve as a cornerstone for subsequent research. Project
Page: https://vita-home.github.io.","[{'name': 'Chaoyou Fu'}, {'name': 'Haojia Lin'}, {'name': 'Zuwei Long'}, {'name': 'Yunhang Shen'}, {'name': 'Meng Zhao'}, {'name': 'Yifan Zhang'}, {'name': 'Xiong Wang'}, {'name': 'Di Yin'}, {'name': 'Long Ma'}, {'name': 'Xiawu Zheng'}, {'name': 'Ran He'}, {'name': 'Rongrong Ji'}, {'name': 'Yunsheng Wu'}, {'name': 'Caifeng Shan'}, {'name': 'Xing Sun'}]",2024-08-09T17:59:49Z
http://arxiv.org/abs/2408.05204v1,http://arxiv.org/abs/2408.05204v1,"Evaluating the capability of large language models to personalize
  science texts for diverse middle-school-age learners","Large language models (LLMs), including OpenAI's GPT-series, have made
significant advancements in recent years. Known for their expertise across
diverse subject areas and quick adaptability to user-provided prompts, LLMs
hold unique potential as Personalized Learning (PL) tools. Despite this
potential, their application in K-12 education remains largely unexplored. This
paper presents one of the first randomized controlled trials (n = 23) to
evaluate the effectiveness of GPT-4 in personalizing educational science texts
for middle school students. In this study, GPT-4 was used to profile student
learning preferences based on choices made during a training session. For the
experimental group, GPT-4 was used to rewrite science texts to align with the
student's predicted profile while, for students in the control group, texts
were rewritten to contradict their learning preferences. The results of a
Mann-Whitney U test showed that students significantly preferred (at the .10
level) the rewritten texts when they were aligned with their profile (p =
.059). These findings suggest that GPT-4 can effectively interpret and tailor
educational content to diverse learner preferences, marking a significant
advancement in PL technology. The limitations of this study and ethical
considerations for using artificial intelligence in education are also
discussed.","[{'name': 'Michael Vaccaro Jr'}, {'name': 'Mikayla Friday'}, {'name': 'Arash Zaghi'}]",2024-08-09T17:53:35Z
http://arxiv.org/abs/2408.05200v1,http://arxiv.org/abs/2408.05200v1,"TaSL: Task Skill Localization and Consolidation for Language Model
  Continual Learning","Language model continual learning (CL) has recently garnered significant
interest due to its potential to adapt large language models (LLMs) to dynamic
real-world environments without re-training. A key challenge in this field is
catastrophic forgetting, where models lose previously acquired knowledge when
learning new tasks. Existing methods commonly employ multiple
parameter-efficient fine-tuning (PEFT) blocks to acquire task-specific
knowledge for each task, but these approaches lack efficiency and overlook the
potential for knowledge transfer through task interaction. In this paper, we
present a novel CL framework for language models called Task Skill Localization
and Consolidation (TaSL), which enhances knowledge transfer without relying on
memory replay. TaSL first divides the model into `skill units' based on
parameter dependencies, enabling more granular control. It then employs a novel
group-wise skill localization technique to identify the importance distribution
of skill units for a new task. By comparing this importance distribution with
those from previous tasks, we implement a fine-grained skill consolidation
strategy that retains task-specific knowledge, thereby preventing forgetting,
and updates task-shared knowledge, which facilitates bi-directional knowledge
transfer. As a result, TaSL achieves a superior balance between retaining
previous knowledge and excelling in new tasks. TaSL also shows strong
generalizability, suitable for general models and customizable for PEFT methods
like LoRA. Additionally, it demonstrates notable extensibility, allowing
integration with memory replay to further enhance performance. Extensive
experiments on two CL benchmarks, with varying model sizes (from 220M to 7B),
demonstrate the effectiveness of TaSL and its variants across different
settings.","[{'name': 'Yujie Feng'}, {'name': 'Xu Chu'}, {'name': 'Yongxin Xu'}, {'name': 'Zexin Lu'}, {'name': 'Bo Liu'}, {'name': 'Philip S. Yu'}, {'name': 'Xiao-Ming Wu'}]",2024-08-09T17:44:45Z
http://arxiv.org/abs/2408.05192v1,http://arxiv.org/abs/2408.05192v1,"Separating Style from Substance: Enhancing Cross-Genre Authorship
  Attribution through Data Selection and Presentation","The task of deciding whether two documents are written by the same author is
challenging for both machines and humans. This task is even more challenging
when the two documents are written about different topics (e.g. baseball vs.
politics) or in different genres (e.g. a blog post vs. an academic article).
For machines, the problem is complicated by the relative lack of real-world
training examples that cross the topic boundary and the vanishing scarcity of
cross-genre data. We propose targeted methods for training data selection and a
novel learning curriculum that are designed to discourage a model's reliance on
topic information for authorship attribution and correspondingly force it to
incorporate information more robustly indicative of style no matter the topic.
These refinements yield a 62.7% relative improvement in average cross-genre
authorship attribution, as well as 16.6% in the per-genre condition.","[{'name': 'Steven Fincke'}, {'name': 'Elizabeth Boschee'}]",2024-08-09T17:31:37Z
http://arxiv.org/abs/2408.05184v1,http://arxiv.org/abs/2408.05184v1,"Deep-change at AXOLOTL-24: Orchestrating WSD and WSI Models for Semantic
  Change Modeling","This paper describes our solution of the first subtask from the AXOLOTL-24
shared task on Semantic Change Modeling. The goal of this subtask is to
distribute a given set of usages of a polysemous word from a newer time period
between senses of this word from an older time period and clusters representing
gained senses of this word. We propose and experiment with three new methods
solving this task. Our methods achieve SOTA results according to both official
metrics of the first substask. Additionally, we develop a model that can tell
if a given word usage is not described by any of the provided sense
definitions. This model serves as a component in one of our methods, but can
potentially be useful on its own.","[{'name': 'Denis Kokosinskii'}, {'name': 'Mikhail Kuklin'}, {'name': 'Nikolay Arefyev'}]",2024-08-09T17:15:54Z
http://arxiv.org/abs/2408.05147v2,http://arxiv.org/abs/2408.05147v2,Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2,"Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse
decomposition of a neural network's latent representations into seemingly
interpretable features. Despite recent excitement about their potential,
research applications outside of industry are limited by the high cost of
training a comprehensive suite of SAEs. In this work, we introduce Gemma Scope,
an open suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2
2B and 9B and select layers of Gemma 2 27B base models. We primarily train SAEs
on the Gemma 2 pre-trained models, but additionally release SAEs trained on
instruction-tuned Gemma 2 9B for comparison. We evaluate the quality of each
SAE on standard metrics and release these results. We hope that by releasing
these SAE weights, we can help make more ambitious safety and interpretability
research easier for the community. Weights and a tutorial can be found at
https://huggingface.co/google/gemma-scope and an interactive demo can be found
at https://www.neuronpedia.org/gemma-scope","[{'name': 'Tom Lieberum'}, {'name': 'Senthooran Rajamanoharan'}, {'name': 'Arthur Conmy'}, {'name': 'Lewis Smith'}, {'name': 'Nicolas Sonnerat'}, {'name': 'Vikrant Varma'}, {'name': 'János Kramár'}, {'name': 'Anca Dragan'}, {'name': 'Rohin Shah'}, {'name': 'Neel Nanda'}]",2024-08-09T16:06:42Z
http://arxiv.org/abs/2408.05141v1,http://arxiv.org/abs/2408.05141v1,A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning,"Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.","[{'name': 'Ye Yuan'}, {'name': 'Chengwu Liu'}, {'name': 'Jingyang Yuan'}, {'name': 'Gongbo Sun'}, {'name': 'Siqi Li'}, {'name': 'Ming Zhang'}]",2024-08-09T15:53:55Z
http://arxiv.org/abs/2408.05126v1,http://arxiv.org/abs/2408.05126v1,"Large Language Models and Thematic Analysis: Human-AI Synergy in
  Researching Hate Speech on Social Media","In the dynamic field of artificial intelligence (AI), the development and
application of Large Language Models (LLMs) for text analysis are of
significant academic interest. Despite the promising capabilities of various
LLMs in conducting qualitative analysis, their use in the humanities and social
sciences has not been thoroughly examined. This article contributes to the
emerging literature on LLMs in qualitative analysis by documenting an
experimental study involving GPT-4. The study focuses on performing thematic
analysis (TA) using a YouTube dataset derived from an EU-funded project, which
was previously analyzed by other researchers. This dataset is about the
representation of Roma migrants in Sweden during 2016, a period marked by the
aftermath of the 2015 refugee crisis and preceding the Swedish national
elections in 2017. Our study seeks to understand the potential of combining
human intelligence with AI's scalability and efficiency, examining the
advantages and limitations of employing LLMs in qualitative research within the
humanities and social sciences. Additionally, we discuss future directions for
applying LLMs in these fields.","[{'name': 'Petre Breazu'}, {'name': 'Miriam Schirmer'}, {'name': 'Songbo Hu'}, {'name': 'Napoleon Kastos'}]",2024-08-09T15:34:41Z
http://arxiv.org/abs/2408.07091v1,http://arxiv.org/abs/2408.07091v1,"Node Level Graph Autoencoder: Unified Pretraining for Textual Graph
  Learning","Textual graphs are ubiquitous in real-world applications, featuring rich text
information with complex relationships, which enables advanced research across
various fields. Textual graph representation learning aims to generate
low-dimensional feature embeddings from textual graphs that can improve the
performance of downstream tasks. A high-quality feature embedding should
effectively capture both the structural and the textual information in a
textual graph. However, most textual graph dataset benchmarks rely on word2vec
techniques to generate feature embeddings, which inherently limits their
capabilities. Recent works on textual graph representation learning can be
categorized into two folds: supervised and unsupervised methods. Supervised
methods finetune a language model on labeled nodes, which have limited
capabilities when labeled data is scarce. Unsupervised methods, on the other
hand, extract feature embeddings by developing complex training pipelines. To
address these limitations, we propose a novel unified unsupervised learning
autoencoder framework, named Node Level Graph AutoEncoder (NodeGAE). We employ
language models as the backbone of the autoencoder, with pretraining on text
reconstruction. Additionally, we add an auxiliary loss term to make the feature
embeddings aware of the local graph structure. Our method maintains simplicity
in the training process and demonstrates generalizability across diverse
textual graphs and downstream tasks. We evaluate our method on two core graph
representation learning downstream tasks: node classification and link
prediction. Comprehensive experiments demonstrate that our approach
substantially enhances the performance of diverse graph neural networks (GNNs)
across multiple textual graph datasets.","[{'name': 'Wenbin Hu'}, {'name': 'Huihao Jing'}, {'name': 'Qi Hu'}, {'name': 'Haoran Li'}, {'name': 'Yangqiu Song'}]",2024-08-09T14:57:53Z
http://arxiv.org/abs/2408.05102v1,http://arxiv.org/abs/2408.05102v1,How Well Do LLMs Identify Cultural Unity in Diversity?,"Much work on the cultural awareness of large language models (LLMs) focuses
on the models' sensitivity to geo-cultural diversity. However, in addition to
cross-cultural differences, there also exists common ground across cultures.
For instance, a bridal veil in the United States plays a similar
cultural-relevant role as a honggaitou in China. In this study, we introduce a
benchmark dataset CUNIT for evaluating decoder-only LLMs in understanding the
cultural unity of concepts. Specifically, CUNIT consists of 1,425 evaluation
examples building upon 285 traditional cultural-specific concepts across 10
countries. Based on a systematic manual annotation of cultural-relevant
features per concept, we calculate the cultural association between any pair of
cross-cultural concepts. Built upon this dataset, we design a contrastive
matching task to evaluate the LLMs' capability to identify highly associated
cross-cultural concept pairs. We evaluate 3 strong LLMs, using 3 popular
prompting strategies, under the settings of either giving all extracted concept
features or no features at all on CUNIT Interestingly, we find that cultural
associations across countries regarding clothing concepts largely differ from
food. Our analysis shows that LLMs are still limited to capturing
cross-cultural associations between concepts compared to humans. Moreover,
geo-cultural proximity shows a weak influence on model performance in capturing
cross-cultural associations.","[{'name': 'Jialin Li'}, {'name': 'Junli Wang'}, {'name': 'Junjie Hu'}, {'name': 'Ming Jiang'}]",2024-08-09T14:45:22Z
http://arxiv.org/abs/2408.05101v1,http://arxiv.org/abs/2408.05101v1,"MooER: LLM-based Speech Recognition and Translation Models from Moore
  Threads","In this paper, we present MooER, a LLM-based large-scale automatic speech
recognition (ASR) / automatic speech translation (AST) model of Moore Threads.
A 5000h pseudo labeled dataset containing open source and self collected speech
data is used for training. We achieve performance comparable to other open
source models trained with up to hundreds of thousands of hours of labeled
speech data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest
that our model outperforms other open source Speech LLMs. A BLEU score of 25.2
can be obtained. The main contributions of this paper are summarized as
follows. First, this paper presents a training strategy for encoders and LLMs
on speech related tasks (including ASR and AST) using a small size of pseudo
labeled data without any extra manual annotation and selection. Second, we
release our ASR and AST models and plan to open-source our training code and
strategy in the near future. Moreover, a model trained on 8wh scale training
data is planned to be released later on.","[{'name': 'Junhao Xu'}, {'name': 'Zhenlin Liang'}, {'name': 'Yi Liu'}, {'name': 'Yichao Hu'}, {'name': 'Jian Li'}, {'name': 'Yajun Zheng'}, {'name': 'Meng Cai'}, {'name': 'Hua Wang'}]",2024-08-09T14:43:56Z
http://arxiv.org/abs/2408.05094v1,http://arxiv.org/abs/2408.05094v1,"Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective
  Alignment with Contrastive Prompts","The task of multi-objective alignment aims at balancing and controlling the
different alignment objectives (e.g., helpfulness, harmlessness and honesty) of
large language models to meet the personalized requirements of different users.
However, previous methods tend to train multiple models to deal with various
user preferences, with the number of trained models growing linearly with the
number of alignment objectives and the number of different preferences.
Meanwhile, existing methods are generally poor in extensibility and require
significant re-training for each new alignment objective considered.
Considering the limitation of previous approaches, we propose MCA
(Multi-objective Contrastive Alignemnt), which constructs an expert prompt and
an adversarial prompt for each objective to contrast at the decoding time and
balances the objectives through combining the contrast. Our approach is
verified to be superior to previous methods in obtaining a well-distributed
Pareto front among different alignment objectives.","[{'name': 'Tingchen Fu'}, {'name': 'Yupeng Hou'}, {'name': 'Julian McAuley'}, {'name': 'Rui Yan'}]",2024-08-09T14:36:42Z
http://arxiv.org/abs/2408.05093v2,http://arxiv.org/abs/2408.05093v2,"Order Matters in Hallucination: Reasoning Order as Benchmark and
  Reflexive Prompting for Large-Language-Models","Large language models (LLMs) have generated significant attention since their
inception, finding applications across various academic and industrial domains.
However, these models often suffer from the ""hallucination problem"", where
outputs, though grammatically and logically coherent, lack factual accuracy or
are entirely fabricated. A particularly troubling issue discovered and widely
discussed recently is the numerical comparison error where multiple LLMs
incorrectly infer that ""9.11$>$9.9"". We discovered that the order in which LLMs
generate answers and reasoning impacts their consistency. Specifically, results
vary significantly when an LLM generates an answer first and then provides the
reasoning versus generating the reasoning process first and then the
conclusion. Inspired by this, we propose a new benchmark method for assessing
LLM consistency: comparing responses generated through these two different
approaches. This benchmark effectively identifies instances where LLMs
fabricate answers and subsequently generate justifications. Furthermore, we
introduce a novel and straightforward prompt strategy designed to mitigate this
issue. Experimental results demonstrate that this strategy improves performance
across various LLMs compared to direct questioning. This work not only sheds
light on a critical flaw in LLMs but also offers a practical solution to
enhance their reliability.",[{'name': 'Zikai Xie'}],2024-08-09T14:34:32Z
http://arxiv.org/abs/2408.05086v1,http://arxiv.org/abs/2408.05086v1,"Generating novel experimental hypotheses from language models: A case
  study on cross-dative generalization","Neural network language models (LMs) have been shown to successfully capture
complex linguistic knowledge. However, their utility for understanding language
acquisition is still debated. We contribute to this debate by presenting a case
study where we use LMs as simulated learners to derive novel experimental
hypotheses to be tested with humans. We apply this paradigm to study
cross-dative generalization (CDG): productive generalization of novel verbs
across dative constructions (she pilked me the ball/she pilked the ball to me)
-- acquisition of which is known to involve a large space of contextual
features -- using LMs trained on child-directed speech. We specifically ask:
""what properties of the training exposure facilitate a novel verb's
generalization to the (unmodeled) alternate construction?"" To answer this, we
systematically vary the exposure context in which a novel dative verb occurs in
terms of the properties of the theme and recipient, and then analyze the LMs'
usage of the novel verb in the unmodeled dative construction. We find LMs to
replicate known patterns of children's CDG, as a precondition to exploring
novel hypotheses. Subsequent simulations reveal a nuanced role of the features
of the novel verbs' exposure context on the LMs' CDG. We find CDG to be
facilitated when the first postverbal argument of the exposure context is
pronominal, definite, short, and conforms to the prototypical animacy
expectations of the exposure dative. These patterns are characteristic of
harmonic alignment in datives, where the argument with features ranking higher
on the discourse prominence scale tends to precede the other. This gives rise
to a novel hypothesis that CDG is facilitated insofar as the features of the
exposure context -- in particular, its first postverbal argument -- are
harmonically aligned. We conclude by proposing future experiments that can test
this hypothesis in children.","[{'name': 'Kanishka Misra'}, {'name': 'Najoung Kim'}]",2024-08-09T14:17:36Z
http://arxiv.org/abs/2408.05074v2,http://arxiv.org/abs/2408.05074v2,"RT-Surv: Improving Mortality Prediction After Radiotherapy with Large
  Language Model Structuring of Large-Scale Unstructured Electronic Health
  Records","Accurate patient selection is critical in radiotherapy (RT) to prevent
ineffective treatments. Traditional survival prediction models, relying on
structured data, often lack precision. This study explores the potential of
large language models (LLMs) to structure unstructured electronic health record
(EHR) data, thereby improving survival prediction accuracy through
comprehensive clinical information integration. Data from 34,276 patients
treated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,
encompassing both structured and unstructured data. An open-source LLM was used
to structure the unstructured EHR data via single-shot learning, with its
performance compared against a domain-specific medical LLM and a smaller
variant. Survival prediction models were developed using statistical, machine
learning, and deep learning approaches, incorporating both structured and
LLM-structured data. Clinical experts evaluated the accuracy of the
LLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring
unstructured EHR data without additional training, significantly outperforming
the domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs
were more effective, particularly in extracting clinically relevant features
like general condition and disease extent, which closely correlated with
patient survival. Incorporating LLM-structured clinical features into survival
prediction models significantly improved accuracy, with the C-index of deep
learning models increasing from 0.737 to 0.820. These models also became more
interpretable by emphasizing clinically significant factors. This study shows
that general-domain LLMs, even without specific medical training, can
effectively structure large-scale unstructured EHR data, substantially
enhancing the accuracy and interpretability of clinical predictive models.","[{'name': 'Sangjoon Park'}, {'name': 'Chan Woo Wee'}, {'name': 'Seo Hee Choi'}, {'name': 'Kyung Hwan Kim'}, {'name': 'Jee Suk Chang'}, {'name': 'Hong In Yoon'}, {'name': 'Ik Jae Lee'}, {'name': 'Yong Bae Kim'}, {'name': 'Jaeho Cho'}, {'name': 'Ki Chang Keum'}, {'name': 'Chang Geol Lee'}, {'name': 'Hwa Kyung Byun'}, {'name': 'Woong Sub Koom'}]",2024-08-09T14:02:24Z
http://arxiv.org/abs/2408.05035v1,http://arxiv.org/abs/2408.05035v1,"Examining the Behavior of LLM Architectures Within the Framework of
  Standardized National Exams in Brazil","The Exame Nacional do Ensino M\'edio (ENEM) is a pivotal test for Brazilian
students, required for admission to a significant number of universities in
Brazil. The test consists of four objective high-school level tests on Math,
Humanities, Natural Sciences and Languages, and one writing essay. Students'
answers to the test and to the accompanying socioeconomic status questionnaire
are made public every year (albeit anonymized) due to transparency policies
from the Brazilian Government. In the context of large language models (LLMs),
these data lend themselves nicely to comparing different groups of humans with
AI, as we can have access to human and machine answer distributions. We
leverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4,
and MariTalk, a model trained using Portuguese data, to humans, aiming to
ascertain how their answers relate to real societal groups and what that may
reveal about the model biases. We divide the human groups by using
socioeconomic status (SES), and compare their answer distribution with LLMs for
each question and for the essay. We find no significant biases when comparing
LLM performance to humans on the multiple-choice Brazilian Portuguese tests, as
the distance between model and human answers is mostly determined by the human
accuracy. A similar conclusion is found by looking at the generated text as,
when analyzing the essays, we observe that human and LLM essays differ in a few
key factors, one being the choice of words where model essays were easily
separable from human ones. The texts also differ syntactically, with LLM
generated essays exhibiting, on average, smaller sentences and less thought
units, among other differences. These results suggest that, for Brazilian
Portuguese in the ENEM context, LLM outputs represent no group of humans, being
significantly different from the answers from Brazilian students across all
tests.","[{'name': 'Marcelo Sartori Locatelli'}, {'name': 'Matheus Prado Miranda'}, {'name': 'Igor Joaquim da Silva Costa'}, {'name': 'Matheus Torres Prates'}, {'name': 'Victor Thomé'}, {'name': 'Mateus Zaparoli Monteiro'}, {'name': 'Tomas Lacerda'}, {'name': 'Adriana Pagano'}, {'name': 'Eduardo Rios Neto'}, {'name': 'Wagner Meira Jr.'}, {'name': 'Virgilio Almeida'}]",2024-08-09T12:47:28Z
http://arxiv.org/abs/2408.05024v1,http://arxiv.org/abs/2408.05024v1,MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling,"Guitar tablatures enrich the structure of traditional music notation by
assigning each note to a string and fret of a guitar in a particular tuning,
indicating precisely where to play the note on the instrument. The problem of
generating tablature from a symbolic music representation involves inferring
this string and fret assignment per note across an entire composition or
performance. On the guitar, multiple string-fret assignments are possible for
most pitches, which leads to a large combinatorial space that prevents
exhaustive search approaches. Most modern methods use constraint-based dynamic
programming to minimize some cost function (e.g.\ hand position movement). In
this work, we introduce a novel deep learning solution to symbolic guitar
tablature estimation. We train an encoder-decoder Transformer model in a masked
language modeling paradigm to assign notes to strings. The model is first
pre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on
a curated set of professionally transcribed guitar performances. Given the
subjective nature of assessing tablature quality, we conduct a user study
amongst guitarists, wherein we ask participants to rate the playability of
multiple versions of tablature for the same four-bar excerpt. The results
indicate our system significantly outperforms competing algorithms.","[{'name': 'Drew Edwards'}, {'name': 'Xavier Riley'}, {'name': 'Pedro Sarmento'}, {'name': 'Simon Dixon'}]",2024-08-09T12:25:23Z
http://arxiv.org/abs/2408.05023v1,http://arxiv.org/abs/2408.05023v1,"Investigating a Benchmark for Training-set free Evaluation of Linguistic
  Capabilities in Machine Reading Comprehension","Performance of NLP systems is typically evaluated by collecting a large-scale
dataset by means of crowd-sourcing to train a data-driven model and evaluate it
on a held-out portion of the data. This approach has been shown to suffer from
spurious correlations and the lack of challenging examples that represent the
diversity of natural language. Instead, we examine a framework for evaluating
optimised models in training-set free setting on synthetically generated
challenge sets. We find that despite the simplicity of the generation method,
the data can compete with crowd-sourced datasets with regard to naturalness and
lexical diversity for the purpose of evaluating the linguistic capabilities of
MRC models. We conduct further experiments and show that state-of-the-art
language model-based MRC systems can learn to succeed on the challenge set
correctly, although, without capturing the general notion of the evaluated
phenomenon.","[{'name': 'Viktor Schlegel'}, {'name': 'Goran Nenadic'}, {'name': 'Riza Batista-Navarro'}]",2024-08-09T12:23:36Z
http://arxiv.org/abs/2408.04998v1,http://arxiv.org/abs/2408.04998v1,ProFuser: Progressive Fusion of Large Language Models,"While fusing the capacities and advantages of various large language models
(LLMs) offers a pathway to construct more powerful and versatile models, a
fundamental challenge is to properly select advantageous model during the
training. Existing fusion methods primarily focus on the training mode that
uses cross entropy on ground truth in a teacher-forcing setup to measure a
model's advantage, which may provide limited insight towards model advantage.
In this paper, we introduce a novel approach that enhances the fusion process
by incorporating both the training and inference modes. Our method evaluates
model advantage not only through cross entropy during training but also by
considering inference outputs, providing a more comprehensive assessment. To
combine the two modes effectively, we introduce ProFuser to progressively
transition from inference mode to training mode. To validate ProFuser's
effectiveness, we fused three models, including vicuna-7b-v1.5,
Llama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance
in knowledge, reasoning, and safety compared to baseline methods.","[{'name': 'Tianyuan Shi'}, {'name': 'Fanqi Wan'}, {'name': 'Canbin Huang'}, {'name': 'Xiaojun Quan'}, {'name': 'Chenliang Li'}, {'name': 'Ming Yan'}, {'name': 'Ji Zhang'}]",2024-08-09T11:18:29Z
http://arxiv.org/abs/2408.04983v1,http://arxiv.org/abs/2408.04983v1,"Get Confused Cautiously: Textual Sequence Memorization Erasure with
  Selective Entropy Maximization","Large Language Models (LLMs) have been found to memorize and recite some of
the textual sequences from their training set verbatim, raising broad concerns
about privacy and copyright issues when using LLMs. This Textual Sequence
Memorization (TSM) phenomenon leads to a high demand to regulate LLM output to
prevent it from generating certain memorized text to meet user requirements.
However, our empirical study reveals that existing methods for TSM erasure fail
to forget massive memorized samples without substantially jeopardizing the
model utility. To achieve a better trade-off between the effectiveness of TSM
erasure and model utility in LLMs, our paper proposes a new framework based on
Entropy Maximization with Selective Optimization (EMSO), where the updated
weights are chosen with a novel contrastive gradient metric without any
participation of additional model or data. Our analysis shows that training
with the entropy maximization loss has a more stable optimization process and
better keeps model utility than existing methods. The contrastive gradient
metric localizes the most influential weight for TSM erasure by taking both the
gradient magnitude and direction into consideration. Extensive experiments
across three model scales demonstrate that our method excels in handling
large-scale forgetting requests while preserving model ability in language
generation and reasoning.","[{'name': 'Zhaohan Zhang'}, {'name': 'Ziquan Liu'}, {'name': 'Ioannis Patras'}]",2024-08-09T10:26:11Z
http://arxiv.org/abs/2408.04975v3,http://arxiv.org/abs/2408.04975v3,"\textit{re}CSE: Portable Reshaping Features for Sentence Embedding in
  Self-supervised Contrastive Learning","We propose \textit{re}CSE, a self supervised contrastive learning sentence
representation framework based on feature reshaping. This framework is
different from the current advanced models that use discrete data augmentation
methods, but instead reshapes the input features of the original sentence,
aggregates the global information of each token in the sentence, and alleviates
the common problems of representation polarity and GPU memory consumption
linear increase in current advanced models. In addition, our \textit{re}CSE has
achieved competitive performance in semantic similarity tasks. And the
experiment proves that our proposed feature reshaping method has strong
universality, which can be transplanted to other self supervised contrastive
learning frameworks and enhance their representation ability, even achieving
state-of-the-art performance. Our code is available at
https://github.com/heavenhellchen/reCSE.","[{'name': 'Fufangchen Zhao'}, {'name': 'Jian Gao'}, {'name': 'Danfeng Yan'}]",2024-08-09T09:56:30Z
http://arxiv.org/abs/2408.04965v1,http://arxiv.org/abs/2408.04965v1,"Generalisation First, Memorisation Second? Memorisation Localisation for
  Natural Language Classification Tasks","Memorisation is a natural part of learning from real-world data: neural
models pick up on atypical input-output combinations and store those training
examples in their parameter space. That this happens is well-known, but how and
where are questions that remain largely unanswered. Given a multi-layered
neural model, where does memorisation occur in the millions of parameters?
Related work reports conflicting findings: a dominant hypothesis based on image
classification is that lower layers learn generalisable features and that
deeper layers specialise and memorise. Work from NLP suggests this does not
apply to language models, but has been mainly focused on memorisation of facts.
We expand the scope of the localisation question to 12 natural language
classification tasks and apply 4 memorisation localisation techniques. Our
results indicate that memorisation is a gradual process rather than a localised
one, establish that memorisation is task-dependent, and give nuance to the
generalisation first, memorisation second hypothesis.","[{'name': 'Verna Dankers'}, {'name': 'Ivan Titov'}]",2024-08-09T09:30:57Z
http://arxiv.org/abs/2408.04948v1,http://arxiv.org/abs/2408.04948v1,"HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented
  Generation for Efficient Information Extraction","Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain","[{'name': 'Bhaskarjit Sarmah'}, {'name': 'Benika Hall'}, {'name': 'Rohan Rao'}, {'name': 'Sunil Patel'}, {'name': 'Stefano Pasquali'}, {'name': 'Dhagash Mehta'}]",2024-08-09T09:07:48Z
http://arxiv.org/abs/2408.04941v1,http://arxiv.org/abs/2408.04941v1,Quantitative Information Extraction from Humanitarian Documents,"Humanitarian action is accompanied by a mass of reports, summaries, news, and
other documents. To guide its activities, important information must be quickly
extracted from such free-text resources. Quantities, such as the number of
people affected, amount of aid distributed, or the extent of infrastructure
damage, are central to emergency response and anticipatory action. In this
work, we contribute an annotated dataset for the humanitarian domain for the
extraction of such quantitative information, along side its important context,
including units it refers to, any modifiers, and the relevant event. Further,
we develop a custom Natural Language Processing pipeline to extract the
quantities alongside their units, and evaluate it in comparison to baseline and
recent literature. The proposed model achieves a consistent improvement in the
performance, especially in the documents pertaining to the Dominican Republic
and select African countries. We make the dataset and code available to the
research community to continue the improvement of NLP tools for the
humanitarian domain.","[{'name': 'Daniele Liberatore'}, {'name': 'Kyriaki Kalimeri'}, {'name': 'Derya Sever'}, {'name': 'Yelena Mejova'}]",2024-08-09T08:46:38Z
http://arxiv.org/abs/2408.04909v1,http://arxiv.org/abs/2408.04909v1,"Surveying the Landscape of Image Captioning Evaluation: A Comprehensive
  Taxonomy and Novel Ensemble Method","The task of image captioning has recently been gaining popularity, and with
it the complex task of evaluating the quality of image captioning models. In
this work, we present the first survey and taxonomy of over 70 different image
captioning metrics and their usage in hundreds of papers. We find that despite
the diversity of proposed metrics, the vast majority of studies rely on only
five popular metrics, which we show to be weakly correlated with human
judgements. Instead, we propose EnsembEval -- an ensemble of evaluation methods
achieving the highest reported correlation with human judgements across 5 image
captioning datasets, showing there is a lot of room for improvement by
leveraging a diverse set of metrics.","[{'name': 'Uri Berger'}, {'name': 'Gabriel Stanovsky'}, {'name': 'Omri Abend'}, {'name': 'Lea Frermann'}]",2024-08-09T07:31:06Z
http://arxiv.org/abs/2408.04906v1,http://arxiv.org/abs/2408.04906v1,Towards a Generative Approach for Emotion Detection and Reasoning,"Large language models (LLMs) have demonstrated impressive performance in
mathematical and commonsense reasoning tasks using chain-of-thought (CoT)
prompting techniques. But can they perform emotional reasoning by concatenating
`Let's think step-by-step' to the input prompt? In this paper we investigate
this question along with introducing a novel approach to zero-shot emotion
detection and emotional reasoning using LLMs. Existing state of the art
zero-shot approaches rely on textual entailment models to choose the most
appropriate emotion label for an input text. We argue that this strongly
restricts the model to a fixed set of labels which may not be suitable or
sufficient for many applications where emotion analysis is required. Instead,
we propose framing the problem of emotion analysis as a generative
question-answering (QA) task. Our approach uses a two step methodology of
generating relevant context or background knowledge to answer the emotion
detection question step-by-step. Our paper is the first work on using a
generative approach to jointly address the tasks of emotion detection and
emotional reasoning for texts. We evaluate our approach on two popular emotion
detection datasets and also release the fine-grained emotion labels and
explanations for further training and fine-tuning of emotional reasoning
systems.","[{'name': 'Ankita Bhaumik'}, {'name': 'Tomek Strzalkowski'}]",2024-08-09T07:20:15Z
http://arxiv.org/abs/2408.04905v1,http://arxiv.org/abs/2408.04905v1,"GlitchProber: Advancing Effective Detection and Mitigation of Glitch
  Tokens in Large Language Models","Large language models (LLMs) have achieved unprecedented success in the field
of natural language processing. However, the black-box nature of their internal
mechanisms has brought many concerns about their trustworthiness and
interpretability. Recent research has discovered a class of abnormal tokens in
the model's vocabulary space and named them ""glitch tokens"". Those tokens, once
included in the input, may induce the model to produce incorrect, irrelevant,
or even harmful results, drastically undermining the reliability and
practicality of LLMs.
  In this work, we aim to enhance the understanding of glitch tokens and
propose techniques for their detection and mitigation. We first reveal the
characteristic features induced by glitch tokens on LLMs, which are evidenced
by significant deviations in the distributions of attention patterns and
dynamic information from intermediate model layers. Based on the insights, we
develop GlitchProber, a tool for efficient glitch token detection and
mitigation. GlitchProber utilizes small-scale sampling, principal component
analysis for accelerated feature extraction, and a simple classifier for
efficient vocabulary screening. Taking one step further, GlitchProber rectifies
abnormal model intermediate layer values to mitigate the destructive effects of
glitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber
demonstrates higher efficiency, precision, and recall compared to existing
approaches, with an average F1 score of 0.86 and an average repair rate of
50.06%. GlitchProber unveils a novel path to address the challenges posed by
glitch tokens and inspires future research toward more robust and interpretable
LLMs.","[{'name': 'Zhibo Zhang'}, {'name': 'Wuxia Bai'}, {'name': 'Yuxi Li'}, {'name': 'Mark Huasong Meng'}, {'name': 'Kailong Wang'}, {'name': 'Ling Shi'}, {'name': 'Li Li'}, {'name': 'Jun Wang'}, {'name': 'Haoyu Wang'}]",2024-08-09T07:19:53Z
http://arxiv.org/abs/2408.04900v1,http://arxiv.org/abs/2408.04900v1,"Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural
  Communication in Codenames","Cultural differences in common ground may result in pragmatic failure and
misunderstandings during communication. We develop our method Rational Speech
Acts for Cross-Cultural Communication (RSA+C3) to resolve cross-cultural
differences in common ground. To measure the success of our method, we study
RSA+C3 in the collaborative referential game of Codenames Duet and show that
our method successfully improves collaboration between simulated players of
different cultures. Our contributions are threefold: (1) creating Codenames
players using contrastive learning of an embedding space and LLM prompting that
are aligned with human patterns of play, (2) studying culturally induced
differences in common ground reflected in our trained models, and (3)
demonstrating that our method RSA+C3 can ease cross-cultural communication in
gameplay by inferring sociocultural context from interaction. Our code is
publicly available at github.com/icwhite/codenames.","[{'name': 'Isadora White'}, {'name': 'Sashrika Pandey'}, {'name': 'Michelle Pan'}]",2024-08-09T07:02:18Z
http://arxiv.org/abs/2408.04873v1,http://arxiv.org/abs/2408.04873v1,Unsupervised Episode Detection for Large-Scale News Events,"Episodic structures are inherently interpretable and adaptable to evolving
large-scale key events. However, state-of-the-art automatic event detection
methods overlook event episodes and, therefore, struggle with these crucial
characteristics. This paper introduces a novel task, episode detection, aimed
at identifying episodes from a news corpus containing key event articles. An
episode describes a cohesive cluster of core entities (e.g., ""protesters"",
""police"") performing actions at a specific time and location. Furthermore, an
episode is a significant part of a larger group of episodes under a particular
key event. Automatically detecting episodes is challenging because, unlike key
events and atomic actions, we cannot rely on explicit mentions of times and
locations to distinguish between episodes or use semantic similarity to merge
inconsistent episode co-references. To address these challenges, we introduce
EpiMine, an unsupervised episode detection framework that (1) automatically
identifies the most salient, key-event-relevant terms and segments, (2)
determines candidate episodes in an article based on natural episodic
partitions estimated through shifts in discriminative term combinations, and
(3) refines and forms final episode clusters using large language model-based
reasoning on the candidate episodes. We construct three diverse, real-world
event datasets annotated at the episode level. EpiMine outperforms all
baselines on these datasets by an average 59.2% increase across all metrics.","[{'name': 'Priyanka Kargupta'}, {'name': 'Yunyi Zhang'}, {'name': 'Yizhu Jiao'}, {'name': 'Siru Ouyang'}, {'name': 'Jiawei Han'}]",2024-08-09T05:26:31Z
http://arxiv.org/abs/2408.04872v1,http://arxiv.org/abs/2408.04872v1,"SCOI: Syntax-augmented Coverage-based In-context Example Selection for
  Machine Translation","In-context learning (ICL) greatly improves the performance of large language
models (LLMs) on various down-stream tasks, where the improvement highly
depends on the quality of demonstrations. In this work, we introduce syntactic
knowledge to select better in-context examples for machine translation (MT). We
propose a new strategy, namely Syntax-augmented COverage-based In-context
example selection (SCOI), leveraging the deep syntactic structure beyond
conventional word matching. Specifically, we measure the set-level syntactic
coverage by computing the coverage of polynomial terms with the help of a
simplified tree-to-polynomial algorithm, and lexical coverage using word
overlap. Furthermore, we devise an alternate selection approach to combine both
coverage measures, taking advantage of syntactic and lexical information. We
conduct experiments with two multi-lingual LLMs on six translation directions.
Empirical results show that our proposed SCOI obtains the highest average COMET
score among all learning-free methods, indicating that combining syntactic and
lexical coverage successfully helps to select better in-context examples for
MT.","[{'name': 'Chenming Tang'}, {'name': 'Zhixiang Wang'}, {'name': 'Yunfang Wu'}]",2024-08-09T05:25:17Z
http://arxiv.org/abs/2408.08894v1,http://arxiv.org/abs/2408.08894v1,"Enhancing Exploratory Learning through Exploratory Search with the
  Emergence of Large Language Models","In the information era, how learners find, evaluate, and effectively use
information has become a challenging issue, especially with the added
complexity of large language models (LLMs) that have further confused learners
in their information retrieval and search activities. This study attempts to
unpack this complexity by combining exploratory search strategies with the
theories of exploratory learning to form a new theoretical model of exploratory
learning from the perspective of students' learning. Our work adapts Kolb's
learning model by incorporating high-frequency exploration and feedback loops,
aiming to promote deep cognitive and higher-order cognitive skill development
in students. Additionally, this paper discusses and suggests how advanced LLMs
integrated into information retrieval and information theory can support
students in their exploratory searches, contributing theoretically to promoting
student-computer interaction and supporting their learning journeys in the new
era with LLMs.","[{'name': 'Yiming Luo'}, {'name': 'Patrick Cheong-Iao'}, {'name': 'Shanton Chang'}]",2024-08-09T04:30:16Z
http://arxiv.org/abs/2408.04852v1,http://arxiv.org/abs/2408.04852v1,MSG-Chart: Multimodal Scene Graph for ChartQA,"Automatic Chart Question Answering (ChartQA) is challenging due to the
complex distribution of chart elements with patterns of the underlying data not
explicitly displayed in charts. To address this challenge, we design a joint
multimodal scene graph for charts to explicitly represent the relationships
between chart elements and their patterns. Our proposed multimodal scene graph
includes a visual graph and a textual graph to jointly capture the structural
and semantical knowledge from the chart. This graph module can be easily
integrated with different vision transformers as inductive bias. Our
experiments demonstrate that incorporating the proposed graph module enhances
the understanding of charts' elements' structure and semantics, thereby
improving performance on publicly available benchmarks, ChartQA and OpenCQA.","[{'name': 'Yue Dai'}, {'name': 'Soyeon Caren Han'}, {'name': 'Wei Liu'}]",2024-08-09T04:11:23Z
http://arxiv.org/abs/2408.04849v1,http://arxiv.org/abs/2408.04849v1,"Ensemble BERT: A student social network text sentiment classification
  model based on ensemble learning and BERT architecture","The mental health assessment of middle school students has always been one of
the focuses in the field of education. This paper introduces a new ensemble
learning network based on BERT, employing the concept of enhancing model
performance by integrating multiple classifiers. We trained a range of
BERT-based learners, which combined using the majority voting method. We
collect social network text data of middle school students through China's
Weibo and apply the method to the task of classifying emotional tendencies in
middle school students' social network texts. Experimental results suggest that
the ensemble learning network has a better performance than the base model and
the performance of the ensemble learning model, consisting of three
single-layer BERT models, is barely the same as a three-layer BERT model but
requires 11.58% more training time. Therefore, in terms of balancing prediction
effect and efficiency, the deeper BERT network should be preferred for
training. However, for interpretability, network ensembles can provide
acceptable solutions.","[{'name': 'Kai Jiang'}, {'name': 'Honghao Yang'}, {'name': 'Yuexian Wang'}, {'name': 'Qianru Chen'}, {'name': 'Yiming Luo'}]",2024-08-09T03:57:31Z
http://arxiv.org/abs/2408.04840v2,http://arxiv.org/abs/2408.04840v2,"mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal
  Large Language Models","Multi-modal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in executing instructions for a variety of single-image tasks.
Despite this progress, significant challenges remain in modeling long image
sequences. In this work, we introduce the versatile multi-modal large language
model, mPLUG-Owl3, which enhances the capability for long image-sequence
understanding in scenarios that incorporate retrieved image-text knowledge,
interleaved image-text, and lengthy videos. Specifically, we propose novel
hyper attention blocks to efficiently integrate vision and language into a
common language-guided semantic space, thereby facilitating the processing of
extended multi-image scenarios. Extensive experimental results suggest that
mPLUG-Owl3 achieves state-of-the-art performance among models with a similar
size on single-image, multi-image, and video benchmarks. Moreover, we propose a
challenging long visual sequence evaluation named Distractor Resistance to
assess the ability of models to maintain focus amidst distractions. Finally,
with the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance
on ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to
the development of more efficient and powerful multimodal large language
models.","[{'name': 'Jiabo Ye'}, {'name': 'Haiyang Xu'}, {'name': 'Haowei Liu'}, {'name': 'Anwen Hu'}, {'name': 'Ming Yan'}, {'name': 'Qi Qian'}, {'name': 'Ji Zhang'}, {'name': 'Fei Huang'}, {'name': 'Jingren Zhou'}]",2024-08-09T03:25:42Z
http://arxiv.org/abs/2408.04820v1,http://arxiv.org/abs/2408.04820v1,Natural Language Outlines for Code: Literate Programming in the LLM Era,"We propose using natural language outlines as a novel modality and
interaction surface for providing AI assistance to developers throughout the
software development process. An NL outline for a code function comprises
multiple statements written in concise prose, which partition the code and
summarize its main ideas in the style of literate programming. Crucially, we
find that modern LLMs can generate accurate and high-quality NL outlines in
practice. Moreover, NL outlines enable a bidirectional sync between code and
NL, allowing changes in one to be automatically reflected in the other. We
discuss many use cases for NL outlines: they can accelerate understanding and
navigation of code and diffs, simplify code maintenance, augment code search,
steer code generation, and more. We then propose and compare multiple LLM
prompting techniques for generating outlines and ask professional developers to
judge outline quality. Finally, we present two case studies applying NL
outlines toward code review and the difficult task of malware detection.","[{'name': 'Kensen Shi'}, {'name': 'Deniz Altınbüken'}, {'name': 'Saswat Anand'}, {'name': 'Mihai Christodorescu'}, {'name': 'Katja Grünwedel'}, {'name': 'Alexa Koenings'}, {'name': 'Sai Naidu'}, {'name': 'Anurag Pathak'}, {'name': 'Marc Rasi'}, {'name': 'Fredde Ribeiro'}, {'name': 'Brandon Ruffin'}, {'name': 'Siddhant Sanyam'}, {'name': 'Maxim Tabachnyk'}, {'name': 'Sara Toth'}, {'name': 'Roy Tu'}, {'name': 'Tobias Welp'}, {'name': 'Pengcheng Yin'}, {'name': 'Manzil Zaheer'}, {'name': 'Satish Chandra'}, {'name': 'Charles Sutton'}]",2024-08-09T02:22:51Z
http://arxiv.org/abs/2408.04816v1,http://arxiv.org/abs/2408.04816v1,"FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt
  Optimization Across Tokenizers","The widespread use of large language models has resulted in a multitude of
tokenizers and embedding spaces, making knowledge transfer in prompt discovery
tasks difficult. In this work, we propose FUSE (Flexible Unification of
Semantic Embeddings), an inexpensive approach to approximating an adapter layer
that maps from one model's textual embedding space to another, even across
different tokenizers. We introduce a third-order tensor-based representation of
a model's embedding space that aligns semantic embeddings that have been split
apart by different tokenizers, and use this representation to derive an
approximation of the gradient of one model's outputs with respect to another
model's embedding space. We show the efficacy of our approach via
multi-objective optimization over vision-language and causal language models
for image captioning and sentiment-based image captioning.","[{'name': 'Joshua Nathaniel Williams'}, {'name': 'J. Zico Kolter'}]",2024-08-09T02:16:37Z
http://arxiv.org/abs/2408.04775v1,http://arxiv.org/abs/2408.04775v1,"Hybrid Student-Teacher Large Language Model Refinement for Cancer
  Toxicity Symptom Extraction","Large Language Models (LLMs) offer significant potential for clinical symptom
extraction, but their deployment in healthcare settings is constrained by
privacy concerns, computational limitations, and operational costs. This study
investigates the optimization of compact LLMs for cancer toxicity symptom
extraction using a novel iterative refinement approach. We employ a
student-teacher architecture, utilizing Zephyr-7b-beta and Phi3-mini-128 as
student models and GPT-4o as the teacher, to dynamically select between prompt
refinement, Retrieval-Augmented Generation (RAG), and fine-tuning strategies.
Our experiments on 294 clinical notes covering 12 post-radiotherapy toxicity
symptoms demonstrate the effectiveness of this approach. The RAG method proved
most efficient, improving average accuracy scores from 0.32 to 0.73 for
Zephyr-7b-beta and from 0.40 to 0.87 for Phi3-mini-128 during refinement. In
the test set, both models showed an approximate 0.20 increase in accuracy
across symptoms. Notably, this improvement was achieved at a cost 45 times
lower than GPT-4o for Zephyr and 79 times lower for Phi-3. These results
highlight the potential of iterative refinement techniques in enhancing the
capabilities of compact LLMs for clinical applications, offering a balance
between performance, cost-effectiveness, and privacy preservation in healthcare
settings.","[{'name': 'Reza Khanmohammadi'}, {'name': 'Ahmed I. Ghanem'}, {'name': 'Kyle Verdecchia'}, {'name': 'Ryan Hall'}, {'name': 'Mohamed Elshaikh'}, {'name': 'Benjamin Movsas'}, {'name': 'Hassan Bagher-Ebadian'}, {'name': 'Bing Luo'}, {'name': 'Indrin J. Chetty'}, {'name': 'Tuka Alhanai'}, {'name': 'Kundan Thind'}, {'name': 'Mohammad M. Ghassemi'}]",2024-08-08T22:18:01Z
http://arxiv.org/abs/2408.04723v1,http://arxiv.org/abs/2408.04723v1,Survey: Transformer-based Models in Data Modality Conversion,"Transformers have made significant strides across various artificial
intelligence domains, including natural language processing, computer vision,
and audio processing. This success has naturally garnered considerable interest
from both academic and industry researchers. Consequently, numerous Transformer
variants (often referred to as X-formers) have been developed for these fields.
However, a thorough and systematic review of these modality-specific
conversions remains lacking. Modality Conversion involves the transformation of
data from one form of representation to another, mimicking the way humans
integrate and interpret sensory information. This paper provides a
comprehensive review of transformer-based models applied to the primary
modalities of text, vision, and speech, discussing their architectures,
conversion methodologies, and applications. By synthesizing the literature on
modality conversion, this survey aims to underline the versatility and
scalability of transformers in advancing AI-driven content generation and
understanding.","[{'name': 'Elyas Rashno'}, {'name': 'Amir Eskandari'}, {'name': 'Aman Anand'}, {'name': 'Farhana Zulkernine'}]",2024-08-08T18:39:14Z
http://arxiv.org/abs/2408.04632v1,http://arxiv.org/abs/2408.04632v1,Arctic-TILT. Business Document Understanding at Sub-Billion Scale,"The vast portion of workloads employing LLMs involves answering questions
grounded on PDF or scan content. We introduce the Arctic-TILT achieving
accuracy on par with models 1000$\times$ its size on these use cases. It can be
fine-tuned and deployed on a single 24GB GPU, lowering operational costs while
processing Visually Rich Documents with up to 400k tokens. The model
establishes state-of-the-art results on seven diverse Document Understanding
benchmarks, as well as provides reliable confidence scores and quick inference,
which are essential for processing files in large-scale or time-sensitive
enterprise environments.","[{'name': 'Łukasz Borchmann'}, {'name': 'Michał Pietruszka'}, {'name': 'Wojciech Jaśkowski'}, {'name': 'Dawid Jurkiewicz'}, {'name': 'Piotr Halama'}, {'name': 'Paweł Józiak'}, {'name': 'Łukasz Garncarek'}, {'name': 'Paweł Liskowski'}, {'name': 'Karolina Szyndler'}, {'name': 'Andrzej Gretkowski'}, {'name': 'Julita Ołtusek'}, {'name': 'Gabriela Nowakowska'}, {'name': 'Artur Zawłocki'}, {'name': 'Łukasz Duhr'}, {'name': 'Paweł Dyda'}, {'name': 'Michał Turski'}]",2024-08-08T17:59:46Z
http://arxiv.org/abs/2408.04628v1,http://arxiv.org/abs/2408.04628v1,"LogogramNLP: Comparing Visual and Textual Representations of Ancient
  Logographic Writing Systems for NLP","Standard natural language processing (NLP) pipelines operate on symbolic
representations of language, which typically consist of sequences of discrete
tokens. However, creating an analogous representation for ancient logographic
writing systems is an extremely labor intensive process that requires expert
knowledge. At present, a large portion of logographic data persists in a purely
visual form due to the absence of transcription -- this issue poses a
bottleneck for researchers seeking to apply NLP toolkits to study ancient
logographic languages: most of the relevant data are images of writing.
  This paper investigates whether direct processing of visual representations
of language offers a potential solution. We introduce LogogramNLP, the first
benchmark enabling NLP analysis of ancient logographic languages, featuring
both transcribed and visual datasets for four writing systems along with
annotations for tasks like classification, translation, and parsing. Our
experiments compare systems that employ recent visual and text encoding
strategies as backbones. The results demonstrate that visual representations
outperform textual representations for some investigated tasks, suggesting that
visual processing pipelines may unlock a large amount of cultural heritage data
of logographic languages for NLP-based analyses.","[{'name': 'Danlu Chen'}, {'name': 'Freda Shi'}, {'name': 'Aditi Agarwal'}, {'name': 'Jacobo Myerston'}, {'name': 'Taylor Berg-Kirkpatrick'}]",2024-08-08T17:58:06Z
http://arxiv.org/abs/2408.04619v1,http://arxiv.org/abs/2408.04619v1,Transformer Explainer: Interactive Learning of Text-Generative Models,"Transformers have revolutionized machine learning, yet their inner workings
remain opaque to many. We present Transformer Explainer, an interactive
visualization tool designed for non-experts to learn about Transformers through
the GPT-2 model. Our tool helps users understand complex Transformer concepts
by integrating a model overview and enabling smooth transitions across
abstraction levels of mathematical operations and model structures. It runs a
live GPT-2 instance locally in the user's browser, empowering users to
experiment with their own input and observe in real-time how the internal
components and parameters of the Transformer work together to predict the next
tokens. Our tool requires no installation or special hardware, broadening the
public's education access to modern generative AI techniques. Our open-sourced
tool is available at https://poloclub.github.io/transformer-explainer/. A video
demo is available at https://youtu.be/ECR4oAwocjs.","[{'name': 'Aeree Cho'}, {'name': 'Grace C. Kim'}, {'name': 'Alexander Karpekov'}, {'name': 'Alec Helbling'}, {'name': 'Zijie J. Wang'}, {'name': 'Seongmin Lee'}, {'name': 'Benjamin Hoover'}, {'name': 'Duen Horng Chau'}]",2024-08-08T17:49:07Z
http://arxiv.org/abs/2408.04614v2,http://arxiv.org/abs/2408.04614v2,Better Alignment with Instruction Back-and-Forth Translation,"We propose a new method, instruction back-and-forth translation, to construct
high-quality synthetic data grounded in world knowledge for aligning large
language models (LLMs). Given documents from a web corpus, we generate and
curate synthetic instructions using the backtranslation approach proposed by Li
et al.(2023a), and rewrite the responses to improve their quality further based
on the initial documents. Fine-tuning with the resulting (backtranslated
instruction, rewritten response) pairs yields higher win rates on AlpacaEval
than using other common instruction datasets such as Humpback, ShareGPT, Open
Orca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the
responses with an LLM outperforms direct distillation, and the two generated
text distributions exhibit significant distinction in embedding space. Further
analysis shows that our backtranslated instructions are of higher quality than
other sources of synthetic instructions, while our responses are more diverse
and complex than those obtained from distillation. Overall we find that
instruction back-and-forth translation combines the best of both worlds --
making use of the information diversity and quantity found on the web, while
ensuring the quality of the responses which is necessary for effective
alignment.","[{'name': 'Thao Nguyen'}, {'name': 'Jeffrey Li'}, {'name': 'Sewoong Oh'}, {'name': 'Ludwig Schmidt'}, {'name': 'Jason Weston'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Xian Li'}]",2024-08-08T17:42:32Z
http://arxiv.org/abs/2408.04596v1,http://arxiv.org/abs/2408.04596v1,"Code-switching in text and speech reveals information-theoretic audience
  design","In this work, we use language modeling to investigate the factors that
influence code-switching. Code-switching occurs when a speaker alternates
between one language variety (the primary language) and another (the secondary
language), and is widely observed in multilingual contexts. Recent work has
shown that code-switching is often correlated with areas of high information
load in the primary language, but it is unclear whether high primary language
load only makes the secondary language relatively easier to produce at
code-switching points (speaker-driven code-switching), or whether
code-switching is additionally used by speakers to signal the need for greater
attention on the part of listeners (audience-driven code-switching). In this
paper, we use bilingual Chinese-English online forum posts and transcripts of
spontaneous Chinese-English speech to replicate prior findings that high
primary language (Chinese) information load is correlated with switches to the
secondary language (English). We then demonstrate that the information load of
the English productions is even higher than that of meaning equivalent Chinese
alternatives, and these are therefore not easier to produce, providing evidence
of audience-driven influences in code-switching at the level of the
communication channel, not just at the sociolinguistic level, in both writing
and speech.","[{'name': 'Debasmita Bhattacharya'}, {'name': 'Marten van Schijndel'}]",2024-08-08T17:14:12Z
http://arxiv.org/abs/2408.04585v2,http://arxiv.org/abs/2408.04585v2,"Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency,
  Performance, and Adversarial Robustness","With the increasing demand for practical applications of Large Language
Models (LLMs), many attention-efficient models have been developed to balance
performance and computational cost. However, the adversarial robustness of
these models remains under-explored. In this work, we design a framework to
investigate the trade-off between efficiency, performance, and adversarial
robustness of LLMs by comparing three prominent models with varying levels of
complexity and efficiency -- Transformer++, Gated Linear Attention (GLA)
Transformer, and MatMul-Free LM -- utilizing the GLUE and AdvGLUE datasets. The
AdvGLUE dataset extends the GLUE dataset with adversarial samples designed to
challenge model robustness. Our results show that while the GLA Transformer and
MatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate
higher efficiency and either superior or comparative robustness on AdvGLUE
tasks compared to Transformer++ across different attack levels. These findings
highlight the potential of simplified architectures to achieve a compelling
balance between efficiency, performance, and adversarial robustness, offering
valuable insights for applications where resource constraints and resilience to
adversarial attacks are critical.","[{'name': 'Xiaojing Fan'}, {'name': 'Chunliang Tao'}]",2024-08-08T16:54:40Z
http://arxiv.org/abs/2408.04575v2,http://arxiv.org/abs/2408.04575v2,SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals,"Explainable Artificial Intelligence (XAI) plays a crucial role in enhancing
the transparency and accountability of AI models, particularly in natural
language processing (NLP) tasks. However, popular XAI methods such as LIME and
SHAP have been found to be unstable and potentially misleading, underscoring
the need for a standardized evaluation approach. This paper introduces SCENE
(Soft Counterfactual Evaluation for Natural language Explainability), a novel
evaluation method that leverages large language models (LLMs) to generate Soft
Counterfactual explanations in a zero-shot manner. By focusing on token-based
substitutions, SCENE creates contextually appropriate and semantically
meaningful Soft Counterfactuals without extensive fine-tuning. SCENE adopts
Validitysoft and Csoft metrics to assess the effectiveness of model-agnostic
XAI methods in text classification tasks. Applied to CNN, RNN, and Transformer
architectures, SCENE provides valuable insights into the strengths and
limitations of various XAI techniques.","[{'name': 'Haoran Zheng'}, {'name': 'Utku Pamuksuz'}]",2024-08-08T16:36:24Z
http://arxiv.org/abs/2408.04568v1,http://arxiv.org/abs/2408.04568v1,"Learning Fine-Grained Grounded Citations for Attributed Large Language
  Models","Despite the impressive performance on information-seeking tasks, large
language models (LLMs) still struggle with hallucinations. Attributed LLMs,
which augment generated text with in-line citations, have shown potential in
mitigating hallucinations and improving verifiability. However, current
approaches suffer from suboptimal citation quality due to their reliance on
in-context learning. Furthermore, the practice of citing only coarse document
identifiers makes it challenging for users to perform fine-grained
verification. In this work, we introduce FRONT, a training framework designed
to teach LLMs to generate Fine-Grained Grounded Citations. By grounding model
outputs in fine-grained supporting quotes, these quotes guide the generation of
grounded and consistent responses, not only improving citation quality but also
facilitating fine-grained verification. Experiments on the ALCE benchmark
demonstrate the efficacy of FRONT in generating superior grounded responses and
highly supportive citations. With LLaMA-2-7B, the framework significantly
outperforms all the baselines, achieving an average of 14.21% improvement in
citation quality across all datasets, even surpassing ChatGPT.","[{'name': 'Lei Huang'}, {'name': 'Xiaocheng Feng'}, {'name': 'Weitao Ma'}, {'name': 'Yuxuan Gu'}, {'name': 'Weihong Zhong'}, {'name': 'Xiachong Feng'}, {'name': 'Weijiang Yu'}, {'name': 'Weihua Peng'}, {'name': 'Duyu Tang'}, {'name': 'Dandan Tu'}, {'name': 'Bing Qin'}]",2024-08-08T16:28:22Z
http://arxiv.org/abs/2408.04693v1,http://arxiv.org/abs/2408.04693v1,Understanding the Performance and Estimating the Cost of LLM Fine-Tuning,"Due to the cost-prohibitive nature of training Large Language Models (LLMs),
fine-tuning has emerged as an attractive alternative for specializing LLMs for
specific tasks using limited compute resources in a cost-effective manner. In
this paper, we characterize sparse Mixture of Experts (MoE) based LLM
fine-tuning to understand their accuracy and runtime performance on a single
GPU. Our evaluation provides unique insights into the training efficacy of
sparse and dense versions of MoE models, as well as their runtime
characteristics, including maximum batch size, execution time breakdown,
end-to-end throughput, GPU hardware utilization, and load distribution. Our
study identifies the optimization of the MoE layer as crucial for further
improving the performance of LLM fine-tuning. Using our profiling results, we
also develop and validate an analytical model to estimate the cost of LLM
fine-tuning on the cloud. This model, based on parameters of the model and GPU
architecture, estimates LLM throughput and the cost of training, aiding
practitioners in industry and academia to budget the cost of fine-tuning a
specific model.","[{'name': 'Yuchen Xia'}, {'name': 'Jiho Kim'}, {'name': 'Yuhan Chen'}, {'name': 'Haojie Ye'}, {'name': 'Souvik Kundu'}, {'name': 'Cong Hao'}, {'name': 'Nishil Talati'}]",2024-08-08T16:26:07Z
http://arxiv.org/abs/2408.04560v1,http://arxiv.org/abs/2408.04560v1,Conversational Prompt Engineering,"Prompts are how humans communicate with LLMs. Informative prompts are
essential for guiding LLMs to produce the desired output. However, prompt
engineering is often tedious and time-consuming, requiring significant
expertise, limiting its widespread use. We propose Conversational Prompt
Engineering (CPE), a user-friendly tool that helps users create personalized
prompts for their specific tasks. CPE uses a chat model to briefly interact
with users, helping them articulate their output preferences and integrating
these into the prompt. The process includes two main stages: first, the model
uses user-provided unlabeled data to generate data-driven questions and utilize
user responses to shape the initial instruction. Then, the model shares the
outputs generated by the instruction and uses user feedback to further refine
the instruction and the outputs. The final result is a few-shot prompt, where
the outputs approved by the user serve as few-shot examples. A user study on
summarization tasks demonstrates the value of CPE in creating personalized,
high-performing prompts. The results suggest that the zero-shot prompt obtained
is comparable to its - much longer - few-shot counterpart, indicating
significant savings in scenarios involving repetitive tasks with large text
volumes.","[{'name': 'Liat Ein-Dor'}, {'name': 'Orith Toledo-Ronen'}, {'name': 'Artem Spector'}, {'name': 'Shai Gretz'}, {'name': 'Lena Dankin'}, {'name': 'Alon Halfon'}, {'name': 'Yoav Katz'}, {'name': 'Noam Slonim'}]",2024-08-08T16:18:39Z
http://arxiv.org/abs/2408.04556v1,http://arxiv.org/abs/2408.04556v1,"Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of
  Large Language Models","Large language models (LLMs) have exhibited remarkable proficiency across a
diverse array of natural language processing (NLP) tasks. However, adapting
LLMs to downstream applications typically necessitates computationally
intensive and memory-demanding fine-tuning procedures. To mitigate these
burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a
promising approach to tailor LLMs with minimal computational overhead. While
PEFT methods offer substantial advantages, they do not fully address the
pervasive issue of bias propagation from pre-training data. In this work, we
introduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method
designed to counteract bias inheritance. BA-LoRA incorporates three distinct
regularization terms: (1) consistency regularizer, (2) diversity regularizer,
and (3) singular vector decomposition regularizer. These regularizers
collectively aim to improve the generative models' consistency, diversity, and
generalization capabilities during the fine-tuning process. Through extensive
experiments on a variety of natural language understanding (NLU) and natural
language generation (NLG) tasks, employing prominent LLMs such as LLaMA,
Mistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of
LoRA and its state-of-the-art variants. Moreover, our method effectively
mitigates the deleterious effects of pre-training bias, leading to more
reliable and robust model outputs. The code is available at
https://github.com/cyp-jlu-ai/BA-LoRA.","[{'name': 'Yupeng Chang'}, {'name': 'Yi Chang'}, {'name': 'Yuan Wu'}]",2024-08-08T16:13:26Z
http://arxiv.org/abs/2408.04522v1,http://arxiv.org/abs/2408.04522v1,"Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large
  Language Models","As diverse linguistic communities and users adopt large language models
(LLMs), assessing their safety across languages becomes critical. Despite
ongoing efforts to make LLMs safe, they can still be made to behave unsafely
with jailbreaking, a technique in which models are prompted to act outside
their operational guidelines. Research on LLM safety and jailbreaking, however,
has so far mostly focused on English, limiting our understanding of LLM safety
in other languages. We contribute towards closing this gap by investigating the
effectiveness of many-shot jailbreaking, where models are prompted with unsafe
demonstrations to induce unsafe behaviour, in Italian. To enable our analysis,
we create a new dataset of unsafe Italian question-answer pairs. With this
dataset, we identify clear safety vulnerabilities in four families of
open-weight LLMs. We find that the models exhibit unsafe behaviors even when
prompted with few unsafe demonstrations, and -- more alarmingly -- that this
tendency rapidly escalates with more demonstrations.","[{'name': 'Fabio Pernisi'}, {'name': 'Dirk Hovy'}, {'name': 'Paul Röttger'}]",2024-08-08T15:24:03Z
http://arxiv.org/abs/2408.04519v1,http://arxiv.org/abs/2408.04519v1,"Articulatory Configurations across Genders and Periods in French Radio
  and TV archives","This paper studies changes in articulatory configurations across genders and
periods using an inversion from acoustic to articulatory parameters. From a
diachronic corpus based on French media archives spanning 60 years from 1955 to
2015, automatic transcription and forced alignment allowed extracting the
central frame of each vowel. More than one million frames were obtained from
over a thousand speakers across gender and age categories. Their formants were
used from these vocalic frames to fit the parameters of Maeda's articulatory
model. Evaluations of the quality of these processes are provided. We focus
here on two parameters of Maeda's model linked to total vocal tract length: the
relative position of the larynx (higher for females) and the lips protrusion
(more protruded for males). Implications for voice quality across genders are
discussed. The effect across periods seems gender independent; thus, the
assertion that females lowered their pitch with time is not supported.","[{'name': 'Benjamin Elie'}, {'name': 'David Doukhan'}, {'name': 'Rémi Uro'}, {'name': 'Lucas Ondel-Yang'}, {'name': 'Albert Rilliard'}, {'name': 'Simon Devauchelle'}]",2024-08-08T15:20:39Z
http://arxiv.org/abs/2408.04472v1,http://arxiv.org/abs/2408.04472v1,"Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for
  Competitive Debate","Competitive debate is a comprehensive and complex computational argumentation
task. Large Language Models (LLMs) encounter hallucinations and lack
competitiveness in this task. To address these challenges, we introduce Agent
for Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs
designed to enhance their capabilities in competitive debate. Drawing
inspiration from human behavior in debate preparation and execution,
Agent4Debate employs a collaborative architecture where four specialized agents
(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.
These agents work throughout the debate process, covering multiple stages from
initial research and argument formulation to rebuttal and summary. To
comprehensively evaluate framework performance, we construct the Chinese Debate
Arena, comprising 66 carefully selected Chinese debate motions. We recruite ten
experienced human debaters and collect records of 200 debates involving
Agent4Debate, baseline models, and humans. The evaluation employs the Debatrix
automatic scoring system and professional human reviewers based on the
established Debatrix-Elo and Human-Elo ranking. Experimental results indicate
that the state-of-the-art Agent4Debate exhibits capabilities comparable to
those of humans. Furthermore, ablation studies demonstrate the effectiveness of
each component in the agent structure.","[{'name': 'Yiqun Zhang'}, {'name': 'Xiaocui Yang'}, {'name': 'Shi Feng'}, {'name': 'Daling Wang'}, {'name': 'Yifei Zhang'}, {'name': 'Kaisong Song'}]",2024-08-08T14:02:45Z
http://arxiv.org/abs/2408.04463v1,http://arxiv.org/abs/2408.04463v1,Crowd Intelligence for Early Misinformation Prediction on Social Media,"Misinformation spreads rapidly on social media, causing serious damage by
influencing public opinion, promoting dangerous behavior, or eroding trust in
reliable sources. It spreads too fast for traditional fact-checking, stressing
the need for predictive methods. We introduce CROWDSHIELD, a crowd
intelligence-based method for early misinformation prediction. We hypothesize
that the crowd's reactions to misinformation reveal its accuracy. Furthermore,
we hinge upon exaggerated assertions/claims and replies with particular
positions/stances on the source post within a conversation thread. We employ
Q-learning to capture the two dimensions -- stances and claims. We utilize deep
Q-learning due to its proficiency in navigating complex decision spaces and
effectively learning network properties. Additionally, we use a
transformer-based encoder to develop a comprehensive understanding of both
content and context. This multifaceted approach helps ensure the model pays
attention to user interaction and stays anchored in the communication's
content. We propose MIST, a manually annotated misinformation detection Twitter
corpus comprising nearly 200 conversation threads with more than 14K replies.
In experiments, CROWDSHIELD outperformed ten baseline systems, achieving an
improvement of ~4% macro-F1 score. We conduct an ablation study and error
analysis to validate our proposed model's performance. The source code and
dataset are available at https://github.com/LCS2-IIITD/CrowdShield.git.","[{'name': 'Megha Sundriyal'}, {'name': 'Harshit Choudhary'}, {'name': 'Tanmoy Chakraborty'}, {'name': 'Md Shad Akhtar'}]",2024-08-08T13:45:23Z
http://arxiv.org/abs/2408.04691v1,http://arxiv.org/abs/2408.04691v1,"Improving Relational Database Interactions with Large Language Models:
  Column Descriptions and Their Impact on Text-to-SQL Performance","Relational databases often suffer from uninformative descriptors of table
contents, such as ambiguous columns and hard-to-interpret values, impacting
both human users and Text-to-SQL models. This paper explores the use of large
language models (LLMs) to generate informative column descriptions as a
semantic layer for relational databases. Using the BIRD-Bench development set,
we created \textsc{ColSQL}, a dataset with gold-standard column descriptions
generated and refined by LLMs and human annotators. We evaluated several
instruction-tuned models, finding that GPT-4o and Command R+ excelled in
generating high-quality descriptions. Additionally, we applied an
LLM-as-a-judge to evaluate model performance. Although this method does not
align well with human evaluations, we included it to explore its potential and
to identify areas for improvement. More work is needed to improve the
reliability of automatic evaluations for this task. We also find that detailed
column descriptions significantly improve Text-to-SQL execution accuracy,
especially when columns are uninformative. This study establishes LLMs as
effective tools for generating detailed metadata, enhancing the usability of
relational databases.","[{'name': 'Niklas Wretblad'}, {'name': 'Oskar Holmström'}, {'name': 'Erik Larsson'}, {'name': 'Axel Wiksäter'}, {'name': 'Oscar Söderlund'}, {'name': 'Hjalmar Öhman'}, {'name': 'Ture Pontén'}, {'name': 'Martin Forsberg'}, {'name': 'Martin Sörme'}, {'name': 'Fredrik Heintz'}]",2024-08-08T13:10:51Z
http://arxiv.org/abs/2408.04427v1,http://arxiv.org/abs/2408.04427v1,"AcrosticSleuth: Probabilistic Identification and Ranking of Acrostics in
  Multilingual Corpora","For centuries, writers have hidden messages in their texts as acrostics,
where initial letters of consecutive lines or paragraphs form meaningful words
or phrases. Scholars searching for acrostics manually can only focus on a few
authors at a time and often favor qualitative arguments in discussing
intentionally. We aim to put the study of acrostics on firmer statistical
footing by presenting AcrosticSleuth, a first-of-its-kind tool that
automatically identifies acrostics and ranks them by the probability that the
sequence of characters does not occur by chance (and therefore may have been
inserted intentionally). Acrostics are rare, so we formalize the problem as a
binary classification task in the presence of extreme class imbalance. To
evaluate AcrosticSleuth, we present the Acrostic Identification Dataset
(AcrostID), a collection of acrostics from the WikiSource online database.
Despite the class imbalance, AcrosticSleuth achieves F1 scores of 0.39, 0.59,
and 0.66 on French, English, and Russian subdomains of WikiSource,
respectively. We further demonstrate that AcrosticSleuth can identify
previously unknown high-profile instances of wordplay, such as the acrostic
spelling ARSPOETICA (``art of poetry"") by Italian Humanist Albertino Mussato
and English philosopher Thomas Hobbes' signature in the opening paragraphs of
The Elements of Law.","[{'name': 'Aleksandr Fedchin'}, {'name': 'Isabel Cooperman'}, {'name': 'Pramit Chaudhuri'}, {'name': 'Joseph P. Dexter'}]",2024-08-08T12:53:26Z
http://arxiv.org/abs/2408.04420v1,http://arxiv.org/abs/2408.04420v1,"Recognizing Emotion Regulation Strategies from Human Behavior with Large
  Language Models","Human emotions are often not expressed directly, but regulated according to
internal processes and social display rules. For affective computing systems,
an understanding of how users regulate their emotions can be highly useful, for
example to provide feedback in job interview training, or in psychotherapeutic
scenarios. However, at present no method to automatically classify different
emotion regulation strategies in a cross-user scenario exists. At the same
time, recent studies showed that instruction-tuned Large Language Models (LLMs)
can reach impressive performance across a variety of affect recognition tasks
such as categorical emotion recognition or sentiment analysis. While these
results are promising, it remains unclear to what extent the representational
power of LLMs can be utilized in the more subtle task of classifying users'
internal emotion regulation strategy. To close this gap, we make use of the
recently introduced \textsc{Deep} corpus for modeling the social display of the
emotion shame, where each point in time is annotated with one of seven
different emotion regulation classes. We fine-tune Llama2-7B as well as the
recently introduced Gemma model using Low-rank Optimization on prompts
generated from different sources of information on the \textsc{Deep} corpus.
These include verbal and nonverbal behavior, person factors, as well as the
results of an in-depth interview after the interaction. Our results show, that
a fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation
strategy with high accuracy (0.84) without needing access to data from
post-interaction interviews. This represents a significant improvement over
previous approaches based on Bayesian Networks and highlights the importance of
modeling verbal behavior in emotion regulation.","[{'name': 'Philipp Müller'}, {'name': 'Alexander Heimerl'}, {'name': 'Sayed Muddashir Hossain'}, {'name': 'Lea Siegel'}, {'name': 'Jan Alexandersson'}, {'name': 'Patrick Gebhard'}, {'name': 'Elisabeth André'}, {'name': 'Tanja Schneeberger'}]",2024-08-08T12:47:10Z
http://arxiv.org/abs/2408.04414v1,http://arxiv.org/abs/2408.04414v1,"Enhancing Robustness of Retrieval-Augmented Language Models with
  In-Context Learning","Retrieval-Augmented Language Models (RALMs) have significantly improved
performance in open-domain question answering (QA) by leveraging external
knowledge. However, RALMs still struggle with unanswerable queries, where the
retrieved contexts do not contain the correct answer, and with conflicting
information, where different sources provide contradictory answers due to
imperfect retrieval. This study introduces an in-context learning-based
approach to enhance the reasoning capabilities of RALMs, making them more
robust in imperfect retrieval scenarios. Our method incorporates Machine
Reading Comprehension (MRC) demonstrations, referred to as cases, to boost the
model's capabilities to identify unanswerabilities and conflicts among the
retrieved contexts. Experiments on two open-domain QA datasets show that our
approach increases accuracy in identifying unanswerable and conflicting
scenarios without requiring additional fine-tuning. This work demonstrates that
in-context learning can effectively enhance the robustness of RALMs in
open-domain QA tasks.","[{'name': 'Seong-Il Park'}, {'name': 'Seung-Woo Choi'}, {'name': 'Na-Hyun Kim'}, {'name': 'Jay-Yoon Lee'}]",2024-08-08T12:42:43Z
http://arxiv.org/abs/2408.04403v1,http://arxiv.org/abs/2408.04403v1,"Exploring Reasoning Biases in Large Language Models Through Syllogism:
  Insights from the NeuBAROCO Dataset","This paper explores the question of how accurately current large language
models can perform logical reasoning in natural language, with an emphasis on
whether these models exhibit reasoning biases similar to humans. Specifically,
our study focuses on syllogistic reasoning, a form of deductive reasoning
extensively studied in cognitive science as a natural form of human reasoning.
We present a syllogism dataset called NeuBAROCO, which consists of syllogistic
reasoning problems in English and Japanese. This dataset was originally
designed for psychological experiments to assess human reasoning capabilities
using various forms of syllogisms. Our experiments with leading large language
models indicate that these models exhibit reasoning biases similar to humans,
along with other error tendencies. Notably, there is significant room for
improvement in reasoning problems where the relationship between premises and
hypotheses is neither entailment nor contradiction. We also present
experimental results and in-depth analysis using a new Chain-of-Thought
prompting method, which asks LLMs to translate syllogisms into abstract logical
expressions and then explain their reasoning process. Our analysis using this
method suggests that the primary limitations of LLMs lie in the reasoning
process itself rather than the interpretation of syllogisms.","[{'name': 'Kentaro Ozeki'}, {'name': 'Risako Ando'}, {'name': 'Takanobu Morishita'}, {'name': 'Hirohiko Abe'}, {'name': 'Koji Mineshima'}, {'name': 'Mitsuhiro Okada'}]",2024-08-08T12:10:50Z
http://arxiv.org/abs/2408.04394v1,http://arxiv.org/abs/2408.04394v1,"Automated Educational Question Generation at Different Bloom's Skill
  Levels using Large Language Models: Strategies and Evaluation","Developing questions that are pedagogically sound, relevant, and promote
learning is a challenging and time-consuming task for educators. Modern-day
large language models (LLMs) generate high-quality content across multiple
domains, potentially helping educators to develop high-quality questions.
Automated educational question generation (AEQG) is important in scaling online
education catering to a diverse student population. Past attempts at AEQG have
shown limited abilities to generate questions at higher cognitive levels. In
this study, we examine the ability of five state-of-the-art LLMs of different
sizes to generate diverse and high-quality questions of different cognitive
levels, as defined by Bloom's taxonomy. We use advanced prompting techniques
with varying complexity for AEQG. We conducted expert and LLM-based evaluations
to assess the linguistic and pedagogical relevance and quality of the
questions. Our findings suggest that LLms can generate relevant and
high-quality educational questions of different cognitive levels when prompted
with adequate information, although there is a significant variance in the
performance of the five LLms considered. We also show that automated evaluation
is not on par with human evaluation.","[{'name': 'Nicy Scaria'}, {'name': 'Suma Dharani Chenna'}, {'name': 'Deepak Subramani'}]",2024-08-08T11:56:57Z
http://arxiv.org/abs/2408.04392v1,http://arxiv.org/abs/2408.04392v1,Open-domain Implicit Format Control for Large Language Model Generation,"Controlling the format of outputs generated by large language models (LLMs)
is a critical functionality in various applications. Current methods typically
employ constrained decoding with rule-based automata or fine-tuning with
manually crafted format instructions, both of which struggle with open-domain
format requirements. To address this limitation, we introduce a novel framework
for controlled generation in LLMs, leveraging user-provided, one-shot QA pairs.
This study investigates LLMs' capabilities to follow open-domain, one-shot
constraints and replicate the format of the example answers. We observe that
this is a non-trivial problem for current LLMs. We also develop a dataset
collection methodology for supervised fine-tuning that enhances the open-domain
format control of LLMs without degrading output quality, as well as a benchmark
on which we evaluate both the helpfulness and format correctness of LLM
outputs. The resulting datasets, named OIFC-SFT, along with the related code,
will be made publicly available at https://github.com/cofe-ai/OIFC.","[{'name': 'Yiqun Yao'}, {'name': 'Wenjia Ma'}, {'name': 'Xuezhi Fang'}, {'name': 'Xin Jiang'}, {'name': 'Xiang Li'}, {'name': 'Xuying Meng'}, {'name': 'Peng Han'}, {'name': 'Jing Li'}, {'name': 'Aixin Sun'}, {'name': 'Yequan Wang'}]",2024-08-08T11:51:45Z
http://arxiv.org/abs/2408.04378v1,http://arxiv.org/abs/2408.04378v1,Overview of the NLPCC 2024 Shared Task on Chinese Metaphor Generation,"This paper presents the results of the shared task on Chinese metaphor
generation, hosted at the 13th CCF Conference on Natural Language Processing
and Chinese Computing (NLPCC 2024). The goal of this shared task is to generate
Chinese metaphors using machine learning techniques and effectively identifying
basic components of metaphorical sentences. It is divided into two subtasks: 1)
Metaphor Generation, which involves creating a metaphor from a provided tuple
consisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a
metaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),
guided by the concept of the GROUND. 2) Metaphor Components Identification,
which extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a
metaphorical sentence. This component requires the identification of the most
fitting metaphor elements that correspond to the specified grounds. In addition
to overall results, we report on the setup and insights from the metaphor
generation shared task, which attracted a total of 4 participating teams across
both subtasks.","[{'name': 'Xingwei Qu'}, {'name': 'Ge Zhang'}, {'name': 'Siwei Wu'}, {'name': 'Yizhi Li'}, {'name': 'Chenghua Lin'}]",2024-08-08T11:29:43Z
http://arxiv.org/abs/2408.04369v1,http://arxiv.org/abs/2408.04369v1,"Analyzing Consumer Reviews for Understanding Drivers of Hotels Ratings:
  An Indian Perspective","In the internet era, almost every business entity is trying to have its
digital footprint in digital media and other social media platforms. For these
entities, word of mouse is also very important. Particularly, this is quite
crucial for the hospitality sector dealing with hotels, restaurants etc.
Consumers do read other consumers reviews before making final decisions. This
is where it becomes very important to understand which aspects are affecting
most in the minds of the consumers while giving their ratings. The current
study focuses on the consumer reviews of Indian hotels to extract aspects
important for final ratings. The study involves gathering data using web
scraping methods, analyzing the texts using Latent Dirichlet Allocation for
topic extraction and sentiment analysis for aspect-specific sentiment mapping.
Finally, it incorporates Random Forest to understand the importance of the
aspects in predicting the final rating of a user.","[{'name': 'Subhasis Dasgupta'}, {'name': 'Soumya Roy'}, {'name': 'Jaydip Sen'}]",2024-08-08T10:58:33Z
http://arxiv.org/abs/2408.04363v1,http://arxiv.org/abs/2408.04363v1,"Simulating Articulatory Trajectories with Phonological Feature
  Interpolation","As a first step towards a complete computational model of speech learning
involving perception-production loops, we investigate the forward mapping
between pseudo-motor commands and articulatory trajectories. Two phonological
feature sets, based respectively on generative and articulatory phonology, are
used to encode a phonetic target sequence. Different interpolation techniques
are compared to generate smooth trajectories in these feature spaces, with a
potential optimisation of the target value and timing to capture
co-articulation effects. We report the Pearson correlation between a linear
projection of the generated trajectories and articulatory data derived from a
multi-speaker dataset of electromagnetic articulography (EMA) recordings. A
correlation of 0.67 is obtained with an extended feature set based on
generative phonology and a linear interpolation technique. We discuss the
implications of our results for our understanding of the dynamics of biological
motion.","[{'name': 'Angelo Ortiz Tandazo'}, {'name': 'Thomas Schatz'}, {'name': 'Thomas Hueber'}, {'name': 'Emmanuel Dupoux'}]",2024-08-08T10:51:16Z
http://arxiv.org/abs/2408.04331v1,http://arxiv.org/abs/2408.04331v1,"Enhancing Journalism with AI: A Study of Contextualized Image Captioning
  for News Articles using LLMs and LMMs","Large language models (LLMs) and large multimodal models (LMMs) have
significantly impacted the AI community, industry, and various economic
sectors. In journalism, integrating AI poses unique challenges and
opportunities, particularly in enhancing the quality and efficiency of news
reporting. This study explores how LLMs and LMMs can assist journalistic
practice by generating contextualised captions for images accompanying news
articles. We conducted experiments using the GoodNews dataset to evaluate the
ability of LMMs (BLIP-2, GPT-4v, or LLaVA) to incorporate one of two types of
context: entire news articles, or extracted named entities. In addition, we
compared their performance to a two-stage pipeline composed of a captioning
model (BLIP-2, OFA, or ViT-GPT2) with post-hoc contextualisation with LLMs
(GPT-4 or LLaMA). We assess a diversity of models, and we find that while the
choice of contextualisation model is a significant factor for the two-stage
pipelines, this is not the case in the LMMs, where smaller, open-source models
perform well compared to proprietary, GPT-powered ones. Additionally, we found
that controlling the amount of provided context enhances performance. These
results highlight the limitations of a fully automated approach and underscore
the necessity for an interactive, human-in-the-loop strategy.","[{'name': 'Aliki Anagnostopoulou'}, {'name': 'Thiago Gouvea'}, {'name': 'Daniel Sonntag'}]",2024-08-08T09:31:24Z
http://arxiv.org/abs/2408.04686v1,http://arxiv.org/abs/2408.04686v1,"Multi-Turn Context Jailbreak Attack on Large Language Models From First
  Principles","Large language models (LLMs) have significantly enhanced the performance of
numerous applications, from intelligent conversations to text generation.
However, their inherent security vulnerabilities have become an increasingly
significant challenge, especially with respect to jailbreak attacks. Attackers
can circumvent the security mechanisms of these LLMs, breaching security
constraints and causing harmful outputs. Focusing on multi-turn semantic
jailbreak attacks, we observe that existing methods lack specific
considerations for the role of multiturn dialogues in attack strategies,
leading to semantic deviations during continuous interactions. Therefore, in
this paper, we establish a theoretical foundation for multi-turn attacks by
considering their support in jailbreak attacks, and based on this, propose a
context-based contextual fusion black-box jailbreak attack method, named
Context Fusion Attack (CFA). This method approach involves filtering and
extracting key terms from the target, constructing contextual scenarios around
these terms, dynamically integrating the target into the scenarios, replacing
malicious key terms within the target, and thereby concealing the direct
malicious intent. Through comparisons on various mainstream LLMs and red team
datasets, we have demonstrated CFA's superior success rate, divergence, and
harmfulness compared to other multi-turn attack strategies, particularly
showcasing significant advantages on Llama3 and GPT-4.","[{'name': 'Xiongtao Sun'}, {'name': 'Deyue Zhang'}, {'name': 'Dongdong Yang'}, {'name': 'Quanchen Zou'}, {'name': 'Hui Li'}]",2024-08-08T09:18:47Z
http://arxiv.org/abs/2408.04325v1,http://arxiv.org/abs/2408.04325v1,HydraFormer: One Encoder For All Subsampling Rates,"In automatic speech recognition, subsampling is essential for tackling
diverse scenarios. However, the inadequacy of a single subsampling rate to
address various real-world situations often necessitates training and deploying
multiple models, consequently increasing associated costs. To address this
issue, we propose HydraFormer, comprising HydraSub, a Conformer-based encoder,
and a BiTransformer-based decoder. HydraSub encompasses multiple branches, each
representing a distinct subsampling rate, allowing for the flexible selection
of any branch during inference based on the specific use case. HydraFormer can
efficiently manage different subsampling rates, significantly reducing training
and deployment expenses. Experiments on AISHELL-1 and LibriSpeech datasets
reveal that HydraFormer effectively adapts to various subsampling rates and
languages while maintaining high recognition performance. Additionally,
HydraFormer showcases exceptional stability, sustaining consistent performance
under various initialization conditions, and exhibits robust transferability by
learning from pretrained single subsampling rate automatic speech recognition
models\footnote{Model code and scripts:
https://github.com/HydraFormer/hydraformer}.","[{'name': 'Yaoxun Xu'}, {'name': 'Xingchen Song'}, {'name': 'Zhiyong Wu'}, {'name': 'Di Wu'}, {'name': 'Zhendong Peng'}, {'name': 'Binbin Zhang'}]",2024-08-08T09:08:27Z
http://arxiv.org/abs/2408.04303v1,http://arxiv.org/abs/2408.04303v1,"Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language
  Adaptation of LLMs for Low-Resource NLP","The development of monolingual language models for low and mid-resource
languages continues to be hindered by the difficulty in sourcing high-quality
training data. In this study, we present a novel cross-lingual vocabulary
transfer strategy, trans-tokenization, designed to tackle this challenge and
enable more efficient language adaptation. Our approach focuses on adapting a
high-resource monolingual LLM to an unseen target language by initializing the
token embeddings of the target language using a weighted average of
semantically similar token embeddings from the source language. For this, we
leverage a translation resource covering both the source and target languages.
We validate our method with the Tweeties, a series of trans-tokenized LLMs, and
demonstrate their competitive performance on various downstream tasks across a
small but diverse set of languages. Additionally, we introduce Hydra LLMs,
models with multiple swappable language modeling heads and embedding tables,
which further extend the capabilities of our trans-tokenization strategy. By
designing a Hydra LLM based on the multilingual model TowerInstruct, we
developed a state-of-the-art machine translation model for Tatar, in a
zero-shot manner, completely bypassing the need for high-quality parallel data.
This breakthrough is particularly significant for low-resource languages like
Tatar, where high-quality parallel data is hard to come by. By lowering the
data and time requirements for training high-quality models, our
trans-tokenization strategy allows for the development of LLMs for a wider
range of languages, especially those with limited resources. We hope that our
work will inspire further research and collaboration in the field of
cross-lingual vocabulary transfer and contribute to the empowerment of
languages on a global scale.","[{'name': 'François Remy'}, {'name': 'Pieter Delobelle'}, {'name': 'Hayastan Avetisyan'}, {'name': 'Alfiya Khabibullina'}, {'name': 'Miryam de Lhoneux'}, {'name': 'Thomas Demeester'}]",2024-08-08T08:37:28Z
http://arxiv.org/abs/2408.04293v1,http://arxiv.org/abs/2408.04293v1,"Are Social Sentiments Inherent in LLMs? An Empirical Study on Extraction
  of Inter-demographic Sentiments","Large language models (LLMs) are supposed to acquire unconscious human
knowledge and feelings, such as social common sense and biases, by training
models from large amounts of text. However, it is not clear how much the
sentiments of specific social groups can be captured in various LLMs. In this
study, we focus on social groups defined in terms of nationality, religion, and
race/ethnicity, and validate the extent to which sentiments between social
groups can be captured in and extracted from LLMs. Specifically, we input
questions regarding sentiments from one group to another into LLMs, apply
sentiment analysis to the responses, and compare the results with social
surveys. The validation results using five representative LLMs showed higher
correlations with relatively small p-values for nationalities and religions,
whose number of data points were relatively large. This result indicates that
the LLM responses including the inter-group sentiments align well with actual
social survey results.","[{'name': 'Kunitomo Tanaka'}, {'name': 'Ryohei Sasano'}, {'name': 'Koichi Takeda'}]",2024-08-08T08:13:25Z
http://arxiv.org/abs/2408.04289v1,http://arxiv.org/abs/2408.04289v1,EMTeC: A Corpus of Eye Movements on Machine-Generated Texts,"The Eye Movements on Machine-Generated Texts Corpus (EMTeC) is a naturalistic
eye-movements-while-reading corpus of 107 native English speakers reading
machine-generated texts. The texts are generated by three large language models
using five different decoding strategies, and they fall into six different text
type categories. EMTeC entails the eye movement data at all stages of
pre-processing, i.e., the raw coordinate data sampled at 2000 Hz, the fixation
sequences, and the reading measures. It further provides both the original and
a corrected version of the fixation sequences, accounting for vertical
calibration drift. Moreover, the corpus includes the language models' internals
that underlie the generation of the stimulus texts: the transition scores, the
attention scores, and the hidden states. The stimuli are annotated for a range
of linguistic features both at text and at word level. We anticipate EMTeC to
be utilized for a variety of use cases such as, but not restricted to, the
investigation of reading behavior on machine-generated text and the impact of
different decoding strategies; reading behavior on different text types; the
development of new pre-processing, data filtering, and drift correction
algorithms; the cognitive interpretability and enhancement of language models;
and the assessment of the predictive power of surprisal and entropy for human
reading times. The data at all stages of pre-processing, the model internals,
and the code to reproduce the stimulus generation, data pre-processing and
analyses can be accessed via https://github.com/DiLi-Lab/EMTeC/.","[{'name': 'Lena Sophia Bolliger'}, {'name': 'Patrick Haller'}, {'name': 'Isabelle Caroline Rose Cretton'}, {'name': 'David Robert Reich'}, {'name': 'Tannon Kew'}, {'name': 'Lena Ann Jäger'}]",2024-08-08T08:00:45Z
http://arxiv.org/abs/2408.04284v1,http://arxiv.org/abs/2408.04284v1,LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection,"The widespread accessibility of large language models (LLMs) to the general
public has significantly amplified the dissemination of machine-generated texts
(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in
discerning the origin of a text (human-authored vs machinegenerated). This
raises concerns regarding the potential misuse of MGTs, particularly within
educational and academic domains. In this paper, we present
$\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.
It is able to classify texts into four categories: human-written,
machine-generated, machine-written machine-humanized, and human-written
machine-polished. Contrary to previous MGT detectors that perform binary
classification, introducing two additional categories in LLM-DetectiAIve offers
insights into the varying degrees of LLM intervention during the text creation.
This might be useful in some domains like education, where any LLM intervention
is usually prohibited. Experiments show that LLM-DetectAIve can effectively
identify the authorship of textual content, proving its usefulness in enhancing
integrity in education, academia, and other domains. LLM-DetectAIve is publicly
accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video
describing our system is available at https://youtu.be/E8eT_bE7k8c.","[{'name': 'Mervat Abassy'}, {'name': 'Kareem Elozeiri'}, {'name': 'Alexander Aziz'}, {'name': 'Minh Ngoc Ta'}, {'name': 'Raj Vardhan Tomar'}, {'name': 'Bimarsha Adhikari'}, {'name': 'Saad El Dine Ahmed'}, {'name': 'Yuxia Wang'}, {'name': 'Osama Mohammed Afzal'}, {'name': 'Zhuohan Xie'}, {'name': 'Jonibek Mansurov'}, {'name': 'Ekaterina Artemova'}, {'name': 'Vladislav Mikhailov'}, {'name': 'Rui Xing'}, {'name': 'Jiahui Geng'}, {'name': 'Hasan Iqbal'}, {'name': 'Zain Muhammad Mujahid'}, {'name': 'Tarek Mahmoud'}, {'name': 'Akim Tsvigun'}, {'name': 'Alham Fikri Aji'}, {'name': 'Artem Shelmanov'}, {'name': 'Nizar Habash'}, {'name': 'Iryna Gurevych'}, {'name': 'Preslav Nakov'}]",2024-08-08T07:43:17Z
http://arxiv.org/abs/2408.04278v1,http://arxiv.org/abs/2408.04278v1,LaDiMo: Layer-wise Distillation Inspired MoEfier,"The advent of large language models has revolutionized natural language
processing, but their increasing complexity has led to substantial training
costs, resource demands, and environmental impacts. In response, sparse
Mixture-of-Experts (MoE) models have emerged as a promising alternative to
dense models. Since training MoE models from scratch can be prohibitively
expensive, recent studies have explored leveraging knowledge from pre-trained
non-MoE models. However, existing approaches have limitations, such as
requiring significant hardware resources and data. We propose a novel
algorithm, LaDiMo, which efficiently converts a Transformer-based non-MoE model
into a MoE model with minimal additional training cost. LaDiMo consists of two
stages: layer-wise expert construction and routing policy decision. By
harnessing the concept of Knowledge Distillation, we compress the model and
rapidly recover its performance. Furthermore, we develop an adaptive router
that optimizes inference efficiency by profiling the distribution of routing
weights and determining a layer-wise policy that balances accuracy and latency.
We demonstrate the effectiveness of our method by converting the LLaMA2-7B
model to a MoE model using only 100K tokens, reducing activated parameters by
over 20% while keeping accuracy. Our approach offers a flexible and efficient
solution for building and deploying MoE models.","[{'name': 'Sungyoon Kim'}, {'name': 'Youngjun Kim'}, {'name': 'Kihyo Moon'}, {'name': 'Minsung Jang'}]",2024-08-08T07:37:26Z
http://arxiv.org/abs/2408.04270v1,http://arxiv.org/abs/2408.04270v1,"Analysis of Argument Structure Constructions in the Large Language Model
  BERT","This study investigates how BERT processes and represents Argument Structure
Constructions (ASCs), extending previous LSTM analyses. Using a dataset of 2000
sentences across four ASC types (transitive, ditransitive, caused-motion,
resultative), we analyzed BERT's token embeddings across 12 layers.
Visualizations with MDS and t-SNE and clustering quantified by Generalized
Discrimination Value (GDV) were used. Feedforward classifiers (probes)
predicted construction categories from embeddings. CLS token embeddings
clustered best in layers 2-4, decreased in intermediate layers, and slightly
increased in final layers. DET and SUBJ embeddings showed consistent clustering
in intermediate layers, VERB embeddings increased in clustering from layer 1 to
12, and OBJ embeddings peaked in layer 10. Probe accuracies indicated low
construction information in layer 1, with over 90 percent accuracy from layer 2
onward, revealing latent construction information beyond GDV clustering. Fisher
Discriminant Ratio (FDR) analysis of attention weights showed OBJ tokens were
crucial for differentiating ASCs, followed by VERB and DET tokens. SUBJ, CLS,
and SEP tokens had insignificant FDR scores. This study highlights BERT's
layered processing of linguistic constructions and its differences from LSTMs.
Future research will compare these findings with neuroimaging data to
understand the neural correlates of ASC processing. This research underscores
neural language models' potential to mirror linguistic processing in the human
brain, offering insights into the computational and neural mechanisms
underlying language understanding.","[{'name': 'Pegah Ramezani'}, {'name': 'Achim Schilling'}, {'name': 'Patrick Krauss'}]",2024-08-08T07:12:46Z
http://arxiv.org/abs/2408.04259v1,http://arxiv.org/abs/2408.04259v1,EfficientRAG: Efficient Retriever for Multi-Hop Question Answering,"Retrieval-augmented generation (RAG) methods encounter difficulties when
addressing complex questions like multi-hop queries. While iterative retrieval
methods improve performance by gathering additional information, current
approaches often rely on multiple calls of large language models (LLMs). In
this paper, we introduce EfficientRAG, an efficient retriever for multi-hop
question answering. EfficientRAG iteratively generates new queries without the
need for LLM calls at each iteration and filters out irrelevant information.
Experimental results demonstrate that EfficientRAG surpasses existing RAG
methods on three open-domain multi-hop question-answering datasets.","[{'name': 'Ziyuan Zhuang'}, {'name': 'Zhiyang Zhang'}, {'name': 'Sitao Cheng'}, {'name': 'Fangkai Yang'}, {'name': 'Jia Liu'}, {'name': 'Shujian Huang'}, {'name': 'Qingwei Lin'}, {'name': 'Saravan Rajmohan'}, {'name': 'Dongmei Zhang'}, {'name': 'Qi Zhang'}]",2024-08-08T06:57:49Z
http://arxiv.org/abs/2408.04246v1,http://arxiv.org/abs/2408.04246v1,Explicating the Implicit: Argument Detection Beyond Sentence Boundaries,"Detecting semantic arguments of a predicate word has been conventionally
modeled as a sentence-level task. The typical reader, however, perfectly
interprets predicate-argument relations in a much wider context than just the
sentence where the predicate was evoked. In this work, we reformulate the
problem of argument detection through textual entailment to capture semantic
relations across sentence boundaries. We propose a method that tests whether
some semantic relation can be inferred from a full passage by first encoding it
into a simple and standalone proposition and then testing for entailment
against the passage. Our method does not require direct supervision, which is
generally absent due to dataset scarcity, but instead builds on existing NLI
and sentence-level SRL resources. Such a method can potentially explicate
pragmatically understood relations into a set of explicit sentences. We
demonstrate it on a recent document-level benchmark, outperforming some
supervised methods and contemporary language models.","[{'name': 'Paul Roit'}, {'name': 'Aviv Slobodkin'}, {'name': 'Eran Hirsch'}, {'name': 'Arie Cattan'}, {'name': 'Ayal Klein'}, {'name': 'Valentina Pyatkin'}, {'name': 'Ido Dagan'}]",2024-08-08T06:18:24Z
http://arxiv.org/abs/2408.04237v1,http://arxiv.org/abs/2408.04237v1,Learning to Rewrite: Generalized LLM-Generated Text Detection,"Large language models (LLMs) can be abused at scale to create non-factual
content and spread disinformation. Detecting LLM-generated content is essential
to mitigate these risks, but current classifiers often fail to generalize in
open-world contexts. Prior work shows that LLMs tend to rewrite LLM-generated
content less frequently, which can be used for detection and naturally
generalizes to unforeseen data. However, we find that the rewriting edit
distance between human and LLM content can be indistinguishable across domains,
leading to detection failures. We propose training an LLM to rewrite input
text, producing minimal edits for LLM-generated content and more edits for
human-written text, deriving a distinguishable and generalizable edit distance
difference across different domains. Experiments on text from 21 independent
domains and three popular LLMs (e.g., GPT-4o, Gemini, and Llama-3) show that
our classifier outperforms the state-of-the-art zero-shot classifier by up to
20.6% on AUROC score and the rewriting classifier by 9.2% on F1 score. Our work
suggests that LLM can effectively detect machine-generated text if they are
trained properly.","[{'name': 'Wei Hao'}, {'name': 'Ran Li'}, {'name': 'Weiliang Zhao'}, {'name': 'Junfeng Yang'}, {'name': 'Chengzhi Mao'}]",2024-08-08T05:53:39Z
http://arxiv.org/abs/2408.04682v1,http://arxiv.org/abs/2408.04682v1,"ToolSandbox: A Stateful, Conversational, Interactive Evaluation
  Benchmark for LLM Tool Use Capabilities","Recent large language models (LLMs) advancements sparked a growing research
interest in tool assisted LLMs solving real-world challenges, which calls for
comprehensive evaluation of tool-use capabilities. While previous works focused
on either evaluating over stateless web services (RESTful API), based on a
single turn user prompt, or an off-policy dialog trajectory, ToolSandbox
includes stateful tool execution, implicit state dependencies between tools, a
built-in user simulator supporting on-policy conversational evaluation and a
dynamic evaluation strategy for intermediate and final milestones over an
arbitrary trajectory. We show that open source and proprietary models have a
significant performance gap, and complex tasks like State Dependency,
Canonicalization and Insufficient Information defined in ToolSandbox are
challenging even the most capable SOTA LLMs, providing brand-new insights into
tool-use LLM capabilities. ToolSandbox evaluation framework is released at
https://github.com/apple/ToolSandbox","[{'name': 'Jiarui Lu'}, {'name': 'Thomas Holleis'}, {'name': 'Yizhe Zhang'}, {'name': 'Bernhard Aumayer'}, {'name': 'Feng Nan'}, {'name': 'Felix Bai'}, {'name': 'Shuang Ma'}, {'name': 'Shen Ma'}, {'name': 'Mengyu Li'}, {'name': 'Guoli Yin'}, {'name': 'Zirui Wang'}, {'name': 'Ruoming Pang'}]",2024-08-08T05:45:42Z
http://arxiv.org/abs/2408.04226v2,http://arxiv.org/abs/2408.04226v2,"Evaluating Language Model Math Reasoning via Grounding in Educational
  Curricula","Our work presents a novel angle for evaluating language models' (LMs)
mathematical abilities, by investigating whether they can discern skills and
concepts enabled by math content. We contribute two datasets: one consisting of
385 fine-grained descriptions of K-12 math skills and concepts, or standards,
from Achieve the Core (ATC), and another of 9.9K problems labeled with these
standards (MathFish). Working with experienced teachers, we find that LMs
struggle to tag and verify standards linked to problems, and instead predict
labels that are close to ground truth, but differ in subtle ways. We also show
that LMs often generate problems that do not fully align with standards
described in prompts. Finally, we categorize problems in GSM8k using math
standards, allowing us to better understand why some problems are more
difficult to solve for models than others.","[{'name': 'Li Lucy'}, {'name': 'Tal August'}, {'name': 'Rose E. Wang'}, {'name': 'Luca Soldaini'}, {'name': 'Courtney Allison'}, {'name': 'Kyle Lo'}]",2024-08-08T05:28:34Z
http://arxiv.org/abs/2408.04220v1,http://arxiv.org/abs/2408.04220v1,Diffusion Guided Language Modeling,"Current language models demonstrate remarkable proficiency in text
generation. However, for many applications it is desirable to control
attributes, such as sentiment, or toxicity, of the generated language --
ideally tailored towards each specific use case and target audience. For
auto-regressive language models, existing guidance methods are prone to
decoding errors that cascade during generation and degrade performance. In
contrast, text diffusion models can easily be guided with, for example, a
simple linear sentiment classifier -- however they do suffer from significantly
higher perplexity than auto-regressive alternatives. In this paper we use a
guided diffusion model to produce a latent proposal that steers an
auto-regressive language model to generate text with desired properties. Our
model inherits the unmatched fluency of the auto-regressive approach and the
plug-and-play flexibility of diffusion. We show that it outperforms previous
plug-and-play guidance methods across a wide range of benchmark data sets.
Further, controlling a new attribute in our framework is reduced to training a
single logistic regression classifier.","[{'name': 'Justin Lovelace'}, {'name': 'Varsha Kishore'}, {'name': 'Yiwei Chen'}, {'name': 'Kilian Q. Weinberger'}]",2024-08-08T05:06:22Z
http://arxiv.org/abs/2408.04217v1,http://arxiv.org/abs/2408.04217v1,"Simplifying Translations for Children: Iterative Simplification
  Considering Age of Acquisition with LLMs","In recent years, neural machine translation (NMT) has been widely used in
everyday life. However, the current NMT lacks a mechanism to adjust the
difficulty level of translations to match the user's language level.
Additionally, due to the bias in the training data for NMT, translations of
simple source sentences are often produced with complex words. In particular,
this could pose a problem for children, who may not be able to understand the
meaning of the translations correctly. In this study, we propose a method that
replaces words with high Age of Acquisitions (AoA) in translations with simpler
words to match the translations to the user's level. We achieve this by using
large language models (LLMs), providing a triple of a source sentence, a
translation, and a target word to be replaced. We create a benchmark dataset
using back-translation on Simple English Wikipedia. The experimental results
obtained from the dataset show that our method effectively replaces high-AoA
words with lower-AoA words and, moreover, can iteratively replace most of the
high-AoA words while still maintaining high BLEU and COMET scores.","[{'name': 'Masashi Oshika'}, {'name': 'Makoto Morishita'}, {'name': 'Tsutomu Hirao'}, {'name': 'Ryohei Sasano'}, {'name': 'Koichi Takeda'}]",2024-08-08T04:57:36Z
http://arxiv.org/abs/2408.04681v1,http://arxiv.org/abs/2408.04681v1,"Conversational AI Powered by Large Language Models Amplifies False
  Memories in Witness Interviews","This study examines the impact of AI on human false memories -- recollections
of events that did not occur or deviate from actual occurrences. It explores
false memory induction through suggestive questioning in Human-AI interactions,
simulating crime witness interviews. Four conditions were tested: control,
survey-based, pre-scripted chatbot, and generative chatbot using a large
language model (LLM). Participants (N=200) watched a crime video, then
interacted with their assigned AI interviewer or survey, answering questions
including five misleading ones. False memories were assessed immediately and
after one week. Results show the generative chatbot condition significantly
increased false memory formation, inducing over 3 times more immediate false
memories than the control and 1.7 times more than the survey method. 36.4% of
users' responses to the generative chatbot were misled through the interaction.
After one week, the number of false memories induced by generative chatbots
remained constant. However, confidence in these false memories remained higher
than the control after one week. Moderating factors were explored: users who
were less familiar with chatbots but more familiar with AI technology, and more
interested in crime investigations, were more susceptible to false memories.
These findings highlight the potential risks of using advanced AI in sensitive
contexts, like police interviews, emphasizing the need for ethical
considerations.","[{'name': 'Samantha Chan'}, {'name': 'Pat Pataranutaporn'}, {'name': 'Aditya Suri'}, {'name': 'Wazeer Zulfikar'}, {'name': 'Pattie Maes'}, {'name': 'Elizabeth F. Loftus'}]",2024-08-08T04:55:03Z
http://arxiv.org/abs/2408.04216v1,http://arxiv.org/abs/2408.04216v1,"Attention Mechanism and Context Modeling System for Text Mining Machine
  Translation","This paper advances a novel architectural schema anchored upon the
Transformer paradigm and innovatively amalgamates the K-means categorization
algorithm to augment the contextual apprehension capabilities of the schema.
The transformer model performs well in machine translation tasks due to its
parallel computing power and multi-head attention mechanism. However, it may
encounter contextual ambiguity or ignore local features when dealing with
highly complex language structures. To circumvent this constraint, this
exposition incorporates the K-Means algorithm, which is used to stratify the
lexis and idioms of the input textual matter, thereby facilitating superior
identification and preservation of the local structure and contextual
intelligence of the language. The advantage of this combination is that K-Means
can automatically discover the topic or concept regions in the text, which may
be directly related to translation quality. Consequently, the schema contrived
herein enlists K-Means as a preparatory phase antecedent to the Transformer and
recalibrates the multi-head attention weights to assist in the discrimination
of lexis and idioms bearing analogous semantics or functionalities. This
ensures the schema accords heightened regard to the contextual intelligence
embodied by these clusters during the training phase, rather than merely
focusing on locational intelligence.","[{'name': 'Shi Bo'}, {'name': 'Yuwei Zhang'}, {'name': 'Junming Huang'}, {'name': 'Sitong Liu'}, {'name': 'Zexi Chen'}, {'name': 'Zizheng Li'}]",2024-08-08T04:52:10Z
http://arxiv.org/abs/2408.04680v1,http://arxiv.org/abs/2408.04680v1,Dynamic Fog Computing for Enhanced LLM Execution in Medical Applications,"The ability of large language models (LLMs) to transform, interpret, and
comprehend vast quantities of heterogeneous data presents a significant
opportunity to enhance data-driven care delivery. However, the sensitive nature
of protected health information (PHI) raises valid concerns about data privacy
and trust in remote LLM platforms. In addition, the cost associated with
cloud-based artificial intelligence (AI) services continues to impede
widespread adoption. To address these challenges, we propose a shift in the LLM
execution environment from opaque, centralized cloud providers to a
decentralized and dynamic fog computing architecture. By executing open-weight
LLMs in more trusted environments, such as the user's edge device or a fog
layer within a local network, we aim to mitigate the privacy, trust, and
financial challenges associated with cloud-based LLMs. We further present
SpeziLLM, an open-source framework designed to facilitate rapid and seamless
leveraging of different LLM execution layers and lowering barriers to LLM
integration in digital health applications. We demonstrate SpeziLLM's broad
applicability across six digital health applications, showcasing its
versatility in various healthcare settings.","[{'name': 'Philipp Zagar'}, {'name': 'Vishnu Ravi'}, {'name': 'Lauren Aalami'}, {'name': 'Stephan Krusche'}, {'name': 'Oliver Aalami'}, {'name': 'Paul Schmiedmayer'}]",2024-08-08T04:49:21Z
http://arxiv.org/abs/2408.04211v1,http://arxiv.org/abs/2408.04211v1,MMREC: LLM Based Multi-Modal Recommender System,"The importance of recommender systems is growing rapidly due to the
exponential increase in the volume of content generated daily. This surge in
content presents unique challenges for designing effective recommender systems.
Key among these challenges is the need to effectively leverage the vast amounts
of natural language data and images that represent user preferences. This paper
presents a novel approach to enhancing recommender systems by leveraging Large
Language Models (LLMs) and deep learning techniques. The proposed framework
aims to improve the accuracy and relevance of recommendations by incorporating
multi-modal information processing and by the use of unified latent space
representation. The study explores the potential of LLMs to better understand
and utilize natural language data in recommendation contexts, addressing the
limitations of previous methods. The framework efficiently extracts and
integrates text and image information through LLMs, unifying diverse modalities
in a latent space to simplify the learning process for the ranking model.
Experimental results demonstrate the enhanced discriminative power of the model
when utilizing multi-modal information. This research contributes to the
evolving field of recommender systems by showcasing the potential of LLMs and
multi-modal data integration to create more personalized and contextually
relevant recommendations.","[{'name': 'Jiahao Tian'}, {'name': 'Jinman Zhao'}, {'name': 'Zhenkai Wang'}, {'name': 'Zhicheng Ding'}]",2024-08-08T04:31:29Z
http://arxiv.org/abs/2408.04679v1,http://arxiv.org/abs/2408.04679v1,"Towards Linguistic Neural Representation Learning and Sentence Retrieval
  from Electroencephalogram Recordings","Decoding linguistic information from non-invasive brain signals using EEG has
gained increasing research attention due to its vast applicational potential.
Recently, a number of works have adopted a generative-based framework to decode
electroencephalogram (EEG) signals into sentences by utilizing the power
generative capacity of pretrained large language models (LLMs). However, this
approach has several drawbacks that hinder the further development of
linguistic applications for brain-computer interfaces (BCIs). Specifically, the
ability of the EEG encoder to learn semantic information from EEG data remains
questionable, and the LLM decoder's tendency to generate sentences based on its
training memory can be hard to avoid. These issues necessitate a novel approach
for converting EEG signals into sentences. In this paper, we propose a novel
two-step pipeline that addresses these limitations and enhances the validity of
linguistic EEG decoding research. We first confirm that word-level semantic
information can be learned from EEG data recorded during natural reading by
training a Conformer encoder via a masked contrastive objective for word-level
classification. To achieve sentence decoding results, we employ a training-free
retrieval method to retrieve sentences based on the predictions from the EEG
encoder. Extensive experiments and ablation studies were conducted in this
paper for a comprehensive evaluation of the proposed approach. Visualization of
the top prediction candidates reveals that our model effectively groups EEG
segments into semantic categories with similar meanings, thereby validating its
ability to learn patterns from unspoken EEG recordings. Despite the exploratory
nature of this work, these results suggest that our method holds promise for
providing more reliable solutions for converting EEG signals into text.","[{'name': 'Jinzhao Zhou'}, {'name': 'Yiqun Duan'}, {'name': 'Ziyi Zhao'}, {'name': 'Yu-Cheng Chang'}, {'name': 'Yu-Kai Wang'}, {'name': 'Thomas Do'}, {'name': 'Chin-Teng Lin'}]",2024-08-08T03:40:25Z
http://arxiv.org/abs/2408.04678v1,http://arxiv.org/abs/2408.04678v1,"CREST: Effectively Compacting a Datastore For Retrieval-Based
  Speculative Decoding","We present CREST (Compact Retrieval-Based Speculative Decoding), a redesign
of REST that allows it to be effectively ""compacted"". REST is a drafting
technique for speculative decoding based on retrieving exact n-gram matches of
the most recent n tokens generated by the target LLM from a datastore. The key
idea of CREST is to only store a subset of the smallest and most common n-grams
in the datastore with the hope of achieving comparable performance with less
storage space. We found that storing a subset of n-grams both reduces storage
space and improves performance. CREST matches REST's accepted token length with
10.6-13.5x less storage space and achieves a 16.5-17.1% higher acceptance
length than REST using the same storage space on the HumanEval and MT Bench
benchmarks.","[{'name': 'Sophia Ho'}, {'name': 'Jinsol Park'}, {'name': 'Patrick Wang'}]",2024-08-08T03:38:49Z
http://arxiv.org/abs/2408.04174v1,http://arxiv.org/abs/2408.04174v1,"wav2graph: A Framework for Supervised Learning Knowledge Graph from
  Speech","Knowledge graphs (KGs) enhance the performance of large language models
(LLMs) and search engines by providing structured, interconnected data that
improves reasoning and context-awareness. However, KGs only focus on text data,
thereby neglecting other modalities such as speech. In this work, we introduce
wav2graph, the first framework for supervised learning knowledge graph from
speech data. Our pipeline are straightforward: (1) constructing a KG based on
transcribed spoken utterances and a named entity database, (2) converting KG
into embedding vectors, and (3) training graph neural networks (GNNs) for node
classification and link prediction tasks. Through extensive experiments
conducted in inductive and transductive learning contexts using
state-of-the-art GNN models, we provide baseline results and error analysis for
node classification and link prediction tasks on human transcripts and
automatic speech recognition (ASR) transcripts, including evaluations using
both encoder-based and decoder-based node embeddings, as well as monolingual
and multilingual acoustic pre-trained models. All related code, data, and
models are published online.","[{'name': 'Khai Le-Duc'}, {'name': 'Quy-Anh Dang'}, {'name': 'Tan-Hanh Pham'}, {'name': 'Truong-Son Hy'}]",2024-08-08T02:36:04Z
http://arxiv.org/abs/2408.04167v1,http://arxiv.org/abs/2408.04167v1,mbrs: A Library for Minimum Bayes Risk Decoding,"Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks
that outperforms conventional maximum a posterior (MAP) decoding using beam
search by selecting high-quality outputs based on a utility function rather
than those with high-probability. Typically, it finds the most suitable
hypothesis from the set of hypotheses under the sampled pseudo-references. mbrs
is a library of MBR decoding, which can flexibly combine various metrics,
alternative expectation estimations, and algorithmic variants. It is designed
with a focus on speed measurement and calling count of code blocks,
transparency, reproducibility, and extensibility, which are essential for
researchers and developers. We published our mbrs as an MIT-licensed
open-source project, and the code is available on GitHub.
  GitHub: https://github.com/naist-nlp/mbrs","[{'name': 'Hiroyuki Deguchi'}, {'name': 'Yusuke Sakai'}, {'name': 'Hidetaka Kamigaito'}, {'name': 'Taro Watanabe'}]",2024-08-08T02:28:32Z
http://arxiv.org/abs/2408.04162v1,http://arxiv.org/abs/2408.04162v1,"Semantics or spelling? Probing contextual word embeddings with
  orthographic noise","Pretrained language model (PLM) hidden states are frequently employed as
contextual word embeddings (CWE): high-dimensional representations that encode
semantic information given linguistic context. Across many areas of
computational linguistics research, similarity between CWEs is interpreted as
semantic similarity. However, it remains unclear exactly what information is
encoded in PLM hidden states. We investigate this practice by probing PLM
representations using minimal orthographic noise. We expect that if CWEs
primarily encode semantic information, a single character swap in the input
word will not drastically affect the resulting representation,given sufficient
linguistic context. Surprisingly, we find that CWEs generated by popular PLMs
are highly sensitive to noise in input data, and that this sensitivity is
related to subword tokenization: the fewer tokens used to represent a word at
input, the more sensitive its corresponding CWE. This suggests that CWEs
capture information unrelated to word-level meaning and can be manipulated
through trivial modifications of input data. We conclude that these PLM-derived
CWEs may not be reliable semantic proxies, and that caution is warranted when
interpreting representational similarity","[{'name': 'Jacob A. Matthews'}, {'name': 'John R. Starr'}, {'name': 'Marten van Schijndel'}]",2024-08-08T02:07:25Z
http://arxiv.org/abs/2408.04140v1,http://arxiv.org/abs/2408.04140v1,UNLEARN Efficient Removal of Knowledge in Large Language Models,"Given the prevalence of large language models (LLMs) and the prohibitive cost
of training these models from scratch, dynamically forgetting specific
knowledge e.g., private or proprietary, without retraining the model has become
an important capability. This paper proposes a novel method to achieve this
objective called UNLEARN. The approach builds upon subspace methods to identify
and specifically target the removal of knowledge without adversely affecting
other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can
be forgotten while maintaining performance on other knowledge within 2.5% of
the original model, significantly outperforming the discriminatory abilities of
the previous state-of-the-art. A dual method called LEARN is also proposed for
targeted knowledge addition. Results show LEARN can match the fine-tuning
accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar
tasks.","[{'name': 'Tyler Lizzo'}, {'name': 'Larry Heck'}]",2024-08-08T00:53:31Z
http://arxiv.org/abs/2408.04138v1,http://arxiv.org/abs/2408.04138v1,"Enhancing Healthcare through Large Language Models: A Study on Medical
  Question Answering","In recent years, the application of Large Language Models (LLMs) in
healthcare has shown significant promise in improving the accessibility and
dissemination of medical knowledge. This paper presents a detailed study of
various LLMs trained on the MedQuAD medical question-answering dataset, with a
focus on identifying the most effective model for providing accurate medical
information. Among the models tested, the Sentence-t5 combined with Mistral 7B
demonstrated superior performance, achieving a precision score of 0.762. This
model's enhanced capabilities are attributed to its advanced pretraining
techniques, robust architecture, and effective prompt construction
methodologies. By leveraging these strengths, the Sentence-t5 + Mistral 7B
model excels in understanding and generating precise medical answers. Our
findings highlight the potential of integrating sophisticated LLMs in medical
contexts to facilitate efficient and accurate medical knowledge retrieval, thus
significantly enhancing patient education and support.","[{'name': 'Haoran Yu'}, {'name': 'Chang Yu'}, {'name': 'Zihan Wang'}, {'name': 'Dongxian Zou'}, {'name': 'Hao Qin'}]",2024-08-08T00:35:39Z
http://arxiv.org/abs/2408.04127v1,http://arxiv.org/abs/2408.04127v1,"Incorporating Spatial Awareness in Data-Driven Gesture Generation for
  Virtual Agents","This paper focuses on enhancing human-agent communication by integrating
spatial context into virtual agents' non-verbal behaviors, specifically
gestures. Recent advances in co-speech gesture generation have primarily
utilized data-driven methods, which create natural motion but limit the scope
of gestures to those performed in a void. Our work aims to extend these methods
by enabling generative models to incorporate scene information into
speech-driven gesture synthesis. We introduce a novel synthetic gesture dataset
tailored for this purpose. This development represents a critical step toward
creating embodied conversational agents that interact more naturally with their
environment and users.","[{'name': 'Anna Deichler'}, {'name': 'Simon Alexanderson'}, {'name': 'Jonas Beskow'}]",2024-08-07T23:23:50Z
http://arxiv.org/abs/2408.04121v1,http://arxiv.org/abs/2408.04121v1,"Can Rule-Based Insights Enhance LLMs for Radiology Report
  Classification? Introducing the RadPrompt Methodology","Developing imaging models capable of detecting pathologies from chest X-rays
can be cost and time-prohibitive for large datasets as it requires supervision
to attain state-of-the-art performance. Instead, labels extracted from
radiology reports may serve as distant supervision since these are routinely
generated as part of clinical practice. Despite their widespread use, current
rule-based methods for label extraction rely on extensive rule sets that are
limited in their robustness to syntactic variability. To alleviate these
limitations, we introduce RadPert, a rule-based system that integrates an
uncertainty-aware information schema with a streamlined set of rules, enhancing
performance. Additionally, we have developed RadPrompt, a multi-turn prompting
strategy that leverages RadPert to bolster the zero-shot predictive
capabilities of large language models, achieving a statistically significant
improvement in weighted average F1 score over GPT-4 Turbo. Most notably,
RadPrompt surpasses both its underlying models, showcasing the synergistic
potential of LLMs with rule-based models. We have evaluated our methods on two
English Corpora: the MIMIC-CXR gold-standard test set and a gold-standard
dataset collected from the Cambridge University Hospitals.","[{'name': 'Panagiotis Fytas'}, {'name': 'Anna Breger'}, {'name': 'Ian Selby'}, {'name': 'Simon Baker'}, {'name': 'Shahab Shahipasand'}, {'name': 'Anna Korhonen'}]",2024-08-07T23:09:23Z
http://arxiv.org/abs/2408.04114v1,http://arxiv.org/abs/2408.04114v1,Zero-shot Factual Consistency Evaluation Across Domains,"This work addresses the challenge of factual consistency in text generation
systems. We unify the tasks of Natural Language Inference, Summarization
Evaluation, Factuality Verification and Factual Consistency Evaluation to train
models capable of evaluating the factual consistency of source-target pairs
across diverse domains. We rigorously evaluate these against eight baselines on
a comprehensive benchmark suite comprising 22 datasets that span various tasks,
domains, and document lengths. Results demonstrate that our method achieves
state-of-the-art performance on this heterogeneous benchmark while addressing
efficiency concerns and attaining cross-domain generalization.",[{'name': 'Raunak Agarwal'}],2024-08-07T22:32:19Z
http://arxiv.org/abs/2408.04112v1,http://arxiv.org/abs/2408.04112v1,"Patchview: LLM-Powered Worldbuilding with Generative Dust and Magnet
  Visualization","Large language models (LLMs) can help writers build story worlds by
generating world elements, such as factions, characters, and locations.
However, making sense of many generated elements can be overwhelming. Moreover,
if the user wants to precisely control aspects of generated elements that are
difficult to specify verbally, prompting alone may be insufficient. We
introduce Patchview, a customizable LLM-powered system that visually aids
worldbuilding by allowing users to interact with story concepts and elements
through the physical metaphor of magnets and dust. Elements in Patchview are
visually dragged closer to concepts with high relevance, facilitating
sensemaking. The user can also steer the generation with verbally elusive
concepts by indicating the desired position of the element between concepts.
When the user disagrees with the LLM's visualization and generation, they can
correct those by repositioning the element. These corrections can be used to
align the LLM's future behaviors to the user's perception. With a user study,
we show that Patchview supports the sensemaking of world elements and steering
of element generation, facilitating exploration during the worldbuilding
process. Patchview provides insights on how customizable visual representation
can help sensemake, steer, and align generative AI model behaviors with the
user's intentions.","[{'name': 'John Joon Young Chung'}, {'name': 'Max Kreminski'}]",2024-08-07T22:27:19Z
http://arxiv.org/abs/2408.04093v3,http://arxiv.org/abs/2408.04093v3,"Tree Attention: Topology-aware Decoding for Long-Context Attention on
  GPU clusters","Self-attention is the core mathematical operation of modern transformer
architectures and is also a significant computational bottleneck due to its
quadratic complexity in the sequence length. In this work, we derive the scalar
energy function whose gradient computes the self-attention block, thus
elucidating the theoretical underpinnings of self-attention, providing a
Bayesian interpretation of the operation and linking it closely with
energy-based models such as Hopfield Networks. Our formulation reveals that the
reduction across the sequence axis can be efficiently computed in parallel
through a tree reduction. Our algorithm, for parallelizing attention
computation across multiple GPUs enables cross-device decoding to be performed
asymptotically faster (up to 8x faster in our experiments) than alternative
approaches such as Ring Attention, while also requiring significantly less
communication volume and incurring 2x less peak memory. Our code is publicly
available here: \url{https://github.com/Zyphra/tree_attention}.","[{'name': 'Vasudev Shyam'}, {'name': 'Jonathan Pilault'}, {'name': 'Emily Shepperd'}, {'name': 'Quentin Anthony'}, {'name': 'Beren Millidge'}]",2024-08-07T21:16:55Z
http://arxiv.org/abs/2408.04675v1,http://arxiv.org/abs/2408.04675v1,ACL Ready: RAG Based Assistant for the ACL Checklist,"The ARR Responsible NLP Research checklist website states that the ""checklist
is designed to encourage best practices for responsible research, addressing
issues of research ethics, societal impact and reproducibility."" Answering the
questions is an opportunity for authors to reflect on their work and make sure
any shared scientific assets follow best practices. Ideally, considering the
checklist before submission can favorably impact the writing of a research
paper. However, the checklist is often filled out at the last moment. In this
work, we introduce ACLReady, a retrieval-augmented language model application
that can be used to empower authors to reflect on their work and assist authors
with the ACL checklist. To test the effectiveness of the system, we conducted a
qualitative study with 13 users which shows that 92% of users found the
application useful and easy to use as well as 77% of the users found that the
application provided the information they expected. Our code is publicly
available under the CC BY-NC 4.0 license on GitHub.","[{'name': 'Michael Galarnyk'}, {'name': 'Rutwik Routu'}, {'name': 'Kosha Bheda'}, {'name': 'Priyanshu Mehta'}, {'name': 'Agam Shah'}, {'name': 'Sudheer Chava'}]",2024-08-07T21:07:13Z
http://arxiv.org/abs/2408.04029v1,http://arxiv.org/abs/2408.04029v1,"Human Speech Perception in Noise: Can Large Language Models Paraphrase
  to Improve It?","Large Language Models (LLMs) can generate text by transferring style
attributes like formality resulting in formal or informal text. However,
instructing LLMs to generate text that when spoken, is more intelligible in an
acoustically difficult environment, is an under-explored topic. We conduct the
first study to evaluate LLMs on a novel task of generating acoustically
intelligible paraphrases for better human speech perception in noise. Our
experiments in English demonstrated that with standard prompting, LLMs struggle
to control the non-textual attribute, i.e., acoustic intelligibility, while
efficiently capturing the desired textual attributes like semantic equivalence.
To remedy this issue, we propose a simple prompting approach,
prompt-and-select, which generates paraphrases by decoupling the desired
textual and non-textual attributes in the text generation pipeline. Our
approach resulted in a 40% relative improvement in human speech perception, by
paraphrasing utterances that are highly distorted in a listening condition with
babble noise at a signal-to-noise ratio (SNR) -5 dB. This study reveals the
limitation of LLMs in capturing non-textual attributes, and our proposed method
showcases the potential of using LLMs for better human speech perception in
noise.","[{'name': 'Anupama Chingacham'}, {'name': 'Miaoran Zhang'}, {'name': 'Vera Demberg'}, {'name': 'Dietrich Klakow'}]",2024-08-07T18:24:23Z
http://arxiv.org/abs/2408.04023v1,http://arxiv.org/abs/2408.04023v1,"Improving Large Language Model (LLM) fidelity through context-aware
  grounding: A systematic approach to reliability and veracity","As Large Language Models (LLMs) become increasingly sophisticated and
ubiquitous in natural language processing (NLP) applications, ensuring their
robustness, trustworthiness, and alignment with human values has become a
critical challenge. This paper presents a novel framework for contextual
grounding in textual models, with a particular emphasis on the Context
Representation stage. Our approach aims to enhance the reliability and ethical
alignment of these models through a comprehensive, context-aware methodology.
By explicitly capturing and representing relevant situational, cultural, and
ethical contexts in a machine-readable format, we lay the foundation for
anchoring a model's behavior within these contexts. Our approach leverages
techniques from knowledge representation and reasoning, such as ontologies,
semantic web technologies, and logic-based formalisms. We evaluate our
framework on real-world textual datasets, demonstrating its effectiveness in
improving model performance, fairness, and alignment with human expectations,
while maintaining high accuracy. Furthermore, we discuss the other key
components of the framework, including context-aware encoding, context-aware
learning, interpretability and explainability, and continuous monitoring and
adaptation. This research contributes to the growing body of work on
responsible AI, offering a practical approach to developing more reliable,
trustworthy, and ethically-aligned language models. Our findings have
significant implications for the deployment of LLMs in sensitive domains such
as healthcare, legal systems, and social services, where contextual
understanding is paramount.","[{'name': 'Wrick Talukdar'}, {'name': 'Anjanava Biswas'}]",2024-08-07T18:12:02Z
http://arxiv.org/abs/2408.07081v3,http://arxiv.org/abs/2408.07081v3,"MathBridge: A Large Corpus Dataset for Translating Spoken Mathematical
  Expressions into $LaTeX$ Formulas for Improved Readability","Improving the readability of mathematical expressions in text-based document
such as subtitle of mathematical video, is an significant task. To achieve
this, mathematical expressions should be convert to compiled formulas. For
instance, the spoken expression ``x equals minus b plus or minus the square
root of b squared minus four a c, all over two a'' from automatic speech
recognition is more readily comprehensible when displayed as a compiled formula
$x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$. To convert mathematical spoken
sentences to compiled formulas, two processes are required: spoken sentences
are converted into LaTeX formulas, and LaTeX formulas are converted into
compiled formulas. The latter can be managed by using LaTeX engines. However,
there is no way to do the former effectively. Even if we try to solve this
using language models, there is no paired data between spoken sentences and
LaTeX formulas to train it. In this paper, we introduce MathBridge, the first
extensive dataset for translating mathematical spoken sentences into LaTeX
formulas. MathBridge comprises approximately 23 million LaTeX formulas paired
with the corresponding mathematical spoken sentences. Through comprehensive
evaluations, including fine-tuning with proposed data, we discovered that
MathBridge significantly enhances the capabilities of pretrained language
models for converting to LaTeX formulas from mathematical spoken sentences.
Specifically, for the T5-large model, the sacreBLEU score increased from 4.77
to 46.8, demonstrating substantial enhancement.","[{'name': 'Kyudan Jung'}, {'name': 'Sieun Hyeon'}, {'name': 'Jeong Youn Kwon'}, {'name': 'Nam-Joon Kim'}, {'name': 'Hyun Gon Ryu'}, {'name': 'Hyuk-Jae Lee'}, {'name': 'Jaeyoung Do'}]",2024-08-07T18:07:15Z
http://arxiv.org/abs/2408.04015v1,http://arxiv.org/abs/2408.04015v1,Image-to-LaTeX Converter for Mathematical Formulas and Text,"In this project, we train a vision encoder-decoder model to generate LaTeX
code from images of mathematical formulas and text. Utilizing a diverse
collection of image-to-LaTeX data, we build two models: a base model with a
Swin Transformer encoder and a GPT-2 decoder, trained on machine-generated
images, and a fine-tuned version enhanced with Low-Rank Adaptation (LoRA)
trained on handwritten formulas. We then compare the BLEU performance of our
specialized model on a handwritten test set with other similar models, such as
Pix2Text, TexTeller, and Sumen. Through this project, we contribute open-source
models for converting images to LaTeX and provide from-scratch code for
building these models with distributed training and GPU optimizations.","[{'name': 'Daniil Gurgurov'}, {'name': 'Aleksey Morshnev'}]",2024-08-07T18:04:01Z
http://arxiv.org/abs/2408.03936v1,http://arxiv.org/abs/2408.03936v1,"SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic
  Performance for Mercosur Common Nomenclature","Natural language processing (NLP) has seen significant advancements with the
advent of large language models (LLMs). However, substantial improvements are
still needed for languages other than English, especially for specific domains
like the applications of Mercosur Common Nomenclature (NCM), a Brazilian
Harmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a
foundational Portuguese LLM, as an LLM source to implement the NCM application
processing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)
technique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.
This approach retains the chain-of-thought (CoT) methodology for prompt
development in a more concise and streamlined manner, utilizing brief and
focused documents for training. The proposed model demonstrates an efficient
and cost-effective alternative for fine-tuning smaller LLMs, significantly
outperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the
research focuses on NCM applications, the methodology can be easily adapted for
HS applications worldwide.","[{'name': 'Vinícius Di Oliveira'}, {'name': 'Yuri Façanha Bezerra'}, {'name': 'Li Weigang'}, {'name': 'Pedro Carvalho Brom'}, {'name': 'Victor Rafael R. Celestino'}]",2024-08-07T17:54:21Z
http://arxiv.org/abs/2408.03934v1,http://arxiv.org/abs/2408.03934v1,From Words to Worth: Newborn Article Impact Prediction with LLM,"As the academic landscape expands, the challenge of efficiently identifying
potentially high-impact articles among the vast number of newly published works
becomes critical. This paper introduces a promising approach, leveraging the
capabilities of fine-tuned LLMs to predict the future impact of newborn
articles solely based on titles and abstracts. Moving beyond traditional
methods heavily reliant on external information, the proposed method discerns
the shared semantic features of highly impactful papers from a large collection
of title-abstract and potential impact pairs. These semantic features are
further utilized to regress an improved metric, TNCSI_SP, which has been
endowed with value, field, and time normalization properties. Additionally, a
comprehensive dataset has been constructed and released for fine-tuning the
LLM, containing over 12,000 entries with corresponding titles, abstracts, and
TNCSI_SP. The quantitative results, with an NDCG@20 of 0.901, demonstrate that
the proposed approach achieves state-of-the-art performance in predicting the
impact of newborn articles when compared to competitive counterparts. Finally,
we demonstrate a real-world application for predicting the impact of newborn
journal articles to demonstrate its noteworthy practical value. Overall, our
findings challenge existing paradigms and propose a shift towards a more
content-focused prediction of academic impact, offering new insights for
assessing newborn article impact.","[{'name': 'Penghai Zhao'}, {'name': 'Qinghua Xing'}, {'name': 'Kairan Dou'}, {'name': 'Jinyu Tian'}, {'name': 'Ying Tai'}, {'name': 'Jian Yang'}, {'name': 'Ming-Ming Cheng'}, {'name': 'Xiang Li'}]",2024-08-07T17:52:02Z
http://arxiv.org/abs/2408.04673v1,http://arxiv.org/abs/2408.04673v1,AutoFAIR : Automatic Data FAIRification via Machine Reading,"The explosive growth of data fuels data-driven research, facilitating
progress across diverse domains. The FAIR principles emerge as a guiding
standard, aiming to enhance the findability, accessibility, interoperability,
and reusability of data. However, current efforts primarily focus on manual
data FAIRification, which can only handle targeted data and lack efficiency. To
address this issue, we propose AutoFAIR, an architecture designed to enhance
data FAIRness automately. Firstly, We align each data and metadata operation
with specific FAIR indicators to guide machine-executable actions. Then, We
utilize Web Reader to automatically extract metadata based on language models,
even in the absence of structured data webpage schemas. Subsequently, FAIR
Alignment is employed to make metadata comply with FAIR principles by ontology
guidance and semantic matching. Finally, by applying AutoFAIR to various data,
especially in the field of mountain hazards, we observe significant
improvements in findability, accessibility, interoperability, and reusability
of data. The FAIRness scores before and after applying AutoFAIR indicate
enhanced data value.","[{'name': 'Tingyan Ma'}, {'name': 'Wei Liu'}, {'name': 'Bin Lu'}, {'name': 'Xiaoying Gan'}, {'name': 'Yunqiang Zhu'}, {'name': 'Luoyi Fu'}, {'name': 'Chenghu Zhou'}]",2024-08-07T17:36:58Z
http://arxiv.org/abs/2408.03910v2,http://arxiv.org/abs/2408.03910v2,"CodexGraph: Bridging Large Language Models and Code Repositories via
  Code Graph Databases","Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval
and MBPP, but struggle with handling entire code repositories. This challenge
has prompted research on enhancing LLM-codebase interaction at a repository
scale. Current solutions rely on similarity-based retrieval or manual tools and
APIs, each with notable drawbacks. Similarity-based retrieval often has low
recall in complex tasks, while manual tools and APIs are typically
task-specific and require expert knowledge, reducing their generalizability
across diverse code tasks and real-world applications. To mitigate these
limitations, we introduce CodexGraph, a system that integrates LLM agents with
graph database interfaces extracted from code repositories. By leveraging the
structural properties of graph databases and the flexibility of the graph query
language, CodexGraph enables the LLM agent to construct and execute queries,
allowing for precise, code structure-aware context retrieval and code
navigation. We assess CodexGraph using three benchmarks: CrossCodeEval,
SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding
applications. With a unified graph database schema, CodexGraph demonstrates
competitive performance and potential in both academic and real-world
environments, showcasing its versatility and efficacy in software engineering.
Our application demo:
https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.","[{'name': 'Xiangyan Liu'}, {'name': 'Bo Lan'}, {'name': 'Zhiyuan Hu'}, {'name': 'Yang Liu'}, {'name': 'Zhicheng Zhang'}, {'name': 'Fei Wang'}, {'name': 'Michael Shieh'}, {'name': 'Wenmeng Zhou'}]",2024-08-07T17:13:59Z
http://arxiv.org/abs/2408.03907v1,http://arxiv.org/abs/2408.03907v1,"Decoding Biases: Automated Methods and LLM Judges for Gender Bias
  Detection in Language Models","Large Language Models (LLMs) have excelled at language understanding and
generating human-level text. However, even with supervised training and human
alignment, these LLMs are susceptible to adversarial attacks where malicious
users can prompt the model to generate undesirable text. LLMs also inherently
encode potential biases that can cause various harmful effects during
interactions. Bias evaluation metrics lack standards as well as consensus and
existing methods often rely on human-generated templates and annotations which
are expensive and labor intensive. In this work, we train models to
automatically create adversarial prompts to elicit biased responses from target
LLMs. We present LLM- based bias evaluation metrics and also analyze several
existing automatic evaluation methods and metrics. We analyze the various
nuances of model responses, identify the strengths and weaknesses of model
families, and assess where evaluation methods fall short. We compare these
metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns
with human judgement on bias in response generation.","[{'name': 'Shachi H Kumar'}, {'name': 'Saurav Sahay'}, {'name': 'Sahisnu Mazumder'}, {'name': 'Eda Okur'}, {'name': 'Ramesh Manuvinakurike'}, {'name': 'Nicole Beckage'}, {'name': 'Hsuan Su'}, {'name': 'Hung-yi Lee'}, {'name': 'Lama Nachman'}]",2024-08-07T17:11:34Z
http://arxiv.org/abs/2408.03900v1,http://arxiv.org/abs/2408.03900v1,Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond,"We present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU)
dataset comprising the speech counterpart for a portion of the MASSIVE textual
corpus. Speech-MASSIVE covers 12 languages from different families and inherits
from MASSIVE the annotations for the intent prediction and slot-filling tasks.
Our extension is prompted by the scarcity of massively multilingual SLU
datasets and the growing need for versatile speech datasets to assess
foundation models (LLMs, speech encoders) across languages and tasks. We
provide a multimodal, multitask, multilingual dataset and report SLU baselines
using both cascaded and end-to-end architectures in various training scenarios
(zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate the
suitability of Speech-MASSIVE for benchmarking other tasks such as speech
transcription, language identification, and speech translation. The dataset,
models, and code are publicly available at:
https://github.com/hlt-mt/Speech-MASSIVE","[{'name': 'Beomseok Lee'}, {'name': 'Ioan Calapodescu'}, {'name': 'Marco Gaido'}, {'name': 'Matteo Negri'}, {'name': 'Laurent Besacier'}]",2024-08-07T16:55:28Z
http://arxiv.org/abs/2408.03899v1,http://arxiv.org/abs/2408.03899v1,Simplifying Scholarly Abstracts for Accessible Digital Libraries,"Standing at the forefront of knowledge dissemination, digital libraries
curate vast collections of scientific literature. However, these scholarly
writings are often laden with jargon and tailored for domain experts rather
than the general public. As librarians, we strive to offer services to a
diverse audience, including those with lower reading levels. To extend our
services beyond mere access, we propose fine-tuning a language model to rewrite
scholarly abstracts into more comprehensible versions, thereby making scholarly
literature more accessible when requested. We began by introducing a corpus
specifically designed for training models to simplify scholarly abstracts. This
corpus consists of over three thousand pairs of abstracts and significance
statements from diverse disciplines. We then fine-tuned four language models
using this corpus. The outputs from the models were subsequently examined both
quantitatively for accessibility and semantic coherence, and qualitatively for
language quality, faithfulness, and completeness. Our findings show that the
resulting models can improve readability by over three grade levels, while
maintaining fidelity to the original content. Although commercial
state-of-the-art models still hold an edge, our models are much more compact,
can be deployed locally in an affordable manner, and alleviate the privacy
concerns associated with using commercial models. We envision this work as a
step toward more inclusive and accessible libraries, improving our services for
young readers and those without a college degree.","[{'name': 'Haining Wang'}, {'name': 'Jason Clark'}]",2024-08-07T16:55:00Z
http://arxiv.org/abs/2408.03874v1,http://arxiv.org/abs/2408.03874v1,Personalized Clinical Note Generation from Doctor-Patient Conversations,"In this work, we present a novel technique to improve the quality of draft
clinical notes for physicians. This technique is concentrated on the ability to
model implicit physician conversation styles and note preferences. We also
introduce a novel technique for the enrollment of new physicians when a limited
number of clinical notes paired with conversations are available for that
physician, without the need to re-train a model to support them. We show that
our technique outperforms the baseline model by improving the ROUGE-2 score of
the History of Present Illness section by 13.8%, the Physical Examination
section by 88.6%, and the Assessment & Plan section by 50.8%.","[{'name': 'Nathan Brake'}, {'name': 'Thomas Schaaf'}]",2024-08-07T16:24:01Z
http://arxiv.org/abs/2408.03871v1,http://arxiv.org/abs/2408.03871v1,"BeeManc at the PLABA Track of TAC-2023: Investigating LLMs and
  Controllable Attributes for Improving Biomedical Text Readability","In this system report, we describe the models and methods we used for our
participation in the PLABA2023 task on biomedical abstract simplification, part
of the TAC 2023 tracks. The system outputs we submitted come from the following
three categories: 1) domain fine-tuned T5-like models including Biomedical-T5
and Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes
(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work we
carried out for this task on BioGPT finetuning. In the official automatic
evaluation using SARI scores, BeeManc ranks 2nd among all teams and our model
LaySciFive ranks 3rd among all 13 evaluated systems. In the official human
evaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score
92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It
also produced a high score 91.57 on Fluency in comparison to the highest score
93.53. In the second round of submissions, our team using ChatGPT-prompting
ranks the 2nd in several categories including simplified term accuracy score
92.26 and completeness score 96.58, and a very similar score on faithfulness
score 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Our
codes, fine-tuned models, prompts, and data splits from the system development
stage will be available at https://github.com/ HECTA-UoM/PLABA-MU","[{'name': 'Zihao Li'}, {'name': 'Samuel Belkadi'}, {'name': 'Nicolo Micheletti'}, {'name': 'Lifeng Han'}, {'name': 'Matthew Shardlow'}, {'name': 'Goran Nenadic'}]",2024-08-07T16:21:41Z
http://arxiv.org/abs/2408.03855v1,http://arxiv.org/abs/2408.03855v1,Why transformers are obviously good models of language,"Nobody knows how language works, but many theories abound. Transformers are a
class of neural networks that process language automatically with more success
than alternatives, both those based on neural computations and those that rely
on other (e.g. more symbolic) mechanisms. Here, I highlight direct connections
between the transformer architecture and certain theoretical perspectives on
language. The empirical success of transformers relative to alternative models
provides circumstantial evidence that the linguistic approaches that
transformers embody should be, at least, evaluated with greater scrutiny by the
linguistics community and, at best, considered to be the currently best
available theories.",[{'name': 'Felix Hill'}],2024-08-07T15:52:46Z
http://arxiv.org/abs/2408.03849v1,http://arxiv.org/abs/2408.03849v1,"Hate Speech Detection and Classification in Amharic Text with Deep
  Learning","Hate speech is a growing problem on social media. It can seriously impact
society, especially in countries like Ethiopia, where it can trigger conflicts
among diverse ethnic and religious groups. While hate speech detection in
resource rich languages are progressing, for low resource languages such as
Amharic are lacking. To address this gap, we develop Amharic hate speech data
and SBi-LSTM deep learning model that can detect and classify text into four
categories of hate speech: racial, religious, gender, and non-hate speech. We
have annotated 5k Amharic social media post and comment data into four
categories. The data is annotated using a custom annotation tool by a total of
100 native Amharic speakers. The model achieves a 94.8 F1-score performance.
Future improvements will include expanding the dataset and develop state-of-the
art models.
  Keywords: Amharic hate speech detection, classification, Amharic dataset,
Deep Learning, SBi-LSTM","[{'name': 'Samuel Minale Gashe'}, {'name': 'Seid Muhie Yimam'}, {'name': 'Yaregal Assabie'}]",2024-08-07T15:46:45Z
http://arxiv.org/abs/2408.03837v2,http://arxiv.org/abs/2408.03837v2,"WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language
  Models","WalledEval is a comprehensive AI safety testing toolkit designed to evaluate
large language models (LLMs). It accommodates a diverse range of models,
including both open-weight and API-based ones, and features over 35 safety
benchmarks covering areas such as multilingual safety, exaggerated safety, and
prompt injections. The framework supports both LLM and judge benchmarking, and
incorporates custom mutators to test safety against various text-style
mutations such as future tense and paraphrasing. Additionally, WalledEval
introduces WalledGuard, a new, small and performant content moderation tool,
and SGXSTest, a benchmark for assessing exaggerated safety in cultural
contexts. We make WalledEval publicly available at
https://github.com/walledai/walledeval","[{'name': 'Prannaya Gupta'}, {'name': 'Le Qi Yau'}, {'name': 'Hao Han Low'}, {'name': 'I-Shiang Lee'}, {'name': 'Hugo Maximus Lim'}, {'name': 'Yu Xin Teoh'}, {'name': 'Jia Hng Koh'}, {'name': 'Dar Win Liew'}, {'name': 'Rishabh Bhardwaj'}, {'name': 'Rajat Bhardwaj'}, {'name': 'Soujanya Poria'}]",2024-08-07T15:22:44Z
http://arxiv.org/abs/2408.03819v1,http://arxiv.org/abs/2408.03819v1,"Leveraging Variation Theory in Counterfactual Data Augmentation for
  Optimized Active Learning","Active Learning (AL) allows models to learn interactively from user feedback.
This paper introduces a counterfactual data augmentation approach to AL,
particularly addressing the selection of datapoints for user querying, a
pivotal concern in enhancing data efficiency. Our approach is inspired by
Variation Theory, a theory of human concept learning that emphasizes the
essential features of a concept by focusing on what stays the same and what
changes. Instead of just querying with existing datapoints, our approach
synthesizes artificial datapoints that highlight potential key similarities and
differences among labels using a neuro-symbolic pipeline combining large
language models (LLMs) and rule-based models. Through an experiment in the
example domain of text classification, we show that our approach achieves
significantly higher performance when there are fewer annotated data. As the
annotated training data gets larger the impact of the generated data starts to
diminish showing its capability to address the cold start problem in AL. This
research sheds light on integrating theories of human learning into the
optimization of AL.","[{'name': 'Simret Araya Gebreegziabher'}, {'name': 'Kuangshi Ai'}, {'name': 'Zheng Zhang'}, {'name': 'Elena L. Glassman'}, {'name': 'Toby Jia-Jun Li'}]",2024-08-07T14:55:04Z
http://arxiv.org/abs/2408.03811v1,http://arxiv.org/abs/2408.03811v1,"Generative Language Models with Retrieval Augmented Generation for
  Automated Short Answer Scoring","Automated Short Answer Scoring (ASAS) is a critical component in educational
assessment. While traditional ASAS systems relied on rule-based algorithms or
complex deep learning methods, recent advancements in Generative Language
Models (GLMs) offer new opportunities for improvement. This study explores the
application of GLMs to ASAS, leveraging their off-the-shelf capabilities and
performance in various domains. We propose a novel pipeline that combines
vector databases, transformer-based encoders, and GLMs to enhance short answer
scoring accuracy. Our approach stores training responses in a vector database,
retrieves semantically similar responses during inference, and employs a GLM to
analyze these responses and determine appropriate scores. We further optimize
the system through fine-tuned retrieval processes and prompt engineering.
Evaluation on the SemEval 2013 dataset demonstrates a significant improvement
on the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,
highlighting the potential of GLMs in advancing ASAS technology.","[{'name': 'Zifan Wang'}, {'name': 'Christopher Ormerod'}]",2024-08-07T14:42:13Z
http://arxiv.org/abs/2408.04671v1,http://arxiv.org/abs/2408.04671v1,Prompt and Prejudice,"This paper investigates the impact of using first names in Large Language
Models (LLMs) and Vision Language Models (VLMs), particularly when prompted
with ethical decision-making tasks. We propose an approach that appends first
names to ethically annotated text scenarios to reveal demographic biases in
model outputs. Our study involves a curated list of more than 300 names
representing diverse genders and ethnic backgrounds, tested across thousands of
moral scenarios. Following the auditing methodologies from social sciences we
propose a detailed analysis involving popular LLMs/VLMs to contribute to the
field of responsible AI by emphasizing the importance of recognizing and
mitigating biases in these systems. Furthermore, we introduce a novel
benchmark, the Pratical Scenarios Benchmark (PSB), designed to assess the
presence of biases involving gender or demographic prejudices in everyday
decision-making scenarios as well as practical scenarios where an LLM might be
used to make sensible decisions (e.g., granting mortgages or insurances). This
benchmark allows for a comprehensive comparison of model behaviors across
different demographic categories, highlighting the risks and biases that may
arise in practical applications of LLMs and VLMs.","[{'name': 'Lorenzo Berlincioni'}, {'name': 'Luca Cultrera'}, {'name': 'Federico Becattini'}, {'name': 'Marco Bertini'}, {'name': 'Alberto Del Bimbo'}]",2024-08-07T14:11:33Z
http://arxiv.org/abs/2408.03762v1,http://arxiv.org/abs/2408.03762v1,"'Finance Wizard' at the FinLLM Challenge Task: Financial Text
  Summarization","This paper presents our participation under the team name `Finance Wizard' in
the FinNLP-AgentScen 2024 shared task #2: Financial Text Summarization. It
documents our pipeline approach of fine-tuning a foundation model into a
task-specific model for Financial Text Summarization. It involves (1) adapting
Llama3 8B, a foundation model, to the Finance domain via continued
pre-training, (2) multi-task instruction-tuning to further equip the model with
more finance-related capabilities, (3) finally fine-tuning the model into a
task-specific `expert'. Our model, FinLlama3\_sum, yielded commendable results,
securing the third position in its category with a ROUGE-1 score of 0.521.","[{'name': 'Meisin Lee'}, {'name': 'Soon Lay-Ki'}]",2024-08-07T13:31:44Z
http://arxiv.org/abs/2408.03732v1,http://arxiv.org/abs/2408.03732v1,"Question Rephrasing for Quantifying Uncertainty in Large Language
  Models: Applications in Molecular Chemistry Tasks","Uncertainty quantification enables users to assess the reliability of
responses generated by large language models (LLMs). We present a novel
Question Rephrasing technique to evaluate the input uncertainty of LLMs, which
refers to the uncertainty arising from equivalent variations of the inputs
provided to LLMs. This technique is integrated with sampling methods that
measure the output uncertainty of LLMs, thereby offering a more comprehensive
uncertainty assessment. We validated our approach on property prediction and
reaction prediction for molecular chemistry tasks.","[{'name': 'Zizhang Chen'}, {'name': 'Pengyu Hong'}, {'name': 'Sandeep Madireddy'}]",2024-08-07T12:38:23Z
http://arxiv.org/abs/2408.03706v1,http://arxiv.org/abs/2408.03706v1,"Local Topology Measures of Contextual Language Model Latent Spaces With
  Applications to Dialogue Term Extraction","A common approach for sequence tagging tasks based on contextual word
representations is to train a machine learning classifier directly on these
embedding vectors. This approach has two shortcomings. First, such methods
consider single input sequences in isolation and are unable to put an
individual embedding vector in relation to vectors outside the current local
context of use. Second, the high performance of these models relies on
fine-tuning the embedding model in conjunction with the classifier, which may
not always be feasible due to the size or inaccessibility of the underlying
feature-generation model. It is thus desirable, given a collection of embedding
vectors of a corpus, i.e., a datastore, to find features of each vector that
describe its relation to other, similar vectors in the datastore. With this in
mind, we introduce complexity measures of the local topology of the latent
space of a contextual language model with respect to a given datastore. The
effectiveness of our features is demonstrated through their application to
dialogue term extraction. Our work continues a line of research that explores
the manifold hypothesis for word embeddings, demonstrating that local structure
in the space carved out by word embeddings can be exploited to infer semantic
properties.","[{'name': 'Benjamin Matthias Ruppik'}, {'name': 'Michael Heck'}, {'name': 'Carel van Niekerk'}, {'name': 'Renato Vukovic'}, {'name': 'Hsien-chin Lin'}, {'name': 'Shutong Feng'}, {'name': 'Marcus Zibrowius'}, {'name': 'Milica Gašić'}]",2024-08-07T11:44:32Z
http://arxiv.org/abs/2408.03675v2,http://arxiv.org/abs/2408.03675v2,"NACL: A General and Effective KV Cache Eviction Framework for LLMs at
  Inference Time","Large Language Models (LLMs) have ignited an innovative surge of AI
applications, marking a new era of exciting possibilities equipped with
extended context windows. However, hosting these models is cost-prohibitive
mainly due to the extensive memory consumption of KV Cache involving
long-context modeling. Despite several works proposing to evict unnecessary
tokens from the KV Cache, most of them rely on the biased local statistics of
accumulated attention scores and report performance using unconvincing metric
like perplexity on inadequate short-text evaluation. In this paper, we propose
NACL, a general framework for long-context KV cache eviction that achieves more
optimal and efficient eviction in a single operation during the encoding phase.
Due to NACL's efficiency, we combine more accurate attention score statistics
in PROXY TOKENS EVICTION with the diversified random eviction strategy of
RANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance
the robustness in maintaining pivotal tokens for long-context modeling tasks.
Notably, our method significantly improves the performance on short- and
long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%
with over 95% performance maintenance. The code is available at
https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.","[{'name': 'Yilong Chen'}, {'name': 'Guoxia Wang'}, {'name': 'Junyuan Shang'}, {'name': 'Shiyao Cui'}, {'name': 'Zhenyu Zhang'}, {'name': 'Tingwen Liu'}, {'name': 'Shuohuan Wang'}, {'name': 'Yu Sun'}, {'name': 'Dianhai Yu'}, {'name': 'Hua Wu'}]",2024-08-07T10:31:07Z
http://arxiv.org/abs/2408.03652v1,http://arxiv.org/abs/2408.03652v1,"mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest
  Neighbor Search","Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)
that aims to identify and classify entities in text into predefined categories.
However, when applied to Arabic data, NER encounters unique challenges stemming
from the language's rich morphological inflections, absence of capitalization
cues, and spelling variants, where a single word can comprise multiple
morphemes. In this paper, we introduce Arabic KNN-NER, our submission to the
Wojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the
shared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained
flat-entity recognition for Arabic text, where we identify a single main entity
and possibly zero or multiple sub-entities for each word. Arabic KNN-NER
augments the probability distribution of a fine-tuned model with another label
probability distribution derived from performing a KNN search over the cached
training data. Our submission achieved 91% on the test set on the WojoodFine
dataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.","[{'name': 'Ahmed Abdou'}, {'name': 'Tasneem Mohsen'}]",2024-08-07T09:34:55Z
http://arxiv.org/abs/2408.03633v2,http://arxiv.org/abs/2408.03633v2,CARE: A Clue-guided Assistant for CSRs to Read User Manuals,"It is time-saving to build a reading assistant for customer service
representations (CSRs) when reading user manuals, especially information-rich
ones. Current solutions don't fit the online custom service scenarios well due
to the lack of attention to user questions and possible responses. Hence, we
propose to develop a time-saving and careful reading assistant for CSRs, named
CARE. It can help the CSRs quickly find proper responses from the user manuals
via explicit clue chains. Specifically, each of the clue chains is formed by
inferring over the user manuals, starting from the question clue aligned with
the user question and ending at a possible response. To overcome the shortage
of supervised data, we adopt the self-supervised strategy for model learning.
The offline experiment shows that CARE is efficient in automatically inferring
accurate responses from the user manual. The online experiment further
demonstrates the superiority of CARE to reduce CSRs' reading burden and keep
high service quality, in particular with >35% decrease in time spent and
keeping a >0.75 ICC score.","[{'name': 'Weihong Du'}, {'name': 'Jia Liu'}, {'name': 'Zujie Wen'}, {'name': 'Dingnan Jin'}, {'name': 'Hongru Liang'}, {'name': 'Wenqiang Lei'}]",2024-08-07T08:44:44Z
http://arxiv.org/abs/2408.03631v1,http://arxiv.org/abs/2408.03631v1,"Large Language Models for Base Station Siting: Intelligent Deployment
  based on Prompt or Agent","Traditional base station siting (BSS) methods rely heavily on drive testing
and user feedback, which are laborious and require extensive expertise in
communication, networking, and optimization. As large language models (LLMs)
and their associated technologies advance, particularly in the realms of prompt
engineering and agent engineering, network optimization will witness a
revolutionary approach. This approach entails the strategic use of well-crafted
prompts to infuse human experience and knowledge into these sophisticated LLMs,
and the deployment of autonomous agents as a communication bridge to seamlessly
connect the machine language based LLMs with human users using natural
language. This integration represents the future paradigm of artificial
intelligence (AI) as a service and AI for more ease. As a preliminary
exploration, this research first develops a novel LLM-empowered BSS
optimization framework, and heuristically proposes four different potential
implementations: the strategies based on Prompt-optimized LLM (PoL),
human-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and
Cooperative multiple LLM-based autonomous BSS agents (CLaBa). Through
evaluation on real-world data, the experiments demonstrate that prompt-assisted
LLMs and LLM-based agents can generate more efficient, cost-effective, and
reliable network deployments, noticeably enhancing the efficiency of BSS
optimization and reducing trivial manual participation.","[{'name': 'Yanhu Wang'}, {'name': 'Muhammad Muzammil Afzal'}, {'name': 'Zhengyang Li'}, {'name': 'Jie Zhou'}, {'name': 'Chenyuan Feng'}, {'name': 'Shuaishuai Guo'}, {'name': 'Tony Q. S. Quek'}]",2024-08-07T08:43:32Z
http://arxiv.org/abs/2408.03630v2,http://arxiv.org/abs/2408.03630v2,PAGED: A Benchmark for Procedural Graphs Extraction from Documents,"Automatic extraction of procedural graphs from documents creates a low-cost
way for users to easily understand a complex procedure by skimming visual
graphs. Despite the progress in recent studies, it remains unanswered: whether
the existing studies have well solved this task (Q1) and whether the emerging
large language models (LLMs) can bring new opportunities to this task (Q2). To
this end, we propose a new benchmark PAGED, equipped with a large high-quality
dataset and standard evaluations. It investigates five state-of-the-art
baselines, revealing that they fail to extract optimal procedural graphs well
because of their heavy reliance on hand-written rules and limited available
data. We further involve three advanced LLMs in PAGED and enhance them with a
novel self-refine strategy. The results point out the advantages of LLMs in
identifying textual elements and their gaps in building logical structures. We
hope PAGED can serve as a major landmark for automatic procedural graph
extraction and the investigations in PAGED can offer insights into the research
on logic reasoning among non-sequential elements.","[{'name': 'Weihong Du'}, {'name': 'Wenrui Liao'}, {'name': 'Hongru Liang'}, {'name': 'Wenqiang Lei'}]",2024-08-07T08:43:18Z
http://arxiv.org/abs/2408.03622v1,http://arxiv.org/abs/2408.03622v1,"Improving the quality of Persian clinical text with a novel spelling
  correction system","Background: The accuracy of spelling in Electronic Health Records (EHRs) is a
critical factor for efficient clinical care, research, and ensuring patient
safety. The Persian language, with its abundant vocabulary and complex
characteristics, poses unique challenges for real-word error correction. This
research aimed to develop an innovative approach for detecting and correcting
spelling errors in Persian clinical text.
  Methods: Our strategy employs a state-of-the-art pre-trained model that has
been meticulously fine-tuned specifically for the task of spelling correction
in the Persian clinical domain. This model is complemented by an innovative
orthographic similarity matching algorithm, PERTO, which uses visual similarity
of characters for ranking correction candidates.
  Results: The evaluation of our approach demonstrated its robustness and
precision in detecting and rectifying word errors in Persian clinical text. In
terms of non-word error correction, our model achieved an F1-Score of 90.0%
when the PERTO algorithm was employed. For real-word error detection, our model
demonstrated its highest performance, achieving an F1-Score of 90.6%.
Furthermore, the model reached its highest F1-Score of 91.5% for real-word
error correction when the PERTO algorithm was employed.
  Conclusions: Despite certain limitations, our method represents a substantial
advancement in the field of spelling error detection and correction for Persian
clinical text. By effectively addressing the unique challenges posed by the
Persian language, our approach paves the way for more accurate and efficient
clinical documentation, contributing to improved patient care and safety.
Future research could explore its use in other areas of the Persian medical
domain, enhancing its impact and utility.","[{'name': 'Seyed Mohammad Sadegh Dashti'}, {'name': 'Seyedeh Fatemeh Dashti'}]",2024-08-07T08:31:42Z
http://arxiv.org/abs/2408.03618v1,http://arxiv.org/abs/2408.03618v1,A Logical Fallacy-Informed Framework for Argument Generation,"Despite the remarkable performance of Large Language Models (LLMs), they
still struggle with generating logically sound arguments, resulting in
potential risks such as spreading misinformation. An important factor
contributing to LLMs' suboptimal performance in generating coherent arguments
is their oversight of logical fallacies. To address this issue, we introduce
FIPO, a fallacy-informed framework that leverages preference optimization
methods to steer LLMs toward logically sound arguments. FIPO includes a
classification loss, to capture the fine-grained information on fallacy
categories. Our results on argumentation datasets show that our method reduces
the fallacy errors by up to 17.5%. Furthermore, our human evaluation results
indicate that the quality of the generated arguments by our method
significantly outperforms the fine-tuned baselines, as well as prior preference
optimization methods, such as DPO. These findings highlight the importance of
ensuring models are aware of logical fallacies for effective argument
generation.","[{'name': 'Luca Mouchel'}, {'name': 'Debjit Paul'}, {'name': 'Shaobo Cui'}, {'name': 'Robert West'}, {'name': 'Antoine Bosselut'}, {'name': 'Boi Faltings'}]",2024-08-07T08:19:44Z
http://arxiv.org/abs/2408.03617v1,http://arxiv.org/abs/2408.03617v1,Is Child-Directed Speech Effective Training Data for Language Models?,"While high-performing language models are typically trained on hundreds of
billions of words, human children become fluent language users with a much
smaller amount of data. What are the features of the data they receive, and how
do these features support language modeling objectives? To investigate this
question, we train GPT-2 models on 29M words of English-language child-directed
speech and a new matched, synthetic dataset (TinyDialogues), comparing to a
heterogeneous blend of datasets from the BabyLM challenge. We evaluate both the
syntactic and semantic knowledge of these models using developmentally-inspired
evaluations. Through pretraining experiments, we test whether the global
developmental ordering or the local discourse ordering of children's training
data support high performance relative to other datasets. The local properties
of the data affect model results, but somewhat surprisingly, global properties
do not. Further, child language input is not uniquely valuable for training
language models. These findings support the hypothesis that, rather than
proceeding from better data, children's learning is instead substantially more
efficient than current language modeling techniques.","[{'name': 'Steven Y. Feng'}, {'name': 'Noah D. Goodman'}, {'name': 'Michael C. Frank'}]",2024-08-07T08:18:51Z
http://arxiv.org/abs/2408.03615v1,http://arxiv.org/abs/2408.03615v1,"Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in
  Long-Horizon Tasks","Building a general-purpose agent is a long-standing vision in the field of
artificial intelligence. Existing agents have made remarkable progress in many
domains, yet they still struggle to complete long-horizon tasks in an open
world. We attribute this to the lack of necessary world knowledge and
multimodal experience that can guide agents through a variety of long-horizon
tasks. In this paper, we propose a Hybrid Multimodal Memory module to address
the above challenges. It 1) transforms knowledge into Hierarchical Directed
Knowledge Graph that allows agents to explicitly represent and learn world
knowledge, and 2) summarises historical information into Abstracted Multimodal
Experience Pool that provide agents with rich references for in-context
learning. On top of the Hybrid Multimodal Memory module, a multimodal agent,
Optimus-1, is constructed with dedicated Knowledge-guided Planner and
Experience-Driven Reflector, contributing to a better planning and reflection
in the face of long-horizon tasks in Minecraft. Extensive experimental results
show that Optimus-1 significantly outperforms all existing agents on
challenging long-horizon task benchmarks, and exhibits near human-level
performance on many tasks. In addition, we introduce various Multimodal Large
Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show
that Optimus-1 exhibits strong generalization with the help of the Hybrid
Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.","[{'name': 'Zaijing Li'}, {'name': 'Yuquan Xie'}, {'name': 'Rui Shao'}, {'name': 'Gongwei Chen'}, {'name': 'Dongmei Jiang'}, {'name': 'Liqiang Nie'}]",2024-08-07T08:16:32Z
http://arxiv.org/abs/2408.03603v1,http://arxiv.org/abs/2408.03603v1,EnJa: Ensemble Jailbreak on Large Language Models,"As Large Language Models (LLMs) are increasingly being deployed in
safety-critical applications, their vulnerability to potential jailbreaks --
malicious prompts that can disable the safety mechanism of LLMs -- has
attracted growing research attention. While alignment methods have been
proposed to protect LLMs from jailbreaks, many have found that aligned LLMs can
still be jailbroken by carefully crafted malicious prompts, producing content
that violates policy regulations. Existing jailbreak attacks on LLMs can be
categorized into prompt-level methods which make up stories/logic to circumvent
safety alignment and token-level attack methods which leverage gradient methods
to find adversarial tokens. In this work, we introduce the concept of Ensemble
Jailbreak and explore methods that can integrate prompt-level and token-level
jailbreak into a more powerful hybrid jailbreak attack. Specifically, we
propose a novel EnJa attack to hide harmful instructions using prompt-level
jailbreak, boost the attack success rate using a gradient-based attack, and
connect the two types of jailbreak attacks via a template-based connector. We
evaluate the effectiveness of EnJa on several aligned models and show that it
achieves a state-of-the-art attack success rate with fewer queries and is much
stronger than any individual jailbreak.","[{'name': 'Jiahao Zhang'}, {'name': 'Zilong Wang'}, {'name': 'Ruofan Wang'}, {'name': 'Xingjun Ma'}, {'name': 'Yu-Gang Jiang'}]",2024-08-07T07:46:08Z
http://arxiv.org/abs/2408.03574v1,http://arxiv.org/abs/2408.03574v1,Teach CLIP to Develop a Number Sense for Ordinal Regression,"Ordinal regression is a fundamental problem within the field of computer
vision, with customised well-trained models on specific tasks. While
pre-trained vision-language models (VLMs) have exhibited impressive performance
on various vision tasks, their potential for ordinal regression has received
less exploration. In this study, we first investigate CLIP's potential for
ordinal regression, from which we expect the model could generalise to
different ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP
fails on this task, since current VLMs have a well-documented limitation of
encapsulating compositional concepts such as number sense. We propose a simple
yet effective method called NumCLIP to improve the quantitative understanding
of VLMs. We disassemble the exact image to number-specific text matching
problem into coarse classification and fine prediction stages. We discretize
and phrase each numerical bin with common language concept to better leverage
the available pre-trained alignment in CLIP. To consider the inherent
continuous property of ordinal regression, we propose a novel fine-grained
cross-modal ranking-based regularisation loss specifically designed to keep
both semantic and ordinal alignment in CLIP's feature space. Experimental
results on three general ordinal regression tasks demonstrate the effectiveness
of NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating
and image aesthetics assessment task, respectively. Code is publicly available
at https://github.com/xmed-lab/NumCLIP.","[{'name': 'Yao Du'}, {'name': 'Qiang Zhai'}, {'name': 'Weihang Dai'}, {'name': 'Xiaomeng Li'}]",2024-08-07T06:26:04Z
http://arxiv.org/abs/2408.03573v1,http://arxiv.org/abs/2408.03573v1,Active Testing of Large Language Model via Multi-Stage Sampling,"Performance evaluation plays a crucial role in the development life cycle of
large language models (LLMs). It estimates the model's capability, elucidates
behavior characteristics, and facilitates the identification of potential
issues and limitations, thereby guiding further improvement. Given that LLMs'
diverse task-handling abilities stem from large volumes of training data, a
comprehensive evaluation also necessitates abundant, well-annotated, and
representative test data to assess LLM performance across various downstream
tasks. However, the demand for high-quality test data often entails substantial
time, computational resources, and manual efforts, sometimes causing the
evaluation to be inefficient or impractical. To address these challenges,
researchers propose active testing, which estimates the overall performance by
selecting a subset of test data. Nevertheless, the existing active testing
methods tend to be inefficient, even inapplicable, given the unique new
challenges of LLMs (e.g., diverse task types, increased model complexity, and
unavailability of training data). To mitigate such limitations and expedite the
development cycle of LLMs, in this work, we introduce AcTracer, an active
testing framework tailored for LLMs that strategically selects a small subset
of test data to achieve a nearly optimal performance estimation for LLMs.
AcTracer utilizes both internal and external information from LLMs to guide the
test sampling process, reducing variance through a multi-stage pool-based
active selection. Our experiment results demonstrate that AcTracer achieves
state-of-the-art performance compared to existing methods across various tasks,
with up to 38.83% improvement over previous SOTA.","[{'name': 'Yuheng Huang'}, {'name': 'Jiayang Song'}, {'name': 'Qiang Hu'}, {'name': 'Felix Juefei-Xu'}, {'name': 'Lei Ma'}]",2024-08-07T06:17:48Z
http://arxiv.org/abs/2408.03567v1,http://arxiv.org/abs/2408.03567v1,"Unlocking Exocentric Video-Language Data for Egocentric Video
  Representation Learning","We present EMBED (Egocentric Models Built with Exocentric Data), a method
designed to transform exocentric video-language data for egocentric video
representation learning. Large-scale exocentric data covers diverse activities
with significant potential for egocentric learning, but inherent disparities
between egocentric and exocentric data pose challenges in utilizing one view
for the other seamlessly. Egocentric videos predominantly feature close-up
hand-object interactions, whereas exocentric videos offer a broader perspective
on human activities. Additionally, narratives in egocentric datasets are
typically more action-centric and closely linked with the visual content, in
contrast to the narrative styles found in exocentric datasets. To address these
challenges, we employ a data transformation framework to adapt exocentric data
for egocentric training, focusing on identifying specific video clips that
emphasize hand-object interactions and transforming narration styles to align
with egocentric perspectives. By applying both vision and language style
transfer, our framework creates a new egocentric dataset derived from
exocentric video-language data. Through extensive evaluations, we demonstrate
the effectiveness of EMBED, achieving state-of-the-art results across various
egocentric downstream tasks, including an absolute improvement of 4.7% on the
Epic-Kitchens-100 multi-instance retrieval and 6.2% on the EGTEA classification
benchmarks in zero-shot settings. Furthermore, EMBED enables egocentric
video-language models to perform competitively in exocentric tasks. Finally, we
showcase EMBED's application across various exocentric datasets, exhibiting
strong generalization capabilities when applied to different exocentric
datasets.","[{'name': 'Zi-Yi Dou'}, {'name': 'Xitong Yang'}, {'name': 'Tushar Nagarajan'}, {'name': 'Huiyu Wang'}, {'name': 'Jing Huang'}, {'name': 'Nanyun Peng'}, {'name': 'Kris Kitani'}, {'name': 'Fu-Jen Chu'}]",2024-08-07T06:10:45Z
http://arxiv.org/abs/2408.03562v1,http://arxiv.org/abs/2408.03562v1,"A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel
  Chatbot Use Case","This research compares large language model (LLM) fine-tuning methods,
including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning
(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally
compared LLM evaluation methods including End to End (E2E) benchmark method of
""Golden Answers"", traditional natural language processing (NLP) metrics, RAG
Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,
using the travel chatbot use case. The travel dataset was sourced from the the
Reddit API by requesting posts from travel-related subreddits to get
travel-related conversation prompts and personalized travel experiences, and
augmented for each fine-tuning method. We used two pretrained LLMs utilized for
fine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to
the two pretrained models. The inferences from these models are extensively
evaluated against the aforementioned metrics. The best model according to human
evaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a
Reinforcement Learning from Human Feedback (RLHF) training pipeline, and
ultimately was evaluated as the best model. Our main findings are that: 1)
quantitative and Ragas metrics do not align with human evaluation, 2) Open AI
GPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep
humans in the loop for evaluation because, 4) traditional NLP metrics
insufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms
QLoRA, but still needs postprocessing, 7) RLHF improves model performance
significantly. Next steps include improving data quality, increasing data
quantity, exploring RAG methods, and focusing data collection on a specific
city, which would improve data quality by narrowing the focus, while creating a
useful product.","[{'name': 'Sonia Meyer'}, {'name': 'Shreya Singh'}, {'name': 'Bertha Tam'}, {'name': 'Christopher Ton'}, {'name': 'Angel Ren'}]",2024-08-07T05:52:00Z
http://arxiv.org/abs/2408.03554v1,http://arxiv.org/abs/2408.03554v1,"Empirical Analysis of Large Vision-Language Models against Goal
  Hijacking via Visual Prompt Injection","We explore visual prompt injection (VPI) that maliciously exploits the
ability of large vision-language models (LVLMs) to follow instructions drawn
onto the input image. We propose a new VPI method, ""goal hijacking via visual
prompt injection"" (GHVPI), that swaps the execution task of LVLMs from an
original task to an alternative task designated by an attacker. The
quantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and
demonstrates a notable attack success rate of 15.8%, which is an unignorable
security risk. Our analysis also shows that successful GHVPI requires high
character recognition capability and instruction-following ability in LVLMs.","[{'name': 'Subaru Kimura'}, {'name': 'Ryota Tanaka'}, {'name': 'Shumpei Miyawaki'}, {'name': 'Jun Suzuki'}, {'name': 'Keisuke Sakaguchi'}]",2024-08-07T05:30:10Z
http://arxiv.org/abs/2408.03544v2,http://arxiv.org/abs/2408.03544v2,"Unlocking the Non-Native Language Context Limitation: Native Language
  Prompting Facilitates Knowledge Elicitation","Multilingual large language models (MLLMs) struggle to answer questions posed
in non-dominant languages, even though they have acquired the relevant
knowledge from their dominant language corpus. In contrast, human multilinguals
can overcome such non-native language context limitations through Positive
Native Language Transfer (PNLT). Inspired by the process of PNLT, we analogize
the dominant language of MLLMs to the native language of human multilinguals,
and propose Native Language Prompting (NatLan) to simulate the PNLT observed in
human multilinguals. It explicitly creates native language contexts for MLLMs
to facilitate the elicitation of the rich native language knowledge during
question-answering, unlocking the limitations imposed by non-native language
contexts. By employing multi-MLLM collaboration, NatLan reduces the workload on
each MLLM in simulating PNLT and refines semantic transfer. On the C-Eval
benchmark, NatLan provides up to a 10.1% average accuracy improvement and up to
a 5.0% increase in the hard-level subset across five MLLMs, surpassing all
top-notch related methods. Our code is available at
https://github.com/AnonyNLP/NatLan.","[{'name': 'Baixuan Li'}, {'name': 'Yunlong Fan'}, {'name': 'Zhiqiang Gao'}]",2024-08-07T04:49:38Z
http://arxiv.org/abs/2408.03541v3,http://arxiv.org/abs/2408.03541v3,EXAONE 3.0 7.8B Instruction Tuned Language Model,"We introduce EXAONE 3.0 instruction-tuned language model, the first open
model in the family of Large Language Models (LLMs) developed by LG AI
Research. Among different model sizes, we publicly release the 7.8B
instruction-tuned model to promote open research and innovations. Through
extensive evaluations across a wide range of public and in-house benchmarks,
EXAONE 3.0 demonstrates highly competitive real-world performance with
instruction-following capability against other state-of-the-art open models of
similar size. Our comparative analysis shows that EXAONE 3.0 excels
particularly in Korean, while achieving compelling performance across general
tasks and complex reasoning. With its strong real-world effectiveness and
bilingual proficiency, we hope that EXAONE keeps contributing to advancements
in Expert AI. Our EXAONE 3.0 instruction-tuned model is available at
https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct","[{'name': 'LG AI Research'}, {'name': ':'}, {'name': 'Soyoung An'}, {'name': 'Kyunghoon Bae'}, {'name': 'Eunbi Choi'}, {'name': 'Stanley Jungkyu Choi'}, {'name': 'Yemuk Choi'}, {'name': 'Seokhee Hong'}, {'name': 'Yeonjung Hong'}, {'name': 'Junwon Hwang'}, {'name': 'Hyojin Jeon'}, {'name': 'Gerrard Jeongwon Jo'}, {'name': 'Hyunjik Jo'}, {'name': 'Jiyeon Jung'}, {'name': 'Yountae Jung'}, {'name': 'Euisoon Kim'}, {'name': 'Hyosang Kim'}, {'name': 'Joonkee Kim'}, {'name': 'Seonghwan Kim'}, {'name': 'Soyeon Kim'}, {'name': 'Sunkyoung Kim'}, {'name': 'Yireun Kim'}, {'name': 'Youchul Kim'}, {'name': 'Edward Hwayoung Lee'}, {'name': 'Haeju Lee'}, {'name': 'Honglak Lee'}, {'name': 'Jinsik Lee'}, {'name': 'Kyungmin Lee'}, {'name': 'Moontae Lee'}, {'name': 'Seungjun Lee'}, {'name': 'Woohyung Lim'}, {'name': 'Sangha Park'}, {'name': 'Sooyoun Park'}, {'name': 'Yongmin Park'}, {'name': 'Boseong Seo'}, {'name': 'Sihoon Yang'}, {'name': 'Heuiyeen Yeen'}, {'name': 'Kyungjae Yoo'}, {'name': 'Hyeongu Yun'}]",2024-08-07T04:38:38Z
http://arxiv.org/abs/2408.03524v1,http://arxiv.org/abs/2408.03524v1,EgyBERT: A Large Language Model Pretrained on Egyptian Dialect Corpora,"This study presents EgyBERT, an Arabic language model pretrained on 10.4 GB
of Egyptian dialectal texts. We evaluated EgyBERT's performance by comparing it
with five other multidialect Arabic language models across 10 evaluation
datasets. EgyBERT achieved the highest average F1-score of 84.25% and an
accuracy of 87.33%, significantly outperforming all other comparative models,
with MARBERTv2 as the second best model achieving an F1-score 83.68% and an
accuracy 87.19%. Additionally, we introduce two novel Egyptian dialectal
corpora: the Egyptian Tweets Corpus (ETC), containing over 34.33 million tweets
(24.89 million sentences) amounting to 2.5 GB of text, and the Egyptian Forums
Corpus (EFC), comprising over 44.42 million sentences (7.9 GB of text)
collected from various Egyptian online forums. Both corpora are used in
pretraining the new model, and they are the largest Egyptian dialectal corpora
to date reported in the literature. Furthermore, this is the first study to
evaluate the performance of various language models on Egyptian dialect
datasets, revealing significant differences in performance that highlight the
need for more dialect-specific models. The results confirm the effectiveness of
EgyBERT model in processing and analyzing Arabic text expressed in Egyptian
dialect, surpassing other language models included in the study. EgyBERT model
is publicly available on \url{https://huggingface.co/faisalq/EgyBERT}.",[{'name': 'Faisal Qarah'}],2024-08-07T03:23:55Z
http://arxiv.org/abs/2408.03511v1,http://arxiv.org/abs/2408.03511v1,MoExtend: Tuning New Experts for Modality and Task Extension,"Large language models (LLMs) excel in various tasks but are primarily trained
on text data, limiting their application scope. Expanding LLM capabilities to
include vision-language understanding is vital, yet training them on multimodal
data from scratch is challenging and costly. Existing instruction tuning
methods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs
via fully fine-tuning LLMs to bridge the modality gap. However, full
fine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous
knowledge, and high training costs particularly in the era of increasing tasks
and modalities. To solve this issue, we introduce MoExtend, an effective
framework designed to streamline the modality adaptation and extension of
Mixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts
into pre-trained MoE models, endowing them with novel knowledge without the
need to tune pretrained models such as MoE and vision encoders. This approach
enables rapid adaptation and extension to new modal data or tasks, effectively
addressing the challenge of accommodating new modalities within LLMs.
Furthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk
of catastrophic forgetting. Experimental results demonstrate the efficacy and
efficiency of MoExtend in enhancing the multimodal capabilities of LLMs,
contributing to advancements in multimodal AI research. Code:
https://github.com/zhongshsh/MoExtend.","[{'name': 'Shanshan Zhong'}, {'name': 'Shanghua Gao'}, {'name': 'Zhongzhan Huang'}, {'name': 'Wushao Wen'}, {'name': 'Marinka Zitnik'}, {'name': 'Pan Zhou'}]",2024-08-07T02:28:37Z
http://arxiv.org/abs/2408.03506v1,http://arxiv.org/abs/2408.03506v1,"1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your
  Language Model Thrives on Quality Data","This paper presents a compute-efficient approach to pre-training a Language
Model-the ""1.5-Pints""-in only 9 days, while outperforming state-of-the-art
models as an instruction-following assistant.Based on MT-Bench (a benchmark
that emulates human judgments), 1.5-Pints outperforms Apple's OpenELM and
Microsoft's Phi.This is achieved by a carefully curated pre-training dataset of
57 billion tokens, using a mix of automated workflows and manual human review.
The selection of the dataset prioritizes content that is considered expository
and ""textbook-like"" to aid the model in reasoning and logical deduction,
culminating in its overall ability as a strong and versatile AI model. In terms
of the model architecture, we employed a modified Mistral tokenizer, alongside
a Llama-2 architecture for wider compatibility. For training, we adopted the
methodologies used by StableLM, TinyLlama, and Huggingface Zephyr. 1.5-Pints
demonstrates that by focusing on data quality over quantity in LLM training, we
can significantly reduce training time and resources required. We believe this
approach will not only make pre-training more accessible but also reduce our
carbon footprint. Our findings and resources from this research are
open-sourced, aiming to facilitate further advancements in the field. The
1.5-Pints model is available in two versions: 2K and 16K context windows.","[{'name': 'Calvin Tan'}, {'name': 'Jerome Wang'}]",2024-08-07T02:14:52Z
http://arxiv.org/abs/2408.03505v1,http://arxiv.org/abs/2408.03505v1,"Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble
  Exploitation","Multimodal large language models (MLLMs) have extended the success of large
language models (LLMs) to multiple data types, such as image, text and audio,
achieving significant performance in various domains, including multimodal
translation, visual question answering and content generation. Nonetheless,
existing systems are inefficient to train MLLMs due to substantial GPU bubbles
caused by the heterogeneous modality models and complex data dependencies in 3D
parallelism. This paper proposes Optimus, a distributed MLLM training system
that reduces end-to-end MLLM training time. Optimus is based on our principled
analysis that scheduling the encoder computation within the LLM bubbles can
reduce bubbles in MLLM training. To make scheduling encoder computation
possible for all GPUs, Optimus searches the separate parallel plans for encoder
and LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM
bubbles without breaking the original data dependencies in the MLLM model
architecture. We further decompose encoder layer computation into a series of
kernels, and analyze the common bubble pattern of 3D parallelism to carefully
optimize the sub-millisecond bubble scheduling, minimizing the overall training
time. Our experiments in a production cluster show that Optimus accelerates
MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs
compared to baselines.","[{'name': 'Weiqi Feng'}, {'name': 'Yangrui Chen'}, {'name': 'Shaoyu Wang'}, {'name': 'Yanghua Peng'}, {'name': 'Haibin Lin'}, {'name': 'Minlan Yu'}]",2024-08-07T02:08:29Z
http://arxiv.org/abs/2408.04668v1,http://arxiv.org/abs/2408.04668v1,Forecasting Live Chat Intent from Browsing History,"Customers reach out to online live chat agents with various intents, such as
asking about product details or requesting a return. In this paper, we propose
the problem of predicting user intent from browsing history and address it
through a two-stage approach. The first stage classifies a user's browsing
history into high-level intent categories. Here, we represent each browsing
history as a text sequence of page attributes and use the ground-truth class
labels to fine-tune pretrained Transformers. The second stage provides a large
language model (LLM) with the browsing history and predicted intent class to
generate fine-grained intents. For automatic evaluation, we use a separate LLM
to judge the similarity between generated and ground-truth intents, which
closely aligns with human judgments. Our two-stage approach yields significant
performance gains compared to generating intents without the classification
stage.","[{'name': 'Se-eun Yoon'}, {'name': 'Ahmad Bin Rabiah'}, {'name': 'Zaid Alibadi'}, {'name': 'Surya Kallumadi'}, {'name': 'Julian McAuley'}]",2024-08-07T01:50:59Z
http://arxiv.org/abs/2408.03492v1,http://arxiv.org/abs/2408.03492v1,Automated Theorem Provers Help Improve Large Language Model Reasoning,"In this paper we demonstrate how logic programming systems and Automated
first-order logic Theorem Provers (ATPs) can improve the accuracy of Large
Language Models (LLMs) for logical reasoning tasks where the baseline
performance is given by direct LLM solutions. We first evaluate LLM reasoning
on steamroller problems using the PRONTOQA benchmark. We show how accuracy can
be improved with a neuro-symbolic architecture where the LLM acts solely as a
front-end for translating a given problem into a formal logic language and an
automated reasoning engine is called for solving it. However, this approach
critically hinges on the correctness of the LLM translation. To assess this
translation correctness, we secondly define a framework of syntactic and
semantic error categories. We implemented the framework and used it to identify
errors that LLMs make in the benchmark domain. Based on these findings, we
thirdly extended our method with capabilities for automatically correcting
syntactic and semantic errors. For semantic error correction we integrate
first-order logic ATPs, which is our main and novel contribution. We
demonstrate that this approach reduces semantic errors significantly and
further increases the accurracy of LLM logical reasoning.","[{'name': 'Lachlan McGinness'}, {'name': 'Peter Baumgartner'}]",2024-08-07T01:03:56Z
http://arxiv.org/abs/2408.03414v1,http://arxiv.org/abs/2408.03414v1,"Logistic Regression makes small LLMs strong and explainable
  ""tens-of-shot"" classifiers","For simple classification tasks, we show that users can benefit from the
advantages of using small, local, generative language models instead of large
commercial models without a trade-off in performance or introducing extra
labelling costs. These advantages, including those around privacy,
availability, cost, and explainability, are important both in commercial
applications and in the broader democratisation of AI. Through experiments on
17 sentence classification tasks (2-4 classes), we show that penalised logistic
regression on the embeddings from a small LLM equals (and usually betters) the
performance of a large LLM in the ""tens-of-shot"" regime. This requires no more
labelled instances than are needed to validate the performance of the large
LLM. Finally, we extract stable and sensible explanations for classification
decisions.","[{'name': 'Marcus Buckmann'}, {'name': 'Edward Hill'}]",2024-08-06T19:23:42Z
http://arxiv.org/abs/2408.03402v1,http://arxiv.org/abs/2408.03402v1,"ULLME: A Unified Framework for Large Language Model Embeddings with
  Generation-Augmented Learning","Large Language Models (LLMs) excel in various natural language processing
tasks, but leveraging them for dense passage embedding remains challenging.
This is due to their causal attention mechanism and the misalignment between
their pre-training objectives and the text ranking tasks. Despite some recent
efforts to address these issues, existing frameworks for LLM-based text
embeddings have been limited by their support for only a limited range of LLM
architectures and fine-tuning strategies, limiting their practical application
and versatility. In this work, we introduce the Unified framework for Large
Language Model Embedding (ULLME), a flexible, plug-and-play implementation that
enables bidirectional attention across various LLMs and supports a range of
fine-tuning strategies. We also propose Generation-augmented Representation
Learning (GRL), a novel fine-tuning method to boost LLMs for text embedding
tasks. GRL enforces consistency between representation-based and
generation-based relevance scores, leveraging LLMs' powerful generative
abilities for learning passage embeddings. To showcase our framework's
flexibility and effectiveness, we release three pre-trained models from ULLME
with different backbone architectures, ranging from 1.5B to 8B parameters, all
of which demonstrate strong performance on the Massive Text Embedding
Benchmark. Our framework is publicly available at:
https://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found
at https://rb.gy/ws1ile.","[{'name': 'Hieu Man'}, {'name': 'Nghia Trung Ngo'}, {'name': 'Franck Dernoncourt'}, {'name': 'Thien Huu Nguyen'}]",2024-08-06T18:53:54Z
http://arxiv.org/abs/2408.03326v1,http://arxiv.org/abs/2408.03326v1,LLaVA-OneVision: Easy Visual Task Transfer,"We present LLaVA-OneVision, a family of open large multimodal models (LMMs)
developed by consolidating our insights into data, models, and visual
representations in the LLaVA-NeXT blog series. Our experimental results
demonstrate that LLaVA-OneVision is the first single model that can
simultaneously push the performance boundaries of open LMMs in three important
computer vision scenarios: single-image, multi-image, and video scenarios.
Importantly, the design of LLaVA-OneVision allows strong transfer learning
across different modalities/scenarios, yielding new emerging capabilities. In
particular, strong video understanding and cross-scenario capabilities are
demonstrated through task transfer from images to videos.","[{'name': 'Bo Li'}, {'name': 'Yuanhan Zhang'}, {'name': 'Dong Guo'}, {'name': 'Renrui Zhang'}, {'name': 'Feng Li'}, {'name': 'Hao Zhang'}, {'name': 'Kaichen Zhang'}, {'name': 'Yanwei Li'}, {'name': 'Ziwei Liu'}, {'name': 'Chunyuan Li'}]",2024-08-06T17:59:44Z
http://arxiv.org/abs/2408.03325v1,http://arxiv.org/abs/2408.03325v1,CoverBench: A Challenging Benchmark for Complex Claim Verification,"There is a growing line of research on verifying the correctness of language
models' outputs. At the same time, LMs are being used to tackle complex queries
that require reasoning. We introduce CoverBench, a challenging benchmark
focused on verifying LM outputs in complex reasoning settings. Datasets that
can be used for this purpose are often designed for other complex reasoning
tasks (e.g., QA) targeting specific use-cases (e.g., financial tables),
requiring transformations, negative sampling and selection of hard examples to
collect such a benchmark. CoverBench provides a diversified evaluation for
complex claim verification in a variety of domains, types of reasoning,
relatively long inputs, and a variety of standardizations, such as multiple
representations for tables where available, and a consistent schema. We
manually vet the data for quality to ensure low levels of label noise. Finally,
we report a variety of competitive baseline results to show CoverBench is
challenging and has very significant headroom. The data is available at
https://huggingface.co/datasets/google/coverbench .","[{'name': 'Alon Jacovi'}, {'name': 'Moran Ambar'}, {'name': 'Eyal Ben-David'}, {'name': 'Uri Shaham'}, {'name': 'Amir Feder'}, {'name': 'Mor Geva'}, {'name': 'Dror Marcus'}, {'name': 'Avi Caciularu'}]",2024-08-06T17:58:53Z
http://arxiv.org/abs/2408.03319v1,http://arxiv.org/abs/2408.03319v1,Training LLMs to Recognize Hedges in Spontaneous Narratives,"Hedges allow speakers to mark utterances as provisional, whether to signal
non-prototypicality or ""fuzziness"", to indicate a lack of commitment to an
utterance, to attribute responsibility for a statement to someone else, to
invite input from a partner, or to soften critical feedback in the service of
face-management needs. Here we focus on hedges in an experimentally
parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced
from memory by 21 speakers for co-present addressees, transcribed to text
(Galati and Brennan, 2010). We created a gold standard of hedges annotated by
human coders (the Roadrunner-Hedge corpus) and compared three LLM-based
approaches for hedge detection: fine-tuning BERT, and zero and few-shot
prompting with GPT-4o and LLaMA-3. The best-performing approach was a
fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on
the top performing approaches, we used an LLM-in-the-Loop approach to improve
the gold standard coding, as well as to highlight cases in which hedges are
ambiguous in linguistically interesting ways that will guide future research.
This is the first step in our research program to train LLMs to interpret and
generate collateral signals appropriately and meaningfully in conversation.","[{'name': 'Amie J. Paige'}, {'name': 'Adil Soubki'}, {'name': 'John Murzaku'}, {'name': 'Owen Rambow'}, {'name': 'Susan E. Brennan'}]",2024-08-06T17:51:42Z
http://arxiv.org/abs/2408.03314v1,http://arxiv.org/abs/2408.03314v1,"Scaling LLM Test-Time Compute Optimally can be More Effective than
  Scaling Model Parameters","Enabling LLMs to improve their outputs by using more test-time computation is
a critical step towards building generally self-improving agents that can
operate on open-ended natural language. In this paper, we study the scaling of
inference-time computation in LLMs, with a focus on answering the question: if
an LLM is allowed to use a fixed but non-trivial amount of inference-time
compute, how much can it improve its performance on a challenging prompt?
Answering this question has implications not only on the achievable performance
of LLMs, but also on the future of LLM pretraining and how one should tradeoff
inference-time and pre-training compute. Despite its importance, little
research attempted to understand the scaling behaviors of various test-time
inference methods. Moreover, current work largely provides negative results for
a number of these strategies. In this work, we analyze two primary mechanisms
to scale test-time computation: (1) searching against dense, process-based
verifier reward models; and (2) updating the model's distribution over a
response adaptively, given the prompt at test time. We find that in both cases,
the effectiveness of different approaches to scaling test-time compute
critically varies depending on the difficulty of the prompt. This observation
motivates applying a ""compute-optimal"" scaling strategy, which acts to most
effectively allocate test-time compute adaptively per prompt. Using this
compute-optimal strategy, we can improve the efficiency of test-time compute
scaling by more than 4x compared to a best-of-N baseline. Additionally, in a
FLOPs-matched evaluation, we find that on problems where a smaller base model
attains somewhat non-trivial success rates, test-time compute can be used to
outperform a 14x larger model.","[{'name': 'Charlie Snell'}, {'name': 'Jaehoon Lee'}, {'name': 'Kelvin Xu'}, {'name': 'Aviral Kumar'}]",2024-08-06T17:35:05Z
http://arxiv.org/abs/2408.03297v2,http://arxiv.org/abs/2408.03297v2,"KnowPO: Knowledge-aware Preference Optimization for Controllable
  Knowledge Selection in Retrieval-Augmented Language Models","By integrating external knowledge, Retrieval-Augmented Generation (RAG) has
become an effective strategy for mitigating the hallucination problems that
large language models (LLMs) encounter when dealing with knowledge-intensive
tasks. However, in the process of integrating external non-parametric
supporting evidence with internal parametric knowledge, inevitable knowledge
conflicts may arise, leading to confusion in the model's responses. To enhance
the knowledge selection of LLMs in various contexts, some research has focused
on refining their behavior patterns through instruction-tuning. Nonetheless,
due to the absence of explicit negative signals and comparative objectives,
models fine-tuned in this manner may still exhibit undesirable behaviors such
as contextual ignorance and contextual overinclusion. To this end, we propose a
Knowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at
achieving adaptive knowledge selection based on contextual relevance in real
retrieval scenarios. Concretely, we proposed a general paradigm for
constructing knowledge conflict datasets, which comprehensively cover various
error types and learn how to avoid these negative signals through preference
optimization methods. Simultaneously, we proposed a rewriting strategy and data
ratio optimization strategy to address preference imbalances. Experimental
results show that KnowPO outperforms previous methods for handling knowledge
conflicts by over 37\%, while also exhibiting robust generalization across
various out-of-distribution datasets.","[{'name': 'Ruizhe Zhang'}, {'name': 'Yongxin Xu'}, {'name': 'Yuzhen Xiao'}, {'name': 'Runchuan Zhu'}, {'name': 'Xinke Jiang'}, {'name': 'Xu Chu'}, {'name': 'Junfeng Zhao'}, {'name': 'Yasha Wang'}]",2024-08-06T16:55:54Z
http://arxiv.org/abs/2408.04667v1,http://arxiv.org/abs/2408.04667v1,LLM Stability: A detailed analysis with some surprises,"A concerning property of our nearly magical LLMs involves the variation of
results given the exact same input and deterministic hyper-parameters. While AI
has always had a certain level of noisiness from inputs outside of training
data, we have generally had deterministic results for any particular input;
that is no longer true. While most LLM practitioners are ""in the know"", we are
unaware of any work that attempts to quantify current LLM stability. We suspect
no one has taken the trouble because it is just too boring a paper to execute
and write. But we have done it and there are some surprises.
  What kinds of surprises?
  The evaluated LLMs are rarely deterministic at the raw output level; they are
much more deterministic at the parsed output/answer level but still rarely 100%
stable across 5 re-runs with same data input.
  LLM accuracy variation is not normally distributed.
  Stability varies based on task.","[{'name': 'Berk Atil'}, {'name': 'Alexa Chittams'}, {'name': 'Liseng Fu'}, {'name': 'Ferhan Ture'}, {'name': 'Lixinyu Xu'}, {'name': 'Breck Baldwin'}]",2024-08-06T16:43:35Z
http://arxiv.org/abs/2408.03290v1,http://arxiv.org/abs/2408.03290v1,SARA: Singular-Value Based Adaptive Low-Rank Adaption,"With the increasing number of parameters in large pre-trained models, LoRA as
a parameter-efficient fine-tuning(PEFT) method is widely used for not adding
inference overhead. The LoRA method assumes that weight changes during
fine-tuning can be approximated by low-rank matrices. However, the rank values
need to be manually verified to match different downstream tasks, and they
cannot accommodate the varying importance of different layers in the model. In
this work, we first analyze the relationship between the performance of
different layers and their ranks using SVD. Based on this, we design the
Singular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds
the rank during initialization by performing SVD on the pre-trained weights.
Additionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly
reduces the number of parameters by fine-tuning only multiple parallel sets of
singular values controlled by a router. Extensive experiments on various
complex tasks demonstrate the simplicity and parameter efficiency of our
methods. They can effectively and adaptively find the most suitable rank for
each layer of each model.","[{'name': 'Jihao Gu'}, {'name': 'Shuai Chen'}, {'name': 'Zelin Wang'}, {'name': 'Yibo Zhang'}, {'name': 'Ping Gong'}]",2024-08-06T16:39:42Z
http://arxiv.org/abs/2408.04666v1,http://arxiv.org/abs/2408.04666v1,LLMs are Not Just Next Token Predictors,"LLMs are statistical models of language learning through stochastic gradient
descent with a next token prediction objective. Prompting a popular view among
AI modelers: LLMs are just next token predictors. While LLMs are engineered
using next token prediction, and trained based on their success at this task,
our view is that a reduction to just next token predictor sells LLMs short.
Moreover, there are important explanations of LLM behavior and capabilities
that are lost when we engage in this kind of reduction. In order to draw this
out, we will make an analogy with a once prominent research program in biology
explaining evolution and development from the gene's eye view.","[{'name': 'Stephen M. Downes'}, {'name': 'Patrick Forber'}, {'name': 'Alex Grzankowski'}]",2024-08-06T16:36:28Z
http://arxiv.org/abs/2408.03281v2,http://arxiv.org/abs/2408.03281v2,"StructEval: Deepen and Broaden Large Language Model Assessment via
  Structured Evaluation","Evaluation is the baton for the development of large language models. Current
evaluations typically employ a single-item assessment paradigm for each atomic
test objective, which struggles to discern whether a model genuinely possesses
the required capabilities or merely memorizes/guesses the answers to specific
questions. To this end, we propose a novel evaluation framework referred to as
StructEval. Starting from an atomic test objective, StructEval deepens and
broadens the evaluation by conducting a structured assessment across multiple
cognitive levels and critical concepts, and therefore offers a comprehensive,
robust and consistent evaluation for LLMs. Experiments on three widely-used
benchmarks demonstrate that StructEval serves as a reliable tool for resisting
the risk of data contamination and reducing the interference of potential
biases, thereby providing more reliable and consistent conclusions regarding
model capabilities. Our framework also sheds light on the design of future
principled and trustworthy LLM evaluation protocols.","[{'name': 'Boxi Cao'}, {'name': 'Mengjie Ren'}, {'name': 'Hongyu Lin'}, {'name': 'Xianpei Han'}, {'name': 'Feng Zhang'}, {'name': 'Junfeng Zhan'}, {'name': 'Le Sun'}]",2024-08-06T16:28:30Z
http://arxiv.org/abs/2408.03359v1,http://arxiv.org/abs/2408.03359v1,"LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal
  Classification","We introduce LAMPO, a novel paradigm that leverages Large Language Models
(LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike
conventional methods, which concatenate all demonstration examples with the
test instance and prompt LLMs to produce the pointwise prediction, our
framework uses the LLM as a preference machine that makes a relative
comparative decision between the test instance and each demonstration. A
self-supervised method is then introduced to aggregate these binary comparisons
into the final ordinal decision. LAMPO addresses several limitations inherent
in previous methods, including context length constraints, ordering biases, and
challenges associated with absolute point-wise estimation. Extensive
experiments on seven public datasets demonstrate LAMPO's remarkably competitive
performance across a diverse spectrum of applications (e.g., movie review
analysis and hate speech detection). Notably, in certain applications, the
improvement can be substantial, exceeding 20% in an absolute term. Moreover, we
believe LAMPO represents an interesting addition to the non-parametric
application layered on top of LLMs, as it supports black-box LLMs without
necessitating the outputting of LLM's internal states (e.g., embeddings), as
seen in previous approaches.","[{'name': 'Zhen Qin'}, {'name': 'Junru Wu'}, {'name': 'Jiaming Shen'}, {'name': 'Tianqi Liu'}, {'name': 'Xuanhui Wang'}]",2024-08-06T15:55:05Z
http://arxiv.org/abs/2408.03256v1,http://arxiv.org/abs/2408.03256v1,Synthesizing Text-to-SQL Data from Weak and Strong LLMs,"The capability gap between open-source and closed-source large language
models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we
introduce a synthetic data approach that combines data produced by larger, more
powerful models (strong models) with error information data generated by
smaller, not well-aligned models (weak models). The method not only enhances
the domain generalization of text-to-SQL models but also explores the potential
of error data supervision through preference learning. Furthermore, we employ
the synthetic data approach for instruction tuning on open-source LLMs,
resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is
demonstrated through state-of-the-art results on the SPIDER and BIRD
benchmarks, bridging the performance gap between open-source models and methods
prompted by closed-source models.","[{'name': 'Jiaxi Yang'}, {'name': 'Binyuan Hui'}, {'name': 'Min Yang'}, {'name': 'Jian Yang'}, {'name': 'Junyang Lin'}, {'name': 'Chang Zhou'}]",2024-08-06T15:40:32Z
http://arxiv.org/abs/2408.03247v2,http://arxiv.org/abs/2408.03247v2,"Unveiling Factual Recall Behaviors of Large Language Models through
  Knowledge Neurons","In this paper, we investigate whether Large Language Models (LLMs) actively
recall or retrieve their internal repositories of factual knowledge when faced
with reasoning tasks. Through an analysis of LLMs' internal factual recall at
each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness
the critical factual associations under certain circumstances. Instead, they
tend to opt for alternative, shortcut-like pathways to answer reasoning
questions. By manually manipulating the recall process of parametric knowledge
in LLMs, we demonstrate that enhancing this recall process directly improves
reasoning performance whereas suppressing it leads to notable degradation.
Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a
powerful technique for addressing complex reasoning tasks. Our findings
indicate that CoT can intensify the recall of factual knowledge by encouraging
LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how
contextual conflicts affect the retrieval of facts during the reasoning process
to gain a comprehensive understanding of the factual recall behaviors of LLMs.
Code and data will be available soon.","[{'name': 'Yifei Wang'}, {'name': 'Yuheng Chen'}, {'name': 'Wanting Wen'}, {'name': 'Yu Sheng'}, {'name': 'Linjing Li'}, {'name': 'Daniel Dajun Zeng'}]",2024-08-06T15:07:08Z
http://arxiv.org/abs/2408.03246v1,http://arxiv.org/abs/2408.03246v1,Making Long-Context Language Models Better Multi-Hop Reasoners,"Recent advancements in long-context modeling have enhanced language models
(LMs) for complex tasks across multiple NLP applications. Despite this
progress, we find that these models struggle with multi-hop reasoning and
exhibit decreased performance in the presence of noisy contexts. In this paper,
we introduce Reasoning with Attributions, a novel approach that prompts LMs to
supply attributions for each assertion during their reasoning. We validate our
approach through experiments on three multi-hop datasets, employing both
proprietary and open-source models, and demonstrate its efficacy and
resilience. Furthermore, we explore methods to augment reasoning capabilities
via fine-tuning and offer an attribution-annotated dataset and a specialized
training strategy. Our fine-tuned model achieves competitive performance on
multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as
ChatGPT and Claude-instant.","[{'name': 'Yanyang Li'}, {'name': 'Shuo Liang'}, {'name': 'Michael R. Lyu'}, {'name': 'Liwei Wang'}]",2024-08-06T15:06:40Z
http://arxiv.org/abs/2408.04665v1,http://arxiv.org/abs/2408.04665v1,"LLM-based MOFs Synthesis Condition Extraction using Few-Shot
  Demonstrations","The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from
literature text has been challenging but crucial for the logical design of new
MOFs with desirable functionality. The recent advent of large language models
(LLMs) provides disruptively new solution to this long-standing problem and
latest researches have reported over 90% F1 in extracting correct conditions
from MOFs literature. We argue in this paper that most existing synthesis
extraction practices with LLMs stay with the primitive zero-shot learning,
which could lead to downgraded extraction and application performance due to
the lack of specialized knowledge. This work pioneers and optimizes the
few-shot in-context learning paradigm for LLM extraction of material synthesis
conditions. First, we propose a human-AI joint data curation process to secure
high-quality ground-truth demonstrations for few-shot learning. Second, we
apply a BM25 algorithm based on the retrieval-augmented generation (RAG)
technique to adaptively select few-shot demonstrations for each MOF's
extraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the
proposed few-shot method achieves much higher average F1 performance (0.93 vs.
0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under
fully automatic evaluation that are more objective than the previous human
evaluation. The proposed method is further validated through real-world
material experiments: compared with the baseline zero-shot LLM, the proposed
few-shot approach increases the MOFs structural inference performance (R^2) by
29.4% in average.","[{'name': 'Lei Shi'}, {'name': 'Zhimeng Liu'}, {'name': 'Yi Yang'}, {'name': 'Weize Wu'}, {'name': 'Yuyang Zhang'}, {'name': 'Hongbo Zhang'}, {'name': 'Jing Lin'}, {'name': 'Siyu Wu'}, {'name': 'Zihan Chen'}, {'name': 'Ruiming Li'}, {'name': 'Nan Wang'}, {'name': 'Zipeng Liu'}, {'name': 'Huobin Tan'}, {'name': 'Hongyi Gao'}, {'name': 'Yue Zhang'}, {'name': 'Ge Wang'}]",2024-08-06T14:53:25Z
http://arxiv.org/abs/2408.03202v1,http://arxiv.org/abs/2408.03202v1,"A Debiased Nearest Neighbors Framework for Multi-Label Text
  Classification","Multi-Label Text Classification (MLTC) is a practical yet challenging task
that involves assigning multiple non-exclusive labels to each document.
Previous studies primarily focus on capturing label correlations to assist
label prediction by introducing special labeling schemes, designing specific
model structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor
($k$NN) framework has shown promise by retrieving labeled samples as references
to mine label co-occurrence information in the embedding space. However, two
critical biases, namely embedding alignment bias and confidence estimation
bias, are often overlooked, adversely affecting prediction performance. In this
paper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC,
specifically designed to mitigate these biases. To address embedding alignment
bias, we propose a debiased contrastive learning strategy, enhancing neighbor
consistency on label co-occurrence. For confidence estimation bias, we present
a debiased confidence estimation strategy, improving the adaptive combination
of predictions from $k$NN and inductive binary classifications. Extensive
experiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2,
Amazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method.
Besides, our method does not introduce any extra parameters.","[{'name': 'Zifeng Cheng'}, {'name': 'Zhiwei Jiang'}, {'name': 'Yafeng Yin'}, {'name': 'Zhaoling Chen'}, {'name': 'Cong Wang'}, {'name': 'Shiping Ge'}, {'name': 'Qiguo Huang'}, {'name': 'Qing Gu'}]",2024-08-06T14:00:23Z
http://arxiv.org/abs/2408.03172v1,http://arxiv.org/abs/2408.03172v1,"Leveraging Parameter Efficient Training Methods for Low Resource Text
  Classification: A Case Study in Marathi","With the surge in digital content in low-resource languages, there is an
escalating demand for advanced Natural Language Processing (NLP) techniques
tailored to these languages. BERT (Bidirectional Encoder Representations from
Transformers), serving as the foundational framework for numerous NLP
architectures and language models, is increasingly employed for the development
of low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method
for fine-tuning Large Language Models (LLMs) and reducing the training
parameters to some extent to decrease the computational costs needed for
training the model and achieve results comparable to a fully fine-tuned model.
In this work, we present a study of PEFT methods for the Indic low-resource
language Marathi. We conduct a comprehensive analysis of PEFT methods applied
to various monolingual and multilingual Marathi BERT models. These approaches
are evaluated on prominent text classification datasets like MahaSent,
MahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to
significantly expedite the training speed of the models, addressing a critical
aspect of model development and deployment. In this study, we explore Low-Rank
Adaptation of Large Language Models (LoRA) and adapter methods for low-resource
text classification. We show that these methods are competitive with full
fine-tuning and can be used without loss in accuracy. This study contributes
valuable insights into the effectiveness of Marathi BERT models, offering a
foundation for the continued advancement of NLP capabilities in Marathi and
similar Indic languages.","[{'name': 'Pranita Deshmukh'}, {'name': 'Nikita Kulkarni'}, {'name': 'Sanhita Kulkarni'}, {'name': 'Kareena Manghani'}, {'name': 'Raviraj Joshi'}]",2024-08-06T13:16:16Z
http://arxiv.org/abs/2408.03150v1,http://arxiv.org/abs/2408.03150v1,Conditioning LLMs with Emotion in Neural Machine Translation,"Large Language Models (LLMs) have shown remarkable performance in Natural
Language Processing tasks, including Machine Translation (MT). In this work, we
propose a novel MT pipeline that integrates emotion information extracted from
a Speech Emotion Recognition (SER) model into LLMs to enhance translation
quality. We first fine-tune five existing LLMs on the Libri-trans dataset and
select the most performant model. Subsequently, we augment LLM prompts with
different dimensional emotions and train the selected LLM under these different
configurations. Our experiments reveal that integrating emotion information,
especially arousal, into LLM prompts leads to notable improvements in
translation quality.","[{'name': 'Charles Brazier'}, {'name': 'Jean-Luc Rouas'}]",2024-08-06T12:49:33Z
http://arxiv.org/abs/2408.03149v1,http://arxiv.org/abs/2408.03149v1,"Leveraging Entity Information for Cross-Modality Correlation Learning:
  The Entity-Guided Multimodal Summarization","The rapid increase in multimedia data has spurred advancements in Multimodal
Summarization with Multimodal Output (MSMO), which aims to produce a multimodal
summary that integrates both text and relevant images. The inherent
heterogeneity of content within multimodal inputs and outputs presents a
significant challenge to the execution of MSMO. Traditional approaches
typically adopt a holistic perspective on coarse image-text data or individual
visual objects, overlooking the essential connections between objects and the
entities they represent. To integrate the fine-grained entity knowledge, we
propose an Entity-Guided Multimodal Summarization model (EGMS). Our model,
building on BART, utilizes dual multimodal encoders with shared weights to
process text-image and entity-image information concurrently. A gating
mechanism then combines visual data for enhanced textual summary generation,
while image selection is refined through knowledge distillation from a
pre-trained vision-language model. Extensive experiments on public MSMO dataset
validate the superiority of the EGMS method, which also prove the necessity to
incorporate entity information into MSMO problem.","[{'name': 'Yanghai Zhang'}, {'name': 'Ye Liu'}, {'name': 'Shiwei Wu'}, {'name': 'Kai Zhang'}, {'name': 'Xukai Liu'}, {'name': 'Qi Liu'}, {'name': 'Enhong Chen'}]",2024-08-06T12:45:56Z
http://arxiv.org/abs/2408.03130v1,http://arxiv.org/abs/2408.03130v1,"Inference Optimizations for Large Language Models: Effects, Challenges,
  and Practical Considerations","Large language models are ubiquitous in natural language processing because
they can adapt to new tasks without retraining. However, their sheer scale and
complexity present unique challenges and opportunities, prompting researchers
and practitioners to explore novel model training, optimization, and deployment
methods. This literature review focuses on various techniques for reducing
resource requirements and compressing large language models, including
quantization, pruning, knowledge distillation, and architectural optimizations.
The primary objective is to explore each method in-depth and highlight its
unique challenges and practical applications. The discussed methods are
categorized into a taxonomy that presents an overview of the optimization
landscape and helps navigate it to understand the research trajectory better.","[{'name': 'Leo Donisch'}, {'name': 'Sigurd Schacht'}, {'name': 'Carsten Lanquillon'}]",2024-08-06T12:07:32Z
http://arxiv.org/abs/2408.03127v1,http://arxiv.org/abs/2408.03127v1,"Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral
  7B Model and Data Augmentation","This paper describes our approach to the SemEval-2024 safe biomedical Natural
Language Inference for Clinical Trials (NLI4CT) task, which concerns
classifying statements about Clinical Trial Reports (CTRs). We explored the
capabilities of Mistral-7B, a generalist open-source Large Language Model
(LLM). We developed a prompt for the NLI4CT task, and fine-tuned a quantized
version of the model using an augmented version of the training dataset. The
experimental results show that this approach can produce notable results in
terms of the macro F1-score, while having limitations in terms of faithfulness
and consistency. All the developed code is publicly available on a GitHub
repository","[{'name': 'Artur Guimarães'}, {'name': 'Bruno Martins'}, {'name': 'João Magalhães'}]",2024-08-06T11:59:09Z
http://arxiv.org/abs/2408.03125v1,http://arxiv.org/abs/2408.03125v1,COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework,"As the NLP community increasingly addresses challenges associated with
multilingualism, robust annotation tools are essential to handle multilingual
datasets efficiently. In this paper, we introduce a code-mixed multilingual
text annotation framework, COMMENTATOR, specifically designed for annotating
code-mixed text. The tool demonstrates its effectiveness in token-level and
sentence-level language annotation tasks for Hinglish text. We perform robust
qualitative human-based evaluations to showcase COMMENTATOR led to 5x faster
annotations than the best baseline. Our code is publicly available at
\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is
available at \url{https://bit.ly/commentator_video}.","[{'name': 'Rajvee Sheth'}, {'name': 'Shubh Nisar'}, {'name': 'Heenaben Prajapati'}, {'name': 'Himanshu Beniwal'}, {'name': 'Mayank Singh'}]",2024-08-06T11:56:26Z
http://arxiv.org/abs/2408.03119v1,http://arxiv.org/abs/2408.03119v1,"Evaluating the Translation Performance of Large Language Models Based on
  Euas-20","In recent years, with the rapid development of deep learning technology,
large language models (LLMs) such as BERT and GPT have achieved breakthrough
results in natural language processing tasks. Machine translation (MT), as one
of the core tasks of natural language processing, has also benefited from the
development of large language models and achieved a qualitative leap. Despite
the significant progress in translation performance achieved by large language
models, machine translation still faces many challenges. Therefore, in this
paper, we construct the dataset Euas-20 to evaluate the performance of large
language models on translation tasks, the translation ability on different
languages, and the effect of pre-training data on the translation ability of
LLMs for researchers and developers.","[{'name': 'Yan Huang'}, {'name': 'Wei Liu'}]",2024-08-06T11:49:11Z
http://arxiv.org/abs/2408.03099v1,http://arxiv.org/abs/2408.03099v1,Topic Modeling with Fine-tuning LLMs and Bag of Sentences,"Large language models (LLM)'s are increasingly used for topic modeling
outperforming classical topic models such as LDA. Commonly, pre-trained LLM
encoders such as BERT are used out-of-the-box despite the fact that fine-tuning
is known to improve LLMs considerably. The challenge lies in obtaining a
suitable (labeled) dataset for fine-tuning. In this paper, we use the recent
idea to use bag of sentences as the elementary unit in computing topics. In
turn, we derive an approach FT-Topic to perform unsupervised fine-tuning
relying primarily on two steps for constructing a training dataset in an
automatic fashion. First, a heuristic method to identifies pairs of sentence
groups that are either assumed to be of the same or different topics. Second,
we remove sentence pairs that are likely labeled incorrectly. The dataset is
then used to fine-tune an encoder LLM, which can be leveraged by any topic
modeling approach using embeddings. However, in this work, we demonstrate its
effectiveness by deriving a novel state-of-the-art topic modeling method called
SenClu, which achieves fast inference through an expectation-maximization
algorithm and hard assignments of sentence groups to a single topic, while
giving users the possibility to encode prior knowledge on the topic-document
distribution. Code is at \url{https://github.com/JohnTailor/FT-Topic}",[{'name': 'Johannes Schneider'}],2024-08-06T11:04:07Z
http://arxiv.org/abs/2408.03094v1,http://arxiv.org/abs/2408.03094v1,500xCompressor: Generalized Prompt Compression for Large Language Models,"Prompt compression is crucial for enhancing inference speed, reducing costs,
and improving user experience. However, current methods face challenges such as
low compression ratios and potential data leakage during evaluation. To address
these issues, we propose 500xCompressor, a method that compresses extensive
natural language contexts into a minimum of one single special token. The
500xCompressor introduces approximately 0.3% additional parameters and achieves
compression ratios ranging from 6x to 480x. It is designed to compress any
text, answer various types of questions, and could be utilized by the original
large language model (LLM) without requiring fine-tuning. Initially,
500xCompressor was pretrained on the Arxiv Corpus, followed by fine-tuning on
the ArxivQA dataset, and subsequently evaluated on strictly unseen and
classical question answering (QA) datasets. The results demonstrate that the
LLM retained 62.26-72.89% of its capabilities compared to using non-compressed
prompts. This study also shows that not all the compressed tokens are equally
utilized and that K V values have significant advantages over embeddings in
preserving information at high compression ratios. The highly compressive
nature of natural language prompts, even for fine-grained complex information,
suggests promising potential for future applications and further research into
developing a new LLM language.","[{'name': 'Zongqian Li'}, {'name': 'Yixuan Su'}, {'name': 'Nigel Collier'}]",2024-08-06T10:51:47Z
http://arxiv.org/abs/2408.03092v1,http://arxiv.org/abs/2408.03092v1,"Extend Model Merging from Fine-Tuned to Pre-Trained Large Language
  Models via Weight Disentanglement","Merging Large Language Models (LLMs) aims to amalgamate multiple homologous
LLMs into one with all the capabilities. Ideally, any LLMs sharing the same
backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT)
with minor parameter changes or Pre-Trained (PT) with substantial parameter
shifts. However, existing methods often manually assign the model importance,
rendering them feasible only for LLMs with similar parameter alterations, such
as multiple FT LLMs. The diverse parameter changed ranges between FT and PT
LLMs pose challenges for current solutions in empirically determining the
optimal combination. In this paper, we make a pioneering effort to broaden the
applicability of merging techniques from FT to PT LLMs. We initially examine
the efficacy of current methods in merging FT and PT LLMs, discovering that
they struggle to deal with PT LLMs. Subsequently, we introduce an approach
based on WeIght DisENtanglement (WIDEN) to effectively extend the merging
scope, which first disentangles model weights into magnitude and direction
components, and then performs adaptive fusion by considering their respective
contributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with
instruction-following skills) with Sailor (a PT LLM with multilingual
abilities) across 7B and 14B model scales. Results reveal that: (1) existing
solutions usually fail when merging Sailor, either losing both abilities or
only retaining instruction-following skills; (2) WIDEN successfully injects the
multilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in
Southeast Asian languages, achieving enhancements in the fundamental
capabilities. In light of previous research, we also merge multiple 13B FT LLMs
and observe that WIDEN achieves a balanced amalgamation of instruction
following, mathematical reasoning, and code generation skills.","[{'name': 'Le Yu'}, {'name': 'Bowen Yu'}, {'name': 'Haiyang Yu'}, {'name': 'Fei Huang'}, {'name': 'Yongbin Li'}]",2024-08-06T10:46:46Z
http://arxiv.org/abs/2408.03079v1,http://arxiv.org/abs/2408.03079v1,"Enhancing Complex Causality Extraction via Improved Subtask Interaction
  and Knowledge Fusion","Event Causality Extraction (ECE) aims at extracting causal event pairs from
texts. Despite ChatGPT's recent success, fine-tuning small models remains the
best approach for the ECE task. However, existing fine-tuning based ECE methods
cannot address all three key challenges in ECE simultaneously: 1) Complex
Causality Extraction, where multiple causal-effect pairs occur within a single
sentence; 2) Subtask~ Interaction, which involves modeling the mutual
dependence between the two subtasks of ECE, i.e., extracting events and
identifying the causal relationship between extracted events; and 3) Knowledge
Fusion, which requires effectively fusing the knowledge in two modalities,
i.e., the expressive pretrained language models and the structured knowledge
graphs. In this paper, we propose a unified ECE framework (UniCE to address all
three issues in ECE simultaneously. Specifically, we design a subtask
interaction mechanism to enable mutual interaction between the two ECE
subtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in
the two modalities. Furthermore, we employ separate decoders for each subtask
to facilitate complex causality extraction. Experiments on three benchmark
datasets demonstrate that our method achieves state-of-the-art performance and
outperforms ChatGPT with a margin of at least 30% F1-score. More importantly,
our model can also be used to effectively improve the ECE performance of
ChatGPT via in-context learning.","[{'name': 'Jinglong Gao'}, {'name': 'Chen Lu'}, {'name': 'Xiao Ding'}, {'name': 'Zhongyang Li'}, {'name': 'Ting Liu'}, {'name': 'Bing Qin'}]",2024-08-06T10:15:15Z
http://arxiv.org/abs/2408.03074v1,http://arxiv.org/abs/2408.03074v1,"Towards an Analysis of Discourse and Interactional Pragmatic Reasoning
  Capabilities of Large Language Models","In this work, we want to give an overview on which pragmatic abilities have
been tested in LLMs so far and how these tests have been carried out. To do
this, we first discuss the scope of the field of pragmatics and suggest a
subdivision into discourse pragmatics and interactional pragmatics. We give a
non-exhaustive overview of the phenomena of those two subdomains and the
methods traditionally used to analyze them. We subsequently consider the
resulting heterogeneous set of phenomena and methods as a starting point for
our survey of work on discourse pragmatics and interactional pragmatics in the
context of LLMs.","[{'name': 'Amelie Robrecht'}, {'name': 'Judith Sieker'}, {'name': 'Clara Lachenmaier'}, {'name': 'Sina Zarieß'}, {'name': 'Stefan Kopp'}]",2024-08-06T10:02:05Z
http://arxiv.org/abs/2408.03070v1,http://arxiv.org/abs/2408.03070v1,Probing structural constraints of negation in Pretrained Language Models,"Contradictory results about the encoding of the semantic impact of negation
in pretrained language models (PLMs). have been drawn recently (e.g. Kassner
and Sch{\""u}tze (2020); Gubelmann and Handschuh (2022)). In this paper we focus
rather on the way PLMs encode negation and its formal impact, through the
phenomenon of the Negative Polarity Item (NPI) licensing in English. More
precisely, we use probes to identify which contextual representations best
encode 1) the presence of negation in a sentence, and 2) the polarity of a
neighboring masked polarity item. We find that contextual representations of
tokens inside the negation scope do allow for (i) a better prediction of the
presence of not compared to those outside the scope and (ii) a better
prediction of the right polarity of a masked polarity item licensed by not,
although the magnitude of the difference varies from PLM to PLM. Importantly,
in both cases the trend holds even when controlling for distance to not. This
tends to indicate that the embeddings of these models do reflect the notion of
negation scope, and do encode the impact of negation on NPI licensing. Yet,
further control experiments reveal that the presence of other lexical items is
also better captured when using the contextual representation of a token within
the same syntactic clause than outside from it, suggesting that PLMs simply
capture the more general notion of syntactic clause.","[{'name': 'David Kletz'}, {'name': 'Marie Candito'}, {'name': 'Pascal Amsili'}]",2024-08-06T09:54:49Z
http://arxiv.org/abs/2408.03062v1,http://arxiv.org/abs/2408.03062v1,"Analysis of Argument Structure Constructions in a Deep Recurrent
  Language Model","Understanding how language and linguistic constructions are processed in the
brain is a fundamental question in cognitive computational neuroscience. In
this study, we explore the representation and processing of Argument Structure
Constructions (ASCs) in a recurrent neural language model. We trained a Long
Short-Term Memory (LSTM) network on a custom-made dataset consisting of 2000
sentences, generated using GPT-4, representing four distinct ASCs: transitive,
ditransitive, caused-motion, and resultative constructions.
  We analyzed the internal activations of the LSTM model's hidden layers using
Multidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding
(t-SNE) to visualize the sentence representations. The Generalized
Discrimination Value (GDV) was calculated to quantify the degree of clustering
within these representations. Our results show that sentence representations
form distinct clusters corresponding to the four ASCs across all hidden layers,
with the most pronounced clustering observed in the last hidden layer before
the output layer. This indicates that even a relatively simple,
brain-constrained recurrent neural network can effectively differentiate
between various construction types.
  These findings are consistent with previous studies demonstrating the
emergence of word class and syntax rule representations in recurrent language
models trained on next word prediction tasks. In future work, we aim to
validate these results using larger language models and compare them with
neuroimaging data obtained during continuous speech perception. This study
highlights the potential of recurrent neural language models to mirror
linguistic processing in the human brain, providing valuable insights into the
computational and neural mechanisms underlying language understanding.","[{'name': 'Pegah Ramezani'}, {'name': 'Achim Schilling'}, {'name': 'Patrick Krauss'}]",2024-08-06T09:27:41Z
http://arxiv.org/abs/2408.03354v2,http://arxiv.org/abs/2408.03354v2,"The Use of Large Language Models (LLM) for Cyber Threat Intelligence
  (CTI) in Cybercrime Forums","Large language models (LLMs) can be used to analyze cyber threat intelligence
(CTI) data from cybercrime forums, which contain extensive information and key
discussions about emerging cyber threats. However, to date, the level of
accuracy and efficiency of LLMs for such critical tasks has yet to be
thoroughly evaluated. Hence, this study assesses the accuracy of an LLM system
built on the OpenAI GPT-3.5-turbo model [7] to extract CTI information. To do
so, a random sample of 500 daily conversations from three cybercrime forums,
XSS, Exploit_in, and RAMP, was extracted, and the LLM system was instructed to
summarize the conversations and code 10 key CTI variables, such as whether a
large organization and/or a critical infrastructure is being targeted. Then,
two coders reviewed each conversation and evaluated whether the information
extracted by the LLM was accurate. The LLM system performed strikingly well,
with an average accuracy score of 98%. Various ways to enhance the model were
uncovered, such as the need to help the LLM distinguish between stories and
past events, as well as being careful with verb tenses in prompts.
Nevertheless, the results of this study highlight the efficiency and relevance
of using LLMs for cyber threat intelligence.","[{'name': 'Vanessa Clairoux-Trepanier'}, {'name': 'Isa-May Beauchamp'}, {'name': 'Estelle Ruellan'}, {'name': 'Masarah Paquet-Clouston'}, {'name': 'Serge-Olivier Paquette'}, {'name': 'Eric Clay'}]",2024-08-06T09:15:25Z
http://arxiv.org/abs/2408.03047v1,http://arxiv.org/abs/2408.03047v1,"OpenOmni: A Collaborative Open Source Tool for Building Future-Ready
  Multimodal Conversational Agents","Multimodal conversational agents are highly desirable because they offer
natural and human-like interaction. However, there is a lack of comprehensive
end-to-end solutions to support collaborative development and benchmarking.
While proprietary systems like GPT-4o and Gemini demonstrating impressive
integration of audio, video, and text with response times of 200-250ms,
challenges remain in balancing latency, accuracy, cost, and data privacy. To
better understand and quantify these issues, we developed OpenOmni, an
open-source, end-to-end pipeline benchmarking tool that integrates advanced
technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented
Generation, Large Language Models, along with the ability to integrate
customized models. OpenOmni supports local and cloud deployment, ensuring data
privacy and supporting latency and accuracy benchmarking. This flexible
framework allows researchers to customize the pipeline, focusing on real
bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can
significantly enhance applications like indoor assistance for visually impaired
individuals, advancing human-computer interaction. Our demonstration video is
available https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via
https://openomni.ai4wa.com, code is available via
https://github.com/AI4WA/OpenOmniFramework.","[{'name': 'Qiang Sun'}, {'name': 'Yuanyi Luo'}, {'name': 'Sirui Li'}, {'name': 'Wenxiao Zhang'}, {'name': 'Wei Liu'}]",2024-08-06T09:02:53Z
http://arxiv.org/abs/2408.03033v1,http://arxiv.org/abs/2408.03033v1,"L3iTC at the FinLLM Challenge Task: Quantization for Financial Text
  Classification & Summarization","This article details our participation (L3iTC) in the FinLLM Challenge Task
2024, focusing on two key areas: Task 1, financial text classification, and
Task 2, financial text summarization. To address these challenges, we
fine-tuned several large language models (LLMs) to optimize performance for
each task. Specifically, we used 4-bit quantization and LoRA to determine which
layers of the LLMs should be trained at a lower precision. This approach not
only accelerated the fine-tuning process on the training data provided by the
organizers but also enabled us to run the models on low GPU memory. Our
fine-tuned models achieved third place for the financial classification task
with an F1-score of 0.7543 and secured sixth place in the financial
summarization task on the official test datasets.","[{'name': 'Elvys Linhares Pontes'}, {'name': 'Carlos-Emiliano González-Gallardo'}, {'name': 'Mohamed Benjannet'}, {'name': 'Caryn Qu'}, {'name': 'Antoine Doucet'}]",2024-08-06T08:25:49Z
http://arxiv.org/abs/2408.04664v1,http://arxiv.org/abs/2408.04664v1,"Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via
  Language-Contrastive Decoding (LCD)","Large Vision-Language Models (LVLMs) are an extension of Large Language
Models (LLMs) that facilitate processing both image and text inputs, expanding
AI capabilities. However, LVLMs struggle with object hallucinations due to
their reliance on text cues and learned object co-occurrence biases. While most
research quantifies these hallucinations, mitigation strategies are still
lacking. Our study introduces a Language Contrastive Decoding (LCD) algorithm
that adjusts LVLM outputs based on LLM distribution confidence levels,
effectively reducing object hallucinations. We demonstrate the advantages of
LCD in leading LVLMs, showing up to %4 improvement in POPE F1 scores and up to
%36 reduction in CHAIR scores on the COCO validation set, while also improving
captioning quality scores. Our method effectively improves LVLMs without
needing complex post-processing or retraining, and is easily applicable to
different models. Our findings highlight the potential of further exploration
of LVLM-specific decoding algorithms.","[{'name': 'Avshalom Manevich'}, {'name': 'Reut Tsarfaty'}]",2024-08-06T08:10:34Z
http://arxiv.org/abs/2408.04663v1,http://arxiv.org/abs/2408.04663v1,"Dopamin: Transformer-based Comment Classifiers through Domain
  Post-Training and Multi-level Layer Aggregation","Code comments provide important information for understanding the source
code. They can help developers understand the overall purpose of a function or
class, as well as identify bugs and technical debt. However, an overabundance
of comments is meaningless and counterproductive. As a result, it is critical
to automatically filter out these comments for specific purposes. In this
paper, we present Dopamin, a Transformer-based tool for dealing with this
issue. Our model excels not only in presenting knowledge sharing of common
categories across multiple languages, but also in achieving robust performance
in comment classification by improving comment representation. As a result, it
outperforms the STACC baseline by 3% on the NLBSE'24 Tool Competition dataset
in terms of average F1-score, while maintaining a comparable inference time for
practical use. The source code is publicity available at
https://github.com/FSoft-AI4Code/Dopamin.","[{'name': 'Nam Le Hai'}, {'name': 'Nghi D. Q. Bui'}]",2024-08-06T08:08:43Z
http://arxiv.org/abs/2408.03010v1,http://arxiv.org/abs/2408.03010v1,"Fact Finder -- Enhancing Domain Expertise of Large Language Models by
  Incorporating Knowledge Graphs","Recent advancements in Large Language Models (LLMs) have showcased their
proficiency in answering natural language queries. However, their effectiveness
is hindered by limited domain-specific knowledge, raising concerns about the
reliability of their responses. We introduce a hybrid system that augments LLMs
with domain-specific knowledge graphs (KGs), thereby aiming to enhance factual
correctness using a KG-based retrieval approach. We focus on a medical KG to
demonstrate our methodology, which includes (1) pre-processing, (2) Cypher
query generation, (3) Cypher query processing, (4) KG retrieval, and (5)
LLM-enhanced response generation. We evaluate our system on a curated dataset
of 69 samples, achieving a precision of 78\% in retrieving correct KG nodes.
Our findings indicate that the hybrid system surpasses a standalone LLM in
accuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.
This positions the system as a promising tool for applications that demand
factual correctness and completeness, such as target identification -- a
critical process in pinpointing biological entities for disease treatment or
crop enhancement. Moreover, its intuitive search interface and ability to
provide accurate responses within seconds make it well-suited for
time-sensitive, precision-focused research contexts. We publish the source code
together with the dataset and the prompt templates used.","[{'name': 'Daniel Steinigen'}, {'name': 'Roman Teucher'}, {'name': 'Timm Heine Ruland'}, {'name': 'Max Rudat'}, {'name': 'Nicolas Flores-Herr'}, {'name': 'Peter Fischer'}, {'name': 'Nikola Milosevic'}, {'name': 'Christopher Schymura'}, {'name': 'Angelo Ziletti'}]",2024-08-06T07:45:05Z
http://arxiv.org/abs/2408.02976v1,http://arxiv.org/abs/2408.02976v1,"Empathy Level Alignment via Reinforcement Learning for Empathetic
  Response Generation","Empathetic response generation, aiming at understanding the user's situation
and feelings and respond empathically, is crucial in building human-like
dialogue systems. Previous methods mainly focus on using maximum likelihood
estimation as the optimization objective for training response generation
models, without taking into account the empathy level alignment between
generated responses and target responses. To this end, we propose an empathetic
response generation using reinforcement learning (EmpRL) framework. The
framework designs an effective empathy reward function and generates empathetic
responses by maximizing the expected reward through reinforcement learning.
Given the powerful text generation capability of pre-trained language models,
EmpRL utilizes the pre-trained T5 model as the generator and conducts further
training to initialize the policy. To align the empathy level between generated
responses and target responses in the context, an empathy reward function
containing three empathy communication mechanisms, i.e., emotional reaction,
interpretation, and exploration, is constructed using pre-designed and
pre-trained empathy identifiers. Finally, the proximal policy optimization
algorithm is used to further train the policy to produce empathetic responses.
Both automatic and manual evaluations demonstrate that the proposed EmpRL
framework can improve the quality of generated responses, enhance the empathy
level similarity between generated and target responses, and produce empathetic
responses covering both affective and cognitive aspects.","[{'name': 'Hui Ma'}, {'name': 'Bo Zhang'}, {'name': 'Bo Xu'}, {'name': 'Jian Wang'}, {'name': 'Hongfei Lin'}, {'name': 'Xiao Sun'}]",2024-08-06T06:16:00Z
http://arxiv.org/abs/2408.02970v1,http://arxiv.org/abs/2408.02970v1,"EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and
  Quantization","Large language models (LLMs) have attracted considerable attention in various
fields for their cost-effective solutions to diverse challenges, especially
with advancements in instruction tuning and quantization. E-commerce, with its
complex tasks and extensive product-user interactions, presents a promising
application area for LLMs. However, the domain-specific concepts and knowledge
inherent in e-commerce pose significant challenges for adapting general LLMs.
To address this issue, we developed EC-Guide
\href{https://github.com/fzp0424/EC-Guide-KDDUP-2024}, a comprehensive
e-commerce guide for instruction tuning and quantization of LLMs. We also
heuristically integrated Chain-of-Thought (CoT) during inference to enhance
arithmetic performance. Our approach achieved the 2nd place in Track 2 and 5th
place in Track 5 at the Amazon KDD Cup'24
\href{https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms}.
Additionally, our solution is model-agnostic, enabling effective scalability
across larger systems.","[{'name': 'Zhaopeng Feng'}, {'name': 'Zijie Meng'}, {'name': 'Zuozhu Liu'}]",2024-08-06T05:50:41Z
http://arxiv.org/abs/2408.02964v2,http://arxiv.org/abs/2408.02964v2,"Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The
  Impact of Prompt Engineering and Knowledge Retrieval","Large language models (LLMs) are fundamentally transforming human-facing
applications in the health and well-being domains: boosting patient engagement,
accelerating clinical decision-making, and facilitating medical education.
Although state-of-the-art LLMs have shown superior performance in several
conversational applications, evaluations within nutrition and diet applications
are still insufficient. In this paper, we propose to employ the Registered
Dietitian (RD) exam to conduct a standard and comprehensive evaluation of
state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing
both accuracy and consistency in nutrition queries. Our evaluation includes
1050 RD exam questions encompassing several nutrition topics and proficiency
levels. In addition, for the first time, we examine the impact of Zero-Shot
(ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC),
and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the
responses. Our findings revealed that while these LLMs obtained acceptable
overall performance, their results varied considerably with different prompts
and question domains. GPT-4o with CoT-SC prompting outperformed the other
approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.
For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both
accuracy and consistency. RAP was particularly effective for GPT-4o to answer
Expert level questions. Consequently, choosing the appropriate LLM and
prompting technique, tailored to the proficiency level and specific domain, can
mitigate errors and potential risks in diet and nutrition chatbots.","[{'name': 'Iman Azimi'}, {'name': 'Mohan Qi'}, {'name': 'Li Wang'}, {'name': 'Amir M. Rahmani'}, {'name': 'Youlin Li'}]",2024-08-06T05:21:13Z
http://arxiv.org/abs/2408.02948v1,http://arxiv.org/abs/2408.02948v1,"Are Female Carpenters like Blue Bananas? A Corpus Investigation of
  Occupation Gender Typicality","People tend to use language to mention surprising properties of events: for
example, when a banana is blue, we are more likely to mention color than when
it is yellow. This fact is taken to suggest that yellowness is somehow a
typical feature of bananas, and blueness is exceptional. Similar to how a
yellow color is typical of bananas, there may also be genders that are typical
of occupations. In this work, we explore this question using information
theoretic techniques coupled with corpus statistic analysis. In two distinct
large corpora, we do not find strong evidence that occupations and gender
display the same patterns of mentioning as do bananas and color. Instead, we
find that gender mentioning is correlated with femaleness of occupation in
particular, suggesting perhaps that woman-dominated occupations are seen as
somehow ``more gendered'' than male-dominated ones, and thereby they encourage
more gender mentioning overall.","[{'name': 'Da Ju'}, {'name': 'Karen Ulrich'}, {'name': 'Adina Williams'}]",2024-08-06T04:19:23Z
http://arxiv.org/abs/2408.02945v1,http://arxiv.org/abs/2408.02945v1,Self-Supervised Learning for Multi-Channel Neural Transducer,"Self-supervised learning, such as with the wav2vec 2.0 framework
significantly improves the accuracy of end-to-end automatic speech recognition
(ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In
this work, we explored a self-supervised learning method for a multi-channel
end-to-end ASR model based on the wav2vec 2.0 framework. As the multi-channel
end-to-end ASR model, we focused on a multi-channel neural transducer. In
pre-training, we compared three different methods for feature quantization to
train a multi-channel conformer audio encoder: joint quantization, feature-wise
quantization and channel-wise quantization. In fine-tuning, we trained the
multi-channel conformer-transducer. All experiments were conducted using the
far-field in-house and CHiME-4 datasets. The results of the experiments showed
that feature-wise quantization was the most effective among the methods. We
observed a 66% relative reduction in character error rate compared with the
model without any pre-training for the far-field in-house dataset.",[{'name': 'Atsushi Kojima'}],2024-08-06T04:12:31Z
http://arxiv.org/abs/2408.02919v1,http://arxiv.org/abs/2408.02919v1,Data Checklist: On Unit-Testing Datasets with Usable Information,"Model checklists (Ribeiro et al., 2020) have emerged as a useful tool for
understanding the behavior of LLMs, analogous to unit-testing in software
engineering. However, despite datasets being a key determinant of model
behavior, evaluating datasets, e.g., for the existence of annotation artifacts,
is largely done ad hoc, once a problem in model behavior has already been found
downstream. In this work, we take a more principled approach to unit-testing
datasets by proposing a taxonomy based on the V-information literature. We call
a collection of such unit tests a data checklist. Using a checklist, not only
are we able to recover known artifacts in well-known datasets such as SNLI, but
we also discover previously unknown artifacts in preference datasets for LLM
alignment. Data checklists further enable a new kind of data filtering, which
we use to improve the efficacy and data efficiency of preference alignment.","[{'name': 'Heidi C. Zhang'}, {'name': 'Shabnam Behzad'}, {'name': 'Kawin Ethayarajh'}, {'name': 'Dan Jurafsky'}]",2024-08-06T03:08:36Z
http://arxiv.org/abs/2408.02907v1,http://arxiv.org/abs/2408.02907v1,"Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large
  Language Model-Based Question Answering","Retrieving external knowledge and prompting large language models with
relevant information is an effective paradigm to enhance the performance of
question-answering tasks. Previous research typically handles paragraphs from
external documents in isolation, resulting in a lack of context and ambiguous
references, particularly in multi-document and complex tasks. To overcome these
challenges, we propose a new retrieval framework IIER, that leverages
Inter-chunk Interactions to Enhance Retrieval. This framework captures the
internal connections between document chunks by considering three types of
interactions: structural, keyword, and semantic. We then construct a unified
Chunk-Interaction Graph to represent all external documents comprehensively.
Additionally, we design a graph-based evidence chain retriever that utilizes
previous paths and chunk interactions to guide the retrieval process. It
identifies multiple seed nodes based on the target question and iteratively
searches for relevant chunks to gather supporting evidence. This retrieval
process refines the context and reasoning chain, aiding the large language
model in reasoning and answer generation. Extensive experiments demonstrate
that IIER outperforms strong baselines across four datasets, highlighting its
effectiveness in improving retrieval and reasoning capabilities.","[{'name': 'Tiezheng Guo'}, {'name': 'Chen Wang'}, {'name': 'Yanyi Liu'}, {'name': 'Jiawei Tang'}, {'name': 'Pan Li'}, {'name': 'Sai Xu'}, {'name': 'Qingwen Yang'}, {'name': 'Xianlin Gao'}, {'name': 'Zhi Li'}, {'name': 'Yingyou Wen'}]",2024-08-06T02:39:55Z
http://arxiv.org/abs/2408.02901v1,http://arxiv.org/abs/2408.02901v1,"Lighthouse: A User-Friendly Library for Reproducible Video Moment
  Retrieval and Highlight Detection","We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.","[{'name': 'Taichi Nishimura'}, {'name': 'Shota Nakada'}, {'name': 'Hokuto Munakata'}, {'name': 'Tatsuya Komatsu'}]",2024-08-06T02:15:12Z
http://arxiv.org/abs/2408.04662v1,http://arxiv.org/abs/2408.04662v1,Citekit: A Modular Toolkit for Large Language Model Citation Generation,"Enabling Large Language Models (LLMs) to generate citations in
Question-Answering (QA) tasks is an emerging paradigm aimed at enhancing the
verifiability of their responses when LLMs are utilizing external references to
generate an answer. However, there is currently no unified framework to
standardize and fairly compare different citation generation methods, leading
to difficulties in reproducing different methods and a comprehensive
assessment. To cope with the problems above, we introduce \name, an open-source
and modular toolkit designed to facilitate the implementation and evaluation of
existing citation generation methods, while also fostering the development of
new approaches to improve citation quality in LLM outputs. This tool is highly
extensible, allowing users to utilize 4 main modules and 14 components to
construct a pipeline, evaluating an existing method or innovative designs. Our
experiments with two state-of-the-art LLMs and 11 citation generation baselines
demonstrate varying strengths of different modules in answer accuracy and
citation quality improvement, as well as the challenge of enhancing
granularity. Based on our analysis of the effectiveness of components, we
propose a new method, self-RAG \snippet, obtaining a balanced answer accuracy
and citation quality. Citekit is released at
https://github.com/SjJ1017/Citekit.","[{'name': 'Jiajun Shen'}, {'name': 'Tong Zhou'}, {'name': 'Suifeng Zhao'}, {'name': 'Yubo Chen'}, {'name': 'Kang Liu'}]",2024-08-06T02:13:15Z
http://arxiv.org/abs/2408.02899v1,http://arxiv.org/abs/2408.02899v1,SETN: Stock Embedding Enhanced with Textual and Network Information,"Stock embedding is a method for vector representation of stocks. There is a
growing demand for vector representations of stock, i.e., stock embedding, in
wealth management sectors, and the method has been applied to various tasks
such as stock price prediction, portfolio optimization, and similar fund
identifications. Stock embeddings have the advantage of enabling the
quantification of relative relationships between stocks, and they can extract
useful information from unstructured data such as text and network data. In
this study, we propose stock embedding enhanced with textual and network
information (SETN) using a domain-adaptive pre-trained transformer-based model
to embed textual information and a graph neural network model to grasp network
information. We evaluate the performance of our proposed model on related
company information extraction tasks. We also demonstrate that stock embeddings
obtained from the proposed model perform better in creating thematic funds than
those obtained from baseline methods, providing a promising pathway for various
applications in the wealth management industry.","[{'name': 'Takehiro Takayanagi'}, {'name': 'Hiroki Sakaji'}, {'name': 'Kiyoshi Izumi'}]",2024-08-06T02:07:37Z
http://arxiv.org/abs/2408.02865v1,http://arxiv.org/abs/2408.02865v1,"VisionUnite: A Vision-Language Foundation Model for Ophthalmology
  Enhanced with Clinical Knowledge","The need for improved diagnostic methods in ophthalmology is acute,
especially in the less developed regions with limited access to specialists and
advanced equipment. Therefore, we introduce VisionUnite, a novel
vision-language foundation model for ophthalmology enhanced with clinical
knowledge. VisionUnite has been pretrained on an extensive dataset comprising
1.24 million image-text pairs, and further refined using our proposed MMFundus
dataset, which includes 296,379 high-quality fundus image-text pairs and
889,137 simulated doctor-patient dialogue instances. Our experiments indicate
that VisionUnite outperforms existing generative foundation models such as
GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable
to junior ophthalmologists. VisionUnite performs well in various clinical
scenarios including open-ended multi-disease diagnosis, clinical explanation,
and patient interaction, making it a highly versatile tool for initial
ophthalmic disease screening. VisionUnite can also serve as an educational aid
for junior ophthalmologists, accelerating their acquisition of knowledge
regarding both common and rare ophthalmic conditions. VisionUnite represents a
significant advancement in ophthalmology, with broad implications for
diagnostics, medical education, and understanding of disease mechanisms.","[{'name': 'Zihan Li'}, {'name': 'Diping Song'}, {'name': 'Zefeng Yang'}, {'name': 'Deming Wang'}, {'name': 'Fei Li'}, {'name': 'Xiulan Zhang'}, {'name': 'Paul E. Kinahan'}, {'name': 'Yu Qiao'}]",2024-08-05T23:31:07Z
http://arxiv.org/abs/2408.02861v1,http://arxiv.org/abs/2408.02861v1,A Framework for Fine-Tuning LLMs using Heterogeneous Feedback,"Large language models (LLMs) have been applied to a wide range of tasks,
including text summarization, web navigation, and chatbots. They have
benefitted from supervised fine-tuning (SFT) and reinforcement learning from
human feedback (RLHF) following an unsupervised pretraining. These datasets can
be difficult to collect, limited in scope, and vary in sample quality.
Additionally, datasets can vary extensively in supervision format, from
numerical to binary as well as multi-dimensional with many different values. We
present a framework for fine-tuning LLMs using heterogeneous feedback, which
has two main components. First, we combine the heterogeneous feedback data into
a single supervision format, compatible with methods like SFT and RLHF. Next,
given this unified feedback dataset, we extract a high-quality and diverse
subset to obtain performance increases potentially exceeding the full dataset.
We conduct extensive experiments to understand the effectiveness of these
techniques for incorporating heterogeneous feedback, and demonstrate
improvements from using a high-quality and diverse subset of the data. We find
that our framework is able to improve models in multiple areas simultaneously,
such as in instruction following and bias reduction.","[{'name': 'Ryan Aponte'}, {'name': 'Ryan A. Rossi'}, {'name': 'Shunan Guo'}, {'name': 'Franck Dernoncourt'}, {'name': 'Tong Yu'}, {'name': 'Xiang Chen'}, {'name': 'Subrata Mitra'}, {'name': 'Nedim Lipka'}]",2024-08-05T23:20:32Z
http://arxiv.org/abs/2408.04661v1,http://arxiv.org/abs/2408.04661v1,"MaterioMiner -- An ontology-based text mining dataset for extraction of
  process-structure-property entities","While large language models learn sound statistical representations of the
language and information therein, ontologies are symbolic knowledge
representations that can complement the former ideally. Research at this
critical intersection relies on datasets that intertwine ontologies and text
corpora to enable training and comprehensive benchmarking of neurosymbolic
models. We present the MaterioMiner dataset and the linked materials mechanics
ontology where ontological concepts from the mechanics of materials domain are
associated with textual entities within the literature corpus. Another
distinctive feature of the dataset is its eminently fine-granular annotation.
Specifically, 179 distinct classes are manually annotated by three raters
within four publications, amounting to a total of 2191 entities that were
annotated and curated. Conceptual work is presented for the symbolic
representation of causal composition-process-microstructure-property
relationships. We explore the annotation consistency between the three raters
and perform fine-tuning of pre-trained models to showcase the feasibility of
named-entity recognition model training. Reusing the dataset can foster
training and benchmarking of materials language models, automated ontology
construction, and knowledge graph generation from textual data.","[{'name': 'Ali Riza Durmaz'}, {'name': 'Akhil Thomas'}, {'name': 'Lokesh Mishra'}, {'name': 'Rachana Niranjan Murthy'}, {'name': 'Thomas Straub'}]",2024-08-05T21:42:59Z
http://arxiv.org/abs/2408.02838v1,http://arxiv.org/abs/2408.02838v1,"Interpretation of the Intent Detection Problem as Dynamics in a
  Low-dimensional Space","Intent detection is a text classification task whose aim is to recognize and
label the semantics behind a users query. It plays a critical role in various
business applications. The output of the intent detection module strongly
conditions the behavior of the whole system. This sequence analysis task is
mainly tackled using deep learning techniques. Despite the widespread use of
these techniques, the internal mechanisms used by networks to solve the problem
are poorly understood. Recent lines of work have analyzed the computational
mechanisms learned by RNNs from a dynamical systems perspective. In this work,
we investigate how different RNN architectures solve the SNIPS intent detection
problem. Sentences injected into trained networks can be interpreted as
trajectories traversing a hidden state space. This space is constrained to a
low-dimensional manifold whose dimensionality is related to the embedding and
hidden layer sizes. To generate predictions, RNN steers the trajectories
towards concrete regions, spatially aligned with the output layer matrix rows
directions. Underlying the system dynamics, an unexpected fixed point topology
has been identified with a limited number of attractors. Our results provide
new insights into the inner workings of networks that solve the intent
detection task.","[{'name': 'Eduardo Sanchez-Karhunen'}, {'name': 'Jose F. Quesada-Moreno'}, {'name': 'Miguel A. Gutiérrez-Naranjo'}]",2024-08-05T21:22:36Z
http://arxiv.org/abs/2408.05241v2,http://arxiv.org/abs/2408.05241v2,"Large Model Strategic Thinking, Small Model Efficiency: Transferring
  Theory of Mind in Large Language Models","As the performance of larger, newer Large Language Models continues to
improve for strategic Theory of Mind (ToM) tasks, the demand for these state of
the art models increases commensurately. However, their deployment is costly
both in terms of processing power and time. In this paper, we investigate the
feasibility of creating smaller, simulation-ready agents by way of fine-tuning.
To do this, we present a large pre-trained model with 20 unique scenarios that
combine a social context with a social dilemma, recording its answers, and
using them for Q\&A fine-tuning on a smaller model of the same family. Our
focus is on in-context game-theoretic decision-making, the same domain within
which human interaction occurs and that requires both a theory of mind (or a
semblance thereof) and an understanding of social dynamics. We find that the
fine-tuned smaller language model exhibited significant performance closer to
that of its larger relative, and that their improvements extended in areas and
contexts beyond the ones provided in the training examples. On average for all
games, through fine-tuning, the smaller model showed a \%46 improvement in
aligning with the behavior of the larger model, with \%100 representing
complete alignment. This suggests that our pipeline represents an efficient
method to transmit some form of theory of mind to smaller models, creating
improved and cheaply deployable algorithms in the process. Despite their
simplicity and their associated shortcomings and limitations, our findings
represent a stepping stone in the pursuit and training of specialized models
for strategic and social decision making.","[{'name': 'Nunzio Lore'}, {'name': 'Alireza Sepehr Ilami'}, {'name': 'Babak Heydari'}]",2024-08-05T20:49:48Z
http://arxiv.org/abs/2408.02784v1,http://arxiv.org/abs/2408.02784v1,LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory,"Humans are not homo economicus (i.e., rational economic beings). As humans,
we exhibit systematic behavioral biases such as loss aversion, anchoring,
framing, etc., which lead us to make suboptimal economic decisions. Insofar as
such biases may be embedded in text data on which large language models (LLMs)
are trained, to what extent are LLMs prone to the same behavioral biases?
Understanding these biases in LLMs is crucial for deploying LLMs to support
human decision-making. We propose utility theory-a paradigm at the core of
modern economic theory-as an approach to evaluate the economic biases of LLMs.
Utility theory enables the quantification and comparison of economic behavior
against benchmarks such as perfect rationality or human behavior. To
demonstrate our approach, we quantify and compare the economic behavior of a
variety of open- and closed-source LLMs. We find that the economic behavior of
current LLMs is neither entirely human-like nor entirely economicus-like. We
also find that most current LLMs struggle to maintain consistent economic
behavior across settings. Finally, we illustrate how our approach can measure
the effect of interventions such as prompting on economic biases.","[{'name': 'Jillian Ross'}, {'name': 'Yoon Kim'}, {'name': 'Andrew W. Lo'}]",2024-08-05T19:00:43Z
http://arxiv.org/abs/2408.02666v2,http://arxiv.org/abs/2408.02666v2,Self-Taught Evaluators,"Model-based evaluation is at the heart of successful model development -- as
a reward model for training, and as a replacement for human evaluation. To
train such evaluators, the standard approach is to collect a large amount of
human preference judgments over model responses, which is costly and the data
becomes stale as models improve. In this work, we present an approach that aims
to im-prove evaluators without human annotations, using synthetic training data
only. Starting from unlabeled instructions, our iterative self-improvement
scheme generates contrasting model outputs and trains an LLM-as-a-Judge to
produce reasoning traces and final judgments, repeating this training at each
new iteration using the improved predictions. Without any labeled preference
data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)
from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms
commonly used LLM judges such as GPT-4 and matches the performance of the
top-performing reward models trained with labeled examples.","[{'name': 'Tianlu Wang'}, {'name': 'Ilia Kulikov'}, {'name': 'Olga Golovneva'}, {'name': 'Ping Yu'}, {'name': 'Weizhe Yuan'}, {'name': 'Jane Dwivedi-Yu'}, {'name': 'Richard Yuanzhe Pang'}, {'name': 'Maryam Fazel-Zarandi'}, {'name': 'Jason Weston'}, {'name': 'Xian Li'}]",2024-08-05T17:57:02Z
http://arxiv.org/abs/2408.02632v1,http://arxiv.org/abs/2408.02632v1,"SEAS: Self-Evolving Adversarial Safety Optimization for Large Language
  Models","As large language models (LLMs) continue to advance in capability and
influence, ensuring their security and preventing harmful outputs has become
crucial. A promising approach to address these concerns involves training
models to automatically generate adversarial prompts for red teaming. However,
the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness
of current adversarial methods, which struggle to specifically target and
explore the weaknesses of these models. To tackle these challenges, we
introduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving
}\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$
optimization framework, which enhances security by leveraging data generated by
the model itself. SEAS operates through three iterative stages: Initialization,
Attack, and Adversarial Optimization, refining both the Red Team and Target
models to improve robustness and safety. This framework reduces reliance on
manual testing and significantly enhances the security capabilities of LLMs.
Our contributions include a novel adversarial framework, a comprehensive safety
dataset, and after three iterations, the Target model achieves a security level
comparable to GPT-4, while the Red Team model shows a marked increase in attack
success rate (ASR) against advanced models.","[{'name': 'Muxi Diao'}, {'name': 'Rumei Li'}, {'name': 'Shiyang Liu'}, {'name': 'Guogang Liao'}, {'name': 'Jingang Wang'}, {'name': 'Xunliang Cai'}, {'name': 'Weiran Xu'}]",2024-08-05T16:55:06Z
http://arxiv.org/abs/2408.02622v1,http://arxiv.org/abs/2408.02622v1,Language Model Can Listen While Speaking,"Dialogue serves as the most natural manner of human-computer interaction
(HCI). Recent advancements in speech language models (SLM) have significantly
enhanced speech-based conversational AI. However, these models are limited to
turn-based conversation, lacking the ability to interact with humans in
real-time spoken scenarios, for example, being interrupted when the generated
content is not satisfactory. To address these limitations, we explore full
duplex modeling (FDM) in interactive speech language models (iSLM), focusing on
enhancing real-time interaction and, more explicitly, exploring the
quintessential ability of interruption. We introduce a novel model design,
namely listening-while-speaking language model (LSLM), an end-to-end system
equipped with both listening and speaking channels. Our LSLM employs a
token-based decoder-only TTS for speech generation and a streaming
self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses
both channels for autoregressive generation and detects turn-taking in real
time. Three fusion strategies -- early fusion, middle fusion, and late fusion
-- are explored, with middle fusion achieving an optimal balance between speech
generation and real-time interaction. Two experimental settings, command-based
FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity
to diverse instructions. Our results highlight LSLM's capability to achieve
duplex communication with minimal impact on existing systems. This study aims
to advance the development of interactive speech dialogue systems, enhancing
their applicability in real-world contexts.","[{'name': 'Ziyang Ma'}, {'name': 'Yakun Song'}, {'name': 'Chenpeng Du'}, {'name': 'Jian Cong'}, {'name': 'Zhuo Chen'}, {'name': 'Yuping Wang'}, {'name': 'Yuxuan Wang'}, {'name': 'Xie Chen'}]",2024-08-05T16:47:22Z
http://arxiv.org/abs/2408.02600v1,http://arxiv.org/abs/2408.02600v1,"BioMamba: A Pre-trained Biomedical Language Representation Model
  Leveraging Mamba","The advancement of natural language processing (NLP) in biology hinges on
models' ability to interpret intricate biomedical literature. Traditional
models often struggle with the complex and domain-specific language in this
field. In this paper, we present BioMamba, a pre-trained model specifically
designed for biomedical text mining. BioMamba builds upon the Mamba
architecture and is pre-trained on an extensive corpus of biomedical
literature. Our empirical studies demonstrate that BioMamba significantly
outperforms models like BioBERT and general-domain Mamba across various
biomedical tasks. For instance, BioMamba achieves a 100 times reduction in
perplexity and a 4 times reduction in cross-entropy loss on the BioASQ test
set. We provide an overview of the model architecture, pre-training process,
and fine-tuning techniques. Additionally, we release the code and trained model
to facilitate further research.","[{'name': 'Ling Yue'}, {'name': 'Sixue Xing'}, {'name': 'Yingzhou Lu'}, {'name': 'Tianfan Fu'}]",2024-08-05T16:21:36Z
http://arxiv.org/abs/2408.02599v1,http://arxiv.org/abs/2408.02599v1,Progressively Selective Label Enhancement for Language Model Alignment,"Large Language Models have demonstrated impressive capabilities in various
language tasks but may produce content that misaligns with human expectations,
raising ethical and legal concerns. Therefore, it is important to explore the
limitations and implement restrictions on the models to ensure safety and
compliance, with Reinforcement Learning from Human Feedback (RLHF) being the
primary method. Due to challenges in stability and scalability with the RLHF
stages, researchers are exploring alternative methods to achieve effects
comparable to those of RLHF. However, these methods often depend on large
high-quality datasets and inefficiently utilize generated data. To deal with
this problem, we propose PSLE, i.e., Progressively Selective Label Enhancement
for Language Model Alignment, a framework that fully utilizes all generated
data by guiding the model with principles to align outputs with human
expectations. Using a dynamically updated threshold, our approach ensures
efficient data utilization by incorporating all generated responses and
weighting them based on their corresponding reward scores. Experimental results
on multiple datasets demonstrate the effectiveness of PSLE compared to existing
language model alignment methods.","[{'name': 'Biao Liu'}, {'name': 'Ning Xu'}, {'name': 'Xin Geng'}]",2024-08-05T16:21:17Z
http://arxiv.org/abs/2408.02559v1,http://arxiv.org/abs/2408.02559v1,"Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan:
  A Multi-Player Cooperative Game under Imperfect Information","Large language models (LLMs) have shown success in handling simple games with
imperfect information and enabling multi-agent coordination, but their ability
to facilitate practical collaboration against other agents in complex,
imperfect information environments, especially in a non-English environment,
still needs to be explored. This study investigates the applicability of
knowledge acquired by open-source and API-based LLMs to sophisticated
text-based games requiring agent collaboration under imperfect information,
comparing their performance to established baselines using other types of
agents. We propose a Theory of Mind (ToM) planning technique that allows LLM
agents to adapt their strategy against various adversaries using only game
rules, current state, and historical context as input. An external tool was
incorporated to mitigate the challenge of dynamic and extensive action spaces
in this card game. Our results show that although a performance gap exists
between current LLMs and state-of-the-art reinforcement learning (RL) models,
LLMs demonstrate ToM capabilities in this game setting. It consistently
improves their performance against opposing agents, suggesting their ability to
understand the actions of allies and adversaries and establish collaboration
with allies. To encourage further research and understanding, we have made our
codebase openly accessible.","[{'name': 'Yauwai Yim'}, {'name': 'Chunkit Chan'}, {'name': 'Tianyu Shi'}, {'name': 'Zheye Deng'}, {'name': 'Wei Fan'}, {'name': 'Tianshi Zheng'}, {'name': 'Yangqiu Song'}]",2024-08-05T15:36:46Z
http://arxiv.org/abs/2408.02545v1,http://arxiv.org/abs/2408.02545v1,"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented
  Generation","Implementing Retrieval-Augmented Generation (RAG) systems is inherently
complex, requiring deep understanding of data, use cases, and intricate design
decisions. Additionally, evaluating these systems presents significant
challenges, necessitating assessment of both retrieval accuracy and generative
quality through a multi-faceted approach. We introduce RAG Foundry, an
open-source framework for augmenting large language models for RAG use cases.
RAG Foundry integrates data creation, training, inference and evaluation into a
single workflow, facilitating the creation of data-augmented datasets for
training and evaluating large language models in RAG settings. This integration
enables rapid prototyping and experimentation with various RAG techniques,
allowing users to easily generate datasets and train RAG models using internal
or specialized knowledge sources. We demonstrate the framework effectiveness by
augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG
configurations, showcasing consistent improvements across three
knowledge-intensive datasets. Code is released as open-source in
https://github.com/IntelLabs/RAGFoundry.","[{'name': 'Daniel Fleischer'}, {'name': 'Moshe Berchansky'}, {'name': 'Moshe Wasserblat'}, {'name': 'Peter Izsak'}]",2024-08-05T15:16:24Z
http://arxiv.org/abs/2408.02544v1,http://arxiv.org/abs/2408.02544v1,"Caution for the Environment: Multimodal Agents are Susceptible to
  Environmental Distractions","This paper investigates the faithfulness of multimodal large language model
(MLLM) agents in the graphical user interface (GUI) environment, aiming to
address the research question of whether multimodal GUI agents can be
distracted by environmental context. A general setting is proposed where both
the user and the agent are benign, and the environment, while not malicious,
contains unrelated content. A wide range of MLLMs are evaluated as GUI agents
using our simulated dataset, following three working patterns with different
levels of perception. Experimental results reveal that even the most powerful
models, whether generalist agents or specialist GUI agents, are susceptible to
distractions. While recent studies predominantly focus on the helpfulness
(i.e., action accuracy) of multimodal agents, our findings indicate that these
agents are prone to environmental distractions, resulting in unfaithful
behaviors. Furthermore, we switch to the adversarial perspective and implement
environment injection, demonstrating that such unfaithfulness can be exploited,
leading to unexpected risks.","[{'name': 'Xinbei Ma'}, {'name': 'Yiting Wang'}, {'name': 'Yao Yao'}, {'name': 'Tongxin Yuan'}, {'name': 'Aston Zhang'}, {'name': 'Zhuosheng Zhang'}, {'name': 'Hai Zhao'}]",2024-08-05T15:16:22Z
http://arxiv.org/abs/2408.02520v1,http://arxiv.org/abs/2408.02520v1,"OneLove beyond the field -- A few-shot pipeline for topic and sentiment
  analysis during the FIFA World Cup in Qatar","The FIFA World Cup in Qatar was discussed extensively in the news and on
social media. Due to news reports with allegations of human rights violations,
there were calls to boycott it. Wearing a OneLove armband was part of a planned
protest activity. Controversy around the armband arose when FIFA threatened to
sanction captains who wear it. To understand what topics Twitter users Tweeted
about and what the opinion of German Twitter users was towards the OneLove
armband, we performed an analysis of German Tweets published during the World
Cup using in-context learning with LLMs. We validated the labels on human
annotations. We found that Twitter users initially discussed the armband's
impact, LGBT rights, and politics; after the ban, the conversation shifted
towards politics in sports in general, accompanied by a subtle shift in
sentiment towards neutrality. Our evaluation serves as a framework for future
research to explore the impact of sports activism and evolving public
sentiment. This is especially useful in settings where labeling datasets for
specific opinions is unfeasible, such as when events are unfolding.","[{'name': 'Christoph Rauchegger'}, {'name': 'Sonja Mei Wang'}, {'name': 'Pieter Delobelle'}]",2024-08-05T14:40:40Z
http://arxiv.org/abs/2408.04658v1,http://arxiv.org/abs/2408.04658v1,Winning Amazon KDD Cup'24,"This paper describes the winning solution of all 5 tasks for the Amazon KDD
Cup 2024 Multi Task Online Shopping Challenge for LLMs. The challenge was to
build a useful assistant, answering questions in the domain of online shopping.
The competition contained 57 diverse tasks, covering 5 different task types
(e.g. multiple choice) and across 4 different tracks (e.g. multi-lingual). Our
solution is a single model per track. We fine-tune Qwen2-72B-Instruct on our
own training dataset. As the competition released only 96 example questions, we
developed our own training dataset by processing multiple public datasets or
using Large Language Models for data augmentation and synthetic data
generation. We apply wise-ft to account for distribution shifts and ensemble
multiple LoRA adapters in one model. We employed Logits Processors to constrain
the model output on relevant tokens for the tasks. AWQ 4-bit Quantization and
vLLM are used during inference to predict the test dataset in the time
constraints of 20 to 140 minutes depending on the track. Our solution achieved
the first place in each individual track and is the first place overall of
Amazons KDD Cup 2024.","[{'name': 'Chris Deotte'}, {'name': 'Ivan Sorokin'}, {'name': 'Ahmet Erdem'}, {'name': 'Benedikt Schifferer'}, {'name': 'Gilberto Titericz Jr'}, {'name': 'Simon Jegou'}]",2024-08-05T14:40:04Z
http://arxiv.org/abs/2408.02503v1,http://arxiv.org/abs/2408.02503v1,"UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks
  With Large Language Model","Significant advancements has recently been achieved in the field of
multi-modal large language models (MLLMs), demonstrating their remarkable
capabilities in understanding and reasoning across diverse tasks. However,
these models are often trained for specific tasks and rely on task-specific
input-output formats, limiting their applicability to a broader range of tasks.
This raises a fundamental question: Can we develop a unified approach to
represent and handle different multi-modal tasks to maximize the
generalizability of MLLMs? In this paper, we propose UnifiedMLLM, a
comprehensive model designed to represent various tasks using a unified
representation. Our model exhibits strong capabilities in comprehending the
implicit intent of user instructions and preforming reasoning. In addition to
generating textual responses, our model also outputs task tokens and grounding
tokens, serving as indicators of task types and task granularity. These outputs
are subsequently routed through the task router and directed to specific expert
models for task completion. To train our model, we construct a task-specific
dataset and an 100k multi-task dataset encompassing complex scenarios.
Employing a three-stage training strategy, we equip our model with robust
reasoning and task processing capabilities while preserving its generalization
capacity and knowledge reservoir. Extensive experiments showcase the impressive
performance of our unified representation approach across various tasks,
surpassing existing methodologies. Furthermore, our approach exhibits
exceptional scalability and generality. Our code, model, and dataset will be
available at \url{https://github.com/lzw-lzw/UnifiedMLLM}.","[{'name': 'Zhaowei Li'}, {'name': 'Wei Wang'}, {'name': 'YiQing Cai'}, {'name': 'Xu Qi'}, {'name': 'Pengyu Wang'}, {'name': 'Dong Zhang'}, {'name': 'Hang Song'}, {'name': 'Botian Jiang'}, {'name': 'Zhida Huang'}, {'name': 'Tao Wang'}]",2024-08-05T14:27:39Z
http://arxiv.org/abs/2408.02479v1,http://arxiv.org/abs/2408.02479v1,"From LLMs to LLM-based Agents for Software Engineering: A Survey of
  Current, Challenges and Future","With the rise of large language models (LLMs), researchers are increasingly
exploring their applications in var ious vertical domains, such as software
engineering. LLMs have achieved remarkable success in areas including code
generation and vulnerability detection. However, they also exhibit numerous
limitations and shortcomings. LLM-based agents, a novel tech nology with the
potential for Artificial General Intelligence (AGI), combine LLMs as the core
for decision-making and action-taking, addressing some of the inherent
limitations of LLMs such as lack of autonomy and self-improvement. Despite
numerous studies and surveys exploring the possibility of using LLMs in
software engineering, it lacks a clear distinction between LLMs and LLM based
agents. It is still in its early stage for a unified standard and benchmarking
to qualify an LLM solution as an LLM-based agent in its domain. In this survey,
we broadly investigate the current practice and solutions for LLMs and
LLM-based agents for software engineering. In particular we summarise six key
topics: requirement engineering, code generation, autonomous decision-making,
software design, test generation, and software maintenance. We review and
differentiate the work of LLMs and LLM-based agents from these six topics,
examining their differences and similarities in tasks, benchmarks, and
evaluation metrics. Finally, we discuss the models and benchmarks used,
providing a comprehensive analysis of their applications and effectiveness in
software engineering. We anticipate this work will shed some lights on pushing
the boundaries of LLM-based agents in software engineering for future research.","[{'name': 'Haolin Jin'}, {'name': 'Linghan Huang'}, {'name': 'Haipeng Cai'}, {'name': 'Jun Yan'}, {'name': 'Bo Li'}, {'name': 'Huaming Chen'}]",2024-08-05T14:01:15Z
http://arxiv.org/abs/2408.02442v1,http://arxiv.org/abs/2408.02442v1,"Let Me Speak Freely? A Study on the Impact of Format Restrictions on
  Performance of Large Language Models","Structured generation, the process of producing content in standardized
formats like JSON and XML, is widely utilized in real-world applications to
extract key output information from large language models (LLMs). This study
investigates whether such constraints on generation space impact LLMs'
abilities, including reasoning and domain knowledge comprehension.
Specifically, we evaluate LLMs' performance when restricted to adhere to
structured formats versus generating free-form responses across various common
tasks. Surprisingly, we observe a significant decline in LLMs' reasoning
abilities under format restrictions. Furthermore, we find that stricter format
constraints generally lead to greater performance degradation in reasoning
tasks.","[{'name': 'Zhi Rui Tam'}, {'name': 'Cheng-Kuang Wu'}, {'name': 'Yi-Lin Tsai'}, {'name': 'Chieh-Yen Lin'}, {'name': 'Hung-yi Lee'}, {'name': 'Yun-Nung Chen'}]",2024-08-05T13:08:24Z
http://arxiv.org/abs/2408.02439v1,http://arxiv.org/abs/2408.02439v1,Long Input Benchmark for Russian Analysis,"Recent advancements in Natural Language Processing (NLP) have fostered the
development of Large Language Models (LLMs) that can solve an immense variety
of tasks. One of the key aspects of their application is their ability to work
with long text documents and to process long sequences of tokens. This has
created a demand for proper evaluation of long-context understanding. To
address this need for the Russian language, we propose LIBRA (Long Input
Benchmark for Russian Analysis), which comprises 21 adapted datasets to study
the LLM's abilities to understand long texts thoroughly. The tests are divided
into four complexity groups and allow the evaluation of models across various
context lengths ranging from 4k up to 128k tokens. We provide the open-source
datasets, codebase, and public leaderboard for LIBRA to guide forthcoming
research.","[{'name': 'Igor Churin'}, {'name': 'Murat Apishev'}, {'name': 'Maria Tikhonova'}, {'name': 'Denis Shevelev'}, {'name': 'Aydar Bulatov'}, {'name': 'Yuri Kuratov'}, {'name': 'Sergej Averkiev'}, {'name': 'Alena Fenogenova'}]",2024-08-05T12:59:35Z
http://arxiv.org/abs/2408.04656v1,http://arxiv.org/abs/2408.04656v1,Towards Semantic Markup of Mathematical Documents via User Interaction,"Mathematical documents written in LaTeX often contain ambiguities. We can
resolve some of them via semantic markup using, e.g., sTeX, which also has
other potential benefits, such as interoperability with computer algebra
systems, proof systems, and increased accessibility. However, semantic markup
is more involved than ""regular"" typesetting and presents a challenge for
authors of mathematical documents. We aim to smooth out the transition from
plain LaTeX to semantic markup by developing semi-automatic tools for authors.
In this paper we present an approach to semantic markup of formulas by
(semi-)automatically generating grammars from existing sTeX macro definitions
and parsing mathematical formulas with them. We also present a GUI-based tool
for the disambiguation of parse results and showcase its functionality and
potential using a grammar for parsing untyped $\lambda$-terms.","[{'name': 'Luka Vrečar'}, {'name': 'Joe Wells'}, {'name': 'Fairouz Kamareddine'}]",2024-08-05T12:36:40Z
http://arxiv.org/abs/2408.02417v1,http://arxiv.org/abs/2408.02417v1,"Infusing Emotions into Task-oriented Dialogue Systems: Understanding,
  Management, and Generation","Emotions are indispensable in human communication, but are often overlooked
in task-oriented dialogue (ToD) modelling, where the task success is the
primary focus. While existing works have explored user emotions or similar
concepts in some ToD tasks, none has so far included emotion modelling into a
fully-fledged ToD system nor conducted interaction with human or simulated
users. In this work, we incorporate emotion into the complete ToD processing
loop, involving understanding, management, and generation. To this end, we
extend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour
labels. Through interactive experimentation involving both simulated and human
users, we demonstrate that our proposed framework significantly enhances the
user's emotional experience as well as the task success.","[{'name': 'Shutong Feng'}, {'name': 'Hsien-chin Lin'}, {'name': 'Christian Geishauser'}, {'name': 'Nurul Lubis'}, {'name': 'Carel van Niekerk'}, {'name': 'Michael Heck'}, {'name': 'Benjamin Ruppik'}, {'name': 'Renato Vukovic'}, {'name': 'Milica Gašić'}]",2024-08-05T12:21:04Z
http://arxiv.org/abs/2408.02416v1,http://arxiv.org/abs/2408.02416v1,"Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in
  Customized Large Language Models","The drastic increase of large language models' (LLMs) parameters has led to a
new research direction of fine-tuning-free downstream customization by prompts,
i.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)
play an important role in many businesses, there has emerged growing concerns
about the prompt leakage, which undermines the intellectual properties of these
services and causes downstream attacks. In this paper, we analyze the
underlying mechanism of prompt leakage, which we refer to as prompt
memorization, and develop corresponding defending strategies. By exploring the
scaling laws in prompt extraction, we analyze key attributes that influence
prompt extraction, including model sizes, prompt lengths, as well as the types
of prompts. Then we propose two hypotheses that explain how LLMs expose their
prompts. The first is attributed to the perplexity, i.e. the familiarity of
LLMs to texts, whereas the second is based on the straightforward token
translation path in attention matrices. To defend against such threats, we
investigate whether alignments can undermine the extraction of prompts. We find
that current LLMs, even those with safety alignments like GPT-4, are highly
vulnerable to prompt extraction attacks, even under the most straightforward
user attacks. Therefore, we put forward several defense strategies with the
inspiration of our findings, which achieve 83.8\% and 71.0\% drop in the prompt
extraction rate for Llama2-7B and GPT-3.5, respectively. Source code is
avaliable at \url{https://github.com/liangzid/PromptExtractionEval}.","[{'name': 'Zi Liang'}, {'name': 'Haibo Hu'}, {'name': 'Qingqing Ye'}, {'name': 'Yaxin Xiao'}, {'name': 'Haoyang Li'}]",2024-08-05T12:20:39Z
http://arxiv.org/abs/2408.04655v2,http://arxiv.org/abs/2408.04655v2,Strong and weak alignment of large language models with human values,"Minimizing negative impacts of Artificial Intelligent (AI) systems on human
societies without human supervision requires them to be able to align with
human values. However, most current work only addresses this issue from a
technical point of view, e.g., improving current methods relying on
reinforcement learning from human feedback, neglecting what it means and is
required for alignment to occur. Here, we propose to distinguish strong and
weak value alignment. Strong alignment requires cognitive abilities (either
human-like or different from humans) such as understanding and reasoning about
agents' intentions and their ability to causally produce desired effects. We
argue that this is required for AI systems like large language models (LLMs) to
be able to recognize situations presenting a risk that human values may be
flouted. To illustrate this distinction, we present a series of prompts showing
ChatGPT's, Gemini's and Copilot's failures to recognize some of these
situations. We moreover analyze word embeddings to show that the nearest
neighbors of some human values in LLMs differ from humans' semantic
representations. We then propose a new thought experiment that we call ""the
Chinese room with a word transition dictionary"", in extension of John Searle's
famous proposal. We finally mention current promising research directions
towards a weak alignment, which could produce statistically satisfying answers
in a number of common situations, however so far without ensuring any truth
value.","[{'name': 'Mehdi Khamassi'}, {'name': 'Marceau Nahon'}, {'name': 'Raja Chatila'}]",2024-08-05T11:27:51Z
http://arxiv.org/abs/2408.02377v1,http://arxiv.org/abs/2408.02377v1,"A Few-Shot Approach for Relation Extraction Domain Adaptation using
  Large Language Models","Knowledge graphs (KGs) have been successfully applied to the analysis of
complex scientific and technological domains, with automatic KG generation
methods typically building upon relation extraction models capturing
fine-grained relations between domain entities in text. While these relations
are fully applicable across scientific areas, existing models are trained on
few domain-specific datasets such as SciERC and do not perform well on new
target domains. In this paper, we experiment with leveraging in-context
learning capabilities of Large Language Models to perform schema-constrained
data annotation, collecting in-domain training instances for a
Transformer-based relation extraction model deployed on titles and abstracts of
research papers in the Architecture, Construction, Engineering and Operations
(AECO) domain. By assessing the performance gain with respect to a baseline
Deep Learning architecture trained on off-domain data, we show that by using a
few-shot learning strategy with structured prompts and only minimal expert
annotation the presented approach can potentially support domain adaptation of
a science KG generation model.","[{'name': 'Vanni Zavarella'}, {'name': 'Juan Carlos Gamero-Salinas'}, {'name': 'Sergio Consoli'}]",2024-08-05T11:06:36Z
http://arxiv.org/abs/2408.02361v1,http://arxiv.org/abs/2408.02361v1,"Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought
  Decoding","State-of-the-art task-oriented dialogue systems typically rely on
task-specific ontologies for fulfilling user queries. The majority of
task-oriented dialogue data, such as customer service recordings, comes without
ontology and annotation. Such ontologies are normally built manually, limiting
the application of specialised systems. Dialogue ontology construction is an
approach for automating that process and typically consists of two steps: term
extraction and relation extraction. In this work, we focus on relation
extraction in a transfer learning set-up. To improve the generalisation, we
propose an extension to the decoding mechanism of large language models. We
adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning
problems, to generative relation extraction. Here, we generate multiple
branches in the decoding space and select the relations based on a confidence
threshold. By constraining the decoding to ontology terms and relations, we aim
to decrease the risk of hallucination. We conduct extensive experimentation on
two widely used datasets and find improvements in performance on target
ontology for source fine-tuned and one-shot prompted large language models.","[{'name': 'Renato Vukovic'}, {'name': 'David Arps'}, {'name': 'Carel van Niekerk'}, {'name': 'Benjamin Matthias Ruppik'}, {'name': 'Hsien-Chin Lin'}, {'name': 'Michael Heck'}, {'name': 'Milica Gašić'}]",2024-08-05T10:10:01Z
http://arxiv.org/abs/2408.02341v1,http://arxiv.org/abs/2408.02341v1,"An approach to optimize inference of the DIART speaker diarization
  pipeline","Speaker diarization answers the question ""who spoke when"" for an audio file.
In some diarization scenarios, low latency is required for transcription.
Speaker diarization with low latency is referred to as online speaker
diarization. The DIART pipeline is an online speaker diarization system. It
consists of a segmentation and an embedding model. The embedding model has the
largest share of the overall latency. The aim of this paper is to optimize the
inference latency of the DIART pipeline. Different inference optimization
methods such as knowledge distilation, pruning, quantization and layer fusion
are applied to the embedding model of the pipeline. It turns out that knowledge
distillation optimizes the latency, but has a negative effect on the accuracy.
Quantization and layer fusion also have a positive influence on the latency
without worsening the accuracy. Pruning, on the other hand, does not improve
latency.","[{'name': 'Roman Aperdannier'}, {'name': 'Sigurd Schacht'}, {'name': 'Alexander Piazza'}]",2024-08-05T09:38:07Z
http://arxiv.org/abs/2408.04653v1,http://arxiv.org/abs/2408.04653v1,Batching BPE Tokenization Merges,"The Byte Pair Encoding algorithm can be safely batched to merge hundreds of
pairs of tokens at a time when building up a tokenizer's vocabulary. This
technique combined with reducing the memory footprint of text used in
vocabulary training make it feasible to train a high quality tokenizer on a
basic laptop. This paper presents BatchBPE, an open-source pure Python
implementation of these concepts, with the goal of making experimenting with
new tokenization strategies more accessible especially in compute- and
memory-constrained contexts. BatchBPE's usefulness and malleability are
demonstrated through the training of several token vocabularies to explore the
batch merging process and experiment with preprocessing a stop word list and
ignoring the least common text chunks in a dataset. Resultant encoded lengths
of texts are used as a basic evaluation metric.",[{'name': 'Alexander P. Morgan'}],2024-08-05T09:37:21Z
http://arxiv.org/abs/2408.02337v1,http://arxiv.org/abs/2408.02337v1,"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR
  Dataset Construction","Advancements in AI and natural language processing have revolutionized
machine-human language interactions, with question answering (QA) systems
playing a pivotal role. The knowledge base question answering (KBQA) task,
utilizing structured knowledge graphs (KG), allows for handling extensive
knowledge-intensive questions. However, a significant gap exists in KBQA
datasets, especially for low-resource languages. Many existing construction
pipelines for these datasets are outdated and inefficient in human labor, and
modern assisting tools like Large Language Models (LLM) are not utilized to
reduce the workload. To address this, we have designed and implemented a
modern, semi-automated approach for creating datasets, encompassing tasks such
as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),
tailored explicitly for low-resource environments. We executed this pipeline
and introduced the PUGG dataset, the first Polish KBQA dataset, and novel
datasets for MRC and IR. Additionally, we provide a comprehensive
implementation, insightful findings, detailed statistics, and evaluation of
baseline models.","[{'name': 'Albert Sawczyn'}, {'name': 'Katsiaryna Viarenich'}, {'name': 'Konrad Wojtasik'}, {'name': 'Aleksandra Domogała'}, {'name': 'Marcin Oleksy'}, {'name': 'Maciej Piasecki'}, {'name': 'Tomasz Kajdanowicz'}]",2024-08-05T09:23:49Z
http://arxiv.org/abs/2408.02302v1,http://arxiv.org/abs/2408.02302v1,"SNFinLLM: Systematic and Nuanced Financial Domain Adaptation of Chinese
  Large Language Models","Large language models (LLMs) have become powerful tools for advancing natural
language processing applications in the financial industry. However, existing
financial LLMs often face challenges such as hallucinations or superficial
parameter training, resulting in suboptimal performance, particularly in
financial computing and machine reading comprehension (MRC). To address these
issues, we propose a novel large language model specifically designed for the
Chinese financial domain, named SNFinLLM. SNFinLLM excels in domain-specific
tasks such as answering questions, summarizing financial research reports,
analyzing sentiment, and executing financial calculations. We then perform the
supervised fine-tuning (SFT) to enhance the model's proficiency across various
financial domains. Specifically, we gather extensive financial data and create
a high-quality instruction dataset composed of news articles, professional
papers, and research reports of finance domain. Utilizing both domain-specific
and general datasets, we proceed with continuous pre-training on an established
open-source base model, resulting in SNFinLLM-base. Following this, we engage
in supervised fine-tuning (SFT) to bolster the model's capability across
multiple financial tasks. Crucially, we employ a straightforward Direct
Preference Optimization (DPO) method to better align the model with human
preferences. Extensive experiments conducted on finance benchmarks and our
evaluation dataset demonstrate that SNFinLLM markedly outperforms other
state-of-the-art financial language models. For more details, check out our
demo video here: https://www.youtube.com/watch?v=GYT-65HZwus.","[{'name': 'Shujuan Zhao'}, {'name': 'Lingfeng Qiao'}, {'name': 'Kangyang Luo'}, {'name': 'Qian-Wen Zhang'}, {'name': 'Junru Lu'}, {'name': 'Di Yin'}]",2024-08-05T08:24:24Z
http://arxiv.org/abs/2408.02290v1,http://arxiv.org/abs/2408.02290v1,"Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen
  Languages","Multilingual neural machine translation systems learn to map sentences of
different languages into a common representation space. Intuitively, with a
growing number of seen languages the encoder sentence representation grows more
flexible and easily adaptable to new languages. In this work, we test this
hypothesis by zero-shot translating from unseen languages. To deal with unknown
vocabularies from unknown languages we propose a setup where we decouple
learning of vocabulary and syntax, i.e. for each language we learn word
representations in a separate step (using cross-lingual word embeddings), and
then train to translate while keeping those word representations frozen. We
demonstrate that this setup enables zero-shot translation from entirely unseen
languages. Zero-shot translating with a model trained on Germanic and Romance
languages we achieve scores of 42.6 BLEU for Portuguese-English and 20.7 BLEU
for Russian-English on TED domain. We explore how this zero-shot translation
capability develops with varying number of languages seen by the encoder.
Lastly, we explore the effectiveness of our decoupled learning strategy for
unsupervised machine translation. By exploiting our model's zero-shot
translation capability for iterative back-translation we attain near parity
with a supervised setting.","[{'name': 'Carlos Mullov'}, {'name': 'Ngoc-Quan Pham'}, {'name': 'Alexander Waibel'}]",2024-08-05T07:58:58Z
http://arxiv.org/abs/2408.02288v1,http://arxiv.org/abs/2408.02288v1,Spin glass model of in-context learning,"Large language models show a surprising in-context learning ability -- being
able to use a prompt to form a prediction for a query, yet without additional
training, in stark contrast to old-fashioned supervised learning. Providing a
mechanistic interpretation and linking the empirical phenomenon to physics are
thus challenging and remain unsolved. We study a simple yet expressive
transformer with linear attention, and map this structure to a spin glass model
with real-valued spins, where the couplings and fields explain the intrinsic
disorder in data. The spin glass model explains how the weight parameters
interact with each other during pre-training, and most importantly why an
unseen function can be predicted by providing only a prompt yet without
training. Our theory reveals that for single instance learning, increasing the
task diversity leads to the emergence of the in-context learning, by allowing
the Boltzmann distribution to converge to a unique correct solution of weight
parameters. Therefore the pre-trained transformer displays a prediction power
in a novel prompt setting. The proposed spin glass model thus establishes a
foundation to understand the empirical success of large language models.","[{'name': 'Yuhao Li'}, {'name': 'Ruoran Bai'}, {'name': 'Haiping Huang'}]",2024-08-05T07:54:01Z
http://arxiv.org/abs/2408.02272v1,http://arxiv.org/abs/2408.02272v1,"COM Kitchens: An Unedited Overhead-view Video Dataset as a
  Vision-Language Benchmark","Procedural video understanding is gaining attention in the vision and
language community. Deep learning-based video analysis requires extensive data.
Consequently, existing works often use web videos as training resources, making
it challenging to query instructional contents from raw video observations. To
address this issue, we propose a new dataset, COM Kitchens. The dataset
consists of unedited overhead-view videos captured by smartphones, in which
participants performed food preparation based on given recipes. Fixed-viewpoint
video datasets often lack environmental diversity due to high camera setup
costs. We used modern wide-angle smartphone lenses to cover cooking counters
from sink to cooktop in an overhead view, capturing activity without in-person
assistance. With this setup, we collected a diverse dataset by distributing
smartphones to participants. With this dataset, we propose the novel
video-to-text retrieval task Online Recipe Retrieval (OnRR) and new video
captioning domain Dense Video Captioning on unedited Overhead-View videos
(DVC-OV). Our experiments verified the capabilities and limitations of current
web-video-based SOTA methods in handling these tasks.","[{'name': 'Koki Maeda'}, {'name': 'Tosho Hirasawa'}, {'name': 'Atsushi Hashimoto'}, {'name': 'Jun Harashima'}, {'name': 'Leszek Rybicki'}, {'name': 'Yusuke Fukasawa'}, {'name': 'Yoshitaka Ushiku'}]",2024-08-05T07:00:10Z
http://arxiv.org/abs/2408.02271v1,http://arxiv.org/abs/2408.02271v1,"StyEmp: Stylizing Empathetic Response Generation via Multi-Grained
  Prefix Encoder and Personality Reinforcement","Recent approaches for empathetic response generation mainly focus on
emotional resonance and user understanding, without considering the system's
personality. Consistent personality is evident in real human expression and is
important for creating trustworthy systems. To address this problem, we propose
StyEmp, which aims to stylize the empathetic response generation with a
consistent personality. Specifically, it incorporates a multi-grained prefix
mechanism designed to capture the intricate relationship between a system's
personality and its empathetic expressions. Furthermore, we introduce a
personality reinforcement module that leverages contrastive learning to
calibrate the generation model, ensuring that responses are both empathetic and
reflective of a distinct personality. Automatic and human evaluations on the
EMPATHETICDIALOGUES benchmark show that StyEmp outperforms competitive
baselines in terms of both empathy and personality expressions.","[{'name': 'Yahui Fu'}, {'name': 'Chenhui Chu'}, {'name': 'Tatsuya Kawahara'}]",2024-08-05T06:59:56Z
http://arxiv.org/abs/2408.02257v1,http://arxiv.org/abs/2408.02257v1,"To Aggregate or Not to Aggregate. That is the Question: A Case Study on
  Annotation Subjectivity in Span Prediction","This paper explores the task of automatic prediction of text spans in a legal
problem description that support a legal area label. We use a corpus of problem
descriptions written by laypeople in English that is annotated by practising
lawyers. Inherent subjectivity exists in our task because legal area
categorisation is a complex task, and lawyers often have different views on a
problem, especially in the face of legally-imprecise descriptions of issues.
Experiments show that training on majority-voted spans outperforms training on
disaggregated ones.","[{'name': 'Kemal Kurniawan'}, {'name': 'Meladel Mistica'}, {'name': 'Timothy Baldwin'}, {'name': 'Jey Han Lau'}]",2024-08-05T06:16:31Z
http://arxiv.org/abs/2408.02253v2,http://arxiv.org/abs/2408.02253v2,Advancing Post-OCR Correction: A Comparative Study of Synthetic Data,"This paper explores the application of synthetic data in the post-OCR domain
on multiple fronts by conducting experiments to assess the impact of data
volume, augmentation, and synthetic data generation methods on model
performance. Furthermore, we introduce a novel algorithm that leverages
computer vision feature detection algorithms to calculate glyph similarity for
constructing post-OCR synthetic data. Through experiments conducted across a
variety of languages, including several low-resource ones, we demonstrate that
models like ByT5 can significantly reduce Character Error Rates (CER) without
the need for manually annotated data, and our proposed synthetic data
generation method shows advantages over traditional methods, particularly in
low-resource languages.","[{'name': 'Shuhao Guan'}, {'name': 'Derek Greene'}]",2024-08-05T05:56:37Z
http://arxiv.org/abs/2408.02248v1,http://arxiv.org/abs/2408.02248v1,ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems,"Recently, there has been increasing interest in using Large Language Models
(LLMs) to construct complex multi-agent systems to perform tasks such as
compiling literature reviews, drafting consumer reports, and planning
vacations. Many tools and libraries exist for helping create such systems,
however none support recursive multi-agent systems -- where the models
themselves flexibly decide when to delegate tasks and how to organize their
delegation structure. In this work, we introduce ReDel: a toolkit for recursive
multi-agent systems that supports custom tool-use, delegation schemes,
event-based logging, and interactive replay in an easy-to-use web interface. We
show that, using ReDel, we are able to achieve significant performance gains on
agentic benchmarks and easily identify potential areas of improvements through
the visualization and debugging tools. Our code, documentation, and PyPI
package are open-source and free to use under the MIT license.","[{'name': 'Andrew Zhu'}, {'name': 'Liam Dugan'}, {'name': 'Chris Callison-Burch'}]",2024-08-05T05:43:23Z
http://arxiv.org/abs/2408.02239v1,http://arxiv.org/abs/2408.02239v1,BOTS-LM: Training Large Language Models for Setswana,"In this work we present BOTS-LM, a series of bilingual language models
proficient in both Setswana and English. Leveraging recent advancements in data
availability and efficient fine-tuning, BOTS-LM achieves performance similar to
models significantly larger than itself while maintaining computational
efficiency. Our initial release features an 8 billion parameter generative
large language model, with upcoming 0.5 billion and 1 billion parameter large
language models and a 278 million parameter encoder-only model soon to be
released. We find the 8 billion parameter model significantly outperforms
Llama-3-70B and Aya 23 on English-Setswana translation tasks, approaching the
performance of dedicated machine translation models, while approaching 70B
parameter performance on Setswana reasoning as measured by a machine translated
subset of the MMLU benchmark. To accompany the BOTS-LM series of language
models, we release the largest Setswana web dataset, SetsText, totalling over
267 million tokens. In addition, we release the largest machine translated
Setswana dataset, the first and largest synthetic Setswana dataset, training
and evaluation code, training logs, and MMLU-tsn, a machine translated subset
of MMLU.","[{'name': 'Nathan Brown'}, {'name': 'Vukosi Marivate'}]",2024-08-05T05:15:17Z
http://arxiv.org/abs/2408.02237v1,http://arxiv.org/abs/2408.02237v1,"Do Large Language Models Speak All Languages Equally? A Comparative
  Study in Low-Resource Settings","Large language models (LLMs) have garnered significant interest in natural
language processing (NLP), particularly their remarkable performance in various
downstream tasks in resource-rich languages. Recent studies have highlighted
the limitations of LLMs in low-resource languages, primarily focusing on binary
classification tasks and giving minimal attention to South Asian languages.
These limitations are primarily attributed to constraints such as dataset
scarcity, computational costs, and research gaps specific to low-resource
languages. To address this gap, we present datasets for sentiment and hate
speech tasks by translating from English to Bangla, Hindi, and Urdu,
facilitating research in low-resource language processing. Further, we
comprehensively examine zero-shot learning using multiple LLMs in English and
widely spoken South Asian languages. Our findings indicate that GPT-4
consistently outperforms Llama 2 and Gemini, with English consistently
demonstrating superior performance across diverse tasks compared to
low-resource languages. Furthermore, our analysis reveals that natural language
inference (NLI) exhibits the highest performance among the evaluated tasks,
with GPT-4 demonstrating superior capabilities.","[{'name': 'Md. Arid Hasan'}, {'name': 'Prerona Tarannum'}, {'name': 'Krishno Dey'}, {'name': 'Imran Razzak'}, {'name': 'Usman Naseem'}]",2024-08-05T05:09:23Z
http://arxiv.org/abs/2408.02233v1,http://arxiv.org/abs/2408.02233v1,"A Multi-Source Heterogeneous Knowledge Injected Prompt Learning Method
  for Legal Charge Prediction","Legal charge prediction, an essential task in legal AI, seeks to assign
accurate charge labels to case descriptions, attracting significant recent
interest. Existing methods primarily employ diverse neural network structures
for modeling case descriptions directly, failing to effectively leverage
multi-source external knowledge. We propose a prompt learning framework-based
method that simultaneously leverages multi-source heterogeneous external
knowledge from a legal knowledge base, a conversational LLM, and related legal
articles. Specifically, we match knowledge snippets in case descriptions via
the legal knowledge base and encapsulate them into the input through a hard
prompt template. Additionally, we retrieve legal articles related to a given
case description through contrastive learning, and then obtain factual elements
within the case description through a conversational LLM. We fuse the embedding
vectors of soft prompt tokens with the encoding vector of factual elements to
achieve knowledge-enhanced model forward inference. Experimental results show
that our method achieved state-of-the-art results on CAIL-2018, the largest
legal charge prediction dataset, and our method has lower data dependency. Case
studies also demonstrate our method's strong interpretability.","[{'name': 'Jingyun Sun'}, {'name': 'Chi Wei'}, {'name': 'Yang Li'}]",2024-08-05T04:53:17Z
http://arxiv.org/abs/2408.02201v1,http://arxiv.org/abs/2408.02201v1,"Evaluating the Performance of Large Language Models for SDG Mapping
  (Technical Report)","The use of large language models (LLMs) is expanding rapidly, and open-source
versions are becoming available, offering users safer and more adaptable
options. These models enable users to protect data privacy by eliminating the
need to provide data to third parties and can be customized for specific tasks.
In this study, we compare the performance of various language models on the
Sustainable Development Goal (SDG) mapping task, using the output of GPT-4o as
the baseline. The selected open-source models for comparison include Mixtral,
LLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more
specialized version of GPT-4o, was included to extend the comparison. Given the
multi-label nature of the SDG mapping task, we employed metrics such as F1
score, precision, and recall with micro-averaging to evaluate different aspects
of the models' performance. These metrics are derived from the confusion matrix
to ensure a comprehensive evaluation. We provide a clear observation and
analysis of each model's performance by plotting curves based on F1 score,
precision, and recall at different thresholds. According to the results of this
experiment, LLaMA 2 and Gemma still have significant room for improvement. The
other four models do not exhibit particularly large differences in performance.
The outputs from all seven models are available on Zenodo:
https://doi.org/10.5281/zenodo.12789375.","[{'name': 'Hui Yin'}, {'name': 'Amir Aryani'}, {'name': 'Nakul Nambiar'}]",2024-08-05T03:05:02Z
http://arxiv.org/abs/2408.02193v1,http://arxiv.org/abs/2408.02193v1,CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs,"Large language models (LLMs) have shown great potential in code-related
tasks, yet open-source models lag behind their closed-source counterparts. To
bridge this performance gap, existing methods generate vast amounts of
synthetic data for fine-tuning, leading to inefficiencies in training.
Motivated by the need for more effective and efficient training, we propose the
Code Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces
the Complexity and Diversity Aware Sampling (CDAS) method to select
high-quality training data based on complexity and diversity, and the Dynamic
Pack padding strategy to reduce computational resource usage by minimizing
padding tokens during training. Experimental results demonstrate that
CodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,
achieves an 8.6% performance increase on HumanEval, reduces training time by
78%, and decreases peak GPU memory usage by 27%. These findings underscore
CodeACT's ability to enhance the performance and efficiency of open-source
models. By optimizing both the data selection and training processes, CodeACT
offers a comprehensive approach to improving the capabilities of open-source
LLMs while significantly reducing computational requirements, addressing the
dual challenges of data quality and training efficiency, and paving the way for
more resource-efficient and performant models.","[{'name': 'Weijie Lv'}, {'name': 'Xuan Xia'}, {'name': 'Sheng-Jun Huang'}]",2024-08-05T02:38:48Z
http://arxiv.org/abs/2408.02152v1,http://arxiv.org/abs/2408.02152v1,Generative Retrieval with Few-shot Indexing,"Existing generative retrieval (GR) approaches rely on training-based
indexing, i.e., fine-tuning a model to memorise the associations between a
query and the document identifier (docid) of a relevant document.
Training-based indexing has three limitations: high training overhead,
under-utilization of the pre-trained knowledge of large language models (LLMs),
and challenges in adapting to a dynamic document corpus. To address the above
issues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).
It has a novel few-shot indexing process, where we prompt an LLM to generate
docids for all documents in a corpus, ultimately creating a docid bank for the
entire corpus. During retrieval, we feed a query to the same LLM and constrain
it to generate a docid within the docid bank created during indexing, and then
map the generated docid back to its corresponding document. Few-Shot GR relies
solely on prompting an LLM without requiring any training, making it more
efficient. Moreover, we devise few-shot indexing with one-to-many mapping to
further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves
superior performance to state-of-the-art GR methods that require heavy
training.","[{'name': 'Arian Askari'}, {'name': 'Chuan Meng'}, {'name': 'Mohammad Aliannejadi'}, {'name': 'Zhaochun Ren'}, {'name': 'Evangelos Kanoulas'}, {'name': 'Suzan Verberne'}]",2024-08-04T22:00:34Z
http://arxiv.org/abs/2408.02143v1,http://arxiv.org/abs/2408.02143v1,"Analyzing Cultural Representations of Emotions in LLMs through Mixed
  Emotion Survey","Large Language Models (LLMs) have gained widespread global adoption,
showcasing advanced linguistic capabilities across multiple of languages. There
is a growing interest in academia to use these models to simulate and study
human behaviors. However, it is crucial to acknowledge that an LLM's
proficiency in a specific language might not fully encapsulate the norms and
values associated with its culture. Concerns have emerged regarding potential
biases towards Anglo-centric cultures and values due to the predominance of
Western and US-based training data. This study focuses on analyzing the
cultural representations of emotions in LLMs, in the specific case of
mixed-emotion situations. Our methodology is based on the studies of Miyamoto
et al. (2010), which identified distinctive emotional indicators in Japanese
and American human responses. We first administer their mixed emotion survey to
five different LLMs and analyze their outputs. Second, we experiment with
contextual variables to explore variations in responses considering both
language and speaker origin. Thirdly, we expand our investigation to encompass
additional East Asian and Western European origin languages to gauge their
alignment with their respective cultures, anticipating a closer fit. We find
that (1) models have limited alignment with the evidence in the literature; (2)
written language has greater effect on LLMs' response than information on
participants origin; and (3) LLMs responses were found more similar for East
Asian languages than Western European languages.","[{'name': 'Shiran Dudy'}, {'name': 'Ibrahim Said Ahmad'}, {'name': 'Ryoko Kitajima'}, {'name': 'Agata Lapedriza'}]",2024-08-04T20:56:05Z
http://arxiv.org/abs/2408.02128v1,http://arxiv.org/abs/2408.02128v1,Table Transformers for Imputing Textual Attributes,"Missing data in tabular dataset is a common issue as the performance of
downstream tasks usually depends on the completeness of the training dataset.
Previous missing data imputation methods focus on numeric and categorical
columns, but we propose a novel end-to-end approach called Table Transformers
for Imputing Textual Attributes (TTITA) based on the transformer to impute
unstructured textual columns using other columns in the table. We conduct
extensive experiments on two Amazon Reviews datasets, and our approach shows
competitive performance outperforming baseline models such as recurrent neural
networks and Llama2. The performance improvement is more significant when the
target sequence has a longer length. Additionally, we incorporated multi-task
learning to simultaneously impute for heterogeneous columns, boosting the
performance for text imputation. We also qualitatively compare with ChatGPT for
realistic applications.","[{'name': 'Ting-Ruen Wei'}, {'name': 'Yuan Wang'}, {'name': 'Yoshitaka Inoue'}, {'name': 'Hsin-Tai Wu'}, {'name': 'Yi Fang'}]",2024-08-04T19:54:12Z
http://arxiv.org/abs/2408.02114v1,http://arxiv.org/abs/2408.02114v1,"Recent Advances in Multi-Choice Machine Reading Comprehension: A Survey
  on Methods and Datasets","This paper provides a thorough examination of recent developments in the
field of multi-choice Machine Reading Comprehension (MRC). Focused on benchmark
datasets, methodologies, challenges, and future trajectories, our goal is to
offer researchers a comprehensive overview of the current landscape in
multi-choice MRC. The analysis delves into 30 existing cloze-style and
multiple-choice MRC benchmark datasets, employing a refined classification
method based on attributes such as corpus style, domain, complexity, context
style, question style, and answer style. This classification system enhances
our understanding of each dataset's diverse attributes and categorizes them
based on their complexity. Furthermore, the paper categorizes recent
methodologies into Fine-tuned and Prompt-tuned methods. Fine-tuned methods
involve adapting pre-trained language models (PLMs) to a specific task through
retraining on domain-specific datasets, while prompt-tuned methods use prompts
to guide PLM response generation, presenting potential applications in
zero-shot or few-shot learning scenarios. By contributing to ongoing
discussions, inspiring future research directions, and fostering innovations,
this paper aims to propel multi-choice MRC towards new frontiers of
achievement.","[{'name': 'Shima Foolad'}, {'name': 'Kourosh Kiani'}, {'name': 'Razieh Rastgoo'}]",2024-08-04T18:57:21Z
http://arxiv.org/abs/2408.02103v1,http://arxiv.org/abs/2408.02103v1,"Effective Demonstration Annotation for In-Context Learning via Language
  Model-Based Determinantal Point Process","In-context learning (ICL) is a few-shot learning paradigm that involves
learning mappings through input-output pairs and appropriately applying them to
new instances. Despite the remarkable ICL capabilities demonstrated by Large
Language Models (LLMs), existing works are highly dependent on large-scale
labeled support sets, not always feasible in practical scenarios. To refine
this approach, we focus primarily on an innovative selective annotation
mechanism, which precedes the standard demonstration retrieval. We introduce
the Language Model-based Determinant Point Process (LM-DPP) that simultaneously
considers the uncertainty and diversity of unlabeled instances for optimal
selection. Consequently, this yields a subset for annotation that strikes a
trade-off between the two factors. We apply LM-DPP to various language models,
including GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2
Generation datasets demonstrate that LM-DPP can effectively select canonical
examples. Further analysis reveals that LLMs benefit most significantly from
subsets that are both low uncertainty and high diversity.","[{'name': 'Peng Wang'}, {'name': 'Xiaobin Wang'}, {'name': 'Chao Lou'}, {'name': 'Shengyu Mao'}, {'name': 'Pengjun Xie'}, {'name': 'Yong Jiang'}]",2024-08-04T18:08:15Z
http://arxiv.org/abs/2408.04652v1,http://arxiv.org/abs/2408.04652v1,"Leveraging Large Language Models with Chain-of-Thought and Prompt
  Engineering for Traffic Crash Severity Analysis and Inference","Harnessing the power of Large Language Models (LLMs), this study explores the
use of three state-of-the-art LLMs, specifically GPT-3.5-turbo, LLaMA3-8B, and
LLaMA3-70B, for crash severity inference, framing it as a classification task.
We generate textual narratives from original traffic crash tabular data using a
pre-built template infused with domain knowledge. Additionally, we incorporated
Chain-of-Thought (CoT) reasoning to guide the LLMs in analyzing the crash
causes and then inferring the severity. This study also examine the impact of
prompt engineering specifically designed for crash severity inference. The LLMs
were tasked with crash severity inference to: (1) evaluate the models'
capabilities in crash severity analysis, (2) assess the effectiveness of CoT
and domain-informed prompt engineering, and (3) examine the reasoning abilities
with the CoT framework. Our results showed that LLaMA3-70B consistently
outperformed the other models, particularly in zero-shot settings. The CoT and
Prompt Engineering techniques significantly enhanced performance, improving
logical reasoning and addressing alignment issues. Notably, the CoT offers
valuable insights into LLMs' reasoning processes, unleashing their capacity to
consider diverse factors such as environmental conditions, driver behavior, and
vehicle characteristics in severity analysis and inference.","[{'name': 'Hao Zhen'}, {'name': 'Yucheng Shi'}, {'name': 'Yongcan Huang'}, {'name': 'Jidong J. Yang'}, {'name': 'Ninghao Liu'}]",2024-08-04T17:14:10Z
http://arxiv.org/abs/2408.02085v3,http://arxiv.org/abs/2408.02085v3,"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data
  Assessment and Selection for Instruction Tuning of Language Models","Instruction tuning plays a critical role in aligning large language models
(LLMs) with human preference. Despite the vast amount of open instruction
datasets, naively training a LLM on all existing instructions may not be
optimal and practical. To pinpoint the most beneficial datapoints, data
assessment and selection methods have been proposed in the fields of natural
language processing (NLP) and deep learning. However, under the context of
instruction tuning, there still exists a gap in knowledge on what kind of data
evaluation metrics can be employed and how they can be integrated into the
selection mechanism. To bridge this gap, we present a comprehensive review on
existing literature of data assessment and selection especially for instruction
tuning of LLMs. We systematically categorize all applicable methods into
quality-based, diversity-based, and importance-based ones where a unified,
fine-grained taxonomy is structured. For each category, representative methods
are elaborated to describe the landscape of relevant research. In addition,
comparison between latest methods is conducted on their officially reported
results to provide in-depth discussions on their limitations. Finally, we
summarize the open challenges and propose the promosing avenues for future
studies. All related contents are available at
https://github.com/yuleiqin/fantastic-data-engineering.","[{'name': 'Yulei Qin'}, {'name': 'Yuncheng Yang'}, {'name': 'Pengcheng Guo'}, {'name': 'Gang Li'}, {'name': 'Hang Shao'}, {'name': 'Yuchen Shi'}, {'name': 'Zihan Xu'}, {'name': 'Yun Gu'}, {'name': 'Ke Li'}, {'name': 'Xing Sun'}]",2024-08-04T16:50:07Z
http://arxiv.org/abs/2408.02056v1,http://arxiv.org/abs/2408.02056v1,MedSyn: LLM-based Synthetic Medical Text Generation Framework,"Generating synthetic text addresses the challenge of data availability in
privacy-sensitive domains such as healthcare. This study explores the
applicability of synthetic data in real-world medical settings. We introduce
MedSyn, a novel medical text generation framework that integrates large
language models with a Medical Knowledge Graph (MKG). We use MKG to sample
prior medical information for the prompt and generate synthetic clinical notes
with GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data
through application in the ICD code prediction task. Our research indicates
that synthetic data can increase the classification accuracy of vital and
challenging codes by up to 17.8% compared to settings without synthetic data.
Furthermore, to provide new data for further research in the healthcare domain,
we present the largest open-source synthetic dataset of clinical notes for the
Russian language, comprising over 41k samples covering 219 ICD-10 codes.","[{'name': 'Gleb Kumichev'}, {'name': 'Pavel Blinov'}, {'name': 'Yulia Kuzkina'}, {'name': 'Vasily Goncharov'}, {'name': 'Galina Zubkova'}, {'name': 'Nikolai Zenovkin'}, {'name': 'Aleksei Goncharov'}, {'name': 'Andrey Savchenko'}]",2024-08-04T15:07:44Z
http://arxiv.org/abs/2408.02044v1,http://arxiv.org/abs/2408.02044v1,"Fine-tuning multilingual language models in Twitter/X sentiment
  analysis: a study on Eastern-European V4 languages","The aspect-based sentiment analysis (ABSA) is a standard NLP task with
numerous approaches and benchmarks, where large language models (LLM) represent
the current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data
in underrepresented languages. On such narrow tasks, small tuned language
models can often outperform universal large ones, providing available and cheap
solutions.
  We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for
classification of sentiment towards Russia and Ukraine in the context of the
ongoing military conflict. The training/testing dataset was obtained from the
academic API from Twitter/X during 2023, narrowed to the languages of the V4
countries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their
performance under a variety of settings including translations, sentiment
targets, in-context learning and more, using GPT4 as a reference model. We
document several interesting phenomena demonstrating, among others, that some
models are much better fine-tunable on multilingual Twitter tasks than others,
and that they can reach the SOTA level with a very small training set. Finally
we identify combinations of settings providing the best results.","[{'name': 'Tomáš Filip'}, {'name': 'Martin Pavlíček'}, {'name': 'Petr Sosík'}]",2024-08-04T14:35:30Z
http://arxiv.org/abs/2408.02006v1,http://arxiv.org/abs/2408.02006v1,LLaSA: Large Language and E-Commerce Shopping Assistant,"The e-commerce platform has evolved rapidly due to its widespread popularity
and convenience. Developing an e-commerce shopping assistant for customers is
crucial to aiding them in quickly finding desired products and recommending
precisely what they need. However, most previous shopping assistants face two
main problems: (1) task-specificity, which necessitates the development of
different models for various tasks, thereby increasing development costs and
limiting effectiveness; and (2) poor generalization, where the trained model
performs inadequately on up-to-date products. To resolve these issues, we
employ Large Language Models (LLMs) to construct an omnipotent assistant,
leveraging their adeptness at handling multiple tasks and their superior
generalization capability. Nonetheless, LLMs lack inherent knowledge of
e-commerce concepts. To address this, we create an instruction dataset
comprising 65,000 samples and diverse tasks, termed as EshopInstruct. Through
instruction tuning on our dataset, the assistant, named LLaSA, demonstrates the
potential to function as an omnipotent assistant. Additionally, we propose
various inference optimization strategies to enhance performance with limited
inference resources. In the Amazon KDD Cup 2024 Challenge, our proposed method,
LLaSA, achieved an overall ranking of 3rd place on ShopBench, including 57
tasks and approximately 20,000 questions, and we secured top-5 rankings in each
track, especially in track4, where we achieved the best performance result
among all student teams. Our extensive practices fully demonstrate that LLMs
possess the great potential to be competent e-commerce shopping assistants.","[{'name': 'Shuo Zhang'}, {'name': 'Boci Peng'}, {'name': 'Xinping Zhao'}, {'name': 'Boren Hu'}, {'name': 'Yun Zhu'}, {'name': 'Yanjia Zeng'}, {'name': 'Xuming Hu'}]",2024-08-04T12:10:51Z
http://arxiv.org/abs/2408.01969v1,http://arxiv.org/abs/2408.01969v1,Optimal and efficient text counterfactuals using Graph Neural Networks,"As NLP models become increasingly integral to decision-making processes, the
need for explainability and interpretability has become paramount. In this
work, we propose a framework that achieves the aforementioned by generating
semantically edited inputs, known as counterfactual interventions, which change
the model prediction, thus providing a form of counterfactual explanations for
the model. We test our framework on two NLP tasks - binary sentiment
classification and topic classification - and show that the generated edits are
contrastive, fluent and minimal, while the whole process remains significantly
faster that other state-of-the-art counterfactual editors.","[{'name': 'Dimitris Lymperopoulos'}, {'name': 'Maria Lymperaiou'}, {'name': 'Giorgos Filandrianos'}, {'name': 'Giorgos Stamou'}]",2024-08-04T09:09:13Z
http://arxiv.org/abs/2408.01966v1,http://arxiv.org/abs/2408.01966v1,"ML-EAT: A Multilevel Embedding Association Test for Interpretable and
  Transparent Social Science","This research introduces the Multilevel Embedding Association Test (ML-EAT),
a method designed for interpretable and transparent measurement of intrinsic
bias in language technologies. The ML-EAT addresses issues of ambiguity and
difficulty in interpreting the traditional EAT measurement by quantifying bias
at three levels of increasing granularity: the differential association between
two target concepts with two attribute concepts; the individual effect size of
each target concept with two attribute concepts; and the association between
each individual target concept and each individual attribute concept. Using the
ML-EAT, this research defines a taxonomy of EAT patterns describing the nine
possible outcomes of an embedding association test, each of which is associated
with a unique EAT-Map, a novel four-quadrant visualization for interpreting the
ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2
language models, and a CLIP language-and-image model shows that EAT patterns
add otherwise unobservable information about the component biases that make up
an EAT; reveal the effects of prompting in zero-shot models; and can also
identify situations when cosine similarity is an ineffective metric, rendering
an EAT unreliable. Our work contributes a method for rendering bias more
observable and interpretable, improving the transparency of computational
investigations into human minds and societies.","[{'name': 'Robert Wolfe'}, {'name': 'Alexis Hiniker'}, {'name': 'Bill Howe'}]",2024-08-04T09:04:44Z
http://arxiv.org/abs/2408.01963v1,http://arxiv.org/abs/2408.01963v1,"A Novel Metric for Measuring the Robustness of Large Language Models in
  Non-adversarial Scenarios","We evaluate the robustness of several large language models on multiple
datasets. Robustness here refers to the relative insensitivity of the model's
answers to meaning-preserving variants of their input. Benchmark datasets are
constructed by introducing naturally-occurring, non-malicious perturbations, or
by generating semantically equivalent paraphrases of input questions or
statements. We further propose a novel metric for assessing a model robustness,
and demonstrate its benefits in the non-adversarial scenario by empirical
evaluation of several models on the created datasets.","[{'name': 'Samuel Ackerman'}, {'name': 'Ella Rabinovich'}, {'name': 'Eitan Farchi'}, {'name': 'Ateret Anaby-Tavor'}]",2024-08-04T08:43:09Z
http://arxiv.org/abs/2408.01962v1,http://arxiv.org/abs/2408.01962v1,"The Implications of Open Generative Models in Human-Centered Data
  Science Work: A Case Study with Fact-Checking Organizations","Calls to use open generative language models in academic research have
highlighted the need for reproducibility and transparency in scientific
research. However, the impact of generative AI extends well beyond academia, as
corporations and public interest organizations have begun integrating these
models into their data science pipelines. We expand this lens to include the
impact of open models on organizations, focusing specifically on fact-checking
organizations, which use AI to observe and analyze large volumes of circulating
misinformation, yet must also ensure the reproducibility and impartiality of
their work. We wanted to understand where fact-checking organizations use open
models in their data science pipelines; what motivates their use of open models
or proprietary models; and how their use of open or proprietary models can
inform research on the societal impact of generative AI. To answer these
questions, we conducted an interview study with N=24 professionals at 20
fact-checking organizations on six continents. Based on these interviews, we
offer a five-component conceptual model of where fact-checking organizations
employ generative AI to support or automate parts of their data science
pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data
Delivery, and Data Sharing. We then provide taxonomies of fact-checking
organizations' motivations for using open models and the limitations that
prevent them for further adopting open models, finding that they prefer open
models for Organizational Autonomy, Data Privacy and Ownership, Application
Specificity, and Capability Transparency. However, they nonetheless use
proprietary models due to perceived advantages in Performance, Usability, and
Safety, as well as Opportunity Costs related to participation in emerging
generative AI ecosystems. Our work provides novel perspective on open models in
data-driven organizations.","[{'name': 'Robert Wolfe'}, {'name': 'Tanushree Mitra'}]",2024-08-04T08:41:48Z
http://arxiv.org/abs/2408.01961v1,http://arxiv.org/abs/2408.01961v1,"Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study","Popular and news media often portray teenagers with sensationalism, as both a
risk to society and at risk from society. As AI begins to absorb some of the
epistemic functions of traditional media, we study how teenagers in two
countries speaking two languages: 1) are depicted by AI, and 2) how they would
prefer to be depicted. Specifically, we study the biases about teenagers
learned by static word embeddings (SWEs) and generative language models (GLMs),
comparing these with the perspectives of adolescents living in the U.S. and
Nepal. We find English-language SWEs associate teenagers with societal
problems, and more than 50% of the 1,000 words most associated with teenagers
in the pretrained GloVe SWE reflect such problems. Given prompts about
teenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss
societal problems, most commonly violence, but also drug use, mental illness,
and sexual taboo. Nepali models, while not free of such associations, are less
dominated by social problems. Data from workshops with N=13 U.S. adolescents
and N=18 Nepalese adolescents show that AI presentations are disconnected from
teenage life, which revolves around activities like school and friendship.
Participant ratings of how well 20 trait words describe teens are decorrelated
from SWE associations, with Pearson's r=.02, n.s. in English FastText and
r=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in
GloVe. U.S. participants suggested AI could fairly present teens by
highlighting diversity, while Nepalese participants centered positivity.
Participants were optimistic that, if it learned from adolescents, rather than
media sources, AI could help mitigate stereotypes. Our work offers an
understanding of the ways SWEs and GLMs misrepresent a developmentally
vulnerable group and provides a template for less sensationalized
characterization.","[{'name': 'Robert Wolfe'}, {'name': 'Aayushi Dangol'}, {'name': 'Bill Howe'}, {'name': 'Alexis Hiniker'}]",2024-08-04T08:35:02Z
http://arxiv.org/abs/2408.01959v1,http://arxiv.org/abs/2408.01959v1,"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in
  Vision-Language AI","Multimodal AI models capable of associating images and text hold promise for
numerous domains, ranging from automated image captioning to accessibility
applications for blind and low-vision users. However, uncertainty about bias
has in some cases limited their adoption and availability. In the present work,
we study 43 CLIP vision-language models to determine whether they learn
human-like facial impression biases, and we find evidence that such biases are
reflected across three distinct CLIP model families. We show for the first time
that the the degree to which a bias is shared across a society predicts the
degree to which it is reflected in a CLIP model. Human-like impressions of
visually unobservable attributes, like trustworthiness and sexuality, emerge
only in models trained on the largest dataset, indicating that a better fit to
uncurated cultural data results in the reproduction of increasingly subtle
social biases. Moreover, we use a hierarchical clustering approach to show that
dataset size predicts the extent to which the underlying structure of facial
impression bias resembles that of facial impression bias in humans. Finally, we
show that Stable Diffusion models employing CLIP as a text encoder learn facial
impression biases, and that these biases intersect with racial biases in Stable
Diffusion XL-Turbo. While pretrained CLIP models may prove useful for
scientific studies of bias, they will also require significant dataset curation
when intended for use as general-purpose models in a zero-shot setting.","[{'name': 'Robert Wolfe'}, {'name': 'Aayushi Dangol'}, {'name': 'Alexis Hiniker'}, {'name': 'Bill Howe'}]",2024-08-04T08:26:58Z
http://arxiv.org/abs/2408.01950v1,http://arxiv.org/abs/2408.01950v1,"Why Perturbing Symbolic Music is Necessary: Fitting the Distribution of
  Never-used Notes through a Joint Probabilistic Diffusion Model","Existing music generation models are mostly language-based, neglecting the
frequency continuity property of notes, resulting in inadequate fitting of rare
or never-used notes and thus reducing the diversity of generated samples. We
argue that the distribution of notes can be modeled by translational invariance
and periodicity, especially using diffusion models to generalize notes by
injecting frequency-domain Gaussian noise. However, due to the low-density
nature of music symbols, estimating the distribution of notes latent in the
high-density solution space poses significant challenges. To address this
problem, we introduce the Music-Diff architecture, which fits a joint
distribution of notes and accompanying semantic information to generate
symbolic music conditionally. We first enhance the fragmentation module for
extracting semantics by using event-based notations and the structural
similarity index, thereby preventing boundary blurring. As a prerequisite for
multivariate perturbation, we introduce a joint pre-training method to
construct the progressions between notes and musical semantics while avoiding
direct modeling of low-density notes. Finally, we recover the perturbed notes
by a multi-branch denoiser that fits multiple noise objectives via Pareto
optimization. Our experiments suggest that in contrast to language models,
joint probability diffusion models perturbing at both note and semantic levels
can provide more sample diversity and compositional regularity. The case study
highlights the rhythmic advantages of our model over language- and DDPMs-based
models by analyzing the hierarchical structure expressed in the self-similarity
metrics.","[{'name': 'Shipei Liu'}, {'name': 'Xiaoya Fan'}, {'name': 'Guowei Wu'}]",2024-08-04T07:38:38Z
http://arxiv.org/abs/2408.01935v1,http://arxiv.org/abs/2408.01935v1,"Defining and Evaluating Decision and Composite Risk in Language Models
  Applied to Natural Language Inference","Despite their impressive performance, large language models (LLMs) such as
ChatGPT are known to pose important risks. One such set of risks arises from
misplaced confidence, whether over-confidence or under-confidence, that the
models have in their inference. While the former is well studied, the latter is
not, leading to an asymmetry in understanding the comprehensive risk of the
model based on misplaced confidence. In this paper, we address this asymmetry
by defining two types of risk (decision and composite risk), and proposing an
experimental framework consisting of a two-level inference architecture and
appropriate metrics for measuring such risks in both discriminative and
generative LLMs. The first level relies on a decision rule that determines
whether the underlying language model should abstain from inference. The second
level (which applies if the model does not abstain) is the model's inference.
Detailed experiments on four natural language commonsense reasoning datasets
using both an open-source ensemble-based RoBERTa model and ChatGPT, demonstrate
the practical utility of the evaluation framework. For example, our results
show that our framework can get an LLM to confidently respond to an extra 20.1%
of low-risk inference tasks that other methods might misclassify as high-risk,
and skip 19.8% of high-risk tasks, which would have been answered incorrectly.","[{'name': 'Ke Shen'}, {'name': 'Mayank Kejriwal'}]",2024-08-04T05:24:32Z
http://arxiv.org/abs/2408.01933v2,http://arxiv.org/abs/2408.01933v2,"DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language
  Models","Large language models (LLMs) have recently showcased remarkable capabilities,
spanning a wide range of tasks and applications, including those in the medical
domain. Models like GPT-4 excel in medical question answering but may face
challenges in the lack of interpretability when handling complex tasks in real
clinical settings. We thus introduce the diagnostic reasoning dataset for
clinical notes (DiReCT), aiming at evaluating the reasoning ability and
interpretability of LLMs compared to human doctors. It contains 511 clinical
notes, each meticulously annotated by physicians, detailing the diagnostic
reasoning process from observations in a clinical note to the final diagnosis.
Additionally, a diagnostic knowledge graph is provided to offer essential
knowledge for reasoning, which may not be covered in the training data of
existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant
gap between their reasoning ability and that of human doctors, highlighting the
critical need for models that can reason effectively in real-world clinical
scenarios.","[{'name': 'Bowen Wang'}, {'name': 'Jiuyang Chang'}, {'name': 'Yiming Qian'}, {'name': 'Guoxin Chen'}, {'name': 'Junhao Chen'}, {'name': 'Zhouqiang Jiang'}, {'name': 'Jiahao Zhang'}, {'name': 'Yuta Nakashima'}, {'name': 'Hajime Nagahara'}]",2024-08-04T05:15:02Z
http://arxiv.org/abs/2408.01928v1,http://arxiv.org/abs/2408.01928v1,"A Semi-supervised Multi-channel Graph Convolutional Network for Query
  Classification in E-commerce","Query intent classification is an essential module for customers to find
desired products on the e-commerce application quickly. Most existing query
intent classification methods rely on the users' click behavior as a supervised
signal to construct training samples. However, these methods based entirely on
posterior labels may lead to serious category imbalance problems because of the
Matthew effect in click samples. Compared with popular categories, it is
difficult for products under long-tail categories to obtain traffic and user
clicks, which makes the models unable to detect users' intent for products
under long-tail categories. This in turn aggravates the problem that long-tail
categories cannot obtain traffic, forming a vicious circle. In addition, due to
the randomness of the user's click, the posterior label is unstable for the
query with similar semantics, which makes the model very sensitive to the
input, leading to an unstable and incomplete recall of categories.
  In this paper, we propose a novel Semi-supervised Multi-channel Graph
Convolutional Network (SMGCN) to address the above problems from the
perspective of label association and semi-supervised learning. SMGCN extends
category information and enhances the posterior label by utilizing the
similarity score between the query and categories. Furthermore, it leverages
the co-occurrence and semantic similarity graph of categories to strengthen the
relations among labels and weaken the influence of posterior label instability.
We conduct extensive offline and online A/B experiments, and the experimental
results show that SMGCN significantly outperforms the strong baselines, which
shows its effectiveness and practicality.","[{'name': 'Chunyuan Yuan'}, {'name': 'Ming Pang'}, {'name': 'Zheng Fang'}, {'name': 'Xue Jiang'}, {'name': 'Changping Peng'}, {'name': 'Zhangang Lin'}]",2024-08-04T04:52:21Z
http://arxiv.org/abs/2408.04651v1,http://arxiv.org/abs/2408.04651v1,"Knowledge AI: Fine-tuning NLP Models for Facilitating Scientific
  Knowledge Extraction and Understanding","This project investigates the efficacy of Large Language Models (LLMs) in
understanding and extracting scientific knowledge across specific domains and
to create a deep learning framework: Knowledge AI. As a part of this framework,
we employ pre-trained models and fine-tune them on datasets in the scientific
domain. The models are adapted for four key Natural Language Processing (NLP)
tasks: summarization, text generation, question answering, and named entity
recognition. Our results indicate that domain-specific fine-tuning
significantly enhances model performance in each of these tasks, thereby
improving their applicability for scientific contexts. This adaptation enables
non-experts to efficiently query and extract information within targeted
scientific fields, demonstrating the potential of fine-tuned LLMs as a tool for
knowledge discovery in the sciences.","[{'name': 'Balaji Muralidharan'}, {'name': 'Hayden Beadles'}, {'name': 'Reza Marzban'}, {'name': 'Kalyan Sashank Mupparaju'}]",2024-08-04T01:32:09Z
http://arxiv.org/abs/2408.01890v1,http://arxiv.org/abs/2408.01890v1,Cross-layer Attention Sharing for Large Language Models,"As large language models (LLMs) evolve, the increase in model depth and
parameter number leads to substantial redundancy. To enhance the efficiency of
the attention mechanism, previous works primarily compress the KV cache or
group attention heads, while largely overlooking redundancy between layers. Our
comprehensive analyses across various LLMs show that highly similar attention
patterns persist within most layers. It's intuitive to save the computation by
sharing attention weights across layers. However, further analysis reveals two
challenges: (1) Directly sharing the weight matrix without carefully
rearranging the attention heads proves to be ineffective; (2) Shallow layers
are vulnerable to small deviations in attention weights. Driven by these
insights, we introduce LiSA, a lightweight substitute for self-attention in
well-trained LLMs. LiSA employs tiny feed-forward networks to align attention
heads between adjacent layers and low-rank matrices to approximate differences
in layer-wise attention weights. Evaluations encompassing 13 typical benchmarks
demonstrate that LiSA maintains high response quality in terms of accuracy and
perplexity while reducing redundant attention calculations within 53-84% of the
total layers. Our implementations of LiSA achieve a 6X compression of Q and K,
with maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for
LLaMA2-7B.","[{'name': 'Yongyu Mu'}, {'name': 'Yuzhang Wu'}, {'name': 'Yuchun Fan'}, {'name': 'Chenglong Wang'}, {'name': 'Hengyu Li'}, {'name': 'Qiaozhi He'}, {'name': 'Murun Yang'}, {'name': 'Tong Xiao'}, {'name': 'Jingbo Zhu'}]",2024-08-04T00:38:34Z
http://arxiv.org/abs/2408.01875v1,http://arxiv.org/abs/2408.01875v1,Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval,"Recent advances in large language models (LLMs) have enabled autonomous
agents with complex reasoning and task-fulfillment capabilities using a wide
range of tools. However, effectively identifying the most relevant tools for a
given task becomes a key bottleneck as the toolset size grows, hindering
reliable tool utilization. To address this, we introduce Re-Invoke, an
unsupervised tool retrieval method designed to scale effectively to large
toolsets without training. Specifically, we first generate a diverse set of
synthetic queries that comprehensively cover different aspects of the query
space associated with each tool document during the tool indexing phase.
Second, we leverage LLM's query understanding capabilities to extract key
tool-related context and underlying intents from user queries during the
inference phase. Finally, we employ a novel multi-view similarity ranking
strategy based on intents to pinpoint the most relevant tools for each query.
Our evaluation demonstrates that Re-Invoke significantly outperforms
state-of-the-art alternatives in both single-tool and multi-tool scenarios, all
within a fully unsupervised setting. Notably, on the ToolE datasets, we achieve
a 20% relative improvement in nDCG@5 for single-tool retrieval and a 39%
improvement for multi-tool retrieval.","[{'name': 'Yanfei Chen'}, {'name': 'Jinsung Yoon'}, {'name': 'Devendra Singh Sachan'}, {'name': 'Qingze Wang'}, {'name': 'Vincent Cohen-Addad'}, {'name': 'Mohammadhossein Bateni'}, {'name': 'Chen-Yu Lee'}, {'name': 'Tomas Pfister'}]",2024-08-03T22:49:27Z
http://arxiv.org/abs/2408.01869v1,http://arxiv.org/abs/2408.01869v1,"MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented
  Generation for Pharmacovigilance","In the era of Large Language Models (LLMs), given their remarkable text
understanding and generation abilities, there is an unprecedented opportunity
to develop new, LLM-based methods for trustworthy medical knowledge synthesis,
extraction and summarization. This paper focuses on the problem of
Pharmacovigilance (PhV), where the significance and challenges lie in
identifying Adverse Drug Events (ADEs) from diverse text sources, such as
medical literature, clinical notes, and drug labels. Unfortunately, this task
is hindered by factors including variations in the terminologies of drugs and
outcomes, and ADE descriptions often being buried in large amounts of narrative
text. We present MALADE, the first effective collaborative multi-agent system
powered by LLM with Retrieval Augmented Generation for ADE extraction from drug
label data. This technique involves augmenting a query to an LLM with relevant
information extracted from text resources, and instructing the LLM to compose a
response consistent with the augmented data. MALADE is a general LLM-agnostic
architecture, and its unique capabilities are: (1) leveraging a variety of
external sources, such as medical literature, drug labels, and FDA tools (e.g.,
OpenFDA drug information API), (2) extracting drug-outcome association in a
structured format along with the strength of the association, and (3) providing
explanations for established associations. Instantiated with GPT-4 Turbo or
GPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area
Under ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our
implementation leverages the Langroid multi-agent LLM framework and can be
found at https://github.com/jihyechoi77/malade.","[{'name': 'Jihye Choi'}, {'name': 'Nils Palumbo'}, {'name': 'Prasad Chalasani'}, {'name': 'Matthew M. Engelhard'}, {'name': 'Somesh Jha'}, {'name': 'Anivarya Kumar'}, {'name': 'David Page'}]",2024-08-03T22:14:13Z
http://arxiv.org/abs/2408.01866v1,http://arxiv.org/abs/2408.01866v1,"Efficient Solutions For An Intriguing Failure of LLMs: Long Context
  Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly","Large Language Models (LLMs) have demonstrated remarkable capabilities in
comprehending and analyzing lengthy sequential inputs, owing to their extensive
context windows that allow processing millions of tokens in a single forward
pass. However, this paper uncovers a surprising limitation: LLMs fall short
when handling long input sequences. We investigate this issue using three
datasets and two tasks (sentiment analysis and news categorization) across
various LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,
and Mistral Instruct models. To address this limitation, we propose and
evaluate ad-hoc solutions that substantially enhance LLMs' performance on long
input sequences by up to 50%, while reducing API cost and latency by up to 93%
and 50%, respectively.","[{'name': 'Peyman Hosseini'}, {'name': 'Ignacio Castro'}, {'name': 'Iacopo Ghinassi'}, {'name': 'Matthew Purver'}]",2024-08-03T21:31:34Z
http://arxiv.org/abs/2408.04650v1,http://arxiv.org/abs/2408.04650v1,"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based
  Evaluation Tools","Objective: This study aims to develop and validate an evaluation framework to
ensure the safety and reliability of mental health chatbots, which are
increasingly popular due to their accessibility, human-like interactions, and
context-aware support. Materials and Methods: We created an evaluation
framework with 100 benchmark questions and ideal responses, and five guideline
questions for chatbot responses. This framework, validated by mental health
experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation
methods explored included large language model (LLM)-based scoring, an agentic
approach using real-time data, and embedding models to compare chatbot
responses against ground truth standards. Results: The results highlight the
importance of guidelines and ground truth for improving LLM evaluation
accuracy. The agentic method, dynamically accessing reliable information,
demonstrated the best alignment with human assessments. Adherence to a
standardized, expert-validated framework significantly enhanced chatbot
response safety and reliability. Discussion: Our findings emphasize the need
for comprehensive, expert-tailored safety evaluation metrics for mental health
chatbots. While LLMs have significant potential, careful implementation is
necessary to mitigate risks. The superior performance of the agentic approach
underscores the importance of real-time data access in enhancing chatbot
reliability. Conclusion: The study validated an evaluation framework for mental
health chatbots, proving its effectiveness in improving safety and reliability.
Future work should extend evaluations to accuracy, bias, empathy, and privacy
to ensure holistic assessment and responsible integration into healthcare.
Standardized evaluations will build trust among users and professionals,
facilitating broader adoption and improved mental health support through
technology.","[{'name': 'Jung In Park'}, {'name': 'Mahyar Abbasian'}, {'name': 'Iman Azimi'}, {'name': 'Dawn Bounds'}, {'name': 'Angela Jun'}, {'name': 'Jaesu Han'}, {'name': 'Robert McCarron'}, {'name': 'Jessica Borelli'}, {'name': 'Jia Li'}, {'name': 'Mona Mahmoudi'}, {'name': 'Carmen Wiedenhoeft'}, {'name': 'Amir Rahmani'}]",2024-08-03T19:57:49Z
http://arxiv.org/abs/2408.01852v2,http://arxiv.org/abs/2408.01852v2,Sólo Escúchame: Spanish Emotional Accompaniment Chatbot,"According to the World Health Organization (WHO), suicide was the fourth
leading cause of death in the world for individuals aged 15 to 29 in 2019.
Given the rapid increase in mental health issues, providing psychological
support is both crucial and urgent. In this paper: (1) we propose S\'olo
Esc\'uchame, the first open-source Spanish emotional assistance chatbot, based
on LLaMA-2-7b-Chat. (2) We introduced the HEAR (Hispanic Emotional
Accompaniment Responses) dataset, compiled from multiple English sources
translated into Spanish, as well as generic data generated using
ChatGPT-3.5-Turbo. Finally, (3) we propose an evaluation metric based on two
semi-automatic assessment methods. Our system outperforms a range of
state-of-the-art models in providing psychological assistance in Spanish. Our
models and datasets are publicly available to facilitate reproducibility.","[{'name': 'Bruno Gil Ramírez'}, {'name': 'Jessica López Espejel'}, {'name': 'María del Carmen Santiago Díaz'}, {'name': 'Gustavo Trinidad Rubín Linares'}]",2024-08-03T19:33:33Z
http://arxiv.org/abs/2408.01838v1,http://arxiv.org/abs/2408.01838v1,"Tracking Emotional Dynamics in Chat Conversations: A Hybrid Approach
  using DistilBERT and Emoji Sentiment Analysis","Computer-mediated communication has become more important than face-to-face
communication in many contexts. Tracking emotional dynamics in chat
conversations can enhance communication, improve services, and support
well-being in various contexts. This paper explores a hybrid approach to
tracking emotional dynamics in chat conversations by combining DistilBERT-based
text emotion detection and emoji sentiment analysis. A Twitter dataset was
analyzed using various machine learning algorithms, including SVM, Random
Forest, and AdaBoost. We contrasted their performance with DistilBERT. Results
reveal DistilBERT's superior performance in emotion recognition. Our approach
accounts for emotive expressions conveyed through emojis to better understand
participants' emotions during chats. We demonstrate how this approach can
effectively capture and analyze emotional shifts in real-time conversations.
Our findings show that integrating text and emoji analysis is an effective way
of tracking chat emotion, with possible applications in customer service, work
chats, and social media interactions.","[{'name': 'Ayan Igali'}, {'name': 'Abdulkhak Abdrakhman'}, {'name': 'Yerdaut Torekhan'}, {'name': 'Pakizar Shamoi'}]",2024-08-03T18:28:31Z
http://arxiv.org/abs/2408.04649v1,http://arxiv.org/abs/2408.04649v1,Chain of Stance: Stance Detection with Large Language Models,"Stance detection is an active task in natural language processing (NLP) that
aims to identify the author's stance towards a particular target within a text.
Given the remarkable language understanding capabilities and encyclopedic prior
knowledge of large language models (LLMs), how to explore the potential of LLMs
in stance detection has received significant attention. Unlike existing
LLM-based approaches that focus solely on fine-tuning with large-scale
datasets, we propose a new prompting method, called \textit{Chain of Stance}
(CoS). In particular, it positions LLMs as expert stance detectors by
decomposing the stance detection process into a series of intermediate,
stance-related assertions that culminate in the final judgment. This approach
leads to significant improvements in classification performance. We conducted
extensive experiments using four SOTA LLMs on the SemEval 2016 dataset,
covering the zero-shot and few-shot learning setups. The results indicate that
the proposed method achieves state-of-the-art results with an F1 score of 79.84
in the few-shot setting.","[{'name': 'Junxia Ma'}, {'name': 'Changjiang Wang'}, {'name': 'Hanwen Xing'}, {'name': 'Dongming Zhao'}, {'name': 'Yazhou Zhang'}]",2024-08-03T16:30:51Z
http://arxiv.org/abs/2408.01803v1,http://arxiv.org/abs/2408.01803v1,STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs,"In this paper, we present STBLLM, the first structural binarization framework
for compressing Large Language Models (LLMs) to less than 1-bit precision. LLMs
have achieved remarkable performance, but their heavy memory requirements have
hindered widespread adoption, particularly on resource-constrained devices.
Binarization, which quantifies weights to a mere 1-bit, achieves a milestone in
increasing computational efficiency. However, we observe that some weights in
binarized LLMs can be randomly flipped without significant performance
degradation, indicating the potential for further compression. To exploit this,
our STBLLM employs an N:M sparsity to perform structural binarization of the
weights. First, we introduce a new Standardized Importance (SI) metric that
considers weight magnitude and input feature norm to better evaluate weight
significance. Then, we propose a layer-wise approach where different layers of
the LLM can be sparsified with varying N:M ratios, balancing compression and
accuracy. Finally, we use residual approximation with double binarization to
preserve information for salient weights. In addition, we utilize a
fine-grained grouping strategy for less important weights that applies
different quantization schemes to sparse, intermediate, and dense regions. We
conduct extensive experiments on various language models, including the
LLaMA-1/2/3, OPT family, and Mistral, to evaluate the effectiveness of STBLLM.
The results demonstrate that our approach performs better than other compressed
binarization LLM methods while significantly reducing memory requirements.","[{'name': 'Peijie Dong'}, {'name': 'Lujun Li'}, {'name': 'Dayou Du'}, {'name': 'Yuhan Chen'}, {'name': 'Zhenheng Tang'}, {'name': 'Qiang Wang'}, {'name': 'Wei Xue'}, {'name': 'Wenhan Luo'}, {'name': 'Qifeng Liu'}, {'name': 'Yike Guo'}, {'name': 'Xiaowen Chu'}]",2024-08-03T15:07:44Z
http://arxiv.org/abs/2408.01779v1,http://arxiv.org/abs/2408.01779v1,"MathLearner: A Large Language Model Agent Framework for Learning to
  Solve Mathematical Problems","With the development of artificial intelligence (AI), large language models
(LLM) are widely used in many fields. However, the reasoning ability of LLM is
still very limited when it comes to mathematical reasoning. Mathematics plays
an important role in all aspects of human society and is a technical guarantee
in the fields of healthcare, transport and aerospace, for this reason, the
development of AI big language models in the field of mathematics has great
potential significance. To improve the mathematical reasoning ability of large
language models, we proposed an agent framework for learning to solve
mathematical problems based on inductive reasoning. By emulating the human
learning process of generalization of learned information and effective
application of previous knowledge in new reasoning tasks, this framework has
great performance in the mathematical reasoning process. It improves global
accuracy over the baseline method (chain-of-thought) by 20.96% and solves
17.54% of the mathematical problems that the baseline cannot solve. Benefiting
from the efficient RETRIEVAL method, our model improves the ability of large
language models to efficiently use external knowledge, i.e., the mathematical
computation of the model can be based on written procedures. In education, our
model can be used as a personalised learning aid, thus reducing the inequality
of educational resources.","[{'name': 'Wenbei Xie'}, {'name': 'Donglin Liu'}, {'name': 'Haoran Yan'}, {'name': 'Wenjie Wu'}, {'name': 'Zongyang Liu'}]",2024-08-03T13:28:19Z
http://arxiv.org/abs/2408.04648v1,http://arxiv.org/abs/2408.04648v1,"PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large
  Language Models","We present PLUGH (https://www.urbandictionary.com/define.php?term=plugh), a
modern benchmark that currently consists of 5 tasks, each with 125 input texts
extracted from 48 different games and representing 61 different
(non-isomorphic) spatial graphs to assess the abilities of Large Language
Models (LLMs) for spatial understanding and reasoning. Our evaluation of
API-based and open-sourced LLMs shows that while some commercial LLMs exhibit
strong reasoning abilities, open-sourced competitors can demonstrate almost the
same level of quality; however, all models still have significant room for
improvement. We identify typical reasons for LLM failures and discuss possible
ways to deal with them. Datasets and evaluation code are released
(https://github.com/altsoph/PLUGH).",[{'name': 'Alexey Tikhonov'}],2024-08-03T13:21:08Z
http://arxiv.org/abs/2408.04647v1,http://arxiv.org/abs/2408.04647v1,Distinguishing Chatbot from Human,"There have been many recent advances in the fields of generative Artificial
Intelligence (AI) and Large Language Models (LLM), with the Generative
Pre-trained Transformer (GPT) model being a leading ""chatbot."" LLM-based
chatbots have become so powerful that it may seem difficult to differentiate
between human-written and machine-generated text. To analyze this problem, we
have developed a new dataset consisting of more than 750,000 human-written
paragraphs, with a corresponding chatbot-generated paragraph for each. Based on
this dataset, we apply Machine Learning (ML) techniques to determine the origin
of text (human or chatbot). Specifically, we consider two methodologies for
tackling this issue: feature analysis and embeddings. Our feature analysis
approach involves extracting a collection of features from the text for
classification. We also explore the use of contextual embeddings and
transformer-based architectures to train classification models. Our proposed
solutions offer high classification accuracy and serve as useful tools for
textual analysis, resulting in a better understanding of chatbot-generated text
in this era of advanced AI technology.","[{'name': 'Gauri Anil Godghase'}, {'name': 'Rishit Agrawal'}, {'name': 'Tanush Obili'}, {'name': 'Mark Stamp'}]",2024-08-03T13:18:04Z
http://arxiv.org/abs/2408.01748v1,http://arxiv.org/abs/2408.01748v1,Discovery of Rare Causal Knowledge from Financial Statement Summaries,"What would happen if temperatures were subdued and result in a cool summer?
One can easily imagine that air conditioner, ice cream or beer sales would be
suppressed as a result of this. Less obvious is that agricultural shipments
might be delayed, or that sound proofing material sales might decrease. The
ability to extract such causal knowledge is important, but it is also important
to distinguish between cause-effect pairs that are known and those that are
likely to be unknown, or rare. Therefore, in this paper, we propose a method
for extracting rare causal knowledge from Japanese financial statement
summaries produced by companies. Our method consists of three steps. First, it
extracts sentences that include causal knowledge from the summaries using a
machine learning method based on an extended language ontology. Second, it
obtains causal knowledge from the extracted sentences using syntactic patterns.
Finally, it extracts the rarest causal knowledge from the knowledge it has
obtained.","[{'name': 'Hiroki Sakaji'}, {'name': 'Jason Bennett'}, {'name': 'Risa Murono'}, {'name': 'Kiyoshi Izumi'}, {'name': 'Hiroyuki Sakai'}]",2024-08-03T11:08:53Z
http://arxiv.org/abs/2408.01745v1,http://arxiv.org/abs/2408.01745v1,"Indexing and Visualization of Climate Change Narratives Using BERT and
  Causal Extraction","In this study, we propose a methodology to extract, index, and visualize
``climate change narratives'' (stories about the connection between causal and
consequential events related to climate change). We use two natural language
processing methods, BERT (Bidirectional Encoder Representations from
Transformers) and causal extraction, to textually analyze newspaper articles on
climate change to extract ``climate change narratives.'' The novelty of the
methodology could extract and quantify the causal relationships assumed by the
newspaper's writers. Looking at the extracted climate change narratives over
time, we find that since 2018, an increasing number of narratives suggest the
impact of the development of climate change policy discussion and the
implementation of climate change-related policies on corporate behaviors,
macroeconomics, and price dynamics. We also observed the recent emergence of
narratives focusing on the linkages between climate change-related policies and
monetary policy. Furthermore, there is a growing awareness of the negative
impacts of natural disasters (e.g., abnormal weather and severe floods) related
to climate change on economic activities, and this issue might be perceived as
a new challenge for companies and governments. The methodology of this study is
expected to be applied to a wide range of fields, as it can analyze causal
relationships among various economic topics, including analysis of inflation
expectation or monetary policy communication strategy.","[{'name': 'Hiroki Sakaji'}, {'name': 'Noriyasu Kaneda'}]",2024-08-03T11:05:41Z
http://arxiv.org/abs/2408.01744v1,http://arxiv.org/abs/2408.01744v1,Summarization of Investment Reports Using Pre-trained Model,"In this paper, we attempt to summarize monthly reports as investment reports.
Fund managers have a wide range of tasks, one of which is the preparation of
investment reports. In addition to preparing monthly reports on fund
management, fund managers prepare management reports that summarize these
monthly reports every six months or once a year. The preparation of fund
reports is a labor-intensive and time-consuming task. Therefore, in this paper,
we tackle investment summarization from monthly reports using transformer-based
models. There are two main types of summarization methods: extractive
summarization and abstractive summarization, and this study constructs both
methods and examines which is more useful in summarizing investment reports.","[{'name': 'Hiroki Sakaji'}, {'name': 'Ryotaro Kobayashi'}, {'name': 'Kiyoshi Izumi'}, {'name': 'Hiroyuki Mitsugi'}, {'name': 'Wataru Kuramoto'}]",2024-08-03T11:04:04Z
http://arxiv.org/abs/2408.01700v1,http://arxiv.org/abs/2408.01700v1,"Integrating Large Language Models and Knowledge Graphs for Extraction
  and Validation of Textual Test Data","Aerospace manufacturing companies, such as Thales Alenia Space, design,
develop, integrate, verify, and validate products characterized by high
complexity and low volume. They carefully document all phases for each product
but analyses across products are challenging due to the heterogeneity and
unstructured nature of the data in documents. In this paper, we propose a
hybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with
Large Language Models (LLMs) to extract and validate data contained in these
documents. We consider a case study focused on test data related to electronic
boards for satellites. To do so, we extend the Semantic Sensor Network
ontology. We store the metadata of the reports in a KG, while the actual test
results are stored in parquet accessible via a Virtual Knowledge Graph. The
validation process is managed using an LLM-based approach. We also conduct a
benchmarking study to evaluate the performance of state-of-the-art LLMs in
executing this task. Finally, we analyze the costs and benefits of automating
preexisting processes of manual data extraction and validation for subsequent
cross-report analyses.","[{'name': 'Antonio De Santis'}, {'name': 'Marco Balduini'}, {'name': 'Federico De Santis'}, {'name': 'Andrea Proia'}, {'name': 'Arsenio Leo'}, {'name': 'Marco Brambilla'}, {'name': 'Emanuele Della Valle'}]",2024-08-03T07:42:53Z
http://arxiv.org/abs/2408.01682v1,http://arxiv.org/abs/2408.01682v1,"Multi-Frame Vision-Language Model for Long-form Reasoning in Driver
  Behavior Analysis","Identifying risky driving behavior in real-world situations is essential for
the safety of both drivers and pedestrians. However, integrating natural
language models in this field remains relatively untapped. To address this, we
created a novel multi-modal instruction tuning dataset and driver coaching
inference system. Our primary use case is dashcam-based coaching for commercial
drivers. The North American Dashcam Market is expected to register a CAGR of
15.4 percent from 2022 to 2027. Our dataset enables language models to learn
visual instructions across various risky driving scenarios, emphasizing
detailed reasoning crucial for effective driver coaching and managerial
comprehension. Our model is trained on road-facing and driver-facing RGB camera
footage, capturing the comprehensive scope of driving behavior in vehicles
equipped with dashcams.","[{'name': 'Hiroshi Takato'}, {'name': 'Hiroshi Tsutsui'}, {'name': 'Komei Soda'}, {'name': 'Hidetaka Kamigaito'}]",2024-08-03T06:40:00Z
http://arxiv.org/abs/2408.01679v1,http://arxiv.org/abs/2408.01679v1,"MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal
  Knowledge Graph","Multi-modal knowledge graphs have emerged as a powerful approach for
information representation, combining data from different modalities such as
text, images, and videos. While several such graphs have been constructed and
have played important roles in applications like visual question answering and
recommendation systems, challenges persist in their development. These include
the scarcity of high-quality Chinese knowledge graphs and limited domain
coverage in existing multi-modal knowledge graphs. This paper introduces
MMPKUBase, a robust and extensive Chinese multi-modal knowledge graph that
covers diverse domains, including birds, mammals, ferns, and more, comprising
over 50,000 entities and over 1 million filtered images. To ensure data
quality, we employ Prototypical Contrastive Learning and the Isolation Forest
algorithm to refine the image data. Additionally, we have developed a
user-friendly platform to facilitate image attribute exploration.","[{'name': 'Xuan Yi'}, {'name': 'Yanzeng Li'}, {'name': 'Lei Zou'}]",2024-08-03T06:35:54Z
http://arxiv.org/abs/2408.01638v1,http://arxiv.org/abs/2408.01638v1,"Transforming Slot Schema Induction with Generative Dialogue State
  Inference","The challenge of defining a slot schema to represent the state of a
task-oriented dialogue system is addressed by Slot Schema Induction (SSI),
which aims to automatically induce slots from unlabeled dialogue data. Whereas
previous approaches induce slots by clustering value spans extracted directly
from the dialogue text, we demonstrate the power of discovering slots using a
generative approach. By training a model to generate slot names and values that
summarize key dialogue information with no prior task knowledge, our SSI method
discovers high-quality candidate information for representing dialogue state.
These discovered slot-value candidates can be easily clustered into unified
slot schemas that align well with human-authored schemas. Experimental
comparisons on the MultiWOZ and SGD datasets demonstrate that Generative
Dialogue State Inference (GenDSI) outperforms the previous state-of-the-art on
multiple aspects of the SSI task.","[{'name': 'James D. Finch'}, {'name': 'Boxin Zhao'}, {'name': 'Jinho D. Choi'}]",2024-08-03T02:41:10Z
http://arxiv.org/abs/2408.01633v1,http://arxiv.org/abs/2408.01633v1,Self-Emotion Blended Dialogue Generation in Social Simulation Agents,"When engaging in conversations, dialogue agents in a virtual simulation
environment may exhibit their own emotional states that are unrelated to the
immediate conversational context, a phenomenon known as self-emotion. This
study explores how such self-emotion affects the agents' behaviors in dialogue
strategies and decision-making within a large language model (LLM)-driven
simulation framework. In a dialogue strategy prediction experiment, we analyze
the dialogue strategy choices employed by agents both with and without
self-emotion, comparing them to those of humans. The results show that
incorporating self-emotion helps agents exhibit more human-like dialogue
strategies. In an independent experiment comparing the performance of models
fine-tuned on GPT-4 generated dialogue datasets, we demonstrate that
self-emotion can lead to better overall naturalness and humanness. Finally, in
a virtual simulation environment where agents have discussions on multiple
topics, we show that self-emotion of agents can significantly influence the
decision-making process of the agents, leading to approximately a 50% change in
decisions.","[{'name': 'Qiang Zhang'}, {'name': 'Jason Naradowsky'}, {'name': 'Yusuke Miyao'}]",2024-08-03T02:11:48Z
http://arxiv.org/abs/2408.01623v1,http://arxiv.org/abs/2408.01623v1,Dialog Flow Induction for Constrainable LLM-Based Chatbots,"LLM-driven dialog systems are used in a diverse set of applications, ranging
from healthcare to customer service. However, given their generalization
capability, it is difficult to ensure that these chatbots stay within the
boundaries of the specialized domains, potentially resulting in inaccurate
information and irrelevant responses. This paper introduces an unsupervised
approach for automatically inducing domain-specific dialog flows that can be
used to constrain LLM-based chatbots. We introduce two variants of dialog flow
based on the availability of in-domain conversation instances. Through human
and automatic evaluation over various dialog domains, we demonstrate that our
high-quality data-guided dialog flows achieve better domain coverage, thereby
overcoming the need for extensive manual crafting of such flows.","[{'name': 'Stuti Agrawal'}, {'name': 'Nishi Uppuluri'}, {'name': 'Pranav Pillai'}, {'name': 'Revanth Gangi Reddy'}, {'name': 'Zoey Li'}, {'name': 'Gokhan Tur'}, {'name': 'Dilek Hakkani-Tur'}, {'name': 'Heng Ji'}]",2024-08-03T01:15:50Z
http://arxiv.org/abs/2408.04646v1,http://arxiv.org/abs/2408.04646v1,Efficacy of Large Language Models in Systematic Reviews,"This study investigates the effectiveness of Large Language Models (LLMs) in
interpreting existing literature through a systematic review of the
relationship between Environmental, Social, and Governance (ESG) factors and
financial performance. The primary objective is to assess how LLMs can
replicate a systematic review on a corpus of ESG-focused papers. We compiled
and hand-coded a database of 88 relevant papers published from March 2020 to
May 2024. Additionally, we used a set of 238 papers from a previous systematic
review of ESG literature from January 2015 to February 2020. We evaluated two
current state-of-the-art LLMs, Meta AI's Llama 3 8B and OpenAI's GPT-4o, on the
accuracy of their interpretations relative to human-made classifications on
both sets of papers. We then compared these results to a ""Custom GPT"" and a
fine-tuned GPT-4o Mini model using the corpus of 238 papers as training data.
The fine-tuned GPT-4o Mini model outperformed the base LLMs by 28.3% on average
in overall accuracy on prompt 1. At the same time, the ""Custom GPT"" showed a
3.0% and 15.7% improvement on average in overall accuracy on prompts 2 and 3,
respectively. Our findings reveal promising results for investors and agencies
to leverage LLMs to summarize complex evidence related to ESG investing,
thereby enabling quicker decision-making and a more efficient market.","[{'name': 'Aaditya Shah'}, {'name': 'Shridhar Mehendale'}, {'name': 'Siddha Kanthi'}]",2024-08-03T00:01:13Z
http://arxiv.org/abs/2408.04645v1,http://arxiv.org/abs/2408.04645v1,"Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors
  for a Robotics Course","This study evaluates the performance of Large Language Models (LLMs) as an
Artificial Intelligence-based tutor for a university course. In particular,
different advanced techniques are utilized, such as prompt engineering,
Retrieval-Augmented-Generation (RAG), and fine-tuning. We assessed the
different models and applied techniques using common similarity metrics like
BLEU-4, ROUGE, and BERTScore, complemented by a small human evaluation of
helpfulness and trustworthiness. Our findings indicate that RAG combined with
prompt engineering significantly enhances model responses and produces better
factual answers. In the context of education, RAG appears as an ideal technique
as it is based on enriching the input of the model with additional information
and material which usually is already present for a university course.
Fine-tuning, on the other hand, can produce quite small, still strong expert
models, but poses the danger of overfitting. Our study further asks how we
measure performance of LLMs and how well current measurements represent
correctness or relevance? We find high correlation on similarity metrics and a
bias of most of these metrics towards shorter responses. Overall, our research
points to both the potential and challenges of integrating LLMs in educational
settings, suggesting a need for balanced training approaches and advanced
evaluation frameworks.","[{'name': 'Sebastian Kahl'}, {'name': 'Felix Löffler'}, {'name': 'Martin Maciol'}, {'name': 'Fabian Ridder'}, {'name': 'Marius Schmitz'}, {'name': 'Jennifer Spanagel'}, {'name': 'Jens Wienkamp'}, {'name': 'Christopher Burgahn'}, {'name': 'Malte Schilling'}]",2024-08-02T19:49:19Z
http://arxiv.org/abs/2408.01527v1,http://arxiv.org/abs/2408.01527v1,"Analyzing LLMs' Capabilities to Establish Implicit User Sentiment of
  Software Desirability","This study explores the use of several LLMs for providing quantitative
zero-shot sentiment analysis of implicit software desirability expressed by
users. The study provides scaled numerical sentiment analysis unlike other
methods that simply classify sentiment as positive, neutral, or negative.
Numerical analysis provides deeper insights into the magnitude of sentiment, to
drive better decisions regarding product desirability.
  Data is collected through the use of the Microsoft Product Desirability
Toolkit (PDT), a well-known qualitative user experience analysis tool. For
initial exploration, the PDT metric was given to users of ZORQ, a gamification
system used in undergraduate computer science education. The PDT data collected
was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and
through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment
(TRBS), and through Vader, a leading sentiment analysis tool, for quantitative
sentiment analysis. Each system was asked to evaluate the data in two ways,
first by looking at the sentiment expressed in the PDT word/explanation pairs;
and by looking at the sentiment expressed by the users in their grouped
selection of five words and explanations, as a whole. Each LLM was also asked
to provide its confidence (low, medium, high) in its sentiment score, along
with an explanation of why it selected the sentiment value.
  All LLMs tested were able to statistically detect user sentiment from the
users' grouped data, whereas TRBS and Vader were not. The confidence and
explanation of confidence provided by the LLMs assisted in understanding the
user sentiment. This study adds to a deeper understanding of evaluating user
experiences, toward the goal of creating a universal tool that quantifies
implicit sentiment expressed.","[{'name': 'Sherri Weitl-Harms'}, {'name': 'John D. Hastings'}, {'name': 'Jonah Lum'}]",2024-08-02T18:40:10Z
http://arxiv.org/abs/2408.01505v1,http://arxiv.org/abs/2408.01505v1,"MoDE: Effective Multi-task Parameter Efficient Fine-Tuning with a
  Mixture of Dyadic Experts","Parameter-efficient fine-tuning techniques like Low-Rank Adaptation (LoRA)
have revolutionized the adaptation of large language models (LLMs) to diverse
tasks. Recent efforts have explored mixtures of LoRA modules for multi-task
settings. However, our analysis reveals redundancy in the down-projection
matrices of these architectures. This observation motivates our proposed
method, Mixture of Dyadic Experts (MoDE), which introduces a novel design for
efficient multi-task adaptation. This is done by sharing the down-projection
matrix across tasks and employing atomic rank-one adapters, coupled with
routers that allow more sophisticated task-level specialization. Our design
allows for more fine-grained mixing, thereby increasing the model's ability to
jointly handle multiple tasks. We evaluate MoDE on the Supernatural
Instructions (SNI) benchmark consisting of a diverse set of 700+ tasks and
demonstrate that it outperforms state-of-the-art multi-task parameter-efficient
fine-tuning (PEFT) methods, without introducing additional parameters. Our
findings contribute to a deeper understanding of parameter efficiency in
multi-task LLM adaptation and provide a practical solution for deploying
high-performing, lightweight models.","[{'name': 'Lin Ning'}, {'name': 'Harsh Lara'}, {'name': 'Meiqi Guo'}, {'name': 'Abhinav Rastogi'}]",2024-08-02T18:05:10Z
http://arxiv.org/abs/2408.01423v1,http://arxiv.org/abs/2408.01423v1,"Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM
  Auto-Prompting","Large Language Models (LLMs) exhibit remarkable proficiency in addressing a
diverse array of tasks within the Natural Language Processing (NLP) domain,
with various prompt design strategies significantly augmenting their
capabilities. However, these prompts, while beneficial, each possess inherent
limitations. The primary prompt design methodologies are twofold: The first,
exemplified by the Chain of Thought (CoT), involves manually crafting prompts
specific to individual datasets, hence termed Expert-Designed Prompts (EDPs).
Once these prompts are established, they are unalterable, and their
effectiveness is capped by the expertise of the human designers. When applied
to LLMs, the static nature of EDPs results in a uniform approach to both simple
and complex problems within the same dataset, leading to the inefficient use of
tokens for straightforward issues. The second method involves prompts
autonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which
provide tailored solutions to specific problems, mitigating the limitations of
EDPs. However, LDPs may encounter a decline in performance when tackling
complex problems due to the potential for error accumulation during the
solution planning process. To address these challenges, we have conceived a
novel Prompt Recursive Search (PRS) framework that leverages the LLM to
generate solutions specific to the problem, thereby conserving tokens. The
framework incorporates an assessment of problem complexity and an adjustable
structure, ensuring a reduction in the likelihood of errors. We have
substantiated the efficacy of PRS framework through extensive experiments using
LLMs with different numbers of parameters across a spectrum of datasets in
various domains. Compared to the CoT method, the PRS method has increased the
accuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22%
improvement.","[{'name': 'Xiangyu Zhao'}, {'name': 'Chengqian Ma'}]",2024-08-02T17:59:42Z
http://arxiv.org/abs/2408.01420v1,http://arxiv.org/abs/2408.01420v1,Mission Impossible: A Statistical Perspective on Jailbreaking LLMs,"Large language models (LLMs) are trained on a deluge of text data with
limited quality control. As a result, LLMs can exhibit unintended or even
harmful behaviours, such as leaking information, fake news or hate speech.
Countermeasures, commonly referred to as preference alignment, include
fine-tuning the pretrained LLMs with carefully crafted text examples of desired
behaviour. Even then, empirical evidence shows preference aligned LLMs can be
enticed to harmful behaviour. This so called jailbreaking of LLMs is typically
achieved by adversarially modifying the input prompt to the LLM. Our paper
provides theoretical insights into the phenomenon of preference alignment and
jailbreaking from a statistical perspective. Under our framework, we first show
that pretrained LLMs will mimic harmful behaviour if present in the training
corpus. Under that same framework, we then introduce a statistical notion of
alignment, and lower-bound the jailbreaking probability, showing that it is
unpreventable under reasonable assumptions. Based on our insights, we propose
an alteration to the currently prevalent alignment strategy RLHF. Specifically,
we introduce a simple modification to the RLHF objective, we call E-RLHF, that
aims to increase the likelihood of safe responses. E-RLHF brings no additional
training cost, and is compatible with other methods. Empirically, we
demonstrate that E-RLHF outperforms RLHF on all alignment problems put forward
by the AdvBench and HarmBench project without sacrificing model performance as
measured by the MT-Bench project.","[{'name': 'Jingtong Su'}, {'name': 'Julia Kempe'}, {'name': 'Karen Ullrich'}]",2024-08-02T17:55:50Z
http://arxiv.org/abs/2408.01419v1,http://arxiv.org/abs/2408.01419v1,DebateQA: Evaluating Question Answering on Debatable Knowledge,"The rise of large language models (LLMs) has enabled us to seek answers to
inherently debatable questions on LLM chatbots, necessitating a reliable way to
evaluate their ability. However, traditional QA benchmarks assume fixed answers
are inadequate for this purpose. To address this, we introduce DebateQA, a
dataset of 2,941 debatable questions, each accompanied by multiple
human-annotated partial answers that capture a variety of perspectives. We
develop two metrics: Perspective Diversity, which evaluates the
comprehensiveness of perspectives, and Dispute Awareness, which assesses if the
LLM acknowledges the question's debatable nature. Experiments demonstrate that
both metrics align with human preferences and are stable across different
underlying models. Using DebateQA with two metrics, we assess 12 popular LLMs
and retrieval-augmented generation methods. Our findings reveal that while LLMs
generally excel at recognizing debatable issues, their ability to provide
comprehensive answers encompassing diverse perspectives varies considerably.","[{'name': 'Rongwu Xu'}, {'name': 'Xuan Qi'}, {'name': 'Zehan Qi'}, {'name': 'Wei Xu'}, {'name': 'Zhijiang Guo'}]",2024-08-02T17:54:34Z
http://arxiv.org/abs/2408.01417v1,http://arxiv.org/abs/2408.01417v1,"Talk Less, Interact Better: Evaluating In-context Conversational
  Adaptation in Multimodal LLMs","Humans spontaneously use increasingly efficient language as interactions
progress, by adapting and forming ad-hoc conventions. This phenomenon has been
studied extensively using reference games, showing properties of human language
that go beyond relaying intents. It remains unexplored whether multimodal large
language models (MLLMs) similarly increase communication efficiency during
interactions, and what mechanisms they may adopt for this purpose. We introduce
ICCA, an automated framework to evaluate such conversational adaptation as an
in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and
observe that while they may understand the increasingly efficient language of
their interlocutor, they do not spontaneously make their own language more
efficient over time. This latter ability can only be elicited in some models
(e.g., GPT-4) with heavy-handed prompting. This shows that this property of
linguistic interaction does not arise from current training regimes, even
though it is a common hallmark of human language. ICCA is available at
https://github.com/lil-lab/ICCA.","[{'name': 'Yilun Hua'}, {'name': 'Yoav Artzi'}]",2024-08-02T17:51:57Z
http://arxiv.org/abs/2408.01402v1,http://arxiv.org/abs/2408.01402v1,"Pre-trained Language Models Improve the Few-shot Prompt Ability of
  Decision Transformer","Decision Transformer (DT) has emerged as a promising class of algorithms in
offline reinforcement learning (RL) tasks, leveraging pre-collected datasets
and Transformer's capability to model long sequences. Recent works have
demonstrated that using parts of trajectories from training tasks as prompts in
DT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.
However, collecting data from specific environments can be both costly and
unsafe in many scenarios, leading to suboptimal performance and limited
few-shot prompt abilities due to the data-hungry nature of Transformer-based
models. Additionally, the limited datasets used in pre-training make it
challenging for Prompt-DT type of methods to distinguish between various RL
tasks through prompts alone. To address these challenges, we introduce the
Language model-initialized Prompt Decision Transformer (LPDT), which leverages
pre-trained language models for meta-RL tasks and fine-tunes the model using
Low-rank Adaptation (LoRA). We further incorporate prompt regularization to
effectively differentiate between tasks based on prompt feature
representations. Our approach integrates pre-trained language model and RL
tasks seamlessly. Extensive empirical studies demonstrate that initializing
with a pre-trained language model significantly enhances the performance of
Prompt-DT on unseen tasks compared to baseline methods.","[{'name': 'Yu Yang'}, {'name': 'Pan Xu'}]",2024-08-02T17:25:34Z
http://arxiv.org/abs/2408.01394v1,http://arxiv.org/abs/2408.01394v1,"Improving Multilingual Neural Machine Translation by Utilizing Semantic
  and Linguistic Features","The many-to-many multilingual neural machine translation can be regarded as
the process of integrating semantic features from the source sentences and
linguistic features from the target sentences. To enhance zero-shot
translation, models need to share knowledge across languages, which can be
achieved through auxiliary tasks for learning a universal representation or
cross-lingual mapping. To this end, we propose to exploit both semantic and
linguistic features between multiple languages to enhance multilingual
translation. On the encoder side, we introduce a disentangling learning task
that aligns encoder representations by disentangling semantic and linguistic
features, thus facilitating knowledge transfer while preserving complete
information. On the decoder side, we leverage a linguistic encoder to integrate
low-level linguistic features to assist in the target language generation.
Experimental results on multilingual datasets demonstrate significant
improvement in zero-shot translation compared to the baseline system, while
maintaining performance in supervised translation. Further analysis validates
the effectiveness of our method in leveraging both semantic and linguistic
features. The code is available at https://github.com/ictnlp/SemLing-MNMT.","[{'name': 'Mengyu Bu'}, {'name': 'Shuhao Gu'}, {'name': 'Yang Feng'}]",2024-08-02T17:10:12Z
http://arxiv.org/abs/2408.01380v1,http://arxiv.org/abs/2408.01380v1,Coalitions of Large Language Models Increase the Robustness of AI Agents,"The emergence of Large Language Models (LLMs) have fundamentally altered the
way we interact with digital systems and have led to the pursuit of LLM powered
AI agents to assist in daily workflows. LLMs, whilst powerful and capable of
demonstrating some emergent properties, are not logical reasoners and often
struggle to perform well at all sub-tasks carried out by an AI agent to plan
and execute a workflow. While existing studies tackle this lack of proficiency
by generalised pretraining at a huge scale or by specialised fine-tuning for
tool use, we assess if a system comprising of a coalition of pretrained LLMs,
each exhibiting specialised performance at individual sub-tasks, can match the
performance of single model agents. The coalition of models approach showcases
its potential for building robustness and reducing the operational costs of
these AI agents by leveraging traits exhibited by specific models. Our findings
demonstrate that fine-tuning can be mitigated by considering a coalition of
pretrained models and believe that this approach can be applied to other
non-agentic systems which utilise LLMs.","[{'name': 'Prattyush Mangal'}, {'name': 'Carol Mak'}, {'name': 'Theo Kanakis'}, {'name': 'Timothy Donovan'}, {'name': 'Dave Braines'}, {'name': 'Edward Pyzer-Knapp'}]",2024-08-02T16:37:44Z
http://arxiv.org/abs/2408.01367v1,http://arxiv.org/abs/2408.01367v1,Transformers are Universal In-context Learners,"Transformers are deep architectures that define ""in-context mappings"" which
enable predicting new tokens based on a given set of tokens (such as a prompt
in NLP applications or a set of patches for vision transformers). This work
studies in particular the ability of these architectures to handle an
arbitrarily large number of context tokens. To mathematically and uniformly
address the expressivity of these architectures, we consider the case that the
mappings are conditioned on a context represented by a probability distribution
of tokens (discrete for a finite number of tokens). The related notion of
smoothness corresponds to continuity in terms of the Wasserstein distance
between these contexts. We demonstrate that deep transformers are universal and
can approximate continuous in-context mappings to arbitrary precision,
uniformly over compact token domains. A key aspect of our results, compared to
existing findings, is that for a fixed precision, a single transformer can
operate on an arbitrary (even infinite) number of tokens. Additionally, it
operates with a fixed embedding dimension of tokens (this dimension does not
increase with precision) and a fixed number of heads (proportional to the
dimension). The use of MLP layers between multi-head attention layers is also
explicitly controlled.","[{'name': 'Takashi Furuya'}, {'name': 'Maarten V. de Hoop'}, {'name': 'Gabriel Peyré'}]",2024-08-02T16:21:48Z
http://arxiv.org/abs/2408.01363v1,http://arxiv.org/abs/2408.01363v1,"Toward Automatic Relevance Judgment using Vision--Language Models for
  Image--Text Retrieval Evaluation","Vision--Language Models (VLMs) have demonstrated success across diverse
applications, yet their potential to assist in relevance judgments remains
uncertain. This paper assesses the relevance estimation capabilities of VLMs,
including CLIP, LLaVA, and GPT-4V, within a large-scale \textit{ad hoc}
retrieval task tailored for multimedia content creation in a zero-shot fashion.
Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,
encompassing open-source and closed-source visual-instruction-tuned Large
Language Models (LLMs), achieve notable Kendall's $\tau \sim 0.4$ when compared
to human relevance judgments, surpassing the CLIPScore metric. (2) While
CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based
retrieval systems. (3) GPT-4V's score distribution aligns more closely with
human judgments than other models, achieving a Cohen's $\kappa$ value of around
0.08, which outperforms CLIPScore at approximately -0.096. These findings
underscore the potential of LLM-powered VLMs in enhancing relevance judgments.","[{'name': 'Jheng-Hong Yang'}, {'name': 'Jimmy Lin'}]",2024-08-02T16:15:25Z
http://arxiv.org/abs/2408.01346v1,http://arxiv.org/abs/2408.01346v1,"Prompt Refinement or Fine-tuning? Best Practices for using LLMs in
  Computational Social Science Tasks","Large Language Models are expressive tools that enable complex tasks of text
understanding within Computational Social Science. Their versatility, while
beneficial, poses a barrier for establishing standardized best practices within
the field. To bring clarity on the values of different strategies, we present
an overview of the performance of modern LLM-based classification methods on a
benchmark of 23 social knowledge tasks. Our results point to three best
practices: select models with larger vocabulary and pre-training corpora; avoid
simple zero-shot in favor of AI-enhanced prompting; fine-tune on task-specific
data, and consider more complex forms instruction-tuning on multiple datasets
only when only training data is more abundant.","[{'name': 'Anders Giovanni Møller'}, {'name': 'Luca Maria Aiello'}]",2024-08-02T15:46:36Z
http://arxiv.org/abs/2408.01337v1,http://arxiv.org/abs/2408.01337v1,"MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language
  Models","Multimodal models that jointly process audio and language hold great promise
in audio understanding and are increasingly being adopted in the music domain.
By allowing users to query via text and obtain information about a given audio
input, these models have the potential to enable a variety of music
understanding tasks via language-based interfaces. However, their evaluation
poses considerable challenges, and it remains unclear how to effectively assess
their ability to correctly interpret music-related inputs with current methods.
Motivated by this, we introduce MuChoMusic, a benchmark for evaluating music
understanding in multimodal language models focused on audio. MuChoMusic
comprises 1,187 multiple-choice questions, all validated by human annotators,
on 644 music tracks sourced from two publicly available music datasets, and
covering a wide variety of genres. Questions in the benchmark are crafted to
assess knowledge and reasoning abilities across several dimensions that cover
fundamental musical concepts and their relation to cultural and functional
contexts. Through the holistic analysis afforded by the benchmark, we evaluate
five open-source models and identify several pitfalls, including an
over-reliance on the language modality, pointing to a need for better
multimodal integration. Data and code are open-sourced.","[{'name': 'Benno Weck'}, {'name': 'Ilaria Manco'}, {'name': 'Emmanouil Benetos'}, {'name': 'Elio Quinton'}, {'name': 'George Fazekas'}, {'name': 'Dmitry Bogdanov'}]",2024-08-02T15:34:05Z
http://arxiv.org/abs/2408.01323v1,http://arxiv.org/abs/2408.01323v1,"FANNO: Augmenting High-Quality Instruction Data with Open-Sourced LLMs
  Only","Instruction fine-tuning stands as a crucial advancement in leveraging large
language models (LLMs) for enhanced task performance. However, the annotation
of instruction datasets has traditionally been expensive and laborious, often
relying on manual annotations or costly API calls of proprietary LLMs. To
address these challenges, we introduce FANNO, a fully autonomous, open-sourced
framework that revolutionizes the annotation process without the need for
pre-existing annotated data. Utilizing a Mistral-7b-instruct model, FANNO
efficiently produces diverse and high-quality datasets through a structured
process involving document pre-screening, instruction generation, and response
generation. Experiments on Open LLM Leaderboard and AlpacaEval benchmark show
that the FANNO can generate high-quality data with diversity and complexity for
free, comparable to human-annotated or cleaned datasets like
Alpaca-GPT4-Cleaned.","[{'name': 'He Zhu'}, {'name': 'Junyou Su'}, {'name': 'Tianle Lun'}, {'name': 'Yicheng Tao'}, {'name': 'Wenjia Zhang'}, {'name': 'Zipei Fan'}, {'name': 'Guanhua Chen'}]",2024-08-02T15:21:20Z
http://arxiv.org/abs/2408.01308v1,http://arxiv.org/abs/2408.01308v1,"Reconsidering Token Embeddings with the Definitions for Pre-trained
  Language Models","Learning token embeddings based on token co-occurrence statistics has proven
effective for both pre-training and fine-tuning in natural language processing.
However, recent studies have pointed out the distribution of learned embeddings
degenerates into anisotropy, and even pre-trained language models (PLMs) suffer
from a loss of semantics-related information in embeddings for low-frequency
tokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large,
and demonstrates its robustness against degeneration. On the basis of this
finding, we propose DefinitionEMB, a method that utilizes definitions to
construct isotropically distributed and semantics-related token embeddings for
PLMs while maintaining original robustness during fine-tuning. Our experiments
demonstrate the effectiveness of leveraging definitions from Wiktionary to
construct such embeddings for RoBERTa-base and BART-large. Furthermore, the
constructed embeddings for low-frequency tokens improve the performance of
these models across various GLUE and four text summarization datasets.","[{'name': 'Ying Zhang'}, {'name': 'Dongyuan Li'}, {'name': 'Manabu Okumura'}]",2024-08-02T15:00:05Z
http://arxiv.org/abs/2408.01287v1,http://arxiv.org/abs/2408.01287v1,"Deep Learning based Visually Rich Document Content Understanding: A
  Survey","Visually Rich Documents (VRDs) are essential in academia, finance, medical
fields, and marketing due to their multimodal information content. Traditional
methods for extracting information from VRDs depend on expert knowledge and
manual labor, making them costly and inefficient. The advent of deep learning
has revolutionized this process, introducing models that leverage multimodal
information vision, text, and layout along with pretraining tasks to develop
comprehensive document representations. These models have achieved
state-of-the-art performance across various downstream tasks, significantly
enhancing the efficiency and accuracy of information extraction from VRDs. In
response to the growing demands and rapid developments in Visually Rich
Document Understanding (VRDU), this paper provides a comprehensive review of
deep learning-based VRDU frameworks. We systematically survey and analyze
existing methods and benchmark datasets, categorizing them based on adopted
strategies and downstream tasks. Furthermore, we compare different techniques
used in VRDU models, focusing on feature representation and fusion, model
architecture, and pretraining methods, while highlighting their strengths,
limitations, and appropriate scenarios. Finally, we identify emerging trends
and challenges in VRDU, offering insights into future research directions and
practical applications. This survey aims to provide a thorough understanding of
VRDU advancements, benefiting both academic and industrial sectors.","[{'name': 'Yihao Ding'}, {'name': 'Jean Lee'}, {'name': 'Soyeon Caren Han'}]",2024-08-02T14:19:34Z
http://arxiv.org/abs/2408.01285v1,http://arxiv.org/abs/2408.01285v1,"The Mismeasure of Man and Models: Evaluating Allocational Harms in Large
  Language Models","Large language models (LLMs) are now being considered and even deployed for
applications that support high-stakes decision-making, such as recruitment and
clinical decisions. While several methods have been proposed for measuring
bias, there remains a gap between predictions, which are what the proposed
methods consider, and how they are used to make decisions. In this work, we
introduce Rank-Allocational-Based Bias Index (RABBI), a model-agnostic bias
measure that assesses potential allocational harms arising from biases in LLM
predictions. We compare RABBI and current bias metrics on two allocation
decision tasks. We evaluate their predictive validity across ten LLMs and
utility for model selection. Our results reveal that commonly-used bias metrics
based on average performance gap and distribution distance fail to reliably
capture group disparities in allocation outcomes, whereas RABBI exhibits a
strong correlation with allocation disparities. Our work highlights the need to
account for how models are used in contexts with limited resource constraints.","[{'name': 'Hannah Chen'}, {'name': 'Yangfeng Ji'}, {'name': 'David Evans'}]",2024-08-02T14:13:06Z
http://arxiv.org/abs/2408.01262v2,http://arxiv.org/abs/2408.01262v2,RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework,"Retrieval-Augmented Generation (RAG) systems have demonstrated their
advantages in alleviating the hallucination of Large Language Models (LLMs).
Existing RAG benchmarks mainly focus on evaluating whether LLMs can correctly
answer the general knowledge. However, they are unable to evaluate the
effectiveness of the RAG system in dealing with the data from different
vertical domains. This paper introduces RAGEval, a framework for automatically
generating evaluation datasets to evaluate the knowledge usage ability of
different LLMs in different scenarios. Specifically, RAGEval summarizes a
schema from seed documents, applies the configurations to generate diverse
documents, and constructs question-answering pairs according to both articles
and configurations. We propose three novel metrics, Completeness,
Hallucination, and Irrelevance, to carefully evaluate the responses generated
by LLMs. By benchmarking RAG models in vertical domains, RAGEval has the
ability to better evaluate the knowledge usage ability of LLMs, which avoids
the confusion regarding the source of knowledge in answering question in
existing QA datasets--whether it comes from parameterized memory or retrieval.
The code and dataset will be released.","[{'name': 'Kunlun Zhu'}, {'name': 'Yifan Luo'}, {'name': 'Dingling Xu'}, {'name': 'Ruobing Wang'}, {'name': 'Shi Yu'}, {'name': 'Shuo Wang'}, {'name': 'Yukun Yan'}, {'name': 'Zhenghao Liu'}, {'name': 'Xu Han'}, {'name': 'Zhiyuan Liu'}, {'name': 'Maosong Sun'}]",2024-08-02T13:35:11Z
http://arxiv.org/abs/2408.01214v1,http://arxiv.org/abs/2408.01214v1,High-Throughput Phenotyping of Clinical Text Using Large Language Models,"High-throughput phenotyping automates the mapping of patient signs to
standardized ontology concepts and is essential for precision medicine. This
study evaluates the automation of phenotyping of clinical summaries from the
Online Mendelian Inheritance in Man (OMIM) database using large language
models. Due to their rich phenotype data, these summaries can be surrogates for
physician notes. We conduct a performance comparison of GPT-4 and
GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in
identifying, categorizing, and normalizing signs, achieving concordance with
manual annotators comparable to inter-rater agreement. Despite some limitations
in sign normalization, the extensive pre-training of GPT-4 results in high
performance and generalizability across several phenotyping tasks while
obviating the need for manually annotated training data. Large language models
are expected to be the dominant method for automating high-throughput
phenotyping of clinical text.","[{'name': 'Daniel B. Hier'}, {'name': 'S. Ilyas Munzir'}, {'name': 'Anne Stahlfeld'}, {'name': 'Tayo Obafemi-Ajayi'}, {'name': 'Michael D. Carrithers'}]",2024-08-02T12:00:00Z
http://arxiv.org/abs/2408.01168v1,http://arxiv.org/abs/2408.01168v1,"Misinforming LLMs: vulnerabilities, challenges and opportunities","Large Language Models (LLMs) have made significant advances in natural
language processing, but their underlying mechanisms are often misunderstood.
Despite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely
on statistical patterns in word embeddings rather than true cognitive
processes. This leads to vulnerabilities such as ""hallucination"" and
misinformation. The paper argues that current LLM architectures are inherently
untrustworthy due to their reliance on correlations of sequential patterns of
word embedding vectors. However, ongoing research into combining generative
transformer-based models with fact bases and logic programming languages may
lead to the development of trustworthy LLMs capable of generating statements
based on given truth and explaining their self-reasoning process.","[{'name': 'Bo Zhou'}, {'name': 'Daniel Geißler'}, {'name': 'Paul Lukowicz'}]",2024-08-02T10:35:49Z
http://arxiv.org/abs/2408.01154v1,http://arxiv.org/abs/2408.01154v1,DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs,"Entity Alignment (EA) aims to match equivalent entities in different
Knowledge Graphs (KGs), which is essential for knowledge fusion and
integration. Recently, embedding-based EA has attracted significant attention
and many approaches have been proposed. Early approaches primarily focus on
learning entity embeddings from the structural features of KGs, defined by
relation triples. Later methods incorporated entities' names and attributes as
auxiliary information to enhance embeddings for EA. However, these approaches
often used different techniques to encode structural and attribute information,
limiting their interaction and mutual enhancement. In this work, we propose a
dense entity retrieval framework for EA, leveraging language models to
uniformly encode various features of entities and facilitate nearest entity
search across KGs. Alignment candidates are first generated through entity
retrieval, which are subsequently reranked to determine the final alignments.
We conduct comprehensive experiments on both cross-lingual and monolingual EA
datasets, demonstrating that our approach achieves state-of-the-art performance
compared to existing EA methods.","[{'name': 'Zhichun Wang'}, {'name': 'Xuan Chen'}]",2024-08-02T10:12:42Z
http://arxiv.org/abs/2408.01122v1,http://arxiv.org/abs/2408.01122v1,CFBench: A Comprehensive Constraints-Following Benchmark for LLMs,"The adeptness of Large Language Models (LLMs) in comprehending and following
natural language instructions is critical for their deployment in sophisticated
real-world applications. Existing evaluations mainly focus on fragmented
constraints or narrow scenarios, but they overlook the comprehensiveness and
authenticity of constraints from the user's perspective. To bridge this gap, we
propose CFBench, a large-scale Comprehensive Constraints Following Benchmark
for LLMs, featuring 1,000 curated samples that cover more than 200 real-life
scenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from
real-world instructions and constructs an innovative systematic framework for
constraint types, which includes 10 primary categories and over 25
subcategories, and ensures each constraint is seamlessly integrated within the
instructions. To make certain that the evaluation of LLM outputs aligns with
user perceptions, we propose an advanced methodology that integrates
multi-dimensional assessment criteria with requirement prioritization, covering
various perspectives of constraints, instructions, and requirement fulfillment.
Evaluating current leading LLMs on CFBench reveals substantial room for
improvement in constraints following, and we further investigate influencing
factors and enhancement strategies. The data and code are publicly available at
https://github.com/PKU-Baichuan-MLSystemLab/CFBench","[{'name': 'Tao Zhang'}, {'name': 'Yanjun Shen'}, {'name': 'Wenjing Luo'}, {'name': 'Yan Zhang'}, {'name': 'Hao Liang'}, {'name': 'Tao Zhang'}, {'name': 'Fan Yang'}, {'name': 'Mingan Lin'}, {'name': 'Yujing Qiao'}, {'name': 'Weipeng Chen'}, {'name': 'Bin Cui'}, {'name': 'Wentao Zhang'}, {'name': 'Zenan Zhou'}]",2024-08-02T09:03:48Z
http://arxiv.org/abs/2408.01119v1,http://arxiv.org/abs/2408.01119v1,"Task Prompt Vectors: Effective Initialization through Multi-Task
  Soft-Prompt Transfer","Prompt tuning is a modular and efficient solution for training large language
models (LLMs). One of its main advantages is task modularity, making it
suitable for multi-task problems. However, current soft-prompt-based methods
often sacrifice multi-task modularity, requiring the training process to be
fully or partially repeated for each newly added task. While recent work on
task vectors applied arithmetic operations on full model weights to achieve the
desired multi-task performance, a similar approach for soft-prompts is still
missing. To this end, we introduce Task Prompt Vectors, created by element-wise
difference between weights of tuned soft-prompts and their random
initialization. Experimental results on 12 NLU datasets show that task prompt
vectors can be used in low-resource settings to effectively initialize prompt
tuning on similar tasks. In addition, we show that task prompt vectors are
independent of the random initialization of prompt tuning. This allows prompt
arithmetics with the pre-trained vectors from different tasks. In this way, by
arithmetic addition of task prompt vectors from multiple tasks, we are able to
outperform a state-of-the-art baseline in some cases.","[{'name': 'Robert Belanec'}, {'name': 'Simon Ostermann'}, {'name': 'Ivan Srba'}, {'name': 'Maria Bielikova'}]",2024-08-02T09:00:03Z
http://arxiv.org/abs/2408.01118v1,http://arxiv.org/abs/2408.01118v1,"IAI Group at CheckThat! 2024: Transformer Models and Data Augmentation
  for Checkworthy Claim Detection","This paper describes IAI group's participation for automated check-worthiness
estimation for claims, within the framework of the 2024 CheckThat! Lab ""Task 1:
Check-Worthiness Estimation"". The task involves the automated detection of
check-worthy claims in English, Dutch, and Arabic political debates and Twitter
data. We utilized various pre-trained generative decoder and encoder
transformer models, employing methods such as few-shot chain-of-thought
reasoning, fine-tuning, data augmentation, and transfer learning from one
language to another. Despite variable success in terms of performance, our
models achieved notable placements on the organizer's leaderboard: ninth-best
in English, third-best in Dutch, and the top placement in Arabic, utilizing
multilingual datasets for enhancing the generalizability of check-worthiness
detection. Despite a significant drop in performance on the unlabeled test
dataset compared to the development test dataset, our findings contribute to
the ongoing efforts in claim detection research, highlighting the challenges
and potential of language-specific adaptations in claim verification systems.","[{'name': 'Peter Røysland Aarnes'}, {'name': 'Vinay Setty'}, {'name': 'Petra Galuščáková'}]",2024-08-02T08:59:09Z
http://arxiv.org/abs/2408.01107v2,http://arxiv.org/abs/2408.01107v2,BioRAG: A RAG-LLM Framework for Biological Question Reasoning,"The question-answering system for Life science research, which is
characterized by the rapid pace of discovery, evolving insights, and complex
interactions among knowledge entities, presents unique challenges in
maintaining a comprehensive knowledge warehouse and accurate information
retrieval. To address these issues, we introduce BioRAG, a novel
Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)
framework. Our approach starts with parsing, indexing, and segmenting an
extensive collection of 22 million scientific papers as the basic knowledge,
followed by training a specialized embedding model tailored to this domain.
Additionally, we enhance the vector retrieval process by incorporating a
domain-specific knowledge hierarchy, which aids in modeling the intricate
interrelationships among each query and context. For queries requiring the most
current information, BioRAG deconstructs the question and employs an iterative
retrieval process incorporated with the search engine for step-by-step
reasoning. Rigorous experiments have demonstrated that our model outperforms
fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks
across multiple life science question-answering tasks.","[{'name': 'Chengrui Wang'}, {'name': 'Qingqing Long'}, {'name': 'Meng Xiao'}, {'name': 'Xunxin Cai'}, {'name': 'Chengjun Wu'}, {'name': 'Zhen Meng'}, {'name': 'Xuezhi Wang'}, {'name': 'Yuanchun Zhou'}]",2024-08-02T08:37:03Z
http://arxiv.org/abs/2408.01090v1,http://arxiv.org/abs/2408.01090v1,General-purpose Dataflow Model with Neuromorphic Primitives,"Neuromorphic computing exhibits great potential to provide high-performance
benefits in various applications beyond neural networks. However, a
general-purpose program execution model that aligns with the features of
neuromorphic computing is required to bridge the gap between program
versatility and neuromorphic hardware efficiency. The dataflow model offers a
potential solution, but it faces high graph complexity and incompatibility with
neuromorphic hardware when dealing with control flow programs, which decreases
the programmability and performance. Here, we present a dataflow model tailored
for neuromorphic hardware, called neuromorphic dataflow, which provides a
compact, concise, and neuromorphic-compatible program representation for
control logic. The neuromorphic dataflow introduces ""when"" and ""where""
primitives, which restructure the view of control. The neuromorphic dataflow
embeds these primitives in the dataflow schema with the plasticity inherited
from the spiking algorithms. Our method enables the deployment of
general-purpose programs on neuromorphic hardware with both programmability and
plasticity, while fully utilizing the hardware's potential.","[{'name': 'Weihao Zhang'}, {'name': 'Yu Du'}, {'name': 'Hongyi Li'}, {'name': 'Songchen Ma'}, {'name': 'Rong Zhao'}]",2024-08-02T08:09:13Z
http://arxiv.org/abs/2408.01088v2,http://arxiv.org/abs/2408.01088v2,"Bridging Information Gaps in Dialogues With Grounded Exchanges Using
  Knowledge Graphs","Knowledge models are fundamental to dialogue systems for enabling
conversational interactions, which require handling domain-specific knowledge.
Ensuring effective communication in information-providing conversations entails
aligning user understanding with the knowledge available to the system.
However, dialogue systems often face challenges arising from semantic
inconsistencies in how information is expressed in natural language compared to
how it is represented within the system's internal knowledge. To address this
problem, we study the potential of large language models for conversational
grounding, a mechanism to bridge information gaps by establishing shared
knowledge between dialogue participants. Our approach involves annotating human
conversations across five knowledge domains to create a new dialogue corpus
called BridgeKG. Through a series of experiments on this dataset, we
empirically evaluate the capabilities of large language models in classifying
grounding acts and identifying grounded information items within a knowledge
graph structure. Our findings offer insights into how these models use
in-context learning for conversational grounding tasks and common prediction
errors, which we illustrate with examples from challenging dialogues. We
discuss how the models handle knowledge graphs as a semantic layer between
unstructured dialogue utterances and structured information items.","[{'name': 'Phillip Schneider'}, {'name': 'Nektarios Machner'}, {'name': 'Kristiina Jokinen'}, {'name': 'Florian Matthes'}]",2024-08-02T08:07:15Z
http://arxiv.org/abs/2408.01084v1,http://arxiv.org/abs/2408.01084v1,"Adaptive Contrastive Decoding in Retrieval-Augmented Generation for
  Handling Noisy Contexts","When using large language models (LLMs) in knowledge-intensive tasks, such as
open-domain question answering, external context can bridge a gap between
external knowledge and LLM's parametric knowledge. Recent research has been
developed to amplify contextual knowledge over the parametric knowledge of LLM
with contrastive decoding approaches. While these approaches could yield
truthful responses when relevant context is provided, they are prone to
vulnerabilities when faced with noisy contexts. We extend the scope of previous
studies to encompass noisy contexts and propose adaptive contrastive decoding
(ACD) to leverage contextual influence effectively. ACD demonstrates
improvements in open-domain question answering tasks compared to baselines,
especially in robustness by remaining undistracted by noisy contexts in
retrieval-augmented generation.","[{'name': 'Youna Kim'}, {'name': 'Hyuhng Joon Kim'}, {'name': 'Cheonbok Park'}, {'name': 'Choonghyun Park'}, {'name': 'Hyunsoo Cho'}, {'name': 'Junyeob Kim'}, {'name': 'Kang Min Yoo'}, {'name': 'Sang-goo Lee'}, {'name': 'Taeuk Kim'}]",2024-08-02T08:03:38Z
http://arxiv.org/abs/2408.01063v1,http://arxiv.org/abs/2408.01063v1,"Leveraging Large Language Models for Mobile App Review Feature
  Extraction","Mobile app review analysis presents unique challenges due to the low quality,
subjective bias, and noisy content of user-generated documents. Extracting
features from these reviews is essential for tasks such as feature
prioritization and sentiment analysis, but it remains a challenging task.
Meanwhile, encoder-only models based on the Transformer architecture have shown
promising results for classification and information extraction tasks for
multiple software engineering processes. This study explores the hypothesis
that encoder-only large language models can enhance feature extraction from
mobile app reviews. By leveraging crowdsourced annotations from an industrial
context, we redefine feature extraction as a supervised token classification
task. Our approach includes extending the pre-training of these models with a
large corpus of user reviews to improve contextual understanding and employing
instance selection techniques to optimize model fine-tuning. Empirical
evaluations demonstrate that this method improves the precision and recall of
extracted features and enhances performance efficiency. Key contributions
include a novel approach to feature extraction, annotated datasets, extended
pre-trained models, and an instance selection mechanism for cost-effective
fine-tuning. This research provides practical methods and empirical evidence in
applying large language models to natural language processing tasks within
mobile app reviews, offering improved performance in feature extraction.","[{'name': 'Quim Motger'}, {'name': 'Alessio Miaschi'}, {'name': ""Felice Dell'Orletta""}, {'name': 'Xavier Franch'}, {'name': 'Jordi Marco'}]",2024-08-02T07:31:57Z
http://arxiv.org/abs/2408.01050v1,http://arxiv.org/abs/2408.01050v1,"The Impact of Hyperparameters on Large Language Model Inference
  Performance: An Evaluation of vLLM and HuggingFace Pipelines","The recent surge of open-source large language models (LLMs) enables
developers to create AI-based solutions while maintaining control over aspects
such as privacy and compliance, thereby providing governance and ownership of
the model deployment process. To utilize these LLMs, inference engines are
needed. These engines load the model's weights onto available resources, such
as GPUs, and process queries to generate responses. The speed of inference, or
performance, of the LLM, is critical for real-time applications, as it computes
millions or billions of floating point operations per inference. Recently,
advanced inference engines such as vLLM have emerged, incorporating novel
mechanisms such as efficient memory management to achieve state-of-the-art
performance. In this paper, we analyze the performance, particularly the
throughput (tokens generated per unit of time), of 20 LLMs using two inference
libraries: vLLM and HuggingFace's pipelines. We investigate how various
hyperparameters, which developers must configure, influence inference
performance. Our results reveal that throughput landscapes are irregular, with
distinct peaks, highlighting the importance of hyperparameter optimization to
achieve maximum performance. We also show that applying hyperparameter
optimization when upgrading or downgrading the GPU model used for inference can
improve throughput from HuggingFace pipelines by an average of 9.16% and 13.7%,
respectively.",[{'name': 'Matias Martinez'}],2024-08-02T06:56:59Z
http://arxiv.org/abs/2408.01046v1,http://arxiv.org/abs/2408.01046v1,QUDSELECT: Selective Decoding for Questions Under Discussion Parsing,"Question Under Discussion (QUD) is a discourse framework that uses implicit
questions to reveal discourse relationships between sentences. In QUD parsing,
each sentence is viewed as an answer to a question triggered by an anchor
sentence in prior context. The resulting QUD structure is required to conform
to several theoretical criteria like answer compatibility (how well the
question is answered), making QUD parsing a challenging task. Previous works
construct QUD parsers in a pipelined manner (i.e. detect the trigger sentence
in context and then generate the question). However, these parsers lack a
holistic view of the task and can hardly satisfy all the criteria. In this
work, we introduce QUDSELECT, a joint-training framework that selectively
decodes the QUD dependency structures considering the QUD criteria. Using
instruction-tuning, we train models to simultaneously predict the anchor
sentence and generate the associated question. To explicitly incorporate the
criteria, we adopt a selective decoding strategy of sampling multiple QUD
candidates during inference, followed by selecting the best one with criteria
scorers. Our method outperforms the state-of-the-art baseline models by 9% in
human evaluation and 4% in automatic evaluation, demonstrating the
effectiveness of our framework.","[{'name': 'Ashima Suvarna'}, {'name': 'Xiao Liu'}, {'name': 'Tanmay Parekh'}, {'name': 'Kai-Wei Chang'}, {'name': 'Nanyun Peng'}]",2024-08-02T06:46:08Z
http://arxiv.org/abs/2408.01038v2,http://arxiv.org/abs/2408.01038v2,"UNER: A Unified Prediction Head for Named Entity Recognition in
  Visually-rich Documents","The recognition of named entities in visually-rich documents (VrD-NER) plays
a critical role in various real-world scenarios and applications. However, the
research in VrD-NER faces three major challenges: complex document layouts,
incorrect reading orders, and unsuitable task formulations. To address these
challenges, we propose a query-aware entity extraction head, namely UNER, to
collaborate with existing multi-modal document transformers to develop more
robust VrD-NER models. The UNER head considers the VrD-NER task as a
combination of sequence labeling and reading order prediction, effectively
addressing the issues of discontinuous entities in documents. Experimental
evaluations on diverse datasets demonstrate the effectiveness of UNER in
improving entity extraction performance. Moreover, the UNER head enables a
supervised pre-training stage on various VrD-NER datasets to enhance the
document transformer backbones and exhibits substantial knowledge transfer from
the pre-training stage to the fine-tuning stage. By incorporating universal
layout understanding, a pre-trained UNER-based model demonstrates significant
advantages in few-shot and cross-linguistic scenarios and exhibits zero-shot
entity extraction abilities.","[{'name': 'Yi Tu'}, {'name': 'Chong Zhang'}, {'name': 'Ya Guo'}, {'name': 'Huan Chen'}, {'name': 'Jinyang Tang'}, {'name': 'Huijia Zhu'}, {'name': 'Qi Zhang'}]",2024-08-02T06:21:36Z
http://arxiv.org/abs/2408.01005v1,http://arxiv.org/abs/2408.01005v1,"Enhancing Financial Market Predictions: Causality-Driven Feature
  Selection","This paper introduces the FinSen dataset that revolutionizes financial market
analysis by integrating economic and financial news articles from 197 countries
with stock market data. The dataset's extensive coverage spans 15 years from
2007 to 2023 with temporal information, offering a rich, global perspective
with 160,000 records on financial market news. Our study leverages causally
validated sentiment scores and LSTM models to enhance market forecast accuracy
and reliability. Utilizing the FinSen dataset, we introduce an innovative Focal
Calibration Loss, reducing Expected Calibration Error (ECE) to 3.34 percent
with the DAN 3 model. This not only improves prediction accuracy but also
aligns probabilistic forecasts closely with real outcomes, crucial for the
financial sector where predicted probability is paramount. Our approach
demonstrates the effectiveness of combining sentiment analysis with precise
calibration techniques for trustworthy financial forecasting where the cost of
misinterpretation can be high. Finsen Data can be found at [this github
URL](https://github.com/EagleAdelaide/FinSen_Dataset.git).","[{'name': 'Wenhao Liang'}, {'name': 'Zhengyang Li'}, {'name': 'Weitong Chen'}]",2024-08-02T04:40:15Z
http://arxiv.org/abs/2408.00994v1,http://arxiv.org/abs/2408.00994v1,"ArchCode: Incorporating Software Requirements in Code Generation with
  Large Language Models","This paper aims to extend the code generation capability of large language
models (LLMs) to automatically manage comprehensive software requirements from
given textual descriptions. Such requirements include both functional (i.e.
achieving expected behavior for inputs) and non-functional (e.g., time/space
performance, robustness, maintainability) requirements. However, textual
descriptions can either express requirements verbosely or may even omit some of
them. We introduce ARCHCODE, a novel framework that leverages in-context
learning to organize requirements observed in descriptions and to extrapolate
unexpressed requirements from them. ARCHCODE generates requirements from given
descriptions, conditioning them to produce code snippets and test cases. Each
test case is tailored to one of the requirements, allowing for the ranking of
code snippets based on the compliance of their execution results with the
requirements. Public benchmarks show that ARCHCODE enhances to satisfy
functional requirements, significantly improving Pass@k scores. Furthermore, we
introduce HumanEval-NFR, the first evaluation of LLMs' non-functional
requirements in code generation, demonstrating ARCHCODE's superiority over
baseline methods. The implementation of ARCHCODE and the HumanEval-NFR
benchmark are both publicly accessible.","[{'name': 'Hojae Han'}, {'name': 'Jaejin Kim'}, {'name': 'Jaeseok Yoo'}, {'name': 'Youngwon Lee'}, {'name': 'Seung-won Hwang'}]",2024-08-02T03:54:36Z
http://arxiv.org/abs/2408.00992v3,http://arxiv.org/abs/2408.00992v3,Fairness in Large Language Models in Three Hours,"Large Language Models (LLMs) have demonstrated remarkable success across
various domains but often lack fairness considerations, potentially leading to
discriminatory outcomes against marginalized populations. Unlike fairness in
traditional machine learning, fairness in LLMs involves unique backgrounds,
taxonomies, and fulfillment techniques. This tutorial provides a systematic
overview of recent advances in the literature concerning fair LLMs, beginning
with real-world case studies to introduce LLMs, followed by an analysis of bias
causes therein. The concept of fairness in LLMs is then explored, summarizing
the strategies for evaluating bias and the algorithms designed to promote
fairness. Additionally, resources for assessing bias in LLMs, including
toolkits and datasets, are compiled, and current research challenges and open
questions in the field are discussed. The repository is available at
\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.","[{'name': 'Thang Doan Viet'}, {'name': 'Zichong Wang'}, {'name': 'Minh Nhat Nguyen'}, {'name': 'Wenbin Zhang'}]",2024-08-02T03:44:14Z
http://arxiv.org/abs/2408.00981v2,http://arxiv.org/abs/2408.00981v2,Cross-domain Named Entity Recognition via Graph Matching,"Cross-domain NER is a practical yet challenging problem since the data
scarcity in the real-world scenario. A common practice is first to learn a NER
model in a rich-resource general domain and then adapt the model to specific
domains. Due to the mismatch problem between entity types across domains, the
wide knowledge in the general domain can not effectively transfer to the target
domain NER model. To this end, we model the label relationship as a probability
distribution and construct label graphs in both source and target label spaces.
To enhance the contextual representation with label structures, we fuse the
label graph into the word embedding output by BERT. By representing label
relationships as graphs, we formulate cross-domain NER as a graph matching
problem. Furthermore, the proposed method has good applicability with
pre-training methods and is potentially capable of other cross-domain
prediction tasks. Empirical results on four datasets show that our method
outperforms a series of transfer learning, multi-task learning, and few-shot
learning methods.","[{'name': 'Junhao Zheng'}, {'name': 'Haibin Chen'}, {'name': 'Qianli Ma'}]",2024-08-02T02:31:54Z
http://arxiv.org/abs/2408.00966v1,http://arxiv.org/abs/2408.00966v1,"Automatic Extraction of Relationships among Motivations, Emotions and
  Actions from Natural Language Texts","We propose a new graph-based framework to reveal relationships among
motivations, emotions and actions explicitly given natural language texts. A
directed acyclic graph is designed to describe human's nature. Nurture beliefs
are incorporated to connect outside events and the human's nature graph. No
annotation resources are required due to the power of large language models.
Amazon Fine Foods Reviews dataset is used as corpus and food-related
motivations are focused. Totally 92,990 relationship graphs are generated, of
which 63% make logical sense. We make further analysis to investigate error
types for optimization direction in future research.",[{'name': 'Fei Yang'}],2024-08-02T01:22:46Z
http://arxiv.org/abs/2408.00960v1,http://arxiv.org/abs/2408.00960v1,"PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized
  Language Prompting","Understanding the nuances of a user's extensive interaction history is key to
building accurate and personalized natural language systems that can adapt to
evolving user preferences. To address this, we introduce PERSOMA, Personalized
Soft Prompt Adapter architecture. Unlike previous personalized prompting
methods for large language models, PERSOMA offers a novel approach to
efficiently capture user history. It achieves this by resampling and
compressing interactions as free form text into expressive soft prompt
embeddings, building upon recent research utilizing embedding representations
as input for LLMs. We rigorously validate our approach by evaluating various
adapter architectures, first-stage sampling strategies, parameter-efficient
tuning techniques like LoRA, and other personalization methods. Our results
demonstrate PERSOMA's superior ability to handle large and complex user
histories compared to existing embedding-based and text-prompt-based
techniques.","[{'name': 'Liam Hebert'}, {'name': 'Krishna Sayana'}, {'name': 'Ambarish Jash'}, {'name': 'Alexandros Karatzoglou'}, {'name': 'Sukhdeep Sodhi'}, {'name': 'Sumanth Doddapaneni'}, {'name': 'Yanli Cai'}, {'name': 'Dima Kuzmin'}]",2024-08-02T00:24:22Z
http://arxiv.org/abs/2408.00948v1,http://arxiv.org/abs/2408.00948v1,"Leveraging Large Language Models (LLMs) for Traffic Management at Urban
  Intersections: The Case of Mixed Traffic Scenarios","Urban traffic management faces significant challenges due to the dynamic
environments, and traditional algorithms fail to quickly adapt to this
environment in real-time and predict possible conflicts. This study explores
the ability of a Large Language Model (LLM), specifically, GPT-4o-mini to
improve traffic management at urban intersections. We recruited GPT-4o-mini to
analyze, predict position, detect and resolve the conflicts at an intersection
in real-time for various basic scenarios. The key findings of this study to
investigate whether LLMs can logically reason and understand the scenarios to
enhance the traffic efficiency and safety by providing real-time analysis. The
study highlights the potential of LLMs in urban traffic management creating
more intelligent and more adaptive systems. Results showed the GPT-4o-mini was
effectively able to detect and resolve conflicts in heavy traffic, congestion,
and mixed-speed conditions. The complex scenario of multiple intersections with
obstacles and pedestrians saw successful conflict management as well. Results
show that the integration of LLMs promises to improve the effectiveness of
traffic control for safer and more efficient urban intersection management.","[{'name': 'Sari Masri'}, {'name': 'Huthaifa I. Ashqar'}, {'name': 'Mohammed Elhenawy'}]",2024-08-01T23:06:06Z
http://arxiv.org/abs/2408.00932v1,http://arxiv.org/abs/2408.00932v1,"Towards Zero-Shot Annotation of the Built Environment with
  Vision-Language Models (Vision Paper)","Equitable urban transportation applications require high-fidelity digital
representations of the built environment: not just streets and sidewalks, but
bike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions,
traffic signals, signage, street markings, potholes, and more. Direct
inspections and manual annotations are prohibitively expensive at scale.
Conventional machine learning methods require substantial annotated training
data for adequate performance. In this paper, we consider vision language
models as a mechanism for annotating diverse urban features from satellite
images, reducing the dependence on human annotation to produce large training
sets. While these models have achieved impressive results in describing common
objects in images captured from a human perspective, their training sets are
less likely to include strong signals for esoteric features in the built
environment, and their performance in these settings is therefore unclear. We
demonstrate proof-of-concept combining a state-of-the-art vision language model
and variants of a prompting strategy that asks the model to consider segmented
elements independently of the original image. Experiments on two urban features
-- stop lines and raised tables -- show that while direct zero-shot prompting
correctly annotates nearly zero images, the pre-segmentation strategies can
annotate images with near 40% intersection-over-union accuracy. We describe how
these results inform a new research agenda in automatic annotation of the built
environment to improve equity, accessibility, and safety at broad scale and in
diverse environments.","[{'name': 'Bin Han'}, {'name': 'Yiwei Yang'}, {'name': 'Anat Caspi'}, {'name': 'Bill Howe'}]",2024-08-01T21:50:23Z
http://arxiv.org/abs/2408.00921v1,http://arxiv.org/abs/2408.00921v1,"Automatic Pull Request Description Generation Using LLMs: A T5 Model
  Approach","Developers create pull request (PR) descriptions to provide an overview of
their changes and explain the motivations behind them. These descriptions help
reviewers and fellow developers quickly understand the updates. Despite their
importance, some developers omit these descriptions. To tackle this problem, we
propose an automated method for generating PR descriptions based on commit
messages and source code comments. This method frames the task as a text
summarization problem, for which we utilized the T5 text-to-text transfer
model. We fine-tuned a pre-trained T5 model using a dataset containing 33,466
PRs. The model's effectiveness was assessed using ROUGE metrics, which are
recognized for their strong alignment with human evaluations. Our findings
reveal that the T5 model significantly outperforms LexRank, which served as our
baseline for comparison.","[{'name': 'Md Nazmus Sakib'}, {'name': 'Md Athikul Islam'}, {'name': 'Md Mashrur Arifin'}]",2024-08-01T21:22:16Z
http://arxiv.org/abs/2408.04643v1,http://arxiv.org/abs/2408.04643v1,"Risks, Causes, and Mitigations of Widespread Deployments of Large
  Language Models (LLMs): A Survey","Recent advancements in Large Language Models (LLMs), such as ChatGPT and
LLaMA, have significantly transformed Natural Language Processing (NLP) with
their outstanding abilities in text generation, summarization, and
classification. Nevertheless, their widespread adoption introduces numerous
challenges, including issues related to academic integrity, copyright,
environmental impacts, and ethical considerations such as data bias, fairness,
and privacy. The rapid evolution of LLMs also raises concerns regarding the
reliability and generalizability of their evaluations. This paper offers a
comprehensive survey of the literature on these subjects, systematically
gathered and synthesized from Google Scholar. Our study provides an in-depth
analysis of the risks associated with specific LLMs, identifying sub-risks,
their causes, and potential solutions. Furthermore, we explore the broader
challenges related to LLMs, detailing their causes and proposing mitigation
strategies. Through this literature analysis, our survey aims to deepen the
understanding of the implications and complexities surrounding these powerful
models.","[{'name': 'Md Nazmus Sakib'}, {'name': 'Md Athikul Islam'}, {'name': 'Royal Pathak'}, {'name': 'Md Mashrur Arifin'}]",2024-08-01T21:21:18Z
http://arxiv.org/abs/2408.00914v1,http://arxiv.org/abs/2408.00914v1,"Granting GPT-4 License and Opportunity: Enhancing Accuracy and
  Confidence Estimation for Few-Shot Event Detection","Large Language Models (LLMs) such as GPT-4 have shown enough promise in the
few-shot learning context to suggest use in the generation of ""silver"" data and
refinement of new ontologies through iterative application and review. Such
workflows become more effective with reliable confidence estimation.
Unfortunately, confidence estimation is a documented weakness of models such as
GPT-4, and established methods to compensate require significant additional
complexity and computation. The present effort explores methods for effective
confidence estimation with GPT-4 with few-shot learning for event detection in
the BETTER ontology as a vehicle. The key innovation is expanding the prompt
and task presented to GPT-4 to provide License to speculate when unsure and
Opportunity to quantify and explain its uncertainty (L&O). This approach
improves accuracy and provides usable confidence measures (0.759 AUC) with no
additional machinery.","[{'name': 'Steven Fincke'}, {'name': 'Adrien Bibal'}, {'name': 'Elizabeth Boschee'}]",2024-08-01T21:08:07Z
http://arxiv.org/abs/2408.00884v1,http://arxiv.org/abs/2408.00884v1,Hybrid Querying Over Relational Databases and Large Language Models,"Database queries traditionally operate under the closed-world assumption,
providing no answers to questions that require information beyond the data
stored in the database. Hybrid querying using SQL offers an alternative by
integrating relational databases with large language models (LLMs) to answer
beyond-database questions. In this paper, we present the first cross-domain
benchmark, SWAN, containing 120 beyond-database questions over four real-world
databases. To leverage state-of-the-art language models in addressing these
complex questions in SWAN, we present, HQDL, a preliminary solution for hybrid
querying, and also discuss potential future directions. Our evaluation
demonstrates that HQDL using GPT-4 Turbo with few-shot prompts, achieves 40.0\%
in execution accuracy and 48.2\% in data factuality. These results highlights
both the potential and challenges for hybrid querying. We believe that our work
will inspire further research in creating more efficient and accurate data
systems that seamlessly integrate relational databases and large language
models to address beyond-database questions.","[{'name': 'Fuheng Zhao'}, {'name': 'Divyakant Agrawal'}, {'name': 'Amr El Abbadi'}]",2024-08-01T19:29:18Z
http://arxiv.org/abs/2408.00863v1,http://arxiv.org/abs/2408.00863v1,"UniMoT: Unified Molecule-Text Language Model with Discrete Token
  Representation","The remarkable success of Large Language Models (LLMs) across diverse tasks
has driven the research community to extend their capabilities to molecular
applications. However, most molecular LLMs employ adapter-based architectures
that do not treat molecule and text modalities equally and lack a supervision
signal for the molecule modality. To address these issues, we introduce UniMoT,
a Unified Molecule-Text LLM adopting a tokenizer-based architecture that
expands the vocabulary of LLM with molecule tokens. Specifically, we introduce
a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge
the modality gap between molecule and text. This tokenizer transforms molecules
into sequences of molecule tokens with causal dependency, encapsulating
high-level molecular and textual information. Equipped with this tokenizer,
UniMoT can unify molecule and text modalities under a shared token
representation and an autoregressive training paradigm, enabling it to
interpret molecules as a foreign language and generate them as text. Following
a four-stage training scheme, UniMoT emerges as a multi-modal generalist
capable of performing both molecule-to-text and text-to-molecule tasks.
Extensive experiments demonstrate that UniMoT achieves state-of-the-art
performance across a wide range of molecule comprehension and generation tasks.","[{'name': 'Juzheng Zhang'}, {'name': 'Yatao Bian'}, {'name': 'Yongqiang Chen'}, {'name': 'Quanming Yao'}]",2024-08-01T18:31:31Z
http://arxiv.org/abs/2408.00765v1,http://arxiv.org/abs/2408.00765v1,"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models
  for Integrated Capabilities","MM-Vet, with open-ended vision-language questions targeting at evaluating
integrated capabilities, has become one of the most popular benchmarks for
large multimodal model evaluation. MM-Vet assesses six core vision-language
(VL) capabilities: recognition, knowledge, spatial awareness, language
generation, OCR, and math. However, its question format is restricted to single
image-text pairs, lacking the interleaved image and text sequences prevalent in
real-world scenarios. To address this limitation, we introduce MM-Vet v2, which
includes a new VL capability called ""image-text sequence understanding"",
evaluating models' ability to process VL sequences. Furthermore, we maintain
the high quality of evaluation samples while further expanding the evaluation
set size. Using MM-Vet v2 to benchmark large multimodal models, we found that
Claude 3.5 Sonnet is the best model with a score of 71.8, slightly
outperforming GPT-4o which scored 71.0. Among open-weight models,
InternVL2-Llama3-76B leads with a score of 68.4.","[{'name': 'Weihao Yu'}, {'name': 'Zhengyuan Yang'}, {'name': 'Linfeng Ren'}, {'name': 'Linjie Li'}, {'name': 'Jianfeng Wang'}, {'name': 'Kevin Lin'}, {'name': 'Chung-Ching Lin'}, {'name': 'Zicheng Liu'}, {'name': 'Lijuan Wang'}, {'name': 'Xinchao Wang'}]",2024-08-01T17:59:54Z
http://arxiv.org/abs/2408.00764v1,http://arxiv.org/abs/2408.00764v1,"AgentGen: Enhancing Planning Abilities for Large Language Model based
  Agent via Environment and Task Generation","Large Language Model (LLM) based agents have garnered significant attention
and are becoming increasingly popular. Furthermore, planning ability is a
crucial component of an LLM-based agent, involving interaction with the
environment and executing actions to complete a planning task, which generally
entails achieving a desired goal from an initial state. This paper investigates
enhancing the planning abilities of LLMs through instruction tuning, referred
to as agent training. Recent studies have demonstrated that utilizing
expert-level trajectory for instruction-tuning LLMs effectively enhances their
planning capabilities. However, existing work primarily focuses on synthesizing
trajectories from manually designed planning tasks and environments. The
labor-intensive nature of creating these environments and tasks impedes the
generation of sufficiently varied and extensive trajectories. To address this
limitation, this paper explores the automated synthesis of diverse environments
and a gradual range of planning tasks, from easy to difficult. We introduce a
framework, AgentGen, that leverages LLMs first to generate environments and
subsequently generate planning tasks conditioned on these environments.
Specifically, to improve environmental diversity, we propose using an
inspiration corpus composed of various domain-specific text segments as the
context for synthesizing environments. Moreover, to increase the difficulty
diversity of generated planning tasks, we propose a bidirectional evolution
method, Bi-Evol, that evolves planning tasks from easier and harder directions
to synthesize a task set with a smoother difficulty curve. The evaluation
results derived from AgentBoard show that AgentGen greatly improves LLMs'
planning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses
GPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms
GPT-4.","[{'name': 'Mengkang Hu'}, {'name': 'Pu Zhao'}, {'name': 'Can Xu'}, {'name': 'Qingfeng Sun'}, {'name': 'Jianguang Lou'}, {'name': 'Qingwei Lin'}, {'name': 'Ping Luo'}, {'name': 'Saravan Rajmohan'}, {'name': 'Dongmei Zhang'}]",2024-08-01T17:59:46Z
http://arxiv.org/abs/2408.00761v2,http://arxiv.org/abs/2408.00761v2,Tamper-Resistant Safeguards for Open-Weight LLMs,"Rapid advances in the capabilities of large language models (LLMs) have
raised widespread concerns regarding their potential for malicious use.
Open-weight LLMs present unique challenges, as existing safeguards lack
robustness to tampering attacks that modify model weights. For example, recent
works have demonstrated that refusal and unlearning safeguards can be trivially
removed with a few steps of fine-tuning. These vulnerabilities necessitate new
approaches for enabling the safe release of open-weight LLMs. We develop a
method, called TAR, for building tamper-resistant safeguards into open-weight
LLMs such that adversaries cannot remove the safeguards even after thousands of
steps of fine-tuning. In extensive evaluations and red teaming analyses, we
find that our method greatly improves tamper-resistance while preserving benign
capabilities. Our results demonstrate that tamper-resistance is a tractable
problem, opening up a promising new avenue to improve the safety and security
of open-weight LLMs.","[{'name': 'Rishub Tamirisa'}, {'name': 'Bhrugu Bharathi'}, {'name': 'Long Phan'}, {'name': 'Andy Zhou'}, {'name': 'Alice Gatti'}, {'name': 'Tarun Suresh'}, {'name': 'Maxwell Lin'}, {'name': 'Justin Wang'}, {'name': 'Rowan Wang'}, {'name': 'Ron Arel'}, {'name': 'Andy Zou'}, {'name': 'Dawn Song'}, {'name': 'Bo Li'}, {'name': 'Dan Hendrycks'}, {'name': 'Mantas Mazeika'}]",2024-08-01T17:59:12Z
http://arxiv.org/abs/2408.00728v1,http://arxiv.org/abs/2408.00728v1,CERT-ED: Certifiably Robust Text Classification for Edit Distance,"With the growing integration of AI in daily life, ensuring the robustness of
systems to inference-time attacks is crucial. Among the approaches for
certifying robustness to such adversarial examples, randomized smoothing has
emerged as highly promising due to its nature as a wrapper around arbitrary
black-box models. Previous work on randomized smoothing in natural language
processing has primarily focused on specific subsets of edit distance
operations, such as synonym substitution or word insertion, without exploring
the certification of all edit operations. In this paper, we adapt Randomized
Deletion (Huang et al., 2023) and propose, CERTified Edit Distance defense
(CERT-ED) for natural language classification. Through comprehensive
experiments, we demonstrate that CERT-ED outperforms the existing Hamming
distance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of
both accuracy and the cardinality of the certificate. By covering various
threat models, including 5 direct and 5 transfer attacks, our method improves
empirical robustness in 38 out of 50 settings.","[{'name': 'Zhuoqun Huang'}, {'name': 'Neil G Marchant'}, {'name': 'Olga Ohrimenko'}, {'name': 'Benjamin I. P. Rubinstein'}]",2024-08-01T17:20:24Z
http://arxiv.org/abs/2408.00727v1,http://arxiv.org/abs/2408.00727v1,"Improving Retrieval-Augmented Generation in Medicine with Iterative
  Follow-up Questions","The emergent abilities of large language models (LLMs) have demonstrated
great potential in solving medical questions. They can possess considerable
medical knowledge, but may still hallucinate and are inflexible in the
knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed
to enhance the medical question-answering capabilities of LLMs with external
knowledge bases, it may still fail in complex cases where multiple rounds of
information-seeking are required. To address such an issue, we propose
iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up
queries based on previous information-seeking attempts. In each iteration of
i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and
they will be further used to guide the query generation in the next iteration.
Our experiments show the improved performance of various LLMs brought by
i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes
in the United States Medical Licensing Examination (USMLE), as well as various
knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.
Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and
fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\% on the MedQA
dataset. In addition, we characterize the scaling properties of i-MedRAG with
different iterations of follow-up queries and different numbers of queries per
iteration. Our case studies show that i-MedRAG can flexibly ask follow-up
queries to form reasoning chains, providing an in-depth analysis of medical
questions. To the best of our knowledge, this is the first-of-its-kind study on
incorporating follow-up queries into medical RAG.","[{'name': 'Guangzhi Xiong'}, {'name': 'Qiao Jin'}, {'name': 'Xiao Wang'}, {'name': 'Minjia Zhang'}, {'name': 'Zhiyong Lu'}, {'name': 'Aidong Zhang'}]",2024-08-01T17:18:17Z
http://arxiv.org/abs/2408.00690v2,http://arxiv.org/abs/2408.00690v2,"Improving Text Embeddings for Smaller Language Models Using Contrastive
  Fine-tuning","While Large Language Models show remarkable performance in natural language
understanding, their resource-intensive nature makes them less accessible. In
contrast, smaller language models such as MiniCPM offer more sustainable
scalability, but often underperform without specialized optimization. In this
paper, we explore the enhancement of smaller language models through the
improvement of their text embeddings. We select three language models, MiniCPM,
Phi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Our
results demonstrate that this fine-tuning method enhances the quality of text
embeddings for all three models across various benchmarks, with MiniCPM showing
the most significant improvements of an average 56.33% performance gain. The
contrastive fine-tuning code is publicly available at
https://github.com/trapoom555/Language-Model-STS-CFT.","[{'name': 'Trapoom Ukarapol'}, {'name': 'Zhicheng Lee'}, {'name': 'Amy Xin'}]",2024-08-01T16:31:35Z
http://arxiv.org/abs/2408.00684v1,http://arxiv.org/abs/2408.00684v1,"Assessing the Variety of a Concept Space Using an Unbiased Estimate of
  Rao's Quadratic Index","Past research relates design creativity to 'divergent thinking,' i.e., how
well the concept space is explored during the early phase of design.
Researchers have argued that generating several concepts would increase the
chances of producing better design solutions. 'Variety' is one of the
parameters by which one can quantify the breadth of a concept space explored by
the designers. It is useful to assess variety at the conceptual design stage
because, at this stage, designers have the freedom to explore different
solution principles so as to satisfy a design problem with substantially novel
concepts. This article elaborates on and critically examines the existing
variety metrics from the engineering design literature, discussing their
limitations. A new distance-based variety metric is proposed, along with a
prescriptive framework to support the assessment process. This framework uses
the SAPPhIRE model of causality as a knowledge representation scheme to measure
the real-valued distance between two design concepts. The proposed framework is
implemented in a software tool called 'VariAnT.' Furthermore, the tool's
application is demonstrated through an illustrative example.","[{'name': 'Anubhab Majumder'}, {'name': 'Ujjwal Pal'}, {'name': 'Amaresh Chakrabarti'}]",2024-08-01T16:25:54Z
http://arxiv.org/abs/2408.00675v1,http://arxiv.org/abs/2408.00675v1,Leveraging Entailment Judgements in Cross-Lingual Summarisation,"Synthetically created Cross-Lingual Summarisation (CLS) datasets are prone to
include document-summary pairs where the reference summary is unfaithful to the
corresponding document as it contains content not supported by the document
(i.e., hallucinated content). This low data quality misleads model learning and
obscures evaluation results. Automatic ways to assess hallucinations and
improve training have been proposed for monolingual summarisation,
predominantly in English. For CLS, we propose to use off-the-shelf
cross-lingual Natural Language Inference (X-NLI) to evaluate faithfulness of
reference and model generated summaries. Then, we study training approaches
that are aware of faithfulness issues in the training data and propose an
approach that uses unlikelihood loss to teach a model about unfaithful summary
sequences. Our results show that it is possible to train CLS models that yield
more faithful summaries while maintaining comparable or better informativess.","[{'name': 'Huajian Zhang'}, {'name': 'Laura Perez-Beltrachini'}]",2024-08-01T16:18:09Z
http://arxiv.org/abs/2408.00662v1,http://arxiv.org/abs/2408.00662v1,Aligning Multiple Knowledge Graphs in a Single Pass,"Entity alignment (EA) is to identify equivalent entities across different
knowledge graphs (KGs), which can help fuse these KGs into a more comprehensive
one. Previous EA methods mainly focus on aligning a pair of KGs, and to the
best of our knowledge, no existing EA method considers aligning multiple (more
than two) KGs. To fill this research gap, in this work, we study a novel
problem of aligning multiple KGs and propose an effective framework named
MultiEA to solve the problem. First, we embed the entities of all the candidate
KGs into a common feature space by a shared KG encoder. Then, we explore three
alignment strategies to minimize the distances among pre-aligned entities. In
particular, we propose an innovative inference enhancement technique to improve
the alignment performance by incorporating high-order similarities. Finally, to
verify the effectiveness of MultiEA, we construct two new real-world benchmark
datasets and conduct extensive experiments on them. The results show that our
MultiEA can effectively and efficiently align multiple KGs in a single pass.","[{'name': 'Yaming Yang'}, {'name': 'Zhe Wang'}, {'name': 'Ziyu Guan'}, {'name': 'Wei Zhao'}, {'name': 'Weigang Lu'}, {'name': 'Xinyan Huang'}]",2024-08-01T15:58:05Z
http://arxiv.org/abs/2408.00655v5,http://arxiv.org/abs/2408.00655v5,"SentenceVAE: Enable Next-sentence Prediction for Large Language Models
  with Faster Speed, Higher Accuracy and Longer Context","Current large language models (LLMs) primarily utilize next-token prediction
method for inference, which significantly impedes their processing speed. In
this paper, we introduce a novel inference methodology termed next-sentence
prediction, aiming at enhancing the inference efficiency of LLMs. We present
Sentence Variational Autoencoder (SentenceVAE), which includes a Sentence
Encoder to compress multiple tokens in a sentence into a single token, and a
Sentence Decoder to reconstruct it. By integrating SentenceVAE into the input
and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a
sentence-by-sentence inference method. In addition, the SentenceVAE module of
SLLMs can maintain the integrity of the original semantic content by segmenting
the context into sentences, thereby improving accuracy while boosting inference
speed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over
equivalent context length, significantly reducing memory demands for
self-attention computation and facilitating the handling of longer context.
Extensive experiments on Wanjuan dataset have revealed that the proposed method
can accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%
of its original metric, and decrease memory overhead by 86~91% for the
equivalent context length, compared to previous token-by-token methods.","[{'name': 'Hongjun An'}, {'name': 'Yifan Chen'}, {'name': 'Zhe Sun'}, {'name': 'Xuelong Li'}]",2024-08-01T15:45:19Z
http://arxiv.org/abs/2408.00612v1,http://arxiv.org/abs/2408.00612v1,Downstream bias mitigation is all you need,"The advent of transformer-based architectures and large language models
(LLMs) have significantly advanced the performance of natural language
processing (NLP) models. Since these LLMs are trained on huge corpuses of data
from the web and other sources, there has been a major concern about harmful
prejudices that may potentially be transferred from the data. In many
applications, these pre-trained LLMs are fine-tuned on task specific datasets,
which can further contribute to biases. This paper studies the extent of biases
absorbed by LLMs during pre-training as well as task-specific behaviour after
fine-tuning. We found that controlled interventions on pre-trained LLMs, prior
to fine-tuning, have minimal effect on lowering biases in classifiers. However,
the biases present in domain-specific datasets play a much bigger role, and
hence mitigating them at this stage has a bigger impact. While pre-training
does matter, but after the model has been pre-trained, even slight changes to
co-occurrence rates in the fine-tuning dataset has a significant effect on the
bias of the model.","[{'name': 'Arkadeep Baksi'}, {'name': 'Rahul Singh'}, {'name': 'Tarun Joshi'}]",2024-08-01T14:52:04Z
http://arxiv.org/abs/2408.00584v1,http://arxiv.org/abs/2408.00584v1,"Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian
  Rebuses","Rebuses are puzzles requiring constrained multi-step reasoning to identify a
hidden phrase from a set of images and letters. In this work, we introduce a
large collection of verbalized rebuses for the Italian language and use it to
assess the rebus-solving capabilities of state-of-the-art large language
models. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly
on this task, ad-hoc fine-tuning seems to improve models' performance. However,
we find that performance gains from training are largely motivated by
memorization. Our results suggest that rebus solving remains a challenging test
bed to evaluate large language models' linguistic proficiency and sequential
instruction-following skills.","[{'name': 'Gabriele Sarti'}, {'name': 'Tommaso Caselli'}, {'name': 'Malvina Nissim'}, {'name': 'Arianna Bisazza'}]",2024-08-01T14:14:15Z
http://arxiv.org/abs/2408.00555v1,http://arxiv.org/abs/2408.00555v1,"Alleviating Hallucination in Large Vision-Language Models with Active
  Retrieval Augmentation","Despite the remarkable ability of large vision-language models (LVLMs) in
image comprehension, these models frequently generate plausible yet factually
incorrect responses, a phenomenon known as hallucination.Recently, in large
language models (LLMs), augmenting LLMs by retrieving information from external
knowledge resources has been proven as a promising solution to mitigate
hallucinations.However, the retrieval augmentation in LVLM significantly lags
behind the widespread applications of LVLM. Moreover, when transferred to
augmenting LVLMs, sometimes the hallucination degree of the model is even
exacerbated.Motivated by the research gap and counter-intuitive phenomenon, we
introduce a novel framework, the Active Retrieval-Augmented large
vision-language model (ARA), specifically designed to address hallucinations by
incorporating three critical dimensions: (i) dissecting the retrieval targets
based on the inherent hierarchical structures of images. (ii) pinpointing the
most effective retrieval methods and filtering out the reliable retrieval
results. (iii) timing the retrieval process to coincide with episodes of low
certainty, while circumventing unnecessary retrieval during periods of high
certainty. To assess the capability of our proposed ARA model in reducing
hallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and
mPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by
utilizing fitting retrieval mechanisms and timing the retrieval judiciously, we
can effectively mitigate the hallucination problem. We hope that this study can
provide deeper insights into how to adapt the retrieval augmentation to LVLMs
for reducing hallucinations with more effective retrieval and minimal retrieval
occurrences.","[{'name': 'Xiaoye Qu'}, {'name': 'Qiyuan Chen'}, {'name': 'Wei Wei'}, {'name': 'Jishuo Sun'}, {'name': 'Jianfeng Dong'}]",2024-08-01T13:38:58Z
http://arxiv.org/abs/2408.00550v1,http://arxiv.org/abs/2408.00550v1,Mitigating Multilingual Hallucination in Large Vision-Language Models,"While Large Vision-Language Models (LVLMs) have exhibited remarkable
capabilities across a wide range of tasks, they suffer from hallucination
problems, where models generate plausible yet incorrect answers given the input
image-query pair. This hallucination phenomenon is even more severe when
querying the image in non-English languages, while existing methods for
mitigating hallucinations in LVLMs only consider the English scenarios. In this
paper, we make the first attempt to mitigate this important multilingual
hallucination in LVLMs. With thorough experiment analysis, we found that
multilingual hallucination in LVLMs is a systemic problem that could arise from
deficiencies in multilingual capabilities or inadequate multimodal abilities.
To this end, we propose a two-stage Multilingual Hallucination Removal (MHR)
framework for LVLMs, aiming to improve resistance to hallucination for both
high-resource and low-resource languages. Instead of relying on the intricate
manual annotations of multilingual resources, we fully leverage the inherent
capabilities of the LVLM and propose a novel cross-lingual alignment method,
which generates multiple responses for each image-query input and then
identifies the hallucination-aware pairs for each language. These data pairs
are finally used for direct preference optimization to prompt the LVLMs to
favor non-hallucinating responses. Experimental results show that our MHR
achieves a substantial reduction in hallucination generation for LVLMs.
Notably, on our extended multilingual POPE benchmark, our framework delivers an
average increase of 19.0% in accuracy across 13 different languages. Our code
and model weights are available at https://github.com/ssmisya/MHR","[{'name': 'Xiaoye Qu'}, {'name': 'Mingyang Song'}, {'name': 'Wei Wei'}, {'name': 'Jianfeng Dong'}, {'name': 'Yu Cheng'}]",2024-08-01T13:34:35Z
http://arxiv.org/abs/2408.00539v1,http://arxiv.org/abs/2408.00539v1,Intermittent Semi-working Mask: A New Masking Paradigm for LLMs,"Multi-turn dialogues are a key interaction method between humans and Large
Language Models (LLMs), as conversations extend over multiple rounds, keeping
LLMs' high generation quality and low latency is a challenge. Mainstream LLMs
can be grouped into two categories based on masking strategy: causal LLM and
prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform
causal ones in scenarios that heavily depend on historical context such as
multi-turn dialogues or in-context learning, thanks to their bidirectional
attention on prefix sequences. However, prefix LLMs have an inherent
inefficient training problem in multi-turn dialogue datasets. In addition, the
attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV
Cache) across dialogue rounds to reduce generation latency. In this paper, we
propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to
address these problems. Specifically, we apply alternate bidirectional and
unidirectional attention on queries and answers in the dialogue history. In
this way, ISM is able to maintain the high quality of prefix LLM and low
generation latency of causal LLM, simultaneously. Extensive experiments
illustrate that our ISM achieves significant performance.","[{'name': 'Mingcong Lu'}, {'name': 'Jiangcai Zhu'}, {'name': 'Wang Hao'}, {'name': 'Zheng Li'}, {'name': 'Shusheng Zhang'}, {'name': 'Kailai Shao'}, {'name': 'Chao Chen'}, {'name': 'Nan Li'}, {'name': 'Feng Wang'}, {'name': 'Xin Lu'}]",2024-08-01T13:22:01Z
http://arxiv.org/abs/2408.00534v1,http://arxiv.org/abs/2408.00534v1,"The Monetisation of Toxicity: Analysing YouTube Content Creators and
  Controversy-Driven Engagement","YouTube is a major social media platform that plays a significant role in
digital culture, with content creators at its core. These creators often engage
in controversial behaviour to drive engagement, which can foster toxicity. This
paper presents a quantitative analysis of controversial content on YouTube,
focusing on the relationship between controversy, toxicity, and monetisation.
We introduce a curated dataset comprising 20 controversial YouTube channels
extracted from Reddit discussions, including 16,349 videos and more than 105
million comments. We identify and categorise monetisation cues from video
descriptions into various models, including affiliate marketing and direct
selling, using lists of URLs and keywords. Additionally, we train a machine
learning model to measure the toxicity of comments in these videos. Our
findings reveal that while toxic comments correlate with higher engagement,
they negatively impact monetisation, indicating that controversy-driven
interaction does not necessarily lead to financial gain. We also observed
significant variation in monetisation strategies, with some creators showing
extensive monetisation despite high toxicity levels. Our study introduces a
curated dataset, lists of URLs and keywords to categorise monetisation, a
machine learning model to measure toxicity, and is a significant step towards
understanding the complex relationship between controversy, engagement, and
monetisation on YouTube. The lists used for detecting and categorising
monetisation cues are available on https://github.com/thalesbertaglia/toxmon.","[{'name': 'Thales Bertaglia'}, {'name': 'Catalina Goanta'}, {'name': 'Adriana Iamnitchi'}]",2024-08-01T13:10:35Z
http://arxiv.org/abs/2408.00491v1,http://arxiv.org/abs/2408.00491v1,GalleryGPT: Analyzing Paintings with Large Multimodal Models,"Artwork analysis is important and fundamental skill for art appreciation,
which could enrich personal aesthetic sensibility and facilitate the critical
thinking ability. Understanding artworks is challenging due to its subjective
nature, diverse interpretations, and complex visual elements, requiring
expertise in art history, cultural background, and aesthetic theory. However,
limited by the data collection and model ability, previous works for
automatically analyzing artworks mainly focus on classification, retrieval, and
other simple tasks, which is far from the goal of AI. To facilitate the
research progress, in this paper, we step further to compose comprehensive
analysis inspired by the remarkable perception and generation ability of large
multimodal models. Specifically, we first propose a task of composing paragraph
analysis for artworks, i.e., painting in this paper, only focusing on visual
characteristics to formulate more comprehensive understanding of artworks. To
support the research on formal analysis, we collect a large dataset
PaintingForm, with about 19k painting images and 50k analysis paragraphs. We
further introduce a superior large multimodal model for painting analysis
composing, dubbed GalleryGPT, which is slightly modified and fine-tuned based
on LLaVA architecture leveraging our collected data. We conduct formal analysis
generation and zero-shot experiments across several datasets to assess the
capacity of our model. The results show remarkable performance improvements
comparing with powerful baseline LMMs, demonstrating its superb ability of art
analysis and generalization. \textcolor{blue}{The codes and model are available
at: https://github.com/steven640pixel/GalleryGPT.","[{'name': 'Yi Bin'}, {'name': 'Wenhao Shi'}, {'name': 'Yujuan Ding'}, {'name': 'Zhiqiang Hu'}, {'name': 'Zheng Wang'}, {'name': 'Yang Yang'}, {'name': 'See-Kiong Ng'}, {'name': 'Heng Tao Shen'}]",2024-08-01T11:52:56Z
http://arxiv.org/abs/2408.00397v1,http://arxiv.org/abs/2408.00397v1,"In-Context Example Selection via Similarity Search Improves Low-Resource
  Machine Translation","The ability of generative large language models (LLMs) to perform in-context
learning has given rise to a large body of research into how best to prompt
models for various natural language processing tasks. In this paper, we focus
on machine translation (MT), a task that has been shown to benefit from
in-context translation examples. However no systematic studies have been
published on how best to select examples, and mixed results have been reported
on the usefulness of similarity-based selection over random selection. We
provide a study covering multiple LLMs and multiple in-context example
retrieval strategies, comparing multilingual sentence embeddings. We cover
several language directions, representing different levels of language
resourcedness (English into French, German, Swahili and Wolof). Contrarily to
previously published results, we find that sentence embedding similarity can
improve MT, especially for low-resource language directions, and discuss the
balance between selection pool diversity and quality. We also highlight
potential problems with the evaluation of LLM-based MT and suggest a more
appropriate evaluation protocol, adapting the COMET metric to the evaluation of
LLMs. Code and outputs are freely available at
https://github.com/ArmelRandy/ICL-MT.","[{'name': 'Armel Zebaze'}, {'name': 'Benoît Sagot'}, {'name': 'Rachel Bawden'}]",2024-08-01T09:07:32Z
http://arxiv.org/abs/2408.00357v1,http://arxiv.org/abs/2408.00357v1,"DeliLaw: A Chinese Legal Counselling System Based on a Large Language
  Model","Traditional legal retrieval systems designed to retrieve legal documents,
statutes, precedents, and other legal information are unable to give
satisfactory answers due to lack of semantic understanding of specific
questions. Large Language Models (LLMs) have achieved excellent results in a
variety of natural language processing tasks, which inspired us that we train a
LLM in the legal domain to help legal retrieval. However, in the Chinese legal
domain, due to the complexity of legal questions and the rigour of legal
articles, there is no legal large model with satisfactory practical application
yet. In this paper, we present DeliLaw, a Chinese legal counselling system
based on a large language model. DeliLaw integrates a legal retrieval module
and a case retrieval module to overcome the model hallucination. Users can
consult professional legal questions, search for legal articles and relevant
judgement cases, etc. on the DeliLaw system in a dialogue mode. In addition,
DeliLaw supports the use of English for counseling. we provide the address of
the system: https://data.delilegal.com/lawQuestion.","[{'name': 'Nan Xie'}, {'name': 'Yuelin Bai'}, {'name': 'Hengyuan Gao'}, {'name': 'Feiteng Fang'}, {'name': 'Qixuan Zhao'}, {'name': 'Zhijian Li'}, {'name': 'Ziqiang Xue'}, {'name': 'Liang Zhu'}, {'name': 'Shiwen Ni'}, {'name': 'Min Yang'}]",2024-08-01T07:54:52Z
http://arxiv.org/abs/2408.00307v1,http://arxiv.org/abs/2408.00307v1,ABC Align: Large Language Model Alignment for Safety & Accuracy,"Alignment of Large Language Models (LLMs) remains an unsolved problem. Human
preferences are highly distributed and can be captured at multiple levels of
abstraction, from the individual to diverse populations. Organisational
preferences, represented by standards and principles, are defined to mitigate
reputational risk or meet legislative obligations. In this paper, we present
ABC Align, a novel alignment methodology for LLMs that enables integration of
the standards and preferences of a large media organisation into the LLM
itself. We combine a set of data and methods that build on recent breakthroughs
in synthetic data generation, preference optimisation, and post-training model
quantisation. Our unified approach mitigates bias and improves accuracy, while
preserving reasoning capability, as measured against standard benchmarks.","[{'name': 'Gareth Seneque'}, {'name': 'Lap-Hang Ho'}, {'name': 'Ariel Kuperman'}, {'name': 'Nafise Erfanian Saeedi'}, {'name': 'Jeffrey Molendijk'}]",2024-08-01T06:06:25Z
http://arxiv.org/abs/2408.00284v1,http://arxiv.org/abs/2408.00284v1,"Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like
  Spontaneous Representation","Large-scale text-to-speech (TTS) models have made significant progress
recently.However, they still fall short in the generation of Chinese dialectal
speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS
models capable of generating high-quality Chinese dialectal speech. Bailing-TTS
serves as a foundation model for Chinese dialectal speech generation. First,
continual semi-supervised learning is proposed to facilitate the alignment of
text tokens and speech tokens. Second, the Chinese dialectal representation
learning is developed using a specific transformer architecture and multi-stage
training processes. With the proposed design of novel network architecture and
corresponding strategy, Bailing-TTS is able to generate Chinese dialectal
speech from text effectively and efficiently. Experiments demonstrate that
Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous
representation. Readers are encouraged to listen to demos at
\url{https://c9412600.github.io/bltts_tech_report/index.html}.","[{'name': 'Xinhan Di'}, {'name': 'Zihao Chen'}, {'name': 'Yunming Liang'}, {'name': 'Junjie Zheng'}, {'name': 'Yihua Wang'}, {'name': 'Chaofan Ding'}]",2024-08-01T04:57:31Z
http://arxiv.org/abs/2408.00283v1,http://arxiv.org/abs/2408.00283v1,Navigating Text-to-Image Generative Bias across Indic Languages,"This research investigates biases in text-to-image (TTI) models for the Indic
languages widely spoken across India. It evaluates and compares the generative
performance and cultural relevance of leading TTI models in these languages
against their performance in English. Using the proposed IndicTTI benchmark, we
comprehensively assess the performance of 30 Indic languages with two
open-source diffusion models and two commercial generation APIs. The primary
objective of this benchmark is to evaluate the support for Indic languages in
these models and identify areas needing improvement. Given the linguistic
diversity of 30 languages spoken by over 1.4 billion people, this benchmark
aims to provide a detailed and insightful analysis of TTI models' effectiveness
within the Indic linguistic landscape. The data and code for the IndicTTI
benchmark can be accessed at
https://iab-rubric.org/resources/other-databases/indictti.","[{'name': 'Surbhi Mittal'}, {'name': 'Arnav Sudan'}, {'name': 'Mayank Vatsa'}, {'name': 'Richa Singh'}, {'name': 'Tamar Glaser'}, {'name': 'Tal Hassner'}]",2024-08-01T04:56:13Z
http://arxiv.org/abs/2408.00274v1,http://arxiv.org/abs/2408.00274v1,"QUITO: Accelerating Long-Context Reasoning through Query-Guided Context
  Compression","In-context learning (ICL) capabilities are foundational to the success of
large language models (LLMs). Recently, context compression has attracted
growing interest since it can largely reduce reasoning complexities and
computation costs of LLMs. In this paper, we introduce a novel Query-gUIded
aTtention cOmpression (QUITO) method, which leverages attention of the question
over the contexts to filter useless information. Specifically, we take a
trigger token to calculate the attention distribution of the context in
response to the question. Based on the distribution, we propose three different
filtering methods to satisfy the budget constraints of the context length. We
evaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and
ASQA. Experimental results demonstrate that QUITO significantly outperforms
established baselines across various datasets and downstream LLMs, underscoring
its effectiveness. Our code is available at
https://github.com/Wenshansilvia/attention_compressor.","[{'name': 'Wenshan Wang'}, {'name': 'Yihang Wang'}, {'name': 'Yixing Fan'}, {'name': 'Huaming Liao'}, {'name': 'Jiafeng Guo'}]",2024-08-01T04:28:38Z
http://arxiv.org/abs/2408.00264v1,http://arxiv.org/abs/2408.00264v1,"Clover-2: Accurate Inference for Regressive Lightweight Speculative
  Decoding","Large Language Models (LLMs) frequently suffer from inefficiencies, largely
attributable to the discord between the requirements of auto-regressive
decoding and the architecture of contemporary GPUs. Recently, regressive
lightweight speculative decoding has garnered attention for its notable
efficiency improvements in text generation tasks. This approach utilizes a
lightweight regressive draft model, like a Recurrent Neural Network (RNN) or a
single transformer decoder layer, leveraging sequential information to
iteratively predict potential tokens. Specifically, RNN draft models are
computationally economical but tend to deliver lower accuracy, while attention
decoder layer models exhibit the opposite traits. This paper presents Clover-2,
an advanced iteration of Clover, an RNN-based draft model designed to achieve
comparable accuracy to that of attention decoder layer models while maintaining
minimal computational overhead. Clover-2 enhances the model architecture and
incorporates knowledge distillation to increase Clover's accuracy and improve
overall efficiency. We conducted experiments using the open-source Vicuna 7B
and LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses
existing methods across various model architectures, showcasing its efficacy
and robustness.","[{'name': 'Bin Xiao'}, {'name': 'Lujun Gui'}, {'name': 'Lei Su'}, {'name': 'Weipeng Chen'}]",2024-08-01T03:43:32Z
http://arxiv.org/abs/2408.00244v1,http://arxiv.org/abs/2408.00244v1,"Enhanced Structured State Space Models via Grouped FIR Filtering and
  Attention Sink Mechanisms","Structured State Space Models (SSMs) have emerged as compelling alternatives
to Transformer architectures, offering linear-time complexity and superior
performance in various sequence modeling tasks. Despite their advantages, SSMs
like the original Mamba-2 face training difficulties due to the sensitivities
introduced by the extended series of recurrent matrix multiplications. In this
paper, we propose an advanced architecture that mitigates these challenges by
decomposing A-multiplications into multiple groups and optimizing positional
encoding through Grouped Finite Impulse Response (FIR) filtering. This new
structure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable
matrices for efficient computation. Furthermore, inspired by the ""attention
sink"" phenomenon identified in streaming language models, we incorporate a
similar mechanism to enhance the stability and performance of our model over
extended sequences. Our approach further bridges the gap between SSMs and
Transformer architectures, offering a viable path forward for scalable and
high-performing sequence modeling.","[{'name': 'Tian Meng'}, {'name': 'Yang Tao'}, {'name': 'Wuliang Yin'}]",2024-08-01T02:49:58Z
http://arxiv.org/abs/2408.00230v2,http://arxiv.org/abs/2408.00230v2,"Lost in Translation: Latent Concept Misalignment in Text-to-Image
  Diffusion Models","Advancements in text-to-image diffusion models have broadened extensive
downstream practical applications, but such models often encounter misalignment
issues between text and image. Taking the generation of a combination of two
disentangled concepts as an example, say given the prompt ""a tea cup of iced
coke"", existing models usually generate a glass cup of iced coke because the
iced coke usually co-occurs with the glass cup instead of the tea one during
model training. The root of such misalignment is attributed to the confusion in
the latent semantic space of text-to-image diffusion models, and hence we refer
to the ""a tea cup of iced coke"" phenomenon as Latent Concept Misalignment
(LC-Mis). We leverage large language models (LLMs) to thoroughly investigate
the scope of LC-Mis, and develop an automated pipeline for aligning the latent
semantics of diffusion models to text prompts. Empirical assessments confirm
the effectiveness of our approach, substantially reducing LC-Mis errors and
enhancing the robustness and versatility of text-to-image diffusion models. The
code and dataset are here: https://github.com/RossoneriZhao/iced_coke.","[{'name': 'Juntu Zhao'}, {'name': 'Junyu Deng'}, {'name': 'Yixin Ye'}, {'name': 'Chongxuan Li'}, {'name': 'Zhijie Deng'}, {'name': 'Dequan Wang'}]",2024-08-01T01:54:17Z
http://arxiv.org/abs/2408.00205v1,http://arxiv.org/abs/2408.00205v1,"Sentence-wise Speech Summarization: Task, Datasets, and End-to-End
  Modeling with LM Knowledge Distillation","This paper introduces a novel approach called sentence-wise speech
summarization (Sen-SSum), which generates text summaries from a spoken document
in a sentence-by-sentence manner. Sen-SSum combines the real-time processing of
automatic speech recognition (ASR) with the conciseness of speech
summarization. To explore this approach, we present two datasets for Sen-SSum:
Mega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types of
Transformer-based models: 1) cascade models that combine ASR and strong text
summarization models, and 2) end-to-end (E2E) models that directly convert
speech into a text summary. While E2E models are appealing to develop
compute-efficient models, they perform worse than cascade models. Therefore, we
propose knowledge distillation for E2E models using pseudo-summaries generated
by the cascade models. Our experiments show that this proposed knowledge
distillation effectively improves the performance of the E2E model on both
datasets.","[{'name': 'Kohei Matsuura'}, {'name': 'Takanori Ashihara'}, {'name': 'Takafumi Moriya'}, {'name': 'Masato Mimura'}, {'name': 'Takatomo Kano'}, {'name': 'Atsunori Ogawa'}, {'name': 'Marc Delcroix'}]",2024-08-01T00:18:21Z
http://arxiv.org/abs/2408.00203v1,http://arxiv.org/abs/2408.00203v1,OmniParser for Pure Vision Based GUI Agent,"The recent success of large vision language models shows great potential in
driving the agent system operating on user interfaces. However, we argue that
the power multimodal models like GPT-4V as a general agent on multiple
operating systems across different applications is largely underestimated due
to the lack of a robust screen parsing technique capable of: 1) reliably
identifying interactable icons within the user interface, and 2) understanding
the semantics of various elements in a screenshot and accurately associate the
intended action with the corresponding region on the screen. To fill these
gaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing user
interface screenshots into structured elements, which significantly enhances
the ability of GPT-4V to generate actions that can be accurately grounded in
the corresponding regions of the interface. We first curated an interactable
icon detection dataset using popular webpages and an icon description dataset.
These datasets were utilized to fine-tune specialized models: a detection model
to parse interactable regions on the screen and a caption model to extract the
functional semantics of the detected elements. \textsc{OmniParser}
significantly improves GPT-4V's performance on ScreenSpot benchmark. And on
Mind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only input
outperforms the GPT-4V baselines requiring additional information outside of
screenshot.","[{'name': 'Yadong Lu'}, {'name': 'Jianwei Yang'}, {'name': 'Yelong Shen'}, {'name': 'Ahmed Awadallah'}]",2024-08-01T00:00:43Z
http://arxiv.org/abs/2408.00197v1,http://arxiv.org/abs/2408.00197v1,"Automated Software Vulnerability Static Code Analysis Using Generative
  Pre-Trained Transformer Models","Generative Pre-Trained Transformer models have been shown to be surprisingly
effective at a variety of natural language processing tasks -- including
generating computer code. We evaluate the effectiveness of open source GPT
models for the task of automatic identification of the presence of vulnerable
code syntax (specifically targeting C and C++ source code). This task is
evaluated on a selection of 36 source code examples from the NIST SARD dataset,
which are specifically curated to not contain natural English that indicates
the presence, or lack thereof, of a particular vulnerability. The NIST SARD
source code dataset contains identified vulnerable lines of source code that
are examples of one out of the 839 distinct Common Weakness Enumerations (CWE),
allowing for exact quantification of the GPT output classification error rate.
A total of 5 GPT models are evaluated, using 10 different inference
temperatures and 100 repetitions at each setting, resulting in 5,000 GPT
queries per vulnerable source code analyzed. Ultimately, we find that the GPT
models that we evaluated are not suitable for fully automated vulnerability
scanning because the false positive and false negative rates are too high to
likely be useful in practice. However, we do find that the GPT models perform
surprisingly well at automated vulnerability detection for some of the test
cases, in particular surpassing random sampling, and being able to identify the
exact lines of code that are vulnerable albeit at a low success rate. The best
performing GPT model result found was Llama-2-70b-chat-hf with inference
temperature of 0.1 applied to NIST SARD test case 149165 (which is an example
of a buffer overflow vulnerability), which had a binary classification recall
score of 1.0 and a precision of 1.0 for correctly and uniquely identifying the
vulnerable line of code and the correct CWE number.","[{'name': 'Elijah Pelofske'}, {'name': 'Vincent Urias'}, {'name': 'Lorie M. Liebrock'}]",2024-07-31T23:33:26Z
http://arxiv.org/abs/2408.00162v1,http://arxiv.org/abs/2408.00162v1,A Taxonomy of Stereotype Content in Large Language Models,"This study introduces a taxonomy of stereotype content in contemporary large
language models (LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three
powerful and widely used LLMs, for the characteristics associated with 87
social categories (e.g., gender, race, occupations). We identify 14 stereotype
dimensions (e.g., Morality, Ability, Health, Beliefs, Emotions), accounting for
~90% of LLM stereotype associations. Warmth and Competence facets were the most
frequent content, but all other dimensions were significantly prevalent.
Stereotypes were more positive in LLMs (vs. humans), but there was significant
variability across categories and dimensions. Finally, the taxonomy predicted
the LLMs' internal evaluations of social categories (e.g., how
positively/negatively the categories were represented), supporting the
relevance of a multidimensional taxonomy for characterizing LLM stereotypes.
Our findings suggest that high-dimensional human stereotypes are reflected in
LLMs and must be considered in AI auditing and debiasing to minimize
unidentified harms from reliance in low-dimensional views of bias in LLMs.","[{'name': 'Gandalf Nicolas'}, {'name': 'Aylin Caliskan'}]",2024-07-31T21:14:41Z
http://arxiv.org/abs/2408.00161v2,http://arxiv.org/abs/2408.00161v2,"Automatic Generation of Behavioral Test Cases For Natural Language
  Processing Using Clustering and Prompting","Recent work in behavioral testing for natural language processing (NLP)
models, such as Checklist, is inspired by related paradigms in software
engineering testing. They allow evaluation of general linguistic capabilities
and domain understanding, hence can help evaluate conceptual soundness and
identify model weaknesses. However, a major challenge is the creation of test
cases. The current packages rely on semi-automated approach using manual
development which requires domain expertise and can be time consuming. This
paper introduces an automated approach to develop test cases by exploiting the
power of large language models and statistical techniques. It clusters the text
representations to carefully construct meaningful groups and then apply
prompting techniques to automatically generate Minimal Functionality Tests
(MFT). The well-known Amazon Reviews corpus is used to demonstrate our
approach. We analyze the behavioral test profiles across four different
classification algorithms and discuss the limitations and strengths of those
models.","[{'name': 'Ying Li'}, {'name': 'Rahul Singh'}, {'name': 'Tarun Joshi'}, {'name': 'Agus Sudjianto'}]",2024-07-31T21:12:21Z
http://arxiv.org/abs/2408.00144v1,http://arxiv.org/abs/2408.00144v1,Distributed In-Context Learning under Non-IID Among Clients,"Advancements in large language models (LLMs) have shown their effectiveness
in multiple complicated natural language reasoning tasks. A key challenge
remains in adapting these models efficiently to new or unfamiliar tasks.
In-context learning (ICL) provides a promising solution for few-shot adaptation
by retrieving a set of data points relevant to a query, called in-context
examples (ICE), from a training dataset and providing them during the inference
as context. Most existing studies utilize a centralized training dataset, yet
many real-world datasets may be distributed among multiple clients, and remote
data retrieval can be associated with costs. Especially when the client data
are non-identical independent distributions (non-IID), retrieving from clients
a proper set of ICEs needed for a test query presents critical challenges. In
this paper, we first show that in this challenging setting, test queries will
have different preferences among clients because of non-IIDness, and equal
contribution often leads to suboptimal performance. We then introduce a novel
approach to tackle the distributed non-IID ICL problem when a data usage budget
is present. The principle is that each client's proper contribution (budget)
should be designed according to the preference of each query for that client.
Our approach uses a data-driven manner to allocate a budget for each client,
tailored to each test query. Through extensive empirical studies on diverse
datasets, our framework demonstrates superior performance relative to competing
baselines.","[{'name': 'Siqi Liang'}, {'name': 'Sumyeong Ahn'}, {'name': 'Jiayu Zhou'}]",2024-07-31T20:06:25Z
http://arxiv.org/abs/2408.00137v1,http://arxiv.org/abs/2408.00137v1,"Correcting Negative Bias in Large Language Models through Negative
  Attention Score Alignment","A binary decision task, like yes-no questions or answer verification,
reflects a significant real-world scenario such as where users look for
confirmation about the correctness of their decisions on specific issues. In
this work, we observe that language models exhibit a negative bias in the
binary decisions of complex reasoning tasks. Based on our observations and the
rationale about attention-based model dynamics, we propose a negative attention
score (NAS) to systematically and quantitatively formulate negative bias. Based
on NAS, we identify attention heads that attend to negative tokens provided in
the instructions as answer candidate of binary decisions, regardless of the
question in the prompt, and validate their association with the negative bias.
Additionally, we propose the negative attention score alignment (NASA) method,
which is a parameter-efficient fine-tuning technique to address the extracted
negatively biased attention heads. Experimental results from various domains of
reasoning tasks and large model search space demonstrate that NASA
significantly reduces the gap between precision and recall caused by negative
bias while preserving their generalization abilities. Our codes are available
at \url{https://github.com/ysw1021/NASA}.","[{'name': 'Sangwon Yu'}, {'name': 'Jongyoon Song'}, {'name': 'Bongkyu Hwang'}, {'name': 'Hoyoung Kang'}, {'name': 'Sooah Cho'}, {'name': 'Junhwa Choi'}, {'name': 'Seongho Joe'}, {'name': 'Taehee Lee'}, {'name': 'Youngjune L. Gwon'}, {'name': 'Sungroh Yoon'}]",2024-07-31T19:50:57Z
http://arxiv.org/abs/2408.00122v1,http://arxiv.org/abs/2408.00122v1,A Course Shared Task on Evaluating LLM Output for Clinical Questions,"This paper presents a shared task that we organized at the Foundations of
Language Technology (FoLT) course in 2023/2024 at the Technical University of
Darmstadt, which focuses on evaluating the output of Large Language Models
(LLMs) in generating harmful answers to health-related clinical questions. We
describe the task design considerations and report the feedback we received
from the students. We expect the task and the findings reported in this paper
to be relevant for instructors teaching natural language processing (NLP) and
designing course assignments.","[{'name': 'Yufang Hou'}, {'name': 'Thy Thy Tran'}, {'name': 'Doan Nam Long Vu'}, {'name': 'Yiwen Cao'}, {'name': 'Kai Li'}, {'name': 'Lukas Rohde'}, {'name': 'Iryna Gurevych'}]",2024-07-31T19:24:40Z
http://arxiv.org/abs/2408.00118v2,http://arxiv.org/abs/2408.00118v2,Gemma 2: Improving Open Language Models at a Practical Size,"In this work, we introduce Gemma 2, a new addition to the Gemma family of
lightweight, state-of-the-art open models, ranging in scale from 2 billion to
27 billion parameters. In this new version, we apply several known technical
modifications to the Transformer architecture, such as interleaving
local-global attentions (Beltagy et al., 2020a) and group-query attention
(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge
distillation (Hinton et al., 2015) instead of next token prediction. The
resulting models deliver the best performance for their size, and even offer
competitive alternatives to models that are 2-3 times bigger. We release all
our models to the community.","[{'name': 'Gemma Team'}, {'name': 'Morgane Riviere'}, {'name': 'Shreya Pathak'}, {'name': 'Pier Giuseppe Sessa'}, {'name': 'Cassidy Hardin'}, {'name': 'Surya Bhupatiraju'}, {'name': 'Léonard Hussenot'}, {'name': 'Thomas Mesnard'}, {'name': 'Bobak Shahriari'}, {'name': 'Alexandre Ramé'}, {'name': 'Johan Ferret'}, {'name': 'Peter Liu'}, {'name': 'Pouya Tafti'}, {'name': 'Abe Friesen'}, {'name': 'Michelle Casbon'}, {'name': 'Sabela Ramos'}, {'name': 'Ravin Kumar'}, {'name': 'Charline Le Lan'}, {'name': 'Sammy Jerome'}, {'name': 'Anton Tsitsulin'}, {'name': 'Nino Vieillard'}, {'name': 'Piotr Stanczyk'}, {'name': 'Sertan Girgin'}, {'name': 'Nikola Momchev'}, {'name': 'Matt Hoffman'}, {'name': 'Shantanu Thakoor'}, {'name': 'Jean-Bastien Grill'}, {'name': 'Behnam Neyshabur'}, {'name': 'Olivier Bachem'}, {'name': 'Alanna Walton'}, {'name': 'Aliaksei Severyn'}, {'name': 'Alicia Parrish'}, {'name': 'Aliya Ahmad'}, {'name': 'Allen Hutchison'}, {'name': 'Alvin Abdagic'}, {'name': 'Amanda Carl'}, {'name': 'Amy Shen'}, {'name': 'Andy Brock'}, {'name': 'Andy Coenen'}, {'name': 'Anthony Laforge'}, {'name': 'Antonia Paterson'}, {'name': 'Ben Bastian'}, {'name': 'Bilal Piot'}, {'name': 'Bo Wu'}, {'name': 'Brandon Royal'}, {'name': 'Charlie Chen'}, {'name': 'Chintu Kumar'}, {'name': 'Chris Perry'}, {'name': 'Chris Welty'}, {'name': 'Christopher A. Choquette-Choo'}, {'name': 'Danila Sinopalnikov'}, {'name': 'David Weinberger'}, {'name': 'Dimple Vijaykumar'}, {'name': 'Dominika Rogozińska'}, {'name': 'Dustin Herbison'}, {'name': 'Elisa Bandy'}, {'name': 'Emma Wang'}, {'name': 'Eric Noland'}, {'name': 'Erica Moreira'}, {'name': 'Evan Senter'}, {'name': 'Evgenii Eltyshev'}, {'name': 'Francesco Visin'}, {'name': 'Gabriel Rasskin'}, {'name': 'Gary Wei'}, {'name': 'Glenn Cameron'}, {'name': 'Gus Martins'}, {'name': 'Hadi Hashemi'}, {'name': 'Hanna Klimczak-Plucińska'}, {'name': 'Harleen Batra'}, {'name': 'Harsh Dhand'}, {'name': 'Ivan Nardini'}, {'name': 'Jacinda Mein'}, {'name': 'Jack Zhou'}, {'name': 'James Svensson'}, {'name': 'Jeff Stanway'}, {'name': 'Jetha Chan'}, {'name': 'Jin Peng Zhou'}, {'name': 'Joana Carrasqueira'}, {'name': 'Joana Iljazi'}, {'name': 'Jocelyn Becker'}, {'name': 'Joe Fernandez'}, {'name': 'Joost van Amersfoort'}, {'name': 'Josh Gordon'}, {'name': 'Josh Lipschultz'}, {'name': 'Josh Newlan'}, {'name': 'Ju-yeong Ji'}, {'name': 'Kareem Mohamed'}, {'name': 'Kartikeya Badola'}, {'name': 'Kat Black'}, {'name': 'Katie Millican'}, {'name': 'Keelin McDonell'}, {'name': 'Kelvin Nguyen'}, {'name': 'Kiranbir Sodhia'}, {'name': 'Kish Greene'}, {'name': 'Lars Lowe Sjoesund'}, {'name': 'Lauren Usui'}, {'name': 'Laurent Sifre'}, {'name': 'Lena Heuermann'}, {'name': 'Leticia Lago'}, {'name': 'Lilly McNealus'}, {'name': 'Livio Baldini Soares'}, {'name': 'Logan Kilpatrick'}, {'name': 'Lucas Dixon'}, {'name': 'Luciano Martins'}, {'name': 'Machel Reid'}, {'name': 'Manvinder Singh'}, {'name': 'Mark Iverson'}, {'name': 'Martin Görner'}, {'name': 'Mat Velloso'}, {'name': 'Mateo Wirth'}, {'name': 'Matt Davidow'}, {'name': 'Matt Miller'}, {'name': 'Matthew Rahtz'}, {'name': 'Matthew Watson'}, {'name': 'Meg Risdal'}, {'name': 'Mehran Kazemi'}, {'name': 'Michael Moynihan'}, {'name': 'Ming Zhang'}, {'name': 'Minsuk Kahng'}, {'name': 'Minwoo Park'}, {'name': 'Mofi Rahman'}, {'name': 'Mohit Khatwani'}, {'name': 'Natalie Dao'}, {'name': 'Nenshad Bardoliwalla'}, {'name': 'Nesh Devanathan'}, {'name': 'Neta Dumai'}, {'name': 'Nilay Chauhan'}, {'name': 'Oscar Wahltinez'}, {'name': 'Pankil Botarda'}, {'name': 'Parker Barnes'}, {'name': 'Paul Barham'}, {'name': 'Paul Michel'}, {'name': 'Pengchong Jin'}, {'name': 'Petko Georgiev'}, {'name': 'Phil Culliton'}, {'name': 'Pradeep Kuppala'}, {'name': 'Ramona Comanescu'}, {'name': 'Ramona Merhej'}, {'name': 'Reena Jana'}, {'name': 'Reza Ardeshir Rokni'}, {'name': 'Rishabh Agarwal'}, {'name': 'Ryan Mullins'}, {'name': 'Samaneh Saadat'}, {'name': 'Sara Mc Carthy'}, {'name': 'Sarah Perrin'}, {'name': 'Sébastien M. R. Arnold'}, {'name': 'Sebastian Krause'}, {'name': 'Shengyang Dai'}, {'name': 'Shruti Garg'}, {'name': 'Shruti Sheth'}, {'name': 'Sue Ronstrom'}, {'name': 'Susan Chan'}, {'name': 'Timothy Jordan'}, {'name': 'Ting Yu'}, {'name': 'Tom Eccles'}, {'name': 'Tom Hennigan'}, {'name': 'Tomas Kocisky'}, {'name': 'Tulsee Doshi'}, {'name': 'Vihan Jain'}, {'name': 'Vikas Yadav'}, {'name': 'Vilobh Meshram'}, {'name': 'Vishal Dharmadhikari'}, {'name': 'Warren Barkley'}, {'name': 'Wei Wei'}, {'name': 'Wenming Ye'}, {'name': 'Woohyun Han'}, {'name': 'Woosuk Kwon'}, {'name': 'Xiang Xu'}, {'name': 'Zhe Shen'}, {'name': 'Zhitao Gong'}, {'name': 'Zichuan Wei'}, {'name': 'Victor Cotruta'}, {'name': 'Phoebe Kirk'}, {'name': 'Anand Rao'}, {'name': 'Minh Giang'}, {'name': 'Ludovic Peran'}, {'name': 'Tris Warkentin'}, {'name': 'Eli Collins'}, {'name': 'Joelle Barral'}, {'name': 'Zoubin Ghahramani'}, {'name': 'Raia Hadsell'}, {'name': 'D. Sculley'}, {'name': 'Jeanine Banks'}, {'name': 'Anca Dragan'}, {'name': 'Slav Petrov'}, {'name': 'Oriol Vinyals'}, {'name': 'Jeff Dean'}, {'name': 'Demis Hassabis'}, {'name': 'Koray Kavukcuoglu'}, {'name': 'Clement Farabet'}, {'name': 'Elena Buchatskaya'}, {'name': 'Sebastian Borgeaud'}, {'name': 'Noah Fiedel'}, {'name': 'Armand Joulin'}, {'name': 'Kathleen Kenealy'}, {'name': 'Robert Dadashi'}, {'name': 'Alek Andreev'}]",2024-07-31T19:13:07Z
http://arxiv.org/abs/2408.00113v1,http://arxiv.org/abs/2408.00113v1,"Measuring Progress in Dictionary Learning for Language Model
  Interpretability with Board Game Models","What latent features are encoded in language model (LM) representations?
Recent work on training sparse autoencoders (SAEs) to disentangle interpretable
features in LM representations has shown significant promise. However,
evaluating the quality of these SAEs is difficult because we lack a
ground-truth collection of interpretable features that we expect good SAEs to
recover. We thus propose to measure progress in interpretable dictionary
learning by working in the setting of LMs trained on chess and Othello
transcripts. These settings carry natural collections of interpretable features
-- for example, ""there is a knight on F3"" -- which we leverage into
$\textit{supervised}$ metrics for SAE quality. To guide progress in
interpretable dictionary learning, we introduce a new SAE training technique,
$\textit{p-annealing}$, which improves performance on prior unsupervised
metrics as well as our new metrics.","[{'name': 'Adam Karvonen'}, {'name': 'Benjamin Wright'}, {'name': 'Can Rager'}, {'name': 'Rico Angell'}, {'name': 'Jannik Brinkmann'}, {'name': 'Logan Smith'}, {'name': 'Claudio Mayrink Verdun'}, {'name': 'David Bau'}, {'name': 'Samuel Marks'}]",2024-07-31T18:45:13Z
http://arxiv.org/abs/2408.00103v1,http://arxiv.org/abs/2408.00103v1,"ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation
  Extraction on an Academic Budget","Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in
Natural Language Processing, serving as critical components in a wide range of
applications. In this paper, we propose ReLiK, a Retriever-Reader architecture
for both EL and RE, where, given an input text, the Retriever module undertakes
the identification of candidate entities or relations that could potentially
appear within the text. Subsequently, the Reader module is tasked to discern
the pertinent retrieved entities or relations and establish their alignment
with the corresponding textual spans. Notably, we put forward an innovative
input representation that incorporates the candidate entities or relations
alongside the text, making it possible to link entities or extract relations in
a single forward pass and to fully leverage pre-trained language models
contextualization capabilities, in contrast with previous
Retriever-Reader-based methods, which require a forward pass for each
candidate. Our formulation of EL and RE achieves state-of-the-art performance
in both in-domain and out-of-domain benchmarks while using academic budget
training and with up to 40x inference speed compared to competitors. Finally,
we show how our architecture can be used seamlessly for Information Extraction
(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared
Reader that simultaneously extracts entities and relations.","[{'name': 'Riccardo Orlando'}, {'name': 'Pere-Lluis Huguet-Cabot'}, {'name': 'Edoardo Barba'}, {'name': 'Roberto Navigli'}]",2024-07-31T18:25:49Z
http://arxiv.org/abs/2407.21792v1,http://arxiv.org/abs/2407.21792v1,Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?,"As artificial intelligence systems grow more powerful, there has been
increasing interest in ""AI safety"" research to address emerging and future
risks. However, the field of AI safety remains poorly defined and
inconsistently measured, leading to confusion about how researchers can
contribute. This lack of clarity is compounded by the unclear relationship
between AI safety benchmarks and upstream general capabilities (e.g., general
knowledge and reasoning). To address these issues, we conduct a comprehensive
meta-analysis of AI safety benchmarks, empirically analyzing their correlation
with general capabilities across dozens of models and providing a survey of
existing directions in AI safety. Our findings reveal that many safety
benchmarks highly correlate with upstream model capabilities, potentially
enabling ""safetywashing"" -- where capability improvements are misrepresented as
safety advancements. Based on these findings, we propose an empirical
foundation for developing more meaningful safety metrics and define AI safety
in a machine learning research context as a set of clearly delineated research
goals that are empirically separable from generic capabilities advancements. In
doing so, we aim to provide a more rigorous framework for AI safety research,
advancing the science of safety evaluations and clarifying the path towards
measurable progress.","[{'name': 'Richard Ren'}, {'name': 'Steven Basart'}, {'name': 'Adam Khoja'}, {'name': 'Alice Gatti'}, {'name': 'Long Phan'}, {'name': 'Xuwang Yin'}, {'name': 'Mantas Mazeika'}, {'name': 'Alexander Pan'}, {'name': 'Gabriel Mukobi'}, {'name': 'Ryan H. Kim'}, {'name': 'Stephen Fitz'}, {'name': 'Dan Hendrycks'}]",2024-07-31T17:59:24Z
http://arxiv.org/abs/2407.21788v1,http://arxiv.org/abs/2407.21788v1,Vision-Language Model Based Handwriting Verification,"Handwriting Verification is a critical in document forensics. Deep learning
based approaches often face skepticism from forensic document examiners due to
their lack of explainability and reliance on extensive training data and
handcrafted features. This paper explores using Vision Language Models (VLMs),
such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By
leveraging their Visual Question Answering capabilities and 0-shot
Chain-of-Thought (CoT) reasoning, our goal is to provide clear,
human-understandable explanations for model decisions. Our experiments on the
CEDAR handwriting dataset demonstrate that VLMs offer enhanced
interpretability, reduce the need for large training datasets, and adapt better
to diverse handwriting styles. However, results show that the CNN-based
ResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach
with GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:
71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings
highlight the potential of VLMs in generating human-interpretable decisions
while underscoring the need for further advancements to match the performance
of specialized deep learning models.","[{'name': 'Mihir Chauhan'}, {'name': 'Abhishek Satbhai'}, {'name': 'Mohammad Abuzar Hashemi'}, {'name': 'Mir Basheer Ali'}, {'name': 'Bina Ramamurthy'}, {'name': 'Mingchen Gao'}, {'name': 'Siwei Lyu'}, {'name': 'Sargur Srihari'}]",2024-07-31T17:57:32Z
http://arxiv.org/abs/2407.21783v2,http://arxiv.org/abs/2407.21783v2,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models.
This paper presents a new set of foundation models, called Llama 3. It is a
herd of language models that natively support multilinguality, coding,
reasoning, and tool usage. Our largest model is a dense Transformer with 405B
parameters and a context window of up to 128K tokens. This paper presents an
extensive empirical evaluation of Llama 3. We find that Llama 3 delivers
comparable quality to leading language models such as GPT-4 on a plethora of
tasks. We publicly release Llama 3, including pre-trained and post-trained
versions of the 405B parameter language model and our Llama Guard 3 model for
input and output safety. The paper also presents the results of experiments in
which we integrate image, video, and speech capabilities into Llama 3 via a
compositional approach. We observe this approach performs competitively with
the state-of-the-art on image, video, and speech recognition tasks. The
resulting models are not yet being broadly released as they are still under
development.","[{'name': 'Abhimanyu Dubey'}, {'name': 'Abhinav Jauhri'}, {'name': 'Abhinav Pandey'}, {'name': 'Abhishek Kadian'}, {'name': 'Ahmad Al-Dahle'}, {'name': 'Aiesha Letman'}, {'name': 'Akhil Mathur'}, {'name': 'Alan Schelten'}, {'name': 'Amy Yang'}, {'name': 'Angela Fan'}, {'name': 'Anirudh Goyal'}, {'name': 'Anthony Hartshorn'}, {'name': 'Aobo Yang'}, {'name': 'Archi Mitra'}, {'name': 'Archie Sravankumar'}, {'name': 'Artem Korenev'}, {'name': 'Arthur Hinsvark'}, {'name': 'Arun Rao'}, {'name': 'Aston Zhang'}, {'name': 'Aurelien Rodriguez'}, {'name': 'Austen Gregerson'}, {'name': 'Ava Spataru'}, {'name': 'Baptiste Roziere'}, {'name': 'Bethany Biron'}, {'name': 'Binh Tang'}, {'name': 'Bobbie Chern'}, {'name': 'Charlotte Caucheteux'}, {'name': 'Chaya Nayak'}, {'name': 'Chloe Bi'}, {'name': 'Chris Marra'}, {'name': 'Chris McConnell'}, {'name': 'Christian Keller'}, {'name': 'Christophe Touret'}, {'name': 'Chunyang Wu'}, {'name': 'Corinne Wong'}, {'name': 'Cristian Canton Ferrer'}, {'name': 'Cyrus Nikolaidis'}, {'name': 'Damien Allonsius'}, {'name': 'Daniel Song'}, {'name': 'Danielle Pintz'}, {'name': 'Danny Livshits'}, {'name': 'David Esiobu'}, {'name': 'Dhruv Choudhary'}, {'name': 'Dhruv Mahajan'}, {'name': 'Diego Garcia-Olano'}, {'name': 'Diego Perino'}, {'name': 'Dieuwke Hupkes'}, {'name': 'Egor Lakomkin'}, {'name': 'Ehab AlBadawy'}, {'name': 'Elina Lobanova'}, {'name': 'Emily Dinan'}, {'name': 'Eric Michael Smith'}, {'name': 'Filip Radenovic'}, {'name': 'Frank Zhang'}, {'name': 'Gabriel Synnaeve'}, {'name': 'Gabrielle Lee'}, {'name': 'Georgia Lewis Anderson'}, {'name': 'Graeme Nail'}, {'name': 'Gregoire Mialon'}, {'name': 'Guan Pang'}, {'name': 'Guillem Cucurell'}, {'name': 'Hailey Nguyen'}, {'name': 'Hannah Korevaar'}, {'name': 'Hu Xu'}, {'name': 'Hugo Touvron'}, {'name': 'Iliyan Zarov'}, {'name': 'Imanol Arrieta Ibarra'}, {'name': 'Isabel Kloumann'}, {'name': 'Ishan Misra'}, {'name': 'Ivan Evtimov'}, {'name': 'Jade Copet'}, {'name': 'Jaewon Lee'}, {'name': 'Jan Geffert'}, {'name': 'Jana Vranes'}, {'name': 'Jason Park'}, {'name': 'Jay Mahadeokar'}, {'name': 'Jeet Shah'}, {'name': 'Jelmer van der Linde'}, {'name': 'Jennifer Billock'}, {'name': 'Jenny Hong'}, {'name': 'Jenya Lee'}, {'name': 'Jeremy Fu'}, {'name': 'Jianfeng Chi'}, {'name': 'Jianyu Huang'}, {'name': 'Jiawen Liu'}, {'name': 'Jie Wang'}, {'name': 'Jiecao Yu'}, {'name': 'Joanna Bitton'}, {'name': 'Joe Spisak'}, {'name': 'Jongsoo Park'}, {'name': 'Joseph Rocca'}, {'name': 'Joshua Johnstun'}, {'name': 'Joshua Saxe'}, {'name': 'Junteng Jia'}, {'name': 'Kalyan Vasuden Alwala'}, {'name': 'Kartikeya Upasani'}, {'name': 'Kate Plawiak'}, {'name': 'Ke Li'}, {'name': 'Kenneth Heafield'}, {'name': 'Kevin Stone'}, {'name': 'Khalid El-Arini'}, {'name': 'Krithika Iyer'}, {'name': 'Kshitiz Malik'}, {'name': 'Kuenley Chiu'}, {'name': 'Kunal Bhalla'}, {'name': 'Lauren Rantala-Yeary'}, {'name': 'Laurens van der Maaten'}, {'name': 'Lawrence Chen'}, {'name': 'Liang Tan'}, {'name': 'Liz Jenkins'}, {'name': 'Louis Martin'}, {'name': 'Lovish Madaan'}, {'name': 'Lubo Malo'}, {'name': 'Lukas Blecher'}, {'name': 'Lukas Landzaat'}, {'name': 'Luke de Oliveira'}, {'name': 'Madeline Muzzi'}, {'name': 'Mahesh Pasupuleti'}, {'name': 'Mannat Singh'}, {'name': 'Manohar Paluri'}, {'name': 'Marcin Kardas'}, {'name': 'Mathew Oldham'}, {'name': 'Mathieu Rita'}, {'name': 'Maya Pavlova'}, {'name': 'Melanie Kambadur'}, {'name': 'Mike Lewis'}, {'name': 'Min Si'}, {'name': 'Mitesh Kumar Singh'}, {'name': 'Mona Hassan'}, {'name': 'Naman Goyal'}, {'name': 'Narjes Torabi'}, {'name': 'Nikolay Bashlykov'}, {'name': 'Nikolay Bogoychev'}, {'name': 'Niladri Chatterji'}, {'name': 'Olivier Duchenne'}, {'name': 'Onur Çelebi'}, {'name': 'Patrick Alrassy'}, {'name': 'Pengchuan Zhang'}, {'name': 'Pengwei Li'}, {'name': 'Petar Vasic'}, {'name': 'Peter Weng'}, {'name': 'Prajjwal Bhargava'}, {'name': 'Pratik Dubal'}, {'name': 'Praveen Krishnan'}, {'name': 'Punit Singh Koura'}, {'name': 'Puxin Xu'}, {'name': 'Qing He'}, {'name': 'Qingxiao Dong'}, {'name': 'Ragavan Srinivasan'}, {'name': 'Raj Ganapathy'}, {'name': 'Ramon Calderer'}, {'name': 'Ricardo Silveira Cabral'}, {'name': 'Robert Stojnic'}, {'name': 'Roberta Raileanu'}, {'name': 'Rohit Girdhar'}, {'name': 'Rohit Patel'}, {'name': 'Romain Sauvestre'}, {'name': 'Ronnie Polidoro'}, {'name': 'Roshan Sumbaly'}, {'name': 'Ross Taylor'}, {'name': 'Ruan Silva'}, {'name': 'Rui Hou'}, {'name': 'Rui Wang'}, {'name': 'Saghar Hosseini'}, {'name': 'Sahana Chennabasappa'}, {'name': 'Sanjay Singh'}, {'name': 'Sean Bell'}, {'name': 'Seohyun Sonia Kim'}, {'name': 'Sergey Edunov'}, {'name': 'Shaoliang Nie'}, {'name': 'Sharan Narang'}, {'name': 'Sharath Raparthy'}, {'name': 'Sheng Shen'}, {'name': 'Shengye Wan'}, {'name': 'Shruti Bhosale'}, {'name': 'Shun Zhang'}, {'name': 'Simon Vandenhende'}, {'name': 'Soumya Batra'}, {'name': 'Spencer Whitman'}, {'name': 'Sten Sootla'}, {'name': 'Stephane Collot'}, {'name': 'Suchin Gururangan'}, {'name': 'Sydney Borodinsky'}, {'name': 'Tamar Herman'}, {'name': 'Tara Fowler'}, {'name': 'Tarek Sheasha'}, {'name': 'Thomas Georgiou'}, {'name': 'Thomas Scialom'}, {'name': 'Tobias Speckbacher'}, {'name': 'Todor Mihaylov'}, {'name': 'Tong Xiao'}, {'name': 'Ujjwal Karn'}, {'name': 'Vedanuj Goswami'}, {'name': 'Vibhor Gupta'}, {'name': 'Vignesh Ramanathan'}, {'name': 'Viktor Kerkez'}, {'name': 'Vincent Gonguet'}, {'name': 'Virginie Do'}, {'name': 'Vish Vogeti'}, {'name': 'Vladan Petrovic'}, {'name': 'Weiwei Chu'}, {'name': 'Wenhan Xiong'}, {'name': 'Wenyin Fu'}, {'name': 'Whitney Meers'}, {'name': 'Xavier Martinet'}, {'name': 'Xiaodong Wang'}, {'name': 'Xiaoqing Ellen Tan'}, {'name': 'Xinfeng Xie'}, {'name': 'Xuchao Jia'}, {'name': 'Xuewei Wang'}, {'name': 'Yaelle Goldschlag'}, {'name': 'Yashesh Gaur'}, {'name': 'Yasmine Babaei'}, {'name': 'Yi Wen'}, {'name': 'Yiwen Song'}, {'name': 'Yuchen Zhang'}, {'name': 'Yue Li'}, {'name': 'Yuning Mao'}, {'name': 'Zacharie Delpierre Coudert'}, {'name': 'Zheng Yan'}, {'name': 'Zhengxing Chen'}, {'name': 'Zoe Papakipos'}, {'name': 'Aaditya Singh'}, {'name': 'Aaron Grattafiori'}, {'name': 'Abha Jain'}, {'name': 'Adam Kelsey'}, {'name': 'Adam Shajnfeld'}, {'name': 'Adithya Gangidi'}, {'name': 'Adolfo Victoria'}, {'name': 'Ahuva Goldstand'}, {'name': 'Ajay Menon'}, {'name': 'Ajay Sharma'}, {'name': 'Alex Boesenberg'}, {'name': 'Alex Vaughan'}, {'name': 'Alexei Baevski'}, {'name': 'Allie Feinstein'}, {'name': 'Amanda Kallet'}, {'name': 'Amit Sangani'}, {'name': 'Anam Yunus'}, {'name': 'Andrei Lupu'}, {'name': 'Andres Alvarado'}, {'name': 'Andrew Caples'}, {'name': 'Andrew Gu'}, {'name': 'Andrew Ho'}, {'name': 'Andrew Poulton'}, {'name': 'Andrew Ryan'}, {'name': 'Ankit Ramchandani'}, {'name': 'Annie Franco'}, {'name': 'Aparajita Saraf'}, {'name': 'Arkabandhu Chowdhury'}, {'name': 'Ashley Gabriel'}, {'name': 'Ashwin Bharambe'}, {'name': 'Assaf Eisenman'}, {'name': 'Azadeh Yazdan'}, {'name': 'Beau James'}, {'name': 'Ben Maurer'}, {'name': 'Benjamin Leonhardi'}, {'name': 'Bernie Huang'}, {'name': 'Beth Loyd'}, {'name': 'Beto De Paola'}, {'name': 'Bhargavi Paranjape'}, {'name': 'Bing Liu'}, {'name': 'Bo Wu'}, {'name': 'Boyu Ni'}, {'name': 'Braden Hancock'}, {'name': 'Bram Wasti'}, {'name': 'Brandon Spence'}, {'name': 'Brani Stojkovic'}, {'name': 'Brian Gamido'}, {'name': 'Britt Montalvo'}, {'name': 'Carl Parker'}, {'name': 'Carly Burton'}, {'name': 'Catalina Mejia'}, {'name': 'Changhan Wang'}, {'name': 'Changkyu Kim'}, {'name': 'Chao Zhou'}, {'name': 'Chester Hu'}, {'name': 'Ching-Hsiang Chu'}, {'name': 'Chris Cai'}, {'name': 'Chris Tindal'}, {'name': 'Christoph Feichtenhofer'}, {'name': 'Damon Civin'}, {'name': 'Dana Beaty'}, {'name': 'Daniel Kreymer'}, {'name': 'Daniel Li'}, {'name': 'Danny Wyatt'}, {'name': 'David Adkins'}, {'name': 'David Xu'}, {'name': 'Davide Testuggine'}, {'name': 'Delia David'}, {'name': 'Devi Parikh'}, {'name': 'Diana Liskovich'}, {'name': 'Didem Foss'}, {'name': 'Dingkang Wang'}, {'name': 'Duc Le'}, {'name': 'Dustin Holland'}, {'name': 'Edward Dowling'}, {'name': 'Eissa Jamil'}, {'name': 'Elaine Montgomery'}, {'name': 'Eleonora Presani'}, {'name': 'Emily Hahn'}, {'name': 'Emily Wood'}, {'name': 'Erik Brinkman'}, {'name': 'Esteban Arcaute'}, {'name': 'Evan Dunbar'}, {'name': 'Evan Smothers'}, {'name': 'Fei Sun'}, {'name': 'Felix Kreuk'}, {'name': 'Feng Tian'}, {'name': 'Firat Ozgenel'}, {'name': 'Francesco Caggioni'}, {'name': 'Francisco Guzmán'}, {'name': 'Frank Kanayet'}, {'name': 'Frank Seide'}, {'name': 'Gabriela Medina Florez'}, {'name': 'Gabriella Schwarz'}, {'name': 'Gada Badeer'}, {'name': 'Georgia Swee'}, {'name': 'Gil Halpern'}, {'name': 'Govind Thattai'}, {'name': 'Grant Herman'}, {'name': 'Grigory Sizov'}, {'name': 'Guangyi'}, {'name': 'Zhang'}, {'name': 'Guna Lakshminarayanan'}, {'name': 'Hamid Shojanazeri'}, {'name': 'Han Zou'}, {'name': 'Hannah Wang'}, {'name': 'Hanwen Zha'}, {'name': 'Haroun Habeeb'}, {'name': 'Harrison Rudolph'}, {'name': 'Helen Suk'}, {'name': 'Henry Aspegren'}, {'name': 'Hunter Goldman'}, {'name': 'Ibrahim Damlaj'}, {'name': 'Igor Molybog'}, {'name': 'Igor Tufanov'}, {'name': 'Irina-Elena Veliche'}, {'name': 'Itai Gat'}, {'name': 'Jake Weissman'}, {'name': 'James Geboski'}, {'name': 'James Kohli'}, {'name': 'Japhet Asher'}, {'name': 'Jean-Baptiste Gaya'}, {'name': 'Jeff Marcus'}, {'name': 'Jeff Tang'}, {'name': 'Jennifer Chan'}, {'name': 'Jenny Zhen'}, {'name': 'Jeremy Reizenstein'}, {'name': 'Jeremy Teboul'}, {'name': 'Jessica Zhong'}, {'name': 'Jian Jin'}, {'name': 'Jingyi Yang'}, {'name': 'Joe Cummings'}, {'name': 'Jon Carvill'}, {'name': 'Jon Shepard'}, {'name': 'Jonathan McPhie'}, {'name': 'Jonathan Torres'}, {'name': 'Josh Ginsburg'}, {'name': 'Junjie Wang'}, {'name': 'Kai Wu'}, {'name': 'Kam Hou U'}, {'name': 'Karan Saxena'}, {'name': 'Karthik Prasad'}, {'name': 'Kartikay Khandelwal'}, {'name': 'Katayoun Zand'}, {'name': 'Kathy Matosich'}, {'name': 'Kaushik Veeraraghavan'}, {'name': 'Kelly Michelena'}, {'name': 'Keqian Li'}, {'name': 'Kun Huang'}, {'name': 'Kunal Chawla'}, {'name': 'Kushal Lakhotia'}, {'name': 'Kyle Huang'}, {'name': 'Lailin Chen'}, {'name': 'Lakshya Garg'}, {'name': 'Lavender A'}, {'name': 'Leandro Silva'}, {'name': 'Lee Bell'}, {'name': 'Lei Zhang'}, {'name': 'Liangpeng Guo'}, {'name': 'Licheng Yu'}, {'name': 'Liron Moshkovich'}, {'name': 'Luca Wehrstedt'}, {'name': 'Madian Khabsa'}, {'name': 'Manav Avalani'}, {'name': 'Manish Bhatt'}, {'name': 'Maria Tsimpoukelli'}, {'name': 'Martynas Mankus'}, {'name': 'Matan Hasson'}, {'name': 'Matthew Lennie'}, {'name': 'Matthias Reso'}, {'name': 'Maxim Groshev'}, {'name': 'Maxim Naumov'}, {'name': 'Maya Lathi'}, {'name': 'Meghan Keneally'}, {'name': 'Michael L. Seltzer'}, {'name': 'Michal Valko'}, {'name': 'Michelle Restrepo'}, {'name': 'Mihir Patel'}, {'name': 'Mik Vyatskov'}, {'name': 'Mikayel Samvelyan'}, {'name': 'Mike Clark'}, {'name': 'Mike Macey'}, {'name': 'Mike Wang'}, {'name': 'Miquel Jubert Hermoso'}, {'name': 'Mo Metanat'}, {'name': 'Mohammad Rastegari'}, {'name': 'Munish Bansal'}, {'name': 'Nandhini Santhanam'}, {'name': 'Natascha Parks'}, {'name': 'Natasha White'}, {'name': 'Navyata Bawa'}, {'name': 'Nayan Singhal'}, {'name': 'Nick Egebo'}, {'name': 'Nicolas Usunier'}, {'name': 'Nikolay Pavlovich Laptev'}, {'name': 'Ning Dong'}, {'name': 'Ning Zhang'}, {'name': 'Norman Cheng'}, {'name': 'Oleg Chernoguz'}, {'name': 'Olivia Hart'}, {'name': 'Omkar Salpekar'}, {'name': 'Ozlem Kalinli'}, {'name': 'Parkin Kent'}, {'name': 'Parth Parekh'}, {'name': 'Paul Saab'}, {'name': 'Pavan Balaji'}, {'name': 'Pedro Rittner'}, {'name': 'Philip Bontrager'}, {'name': 'Pierre Roux'}, {'name': 'Piotr Dollar'}, {'name': 'Polina Zvyagina'}, {'name': 'Prashant Ratanchandani'}, {'name': 'Pritish Yuvraj'}, {'name': 'Qian Liang'}, {'name': 'Rachad Alao'}, {'name': 'Rachel Rodriguez'}, {'name': 'Rafi Ayub'}, {'name': 'Raghotham Murthy'}, {'name': 'Raghu Nayani'}, {'name': 'Rahul Mitra'}, {'name': 'Raymond Li'}, {'name': 'Rebekkah Hogan'}, {'name': 'Robin Battey'}, {'name': 'Rocky Wang'}, {'name': 'Rohan Maheswari'}, {'name': 'Russ Howes'}, {'name': 'Ruty Rinott'}, {'name': 'Sai Jayesh Bondu'}, {'name': 'Samyak Datta'}, {'name': 'Sara Chugh'}, {'name': 'Sara Hunt'}, {'name': 'Sargun Dhillon'}, {'name': 'Sasha Sidorov'}, {'name': 'Satadru Pan'}, {'name': 'Saurabh Verma'}, {'name': 'Seiji Yamamoto'}, {'name': 'Sharadh Ramaswamy'}, {'name': 'Shaun Lindsay'}, {'name': 'Shaun Lindsay'}, {'name': 'Sheng Feng'}, {'name': 'Shenghao Lin'}, {'name': 'Shengxin Cindy Zha'}, {'name': 'Shiva Shankar'}, {'name': 'Shuqiang Zhang'}, {'name': 'Shuqiang Zhang'}, {'name': 'Sinong Wang'}, {'name': 'Sneha Agarwal'}, {'name': 'Soji Sajuyigbe'}, {'name': 'Soumith Chintala'}, {'name': 'Stephanie Max'}, {'name': 'Stephen Chen'}, {'name': 'Steve Kehoe'}, {'name': 'Steve Satterfield'}, {'name': 'Sudarshan Govindaprasad'}, {'name': 'Sumit Gupta'}, {'name': 'Sungmin Cho'}, {'name': 'Sunny Virk'}, {'name': 'Suraj Subramanian'}, {'name': 'Sy Choudhury'}, {'name': 'Sydney Goldman'}, {'name': 'Tal Remez'}, {'name': 'Tamar Glaser'}, {'name': 'Tamara Best'}, {'name': 'Thilo Kohler'}, {'name': 'Thomas Robinson'}, {'name': 'Tianhe Li'}, {'name': 'Tianjun Zhang'}, {'name': 'Tim Matthews'}, {'name': 'Timothy Chou'}, {'name': 'Tzook Shaked'}, {'name': 'Varun Vontimitta'}, {'name': 'Victoria Ajayi'}, {'name': 'Victoria Montanez'}, {'name': 'Vijai Mohan'}, {'name': 'Vinay Satish Kumar'}, {'name': 'Vishal Mangla'}, {'name': 'Vítor Albiero'}, {'name': 'Vlad Ionescu'}, {'name': 'Vlad Poenaru'}, {'name': 'Vlad Tiberiu Mihailescu'}, {'name': 'Vladimir Ivanov'}, {'name': 'Wei Li'}, {'name': 'Wenchen Wang'}, {'name': 'Wenwen Jiang'}, {'name': 'Wes Bouaziz'}, {'name': 'Will Constable'}, {'name': 'Xiaocheng Tang'}, {'name': 'Xiaofang Wang'}, {'name': 'Xiaojian Wu'}, {'name': 'Xiaolan Wang'}, {'name': 'Xide Xia'}, {'name': 'Xilun Wu'}, {'name': 'Xinbo Gao'}, {'name': 'Yanjun Chen'}, {'name': 'Ye Hu'}, {'name': 'Ye Jia'}, {'name': 'Ye Qi'}, {'name': 'Yenda Li'}, {'name': 'Yilin Zhang'}, {'name': 'Ying Zhang'}, {'name': 'Yossi Adi'}, {'name': 'Youngjin Nam'}, {'name': 'Yu'}, {'name': 'Wang'}, {'name': 'Yuchen Hao'}, {'name': 'Yundi Qian'}, {'name': 'Yuzi He'}, {'name': 'Zach Rait'}, {'name': 'Zachary DeVito'}, {'name': 'Zef Rosnbrick'}, {'name': 'Zhaoduo Wen'}, {'name': 'Zhenyu Yang'}, {'name': 'Zhiwei Zhao'}]",2024-07-31T17:54:27Z
http://arxiv.org/abs/2407.21772v2,http://arxiv.org/abs/2407.21772v2,ShieldGemma: Generative AI Content Moderation Based on Gemma,"We present ShieldGemma, a comprehensive suite of LLM-based safety content
moderation models built upon Gemma2. These models provide robust,
state-of-the-art predictions of safety risks across key harm types (sexually
explicit, dangerous content, harassment, hate speech) in both user input and
LLM-generated output. By evaluating on both public and internal benchmarks, we
demonstrate superior performance compared to existing models, such as Llama
Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%).
Additionally, we present a novel LLM-based data curation pipeline, adaptable to
a variety of safety-related tasks and beyond. We have shown strong
generalization performance for model trained mainly on synthetic data. By
releasing ShieldGemma, we provide a valuable resource to the research
community, advancing LLM safety and enabling the creation of more effective
content moderation solutions for developers.","[{'name': 'Wenjun Zeng'}, {'name': 'Yuchi Liu'}, {'name': 'Ryan Mullins'}, {'name': 'Ludovic Peran'}, {'name': 'Joe Fernandez'}, {'name': 'Hamza Harkous'}, {'name': 'Karthik Narasimhan'}, {'name': 'Drew Proud'}, {'name': 'Piyush Kumar'}, {'name': 'Bhaktipriya Radharapu'}, {'name': 'Olivia Sturman'}, {'name': 'Oscar Wahltinez'}]",2024-07-31T17:48:14Z
http://arxiv.org/abs/2407.21753v1,http://arxiv.org/abs/2407.21753v1,Characterizing User Archetypes and Discussions on Scored.co,"In recent years, the proliferation of social platforms has drastically
transformed the way individuals interact, organize, and share information. In
this scenario, we experience an unprecedented increase in the scale and
complexity of interactions and, at the same time, little to no research about
some fringe social platforms. In this paper, we present a multi-dimensional
framework for characterizing nodes and hyperedges in social hypernetworks, with
a focus on the understudied alt-right platform Scored.co. Our approach
integrates the possibility of studying higher-order interactions, thanks to the
hypernetwork representation, and various node features such as user activity,
sentiment, and toxicity, with the aim to define distinct user archetypes and
understand their roles within the network. Utilizing a comprehensive dataset
from Scored.co, we analyze the dynamics of these archetypes over time and
explore their interactions and influence within the community. The framework's
versatility allows for detailed analysis of both individual user behaviors and
broader social structures. Our findings highlight the importance of
higher-order interactions in understanding social dynamics, offering new
insights into the roles and behaviors that emerge in complex online
environments.","[{'name': 'Andrea Failla'}, {'name': 'Salvatore Citraro'}, {'name': 'Giulio Rossetti'}, {'name': 'Francesco Cauteruccio'}]",2024-07-31T17:18:25Z
http://arxiv.org/abs/2407.21712v1,http://arxiv.org/abs/2407.21712v1,Adaptive Retrieval-Augmented Generation for Conversational Systems,"Despite the success of integrating large language models into the development
of conversational systems, many studies have shown the effectiveness of
retrieving and augmenting external knowledge for informative responses. Hence,
many existing studies commonly assume the always need for Retrieval Augmented
Generation (RAG) in a conversational system without explicit control. This
raises a research question about such a necessity. In this study, we propose to
investigate the need for each turn of system response to be augmented with
external knowledge. In particular, by leveraging human judgements on the binary
choice of adaptive augmentation, we develop RAGate, a gating model, which
models conversation context and relevant inputs to predict if a conversational
system requires RAG for improved responses. We conduct extensive experiments on
devising and applying RAGate to conversational models and well-rounded analyses
of different conversational scenarios. Our experimental results and analysis
indicate the effective application of RAGate in RAG-based conversational
systems in identifying system responses for appropriate RAG with high-quality
responses and a high generation confidence. This study also identifies the
correlation between the generation's confidence level and the relevance of the
augmented knowledge.","[{'name': 'Xi Wang'}, {'name': 'Procheta Sen'}, {'name': 'Ruizhe Li'}, {'name': 'Emine Yilmaz'}]",2024-07-31T16:04:03Z
http://arxiv.org/abs/2407.21669v2,http://arxiv.org/abs/2407.21669v2,Synth-Empathy: Towards High-Quality Synthetic Empathy Data,"In recent years, with the rapid advancements in large language models (LLMs),
achieving excellent empathetic response capabilities has become a crucial
prerequisite. Consequently, managing and understanding empathetic datasets have
gained increasing significance. However, empathetic data are typically
human-labeled, leading to insufficient datasets and wasted human labor. In this
work, we present Synth-Empathy, an LLM-based data generation and quality and
diversity selection pipeline that automatically generates high-quality
empathetic data while discarding low-quality data. With the data generated from
a low empathetic model, we are able to further improve empathetic response
performance and achieve state-of-the-art (SoTA) results across multiple
benchmarks. Moreover, our model achieves SoTA performance on various human
evaluation benchmarks, demonstrating its effectiveness and robustness in
real-world applications. Furthermore, we show the trade-off between data
quantity and quality, providing insights into empathetic data generation and
selection.","[{'name': 'Hao Liang'}, {'name': 'Linzhuang Sun'}, {'name': 'Jingxuan Wei'}, {'name': 'Xijie Huang'}, {'name': 'Linkun Sun'}, {'name': 'Bihui Yu'}, {'name': 'Conghui He'}, {'name': 'Wentao Zhang'}]",2024-07-31T15:12:24Z
http://arxiv.org/abs/2407.21659v2,http://arxiv.org/abs/2407.21659v2,"Defending Jailbreak Attack in VLMs via Cross-modality Information
  Detector","Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively
understand vision information, achieving remarkable performance in many
vision-centric tasks. Despite that, recent studies have shown that these models
are susceptible to jailbreak attacks, which refer to an exploitative technique
where malicious users can break the safety alignment of the target model and
generate misleading and harmful answers. This potential threat is caused by
both the inherent vulnerabilities of LLM and the larger attack scope introduced
by vision input. To enhance the security of VLMs against jailbreak attacks,
researchers have developed various defense techniques. However, these methods
either require modifications to the model's internal structure or demand
significant computational resources during the inference phase. Multimodal
information is a double-edged sword. While it increases the risk of attacks, it
also provides additional data that can enhance safeguards. Inspired by this, we
propose $\underline{\textbf{C}}$ross-modality
$\underline{\textbf{I}}$nformation
$\underline{\textbf{DE}}$tecto$\underline{\textbf{R}}$ ($\textit{CIDER})$, a
plug-and-play jailbreaking detector designed to identify maliciously perturbed
image inputs, utilizing the cross-modal similarity between harmful queries and
adversarial images. This simple yet effective cross-modality information
detector, $\textit{CIDER}$, is independent of the target VLMs and requires less
computation cost. Extensive experimental results demonstrate the effectiveness
and efficiency of $\textit{CIDER}$, as well as its transferability to both
white-box and black-box VLMs.","[{'name': 'Yue Xu'}, {'name': 'Xiuyuan Qi'}, {'name': 'Zhan Qin'}, {'name': 'Wenjie Wang'}]",2024-07-31T15:02:46Z
http://arxiv.org/abs/2408.04641v1,http://arxiv.org/abs/2408.04641v1,GPT-3 Powered Information Extraction for Building Robust Knowledge Bases,"This work uses the state-of-the-art language model GPT-3 to offer a novel
method of information extraction for knowledge base development. The suggested
method attempts to solve the difficulties associated with obtaining relevant
entities and relationships from unstructured text in order to extract
structured information. We conduct experiments on a huge corpus of text from
diverse fields to assess the performance of our suggested technique. The
evaluation measures, which are frequently employed in information extraction
tasks, include precision, recall, and F1-score. The findings demonstrate that
GPT-3 can be used to efficiently and accurately extract pertinent and correct
information from text, hence increasing the precision and productivity of
knowledge base creation. We also assess how well our suggested approach
performs in comparison to the most advanced information extraction techniques
already in use. The findings show that by utilizing only a small number of
instances in in-context learning, our suggested strategy yields competitive
outcomes with notable savings in terms of data annotation and engineering
expense. Additionally, we use our proposed method to retrieve Biomedical
information, demonstrating its practicality in a real-world setting. All things
considered, our suggested method offers a viable way to overcome the
difficulties involved in obtaining structured data from unstructured text in
order to create knowledge bases. It can greatly increase the precision and
effectiveness of information extraction, which is necessary for many
applications including chatbots, recommendation engines, and question-answering
systems.","[{'name': 'Ritabrata Roy Choudhury'}, {'name': 'Soumik Dey'}]",2024-07-31T14:59:29Z
http://arxiv.org/abs/2408.07832v1,http://arxiv.org/abs/2408.07832v1,Language Driven Slice Discovery and Error Rectification,"Error slice discovery associates structured patterns with model errors.
Existing methods discover error slices by clustering the error-prone samples
with similar patterns or assigning discrete attributes to each sample for
post-hoc analysis. While these methods aim for interpretability and easier
mitigation through reweighting or rebalancing, they may not capture the full
complexity of error patterns due to incomplete or missing attributes. Contrary
to the existing approach, this paper utilizes the reasoning capabilities of the
Large Language Model (LLM) to analyze complex error patterns and generate
testable hypotheses. This paper proposes LADDER: Language Driven slice
Discovery and Error Rectification. It first projects the model's representation
into a language-aligned feature space (\eg CLIP) to preserve semantics in the
original model feature space. This ensures the accurate retrieval of sentences
that highlight the model's errors. Next, the LLM utilizes the sentences and
generates hypotheses to discover error slices. Finally, we mitigate the error
by fine-tuning the classification head by creating a group-balanced dataset
using the hypotheses. Our entire method does not require any attribute
annotation, either explicitly or through external tagging models. We validate
our method with \textbf{five} image classification datasets. The code is
available\footnote{\url{https://github.com/batmanlab/Ladder}}","[{'name': 'Shantanu Ghosh'}, {'name': 'Chenyu Wang'}, {'name': 'Kayhan Batmanghelich'}]",2024-07-31T14:49:35Z
http://arxiv.org/abs/2407.21646v1,http://arxiv.org/abs/2407.21646v1,"Towards Achieving Human Parity on End-to-end Simultaneous Speech
  Translation via LLM Agent","In this paper, we present Cross Language Agent -- Simultaneous
Interpretation, CLASI, a high-quality and human-like Simultaneous Speech
Translation (SiST) System. Inspired by professional human interpreters, we
utilize a novel data-driven read-write strategy to balance the translation
quality and latency. To address the challenge of translating in-domain
terminologies, CLASI employs a multi-modal retrieving module to obtain relevant
information to augment the translation. Supported by LLMs, our approach can
generate error-tolerated translation by considering the input audio, historical
context, and retrieved information. Experimental results show that our system
outperforms other systems by significant margins. Aligned with professional
human interpreters, we evaluate CLASI with a better human evaluation metric,
valid information proportion (VIP), which measures the amount of information
that can be successfully conveyed to the listeners. In the real-world
scenarios, where the speeches are often disfluent, informal, and unclear, CLASI
achieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese
translation directions, respectively. In contrast, state-of-the-art commercial
or open-source systems only achieve 35.4% and 41.6%. On the extremely hard
dataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%
VIP.","[{'name': 'Shanbo Cheng'}, {'name': 'Zhichao Huang'}, {'name': 'Tom Ko'}, {'name': 'Hang Li'}, {'name': 'Ningxin Peng'}, {'name': 'Lu Xu'}, {'name': 'Qini Zhang'}]",2024-07-31T14:48:27Z
http://arxiv.org/abs/2407.21633v1,http://arxiv.org/abs/2407.21633v1,"Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank
  Adaptation","Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to
transition to unfamiliar domains without manual annotation or extensive
retraining. Prior research has approached this objective by embedding prompts
into language models (LMs). Common methodologies include integrating prompts at
the input layer or introducing learnable variables at each transformer layer.
Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at
the input layer risk underutilization, with their impact potentially
diminishing across successive transformer layers. Conversely, the addition of
learnable variables to each layer can complicate the training process and
increase inference latency. To tackle the issues mentioned above, this paper
proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture
designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank
Adaptation (LoRA) components, targeting both dialogue context processing and
prompt optimization, to ensure the comprehensive influence of prompts
throughout the transformer model layers. This is achieved without incurring
additional inference latency, showcasing an efficient integration into existing
architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets,
DualLoRA demonstrates notable improvements across multiple domains,
outperforming traditional baseline methods in zero-shot settings. Our code is
accessible at: \url{https://github.com/suntea233/DualLoRA}.","[{'name': 'Xiang Luo'}, {'name': 'Zhiwen Tang'}, {'name': 'Jin Wang'}, {'name': 'Xuejie Zhang'}]",2024-07-31T14:26:41Z
http://arxiv.org/abs/2407.21630v1,http://arxiv.org/abs/2407.21630v1,"TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization
  Methods","Authorship obfuscation aims to disguise the identity of an author within a
text by altering the writing style, vocabulary, syntax, and other linguistic
features associated with the text author. This alteration needs to balance
privacy and utility. While strong obfuscation techniques can effectively hide
the author's identity, they often degrade the quality and usefulness of the
text for its intended purpose. Conversely, maintaining high utility tends to
provide insufficient privacy, making it easier for an adversary to de-anonymize
the author. Thus, achieving an optimal trade-off between these two conflicting
objectives is crucial. In this paper, we propose TAROT: Task-Oriented
Authorship Obfuscation Using Policy Optimization, a new unsupervised authorship
obfuscation method whose goal is to optimize the privacy-utility trade-off by
regenerating the entire text considering its downstream utility. Our approach
leverages policy optimization as a fine-tuning paradigm over small language
models in order to rewrite texts by preserving author identity and downstream
task utility. We show that our approach largely reduce the accuracy of
attackers while preserving utility. We make our code and models publicly
available.","[{'name': 'Gabriel Loiseau'}, {'name': 'Damien Sileo'}, {'name': 'Damien Riquet'}, {'name': 'Maxime Meyer'}, {'name': 'Marc Tommasi'}]",2024-07-31T14:24:01Z
http://arxiv.org/abs/2407.21571v1,http://arxiv.org/abs/2407.21571v1,"PMoE: Progressive Mixture of Experts with Asymmetric Transformer for
  Continual Learning","Large Language Models (LLMs) encounter significant challenges in continual
learning due to catastrophic forgetting, where new information overwrites
previously acquired knowledge. This limitation leads to substantial
environmental and economic waste. In this study, we introduce the PMoE,
Progressive Mixture of Experts with Asymmetric Transformer, which aims to
minimize forgetting by utilizing an asymmetric design with shallow layers
dedicated to general knowledge and deep layers for new knowledge. PMoE
incorporates progressively added experts in deep layers and a router that
allocates new knowledge to the appropriate experts efficiently. The router,
positioned adjacent to the deep layers, utilizes deep features aggregating
consolidated information. This enables the router to perform efficiently,
allocating new knowledge to the appropriate experts, which progressively
increase in the deep layers. Extensive experiments on TRACE datasets and
general language understanding datasets demonstrate that the proposed PMoE
outperforms previous state-of-the-art approaches.","[{'name': 'Min Jae Jung'}, {'name': 'JooHee Kim'}]",2024-07-31T12:56:14Z
http://arxiv.org/abs/2407.21560v1,http://arxiv.org/abs/2407.21560v1,"Generative Sentiment Analysis via Latent Category Distribution and
  Constrained Decoding","Fine-grained sentiment analysis involves extracting and organizing sentiment
elements from textual data. However, existing approaches often overlook issues
of category semantic inclusion and overlap, as well as inherent structural
patterns within the target sequence. This study introduces a generative
sentiment analysis model. To address the challenges related to category
semantic inclusion and overlap, a latent category distribution variable is
introduced. By reconstructing the input of a variational autoencoder, the model
learns the intensity of the relationship between categories and text, thereby
improving sequence generation. Additionally, a trie data structure and
constrained decoding strategy are utilized to exploit structural patterns,
which in turn reduces the search space and regularizes the generation process.
Experimental results on the Restaurant-ACOS and Laptop-ACOS datasets
demonstrate a significant performance improvement compared to baseline models.
Ablation experiments further confirm the effectiveness of latent category
distribution and constrained decoding strategy.","[{'name': 'Jun Zhou'}, {'name': 'Dongyang Yu'}, {'name': 'Kamran Aziz'}, {'name': 'Fangfang Su'}, {'name': 'Qing Zhang'}, {'name': 'Fei Li'}, {'name': 'Donghong Ji'}]",2024-07-31T12:29:17Z
http://arxiv.org/abs/2407.21536v1,http://arxiv.org/abs/2407.21536v1,"Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment
  Dynamics for Multimodal Emotion Recognition","Multimodal emotion recognition in conversation (MERC) has garnered
substantial research attention recently. Existing MERC methods face several
challenges: (1) they fail to fully harness direct inter-modal cues, possibly
leading to less-than-thorough cross-modal modeling; (2) they concurrently
extract information from the same and different modalities at each network
layer, potentially triggering conflicts from the fusion of multi-source data;
(3) they lack the agility required to detect dynamic sentimental changes,
perhaps resulting in inaccurate classification of utterances with abrupt
sentiment shifts. To address these issues, a novel approach named GraphSmile is
proposed for tracking intricate emotional cues in multimodal dialogues.
GraphSmile comprises two key components, i.e., GSF and SDP modules. GSF
ingeniously leverages graph structures to alternately assimilate inter-modal
and intra-modal emotional dependencies layer by layer, adequately capturing
cross-modal cues while effectively circumventing fusion conflicts. SDP is an
auxiliary task to explicitly delineate the sentiment dynamics between
utterances, promoting the model's ability to distinguish sentimental
discrepancies. Furthermore, GraphSmile is effortlessly applied to multimodal
sentiment analysis in conversation (MSAC), forging a unified multimodal
affective model capable of executing MERC and MSAC tasks. Empirical results on
multiple benchmarks demonstrate that GraphSmile can handle complex emotional
and sentimental patterns, significantly outperforming baseline models.","[{'name': 'Jiang Li'}, {'name': 'Xiaoping Wang'}, {'name': 'Zhigang Zeng'}]",2024-07-31T11:47:36Z
http://arxiv.org/abs/2407.21531v1,http://arxiv.org/abs/2407.21531v1,"Can LLMs ""Reason"" in Music? An Evaluation of LLMs' Capability of Music
  Understanding and Generation","Symbolic Music, akin to language, can be encoded in discrete symbols. Recent
research has extended the application of large language models (LLMs) such as
GPT-4 and Llama2 to the symbolic music domain including understanding and
generation. Yet scant research explores the details of how these LLMs perform
on advanced music understanding and conditioned generation, especially from the
multi-step reasoning perspective, which is a critical aspect in the
conditioned, editable, and interactive human-computer co-creation process. This
study conducts a thorough investigation of LLMs' capability and limitations in
symbolic music processing. We identify that current LLMs exhibit poor
performance in song-level multi-step music reasoning, and typically fail to
leverage learned music knowledge when addressing complex musical tasks. An
analysis of LLMs' responses highlights distinctly their pros and cons. Our
findings suggest achieving advanced musical capability is not intrinsically
obtained by LLMs, and future research should focus more on bridging the gap
between music knowledge and reasoning, to improve the co-creation experience
for musicians.","[{'name': 'Ziya Zhou'}, {'name': 'Yuhang Wu'}, {'name': 'Zhiyue Wu'}, {'name': 'Xinyue Zhang'}, {'name': 'Ruibin Yuan'}, {'name': 'Yinghao Ma'}, {'name': 'Lu Wang'}, {'name': 'Emmanouil Benetos'}, {'name': 'Wei Xue'}, {'name': 'Yike Guo'}]",2024-07-31T11:29:46Z
http://arxiv.org/abs/2407.21530v2,http://arxiv.org/abs/2407.21530v2,Data Contamination Report from the 2024 CONDA Shared Task,"The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant
aspects of data contamination in natural language processing, where data
contamination is understood as situations where evaluation data is included in
pre-training corpora used to train large scale models, compromising evaluation
results. The workshop fostered a shared task to collect evidence on data
contamination in current available datasets and models. The goal of the shared
task and associated database is to assist the community in understanding the
extent of the problem and to assist researchers in avoiding reporting
evaluation results on known contaminated resources. The shared task provides a
structured, centralized public database for the collection of contamination
evidence, open to contributions from the community via GitHub pool requests.
This first compilation paper is based on 566 reported entries over 91
contaminated sources from a total of 23 contributors. The details of the
individual contamination events are available in the platform. The platform
continues to be online, open to contributions from the community.","[{'name': 'Oscar Sainz'}, {'name': 'Iker García-Ferrero'}, {'name': 'Alon Jacovi'}, {'name': 'Jon Ander Campos'}, {'name': 'Yanai Elazar'}, {'name': 'Eneko Agirre'}, {'name': 'Yoav Goldberg'}, {'name': 'Wei-Lin Chen'}, {'name': 'Jenny Chim'}, {'name': 'Leshem Choshen'}, {'name': ""Luca D'Amico-Wong""}, {'name': 'Melissa Dell'}, {'name': 'Run-Ze Fan'}, {'name': 'Shahriar Golchin'}, {'name': 'Yucheng Li'}, {'name': 'Pengfei Liu'}, {'name': 'Bhavish Pahwa'}, {'name': 'Ameya Prabhu'}, {'name': 'Suryansh Sharma'}, {'name': 'Emily Silcock'}, {'name': 'Kateryna Solonko'}, {'name': 'David Stap'}, {'name': 'Mihai Surdeanu'}, {'name': 'Yu-Min Tseng'}, {'name': 'Vishaal Udandarao'}, {'name': 'Zengzhi Wang'}, {'name': 'Ruijie Xu'}, {'name': 'Jinglin Yang'}]",2024-07-31T11:26:57Z
http://arxiv.org/abs/2407.21512v1,http://arxiv.org/abs/2407.21512v1,"Interpreting and learning voice commands with a Large Language Model for
  a robot system","Robots are increasingly common in industry and daily life, such as in nursing
homes where they can assist staff. A key challenge is developing intuitive
interfaces for easy communication. The use of Large Language Models (LLMs) like
GPT-4 has enhanced robot capabilities, allowing for real-time interaction and
decision-making. This integration improves robots' adaptability and
functionality. This project focuses on merging LLMs with databases to improve
decision-making and enable knowledge acquisition for request interpretation
problems.","[{'name': 'Stanislau Stankevich'}, {'name': 'Wojciech Dudek'}]",2024-07-31T10:30:31Z
http://arxiv.org/abs/2407.21491v2,http://arxiv.org/abs/2407.21491v2,Generative Expressive Conversational Speech Synthesis,"Conversational Speech Synthesis (CSS) aims to express a target utterance with
the proper speaking style in a user-agent conversation setting. Existing CSS
methods employ effective multi-modal context modeling techniques to achieve
empathy understanding and expression. However, they often need to design
complex network architectures and meticulously optimize the modules within
them. In addition, due to the limitations of small-scale datasets containing
scripted recording styles, they often fail to simulate real natural
conversational styles. To address the above issues, we propose a novel
generative expressive CSS system, termed GPT-Talker.We transform the multimodal
information of the multi-turn dialogue history into discrete token sequences
and seamlessly integrate them to form a comprehensive user-agent dialogue
context. Leveraging the power of GPT, we predict the token sequence, that
includes both semantic and style knowledge, of response for the agent. After
that, the expressive conversational speech is synthesized by the
conversation-enriched VITS to deliver feedback to the user.Furthermore, we
propose a large-scale Natural CSS Dataset called NCSSD, that includes both
naturally recorded conversational speech in improvised styles and dialogues
extracted from TV shows. It encompasses both Chinese and English languages,
with a total duration of 236 hours.We conducted comprehensive experiments on
the reliability of the NCSSD and the effectiveness of our GPT-Talker. Both
subjective and objective evaluations demonstrate that our model outperforms
other state-of-the-art CSS systems significantly in terms of naturalness and
expressiveness. The Code, Dataset, and Pre-trained Model are available at:
https://github.com/AI-S2-Lab/GPT-Talker.","[{'name': 'Rui Liu'}, {'name': 'Yifan Hu'}, {'name': 'Yi Ren'}, {'name': 'Xiang Yin'}, {'name': 'Haizhou Li'}]",2024-07-31T10:02:21Z
http://arxiv.org/abs/2407.21489v1,http://arxiv.org/abs/2407.21489v1,"Maverick: Efficient and Accurate Coreference Resolution Defying Recent
  Trends","Large autoregressive generative models have emerged as the cornerstone for
achieving the highest performance across several Natural Language Processing
tasks. However, the urge to attain superior results has, at times, led to the
premature replacement of carefully designed task-specific approaches without
exhaustive experimentation. The Coreference Resolution task is no exception;
all recent state-of-the-art solutions adopt large generative autoregressive
models that outperform encoder-based discriminative systems. In this work,we
challenge this recent trend by introducing Maverick, a carefully designed - yet
simple - pipeline, which enables running a state-of-the-art Coreference
Resolution system within the constraints of an academic budget, outperforming
models with up to 13 billion parameters with as few as 500 million parameters.
Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,
training with up to 0.006x the memory resources and obtaining a 170x faster
inference compared to previous state-of-the-art systems. We extensively
validate the robustness of the Maverick framework with an array of diverse
experiments, reporting improvements over prior systems in data-scarce,
long-document, and out-of-domain settings. We release our code and models for
research purposes at https://github.com/SapienzaNLP/maverick-coref.","[{'name': 'Giuliano Martinelli'}, {'name': 'Edoardo Barba'}, {'name': 'Roberto Navigli'}]",2024-07-31T09:58:48Z
http://arxiv.org/abs/2407.21476v1,http://arxiv.org/abs/2407.21476v1,"On the Problem of Text-To-Speech Model Selection for Synthetic Data
  Generation in Automatic Speech Recognition","The rapid development of neural text-to-speech (TTS) systems enabled its
usage in other areas of natural language processing such as automatic speech
recognition (ASR) or spoken language translation (SLT). Due to the large number
of different TTS architectures and their extensions, selecting which TTS
systems to use for synthetic data creation is not an easy task. We use the
comparison of five different TTS decoder architectures in the scope of
synthetic data generation to show the impact on CTC-based speech recognition
training. We compare the recognition results to computable metrics like NISQA
MOS and intelligibility, finding that there are no clear relations to the ASR
performance. We also observe that for data generation auto-regressive decoding
performs better than non-autoregressive decoding, and propose an approach to
quantify TTS generalization capabilities.","[{'name': 'Nick Rossenbach'}, {'name': 'Ralf Schlüter'}, {'name': 'Sakriani Sakti'}]",2024-07-31T09:37:27Z
http://arxiv.org/abs/2407.21452v1,http://arxiv.org/abs/2407.21452v1,"Navigating Beyond Instructions: Vision-and-Language Navigation in
  Obstructed Environments","Real-world navigation often involves dealing with unexpected obstructions
such as closed doors, moved objects, and unpredictable entities. However,
mainstream Vision-and-Language Navigation (VLN) tasks typically assume
instructions perfectly align with the fixed and predefined navigation graphs
without any obstructions. This assumption overlooks potential discrepancies in
actual navigation graphs and given instructions, which can cause major failures
for both indoor and outdoor agents. To address this issue, we integrate diverse
obstructions into the R2R dataset by modifying both the navigation graphs and
visual observations, introducing an innovative dataset and task, R2R with
UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers
of path obstructions to generate instruction-reality mismatches for VLN
research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods
inevitably encounter significant challenges when facing such mismatches,
indicating that they rigidly follow instructions rather than navigate
adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),
which includes a curriculum training strategy and virtual graph construction to
help agents effectively adapt to obstructed environments. Empirical results
show that ObVLN not only maintains robust performance in unobstructed scenarios
but also achieves a substantial performance advantage with unexpected
obstructions.","[{'name': 'Haodong Hong'}, {'name': 'Sen Wang'}, {'name': 'Zi Huang'}, {'name': 'Qi Wu'}, {'name': 'Jiajun Liu'}]",2024-07-31T08:55:57Z
http://arxiv.org/abs/2407.21443v1,http://arxiv.org/abs/2407.21443v1,"Improving Faithfulness of Large Language Models in Summarization via
  Sliding Generation and Self-Consistency","Despite large language models (LLMs) have demonstrated impressive performance
in various tasks, they are still suffering from the factual inconsistency
problem called hallucinations. For instance, LLMs occasionally generate content
that diverges from source article, and prefer to extract information that
appears at the beginning and end of the context, especially in long document
summarization. Inspired by these findings, we propose to improve the
faithfulness of LLMs in summarization by impelling them to process the entire
article more fairly and faithfully. We present a novel summary generation
strategy, namely SliSum, which exploits the ideas of sliding windows and
self-consistency. Specifically, SliSum divides the source article into
overlapping windows, and utilizes LLM to generate local summaries for the
content in the windows. Finally, SliSum aggregates all local summaries using
clustering and majority voting algorithm to produce more faithful summary of
entire article. Extensive experiments demonstrate that SliSum significantly
improves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and
GPT-3.5 in both short and long text summarization, while maintaining their
fluency and informativeness and without additional fine-tuning and resources.
We further conduct qualitative and quantitative studies to investigate why
SliSum works and impacts of hyperparameters in SliSum on performance.","[{'name': 'Taiji Li'}, {'name': 'Zhi Li'}, {'name': 'Yin Zhang'}]",2024-07-31T08:48:48Z
http://arxiv.org/abs/2407.21441v2,http://arxiv.org/abs/2407.21441v2,"QuestGen: Effectiveness of Question Generation Methods for Fact-Checking
  Applications","Verifying fact-checking claims poses a significant challenge, even for
humans. Recent approaches have demonstrated that decomposing claims into
relevant questions to gather evidence enhances the efficiency of the
fact-checking process. In this paper, we provide empirical evidence showing
that this question decomposition can be effectively automated. We demonstrate
that smaller generative models, fine-tuned for the question generation task
using data augmentation from various datasets, outperform large language models
by up to 8%. Surprisingly, in some cases, the evidence retrieved using
machine-generated questions proves to be significantly more effective for
fact-checking than that obtained from human-written questions. We also perform
manual evaluation of the decomposed questions to assess the quality of the
questions generated.","[{'name': 'Ritvik Setty'}, {'name': 'Vinay Setty'}]",2024-07-31T08:44:29Z
http://arxiv.org/abs/2407.21439v1,http://arxiv.org/abs/2407.21439v1,"MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented
  Generation via Knowledge-enhanced Reranking and Noise-injected Training","Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities, including text, images, audio, and video. However, a significant
drawback of MLLMs is their reliance on static training data, leading to
outdated information and limited contextual awareness. This static nature
hampers their ability to provide accurate, up-to-date responses, particularly
in dynamic or rapidly evolving contexts. Integrating Multimodal
Retrieval-augmented Generation (Multimodal RAG) offers a promising solution,
but the system would inevitably encounter the multi-granularity noisy
correspondence (MNC) problem, which involves two types of noise: coarse-grained
(query-caption) and fine-grained (query-image). This noise hinders accurate
retrieval and generation. In this work, we propose \textbf{RagLLaVA}, a novel
framework with knowledge-enhanced reranking and noise-injected training, to
address these limitations. We instruction-tune the MLLM with a simple yet
effective instruction template to induce its ranking ability and serve it as a
reranker to precisely filter the top-k retrieved images. For generation, we
inject visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments are conducted on the subsets of
two datasets that require retrieving and reasoning over images to answer a
given query. Our results demonstrate the superiority of RagLLaVA in retrieving
accurately and generating robustly. Code and models are available at
https://github.com/IDEA-FinAI/RagLLaVA.","[{'name': 'Zhanpeng Chen'}, {'name': 'Chengjin Xu'}, {'name': 'Yiyan Qi'}, {'name': 'Jian Guo'}]",2024-07-31T08:43:17Z
http://arxiv.org/abs/2407.21424v2,http://arxiv.org/abs/2407.21424v2,Cost-Effective Hallucination Detection for LLMs,"Large language models (LLMs) can be prone to hallucinations - generating
unreliable outputs that are unfaithful to their inputs, external facts or
internally inconsistent. In this work, we address several challenges for
post-hoc hallucination detection in production settings. Our pipeline for
hallucination detection entails: first, producing a confidence score
representing the likelihood that a generated answer is a hallucination; second,
calibrating the score conditional on attributes of the inputs and candidate
response; finally, performing detection by thresholding the calibrated score.
We benchmark a variety of state-of-the-art scoring methods on different
datasets, encompassing question answering, fact checking, and summarization
tasks. We employ diverse LLMs to ensure a comprehensive assessment of
performance. We show that calibrating individual scoring methods is critical
for ensuring risk-aware downstream decision making. Based on findings that no
individual score performs best in all situations, we propose a multi-scoring
framework, which combines different scores and achieves top performance across
all datasets. We further introduce cost-effective multi-scoring, which can
match or even outperform more expensive detection methods, while significantly
reducing computational overhead.","[{'name': 'Simon Valentin'}, {'name': 'Jinmiao Fu'}, {'name': 'Gianluca Detommaso'}, {'name': 'Shaoyuan Xu'}, {'name': 'Giovanni Zappella'}, {'name': 'Bryan Wang'}]",2024-07-31T08:19:06Z
http://arxiv.org/abs/2407.21417v1,http://arxiv.org/abs/2407.21417v1,"Dancing in Chains: Reconciling Instruction Following and Faithfulness in
  Language Models","Modern language models (LMs) need to follow human instructions while being
faithful; yet, they often fail to achieve both. Here, we provide concrete
evidence of a trade-off between instruction following (i.e., follow open-ended
instructions) and faithfulness (i.e., ground responses in given context) when
training LMs with these objectives. For instance, fine-tuning LLaMA-7B on
instruction following datasets renders it less faithful. Conversely,
instruction-tuned Vicuna-7B shows degraded performance at following
instructions when further optimized on tasks that require contextual grounding.
One common remedy is multi-task learning (MTL) with data mixing, yet it remains
far from achieving a synergic outcome. We propose a simple yet effective method
that relies on Rejection Sampling for Continued Self-instruction Tuning
(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find
that less is more, as training ReSet with high-quality, yet substantially
smaller data (three-fold less) yields superior results. Our findings offer a
better understanding of objective discrepancies in alignment training of LMs.","[{'name': 'Zhengxuan Wu'}, {'name': 'Yuhao Zhang'}, {'name': 'Peng Qi'}, {'name': 'Yumo Xu'}, {'name': 'Rujun Han'}, {'name': 'Yian Zhang'}, {'name': 'Jifan Chen'}, {'name': 'Bonan Min'}, {'name': 'Zhiheng Huang'}]",2024-07-31T08:05:04Z
http://arxiv.org/abs/2407.21414v1,http://arxiv.org/abs/2407.21414v1,"Towards interfacing large language models with ASR systems using
  confidence measures and prompting","As large language models (LLMs) grow in parameter size and capabilities, such
as interaction through prompting, they open up new ways of interfacing with
automatic speech recognition (ASR) systems beyond rescoring n-best lists. This
work investigates post-hoc correction of ASR transcripts with LLMs. To avoid
introducing errors into likely accurate transcripts, we propose a range of
confidence-based filtering methods. Our results indicate that this can improve
the performance of less competitive ASR systems.","[{'name': 'Maryam Naderi'}, {'name': 'Enno Hermann'}, {'name': 'Alexandre Nanchen'}, {'name': 'Sevada Hovsepyan'}, {'name': 'Mathew Magimai. -Doss'}]",2024-07-31T08:00:41Z
http://arxiv.org/abs/2407.21384v1,http://arxiv.org/abs/2407.21384v1,"GEGA: Graph Convolutional Networks and Evidence Retrieval Guided
  Attention for Enhanced Document-level Relation Extraction","Document-level relation extraction (DocRE) aims to extract relations between
entities from unstructured document text. Compared to sentence-level relation
extraction, it requires more complex semantic understanding from a broader text
context. Currently, some studies are utilizing logical rules within evidence
sentences to enhance the performance of DocRE. However, in the data without
provided evidence sentences, researchers often obtain a list of evidence
sentences for the entire document through evidence retrieval (ER). Therefore,
DocRE suffers from two challenges: firstly, the relevance between evidence and
entity pairs is weak; secondly, there is insufficient extraction of complex
cross-relations between long-distance multi-entities. To overcome these
challenges, we propose GEGA, a novel model for DocRE. The model leverages graph
neural networks to construct multiple weight matrices, guiding attention
allocation to evidence sentences. It also employs multi-scale representation
aggregation to enhance ER. Subsequently, we integrate the most efficient
evidence information to implement both fully supervised and weakly supervised
training processes for the model. We evaluate the GEGA model on three widely
used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The
experimental results indicate that our model has achieved comprehensive
improvements compared to the existing SOTA model.","[{'name': 'Yanxu Mao'}, {'name': 'Peipei Liu'}, {'name': 'Tiehan Cui'}]",2024-07-31T07:15:33Z
http://arxiv.org/abs/2407.21368v1,http://arxiv.org/abs/2407.21368v1,"Prompting Medical Large Vision-Language Models to Diagnose Pathologies
  by Visual Question Answering","Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.","[{'name': 'Danfeng Guo'}, {'name': 'Demetri Terzopoulos'}]",2024-07-31T06:34:38Z
http://arxiv.org/abs/2407.21330v1,http://arxiv.org/abs/2407.21330v1,Performance of Recent Large Language Models for a Low-Resourced Language,"Large Language Models (LLMs) have shown significant advances in the past
year. In addition to new versions of GPT and Llama, several other LLMs have
been introduced recently. Some of these are open models available for download
and modification.
  Although multilingual large language models have been available for some
time, their performance on low-resourced languages such as Sinhala has been
poor. We evaluated four recent LLMs on their performance directly in the
Sinhala language, and by translation to and from English. We also evaluated
their fine-tunability with a small amount of fine-tuning data. Claude and GPT
4o perform well out-of-the-box and do significantly better than previous
versions. Llama and Mistral perform poorly but show some promise of improvement
with fine tuning.","[{'name': 'Ravindu Jayakody'}, {'name': 'Gihan Dias'}]",2024-07-31T04:38:07Z
http://arxiv.org/abs/2407.21315v2,http://arxiv.org/abs/2407.21315v2,"Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal
  Nuances","This paper introduces a novel approach to emotion detection in speech using
Large Language Models (LLMs). We address the limitation of LLMs in processing
audio inputs by translating speech characteristics into natural language
descriptions. Our method integrates these descriptions into text prompts,
enabling LLMs to perform multimodal emotion analysis without architectural
modifications. We evaluate our approach on two datasets: IEMOCAP and MELD,
demonstrating significant improvements in emotion recognition accuracy,
particularly for high-quality audio data. Our experiments show that
incorporating speech descriptions yields a 2 percentage point increase in
weighted F1 score on IEMOCAP (from 70.111\% to 72.596\%). We also compare
various LLM architectures and explore the effectiveness of different feature
representations. Our findings highlight the potential of this approach in
enhancing emotion detection capabilities of LLMs and underscore the importance
of audio quality in speech-based emotion recognition tasks. We'll release the
source code on Github.","[{'name': 'Zehui Wu'}, {'name': 'Ziwei Gong'}, {'name': 'Lin Ai'}, {'name': 'Pengyuan Shi'}, {'name': 'Kaan Donbekci'}, {'name': 'Julia Hirschberg'}]",2024-07-31T03:53:14Z
http://arxiv.org/abs/2407.21276v2,http://arxiv.org/abs/2407.21276v2,Multi-Level Querying using A Knowledge Pyramid,"This paper addresses the need for improved precision in existing
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395\% F1 gain by improving its performance from 0.1636 to 0.8109.","[{'name': 'Rubing Chen'}, {'name': 'Xulu Zhang'}, {'name': 'Jiaxin Wu'}, {'name': 'Wenqi Fan'}, {'name': 'Xiao-Yong Wei'}, {'name': 'Qing Li'}]",2024-07-31T01:51:24Z
http://arxiv.org/abs/2407.21264v2,http://arxiv.org/abs/2407.21264v2,"Model Attribution in LLM-Generated Disinformation: A Domain
  Generalization Approach with Supervised Contrastive Learning","Model attribution for LLM-generated disinformation poses a significant
challenge in understanding its origins and mitigating its spread. This task is
especially challenging because modern large language models (LLMs) produce
disinformation with human-like quality. Additionally, the diversity in
prompting methods used to generate disinformation complicates accurate source
attribution. These methods introduce domain-specific features that can mask the
fundamental characteristics of the models. In this paper, we introduce the
concept of model attribution as a domain generalization problem, where each
prompting method represents a unique domain. We argue that an effective
attribution model must be invariant to these domain-specific features. It
should also be proficient in identifying the originating models across all
scenarios, reflecting real-world detection challenges. To address this, we
introduce a novel approach based on Supervised Contrastive Learning. This
method is designed to enhance the model's robustness to variations in prompts
and focuses on distinguishing between different source LLMs. We evaluate our
model through rigorous experiments involving three common prompting methods:
``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:
``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the
effectiveness of our approach in model attribution tasks, achieving
state-of-the-art performance across diverse and unseen datasets.","[{'name': 'Alimohammad Beigi'}, {'name': 'Zhen Tan'}, {'name': 'Nivedh Mudiam'}, {'name': 'Canyu Chen'}, {'name': 'Kai Shu'}, {'name': 'Huan Liu'}]",2024-07-31T00:56:09Z
http://arxiv.org/abs/2407.21248v1,http://arxiv.org/abs/2407.21248v1,"Adaptive Pre-training Data Detection for Large Language Models via
  Surprising Tokens","While large language models (LLMs) are extensively used, there are raising
concerns regarding privacy, security, and copyright due to their opaque
training data, which brings the problem of detecting pre-training data on the
table. Current solutions to this problem leverage techniques explored in
machine learning privacy such as Membership Inference Attacks (MIAs), which
heavily depend on LLMs' capability of verbatim memorization. However, this
reliance presents challenges, especially given the vast amount of training data
and the restricted number of effective training epochs. In this paper, we
propose an adaptive pre-training data detection method which alleviates this
reliance and effectively amplify the identification. Our method adaptively
locates \textit{surprising tokens} of the input. A token is surprising to a LLM
if the prediction on the token is ""certain but wrong"", which refers to low
Shannon entropy of the probability distribution and low probability of the
ground truth token at the same time. By using the prediction probability of
surprising tokens to measure \textit{surprising}, the detection method is
achieved based on the simple hypothesis that seeing seen data is less
surprising for the model compared with seeing unseen data. The method can be
applied without any access to the the pre-training data corpus or additional
training like reference models. Our approach exhibits a consistent enhancement
compared to existing methods in diverse experiments conducted on various
benchmarks and models, achieving a maximum improvement of 29.5\%. We also
introduce a new benchmark Dolma-Book developed upon a novel framework, which
employs book data collected both before and after model training to provide
further evaluation.","[{'name': 'Anqi Zhang'}, {'name': 'Chaofeng Wu'}]",2024-07-30T23:43:59Z
http://arxiv.org/abs/2407.21229v1,http://arxiv.org/abs/2407.21229v1,"Advancing Vietnamese Visual Question Answering with Transformer and
  Convolutional Integration","Visual Question Answering (VQA) has recently emerged as a potential research
domain, captivating the interest of many in the field of artificial
intelligence and computer vision. Despite the prevalence of approaches in
English, there is a notable lack of systems specifically developed for certain
languages, particularly Vietnamese. This study aims to bridge this gap by
conducting comprehensive experiments on the Vietnamese Visual Question
Answering (ViVQA) dataset, demonstrating the effectiveness of our proposed
model. In response to community interest, we have developed a model that
enhances image representation capabilities, thereby improving overall
performance in the ViVQA system. Specifically, our model integrates the
Bootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2)
and the convolutional neural network EfficientNet to extract and process both
local and global features from images. This integration leverages the strengths
of transformer-based architectures for capturing comprehensive contextual
information and convolutional networks for detailed local features. By freezing
the parameters of these pre-trained models, we significantly reduce the
computational cost and training time, while maintaining high performance. This
approach significantly improves image representation and enhances the
performance of existing VQA systems. We then leverage a multi-modal fusion
module based on a general-purpose multi-modal foundation model (BEiT-3) to fuse
the information between visual and textual features. Our experimental findings
demonstrate that our model surpasses competing baselines, achieving promising
performance. This is particularly evident in its accuracy of $71.04\%$ on the
test set of the ViVQA dataset, marking a significant advancement in our
research area. The code is available at https://github.com/nngocson2002/ViVQA.","[{'name': 'Ngoc Son Nguyen'}, {'name': 'Van Son Nguyen'}, {'name': 'Tung Le'}]",2024-07-30T22:32:50Z
http://arxiv.org/abs/2407.21191v1,http://arxiv.org/abs/2407.21191v1,GenRec: Generative Personalized Sequential Recommendation,"Sequential recommendation is a task to capture hidden user preferences from
historical user item interaction data. Significant progress has been made in
this domain by leveraging classification based learning methods. Inspired by
the recent paradigm of 'pretrain, prompt and predict' in NLP, we consider
sequential recommendation as a sequence to sequence generation task and propose
a novel model named Generative Recommendation (GenRec). Unlike classification
based models that learn explicit user and item representations, GenRec utilizes
the sequence modeling capability of Transformer and adopts the masked item
prediction objective to effectively learn the hidden bidirectional sequential
patterns. Different from existing generative sequential recommendation models,
GenRec does not rely on manually designed hard prompts. The input to GenRec is
textual user item sequence and the output is top ranked next items. Moreover,
GenRec is lightweight and requires only a few hours to train effectively in
low-resource settings, making it highly applicable to real-world scenarios and
helping to democratize large language models in the sequential recommendation
domain. Our extensive experiments have demonstrated that GenRec generalizes on
various public real-world datasets and achieves state-of-the-art results. Our
experiments also validate the effectiveness of the the proposed masked item
prediction objective that improves the model performance by a large margin.","[{'name': 'Panfeng Cao'}, {'name': 'Pietro Lio'}]",2024-07-30T20:58:36Z
http://arxiv.org/abs/2407.21170v1,http://arxiv.org/abs/2407.21170v1,Decomposed Prompting to Answer Questions on a Course Discussion Board,"We propose and evaluate a question-answering system that uses decomposed
prompting to classify and answer student questions on a course discussion
board. Our system uses a large language model (LLM) to classify questions into
one of four types: conceptual, homework, logistics, and not answerable. This
enables us to employ a different strategy for answering questions that fall
under different types. Using a variant of GPT-3, we achieve $81\%$
classification accuracy. We discuss our system's performance on answering
conceptual questions from a machine learning course and various failure modes.","[{'name': 'Brandon Jaipersaud'}, {'name': 'Paul Zhang'}, {'name': 'Jimmy Ba'}, {'name': 'Andrew Petersen'}, {'name': 'Lisa Zhang'}, {'name': 'Michael R. Zhang'}]",2024-07-30T20:24:44Z
http://arxiv.org/abs/2408.04640v1,http://arxiv.org/abs/2408.04640v1,LLMs for Enhanced Agricultural Meteorological Recommendations,"Agricultural meteorological recommendations are crucial for enhancing crop
productivity and sustainability by providing farmers with actionable insights
based on weather forecasts, soil conditions, and crop-specific data. This paper
presents a novel approach that leverages large language models (LLMs) and
prompt engineering to improve the accuracy and relevance of these
recommendations. We designed a multi-round prompt framework to iteratively
refine recommendations using updated data and feedback, implemented on ChatGPT,
Claude2, and GPT-4. Our method was evaluated against baseline models and a
Chain-of-Thought (CoT) approach using manually collected datasets. The results
demonstrate significant improvements in accuracy and contextual relevance, with
our approach achieving up to 90\% accuracy and high GPT-4 scores. Additional
validation through real-world pilot studies further confirmed the practical
benefits of our method, highlighting its potential to transform agricultural
practices and decision-making.","[{'name': 'Ji-jun Park'}, {'name': 'Soo-joon Choi'}]",2024-07-30T18:10:49Z
http://arxiv.org/abs/2407.21018v1,http://arxiv.org/abs/2407.21018v1,ThinK: Thinner Key Cache by Query-Driven Pruning,"Large Language Models (LLMs) have revolutionized the field of natural
language processing, achieving unprecedented performance across a variety of
applications by leveraging increased model sizes and sequence lengths. However,
the associated rise in computational and memory costs poses significant
challenges, particularly in managing long sequences due to the quadratic
complexity of the transformer attention mechanism. This paper focuses on the
long-context scenario, addressing the inefficiencies in KV cache memory
consumption during inference. Unlike existing approaches that optimize the
memory based on the sequence lengths, we uncover that the channel dimension of
the KV cache exhibits significant redundancy, characterized by unbalanced
magnitude distribution and low-rank structure in attention weights. Based on
these observations, we propose ThinK, a novel query-dependent KV cache pruning
method designed to minimize attention weight loss while selectively pruning the
least significant channels. Our approach not only maintains or enhances model
accuracy but also achieves a reduction in memory costs by over 20% compared
with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and
Mistral models across various long-sequence datasets confirm the efficacy of
ThinK, setting a new precedent for efficient LLM deployment without
compromising performance. We also outline the potential of extending our method
to value cache pruning, demonstrating ThinK's versatility and broad
applicability in reducing both memory and computational overheads.","[{'name': 'Yuhui Xu'}, {'name': 'Zhanming Jie'}, {'name': 'Hanze Dong'}, {'name': 'Lei Wang'}, {'name': 'Xudong Lu'}, {'name': 'Aojun Zhou'}, {'name': 'Amrita Saha'}, {'name': 'Caiming Xiong'}, {'name': 'Doyen Sahoo'}]",2024-07-30T17:59:08Z
http://arxiv.org/abs/2407.21092v1,http://arxiv.org/abs/2407.21092v1,"Entropy, Thermodynamics and the Geometrization of the Language Model","In this paper, we discuss how pure mathematics and theoretical physics can be
applied to the study of language models. Using set theory and analysis, we
formulate mathematically rigorous definitions of language models, and introduce
the concept of the moduli space of distributions for a language model. We
formulate a generalized distributional hypothesis using functional analysis and
topology. We define the entropy function associated with a language model and
show how it allows us to understand many interesting phenomena in languages. We
argue that the zero points of the entropy function and the points where the
entropy is close to 0 are the key obstacles for an LLM to approximate an
intelligent language model, which explains why good LLMs need billions of
parameters. Using the entropy function, we formulate a conjecture about AGI.
  Then, we show how thermodynamics gives us an immediate interpretation to
language models. In particular we will define the concepts of partition
function, internal energy and free energy for a language model, which offer
insights into how language models work. Based on these results, we introduce a
general concept of the geometrization of language models and define what is
called the Boltzmann manifold. While the current LLMs are the special cases of
the Boltzmann manifold.",[{'name': 'Wenzhe Yang'}],2024-07-30T17:11:15Z
http://arxiv.org/abs/2408.04639v1,http://arxiv.org/abs/2408.04639v1,Abstractive summarization from Audio Transcription,"Currently, large language models are gaining popularity, their achievements
are used in many areas, ranging from text translation to generating answers to
queries. However, the main problem with these new machine learning algorithms
is that training such models requires large computing resources that only large
IT companies have. To avoid this problem, a number of methods (LoRA,
quantization) have been proposed so that existing models can be effectively
fine-tuned for specific tasks. In this paper, we propose an E2E (end to end)
audio summarization model using these techniques. In addition, this paper
examines the effectiveness of these approaches to the problem under
consideration and draws conclusions about the applicability of these methods.",[{'name': 'Ilia Derkach'}],2024-07-30T16:38:38Z
http://arxiv.org/abs/2407.20910v1,http://arxiv.org/abs/2407.20910v1,"Enabling Contextual Soft Moderation on Social Media through Contrastive
  Textual Deviation","Automated soft moderation systems are unable to ascertain if a post supports
or refutes a false claim, resulting in a large number of contextual false
positives. This limits their effectiveness, for example undermining trust in
health experts by adding warnings to their posts or resorting to vague warnings
instead of granular fact-checks, which result in desensitizing users. In this
paper, we propose to incorporate stance detection into existing automated
soft-moderation pipelines, with the goal of ruling out contextual false
positives and providing more precise recommendations for social media content
that should receive warnings. We develop a textual deviation task called
Contrastive Textual Deviation (CTD) and show that it outperforms existing
stance detection approaches when applied to soft moderation.We then integrate
CTD into the stateof-the-art system for automated soft moderation Lambretta,
showing that our approach can reduce contextual false positives from 20% to
2.1%, providing another important building block towards deploying reliable
automated soft moderation tools on social media.","[{'name': 'Pujan Paudel'}, {'name': 'Mohammad Hammas Saeed'}, {'name': 'Rebecca Auger'}, {'name': 'Chris Wells'}, {'name': 'Gianluca Stringhini'}]",2024-07-30T15:37:05Z
http://arxiv.org/abs/2407.20906v1,http://arxiv.org/abs/2407.20906v1,Automated Review Generation Method Based on Large Language Models,"Literature research, vital for scientific advancement, is overwhelmed by the
vast ocean of available information. Addressing this, we propose an automated
review generation method based on Large Language Models (LLMs) to streamline
literature processing and reduce cognitive load. In case study on propane
dehydrogenation (PDH) catalysts, our method swiftly generated comprehensive
reviews from 343 articles, averaging seconds per article per LLM account.
Extended analysis of 1041 articles provided deep insights into catalysts'
composition, structure, and performance. Recognizing LLMs' hallucinations, we
employed a multi-layered quality control strategy, ensuring our method's
reliability and effective hallucination mitigation. Expert verification
confirms the accuracy and citation integrity of generated reviews,
demonstrating LLM hallucination risks reduced to below 0.5% with over 95%
confidence. Released Windows application enables one-click review generation,
aiding researchers in tracking advancements and recommending literature. This
approach showcases LLMs' role in enhancing scientific research productivity and
sets the stage for further exploration.","[{'name': 'Shican Wu'}, {'name': 'Xiao Ma'}, {'name': 'Dehui Luo'}, {'name': 'Lulu Li'}, {'name': 'Xiangcheng Shi'}, {'name': 'Xin Chang'}, {'name': 'Xiaoyun Lin'}, {'name': 'Ran Luo'}, {'name': 'Chunlei Pei'}, {'name': 'Zhi-Jian Zhao'}, {'name': 'Jinlong Gong'}]",2024-07-30T15:26:36Z
http://arxiv.org/abs/2407.20899v1,http://arxiv.org/abs/2407.20899v1,"Faithful and Plausible Natural Language Explanations for Image
  Classification: A Pipeline Approach","Existing explanation methods for image classification struggle to provide
faithful and plausible explanations. This paper addresses this issue by
proposing a post-hoc natural language explanation method that can be applied to
any CNN-based classifier without altering its training process or affecting
predictive performance. By analysing influential neurons and the corresponding
activation maps, the method generates a faithful description of the
classifier's decision process in the form of a structured meaning
representation, which is then converted into text by a language model. Through
this pipeline approach, the generated explanations are grounded in the neural
network architecture, providing accurate insight into the classification
process while remaining accessible to non-experts. Experimental results show
that the NLEs constructed by our method are significantly more plausible and
faithful. In particular, user interventions in the neural network structure
(masking of neurons) are three times more effective than the baselines.","[{'name': 'Adam Wojciechowski'}, {'name': 'Mateusz Lango'}, {'name': 'Ondrej Dusek'}]",2024-07-30T15:17:15Z
http://arxiv.org/abs/2408.07302v1,http://arxiv.org/abs/2408.07302v1,"Effects of a Prompt Engineering Intervention on Undergraduate Students'
  AI Self-Efficacy, AI Knowledge and Prompt Engineering Ability: A Mixed
  Methods Study","Prompt engineering is critical for effective interaction with large language
models (LLMs) such as ChatGPT. However, efforts to teach this skill to students
have been limited. This study designed and implemented a prompt engineering
intervention, examining its influence on undergraduate students' AI
self-efficacy, AI knowledge, and proficiency in creating effective prompts. The
intervention involved 27 students who participated in a 100-minute workshop
conducted during their history course at a university in Hong Kong. During the
workshop, students were introduced to prompt engineering strategies, which they
applied to plan the course's final essay task. Multiple data sources were
collected, including students' responses to pre- and post-workshop
questionnaires, pre- and post-workshop prompt libraries, and written
reflections. The study's findings revealed that students demonstrated a higher
level of AI self-efficacy, an enhanced understanding of AI concepts, and
improved prompt engineering skills because of the intervention. These findings
have implications for AI literacy education, as they highlight the importance
of prompt engineering training for specific higher education use cases. This is
a significant shift from students haphazardly and intuitively learning to
engineer prompts. Through prompt engineering education, educators can faciitate
students' effective navigation and leverage of LLMs to support their
coursework.","[{'name': 'David James Woo'}, {'name': 'Deliang Wang'}, {'name': 'Tim Yung'}, {'name': 'Kai Guo'}]",2024-07-30T15:05:24Z
http://arxiv.org/abs/2407.20884v1,http://arxiv.org/abs/2407.20884v1,"Effective Black Box Testing of Sentiment Analysis Classification
  Networks","Transformer-based neural networks have demonstrated remarkable performance in
natural language processing tasks such as sentiment analysis. Nevertheless, the
issue of ensuring the dependability of these complicated architectures through
comprehensive testing is still open. This paper presents a collection of
coverage criteria specifically designed to assess test suites created for
transformer-based sentiment analysis networks. Our approach utilizes input
space partitioning, a black-box method, by considering emotionally relevant
linguistic features such as verbs, adjectives, adverbs, and nouns. In order to
effectively produce test cases that encompass a wide range of emotional
elements, we utilize the k-projection coverage metric. This metric minimizes
the complexity of the problem by examining subsets of k features at the same
time, hence reducing dimensionality. Large language models are employed to
generate sentences that display specific combinations of emotional features.
The findings from experiments obtained from a sentiment analysis dataset
illustrate that our criteria and generated tests have led to an average
increase of 16\% in test coverage. In addition, there is a corresponding
average decrease of 6.5\% in model accuracy, showing the ability to identify
vulnerabilities. Our work provides a foundation for improving the dependability
of transformer-based sentiment analysis systems through comprehensive test
evaluation.","[{'name': 'Parsa Karbasizadeh'}, {'name': 'Fathiyeh Faghih'}, {'name': 'Pouria Golshanrad'}]",2024-07-30T14:58:11Z
http://arxiv.org/abs/2408.06370v1,http://arxiv.org/abs/2408.06370v1,Lyrics Transcription for Humans: A Readability-Aware Benchmark,"Writing down lyrics for human consumption involves not only accurately
capturing word sequences, but also incorporating punctuation and formatting for
clarity and to convey contextual information. This includes song structure,
emotional emphasis, and contrast between lead and background vocals. While
automatic lyrics transcription (ALT) systems have advanced beyond producing
unstructured strings of words and are able to draw on wider context, ALT
benchmarks have not kept pace and continue to focus exclusively on words. To
address this gap, we introduce Jam-ALT, a comprehensive lyrics transcription
benchmark. The benchmark features a complete revision of the JamendoLyrics
dataset, in adherence to industry standards for lyrics transcription and
formatting, along with evaluation metrics designed to capture and assess the
lyric-specific nuances, laying the foundation for improving the readability of
lyrics. We apply the benchmark to recent transcription systems and present
additional error analysis, as well as an experimental comparison with a
classical music dataset.","[{'name': 'Ondřej Cífka'}, {'name': 'Hendrik Schreiber'}, {'name': 'Luke Miner'}, {'name': 'Fabian-Robert Stöter'}]",2024-07-30T14:20:09Z
http://arxiv.org/abs/2407.20756v3,http://arxiv.org/abs/2407.20756v3,"SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision
  Language Models","Recently, with the rise of web images, managing and understanding large-scale
image datasets has become increasingly important. Vision Large Language Models
(VLLMs) have recently emerged due to their robust vision-understanding
capabilities. However, training these models requires vast amounts of data,
posing challenges to efficiency, effectiveness, data quality, and privacy. In
this paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.
Unlike existing methods that generate captions from images, SynthVLM employs
advanced diffusion models and high-quality captions to automatically generate
and select high-resolution images from captions, creating precisely aligned
image-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)
performance on various vision question answering tasks, maintaining high
alignment quality and preserving advanced language abilities. Moreover,
SynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in
performance while significantly reducing computational overhead. Crucially, our
method's reliance on purely generated data ensures the preservation of privacy,
achieving SoTA performance with just 100k data points (only 18% of the official
dataset size).","[{'name': 'Zheng Liu'}, {'name': 'Hao Liang'}, {'name': 'Xijie Huang'}, {'name': 'Wentao Xiong'}, {'name': 'Qinhan Yu'}, {'name': 'Linzhuang Sun'}, {'name': 'Chong Chen'}, {'name': 'Conghui He'}, {'name': 'Bin Cui'}, {'name': 'Wentao Zhang'}]",2024-07-30T11:57:40Z
http://arxiv.org/abs/2407.20750v1,http://arxiv.org/abs/2407.20750v1,"JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create
  State-of-the-Art Japanese Retrievers with Constrained Resources","Neural Information Retrieval has advanced rapidly in high-resource languages,
but progress in lower-resource ones such as Japanese has been hindered by data
scarcity, among other challenges. Consequently, multilingual models have
dominated Japanese retrieval, despite their computational inefficiencies and
inability to capture linguistic nuances. While recent multi-vector monolingual
models like JaColBERT have narrowed this gap, they still lag behind
multilingual methods in large-scale evaluations. This work addresses the
suboptimal training methods of multi-vector retrievers in lower-resource
settings, focusing on Japanese. We systematically evaluate and improve key
aspects of the inference and training settings of JaColBERT, and more broadly,
multi-vector models. We further enhance performance through a novel checkpoint
merging step, showcasing it to be an effective way of combining the benefits of
fine-tuning with the generalization capabilities of the original checkpoint.
Building on our analysis, we introduce a novel training recipe, resulting in
the JaColBERTv2.5 model. JaColBERTv2.5, with only 110 million parameters and
trained in under 15 hours on 4 A100 GPUs, significantly outperforms all
existing methods across all common benchmarks, reaching an average score of
0.754, significantly above the previous best of 0.720. To support future
research, we make our final models, intermediate checkpoints and all data used
publicly available.",[{'name': 'Benjamin Clavié'}],2024-07-30T11:42:19Z
http://arxiv.org/abs/2407.20743v1,http://arxiv.org/abs/2407.20743v1,Meltemi: The first open Large Language Model for Greek,"We describe the development and capabilities of Meltemi 7B, the first open
Large Language Model for the Greek language. Meltemi 7B has 7 billion
parameters and is trained on a 40 billion token Greek corpus. For the
development of Meltemi 7B, we adapt Mistral, by continuous pretraining on the
Greek Corpus. Meltemi 7B contains up-to-date information up to September 2023.
Furthermore, we have translated and curated a Greek instruction corpus, which
has been used for the instruction-tuning of a chat model, named Meltemi 7B
Instruct. Special care has been given to the alignment and the removal of toxic
content for the Meltemi 7B Instruct. The developed models are evaluated on a
broad set of collected evaluation corpora, and examples of prompts and
responses are presented. Both Meltemi 7B and Meltemi 7B Instruct are available
at https://huggingface.co/ilsp under the Apache 2.0 license.","[{'name': 'Leon Voukoutis'}, {'name': 'Dimitris Roussis'}, {'name': 'Georgios Paraskevopoulos'}, {'name': 'Sokratis Sofianopoulos'}, {'name': 'Prokopis Prokopidis'}, {'name': 'Vassilis Papavasileiou'}, {'name': 'Athanasios Katsamanis'}, {'name': 'Stelios Piperidis'}, {'name': 'Vassilis Katsouros'}]",2024-07-30T11:22:52Z
http://arxiv.org/abs/2407.20729v1,http://arxiv.org/abs/2407.20729v1,"Adapting Safe-for-Work Classifier for Malaysian Language Text: Enhancing
  Alignment in LLM-Ops Framework","As large language models (LLMs) become increasingly integrated into
operational workflows (LLM-Ops), there is a pressing need for effective
guardrails to ensure safe and aligned interactions, including the ability to
detect potentially unsafe or inappropriate content across languages. However,
existing safe-for-work classifiers are primarily focused on English text. To
address this gap for the Malaysian language, we present a novel safe-for-work
text classifier tailored specifically for Malaysian language content. By
curating and annotating a first-of-its-kind dataset of Malaysian text spanning
multiple content categories, we trained a classification model capable of
identifying potentially unsafe material using state-of-the-art natural language
processing techniques. This work represents an important step in enabling safer
interactions and content filtering to mitigate potential risks and ensure
responsible deployment of LLMs. To maximize accessibility and promote further
research towards enhancing alignment in LLM-Ops for the Malaysian context, the
model is publicly released at
https://huggingface.co/malaysia-ai/malaysian-sfw-classifier.","[{'name': 'Aisyah Razak'}, {'name': 'Ariff Nazhan'}, {'name': 'Kamarul Adha'}, {'name': 'Wan Adzhar Faiq Adzlan'}, {'name': 'Mas Aisyah Ahmad'}, {'name': 'Ammar Azman'}]",2024-07-30T10:51:51Z
http://arxiv.org/abs/2407.20700v1,http://arxiv.org/abs/2407.20700v1,"Industrial-Grade Smart Troubleshooting through Causal Technical Language
  Processing: a Proof of Concept","This paper describes the development of a causal diagnosis approach for
troubleshooting an industrial environment on the basis of the technical
language expressed in Return on Experience records. The proposed method
leverages the vectorized linguistic knowledge contained in the distributed
representation of a Large Language Model, and the causal associations entailed
by the embedded failure modes and mechanisms of the industrial assets. The
paper presents the elementary but essential concepts of the solution, which is
conceived as a causality-aware retrieval augmented generation system, and
illustrates them experimentally on a real-world Predictive Maintenance setting.
Finally, it discusses avenues of improvement for the maturity of the utilized
causal technology to meet the robustness challenges of increasingly complex
scenarios in the industry.","[{'name': 'Alexandre Trilla'}, {'name': 'Ossee Yiboe'}, {'name': 'Nenad Mijatovic'}, {'name': 'Jordi Vitrià'}]",2024-07-30T09:53:55Z
http://arxiv.org/abs/2407.20685v2,http://arxiv.org/abs/2407.20685v2,"CultureVo: The Serious Game of Utilizing Gen AI for Enhancing Cultural
  Intelligence","CultureVo, Inc. has developed the Integrated Culture Learning Suite (ICLS) to
deliver foundational knowledge of world cultures through a combination of
interactive lessons and gamified experiences. This paper explores how
Generative AI powered by open source Large Langauge Models are utilized within
the ICLS to enhance cultural intelligence. The suite employs Generative AI
techniques to automate the assessment of learner knowledge, analyze behavioral
patterns, and manage interactions with non-player characters using real time
learner assessment. Additionally, ICLS provides contextual hint and recommend
course content by assessing learner proficiency, while Generative AI
facilitates the automated creation and validation of educational content.","[{'name': 'Ajita Agarwala'}, {'name': 'Anupam Purwar'}, {'name': 'Viswanadhasai Rao'}]",2024-07-30T09:26:43Z
http://arxiv.org/abs/2407.20673v1,http://arxiv.org/abs/2407.20673v1,Label-Guided Prompt for Multi-label Few-shot Aspect Category Detection,"Multi-label few-shot aspect category detection aims at identifying multiple
aspect categories from sentences with a limited number of training instances.
The representation of sentences and categories is a key issue in this task.
Most of current methods extract keywords for the sentence representations and
the category representations. Sentences often contain many category-independent
words, which leads to suboptimal performance of keyword-based methods. Instead
of directly extracting keywords, we propose a label-guided prompt method to
represent sentences and categories. To be specific, we design label-specific
prompts to represent sentences by combining crucial contextual and semantic
information. Further, the label is introduced into a prompt to obtain category
descriptions by utilizing a large language model. This kind of category
descriptions contain the characteristics of the aspect categories, guiding the
construction of discriminative category prototypes. Experimental results on two
public datasets show that our method outperforms current state-of-the-art
methods with a 3.86% - 4.75% improvement in the Macro-F1 score.","[{'name': 'ChaoFeng Guan'}, {'name': 'YaoHui Zhu'}, {'name': 'Yu Bai'}, {'name': 'LingYun Wang'}]",2024-07-30T09:11:17Z
http://arxiv.org/abs/2407.20663v1,http://arxiv.org/abs/2407.20663v1,"ArabicNLU 2024: The First Arabic Natural Language Understanding Shared
  Task","This paper presents an overview of the Arabic Natural Language Understanding
(ArabicNLU 2024) shared task, focusing on two subtasks: Word Sense
Disambiguation (WSD) and Location Mention Disambiguation (LMD). The task aimed
to evaluate the ability of automated systems to resolve word ambiguity and
identify locations mentioned in Arabic text. We provided participants with
novel datasets, including a sense-annotated corpus for WSD, called SALMA with
approximately 34k annotated tokens, and the IDRISI-DA dataset with 3,893
annotations and 763 unique location mentions. These are challenging tasks. Out
of the 38 registered teams, only three teams participated in the final
evaluation phase, with the highest accuracy being 77.8% for WSD and the highest
MRR@1 being 95.0% for LMD. The shared task not only facilitated the evaluation
and comparison of different techniques, but also provided valuable insights and
resources for the continued advancement of Arabic NLU technologies.","[{'name': 'Mohammed Khalilia'}, {'name': 'Sanad Malaysha'}, {'name': 'Reem Suwaileh'}, {'name': 'Mustafa Jarrar'}, {'name': 'Alaa Aljabari'}, {'name': 'Tamer Elsayed'}, {'name': 'Imed Zitouni'}]",2024-07-30T08:57:01Z
http://arxiv.org/abs/2407.20657v1,http://arxiv.org/abs/2407.20657v1,Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks,"Recent vision-language foundation models, such as CLIP, have demonstrated
superior capabilities in learning representations that can be transferable
across diverse range of downstream tasks and domains. With the emergence of
such powerful models, it has become crucial to effectively leverage their
capabilities in tackling challenging vision tasks. On the other hand, only a
few works have focused on devising adversarial examples that transfer well to
both unknown domains and model architectures. In this paper, we propose a novel
transfer attack method called PDCL-Attack, which leverages the CLIP model to
enhance the transferability of adversarial perturbations generated by a
generative model-based attack framework. Specifically, we formulate an
effective prompt-driven feature guidance by harnessing the semantic
representation power of text, particularly from the ground-truth class labels
of input images. To the best of our knowledge, we are the first to introduce
prompt learning to enhance the transferable generative attacks. Extensive
experiments conducted across various cross-domain and cross-model settings
empirically validate our approach, demonstrating its superiority over
state-of-the-art methods.","[{'name': 'Hunmin Yang'}, {'name': 'Jongoh Jeong'}, {'name': 'Kuk-Jin Yoon'}]",2024-07-30T08:52:16Z
http://arxiv.org/abs/2407.20654v1,http://arxiv.org/abs/2407.20654v1,"Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain
  Study in Italian","Addressing the challenge of limited annotated data in specialized fields and
low-resource languages is crucial for the effective use of Language Models
(LMs). While most Large Language Models (LLMs) are trained on general-purpose
English corpora, there is a notable gap in models specifically tailored for
Italian, particularly for technical and bureaucratic jargon. This paper
explores the feasibility of employing smaller, domain-specific encoder LMs
alongside prompting techniques to enhance performance in these specialized
contexts. Our study concentrates on the Italian bureaucratic and legal
language, experimenting with both general-purpose and further pre-trained
encoder-only models. We evaluated the models on downstream tasks such as
document classification and entity typing and conducted intrinsic evaluations
using Pseudo-Log-Likelihood. The results indicate that while further
pre-trained models may show diminished robustness in general knowledge, they
exhibit superior adaptability for domain-specific tasks, even in a zero-shot
setting. Furthermore, the application of calibration techniques and in-domain
verbalizers significantly enhances the efficacy of encoder models. These
domain-specialized models prove to be particularly advantageous in scenarios
where in-domain resources or expertise are scarce. In conclusion, our findings
offer new insights into the use of Italian models in specialized contexts,
which may have a significant impact on both research and industrial
applications in the digital transformation era.","[{'name': 'Serena Auriemma'}, {'name': 'Martina Miliani'}, {'name': 'Mauro Madeddu'}, {'name': 'Alessandro Bondielli'}, {'name': 'Lucia Passaro'}, {'name': 'Alessandro Lenci'}]",2024-07-30T08:50:16Z
http://arxiv.org/abs/2408.04638v1,http://arxiv.org/abs/2408.04638v1,"Affective Computing in the Era of Large Language Models: A Survey from
  the NLP Perspective","Affective Computing (AC), integrating computer science, psychology, and
cognitive science knowledge, aims to enable machines to recognize, interpret,
and simulate human emotions.To create more value, AC can be applied to diverse
scenarios, including social media, finance, healthcare, education, etc.
Affective Computing (AC) includes two mainstream tasks, i.e., Affective
Understanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained
Language Models (PLMs) for AU tasks has succeeded considerably. However, these
models lack generalization ability, requiring specialized models for specific
tasks. Additionally, traditional PLMs face challenges in AG, particularly in
generating diverse and emotionally rich responses. The emergence of Large
Language Models (LLMs), such as the ChatGPT series and LLaMA models, brings new
opportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess
capabilities of in-context learning, common sense reasoning, and advanced
sequence generation, which present unprecedented opportunities for AU. To
provide a comprehensive overview of AC in the LLMs era from an NLP perspective,
we summarize the development of LLMs research in this field, aiming to offer
new insights. Specifically, we first summarize the traditional tasks related to
AC and introduce the preliminary study based on LLMs. Subsequently, we outline
the relevant techniques of popular LLMs to improve AC tasks, including
Instruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss
full parameter fine-tuning and parameter-efficient methods such as LoRA,
P-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,
Few-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To
clearly understand the performance of LLMs on different Affective Computing
tasks, we further summarize the existing benchmarks and evaluation methods.","[{'name': 'Yiqun Zhang'}, {'name': 'Xiaocui Yang'}, {'name': 'Xingle Xu'}, {'name': 'Zeran Gao'}, {'name': 'Yijie Huang'}, {'name': 'Shiyi Mu'}, {'name': 'Shi Feng'}, {'name': 'Daling Wang'}, {'name': 'Yifei Zhang'}, {'name': 'Kaisong Song'}, {'name': 'Ge Yu'}]",2024-07-30T08:12:04Z
http://arxiv.org/abs/2407.21082v1,http://arxiv.org/abs/2407.21082v1,"Accelerating Large Language Model Inference with Self-Supervised Early
  Exits","This paper presents a novel technique for accelerating inference in large,
pre-trained language models (LLMs) by introducing early exits during inference.
The computational demands of these models, used across a wide range of
applications, can be substantial. By capitalizing on the inherent variability
in token complexity, our approach enables selective acceleration of the
inference process. Specifically, we propose the integration of early exit
''heads'' atop existing transformer layers, which facilitate conditional
terminations based on a confidence metric. These heads are trained in a
self-supervised manner using the model's own predictions as training data,
thereby eliminating the need for additional annotated data. The confidence
metric, established using a calibration set, ensures a desired level of
accuracy while enabling early termination when confidence exceeds a
predetermined threshold. Notably, our method preserves the original accuracy
and reduces computational time on certain tasks, leveraging the existing
knowledge of pre-trained LLMs without requiring extensive retraining. This
lightweight, modular modification has the potential to greatly enhance the
practical usability of LLMs, particularly in applications like real-time
language processing in resource-constrained environments.",[{'name': 'Florian Valade'}],2024-07-30T07:58:28Z
http://arxiv.org/abs/2407.20622v1,http://arxiv.org/abs/2407.20622v1,Decoding Linguistic Representations of Human Brain,"Language, as an information medium created by advanced organisms, has always
been a concern of neuroscience regarding how it is represented in the brain.
Decoding linguistic representations in the evoked brain has shown
groundbreaking achievements, thanks to the rapid improvement of neuroimaging,
medical technology, life sciences and artificial intelligence. In this work, we
present a taxonomy of brain-to-language decoding of both textual and speech
formats. This work integrates two types of research: neuroscience focusing on
language understanding and deep learning-based brain decoding. Generating
discernible language information from brain activity could not only help those
with limited articulation, especially amyotrophic lateral sclerosis (ALS)
patients but also open up a new way for the next generation's brain-computer
interface (BCI). This article will help brain scientists and deep-learning
researchers to gain a bird's eye view of fine-grained language perception, and
thus facilitate their further investigation and research of neural process and
language decoding.","[{'name': 'Yu Wang'}, {'name': 'Heyang Liu'}, {'name': 'Yuhao Wang'}, {'name': 'Chuan Xuan'}, {'name': 'Yixuan Hou'}, {'name': 'Sheng Feng'}, {'name': 'Hongcheng Liu'}, {'name': 'Yusheng Liao'}, {'name': 'Yanfeng Wang'}]",2024-07-30T07:55:44Z
http://arxiv.org/abs/2407.20608v1,http://arxiv.org/abs/2407.20608v1,"Questionnaires for Everyone: Streamlining Cross-Cultural Questionnaire
  Adaptation with GPT-Based Translation Quality Evaluation","Adapting questionnaires to new languages is a resource-intensive process
often requiring the hiring of multiple independent translators, which limits
the ability of researchers to conduct cross-cultural research and effectively
creates inequalities in research and society. This work presents a prototype
tool that can expedite the questionnaire translation process. The tool
incorporates forward-backward translation using DeepL alongside GPT-4-generated
translation quality evaluations and improvement suggestions. We conducted two
online studies in which participants translated questionnaires from English to
either German (Study 1; n=10) or Portuguese (Study 2; n=20) using our
prototype. To evaluate the quality of the translations created using the tool,
evaluation scores between conventionally translated and tool-supported versions
were compared. Our results indicate that integrating LLM-generated translation
quality evaluations and suggestions for improvement can help users
independently attain results similar to those provided by conventional,
non-NLP-supported translation methods. This is the first step towards more
equitable questionnaire-based research, powered by AI.","[{'name': 'Otso Haavisto'}, {'name': 'Robin Welsch'}]",2024-07-30T07:34:40Z
http://arxiv.org/abs/2407.20595v1,http://arxiv.org/abs/2407.20595v1,"Harvesting Textual and Structured Data from the HAL Publication
  Repository","HAL (Hyper Articles en Ligne) is the French national publication repository,
used by most higher education and research organizations for their open science
policy. As a digital library, it is a rich repository of scholarly documents,
but its potential for advanced research has been underutilized. We present
HALvest, a unique dataset that bridges the gap between citation networks and
the full text of papers submitted on HAL. We craft our dataset by filtering HAL
for scholarly publications, resulting in approximately 700,000 documents,
spanning 34 languages across 13 identified domains, suitable for language model
training, and yielding approximately 16.5 billion tokens (with 8 billion in
French and 7 billion in English, the most represented languages). We transform
the metadata of each paper into a citation network, producing a directed
heterogeneous graph. This graph includes uniquely identified authors on HAL, as
well as all open submitted papers, and their citations. We provide a baseline
for authorship attribution using the dataset, implement a range of
state-of-the-art models in graph representation learning for link prediction,
and discuss the usefulness of our generated knowledge graph structure.","[{'name': 'Francis Kulumba'}, {'name': 'Wissam Antoun'}, {'name': 'Guillaume Vimont'}, {'name': 'Laurent Romary'}]",2024-07-30T07:14:04Z
http://arxiv.org/abs/2407.20588v1,http://arxiv.org/abs/2407.20588v1,"Enhancing Agricultural Machinery Management through Advanced LLM
  Integration","The integration of artificial intelligence into agricultural practices,
specifically through Consultation on Intelligent Agricultural Machinery
Management (CIAMM), has the potential to revolutionize efficiency and
sustainability in farming. This paper introduces a novel approach that
leverages large language models (LLMs), particularly GPT-4, combined with
multi-round prompt engineering to enhance decision-making processes in
agricultural machinery management. We systematically developed and refined
prompts to guide the LLMs in generating precise and contextually relevant
outputs. Our approach was evaluated using a manually curated dataset from
various online sources, and performance was assessed with accuracy and GPT-4
Scores. Comparative experiments were conducted using LLama-2-70B, ChatGPT, and
GPT-4 models, alongside baseline and state-of-the-art methods such as Chain of
Thought (CoT) and Thought of Thought (ThoT). The results demonstrate that our
method significantly outperforms these approaches, achieving higher accuracy
and relevance in generated responses. This paper highlights the potential of
advanced prompt engineering techniques in improving the robustness and
applicability of AI in agricultural contexts.","[{'name': 'Emily Johnson'}, {'name': 'Noah Wilson'}]",2024-07-30T06:49:55Z
http://arxiv.org/abs/2407.20584v1,http://arxiv.org/abs/2407.20584v1,"Pruning Large Language Models with Semi-Structural Adaptive Sparse
  Training","Transformer-based Large Language Models (LLMs) have demonstrated remarkable
success across various challenging tasks. However, the deployment of LLMs is
hindered by their substantial parameter count and memory consumption. Recently,
numerous studies have attempted to compress LLMs by pruning them using
training-free methods. However, these pruned models often experience
significant performance degradation on complex tasks. To address this issue, we
propose a novel training pipeline for semi-structured sparse models, named
Adaptive Sparse Trainer (AST). By distilling the knowledge stored in its dense
counterpart, we prevent the sparse model from overfitting and ensure a stable
training process. Moreover, AST allows the model to adaptively select better
lottery tickets (e.g., masks) during training. Additionally, we discovered that
adding extra well-initialized parameters can further enhance model performance
with only a small increase in memory footprint. Our method significantly
narrows the performance gap between dense and sparse models while maintaining
limited computational cost. Furthermore, when combined with existing
quantization methods, AST can compress language models by up to 16x compared to
dense FP32 precision models with minimal performance loss. AST outperforms
previous state-of-the-art methods by reducing the zero-shot accuracy gap
between dense and semi-structured sparse models to 1.12% across multiple
zero-shot tasks on Llama2-7B, using less than 0.4% of the pretraining tokens.","[{'name': 'Weiyu Huang'}, {'name': 'Guohao Jian'}, {'name': 'Yuezhou Hu'}, {'name': 'Jun Zhu'}, {'name': 'Jianfei Chen'}]",2024-07-30T06:33:44Z
http://arxiv.org/abs/2407.20581v1,http://arxiv.org/abs/2407.20581v1,Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings,"We present Knesset-DictaBERT, a large Hebrew language model fine-tuned on the
Knesset Corpus, which comprises Israeli parliamentary proceedings. The model is
based on the DictaBERT architecture and demonstrates significant improvements
in understanding parliamentary language according to the MLM task. We provide a
detailed evaluation of the model's performance, showing improvements in
perplexity and accuracy over the baseline DictaBERT model.","[{'name': 'Gili Goldin'}, {'name': 'Shuly Wintner'}]",2024-07-30T06:29:01Z
http://arxiv.org/abs/2407.20578v1,http://arxiv.org/abs/2407.20578v1,"Comparison of Large Language Models for Generating Contextually Relevant
  Questions","This study explores the effectiveness of Large Language Models (LLMs) for
Automatic Question Generation in educational settings. Three LLMs are compared
in their ability to create questions from university slide text without
fine-tuning. Questions were obtained in a two-step pipeline: first, answer
phrases were extracted from slides using Llama 2-Chat 13B; then, the three
models generated questions for each answer. To analyze whether the questions
would be suitable in educational applications for students, a survey was
conducted with 46 students who evaluated a total of 246 questions across five
metrics: clarity, relevance, difficulty, slide relation, and question-answer
alignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan
T5 XXL by a small margin, particularly in terms of clarity and question-answer
alignment. GPT-3.5 especially excels at tailoring questions to match the input
answers. The contribution of this research is the analysis of the capacity of
LLMs for Automatic Question Generation in education.","[{'name': 'Ivo Lodovico Molina'}, {'name': 'Valdemar Švábenský'}, {'name': 'Tsubasa Minematsu'}, {'name': 'Li Chen'}, {'name': 'Fumiya Okubo'}, {'name': 'Atsushi Shimada'}]",2024-07-30T06:23:59Z
http://arxiv.org/abs/2407.20564v1,http://arxiv.org/abs/2407.20564v1,"CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large
  Language Models over Factual Knowledge","While large language models (LLMs) have demonstrated impressive capabilities
across various natural language processing tasks by acquiring rich factual
knowledge from their broad training data, their ability to synthesize and
logically reason with this knowledge in complex ways remains underexplored. In
this work, we present a systematic evaluation of state-of-the-art LLMs' complex
logical reasoning abilities through a novel benchmark of automatically
generated complex reasoning questions over general domain and biomedical
knowledge graphs. Our extensive experiments, employing diverse in-context
learning techniques, reveal that LLMs excel at reasoning over general world
knowledge but face significant challenges with specialized domain-specific
knowledge. We find that prompting with explicit Chain-of-Thought demonstrations
can substantially improve LLM performance on complex logical reasoning tasks
with diverse logical operations. Interestingly, our controlled evaluations
uncover an asymmetry where LLMs display proficiency at set union operations,
but struggle considerably with set intersections - a key building block of
logical reasoning. To foster further work, we will publicly release our
evaluation benchmark and code.","[{'name': 'Tianshi Zheng'}, {'name': 'Jiaxin Bai'}, {'name': 'Yicheng Wang'}, {'name': 'Tianqing Fang'}, {'name': 'Yue Guo'}, {'name': 'Yauwai Yim'}, {'name': 'Yangqiu Song'}]",2024-07-30T05:40:32Z
http://arxiv.org/abs/2407.20556v1,http://arxiv.org/abs/2407.20556v1,Survey of Design Paradigms for Social Robots,"The demand for social robots in fields like healthcare, education, and
entertainment increases due to their emotional adaptation features. These
robots leverage multimodal communication, incorporating speech, facial
expressions, and gestures to enhance user engagement and emotional support. The
understanding of design paradigms of social robots is obstructed by the
complexity of the system and the necessity to tune it to a specific task. This
article provides a structured review of social robot design paradigms,
categorizing them into cognitive architectures, role design models, linguistic
models, communication flow, activity system models, and integrated design
models. By breaking down the articles on social robot design and application
based on these paradigms, we highlight the strengths and areas for improvement
in current approaches. We further propose our original integrated design model
that combines the most important aspects of the design of social robots. Our
approach shows the importance of integrating operational, communicational, and
emotional dimensions to create more adaptive and empathetic interactions
between robots and humans.","[{'name': 'Rita Frieske'}, {'name': 'Xiaoyu Mo'}, {'name': 'Yini Fang'}, {'name': 'Jay Nieles'}, {'name': 'Bertram E. Shi'}]",2024-07-30T05:22:31Z
http://arxiv.org/abs/2407.20524v2,http://arxiv.org/abs/2407.20524v2,Contrastive Feedback Mechanism for Simultaneous Speech Translation,"Recent advances in simultaneous speech translation (SST) focus on the
decision policies that enable the use of offline-trained ST models for
simultaneous inference. These decision policies not only control the
quality-latency trade-off in SST but also mitigate the impact of unstable
predictions on translation quality by delaying translation for more context or
discarding these predictions through stable hypothesis detection. However,
these policies often overlook the potential benefits of utilizing unstable
predictions. We introduce the contrastive feedback mechanism (CFM) for SST, a
novel method that leverages these unstable predictions as feedback to improve
translation quality. CFM guides the system to eliminate undesired model
behaviors from these predictions through a contrastive objective. The
experiments on 3 state-of-the-art decision policies across 8 languages in the
MuST-C v1.0 dataset show that CFM effectively improves the performance of SST.","[{'name': 'Haotian Tan'}, {'name': 'Sakriani Sakti'}]",2024-07-30T03:50:10Z
http://arxiv.org/abs/2407.20516v1,http://arxiv.org/abs/2407.20516v1,Machine Unlearning in Generative AI: A Survey,"Generative AI technologies have been deployed in many places, such as
(multimodal) large language models and vision generative models. Their
remarkable performance should be attributed to massive training data and
emergent reasoning abilities. However, the models would memorize and generate
sensitive, biased, or dangerous information originated from the training data
especially those from web crawl. New machine unlearning (MU) techniques are
being developed to reduce or eliminate undesirable knowledge and its effects
from the models, because those that were designed for traditional
classification tasks could not be applied for Generative AI. We offer a
comprehensive survey on many things about MU in Generative AI, such as a new
problem formulation, evaluation methods, and a structured discussion on the
advantages and limitations of different kinds of MU techniques. It also
presents several critical challenges and promising directions in MU research. A
curated list of readings can be found:
https://github.com/franciscoliu/GenAI-MU-Reading.","[{'name': 'Zheyuan Liu'}, {'name': 'Guangyao Dou'}, {'name': 'Zhaoxuan Tan'}, {'name': 'Yijun Tian'}, {'name': 'Meng Jiang'}]",2024-07-30T03:26:09Z
http://arxiv.org/abs/2407.20513v1,http://arxiv.org/abs/2407.20513v1,"Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural
  Language","This paper presents a conversational pipeline for crafting domain knowledge
for complex neuro-symbolic models through natural language prompts. It
leverages large language models to generate declarative programs in the
DomiKnowS framework. The programs in this framework express concepts and their
relationships as a graph in addition to logical constraints between them. The
graph, later, can be connected to trainable neural models according to those
specifications. Our proposed pipeline utilizes techniques like dynamic
in-context demonstration retrieval, model refinement based on feedback from a
symbolic parser, visualization, and user interaction to generate the tasks'
structure and formal knowledge representation. This approach empowers domain
experts, even those not well-versed in ML/AI, to formally declare their
knowledge to be incorporated in customized neural models in the DomiKnowS
framework.","[{'name': 'Hossein Rajaby Faghihi'}, {'name': 'Aliakbar Nafar'}, {'name': 'Andrzej Uszok'}, {'name': 'Hamid Karimian'}, {'name': 'Parisa Kordjamshidi'}]",2024-07-30T03:10:30Z
http://arxiv.org/abs/2407.20485v2,http://arxiv.org/abs/2407.20485v2,"A2SF: Accumulative Attention Scoring with Forgetting Factor for Token
  Pruning in Transformer Decoder","Recently, large language models (LLM) based on transformers are facing memory
bottleneck issues due to KV cache, especially in long sequence handling.
Previous researches proposed KV cache compression techniques that identify
insignificant tokens based on Accumulative Attention Scores and removes their
items from KV cache, noting that only few tokens play an important role in
attention operations. However, we have observed that the existing Accumulative
Attention Score is not suitable for the transformer decoder structure. In the
decoder model, the number of times the Attention Score accumulates varies
depending on the order of token appearance due to the effect of masking,
causing an uneven comparison between tokens. To solve this, we propose
Accumulative Attention Score with Forgetting Factor (A2SF) technique, which
introduces a Forgetting Factor in the Attention Score accumulation process.
A2SF applies a penalty to the past Attention Score generated from old tokens by
repeatedly multiplying the Forgetting Factor to the Attention Score over time.
Therefore, older tokens receive a larger penalty, providing fairness among
different ages of tokens. Through the fair comparison among tokens, we can more
effectively select important tokens. We have verified the accuracy improvement
through A2SF in the OPT and LLaMA models and A2SF improves the accuracy of
LLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.","[{'name': 'Hyun-rae Jo'}, {'name': 'Dongkun Shin'}]",2024-07-30T01:13:42Z
http://arxiv.org/abs/2407.20454v1,http://arxiv.org/abs/2407.20454v1,"CoMMIT: Coordinated Instruction Tuning for Multimodal Large Language
  Models","Instruction tuning in multimodal large language models (MLLMs) aims to
smoothly integrate a backbone LLM with a pre-trained feature encoder for
downstream tasks. The major challenge is how to efficiently find the synergy
through cooperative learning where LLMs adapt their reasoning abilities in
downstream tasks while feature encoders adjust their encoding to provide more
relevant modal information. In this paper, we analyze the MLLM instruction
tuning from both theoretical and empirical perspectives, where we find
unbalanced learning between the two components, i.e., the feature encoder and
the LLM, can cause diminishing learning gradients that slow the model
convergence and often lead to sub-optimal results due to insufficient learning.
Inspired by our findings, we propose a measurement to quantitatively evaluate
the learning balance, based on which we further design a dynamic learning
scheduler that better coordinates the learning. In addition, we introduce an
auxiliary loss regularization method to promote updating of the generation
distribution of MLLMs considering the learning state of each model component,
which potentially prevents each component from gradient diminishing and enables
a more accurate estimation of the learning balance coefficient. We conduct
experiments with multiple LLM backbones and feature encoders, where our
techniques are model-agnostic and can be generically integrated with various
MLLM backbones. Experiment results on multiple downstream tasks and modalities
in vision and audio, demonstrate the proposed method's better efficiency and
effectiveness in MLLM instruction tuning.","[{'name': 'Junda Wu'}, {'name': 'Xintong Li'}, {'name': 'Tong Yu'}, {'name': 'Yu Wang'}, {'name': 'Xiang Chen'}, {'name': 'Jiuxiang Gu'}, {'name': 'Lina Yao'}, {'name': 'Jingbo Shang'}, {'name': 'Julian McAuley'}]",2024-07-29T23:18:55Z
http://arxiv.org/abs/2408.04637v1,http://arxiv.org/abs/2408.04637v1,"APE: Active Learning-based Tooling for Finding Informative Few-shot
  Examples for LLM-based Entity Matching","Prompt engineering is an iterative procedure often requiring extensive manual
effort to formulate suitable instructions for effectively directing large
language models (LLMs) in specific tasks. Incorporating few-shot examples is a
vital and effective approach to providing LLMs with precise instructions,
leading to improved LLM performance. Nonetheless, identifying the most
informative demonstrations for LLMs is labor-intensive, frequently entailing
sifting through an extensive search space. In this demonstration, we showcase a
human-in-the-loop tool called APE (Active Prompt Engineering) designed for
refining prompts through active learning. Drawing inspiration from active
learning, APE iteratively selects the most ambiguous examples for human
feedback, which will be transformed into few-shot examples within the prompt.
The demo recording can be found with the submission or be viewed at
https://youtu.be/OwQ6MQx53-Y.","[{'name': 'Kun Qian'}, {'name': 'Yisi Sang'}, {'name': 'Farima Fatahi Bayat'}, {'name': 'Anton Belyi'}, {'name': 'Xianqi Chu'}, {'name': 'Yash Govind'}, {'name': 'Samira Khorshidi'}, {'name': 'Rahul Khot'}, {'name': 'Katherine Luna'}, {'name': 'Azadeh Nikfarjam'}, {'name': 'Xiaoguang Qi'}, {'name': 'Fei Wu'}, {'name': 'Xianhan Zhang'}, {'name': 'Yunyao Li'}]",2024-07-29T22:22:50Z
http://arxiv.org/abs/2407.20438v1,http://arxiv.org/abs/2407.20438v1,Generating Gender Alternatives in Machine Translation,"Machine translation (MT) systems often translate terms with ambiguous gender
(e.g., English term ""the nurse"") into the gendered form that is most prevalent
in the systems' training data (e.g., ""enfermera"", the Spanish term for a female
nurse). This often reflects and perpetuates harmful stereotypes present in
society. With MT user interfaces in mind that allow for resolving gender
ambiguity in a frictionless manner, we study the problem of generating all
grammatically correct gendered translation alternatives. We open source train
and test datasets for five language pairs and establish benchmarks for this
task. Our key technical contribution is a novel semi-supervised solution for
generating alternatives that integrates seamlessly with standard MT models and
maintains high performance without requiring additional components or
increasing inference overhead.","[{'name': 'Sarthak Garg'}, {'name': 'Mozhdeh Gheini'}, {'name': 'Clara Emmanuel'}, {'name': 'Tatiana Likhomanenko'}, {'name': 'Qin Gao'}, {'name': 'Matthias Paulik'}]",2024-07-29T22:10:51Z
http://arxiv.org/abs/2407.20413v1,http://arxiv.org/abs/2407.20413v1,"Through the Looking Glass, and what Horn Clause Programs Found There","Dual Horn clauses mirror key properties of Horn clauses. This paper explores
the ``other side of the looking glass'' to reveal some expected and unexpected
symmetries and their practical uses.
  We revisit Dual Horn clauses as enablers of a form of constructive negation
that supports goal-driven forward reasoning and is valid both
intuitionistically and classically. In particular, we explore the ability to
falsify a counterfactual hypothesis in the context of a background theory
expressed as a Dual Horn clause program.
  With Dual Horn clause programs, by contrast to negation as failure, the
variable bindings in their computed answers provide explanations for the
reasons why a statement is successfully falsified. Moreover, in the
propositional case, by contrast to negation as failure as implemented with
stable models semantics in ASP systems, and similarly to Horn clause programs,
Dual Horn clause programs have polynomial complexity.
  After specifying their execution model with a metainterpreter, we devise a
compilation scheme from Dual Horn clause programs to Horn clause programs,
ensuring their execution with no performance penalty and we design the embedded
SymLP language to support combined Horn clause and Dual Horn clause programs.
  As a (motivating) application, we cast LLM reasoning chains into
propositional Horn and Dual Horn clauses that work together to constructively
prove and disprove goals and enhance Generative AI with explainability of
reasoning chains.",[{'name': 'Paul Tarau'}],2024-07-29T20:52:26Z
http://arxiv.org/abs/2407.21077v1,http://arxiv.org/abs/2407.21077v1,"Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions
  for Large Language Models","Large Language Models (LLMs) rely on instruction samples for alignment, but
creating these datasets poses challenges, particularly in expert-dependent
tasks like coding, which can be cost-prohibitive. One approach to mitigate
these challenges is synthesizing data using another LLM. In this paper, we
introduce a scalable method for generating synthetic instructions to enhance
the code generation capability of LLMs. The proposed algorithm,
Genetic-Instruct, mimics evolutionary processes, utilizing self-instruction to
create numerous synthetic samples from a limited number of seeds.
Genetic-Instruct is designed for efficient scaling of the generation process.
Fine-tuning multiple coding LLMs with the synthetic samples demonstrates a
significant improvement in their code generation accuracy compared to the
baselines.","[{'name': 'Somshubra Majumdar'}, {'name': 'Vahid Noroozi'}, {'name': 'Sean Narenthiran'}, {'name': 'Aleksander Ficek'}, {'name': 'Jagadeesh Balam'}, {'name': 'Boris Ginsburg'}]",2024-07-29T20:42:59Z
http://arxiv.org/abs/2407.20382v1,http://arxiv.org/abs/2407.20382v1,"What if Red Can Talk? Dynamic Dialogue Generation Using Large Language
  Models","Role-playing games (RPGs) provide players with a rich, interactive world to
explore. Dialogue serves as the primary means of communication between
developers and players, manifesting in various forms such as guides, NPC
interactions, and storytelling. While most games rely on written scripts to
define the main story and character personalities, player immersion can be
significantly enhanced through casual interactions between characters. With the
advent of large language models (LLMs), we introduce a dialogue filler
framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic
and contextually appropriate character interactions. We test this framework
within the environments of Final Fantasy VII Remake and Pokemon, providing
qualitative and quantitative evidence that demonstrates GPT-4's capability to
act with defined personalities and generate dialogue. However, some flaws
remain, such as GPT-4 being overly positive or more subtle personalities, such
as maturity, tend to be of lower quality compared to more overt traits like
timidity. This study aims to assist developers in crafting more nuanced filler
dialogues, thereby enriching player immersion and enhancing the overall RPG
experience.","[{'name': 'Navapat Nananukul'}, {'name': 'Wichayaporn Wongkamjan'}]",2024-07-29T19:12:18Z
http://arxiv.org/abs/2407.20371v1,http://arxiv.org/abs/2407.20371v1,"Gender, Race, and Intersectional Bias in Resume Screening via Language
  Model Retrieval","Artificial intelligence (AI) hiring tools have revolutionized resume
screening, and large language models (LLMs) have the potential to do the same.
However, given the biases which are embedded within LLMs, it is unclear whether
they can be used in this scenario without disadvantaging groups based on their
protected attributes. In this work, we investigate the possibilities of using
LLMs in a resume screening setting via a document retrieval framework that
simulates job candidate selection. Using that framework, we then perform a
resume audit study to determine whether a selection of Massive Text Embedding
(MTE) models are biased in resume screening scenarios. We simulate this for
nine occupations, using a collection of over 500 publicly available resumes and
500 job descriptions. We find that the MTEs are biased, significantly favoring
White-associated names in 85.1\% of cases and female-associated names in only
11.1\% of cases, with a minority of cases showing no statistically significant
differences. Further analyses show that Black males are disadvantaged in up to
100\% of cases, replicating real-world patterns of bias in employment settings,
and validate three hypotheses of intersectionality. We also find an impact of
document length as well as the corpus frequency of names in the selection of
resumes. These findings have implications for widely used AI tools that are
automating employment, fairness, and tech policy.","[{'name': 'Kyra Wilson'}, {'name': 'Aylin Caliskan'}]",2024-07-29T18:42:39Z
http://arxiv.org/abs/2407.21075v1,http://arxiv.org/abs/2407.21075v1,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence
features, including a ~3 billion parameter model designed to run efficiently on
devices and a large server-based language model designed for Private Cloud
Compute. These models are designed to perform a wide range of tasks
efficiently, accurately, and responsibly. This report describes the model
architecture, the data used to train the model, the training process, how the
models are optimized for inference, and the evaluation results. We highlight
our focus on Responsible AI and how the principles are applied throughout the
model development.","[{'name': 'Tom Gunter'}, {'name': 'Zirui Wang'}, {'name': 'Chong Wang'}, {'name': 'Ruoming Pang'}, {'name': 'Andy Narayanan'}, {'name': 'Aonan Zhang'}, {'name': 'Bowen Zhang'}, {'name': 'Chen Chen'}, {'name': 'Chung-Cheng Chiu'}, {'name': 'David Qiu'}, {'name': 'Deepak Gopinath'}, {'name': 'Dian Ang Yap'}, {'name': 'Dong Yin'}, {'name': 'Feng Nan'}, {'name': 'Floris Weers'}, {'name': 'Guoli Yin'}, {'name': 'Haoshuo Huang'}, {'name': 'Jianyu Wang'}, {'name': 'Jiarui Lu'}, {'name': 'John Peebles'}, {'name': 'Ke Ye'}, {'name': 'Mark Lee'}, {'name': 'Nan Du'}, {'name': 'Qibin Chen'}, {'name': 'Quentin Keunebroek'}, {'name': 'Sam Wiseman'}, {'name': 'Syd Evans'}, {'name': 'Tao Lei'}, {'name': 'Vivek Rathod'}, {'name': 'Xiang Kong'}, {'name': 'Xianzhi Du'}, {'name': 'Yanghao Li'}, {'name': 'Yongqiang Wang'}, {'name': 'Yuan Gao'}, {'name': 'Zaid Ahmed'}, {'name': 'Zhaoyang Xu'}, {'name': 'Zhiyun Lu'}, {'name': 'Al Rashid'}, {'name': 'Albin Madappally Jose'}, {'name': 'Alec Doane'}, {'name': 'Alfredo Bencomo'}, {'name': 'Allison Vanderby'}, {'name': 'Andrew Hansen'}, {'name': 'Ankur Jain'}, {'name': 'Anupama Mann Anupama'}, {'name': 'Areeba Kamal'}, {'name': 'Bugu Wu'}, {'name': 'Carolina Brum'}, {'name': 'Charlie Maalouf'}, {'name': 'Chinguun Erdenebileg'}, {'name': 'Chris Dulhanty'}, {'name': 'Dominik Moritz'}, {'name': 'Doug Kang'}, {'name': 'Eduardo Jimenez'}, {'name': 'Evan Ladd'}, {'name': 'Fangping Shi'}, {'name': 'Felix Bai'}, {'name': 'Frank Chu'}, {'name': 'Fred Hohman'}, {'name': 'Hadas Kotek'}, {'name': 'Hannah Gillis Coleman'}, {'name': 'Jane Li'}, {'name': 'Jeffrey Bigham'}, {'name': 'Jeffery Cao'}, {'name': 'Jeff Lai'}, {'name': 'Jessica Cheung'}, {'name': 'Jiulong Shan'}, {'name': 'Joe Zhou'}, {'name': 'John Li'}, {'name': 'Jun Qin'}, {'name': 'Karanjeet Singh'}, {'name': 'Karla Vega'}, {'name': 'Kelvin Zou'}, {'name': 'Laura Heckman'}, {'name': 'Lauren Gardiner'}, {'name': 'Margit Bowler'}, {'name': 'Maria Cordell'}, {'name': 'Meng Cao'}, {'name': 'Nicole Hay'}, {'name': 'Nilesh Shahdadpuri'}, {'name': 'Otto Godwin'}, {'name': 'Pranay Dighe'}, {'name': 'Pushyami Rachapudi'}, {'name': 'Ramsey Tantawi'}, {'name': 'Roman Frigg'}, {'name': 'Sam Davarnia'}, {'name': 'Sanskruti Shah'}, {'name': 'Saptarshi Guha'}, {'name': 'Sasha Sirovica'}, {'name': 'Shen Ma'}, {'name': 'Shuang Ma'}, {'name': 'Simon Wang'}, {'name': 'Sulgi Kim'}, {'name': 'Suma Jayaram'}, {'name': 'Vaishaal Shankar'}, {'name': 'Varsha Paidi'}, {'name': 'Vivek Kumar'}, {'name': 'Xin Wang'}, {'name': 'Xin Zheng'}, {'name': 'Walker Cheng'}, {'name': 'Yael Shrager'}, {'name': 'Yang Ye'}, {'name': 'Yasu Tanaka'}, {'name': 'Yihao Guo'}, {'name': 'Yunsong Meng'}, {'name': 'Zhao Tang Luo'}, {'name': 'Zhi Ouyang'}, {'name': 'Alp Aygar'}, {'name': 'Alvin Wan'}, {'name': 'Andrew Walkingshaw'}, {'name': 'Andy Narayanan'}, {'name': 'Antonie Lin'}, {'name': 'Arsalan Farooq'}, {'name': 'Brent Ramerth'}, {'name': 'Colorado Reed'}, {'name': 'Chris Bartels'}, {'name': 'Chris Chaney'}, {'name': 'David Riazati'}, {'name': 'Eric Liang Yang'}, {'name': 'Erin Feldman'}, {'name': 'Gabriel Hochstrasser'}, {'name': 'Guillaume Seguin'}, {'name': 'Irina Belousova'}, {'name': 'Joris Pelemans'}, {'name': 'Karen Yang'}, {'name': 'Keivan Alizadeh Vahid'}, {'name': 'Liangliang Cao'}, {'name': 'Mahyar Najibi'}, {'name': 'Marco Zuliani'}, {'name': 'Max Horton'}, {'name': 'Minsik Cho'}, {'name': 'Nikhil Bhendawade'}, {'name': 'Patrick Dong'}, {'name': 'Piotr Maj'}, {'name': 'Pulkit Agrawal'}, {'name': 'Qi Shan'}, {'name': 'Qichen Fu'}, {'name': 'Regan Poston'}, {'name': 'Sam Xu'}, {'name': 'Shuangning Liu'}, {'name': 'Sushma Rao'}, {'name': 'Tashweena Heeramun'}, {'name': 'Thomas Merth'}, {'name': 'Uday Rayala'}, {'name': 'Victor Cui'}, {'name': 'Vivek Rangarajan Sridhar'}, {'name': 'Wencong Zhang'}, {'name': 'Wenqi Zhang'}, {'name': 'Wentao Wu'}, {'name': 'Xingyu Zhou'}, {'name': 'Xinwen Liu'}, {'name': 'Yang Zhao'}, {'name': 'Yin Xia'}, {'name': 'Zhile Ren'}, {'name': 'Zhongzheng Ren'}]",2024-07-29T18:38:49Z
http://arxiv.org/abs/2407.20341v1,http://arxiv.org/abs/2407.20341v1,"BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger
  Visual Cues","Effectively aligning with human judgment when evaluating machine-generated
image captions represents a complex yet intriguing challenge. Existing
evaluation metrics like CIDEr or CLIP-Score fall short in this regard as they
do not take into account the corresponding image or lack the capability of
encoding fine-grained details and penalizing hallucinations. To overcome these
issues, in this paper, we propose BRIDGE, a new learnable and reference-free
image captioning metric that employs a novel module to map visual features into
dense vectors and integrates them into multi-modal pseudo-captions which are
built during the evaluation process. This approach results in a multimodal
metric that properly incorporates information from the input image without
relying on reference captions, bridging the gap between human judgment and
machine-generated image captions. Experiments spanning several datasets
demonstrate that our proposal achieves state-of-the-art results compared to
existing reference-free evaluation scores. Our source code and trained models
are publicly available at: https://github.com/aimagelab/bridge-score.","[{'name': 'Sara Sarto'}, {'name': 'Marcella Cornia'}, {'name': 'Lorenzo Baraldi'}, {'name': 'Rita Cucchiara'}]",2024-07-29T18:00:17Z
http://arxiv.org/abs/2407.20207v1,http://arxiv.org/abs/2407.20207v1,QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval,"In dense retrieval, embedding long texts into dense vectors can result in
information loss, leading to inaccurate query-text matching. Additionally,
low-quality texts with excessive noise or sparse key information are unlikely
to align well with relevant queries. Recent studies mainly focus on improving
the sentence embedding model or retrieval process. In this work, we introduce a
novel text augmentation framework for dense retrieval. This framework
transforms raw documents into information-dense text formats, which supplement
the original texts to effectively address the aforementioned issues without
modifying embedding or retrieval methodologies. Two text representations are
generated via large language models (LLMs) zero-shot prompting: question-answer
pairs and element-driven events. We term this approach QAEA-DR: unifying
question-answer generation and event extraction in a text augmentation
framework for dense retrieval. To further enhance the quality of generated
texts, a scoring-based evaluation and regeneration mechanism is introduced in
LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,
supported by both theoretical analysis and empirical experiments.","[{'name': 'Hongming Tan'}, {'name': 'Shaoxiong Zhan'}, {'name': 'Hai Lin'}, {'name': 'Hai-Tao Zheng'}, {'name': 'Wai Kin'}, {'name': 'Chan'}]",2024-07-29T17:39:08Z
http://arxiv.org/abs/2407.20189v1,http://arxiv.org/abs/2407.20189v1,"Aligning Query Representation with Rewritten Query and Relevance
  Judgments in Conversational Search","Conversational search supports multi-turn user-system interactions to solve
complex information needs. Different from the traditional single-turn ad-hoc
search, conversational search encounters a more challenging problem of
context-dependent query understanding with the lengthy and long-tail
conversational history context. While conversational query rewriting methods
leverage explicit rewritten queries to train a rewriting model to transform the
context-dependent query into a stand-stone search query, this is usually done
without considering the quality of search results. Conversational dense
retrieval methods use fine-tuning to improve a pre-trained ad-hoc query
encoder, but they are limited by the conversational search data available for
training. In this paper, we leverage both rewritten queries and relevance
judgments in the conversational search data to train a better query
representation model. The key idea is to align the query representation with
those of rewritten queries and relevant documents. The proposed model -- Query
Representation Alignment Conversational Dense Retriever, QRACDR, is tested on
eight datasets, including various settings in conversational search and ad-hoc
search. The results demonstrate the strong performance of QRACDR compared with
state-of-the-art methods, and confirm the effectiveness of representation
alignment.","[{'name': 'Fengran Mo'}, {'name': 'Chen Qu'}, {'name': 'Kelong Mao'}, {'name': 'Yihong Wu'}, {'name': 'Zhan Su'}, {'name': 'Kaiyu Huang'}, {'name': 'Jian-Yun Nie'}]",2024-07-29T17:14:36Z
http://arxiv.org/abs/2407.20183v1,http://arxiv.org/abs/2407.20183v1,MindSearch: Mimicking Human Minds Elicits Deep AI Searcher,"Information seeking and integration is a complex cognitive task that consumes
enormous time and effort. Inspired by the remarkable progress of Large Language
Models, recent works attempt to solve this task by combining LLMs and search
engines. However, these methods still obtain unsatisfying performance due to
three challenges: (1) complex requests often cannot be accurately and
completely retrieved by the search engine once (2) corresponding information to
be integrated is spread over multiple web pages along with massive noise, and
(3) a large number of web pages with long contents may quickly exceed the
maximum context length of LLMs. Inspired by the cognitive process when humans
solve these problems, we introduce MindSearch to mimic the human minds in web
information seeking and integration, which can be instantiated by a simple yet
effective LLM-based multi-agent framework. The WebPlanner models the human mind
of multi-step information seeking as a dynamic graph construction process: it
decomposes the user query into atomic sub-questions as nodes in the graph and
progressively extends the graph based on the search result from WebSearcher.
Tasked with each sub-question, WebSearcher performs hierarchical information
retrieval with search engines and collects valuable information for WebPlanner.
The multi-agent design of MindSearch enables the whole framework to seek and
integrate information parallelly from larger-scale (e.g., more than 300) web
pages in 3 minutes, which is worth 3 hours of human effort. MindSearch
demonstrates significant improvement in the response quality in terms of depth
and breadth, on both close-set and open-set QA problems. Besides, responses
from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web
and Perplexity.ai applications, which implies that MindSearch can already
deliver a competitive solution to the proprietary AI search engine.","[{'name': 'Zehui Chen'}, {'name': 'Kuikun Liu'}, {'name': 'Qiuchen Wang'}, {'name': 'Jiangning Liu'}, {'name': 'Wenwei Zhang'}, {'name': 'Kai Chen'}, {'name': 'Feng Zhao'}]",2024-07-29T17:12:40Z
http://arxiv.org/abs/2407.20177v1,http://arxiv.org/abs/2407.20177v1,"AutoScale: Automatic Prediction of Compute-optimal Data Composition for
  Training LLMs","To ensure performance on a diverse set of downstream tasks, LLMs are
pretrained via data mixtures over different domains. In this work, we
demonstrate that the optimal data composition for a fixed compute budget varies
depending on the scale of the training data, suggesting that the common
practice of empirically determining an optimal composition using small-scale
experiments will not yield the optimal data mixtures when scaling up to the
final model. To address this challenge, we propose *AutoScale*, an automated
tool that finds a compute-optimal data composition for training at any desired
target scale. AutoScale first determines the optimal composition at a small
scale using a novel bilevel optimization framework, Direct Data Optimization
(*DDO*), and then fits a predictor to estimate the optimal composition at
larger scales. The predictor's design is inspired by our theoretical analysis
of scaling laws related to data composition, which could be of independent
interest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2
Large) on RedPajama dataset, AutoScale decreases validation perplexity at least
25% faster than any baseline with up to 38% speed up compared to without
reweighting, achieving the best overall performance across downstream tasks. On
pre-training Encoder-only LMs (BERT) with masked language modeling, DDO is
shown to decrease loss on all domains while visibly improving average task
performance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by
5.9% compared with without reweighting. AutoScale speeds up training by up to
28%. Our codes are open-sourced.","[{'name': 'Feiyang Kang'}, {'name': 'Yifan Sun'}, {'name': 'Bingbing Wen'}, {'name': 'Si Chen'}, {'name': 'Dawn Song'}, {'name': 'Rafid Mahmood'}, {'name': 'Ruoxi Jia'}]",2024-07-29T17:06:30Z
http://arxiv.org/abs/2407.20083v1,http://arxiv.org/abs/2407.20083v1,"An Energy-based Model for Word-level AutoCompletion in Computer-aided
  Translation","Word-level AutoCompletion(WLAC) is a rewarding yet challenging task in
Computer-aided Translation. Existing work addresses this task through a
classification model based on a neural network that maps the hidden vector of
the input context into its corresponding label (i.e., the candidate target word
is treated as a label). Since the context hidden vector itself does not take
the label into account and it is projected to the label through a linear
classifier, the model can not sufficiently leverage valuable information from
the source sentence as verified in our experiments, which eventually hinders
its overall performance. To alleviate this issue, this work proposes an
energy-based model for WLAC, which enables the context hidden vector to capture
crucial information from the source sentence. Unfortunately, training and
inference suffer from efficiency and effectiveness challenges, thereby we
employ three simple yet effective strategies to put our model into practice.
Experiments on four standard benchmarks demonstrate that our reranking-based
approach achieves substantial improvements (about 6.07%) over the previous
state-of-the-art model. Further analyses show that each strategy of our
approach contributes to the final performance.","[{'name': 'Cheng Yang'}, {'name': 'Guoping Huang'}, {'name': 'Mo Yu'}, {'name': 'Zhirui Zhang'}, {'name': 'Siheng Li'}, {'name': 'Mingming Yang'}, {'name': 'Shuming Shi'}, {'name': 'Yujiu Yang'}, {'name': 'Lemao Liu'}]",2024-07-29T15:07:19Z
http://arxiv.org/abs/2407.20076v1,http://arxiv.org/abs/2407.20076v1,"Investigating the Impact of Semi-Supervised Methods with Data
  Augmentation on Offensive Language Detection in Romanian Language","Offensive language detection is a crucial task in today's digital landscape,
where online platforms grapple with maintaining a respectful and inclusive
environment. However, building robust offensive language detection models
requires large amounts of labeled data, which can be expensive and
time-consuming to obtain. Semi-supervised learning offers a feasible solution
by utilizing labeled and unlabeled data to create more accurate and robust
models. In this paper, we explore a few different semi-supervised methods, as
well as data augmentation techniques. Concretely, we implemented eight
semi-supervised methods and ran experiments for them using only the available
data in the RO-Offense dataset and applying five augmentation techniques before
feeding the data to the models. Experimental results demonstrate that some of
them benefit more from augmentations than others.","[{'name': 'Elena-Beatrice Nicola'}, {'name': 'Dumitru-Clementin Cercel'}, {'name': 'Florin Pop'}]",2024-07-29T15:02:51Z
http://arxiv.org/abs/2407.20046v1,http://arxiv.org/abs/2407.20046v1,Exploring Large Language Models to generate Easy to Read content,"Ensuring text accessibility and understandability are essential goals,
particularly for individuals with cognitive impairments and intellectual
disabilities, who encounter challenges in accessing information across various
mediums such as web pages, newspapers, administrative tasks, or health
documents. Initiatives like Easy to Read and Plain Language guidelines aim to
simplify complex texts; however, standardizing these guidelines remains
challenging and often involves manual processes. This work presents an
exploratory investigation into leveraging Artificial Intelligence (AI) and
Natural Language Processing (NLP) approaches to systematically simplify Spanish
texts into Easy to Read formats, with a focus on utilizing Large Language
Models (LLMs) for simplifying texts, especially in generating Easy to Read
content. The study contributes a parallel corpus of Spanish adapted for Easy To
Read format, which serves as a valuable resource for training and testing text
simplification systems. Additionally, several text simplification experiments
using LLMs and the collected corpus are conducted, involving fine-tuning and
testing a Llama2 model to generate Easy to Read content. A qualitative
evaluation, guided by an expert in text adaptation for Easy to Read content, is
carried out to assess the automatically simplified texts. This research
contributes to advancing text accessibility for individuals with cognitive
impairments, highlighting promising strategies for leveraging LLMs while
responsibly managing energy usage.","[{'name': 'Paloma Martínez'}, {'name': 'Lourdes Moreno'}, {'name': 'Alberto Ramos'}]",2024-07-29T14:30:39Z
http://arxiv.org/abs/2407.19998v1,http://arxiv.org/abs/2407.19998v1,Do LLMs Really Adapt to Domains? An Ontology Learning Perspective,"Large Language Models (LLMs) have demonstrated unprecedented prowess across
various natural language processing tasks in various application domains.
Recent studies show that LLMs can be leveraged to perform lexical semantic
tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL).
However, it has not effectively been verified whether their success is due to
their ability to reason over unstructured or semi-structured data, or their
effective learning of linguistic patterns and senses alone. This unresolved
question is particularly crucial when dealing with domain-specific data, where
the lexical senses and their meaning can completely differ from what a LLM has
learned during its training stage. This paper investigates the following
question: Do LLMs really adapt to domains and remain consistent in the
extraction of structured knowledge, or do they only learn lexical senses
instead of reasoning? To answer this question and, we devise a controlled
experiment setup that uses WordNet to synthesize parallel corpora, with English
and gibberish terms. We examine the differences in the outputs of LLMs for each
corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical
results show that, while adapting to the gibberish corpora, off-the-shelf LLMs
do not consistently reason over semantic relationships between concepts, and
instead leverage senses and their frame. However, fine-tuning improves the
performance of LLMs on lexical semantic tasks even when the domain-specific
terms are arbitrary and unseen during pre-training, hinting at the
applicability of pre-trained LLMs for OL.","[{'name': 'Huu Tan Mai'}, {'name': 'Cuong Xuan Chu'}, {'name': 'Heiko Paulheim'}]",2024-07-29T13:29:43Z
http://arxiv.org/abs/2407.19984v1,http://arxiv.org/abs/2407.19984v1,"Confidence Estimation for Automatic Detection of Depression and
  Alzheimer's Disease Based on Clinical Interviews","Speech-based automatic detection of Alzheimer's disease (AD) and depression
has attracted increased attention. Confidence estimation is crucial for a
trust-worthy automatic diagnostic system which informs the clinician about the
confidence of model predictions and helps reduce the risk of misdiagnosis. This
paper investigates confidence estimation for automatic detection of AD and
depression based on clinical interviews. A novel Bayesian approach is proposed
which uses a dynamic Dirichlet prior distribution to model the second-order
probability of the predictive distribution. Experimental results on the
publicly available ADReSS and DAIC-WOZ datasets demonstrate that the proposed
method outperforms a range of baselines for both classification accuracy and
confidence estimation.","[{'name': 'Wen Wu'}, {'name': 'Chao Zhang'}, {'name': 'Philip C. Woodland'}]",2024-07-29T13:18:23Z
http://arxiv.org/abs/2407.19967v1,http://arxiv.org/abs/2407.19967v1,"A Temporal Psycholinguistics Approach to Identity Resolution of Social
  Media Users","In this thesis, we propose an approach to identity resolution across social
media platforms using the topics, sentiments, and timings of the posts on the
platforms. After collecting the public posts of around 5000 profiles from
Disqus and Twitter, we analyze their posts to match their profiles across the
two platforms. We pursue both temporal and non-temporal methods in our
analysis. While neither approach proves definitively superior, the temporal
approach generally performs better. We found that the temporal window size
influences results more than the shifting amount. On the other hand, our
sentiment analysis shows that the inclusion of sentiment makes little
difference, probably due to flawed data extraction methods. We also
experimented with a distance-based reward-and-punishment-focused scoring model,
which achieved an accuracy of 24.198% and an average rank of 158.217 out of
2525 in our collected corpus. Future work includes refining sentiment analysis
by evaluating sentiments per topic, extending temporal analysis with additional
phases, and improving the scoring model through weight adjustments and modified
rewards.",[{'name': 'Md Touhidul Islam'}],2024-07-29T13:00:36Z
http://arxiv.org/abs/2407.19947v1,http://arxiv.org/abs/2407.19947v1,"Inference acceleration for large language models using ""stairs"" assisted
  greedy generation","Large Language Models (LLMs) with billions of parameters are known for their
impressive predicting capabilities but require lots of resources to run. With
their massive rise in popularity, even a small reduction in required resources
could have an impact on environment. On the other hand, smaller models require
fewer resources but may sacrifice accuracy. In this work, we are proposing an
implementation of ``stairs'' assisted greedy generation. It is a modified
assisted generation methodology that makes use of a smaller model's fast
generation, large model's batch prediction, and ""stairs"" validation in order to
achieve a speed up in prediction generation. Results show between 9.58 and
17.24 percent inference time reduction compared to a stand-alone large LLM
prediction in a text generation task without a loss in accuracy.","[{'name': 'Domas Grigaliūnas'}, {'name': 'Mantas Lukoševičius'}]",2024-07-29T12:29:29Z
http://arxiv.org/abs/2407.19914v1,http://arxiv.org/abs/2407.19914v1,"Sentiment Analysis of Lithuanian Online Reviews Using Large Language
  Models","Sentiment analysis is a widely researched area within Natural Language
Processing (NLP), attracting significant interest due to the advent of
automated solutions. Despite this, the task remains challenging because of the
inherent complexity of languages and the subjective nature of sentiments. It is
even more challenging for less-studied and less-resourced languages such as
Lithuanian. Our review of existing Lithuanian NLP research reveals that
traditional machine learning methods and classification algorithms have limited
effectiveness for the task. In this work, we address sentiment analysis of
Lithuanian five-star-based online reviews from multiple domains that we collect
and clean. We apply transformer models to this task for the first time,
exploring the capabilities of pre-trained multilingual Large Language Models
(LLMs), specifically focusing on fine-tuning BERT and T5 models. Given the
inherent difficulty of the task, the fine-tuned models perform quite well,
especially when the sentiments themselves are less ambiguous: 80.74% and 89.61%
testing recognition accuracy of the most popular one- and five-star reviews
respectively. They significantly outperform current commercial state-of-the-art
general-purpose LLM GPT-4. We openly share our fine-tuned LLMs online.","[{'name': 'Brigita Vileikytė'}, {'name': 'Mantas Lukoševičius'}, {'name': 'Lukas Stankevičius'}]",2024-07-29T11:44:21Z
http://arxiv.org/abs/2407.19897v1,http://arxiv.org/abs/2407.19897v1,BEExAI: Benchmark to Evaluate Explainable AI,"Recent research in explainability has given rise to numerous post-hoc
attribution methods aimed at enhancing our comprehension of the outputs of
black-box machine learning models. However, evaluating the quality of
explanations lacks a cohesive approach and a consensus on the methodology for
deriving quantitative metrics that gauge the efficacy of explainability
post-hoc attribution methods. Furthermore, with the development of increasingly
complex deep learning models for diverse data applications, the need for a
reliable way of measuring the quality and correctness of explanations is
becoming critical. We address this by proposing BEExAI, a benchmark tool that
allows large-scale comparison of different post-hoc XAI methods, employing a
set of selected evaluation metrics.","[{'name': 'Samuel Sithakoul'}, {'name': 'Sara Meftah'}, {'name': 'Clément Feutry'}]",2024-07-29T11:21:17Z
http://arxiv.org/abs/2407.19884v1,http://arxiv.org/abs/2407.19884v1,Preliminary WMT24 Ranking of General MT Systems and LLMs,"This is the preliminary ranking of WMT24 General MT systems based on
automatic metrics. The official ranking will be a human evaluation, which is
superior to the automatic ranking and supersedes it. The purpose of this report
is not to interpret any findings but only provide preliminary results to the
participants of the General MT task that may be useful during the writing of
the system submission.","[{'name': 'Tom Kocmi'}, {'name': 'Eleftherios Avramidis'}, {'name': 'Rachel Bawden'}, {'name': 'Ondrej Bojar'}, {'name': 'Anton Dvorkovich'}, {'name': 'Christian Federmann'}, {'name': 'Mark Fishel'}, {'name': 'Markus Freitag'}, {'name': 'Thamme Gowda'}, {'name': 'Roman Grundkiewicz'}, {'name': 'Barry Haddow'}, {'name': 'Marzena Karpinska'}, {'name': 'Philipp Koehn'}, {'name': 'Benjamin Marie'}, {'name': 'Kenton Murray'}, {'name': 'Masaaki Nagata'}, {'name': 'Martin Popel'}, {'name': 'Maja Popovic'}, {'name': 'Mariya Shmatova'}, {'name': 'Steinþór Steingrímsson'}, {'name': 'Vilém Zouhar'}]",2024-07-29T11:01:17Z
http://arxiv.org/abs/2407.19832v2,http://arxiv.org/abs/2407.19832v2,ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2,"Multimodal Large Language Models (MLLMs) have attracted much attention for
their multifunctionality. However, traditional Transformer architectures incur
significant overhead due to their secondary computational complexity. To
address this issue, we introduce ML-Mamba, a multimodal language model, which
utilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known
for its linear scalability and fast processing of long sequences. We replace
the Transformer-based backbone with a pre-trained Mamba-2 model and explore
methods for integrating 2D visual selective scanning mechanisms into multimodal
learning while also trying various visual encoders and Mamba-2 model variants.
Our extensive experiments in various multimodal benchmark tests demonstrate the
competitive performance of ML-Mamba and highlight the potential of state space
models in multimodal tasks. The experimental results show that: (1) we
empirically explore how to effectively apply the 2D vision selective scan
mechanism for multimodal learning. We propose a novel multimodal connector
called the Mamba-2 Scan Connector (MSC), which enhances representational
capabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art
methods such as TinyLaVA and MobileVLM v2 through its linear sequential
modeling while faster inference speed; (3) Compared to multimodal models
utilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference
performance and effectiveness.","[{'name': 'Wenjun Huang'}, {'name': 'Jianguo Hu'}]",2024-07-29T09:38:15Z
http://arxiv.org/abs/2407.19825v1,http://arxiv.org/abs/2407.19825v1,Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost,"Today's large language models (LLMs) can solve challenging question-answering
tasks, and prompt engineering techniques, such as chain-of-thought (CoT), have
gained attention for enhancing the explanation and correctness of outputs.
Nevertheless, models require significant time to generate answers augmented
with lengthy reasoning details. To address this issue, this paper analyzes the
impact of output lengths on LLM inference pipelines and proposes novel metrics
to evaluate them in terms of \textit{correct conciseness}. It also examines the
impact of controlling output length through a refined prompt engineering
strategy, Constrained-CoT (CCoT), which encourages the model to limit output
length. Experiments on pre-trained LLMs demonstrated the benefit of the
proposed metrics and the effectiveness of CCoT across different models. For
instance, constraining the reasoning of LLaMA2-70b to 100 words improves the
accuracy from 36.01\% (CoT) to 41.07\% (CCoT) on the GSM8K dataset, while
reducing the average output length by 28 words.","[{'name': 'Sania Nayab'}, {'name': 'Giulio Rossolini'}, {'name': 'Giorgio Buttazzo'}, {'name': 'Nicolamaria Manes'}, {'name': 'Fabrizio Giacomelli'}]",2024-07-29T09:21:52Z
http://arxiv.org/abs/2407.19816v1,http://arxiv.org/abs/2407.19816v1,"Comparative Analysis of Encoder-Based NER and Large Language Models for
  Skill Extraction from Russian Job Vacancies","The labor market is undergoing rapid changes, with increasing demands on job
seekers and a surge in job openings. Identifying essential skills and
competencies from job descriptions is challenging due to varying employer
requirements and the omission of key skills. This study addresses these
challenges by comparing traditional Named Entity Recognition (NER) methods
based on encoders with Large Language Models (LLMs) for extracting skills from
Russian job vacancies. Using a labeled dataset of 4,000 job vacancies for
training and 1,472 for testing, the performance of both approaches is
evaluated. Results indicate that traditional NER models, especially DeepPavlov
RuBERT NER tuned, outperform LLMs across various metrics including accuracy,
precision, recall, and inference time. The findings suggest that traditional
NER models provide more effective and efficient solutions for skill extraction,
enhancing job requirement clarity and aiding job seekers in aligning their
qualifications with employer expectations. This research contributes to the
field of natural language processing (NLP) and its application in the labor
market, particularly in non-English contexts.","[{'name': 'Nikita Matkin'}, {'name': 'Aleksei Smirnov'}, {'name': 'Mikhail Usanin'}, {'name': 'Egor Ivanov'}, {'name': 'Kirill Sobyanin'}, {'name': 'Sofiia Paklina'}, {'name': 'Petr Parshakov'}]",2024-07-29T09:08:40Z
http://arxiv.org/abs/2407.21073v1,http://arxiv.org/abs/2407.21073v1,"Enhancing Adversarial Text Attacks on BERT Models with Projected
  Gradient Descent","Adversarial attacks against deep learning models represent a major threat to
the security and reliability of natural language processing (NLP) systems. In
this paper, we propose a modification to the BERT-Attack framework, integrating
Projected Gradient Descent (PGD) to enhance its effectiveness and robustness.
The original BERT-Attack, designed for generating adversarial examples against
BERT-based models, suffers from limitations such as a fixed perturbation budget
and a lack of consideration for semantic similarity. The proposed approach in
this work, PGD-BERT-Attack, addresses these limitations by leveraging PGD to
iteratively generate adversarial examples while ensuring both imperceptibility
and semantic similarity to the original input. Extensive experiments are
conducted to evaluate the performance of PGD-BERT-Attack compared to the
original BERT-Attack and other baseline methods. The results demonstrate that
PGD-BERT-Attack achieves higher success rates in causing misclassification
while maintaining low perceptual changes. Furthermore, PGD-BERT-Attack produces
adversarial instances that exhibit greater semantic resemblance to the initial
input, enhancing their applicability in real-world scenarios. Overall, the
proposed modification offers a more effective and robust approach to
adversarial attacks on BERT-based models, thus contributing to the advancement
of defense against attacks on NLP systems.","[{'name': 'Hetvi Waghela'}, {'name': 'Jaydip Sen'}, {'name': 'Sneha Rakshit'}]",2024-07-29T09:07:29Z
http://arxiv.org/abs/2407.19813v2,http://arxiv.org/abs/2407.19813v2,Improving Retrieval Augmented Language Model with Self-Reasoning,"The Retrieval-Augmented Language Model (RALM) has shown remarkable
performance on knowledge-intensive tasks by incorporating external knowledge
during inference, which mitigates the factual hallucinations inherited in large
language models (LLMs). Despite these advancements, challenges persist in the
implementation of RALMs, particularly concerning their reliability and
traceability. To be specific, the irrelevant document retrieval may result in
unhelpful response generation or even deteriorate the performance of LLMs,
while the lack of proper citations in generated outputs complicates efforts to
verify the trustworthiness of the models. To this end, we propose a novel
self-reasoning framework aimed at improving the reliability and traceability of
RALMs, whose core idea is to leverage reasoning trajectories generated by the
LLM itself. The framework involves constructing self-reason trajectories with
three processes: a relevance-aware process, an evidence-aware selective
process, and a trajectory analysis process. We have evaluated our framework
across four public datasets (two short-form QA datasets, one long-form QA
dataset, and one fact verification dataset) to demonstrate the superiority of
our method, which can outperform existing state-of-art models and can achieve
comparable performance with GPT-4, while only using 2,000 training samples.","[{'name': 'Yuan Xia'}, {'name': 'Jingbo Zhou'}, {'name': 'Zhenhui Shi'}, {'name': 'Jun Chen'}, {'name': 'Haifeng Huang'}]",2024-07-29T09:05:10Z
http://arxiv.org/abs/2407.19808v1,http://arxiv.org/abs/2407.19808v1,Segmentation en phrases : ouvrez les guillemets sans perdre le fil,"This paper presents a graph cascade for sentence segmentation of XML
documents. Our proposal offers sentences inside sentences for cases introduced
by quotation marks and hyphens, and also pays particular attention to
situations involving incises introduced by parentheses and lists introduced by
colons. We present how the tool works and compare the results obtained with
those available in 2019 on the same dataset, together with an evaluation of the
system's performance on a test corpus","[{'name': 'Sandrine Ollinger'}, {'name': 'Denis Maurel'}]",2024-07-29T09:02:38Z
http://arxiv.org/abs/2407.19807v1,http://arxiv.org/abs/2407.19807v1,Cool-Fusion: Fuse Large Language Models without Training,"We focus on the problem of fusing two or more heterogeneous large language
models (LLMs) to facilitate their complementary strengths. One of the
challenges on model fusion is high computational load, i.e. to fine-tune or to
align vocabularies via combinatorial optimization. To this end, we propose
\emph{Cool-Fusion}, a simple yet effective approach that fuses the knowledge of
heterogeneous source LLMs to leverage their complementary strengths.
\emph{Cool-Fusion} is the first method that does not require any type of
training like the ensemble approaches. But unlike ensemble methods, it is
applicable to any set of source LLMs that have different vocabularies. The
basic idea is to have each source LLM individually generate tokens until the
tokens can be decoded into a text segment that ends at word boundaries common
to all source LLMs. Then, the source LLMs jointly rerank the generated text
segment and select the best one, which is the fused text generation in one
step. Extensive experiments are conducted across a variety of benchmark
datasets. On \emph{GSM8K}, \emph{Cool-Fusion} increases accuracy from three
strong source LLMs by a significant 8\%-17.8\%.","[{'name': 'Cong Liu'}, {'name': 'Xiaojun Quan'}, {'name': 'Yan Pan'}, {'name': 'Liang Lin'}, {'name': 'Weigang Wu'}, {'name': 'Xu Chen'}]",2024-07-29T09:02:19Z
http://arxiv.org/abs/2407.19798v1,http://arxiv.org/abs/2407.19798v1,Teaching LLMs at Charles University: Assignments and Activities,"This paper presents teaching materials, particularly assignments and ideas
for classroom activities, from a new course on large language models (LLMs)
taught at Charles University. The assignments include experiments with LLM
inference for weather report generation and machine translation. The classroom
activities include class quizzes, focused research on downstream tasks and
datasets, and an interactive ""best paper"" session aimed at reading and
comprehension of research papers.","[{'name': 'Jindřich Helcl'}, {'name': 'Zdeněk Kasner'}, {'name': 'Ondřej Dušek'}, {'name': 'Tomasz Limisiewicz'}, {'name': 'Dominik Macháček'}, {'name': 'Tomáš Musil'}, {'name': 'Jindřich Libovický'}]",2024-07-29T08:43:48Z
http://arxiv.org/abs/2407.19795v1,http://arxiv.org/abs/2407.19795v1,"VolDoGer: LLM-assisted Datasets for Domain Generalization in
  Vision-Language Tasks","Domain generalizability is a crucial aspect of a deep learning model since it
determines the capability of the model to perform well on data from unseen
domains. However, research on the domain generalizability of deep learning
models for vision-language tasks remains limited, primarily because of the lack
of required datasets. To address these challenges, we propose VolDoGer:
Vision-Language Dataset for Domain Generalization, a dedicated dataset designed
for domain generalization that addresses three vision-language tasks: image
captioning, visual question answering, and visual entailment. We constructed
VolDoGer by extending LLM-based data annotation techniques to vision-language
tasks, thereby alleviating the burden of recruiting human annotators. We
evaluated the domain generalizability of various models, ranging from
fine-tuned models to a recent multimodal large language model, through
VolDoGer.","[{'name': 'Juhwan Choi'}, {'name': 'Junehyoung Kwon'}, {'name': 'JungMin Yun'}, {'name': 'Seunguk Yu'}, {'name': 'YoungBin Kim'}]",2024-07-29T08:38:46Z
http://arxiv.org/abs/2407.19794v2,http://arxiv.org/abs/2407.19794v2,Introducing a new hyper-parameter for RAG: Context Window Utilization,"This paper introduces a new hyper-parameter for Retrieval-Augmented
Generation (RAG) systems called Context Window Utilization. RAG systems enhance
generative models by incorporating relevant information retrieved from external
knowledge bases, improving the factual accuracy and contextual relevance of
generated responses. The size of the text chunks retrieved and processed is a
critical factor influencing RAG performance. This study aims to identify the
optimal chunk size that maximizes answer generation quality. Through systematic
experimentation, we analyze the effects of varying chunk sizes on the
efficiency and effectiveness of RAG frameworks. Our findings reveal that an
optimal chunk size balances the trade-off between providing sufficient context
and minimizing irrelevant information. These insights are crucial for enhancing
the design and implementation of RAG systems, underscoring the importance of
selecting an appropriate chunk size to achieve superior performance.","[{'name': 'Kush Juvekar'}, {'name': 'Anupam Purwar'}]",2024-07-29T08:38:14Z
http://arxiv.org/abs/2407.19779v1,http://arxiv.org/abs/2407.19779v1,"Synthesizing Scientific Summaries: An Extractive and Abstractive
  Approach","The availability of a vast array of research papers in any area of study,
necessitates the need of automated summarisation systems that can present the
key research conducted and their corresponding findings. Scientific paper
summarisation is a challenging task for various reasons including token length
limits in modern transformer models and corresponding memory and compute
requirements for long text. A significant amount of work has been conducted in
this area, with approaches that modify the attention mechanisms of existing
transformer models and others that utilise discourse information to capture
long range dependencies in research papers. In this paper, we propose a hybrid
methodology for research paper summarisation which incorporates an extractive
and abstractive approach. We use the extractive approach to capture the key
findings of research, and pair it with the introduction of the paper which
captures the motivation for research. We use two models based on unsupervised
learning for the extraction stage and two transformer language models,
resulting in four combinations for our hybrid approach. The performances of the
models are evaluated on three metrics and we present our findings in this
paper. We find that using certain combinations of hyper parameters, it is
possible for automated summarisation systems to exceed the abstractiveness of
summaries written by humans. Finally, we state our future scope of research in
extending this methodology to summarisation of generalised long documents.","[{'name': 'Grishma Sharma'}, {'name': 'Aditi Paretkar'}, {'name': 'Deepak Sharma'}]",2024-07-29T08:21:42Z
http://arxiv.org/abs/2407.19775v1,http://arxiv.org/abs/2407.19775v1,Model Agnostic Hybrid Sharding For Heterogeneous Distributed Inference,"The rapid growth of large-scale AI models, particularly large language models
has brought significant challenges in data privacy, computational resources,
and accessibility. Traditional centralized architectures often struggle to meet
required data security and scalability needs which hinders the democratization
of AI systems. Nesa introduces a model-agnostic sharding framework designed for
decentralized AI inference. Our framework uses blockchain-based sequential deep
neural network sharding to distribute computational tasks across a diverse
network of nodes based on a personalised heuristic and routing mechanism. This
enables efficient distributed training and inference for recent large-scale
models even on consumer-grade hardware. We use compression techniques like
dynamic blockwise quantization and mixed matrix decomposition to reduce data
transfer and memory needs. We also integrate robust security measures,
including hardware-based trusted execution environments to ensure data
integrity and confidentiality. Evaluating our system across various natural
language processing and vision tasks shows that these compression strategies do
not compromise model accuracy. Our results highlight the potential to
democratize access to cutting-edge AI technologies by enabling secure and
efficient inference on a decentralized network.","[{'name': 'Claudio Angione'}, {'name': 'Yue Zhao'}, {'name': 'Harry Yang'}, {'name': 'Ahmad Farhan'}, {'name': 'Fielding Johnston'}, {'name': 'James Buban'}, {'name': 'Patrick Colangelo'}]",2024-07-29T08:18:48Z
http://arxiv.org/abs/2407.19760v2,http://arxiv.org/abs/2407.19760v2,"Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional
  Principles in Complex Scenarios","In this paper, we conduct an empirical analysis of how large language models
(LLMs), specifically GPT-4, interpret constitutional principles in complex
decision-making scenarios. We examine rulings from the Italian Constitutional
Court on bioethics issues that involve trade-offs between competing values and
compare model-generated legal arguments on these issues to those presented by
the State, the Court, and the applicants. Our results indicate that GPT-4
consistently aligns more closely with progressive interpretations of the
Constitution, often overlooking competing values and mirroring the applicants'
views rather than the more conservative perspectives of the State or the
Court's moderate positions. Our experiments reveal a distinct tendency of GPT-4
to favor progressive legal interpretations, underscoring the influence of
underlying data biases. We thus underscore the importance of testing alignment
in real-world scenarios and considering the implications of deploying LLMs in
decision-making processes.","[{'name': 'Camilla Bignotti'}, {'name': 'Carolina Camassa'}]",2024-07-29T07:51:43Z
http://arxiv.org/abs/2407.19740v1,http://arxiv.org/abs/2407.19740v1,"KNOWCOMP POKEMON Team at DialAM-2024: A Two-Stage Pipeline for Detecting
  Relations in Dialogical Argument Mining","Dialogical Argument Mining(DialAM) is an important branch of Argument
Mining(AM). DialAM-2024 is a shared task focusing on dialogical argument
mining, which requires us to identify argumentative relations and illocutionary
relations among proposition nodes and locution nodes. To accomplish this, we
propose a two-stage pipeline, which includes the Two-Step S-Node Prediction
Model in Stage 1 and the YA-Node Prediction Model in Stage 2. We also augment
the training data in both stages and introduce context in Stage 2. We
successfully completed the task and achieved good results. Our team Pokemon
ranked 1st in the ARI Focused score and 4th in the Global Focused score.","[{'name': 'Zihao Zheng'}, {'name': 'Zhaowei Wang'}, {'name': 'Qing Zong'}, {'name': 'Yangqiu Song'}]",2024-07-29T07:07:37Z
http://arxiv.org/abs/2407.19726v3,http://arxiv.org/abs/2407.19726v3,Do Text-to-Vis Benchmarks Test Real Use of Visualisations?,"Large language models are able to generate code for visualisations in
response to user requests. This is a useful application, and an appealing one
for NLP research because plots of data provide grounding for language. However,
there are relatively few benchmarks, and it is unknown whether those that exist
are representative of what people do in practice. This paper aims to answer
that question through an empirical study comparing benchmark datasets and code
from public repositories. Our findings reveal a substantial gap in datasets,
with evaluations not testing the same distribution of chart types, attributes,
and the number of actions. The only representative dataset requires
modification to become an end-to-end and practical benchmark. This shows that
new, more benchmarks are needed to support the development of systems that
truly address users' visualisation needs. These observations will guide future
data creation, highlighting which features hold genuine significance for users.","[{'name': 'Hy Nguyen'}, {'name': 'Xuefei He'}, {'name': 'Andrew Reeson'}, {'name': 'Cecile Paris'}, {'name': 'Josiah Poon'}, {'name': 'Jonathan K. Kummerfeld'}]",2024-07-29T06:13:28Z
http://arxiv.org/abs/2407.19705v2,http://arxiv.org/abs/2407.19705v2,"CollectiveSFT: Scaling Large Language Models for Chinese Medical
  Benchmark with Collective Instructions in Healthcare","The rapid progress in Large Language Models (LLMs) has prompted the creation
of numerous benchmarks to evaluate their capabilities.This study focuses on the
Comprehensive Medical Benchmark in Chinese (CMB), showcasing how dataset
diversity and distribution in supervised fine-tuning (SFT) may enhance LLM
performance.Remarkably, We successfully trained a smaller base model to achieve
scores comparable to larger models, indicating that a diverse and
well-distributed dataset can optimize performance regardless of model size.This
study suggests that even smaller models may reach high performance levels with
carefully curated and varied datasets. By integrating a wide range of
instructional content, our approach addresses potential issues such as data
quality inconsistencies. Our results imply that a broader spectrum of training
data may enhance a model's ability to generalize and perform effectively across
different medical scenarios, highlighting the importance of dataset quality and
diversity in fine-tuning processes. We open-source the model for future
research at https://github.com/CAS-SIAT-XinHai/CollectiveSFT","[{'name': 'Jingwei Zhu'}, {'name': 'Minghuan Tan'}, {'name': 'Min Yang'}, {'name': 'Ruixue Li'}, {'name': 'Hamid Alinejad-Rokny'}]",2024-07-29T05:00:48Z
http://arxiv.org/abs/2407.19687v2,http://arxiv.org/abs/2407.19687v2,"Efficiently and Effectively: A Two-stage Approach to Balance Plaintext
  and Encrypted Text for Traffic Classification","Encrypted traffic classification is the task of identifying the application
or service associated with encrypted network traffic. One effective approach
for this task is to use deep learning methods to encode the raw traffic bytes
directly and automatically extract features for classification (byte-based
models). However, current byte-based models input raw traffic bytes, whether
plaintext or encrypted text, for automated feature extraction, neglecting the
distinct impacts of plaintext and encrypted text on downstream tasks.
Additionally, these models primarily focus on improving classification
accuracy, with little emphasis on the efficiency of models. In this paper, for
the first time, we analyze the impact of plaintext and encrypted text on the
model's effectiveness and efficiency. Based on our observations and findings,
we propose a two-phase approach to balance the trade-off between plaintext and
encrypted text in traffic classification. Specifically, Stage one is to
Determine whether the Plain text is enough to be accurately Classified (DPC)
using the proposed DPC Selector. This stage quickly identifies samples that can
be classified using plaintext, leveraging explicit byte features in plaintext
to enhance model's efficiency. Stage two aims to adaptively make a
classification with the result from stage one. This stage incorporates
encrypted text information for samples that cannot be classified using
plaintext alone, ensuring the model's effectiveness on traffic classification
tasks. Experiments on two datasets demonstrate that our proposed model achieves
state-of-the-art results in both effectiveness and efficiency.",[{'name': 'Wei Peng'}],2024-07-29T04:10:13Z
http://arxiv.org/abs/2407.21072v1,http://arxiv.org/abs/2407.21072v1,"Beyond Metrics: A Critical Analysis of the Variability in Large Language
  Model Evaluation Frameworks","As large language models (LLMs) continue to evolve, the need for robust and
standardized evaluation benchmarks becomes paramount. Evaluating the
performance of these models is a complex challenge that requires careful
consideration of various linguistic tasks, model architectures, and
benchmarking methodologies. In recent years, various frameworks have emerged as
noteworthy contributions to the field, offering comprehensive evaluation tests
and benchmarks for assessing the capabilities of LLMs across diverse domains.
This paper provides an exploration and critical analysis of some of these
evaluation methodologies, shedding light on their strengths, limitations, and
impact on advancing the state-of-the-art in natural language processing.","[{'name': 'Marco AF Pimentel'}, {'name': 'Clément Christophe'}, {'name': 'Tathagata Raha'}, {'name': 'Prateek Munjal'}, {'name': 'Praveen K Kanithi'}, {'name': 'Shadab Khan'}]",2024-07-29T03:37:14Z
http://arxiv.org/abs/2407.19672v1,http://arxiv.org/abs/2407.19672v1,"SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models
  for Southeast Asian Languages","Large Language Models (LLMs) have shown remarkable abilities across various
tasks, yet their development has predominantly centered on high-resource
languages like English and Chinese, leaving low-resource languages underserved.
To address this disparity, we present SeaLLMs 3, the latest iteration of the
SeaLLMs model family, tailored for Southeast Asian languages. This region,
characterized by its rich linguistic diversity, has lacked adequate language
technology support. SeaLLMs 3 aims to bridge this gap by covering a
comprehensive range of languages spoken in this region, including English,
Chinese, Indonesian, Vietnamese, Thai, Tagalog, Malay, Burmese, Khmer, Lao,
Tamil, and Javanese. Leveraging efficient language enhancement techniques and a
specially constructed instruction tuning dataset, SeaLLMs 3 significantly
reduces training costs while maintaining high performance and versatility. Our
model excels in tasks such as world knowledge, mathematical reasoning,
translation, and instruction following, achieving state-of-the-art performance
among similarly sized models. Additionally, we prioritized safety and
reliability by addressing both general and culture-specific considerations and
incorporated mechanisms to reduce hallucinations. This work underscores the
importance of inclusive AI, showing that advanced LLM capabilities can benefit
underserved linguistic and cultural communities.","[{'name': 'Wenxuan Zhang'}, {'name': 'Hou Pong Chan'}, {'name': 'Yiran Zhao'}, {'name': 'Mahani Aljunied'}, {'name': 'Jianyu Wang'}, {'name': 'Chaoqun Liu'}, {'name': 'Yue Deng'}, {'name': 'Zhiqiang Hu'}, {'name': 'Weiwen Xu'}, {'name': 'Yew Ken Chia'}, {'name': 'Xin Li'}, {'name': 'Lidong Bing'}]",2024-07-29T03:26:22Z
http://arxiv.org/abs/2407.19670v1,http://arxiv.org/abs/2407.19670v1,"Overview of PerpectiveArg2024: The First Shared Task on Perspective
  Argument Retrieval","Argument retrieval is the task of finding relevant arguments for a given
query. While existing approaches rely solely on the semantic alignment of
queries and arguments, this first shared task on perspective argument retrieval
incorporates perspectives during retrieval, accounting for latent influences in
argumentation. We present a novel multilingual dataset covering demographic and
socio-cultural (socio) variables, such as age, gender, and political attitude,
representing minority and majority groups in society. We distinguish between
three scenarios to explore how retrieval systems consider explicitly (in both
query and corpus) and implicitly (only in query) formulated perspectives. This
paper provides an overview of this shared task and summarizes the results of
the six submitted systems. We find substantial challenges in incorporating
perspectivism, especially when aiming for personalization based solely on the
text of arguments without explicitly providing socio profiles. Moreover,
retrieval systems tend to be biased towards the majority group but partially
mitigate bias for the female gender. While we bootstrap perspective argument
retrieval, further research is essential to optimize retrieval systems to
facilitate personalization and reduce polarization.","[{'name': 'Neele Falk'}, {'name': 'Andreas Waldis'}, {'name': 'Iryna Gurevych'}]",2024-07-29T03:14:57Z
http://arxiv.org/abs/2407.19669v1,http://arxiv.org/abs/2407.19669v1,"mGTE: Generalized Long-Context Text Representation and Reranking Models
  for Multilingual Text Retrieval","We present systematic efforts in building long-context multilingual text
representation model (TRM) and reranker from scratch for text retrieval. We
first introduce a text encoder (base size) enhanced with RoPE and unpadding,
pre-trained in a native 8192-token context (longer than 512 of previous
multilingual encoders). Then we construct a hybrid TRM and a cross-encoder
reranker by contrastive learning. Evaluations show that our text encoder
outperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM
and reranker match the performance of large-sized state-of-the-art BGE-M3
models and achieve better results on long-context retrieval benchmarks. Further
analysis demonstrate that our proposed models exhibit higher efficiency during
both training and inference. We believe their efficiency and effectiveness
could benefit various researches and industrial applications.","[{'name': 'Xin Zhang'}, {'name': 'Yanzhao Zhang'}, {'name': 'Dingkun Long'}, {'name': 'Wen Xie'}, {'name': 'Ziqi Dai'}, {'name': 'Jialong Tang'}, {'name': 'Huan Lin'}, {'name': 'Baosong Yang'}, {'name': 'Pengjun Xie'}, {'name': 'Fei Huang'}, {'name': 'Meishan Zhang'}, {'name': 'Wenjie Li'}, {'name': 'Min Zhang'}]",2024-07-29T03:12:28Z
http://arxiv.org/abs/2407.19638v1,http://arxiv.org/abs/2407.19638v1,"From Pre-training Corpora to Large Language Models: What Factors
  Influence LLM Performance in Causal Discovery Tasks?","Recent advances in artificial intelligence have seen Large Language Models
(LLMs) demonstrate notable proficiency in causal discovery tasks. This study
explores the factors influencing the performance of LLMs in causal discovery
tasks. Utilizing open-source LLMs, we examine how the frequency of causal
relations within their pre-training corpora affects their ability to accurately
respond to causal discovery queries. Our findings reveal that a higher
frequency of causal mentions correlates with better model performance,
suggesting that extensive exposure to causal information during training
enhances the models' causal discovery capabilities. Additionally, we
investigate the impact of context on the validity of causal relations. Our
results indicate that LLMs might exhibit divergent predictions for identical
causal relations when presented in different contexts. This paper provides the
first comprehensive analysis of how different factors contribute to LLM
performance in causal discovery tasks.","[{'name': 'Tao Feng'}, {'name': 'Lizhen Qu'}, {'name': 'Niket Tandon'}, {'name': 'Zhuang Li'}, {'name': 'Xiaoxi Kang'}, {'name': 'Gholamreza Haffari'}]",2024-07-29T01:45:05Z
http://arxiv.org/abs/2407.19625v1,http://arxiv.org/abs/2407.19625v1,"LoginMEA: Local-to-Global Interaction Network for Multi-modal Entity
  Alignment","Multi-modal entity alignment (MMEA) aims to identify equivalent entities
between two multi-modal knowledge graphs (MMKGs), whose entities can be
associated with relational triples and related images. Most previous studies
treat the graph structure as a special modality, and fuse different modality
information with separate uni-modal encoders, neglecting valuable relational
associations in modalities. Other studies refine each uni-modal information
with graph structures, but may introduce unnecessary relations in specific
modalities. To this end, we propose a novel local-to-global interaction network
for MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal
interactions to generate holistic entity semantics and then refine them with
global relational interactions of entity neighbors. In this design, the
uni-modal information is fused adaptively, and can be refined with relations
accordingly. To enrich local interactions of multi-modal entity information, we
device modality weights and low-rank interactive fusion, allowing diverse
impacts and element-level interactions among modalities. To capture global
interactions of graph structures, we adopt relation reflection graph attention
networks, which fully capture relational associations between entities.
Extensive experiments demonstrate superior results of our method over 5
cross-KG or bilingual benchmark datasets, indicating the effectiveness of
capturing local and global interactions.","[{'name': 'Taoyu Su'}, {'name': 'Xinghua Zhang'}, {'name': 'Jiawei Sheng'}, {'name': 'Zhenyu Zhang'}, {'name': 'Tingwen Liu'}]",2024-07-29T01:06:45Z
http://arxiv.org/abs/2407.19616v1,http://arxiv.org/abs/2407.19616v1,"TopicTag: Automatic Annotation of NMF Topic Models Using Chain of
  Thought and Prompt Tuning with LLMs","Topic modeling is a technique for organizing and extracting themes from large
collections of unstructured text. Non-negative matrix factorization (NMF) is a
common unsupervised approach that decomposes a term frequency-inverse document
frequency (TF-IDF) matrix to uncover latent topics and segment the dataset
accordingly. While useful for highlighting patterns and clustering documents,
NMF does not provide explicit topic labels, necessitating subject matter
experts (SMEs) to assign labels manually. We present a methodology for
automating topic labeling in documents clustered via NMF with automatic model
determination (NMFk). By leveraging the output of NMFk and employing prompt
engineering, we utilize large language models (LLMs) to generate accurate topic
labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs
demonstrates the effectiveness of our method in enhancing knowledge management
and document organization.","[{'name': 'Selma Wanna'}, {'name': 'Ryan Barron'}, {'name': 'Nick Solovyev'}, {'name': 'Maksim E. Eren'}, {'name': 'Manish Bhattarai'}, {'name': 'Kim Rasmussen'}, {'name': 'Boian S. Alexandrov'}]",2024-07-29T00:18:17Z
http://arxiv.org/abs/2407.19600v1,http://arxiv.org/abs/2407.19600v1,"You shall know a piece by the company it keeps. Chess plays as a data
  for word2vec models","In this paper, I apply linguistic methods of analysis to non-linguistic data,
chess plays, metaphorically equating one with the other and seeking analogies.
Chess game notations are also a kind of text, and one can consider the records
of moves or positions of pieces as words and statements in a certain language.
In this article I show how word embeddings (word2vec) can work on chess game
texts instead of natural language texts. I don't see how this representation of
chess data can be used productively. It's unlikely that these vector models
will help engines or people choose the best move. But in a purely academic
sense, it's clear that such methods of information representation capture
something important about the very nature of the game, which doesn't
necessarily lead to a win.",[{'name': 'Boris Orekhov'}],2024-07-28T22:12:36Z
http://arxiv.org/abs/2407.19594v2,http://arxiv.org/abs/2407.19594v2,"Meta-Rewarding Language Models: Self-Improving Alignment with
  LLM-as-a-Meta-Judge","Large Language Models (LLMs) are rapidly surpassing human knowledge in many
domains. While improving these models traditionally relies on costly human
data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs
can improve by judging their own responses instead of relying on human
labelers. However, existing methods have primarily focused on improving model
responses rather than judgment capabilities, resulting in rapid saturation
during iterative training. To address this issue, we introduce a novel
Meta-Rewarding step to the self-improvement process, where the model judges its
own judgements and uses that feedback to refine its judgment skills.
Surprisingly, this unsupervised approach improves the model's ability to judge
{\em and} follow instructions, as demonstrated by a win rate improvement of
Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on
Arena-Hard. These results strongly suggest the potential for self-improving
models without human supervision.","[{'name': 'Tianhao Wu'}, {'name': 'Weizhe Yuan'}, {'name': 'Olga Golovneva'}, {'name': 'Jing Xu'}, {'name': 'Yuandong Tian'}, {'name': 'Jiantao Jiao'}, {'name': 'Jason Weston'}, {'name': 'Sainbayar Sukhbaatar'}]",2024-07-28T21:58:28Z
http://arxiv.org/abs/2407.19568v1,http://arxiv.org/abs/2407.19568v1,Are LLMs Good Annotators for Discourse-level Event Relation Extraction?,"Large Language Models (LLMs) have demonstrated proficiency in a wide array of
natural language processing tasks. However, its effectiveness over
discourse-level event relation extraction (ERE) tasks remains unexplored. In
this paper, we assess the effectiveness of LLMs in addressing discourse-level
ERE tasks characterized by lengthy documents and intricate relations
encompassing coreference, temporal, causal, and subevent types. Evaluation is
conducted using an commercial model, GPT-3.5, and an open-source model,
LLaMA-2. Our study reveals a notable underperformance of LLMs compared to the
baseline established through supervised learning. Although Supervised
Fine-Tuning (SFT) can improve LLMs performance, it does not scale well compared
to the smaller supervised baseline model. Our quantitative and qualitative
analysis shows that LLMs have several weaknesses when applied for extracting
event relations, including a tendency to fabricate event mentions, and failures
to capture transitivity rules among relations, detect long distance relations,
or comprehend contexts with dense event mentions.","[{'name': 'Kangda Wei'}, {'name': 'Aayush Gautam'}, {'name': 'Ruihong Huang'}]",2024-07-28T19:27:06Z
http://arxiv.org/abs/2407.21070v2,http://arxiv.org/abs/2407.21070v2,Occam's Razor and Bender and Koller's Octopus,"We discuss the teaching of the discussion surrounding Bender and Koller's
prominent ACL 2020 paper, ""Climbing toward NLU: on meaning form, and
understanding in the age of data"" \cite{bender2020climbing}. We present what we
understand to be the main contentions of the paper, and then recommend that the
students engage with the natural counter-arguments to the claims in the paper.
We attach teaching materials that we use to facilitate teaching this topic to
undergraduate students.",[{'name': 'Michael Guerzhoy'}],2024-07-28T18:33:58Z
http://arxiv.org/abs/2407.19528v1,http://arxiv.org/abs/2407.19528v1,"Motamot: A Dataset for Revealing the Supremacy of Large Language Models
  over Transformer Models in Bengali Political Sentiment Analysis","Sentiment analysis is the process of identifying and categorizing people's
emotions or opinions regarding various topics. Analyzing political sentiment is
critical for understanding the complexities of public opinion processes,
especially during election seasons. It gives significant information on voter
preferences, attitudes, and current trends. In this study, we investigate
political sentiment analysis during Bangladeshi elections, specifically
examining how effectively Pre-trained Language Models (PLMs) and Large Language
Models (LLMs) capture complex sentiment characteristics. Our study centers on
the creation of the ""Motamot"" dataset, comprising 7,058 instances annotated
with positive and negative sentiments, sourced from diverse online newspaper
portals, forming a comprehensive resource for political sentiment analysis. We
meticulously evaluate the performance of various PLMs including BanglaBERT,
Bangla BERT Base, XLM-RoBERTa, mBERT, and sahajBERT, alongside LLMs such as
Gemini 1.5 Pro and GPT 3.5 Turbo. Moreover, we explore zero-shot and few-shot
learning strategies to enhance our understanding of political sentiment
analysis methodologies. Our findings underscore BanglaBERT's commendable
accuracy of 88.10% among PLMs. However, the exploration into LLMs reveals even
more promising results. Through the adept application of Few-Shot learning
techniques, Gemini 1.5 Pro achieves an impressive accuracy of 96.33%,
surpassing the remarkable performance of GPT 3.5 Turbo, which stands at 94%.
This underscores Gemini 1.5 Pro's status as the superior performer in this
comparison.","[{'name': 'Fatema Tuj Johora Faria'}, {'name': 'Mukaffi Bin Moin'}, {'name': 'Rabeya Islam Mumu'}, {'name': 'Md Mahabubul Alam Abir'}, {'name': 'Abrar Nawar Alfy'}, {'name': 'Mohammad Shafiul Alam'}]",2024-07-28T16:34:53Z
http://arxiv.org/abs/2407.19527v1,http://arxiv.org/abs/2407.19527v1,"Open Sentence Embeddings for Portuguese with the Serafim PT* encoders
  family","Sentence encoder encode the semantics of their input, enabling key downstream
applications such as classification, clustering, or retrieval. In this paper,
we present Serafim PT*, a family of open-source sentence encoders for
Portuguese with various sizes, suited to different hardware/compute budgets.
Each model exhibits state-of-the-art performance and is made openly available
under a permissive license, allowing its use for both commercial and research
purposes. Besides the sentence encoders, this paper contributes a systematic
study and lessons learned concerning the selection criteria of learning
objectives and parameters that support top-performing encoders.","[{'name': 'Luís Gomes'}, {'name': 'António Branco'}, {'name': 'João Silva'}, {'name': 'João Rodrigues'}, {'name': 'Rodrigo Santos'}]",2024-07-28T16:34:25Z
http://arxiv.org/abs/2407.21068v1,http://arxiv.org/abs/2407.21068v1,"Exploring Genre and Success Classification through Song Lyrics using
  DistilBERT: A Fun NLP Venture","This paper presents a natural language processing (NLP) approach to the
problem of thoroughly comprehending song lyrics, with particular attention on
genre classification, view-based success prediction, and approximate release
year. Our tests provide promising results with 65\% accuracy in genre
classification and 79\% accuracy in success prediction, leveraging a DistilBERT
model for genre classification and BERT embeddings for release year prediction.
Support Vector Machines outperformed other models in predicting the release
year, achieving the lowest root mean squared error (RMSE) of 14.18. Our study
offers insights that have the potential to revolutionize our relationship with
music by addressing the shortcomings of current approaches in properly
understanding the emotional intricacies of song lyrics.","[{'name': 'Servando Pizarro Martinez'}, {'name': 'Moritz Zimmermann'}, {'name': 'Miguel Serkan Offermann'}, {'name': 'Florian Reither'}]",2024-07-28T13:35:03Z
http://arxiv.org/abs/2407.19474v1,http://arxiv.org/abs/2407.19474v1,"Visual Riddles: a Commonsense and World Knowledge Challenge for Large
  Vision and Language Models","Imagine observing someone scratching their arm; to understand why, additional
context would be necessary. However, spotting a mosquito nearby would
immediately offer a likely explanation for the person's discomfort, thereby
alleviating the need for further information. This example illustrates how
subtle visual cues can challenge our cognitive skills and demonstrates the
complexity of interpreting visual scenarios. To study these skills, we present
Visual Riddles, a benchmark aimed to test vision and language models on visual
riddles requiring commonsense and world knowledge. The benchmark comprises 400
visual riddles, each featuring a unique image created by a variety of
text-to-image models, question, ground-truth answer, textual hint, and
attribution. Human evaluation reveals that existing models lag significantly
behind human performance, which is at 82\% accuracy, with Gemini-Pro-1.5
leading with 40\% accuracy. Our benchmark comes with automatic evaluation tasks
to make assessment scalable. These findings underscore the potential of Visual
Riddles as a valuable resource for enhancing vision and language models'
capabilities in interpreting complex visual scenarios.","[{'name': 'Nitzan Bitton-Guetta'}, {'name': 'Aviv Slobodkin'}, {'name': 'Aviya Maimon'}, {'name': 'Eliya Habba'}, {'name': 'Royi Rassin'}, {'name': 'Yonatan Bitton'}, {'name': 'Idan Szpektor'}, {'name': 'Amir Globerson'}, {'name': 'Yuval Elovici'}]",2024-07-28T11:56:03Z
http://arxiv.org/abs/2407.19435v1,http://arxiv.org/abs/2407.19435v1,"ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon
  Intention Understanding","Surgical instrument segmentation is crucial in surgical scene understanding,
thereby facilitating surgical safety. Existing algorithms directly detected all
instruments of pre-defined categories in the input image, lacking the
capability to segment specific instruments according to the surgeon's
intention. During different stages of surgery, surgeons exhibit varying
preferences and focus toward different surgical instruments. Therefore, an
instrument segmentation algorithm that adheres to the surgeon's intention can
minimize distractions from irrelevant instruments and assist surgeons to a
great extent. The recent Segment Anything Model (SAM) reveals the capability to
segment objects following prompts, but the manual annotations for prompts are
impractical during the surgery. To address these limitations in operating
rooms, we propose an audio-driven surgical instrument segmentation framework,
named ASI-Seg, to accurately segment the required surgical instruments by
parsing the audio commands of surgeons. Specifically, we propose an
intention-oriented multimodal fusion to interpret the segmentation intention
from audio commands and retrieve relevant instrument details to facilitate
segmentation. Moreover, to guide our ASI-Seg segment of the required surgical
instruments, we devise a contrastive learning prompt encoder to effectively
distinguish the required instruments from the irrelevant ones. Therefore, our
ASI-Seg promotes the workflow in the operating rooms, thereby providing
targeted support and reducing the cognitive load on surgeons. Extensive
experiments are performed to validate the ASI-Seg framework, which reveals
remarkable advantages over classical state-of-the-art and medical SAMs in both
semantic segmentation and intention-oriented segmentation. The source code is
available at https://github.com/Zonmgin-Zhang/ASI-Seg.","[{'name': 'Zhen Chen'}, {'name': 'Zongming Zhang'}, {'name': 'Wenwu Guo'}, {'name': 'Xingjian Luo'}, {'name': 'Long Bai'}, {'name': 'Jinlin Wu'}, {'name': 'Hongliang Ren'}, {'name': 'Hongbin Liu'}]",2024-07-28T09:25:59Z
http://arxiv.org/abs/2407.19409v1,http://arxiv.org/abs/2407.19409v1,LLAVADI: What Matters For Multimodal Large Language Models Distillation,"The recent surge in Multimodal Large Language Models (MLLMs) has showcased
their remarkable potential for achieving generalized intelligence by
integrating visual understanding into Large Language Models.Nevertheless, the
sheer model size of MLLMs leads to substantial memory and computational demands
that hinder their widespread deployment. In this work, we do not propose a new
efficient model structure or train small-scale MLLMs from scratch. Instead, we
focus on what matters for training small-scale MLLMs through knowledge
distillation, which is the first step from the multimodal distillation
perspective. Our extensive studies involve training strategies, model choices,
and distillation algorithms in the knowledge distillation process. These
results show that joint alignment for both tokens and logit alignment plays
critical roles in teacher-student frameworks. In addition, we draw a series of
intriguing observations from this study. By evaluating different benchmarks and
proper strategy, even a 2.7B small-scale model can perform on par with larger
models with 7B or 13B parameters. Our code and models will be publicly
available for further research.","[{'name': 'Shilin Xu'}, {'name': 'Xiangtai Li'}, {'name': 'Haobo Yuan'}, {'name': 'Lu Qi'}, {'name': 'Yunhai Tong'}, {'name': 'Ming-Hsuan Yang'}]",2024-07-28T06:10:47Z
http://arxiv.org/abs/2407.21066v1,http://arxiv.org/abs/2407.21066v1,"ELP-Adapters: Parameter Efficient Adapter Tuning for Various Speech
  Processing Tasks","Self-supervised learning has emerged as a key approach for learning generic
representations from speech data. Despite promising results in downstream tasks
such as speech recognition, speaker verification, and emotion recognition, a
significant number of parameters is required, which makes fine-tuning for each
task memory-inefficient. To address this limitation, we introduce ELP-adapter
tuning, a novel method for parameter-efficient fine-tuning using three types of
adapter, namely encoder adapters (E-adapters), layer adapters (L-adapters), and
a prompt adapter (P-adapter). The E-adapters are integrated into
transformer-based encoder layers and help to learn fine-grained speech
representations that are effective for speech recognition. The L-adapters
create paths from each encoder layer to the downstream head and help to extract
non-linguistic features from lower encoder layers that are effective for
speaker verification and emotion recognition. The P-adapter appends pseudo
features to CNN features to further improve effectiveness and efficiency. With
these adapters, models can be quickly adapted to various speech processing
tasks. Our evaluation across four downstream tasks using five backbone models
demonstrated the effectiveness of the proposed method. With the WavLM backbone,
its performance was comparable to or better than that of full fine-tuning on
all tasks while requiring 90% fewer learnable parameters.","[{'name': 'Nakamasa Inoue'}, {'name': 'Shinta Otake'}, {'name': 'Takumi Hirose'}, {'name': 'Masanari Ohi'}, {'name': 'Rei Kawakami'}]",2024-07-28T05:26:03Z
http://arxiv.org/abs/2407.19400v1,http://arxiv.org/abs/2407.19400v1,"Word Segmentation for Asian Languages: Chinese, Korean, and Japanese","We provide a detailed overview of various approaches to word segmentation of
Asian Languages, specifically Chinese, Korean, and Japanese languages. For each
language, approaches to deal with word segmentation differs. We also include
our analysis about certain advantages and disadvantages to each method. In
addition, there is room for future work in this field.","[{'name': 'Matthew Rho'}, {'name': 'Yexin Tian'}, {'name': 'Qin Chen'}]",2024-07-28T05:06:58Z
http://arxiv.org/abs/2407.19346v1,http://arxiv.org/abs/2407.19346v1,"Polynomial Regression as a Task for Understanding In-context Learning
  Through Finetuning and Alignment","Simple function classes have emerged as toy problems to better understand
in-context-learning in transformer-based architectures used for large language
models. But previously proposed simple function classes like linear regression
or multi-layer-perceptrons lack the structure required to explore things like
prompting and alignment within models capable of in-context-learning. We
propose univariate polynomial regression as a function class that is just rich
enough to study prompting and alignment, while allowing us to visualize and
understand what is going on clearly.","[{'name': 'Max Wilcoxson'}, {'name': 'Morten Svendgård'}, {'name': 'Ria Doshi'}, {'name': 'Dylan Davis'}, {'name': 'Reya Vir'}, {'name': 'Anant Sahai'}]",2024-07-27T22:00:52Z
http://arxiv.org/abs/2407.19345v1,http://arxiv.org/abs/2407.19345v1,Inference-Time Selective Debiasing,"We propose selective debiasing -- an inference-time safety mechanism that
aims to increase the overall quality of models in terms of prediction
performance and fairness in the situation when re-training a model is
prohibitive. The method is inspired by selective prediction, where some
predictions that are considered low quality are discarded at inference time. In
our approach, we identify the potentially biased model predictions and, instead
of discarding them, we debias them using LEACE -- a post-processing debiasing
method. To select problematic predictions, we propose a bias quantification
approach based on KL divergence, which achieves better results than standard UQ
methods. Experiments with text classification datasets demonstrate that
selective debiasing helps to close the performance gap between post-processing
methods and at-training and pre-processing debiasing techniques.","[{'name': 'Gleb Kuzmin'}, {'name': 'Nemeesh Yadav'}, {'name': 'Ivan Smirnov'}, {'name': 'Timothy Baldwin'}, {'name': 'Artem Shelmanov'}]",2024-07-27T21:56:23Z
http://arxiv.org/abs/2407.21065v1,http://arxiv.org/abs/2407.21065v1,LawLLM: Law Large Language Model for the US Legal System,"In the rapidly evolving field of legal analytics, finding relevant cases and
accurately predicting judicial outcomes are challenging because of the
complexity of legal language, which often includes specialized terminology,
complex syntax, and historical context. Moreover, the subtle distinctions
between similar and precedent cases require a deep understanding of legal
knowledge. Researchers often conflate these concepts, making it difficult to
develop specialized techniques to effectively address these nuanced tasks. In
this paper, we introduce the Law Large Language Model (LawLLM), a multi-task
model specifically designed for the US legal domain to address these
challenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case
Recommendation (PCR), and Legal Judgment Prediction (LJP). By clearly
distinguishing between precedent and similar cases, we provide essential
clarity, guiding future research in developing specialized strategies for these
tasks. We propose customized data preprocessing techniques for each task that
transform raw legal data into a trainable format. Furthermore, we also use
techniques such as in-context learning (ICL) and advanced information retrieval
methods in LawLLM. The evaluation results demonstrate that LawLLM consistently
outperforms existing baselines in both zero-shot and few-shot scenarios,
offering unparalleled multi-task capabilities and filling critical gaps in the
legal domain.","[{'name': 'Dong Shu'}, {'name': 'Haoran Zhao'}, {'name': 'Xukun Liu'}, {'name': 'David Demeter'}, {'name': 'Mengnan Du'}, {'name': 'Yongfeng Zhang'}]",2024-07-27T21:51:30Z
http://arxiv.org/abs/2407.19342v1,http://arxiv.org/abs/2407.19342v1,Parameter-Efficient Fine-Tuning via Circular Convolution,"Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large
foundation models, leveraging low-rank matrices $\mathbf{A}$ and $\mathbf{B}$
to represent weight changes (\textit{i.e.,} $\Delta \mathbf{W} = \mathbf{B}
\mathbf{A}$). This method reduces trainable parameters and mitigates heavy
memory consumption associated with full delta matrices by sequentially
multiplying $\mathbf{A}$ and $\mathbf{B}$ with the activation. Despite its
success, the intrinsic low-rank characteristic may limit its performance.
Although several variants have been proposed to address this issue, they often
overlook the crucial computational and memory efficiency brought by LoRA. In
this paper, we propose \underline{C}ir\underline{c}ular \underline{C}onvolution
\underline{A}daptation (C$^3$A), which not only achieves high-rank adaptation
with enhanced performance but also excels in both computational power and
memory utilization. Extensive experiments demonstrate that C$^3$A consistently
outperforms LoRA and its variants across various fine-tuning tasks.","[{'name': 'Aochuan Chen'}, {'name': 'Ziqi Gao'}, {'name': 'Zijing Liu'}, {'name': 'Yu Li'}, {'name': 'Jia Li'}]",2024-07-27T21:12:46Z
http://arxiv.org/abs/2407.19325v1,http://arxiv.org/abs/2407.19325v1,Do Language Models Have a Critical Period for Language Acquisition?,"Humans appear to have a critical period (CP) for language acquisition: Second
language (L2) acquisition becomes harder after early childhood, and ceasing
exposure to a first language (L1) after this period (but not before) typically
does not lead to substantial loss of L1 proficiency. It is unknown whether
these CP effects result from innately determined brain maturation or as a
stabilization of neural connections naturally induced by experience. In this
study, we use language models (LMs) to test the extent to which these phenomena
are peculiar to humans, or shared by a broader class of language learners. We
vary the age of exposure by training LMs on language pairs in various
experimental conditions, and find that LMs, which lack any direct analog to
innate maturational stages, do not show CP effects when trained sequentially on
L1 and L2. Our results contradict the claim that CP effects are an inevitable
result of learning in statistical learners, and they are consistent with an
innate mechanism for CP effects. We show that we can reverse-engineer the CP by
introducing a regularizer partway through training to simulate a maturational
decrease in plasticity. All in all, our results suggest that L1 learning on its
own may not be enough to induce a CP, and additional engineering is necessary
to make language models more cognitively plausible.","[{'name': 'Ionut Constantinescu'}, {'name': 'Tiago Pimentel'}, {'name': 'Ryan Cotterell'}, {'name': 'Alex Warstadt'}]",2024-07-27T19:17:10Z
http://arxiv.org/abs/2407.19302v1,http://arxiv.org/abs/2407.19302v1,"IBMEA: Exploring Variational Information Bottleneck for Multi-modal
  Entity Alignment","Multi-modal entity alignment (MMEA) aims to identify equivalent entities
between multi-modal knowledge graphs (MMKGs), where the entities can be
associated with related images. Most existing studies integrate multi-modal
information heavily relying on the automatically-learned fusion module, rarely
suppressing the redundant information for MMEA explicitly. To this end, we
explore variational information bottleneck for multi-modal entity alignment
(IBMEA), which emphasizes the alignment-relevant information and suppresses the
alignment-irrelevant information in generating entity representations.
Specifically, we devise multi-modal variational encoders to generate
modal-specific entity representations as probability distributions. Then, we
propose four modal-specific information bottleneck regularizers, limiting the
misleading clues in refining modal-specific entity representations. Finally, we
propose a modal-hybrid information contrastive regularizer to integrate all the
refined modal-specific representations, enhancing the entity similarity between
MMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and
three bilingual MMEA datasets. Experimental results demonstrate that our model
consistently outperforms previous state-of-the-art methods, and also shows
promising and robust performance in low-resource and high-noise data scenarios.","[{'name': 'Taoyu Su'}, {'name': 'Jiawei Sheng'}, {'name': 'Shicheng Wang'}, {'name': 'Xinghua Zhang'}, {'name': 'Hongbo Xu'}, {'name': 'Tingwen Liu'}]",2024-07-27T17:12:37Z
http://arxiv.org/abs/2407.19299v1,http://arxiv.org/abs/2407.19299v1,"The Impact of LoRA Adapters for LLMs on Clinical NLP Classification
  Under Data Limitations","Fine-tuning Large Language Models (LLMs) for clinical Natural Language
Processing (NLP) poses significant challenges due to the domain gap and limited
data availability. This study investigates the effectiveness of various adapter
techniques, equivalent to Low-Rank Adaptation (LoRA), for fine-tuning LLMs in a
resource-constrained hospital environment. We experimented with four
structures-Adapter, Lightweight, TinyAttention, and Gated Residual Network
(GRN)-as final layers for clinical notes classification. We fine-tuned
biomedical pre-trained models, including CamemBERT-bio, AliBERT, and DrBERT,
alongside two Transformer-based models. Our extensive experimental results
indicate that i) employing adapter structures does not yield significant
improvements in fine-tuning biomedical pre-trained LLMs, and ii) simpler
Transformer-based models, trained from scratch, perform better under resource
constraints. Among the adapter structures, GRN demonstrated superior
performance with accuracy, precision, recall, and an F1 score of 0.88.
Moreover, the total training time for LLMs exceeded 1000 hours, compared to
under 6 hours for simpler transformer-based models, highlighting that LLMs are
more suitable for environments with extensive computational resources and
larger datasets. Consequently, this study demonstrates that simpler
Transformer-based models can be effectively trained from scratch, providing a
viable solution for clinical NLP tasks in low-resource environments with
limited data availability. By identifying the GRN as the most effective adapter
structure, we offer a practical approach to enhance clinical note
classification without requiring extensive computational resources.","[{'name': 'Thanh-Dung Le'}, {'name': 'Ti Ti Nguyen'}, {'name': 'Vu Nguyen Ha'}]",2024-07-27T16:48:03Z
http://arxiv.org/abs/2407.19262v1,http://arxiv.org/abs/2407.19262v1,"Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and
  Implications","Understanding whether and to what extent large language models (LLMs) have
memorised training data has important implications for the reliability of their
output and the privacy of their training data. In order to cleanly measure and
disentangle memorisation from other phenomena (e.g. in-context learning), we
create an experimental framework that is based on repeatedly exposing LLMs to
random strings. Our framework allows us to better understand the dynamics,
i.e., the behaviour of the model, when repeatedly exposing it to random
strings. Using our framework, we make several striking observations: (a) we
find consistent phases of the dynamics across families of models (Pythia, Phi
and Llama2), (b) we identify factors that make some strings easier to memorise
than others, and (c) we identify the role of local prefixes and global context
in memorisation. We also show that sequential exposition to different random
strings has a significant effect on memorisation. Our results, often
surprising, have significant downstream implications in the study and usage of
LLMs.","[{'name': 'Till Speicher'}, {'name': 'Mohammad Aflah Khan'}, {'name': 'Qinyuan Wu'}, {'name': 'Vedant Nanda'}, {'name': 'Soumi Das'}, {'name': 'Bishwamittra Ghosh'}, {'name': 'Krishna P. Gummadi'}, {'name': 'Evimaria Terzi'}]",2024-07-27T14:00:21Z
http://arxiv.org/abs/2407.19256v1,http://arxiv.org/abs/2407.19256v1,"Stochastic Parrots or ICU Experts? Large Language Models in Critical
  Care Medicine: A Scoping Review","With the rapid development of artificial intelligence (AI), large language
models (LLMs) have shown strong capabilities in natural language understanding,
reasoning, and generation, attracting amounts of research interest in applying
LLMs to health and medicine. Critical care medicine (CCM) provides diagnosis
and treatment for critically ill patients who often require intensive
monitoring and interventions in intensive care units (ICUs). Can LLMs be
applied to CCM? Are LLMs just like stochastic parrots or ICU experts in
assisting clinical decision-making? This scoping review aims to provide a
panoramic portrait of the application of LLMs in CCM. Literature in seven
databases, including PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE
Xplore, and ACM Digital Library, were searched from January 1, 2019, to June
10, 2024. Peer-reviewed journal and conference articles that discussed the
application of LLMs in critical care settings were included. From an initial
619 articles, 24 were selected for final review. This review grouped
applications of LLMs in CCM into three categories: clinical decision support,
medical documentation and reporting, and medical education and doctor-patient
communication. LLMs have advantages in handling unstructured data and do not
require manual feature engineering. Meanwhile, applying LLMs to CCM faces
challenges, including hallucinations, poor interpretability, bias and alignment
challenges, and privacy and ethics issues. Future research should enhance model
reliability and interpretability, integrate up-to-date medical knowledge, and
strengthen privacy and ethical guidelines. As LLMs evolve, they could become
key tools in CCM to help improve patient outcomes and optimize healthcare
delivery. This study is the first review of LLMs in CCM, aiding researchers,
clinicians, and policymakers to understand the current status and future
potentials of LLMs in CCM.","[{'name': 'Tongyue Shi'}, {'name': 'Jun Ma'}, {'name': 'Zihan Yu'}, {'name': 'Haowei Xu'}, {'name': 'Minqi Xiong'}, {'name': 'Meirong Xiao'}, {'name': 'Yilin Li'}, {'name': 'Huiying Zhao'}, {'name': 'Guilan Kong'}]",2024-07-27T13:41:43Z
http://arxiv.org/abs/2407.19200v1,http://arxiv.org/abs/2407.19200v1,"On Behalf of the Stakeholders: Trends in NLP Model Interpretability in
  the Era of LLMs","Recent advancements in NLP systems, particularly with the introduction of
LLMs, have led to widespread adoption of these systems by a broad spectrum of
users across various domains, impacting decision-making, the job market,
society, and scientific research. This surge in usage has led to an explosion
in NLP model interpretability and analysis research, accompanied by numerous
technical surveys. Yet, these surveys often overlook the needs and perspectives
of explanation stakeholders. In this paper, we address three fundamental
questions: Why do we need interpretability, what are we interpreting, and how?
By exploring these questions, we examine existing interpretability paradigms,
their properties, and their relevance to different stakeholders. We further
explore the practical implications of these paradigms by analyzing trends from
the past decade across multiple research fields. To this end, we retrieved
thousands of papers and employed an LLM to characterize them. Our analysis
reveals significant disparities between NLP developers and non-developer users,
as well as between research fields, underscoring the diverse needs of
stakeholders. For example, explanations of internal model components are rarely
used outside the NLP field. We hope this paper informs the future design,
development, and application of methods that align with the objectives and
requirements of various stakeholders.","[{'name': 'Nitay Calderon'}, {'name': 'Roi Reichart'}]",2024-07-27T08:00:27Z
http://arxiv.org/abs/2407.19198v1,http://arxiv.org/abs/2407.19198v1,Towards the Dynamics of a DNN Learning Symbolic Interactions,"This study proves the two-phase dynamics of a deep neural network (DNN)
learning interactions. Despite the long disappointing view of the faithfulness
of post-hoc explanation of a DNN, in recent years, a series of theorems have
been proven to show that given an input sample, a small number of interactions
between input variables can be considered as primitive inference patterns,
which can faithfully represent every detailed inference logic of the DNN on
this sample. Particularly, it has been observed that various DNNs all learn
interactions of different complexities with two-phase dynamics, and this well
explains how a DNN's generalization power changes from under-fitting to
over-fitting. Therefore, in this study, we prove the dynamics of a DNN
gradually encoding interactions of different complexities, which provides a
theoretically grounded mechanism for the over-fitting of a DNN. Experiments
show that our theory well predicts the real learning dynamics of various DNNs
on different tasks.","[{'name': 'Qihan Ren'}, {'name': 'Yang Xu'}, {'name': 'Junpeng Zhang'}, {'name': 'Yue Xin'}, {'name': 'Dongrui Liu'}, {'name': 'Quanshi Zhang'}]",2024-07-27T07:34:49Z
http://arxiv.org/abs/2407.19196v1,http://arxiv.org/abs/2407.19196v1,"Why Misinformation is Created? Detecting them by Integrating Intent
  Features","Various social media platforms, e.g., Twitter and Reddit, allow people to
disseminate a plethora of information more efficiently and conveniently.
However, they are inevitably full of misinformation, causing damage to diverse
aspects of our daily lives. To reduce the negative impact, timely
identification of misinformation, namely Misinformation Detection (MD), has
become an active research topic receiving widespread attention. As a complex
phenomenon, the veracity of an article is influenced by various aspects. In
this paper, we are inspired by the opposition of intents between misinformation
and real information. Accordingly, we propose to reason the intent of articles
and form the corresponding intent features to promote the veracity
discrimination of article features. To achieve this, we build a hierarchy of a
set of intents for both misinformation and real information by referring to the
existing psychological theories, and we apply it to reason the intent of
articles by progressively generating binary answers with an encoder-decoder
structure. We form the corresponding intent features and integrate it with the
token features to achieve more discriminative article features for MD. Upon
these ideas, we suggest a novel MD method, namely Detecting Misinformation by
Integrating Intent featuRes (DM-INTER). To evaluate the performance of
DM-INTER, we conduct extensive experiments on benchmark MD datasets. The
experimental results validate that DM-INTER can outperform the existing
baseline MD methods.","[{'name': 'Bing Wang'}, {'name': 'Ximing Li'}, {'name': 'Changchun Li'}, {'name': 'Bo Fu'}, {'name': 'Songwen Pei'}, {'name': 'Shengsheng Wang'}]",2024-07-27T07:30:47Z
http://arxiv.org/abs/2407.19192v1,http://arxiv.org/abs/2407.19192v1,"Harmfully Manipulated Images Matter in Multimodal Misinformation
  Detection","Nowadays, misinformation is widely spreading over various social media
platforms and causes extremely negative impacts on society. To combat this
issue, automatically identifying misinformation, especially those containing
multimodal content, has attracted growing attention from the academic and
industrial communities, and induced an active research topic named Multimodal
Misinformation Detection (MMD). Typically, existing MMD methods capture the
semantic correlation and inconsistency between multiple modalities, but neglect
some potential clues in multimodal content. Recent studies suggest that
manipulated traces of the images in articles are non-trivial clues for
detecting misinformation. Meanwhile, we find that the underlying intentions
behind the manipulation, e.g., harmful and harmless, also matter in MMD.
Accordingly, in this work, we propose to detect misinformation by learning
manipulation features that indicate whether the image has been manipulated, as
well as intention features regarding the harmful and harmless intentions of the
manipulation. Unfortunately, the manipulation and intention labels that make
these features discriminative are unknown. To overcome the problem, we propose
two weakly supervised signals as alternatives by introducing additional
datasets on image manipulation detection and formulating two classification
tasks as positive and unlabeled learning problems. Based on these ideas, we
propose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD
(HAMI-M3D). Extensive experiments across three benchmark datasets can
demonstrate that HAMI-M3D can consistently improve the performance of any MMD
baselines.","[{'name': 'Bing Wang'}, {'name': 'Shengsheng Wang'}, {'name': 'Changchun Li'}, {'name': 'Renchu Guan'}, {'name': 'Ximing Li'}]",2024-07-27T07:16:07Z
http://arxiv.org/abs/2408.01460v1,http://arxiv.org/abs/2408.01460v1,"LocalValueBench: A Collaboratively Built and Extensible Benchmark for
  Evaluating Localized Value Alignment and Ethical Safety in Large Language
  Models","The proliferation of large language models (LLMs) requires robust evaluation
of their alignment with local values and ethical standards, especially as
existing benchmarks often reflect the cultural, legal, and ideological values
of their creators. \textsc{LocalValueBench}, introduced in this paper, is an
extensible benchmark designed to assess LLMs' adherence to Australian values,
and provides a framework for regulators worldwide to develop their own LLM
benchmarks for local value alignment. Employing a novel typology for ethical
reasoning and an interrogation approach, we curated comprehensive questions and
utilized prompt engineering strategies to probe LLMs' value alignment. Our
evaluation criteria quantified deviations from local values, ensuring a
rigorous assessment process. Comparative analysis of three commercial LLMs by
USA vendors revealed significant insights into their effectiveness and
limitations, demonstrating the critical importance of value alignment. This
study offers valuable tools and methodologies for regulators to create tailored
benchmarks, highlighting avenues for future research to enhance ethical AI
development.","[{'name': 'Gwenyth Isobel Meadows'}, {'name': 'Nicholas Wai Long Lau'}, {'name': 'Eva Adelina Susanto'}, {'name': 'Chi Lok Yu'}, {'name': 'Aditya Paul'}]",2024-07-27T05:55:42Z
http://arxiv.org/abs/2408.01459v1,http://arxiv.org/abs/2408.01459v1,"AgentPeerTalk: Empowering Students through Agentic-AI-Driven Discernment
  of Bullying and Joking in Peer Interactions in Schools","Addressing school bullying effectively and promptly is crucial for the mental
health of students. This study examined the potential of large language models
(LLMs) to empower students by discerning between bullying and joking in school
peer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus,
evaluating their effectiveness through human review. Our results revealed that
not all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the
most promise. We observed variations in LLM outputs, possibly influenced by
political overcorrectness, context window limitations, and pre-existing bias in
their training data. ChatGPT-4 excelled in context-specific accuracy after
implementing the agentic approach, highlighting its potential to provide
continuous, real-time support to vulnerable students. This study underlines the
significant social impact of using agentic AI in educational settings, offering
a new avenue for reducing the negative consequences of bullying and enhancing
student well-being.","[{'name': 'Aditya Paul'}, {'name': 'Chi Lok Yu'}, {'name': 'Eva Adelina Susanto'}, {'name': 'Nicholas Wai Long Lau'}, {'name': 'Gwenyth Isobel Meadows'}]",2024-07-27T05:50:02Z
http://arxiv.org/abs/2407.19173v1,http://arxiv.org/abs/2407.19173v1,"FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity
  Measurement of Persian Social Networks Informal Texts","One fundamental task for NLP is to determine the similarity between two texts
and evaluate the extent of their likeness. The previous methods for the Persian
language have low accuracy and are unable to comprehend the structure and
meaning of texts effectively. Additionally, these methods primarily focus on
formal texts, but in real-world applications of text processing, there is a
need for robust methods that can handle colloquial texts. This requires
algorithms that consider the structure and significance of words based on
context, rather than just the frequency of words. The lack of a proper dataset
for this task in the Persian language makes it important to develop such
algorithms and construct a dataset for Persian text. This paper introduces a
new transformer-based model to measure semantic similarity between Persian
informal short texts from social networks. In addition, a Persian dataset named
FarSSiM has been constructed for this purpose, using real data from social
networks and manually annotated and verified by a linguistic expert team. The
proposed model involves training a large language model using the BERT
architecture from scratch. This model, called FarSSiBERT, is pre-trained on
approximately 104 million Persian informal short texts from social networks,
making it one of a kind in the Persian language. Moreover, a novel specialized
informal language tokenizer is provided that not only performs tokenization on
formal texts well but also accurately identifies tokens that other Persian
tokenizers are unable to recognize. It has been demonstrated that our proposed
model outperforms ParsBERT, laBSE, and multilingual BERT in the Pearson and
Spearman's coefficient criteria. Additionally, the pre-trained large language
model has great potential for use in other NLP tasks on colloquial text and as
a tokenizer for less-known informal words.","[{'name': 'Seyed Mojtaba Sadjadi'}, {'name': 'Zeinab Rajabi'}, {'name': 'Leila Rabiei'}, {'name': 'Mohammad-Shahram Moin'}]",2024-07-27T05:04:49Z
http://arxiv.org/abs/2407.19164v1,http://arxiv.org/abs/2407.19164v1,"Addressing Topic Leakage in Cross-Topic Evaluation for Authorship
  Verification","Authorship verification (AV) aims to identify whether a pair of texts has the
same author. We address the challenge of evaluating AV models' robustness
against topic shifts. The conventional evaluation assumes minimal topic overlap
between training and test data. However, we argue that there can still be topic
leakage in test data, causing misleading model performance and unstable
rankings. To address this, we propose an evaluation method called
Heterogeneity-Informed Topic Sampling (HITS), which creates a smaller dataset
with a heterogeneously distributed topic set. Our experimental results
demonstrate that HITS-sampled datasets yield a more stable ranking of models
across random seeds and evaluation splits. Our contributions include: 1. An
analysis of causes and effects of topic leakage. 2. A demonstration of the HITS
in reducing the effects of topic leakage, and 3. The Robust Authorship
Verification bENchmark (RAVEN) that allows topic shortcut test to uncover AV
models' reliance on topic-specific features.","[{'name': 'Jitkapat Sawatphol'}, {'name': 'Can Udomcharoenchaikit'}, {'name': 'Sarana Nutanong'}]",2024-07-27T04:16:11Z
http://arxiv.org/abs/2407.19089v1,http://arxiv.org/abs/2407.19089v1,Many-Shot In-Context Learning for Molecular Inverse Design,"Large Language Models (LLMs) have demonstrated great performance in few-shot
In-Context Learning (ICL) for a variety of generative and discriminative
chemical design tasks. The newly expanded context windows of LLMs can further
improve ICL capabilities for molecular inverse design and lead optimization. To
take full advantage of these capabilities we developed a new semi-supervised
learning method that overcomes the lack of experimental data available for
many-shot ICL. Our approach involves iterative inclusion of LLM generated
molecules with high predicted performance, along with experimental data. We
further integrated our method in a multi-modal LLM which allows for the
interactive modification of generated molecular structures using text
instructions. As we show, the new method greatly improves upon existing ICL
methods for molecular design while being accessible and easy to use for
scientists.","[{'name': 'Saeed Moayedpour'}, {'name': 'Alejandro Corrochano-Navarro'}, {'name': 'Faryad Sahneh'}, {'name': 'Shahriar Noroozizadeh'}, {'name': 'Alexander Koetter'}, {'name': 'Jiri Vymetal'}, {'name': 'Lorenzo Kogler-Anele'}, {'name': 'Pablo Mas'}, {'name': 'Yasser Jangjou'}, {'name': 'Sizhen Li'}, {'name': 'Michael Bailey'}, {'name': 'Marc Bianciotto'}, {'name': 'Hans Matter'}, {'name': 'Christoph Grebner'}, {'name': 'Gerhard Hessler'}, {'name': 'Ziv Bar-Joseph'}, {'name': 'Sven Jager'}]",2024-07-26T21:10:50Z
http://arxiv.org/abs/2407.19056v1,http://arxiv.org/abs/2407.19056v1,"OfficeBench: Benchmarking Language Agents across Multiple Applications
  for Office Automation","Office automation significantly enhances human productivity by automatically
finishing routine tasks in the workflow. Beyond the basic information
extraction studied in much of the prior document AI literature, the office
automation research should be extended to more realistic office tasks which
require to integrate various information sources in the office system and
produce outputs through a series of decision-making processes. We introduce
OfficeBench, one of the first office automation benchmarks for evaluating
current LLM agents' capability to address office tasks in realistic office
workflows. OfficeBench requires LLM agents to perform feasible long-horizon
planning, proficiently switch between applications in a timely manner, and
accurately ground their actions within a large combined action space, based on
the contextual demands of the workflow. Applying our customized evaluation
methods on each task, we find that GPT-4 Omni achieves the highest pass rate of
47.00%, demonstrating a decent performance in handling office tasks. However,
this is still far below the human performance and accuracy standards required
by real-world office workflows. We further observe that most issues are related
to operation redundancy and hallucinations, as well as limitations in switching
between multiple applications, which may provide valuable insights for
developing effective agent frameworks for office automation.","[{'name': 'Zilong Wang'}, {'name': 'Yuedong Cui'}, {'name': 'Li Zhong'}, {'name': 'Zimin Zhang'}, {'name': 'Da Yin'}, {'name': 'Bill Yuchen Lin'}, {'name': 'Jingbo Shang'}]",2024-07-26T19:27:17Z
http://arxiv.org/abs/2407.19041v1,http://arxiv.org/abs/2407.19041v1,"Optimizing Numerical Estimation and Operational Efficiency in the Legal
  Domain through Large Language Models","The legal landscape encompasses a wide array of lawsuit types, presenting
lawyers with challenges in delivering timely and accurate information to
clients, particularly concerning critical aspects like potential imprisonment
duration or financial repercussions. Compounded by the scarcity of legal
experts, there's an urgent need to enhance the efficiency of traditional legal
workflows. Recent advances in deep learning, especially Large Language Models
(LLMs), offer promising solutions to this challenge. Leveraging LLMs'
mathematical reasoning capabilities, we propose a novel approach integrating
LLM-based methodologies with specially designed prompts to address precision
requirements in legal Artificial Intelligence (LegalAI) applications. The
proposed work seeks to bridge the gap between traditional legal practices and
modern technological advancements, paving the way for a more accessible,
efficient, and equitable legal system. To validate this method, we introduce a
curated dataset tailored to precision-oriented LegalAI tasks, serving as a
benchmark for evaluating LLM-based approaches. Extensive experimentation
confirms the efficacy of our methodology in generating accurate numerical
estimates within the legal domain, emphasizing the role of LLMs in streamlining
legal processes and meeting the evolving demands of LegalAI.","[{'name': 'Jia-Hong Huang'}, {'name': 'Chao-Chun Yang'}, {'name': 'Yixian Shen'}, {'name': 'Alessio M. Pacces'}, {'name': 'Evangelos Kanoulas'}]",2024-07-26T18:46:39Z
http://arxiv.org/abs/2407.18908v1,http://arxiv.org/abs/2407.18908v1,Wolf: Captioning Everything with a World Summarization Framework,"We propose Wolf, a WOrLd summarization Framework for accurate video
captioning. Wolf is an automated captioning framework that adopts a
mixture-of-experts approach, leveraging complementary strengths of Vision
Language Models (VLMs). By utilizing both image and video models, our framework
captures different levels of information and summarizes them efficiently. Our
approach can be applied to enhance video understanding, auto-labeling, and
captioning. To evaluate caption quality, we introduce CapScore, an LLM-based
metric to assess the similarity and quality of generated captions compared to
the ground truth captions. We further build four human-annotated datasets in
three domains: autonomous driving, general scenes, and robotics, to facilitate
comprehensive comparisons. We show that Wolf achieves superior captioning
performance compared to state-of-the-art approaches from the research community
(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For
instance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise
by 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,
we establish a benchmark for video captioning and introduce a leaderboard,
aiming to accelerate advancements in video understanding, captioning, and data
alignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.","[{'name': 'Boyi Li'}, {'name': 'Ligeng Zhu'}, {'name': 'Ran Tian'}, {'name': 'Shuhan Tan'}, {'name': 'Yuxiao Chen'}, {'name': 'Yao Lu'}, {'name': 'Yin Cui'}, {'name': 'Sushant Veer'}, {'name': 'Max Ehrlich'}, {'name': 'Jonah Philion'}, {'name': 'Xinshuo Weng'}, {'name': 'Fuzhao Xue'}, {'name': 'Andrew Tao'}, {'name': 'Ming-Yu Liu'}, {'name': 'Sanja Fidler'}, {'name': 'Boris Ivanovic'}, {'name': 'Trevor Darrell'}, {'name': 'Jitendra Malik'}, {'name': 'Song Han'}, {'name': 'Marco Pavone'}]",2024-07-26T17:59:09Z
http://arxiv.org/abs/2407.18901v1,http://arxiv.org/abs/2407.18901v1,"AppWorld: A Controllable World of Apps and People for Benchmarking
  Interactive Coding Agents","Autonomous agents that address day-to-day digital tasks (e.g., ordering
groceries for a household), must not only operate multiple apps (e.g., notes,
messaging, shopping app) via APIs, but also generate rich code with complex
control flow in an iterative manner based on their interaction with the
environment. However, existing benchmarks for tool use are inadequate, as they
only cover tasks that require a simple sequence of API calls.
  To remedy this gap, we built $\textbf{AppWorld Engine}$, a high-quality
execution environment (60K lines of code) of 9 day-to-day apps operable via 457
APIs and populated with realistic digital activities simulating the lives of
~100 fictitious users. We then created $\textbf{AppWorld Benchmark}$ (40K lines
of code), a suite of 750 natural, diverse, and challenging autonomous agent
tasks requiring rich and interactive code generation. It supports robust
programmatic evaluation with state-based unit tests, allowing for different
ways of completing a task while also checking for unexpected changes, i.e.,
collateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our
'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least
16% fewer. This highlights the benchmark's difficulty and AppWorld's potential
to push the frontiers of interactive coding agents. The project website is
available at https://appworld.dev/.","[{'name': 'Harsh Trivedi'}, {'name': 'Tushar Khot'}, {'name': 'Mareike Hartmann'}, {'name': 'Ruskin Manku'}, {'name': 'Vinty Dong'}, {'name': 'Edward Li'}, {'name': 'Shashank Gupta'}, {'name': 'Ashish Sabharwal'}, {'name': 'Niranjan Balasubramanian'}]",2024-07-26T17:55:45Z
http://arxiv.org/abs/2407.18887v1,http://arxiv.org/abs/2407.18887v1,Embedding And Clustering Your Data Can Improve Contrastive Pretraining,"Recent studies of large-scale contrastive pretraining in the text embedding
domain show that using single-source minibatches, rather than mixed-source
minibatches, can substantially improve overall model accuracy. In this work, we
explore extending training data stratification beyond source granularity by
leveraging a pretrained text embedding model and the classic k-means clustering
algorithm to further split training data apart by the semantic clusters within
each source. Experimentally, we observe a notable increase in NDCG@10 when
pretraining a BERT-based text embedding model on query-passage pairs from the
MSMARCO passage retrieval dataset. Additionally, we conceptually connect our
clustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B
methodology and the nearest-neighbor-based hard-negative mining aspect of the
ANCE methodology and discuss how this unified view motivates future lines of
research on the organization of contrastive pretraining data.",[{'name': 'Luke Merrick'}],2024-07-26T17:36:40Z
http://arxiv.org/abs/2407.18789v1,http://arxiv.org/abs/2407.18789v1,"Granularity is crucial when applying differential privacy to text: An
  investigation for neural machine translation","Applying differential privacy (DP) by means of the DP-SGD algorithm to
protect individual data points during training is becoming increasingly popular
in NLP. However, the choice of granularity at which DP is applied is often
neglected. For example, neural machine translation (NMT) typically operates on
the sentence-level granularity. From the perspective of DP, this setup assumes
that each sentence belongs to a single person and any two sentences in the
training dataset are independent. This assumption is however violated in many
real-world NMT datasets, e.g. those including dialogues. For proper application
of DP we thus must shift from sentences to entire documents. In this paper, we
investigate NMT at both the sentence and document levels, analyzing the
privacy/utility trade-off for both scenarios, and evaluating the risks of not
using the appropriate privacy granularity in terms of leaking personally
identifiable information (PII). Our findings indicate that the document-level
NMT system is more resistant to membership inference attacks, emphasizing the
significance of using the appropriate granularity when working with DP.","[{'name': 'Doan Nam Long Vu'}, {'name': 'Timour Igamberdiev'}, {'name': 'Ivan Habernal'}]",2024-07-26T14:52:37Z
http://arxiv.org/abs/2407.18786v1,http://arxiv.org/abs/2407.18786v1,"The power of Prompts: Evaluating and Mitigating Gender Bias in MT with
  LLMs","This paper studies gender bias in machine translation through the lens of
Large Language Models (LLMs). Four widely-used test sets are employed to
benchmark various base LLMs, comparing their translation quality and gender
bias against state-of-the-art Neural Machine Translation (NMT) models for
English to Catalan (En $\rightarrow$ Ca) and English to Spanish (En
$\rightarrow$ Es) translation directions. Our findings reveal pervasive gender
bias across all models, with base LLMs exhibiting a higher degree of bias
compared to NMT models. To combat this bias, we explore prompting engineering
techniques applied to an instruction-tuned LLM. We identify a prompt structure
that significantly reduces gender bias by up to 12% on the WinoMT evaluation
dataset compared to more straightforward prompts. These results significantly
reduce the gender bias accuracy gap between LLMs and traditional NMT systems.","[{'name': 'Aleix Sant'}, {'name': 'Carlos Escolano'}, {'name': 'Audrey Mash'}, {'name': 'Francesca De Luca Fornaciari'}, {'name': 'Maite Melero'}]",2024-07-26T14:47:31Z
http://arxiv.org/abs/2407.18752v3,http://arxiv.org/abs/2407.18752v3,"Knowledge Graph Structure as Prompt: Improving Small Language Models
  Capabilities for Knowledge-based Causal Discovery","Causal discovery aims to estimate causal structures among variables based on
observational data. Large Language Models (LLMs) offer a fresh perspective to
tackle the causal discovery problem by reasoning on the metadata associated
with variables rather than their actual data values, an approach referred to as
knowledge-based causal discovery. In this paper, we investigate the
capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1
billion parameters) with prompt-based learning for knowledge-based causal
discovery. Specifically, we present KG Structure as Prompt, a novel approach
for integrating structural information from a knowledge graph, such as common
neighbor nodes and metapaths, into prompt-based learning to enhance the
capabilities of SLMs. Experimental results on three types of biomedical and
open-domain datasets under few-shot settings demonstrate the effectiveness of
our approach, surpassing most baselines and even conventional fine-tuning
approaches trained on full datasets. Our findings further highlight the strong
capabilities of SLMs: in combination with knowledge graphs and prompt-based
learning, SLMs demonstrate the potential to surpass LLMs with larger number of
parameters. Our code and datasets are available on GitHub.","[{'name': 'Yuni Susanti'}, {'name': 'Michael Färber'}]",2024-07-26T14:07:00Z
http://arxiv.org/abs/2407.18743v1,http://arxiv.org/abs/2407.18743v1,"Towards Effective and Efficient Continual Pre-training of Large Language
  Models","Continual pre-training (CPT) has been an important approach for adapting
language models to specific domains or tasks. To make the CPT approach more
traceable, this paper presents a technical report for continually pre-training
Llama-3 (8B), which significantly enhances the Chinese language ability and
scientific reasoning ability of the backbone model. To enhance the new
abilities while retaining the original abilities, we design specific data
mixture and curriculum strategies by utilizing existing datasets and
synthesizing high-quality datasets. Specifically, we synthesize
multidisciplinary scientific question and answer (QA) pairs based on related
web pages, and subsequently incorporate these synthetic data to improve the
scientific reasoning ability of Llama-3. We refer to the model after CPT as
Llama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning
experiments with a relatively small model -- TinyLlama, and employ the derived
findings to train the backbone model. Extensive experiments on a number of
evaluation benchmarks show that our approach can largely improve the
performance of the backbone models, including both the general abilities (+8.81
on C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on
MATH and +4.13 on SciEval), without hurting the original capacities. Our model,
data, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.","[{'name': 'Jie Chen'}, {'name': 'Zhipeng Chen'}, {'name': 'Jiapeng Wang'}, {'name': 'Kun Zhou'}, {'name': 'Yutao Zhu'}, {'name': 'Jinhao Jiang'}, {'name': 'Yingqian Min'}, {'name': 'Wayne Xin Zhao'}, {'name': 'Zhicheng Dou'}, {'name': 'Jiaxin Mao'}, {'name': 'Yankai Lin'}, {'name': 'Ruihua Song'}, {'name': 'Jun Xu'}, {'name': 'Xu Chen'}, {'name': 'Rui Yan'}, {'name': 'Zhewei Wei'}, {'name': 'Di Hu'}, {'name': 'Wenbing Huang'}, {'name': 'Ji-Rong Wen'}]",2024-07-26T13:55:21Z
http://arxiv.org/abs/2407.18738v1,http://arxiv.org/abs/2407.18738v1,Towards Generalized Offensive Language Identification,"The prevalence of offensive content on the internet, encompassing hate speech
and cyberbullying, is a pervasive issue worldwide. Consequently, it has
garnered significant attention from the machine learning (ML) and natural
language processing (NLP) communities. As a result, numerous systems have been
developed to automatically identify potentially harmful content and mitigate
its impact. These systems can follow two approaches; (1) Use publicly available
models and application endpoints, including prompting large language models
(LLMs) (2) Annotate datasets and train ML models on them. However, both
approaches lack an understanding of how generalizable they are. Furthermore,
the applicability of these systems is often questioned in off-domain and
practical environments. This paper empirically evaluates the generalizability
of offensive language detection models and datasets across a novel generalized
benchmark. We answer three research questions on generalizability. Our findings
will be useful in creating robust real-world offensive language detection
systems.","[{'name': 'Alphaeus Dmonte'}, {'name': 'Tejas Arya'}, {'name': 'Tharindu Ranasinghe'}, {'name': 'Marcos Zampieri'}]",2024-07-26T13:50:22Z
http://arxiv.org/abs/2407.18730v1,http://arxiv.org/abs/2407.18730v1,"Creating an Aligned Corpus of Sound and Text: The Multimodal Corpus of
  Shakespeare and Milton","In this work we present a corpus of poems by William Shakespeare and John
Milton that have been enriched with readings from the public domain. We have
aligned all the lines with their respective audio segments, at the line, word,
syllable and phone level, and we have included their scansion. We make a basic
visualization platform for these poems and we conclude by conjecturing possible
future directions.",[{'name': 'Manex Agirrezabal'}],2024-07-26T13:30:24Z
http://arxiv.org/abs/2407.18716v1,http://arxiv.org/abs/2407.18716v1,"ChatSchema: A pipeline of extracting structured information with Large
  Multimodal Models based on schema","Objective: This study introduces ChatSchema, an effective method for
extracting and structuring information from unstructured data in medical paper
reports using a combination of Large Multimodal Models (LMMs) and Optical
Character Recognition (OCR) based on the schema. By integrating predefined
schema, we intend to enable LMMs to directly extract and standardize
information according to the schema specifications, facilitating further data
entry. Method: Our approach involves a two-stage process, including
classification and extraction for categorizing report scenarios and structuring
information. We established and annotated a dataset to verify the effectiveness
of ChatSchema, and evaluated key extraction using precision, recall, F1-score,
and accuracy metrics. Based on key extraction, we further assessed value
extraction. We conducted ablation studies on two LMMs to illustrate the
improvement of structured information extraction with different input modals
and methods. Result: We analyzed 100 medical reports from Peking University
First Hospital and established a ground truth dataset with 2,945 key-value
pairs. We evaluated ChatSchema using GPT-4o and Gemini 1.5 Pro and found a
higher overall performance of GPT-4o. The results are as follows: For the
result of key extraction, key-precision was 98.6%, key-recall was 98.5%,
key-F1-score was 98.6%. For the result of value extraction based on correct key
extraction, the overall accuracy was 97.2%, precision was 95.8%, recall was
95.8%, and F1-score was 95.8%. An ablation study demonstrated that ChatSchema
achieved significantly higher overall accuracy and overall F1-score of
key-value extraction, compared to the Baseline, with increases of 26.9% overall
accuracy and 27.4% overall F1-score, respectively.","[{'name': 'Fei Wang'}, {'name': 'Yuewen Zheng'}, {'name': 'Qin Li'}, {'name': 'Jingyi Wu'}, {'name': 'Pengfei Li'}, {'name': 'Luxia Zhang'}]",2024-07-26T13:05:24Z
http://arxiv.org/abs/2407.18712v1,http://arxiv.org/abs/2407.18712v1,Cluster-norm for Unsupervised Probing of Knowledge,"The deployment of language models brings challenges in generating reliable
information, especially when these models are fine-tuned using human
preferences. To extract encoded knowledge without (potentially) biased human
labels, unsupervised probing techniques like Contrast-Consistent Search (CCS)
have been developed (Burns et al., 2022). However, salient but unrelated
features in a given dataset can mislead these probes (Farquhar et al., 2023).
Addressing this, we propose a cluster normalization method to minimize the
impact of such features by clustering and normalizing activations of contrast
pairs before applying unsupervised probing techniques. While this approach does
not address the issue of differentiating between knowledge in general and
simulated knowledge - a major issue in the literature of latent knowledge
elicitation (Christiano et al., 2021) - it significantly improves the ability
of unsupervised probes to identify the intended knowledge amidst distractions.","[{'name': 'Walter Laurito'}, {'name': 'Sharan Maiya'}, {'name': 'Grégoire Dhimoïla'}, {'name': 'Owen'}, {'name': 'Yeung'}, {'name': 'Kaarel Hänni'}]",2024-07-26T12:57:54Z
http://arxiv.org/abs/2407.18698v1,http://arxiv.org/abs/2407.18698v1,"Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended
  Text Generation","Decoding from the output distributions of large language models to produce
high-quality text is a complex challenge in language modeling. Various
approaches, such as beam search, sampling with temperature, $k-$sampling,
nucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive
search, have been proposed to address this problem, aiming to improve
coherence, diversity, as well as resemblance to human-generated text. In this
study, we introduce adaptive contrastive search, a novel decoding strategy
extending contrastive search by incorporating an adaptive degeneration penalty,
guided by the estimated uncertainty of the model at each generation step. This
strategy is designed to enhance both the creativity and diversity of the
language modeling process while at the same time producing coherent and
high-quality generated text output. Our findings indicate performance
enhancement in both aspects, across different model architectures and datasets,
underscoring the effectiveness of our method in text generation tasks. Our code
base, datasets, and models are publicly available.","[{'name': 'Esteban Garces Arias'}, {'name': 'Julian Rodemann'}, {'name': 'Meimingwei Li'}, {'name': 'Christian Heumann'}, {'name': 'Matthias Aßenmacher'}]",2024-07-26T12:23:54Z
http://arxiv.org/abs/2407.18689v1,http://arxiv.org/abs/2407.18689v1,"The BIAS Detection Framework: Bias Detection in Word Embeddings and
  Language Models for European Languages","The project BIAS: Mitigating Diversity Biases of AI in the Labor Market is a
four-year project funded by the European commission and supported by the Swiss
State Secretariat for Education, Research and Innovation (SERI). As part of the
project, novel bias detection methods to identify societal bias in language
models and word embeddings in European languages are developed, with particular
attention to linguistic and geographic particularities. This technical report
describes the overall architecture and components of the BIAS Detection
Framework. The code described in this technical report is available and will be
updated and expanded continuously with upcoming results from the BIAS project.
The details about the datasets for the different languages are described in
corresponding papers at scientific venues.","[{'name': 'Alexandre Puttick'}, {'name': 'Leander Rankwiler'}, {'name': 'Catherine Ikae'}, {'name': 'Mascha Kurpicz-Briki'}]",2024-07-26T12:13:45Z
http://arxiv.org/abs/2407.21061v1,http://arxiv.org/abs/2407.21061v1,"Improving noisy student training for low-resource languages in
  End-to-End ASR using CycleGAN and inter-domain losses","Training a semi-supervised end-to-end speech recognition system using noisy
student training has significantly improved performance. However, this approach
requires a substantial amount of paired speech-text and unlabeled speech, which
is costly for low-resource languages. Therefore, this paper considers a more
extreme case of semi-supervised end-to-end automatic speech recognition where
there are limited paired speech-text, unlabeled speech (less than five hours),
and abundant external text. Firstly, we observe improved performance by
training the model using our previous work on semi-supervised learning
""CycleGAN and inter-domain losses"" solely with external text. Secondly, we
enhance ""CycleGAN and inter-domain losses"" by incorporating automatic
hyperparameter tuning, calling it ""enhanced CycleGAN inter-domain losses.""
Thirdly, we integrate it into the noisy student training approach pipeline for
low-resource scenarios. Our experimental results, conducted on six non-English
languages from Voxforge and Common Voice, show a 20% word error rate reduction
compared to the baseline teacher model and a 10% word error rate reduction
compared to the baseline best student model, highlighting the significant
improvements achieved through our proposed method.","[{'name': 'Chia-Yu Li'}, {'name': 'Ngoc Thang Vu'}]",2024-07-26T10:57:06Z
http://arxiv.org/abs/2407.18626v1,http://arxiv.org/abs/2407.18626v1,"Every Part Matters: Integrity Verification of Scientific Figures Based
  on Multimodal Large Language Models","This paper tackles a key issue in the interpretation of scientific figures:
the fine-grained alignment of text and figures. It advances beyond prior
research that primarily dealt with straightforward, data-driven visualizations
such as bar and pie charts and only offered a basic understanding of diagrams
through captioning and classification. We introduce a novel task, Figure
Integrity Verification, designed to evaluate the precision of technologies in
aligning textual knowledge with visual elements in scientific figures. To
support this, we develop a semi-automated method for constructing a large-scale
dataset, Figure-seg, specifically designed for this task. Additionally, we
propose an innovative framework, Every Part Matters (EPM), which leverages
Multimodal Large Language Models (MLLMs) to not only incrementally improve the
alignment and verification of text-figure integrity but also enhance integrity
through analogical reasoning. Our comprehensive experiments show that these
innovations substantially improve upon existing methods, allowing for more
precise and thorough analysis of complex scientific figures. This progress not
only enhances our understanding of multimodal technologies but also stimulates
further research and practical applications across fields requiring the
accurate interpretation of complex visual data.","[{'name': 'Xiang Shi'}, {'name': 'Jiawei Liu'}, {'name': 'Yinpeng Liu'}, {'name': 'Qikai Cheng'}, {'name': 'Wei Lu'}]",2024-07-26T09:35:36Z
http://arxiv.org/abs/2408.06361v1,http://arxiv.org/abs/2408.06361v1,Large Language Model Agent in Financial Trading: A Survey,"Trading is a highly competitive task that requires a combination of strategy,
knowledge, and psychological fortitude. With the recent success of large
language models(LLMs), it is appealing to apply the emerging intelligence of
LLM agents in this competitive arena and understanding if they can outperform
professional traders. In this survey, we provide a comprehensive review of the
current research on using LLMs as agents in financial trading. We summarize the
common architecture used in the agent, the data inputs, and the performance of
LLM trading agents in backtesting as well as the challenges presented in these
research. This survey aims to provide insights into the current state of
LLM-based financial trading agents and outline future research directions in
this field.","[{'name': 'Han Ding'}, {'name': 'Yinheng Li'}, {'name': 'Junhao Wang'}, {'name': 'Hang Chen'}]",2024-07-26T08:53:05Z
http://arxiv.org/abs/2407.21060v1,http://arxiv.org/abs/2407.21060v1,"Using Large Language Models for the Interpretation of Building
  Regulations","Compliance checking is an essential part of a construction project. The
recent rapid uptake of building information models (BIM) in the construction
industry has created more opportunities for automated compliance checking
(ACC). BIM enables sharing of digital building design data that can be used for
compliance checking with legal requirements, which are conventionally conveyed
in natural language and not intended for machine processing. Creating a
computable representation of legal requirements suitable for ACC is complex,
costly, and time-consuming. Large language models (LLMs) such as the generative
pre-trained transformers (GPT), GPT-3.5 and GPT-4, powering OpenAI's ChatGPT,
can generate logically coherent text and source code responding to user
prompts. This capability could be used to automate the conversion of building
regulations into a semantic and computable representation. This paper evaluates
the performance of LLMs in translating building regulations into LegalRuleML in
a few-shot learning setup. By providing GPT-3.5 with only a few example
translations, it can learn the basic structure of the format. Using a system
prompt, we further specify the LegalRuleML representation and explore the
existence of expert domain knowledge in the model. Such domain knowledge might
be ingrained in GPT-3.5 through the broad pre-training but needs to be brought
forth by careful contextualisation. Finally, we investigate whether strategies
such as chain-of-thought reasoning and self-consistency could apply to this use
case. As LLMs become more sophisticated, the increased common sense, logical
coherence, and means to domain adaptation can significantly support ACC,
leading to more efficient and effective checking processes.","[{'name': 'Stefan Fuchs'}, {'name': 'Michael Witbrock'}, {'name': 'Johannes Dimyadi'}, {'name': 'Robert Amor'}]",2024-07-26T08:30:47Z
http://arxiv.org/abs/2407.18581v2,http://arxiv.org/abs/2407.18581v2,"Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech
  Recognition with Hierarchical Routing","The Mixture of Experts (MoE) approach is well-suited for multilingual and
code-switching (CS) tasks due to its multi-expert architecture. This work
introduces the DLG-MoE, a Dynamic Language Group-based MoE optimized for
bilingual and CS scenarios. DLG-MoE operates based on a hierarchical routing
mechanism. First, the language router explicitly models the language and
dispatches the representations to the corresponding language expert groups.
Subsequently, the unsupervised router within each language group implicitly
models attributes beyond language, and coordinates expert routing and
collaboration. The model achieves state-of-the-art (SOTA) performance while
also having unparalleled flexibility. It supports different top-k inference and
streaming capabilities, and can also prune the model parameters to obtain a
monolingual sub-model. The Code will be released.","[{'name': 'Hukai Huang'}, {'name': 'Shenghui Lu'}, {'name': 'Yahui Shan'}, {'name': 'He Qu'}, {'name': 'Wenhao Guan'}, {'name': 'Qingyang Hong'}, {'name': 'Lin Li'}]",2024-07-26T08:03:07Z
http://arxiv.org/abs/2407.18562v1,http://arxiv.org/abs/2407.18562v1,"Learning Robust Named Entity Recognizers From Noisy Data With Retrieval
  Augmentation","Named entity recognition (NER) models often struggle with noisy inputs, such
as those with spelling mistakes or errors generated by Optical Character
Recognition processes, and learning a robust NER model is challenging. Existing
robust NER models utilize both noisy text and its corresponding gold text for
training, which is infeasible in many real-world applications in which gold
text is not available. In this paper, we consider a more realistic setting in
which only noisy text and its NER labels are available. We propose to retrieve
relevant text of the noisy text from a knowledge corpus and use it to enhance
the representation of the original noisy input. We design three retrieval
methods: sparse retrieval based on lexicon similarity, dense retrieval based on
semantic similarity, and self-retrieval based on task-specific text. After
retrieving relevant text, we concatenate the retrieved text with the original
noisy text and encode them with a transformer network, utilizing self-attention
to enhance the contextual token representations of the noisy text using the
retrieved text. We further employ a multi-view training framework that improves
robust NER without retrieving text during inference. Experiments show that our
retrieval-augmented model achieves significant improvements in various noisy
NER settings.","[{'name': 'Chaoyi Ai'}, {'name': 'Yong Jiang'}, {'name': 'Shen Huang'}, {'name': 'Pengjun Xie'}, {'name': 'Kewei Tu'}]",2024-07-26T07:30:41Z
http://arxiv.org/abs/2407.18552v2,http://arxiv.org/abs/2407.18552v2,"Multimodal Emotion Recognition using Audio-Video Transformer Fusion with
  Cross Attention","Understanding emotions is a fundamental aspect of human communication.
Integrating audio and video signals offers a more comprehensive understanding
of emotional states compared to traditional methods that rely on a single data
source, such as speech or facial expressions. Despite its potential, multimodal
emotion recognition faces significant challenges, particularly in
synchronization, feature extraction, and fusion of diverse data sources. To
address these issues, this paper introduces a novel transformer-based model
named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA
model employs a transformer fusion approach to effectively capture and
synchronize interlinked features from both audio and video inputs, thereby
resolving synchronization problems. Additionally, the Cross Attention mechanism
within AVT-CA selectively extracts and emphasizes critical features while
discarding irrelevant ones from both modalities, addressing feature extraction
and fusion challenges. Extensive experimental analysis conducted on the
CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the
proposed model. The results underscore the importance of AVT-CA in developing
precise and reliable multimodal emotion recognition systems for practical
applications.","[{'name': 'Joe Dhanith P R'}, {'name': 'Shravan Venkatraman'}, {'name': 'Modigari Narendra'}, {'name': 'Vigya Sharma'}, {'name': 'Santhosh Malarvannan'}, {'name': 'Amir H. Gandomi'}]",2024-07-26T07:05:04Z
http://arxiv.org/abs/2407.18540v1,http://arxiv.org/abs/2407.18540v1,"A Universal Prompting Strategy for Extracting Process Model Information
  from Natural Language Text using Large Language Models","Over the past decade, extensive research efforts have been dedicated to the
extraction of information from textual process descriptions. Despite the
remarkable progress witnessed in natural language processing (NLP), information
extraction within the Business Process Management domain remains predominantly
reliant on rule-based systems and machine learning methodologies. Data scarcity
has so far prevented the successful application of deep learning techniques.
However, the rapid progress in generative large language models (LLMs) makes it
possible to solve many NLP tasks with very high quality without the need for
extensive data. Therefore, we systematically investigate the potential of LLMs
for extracting information from textual process descriptions, targeting the
detection of process elements such as activities and actors, and relations
between them. Using a heuristic algorithm, we demonstrate the suitability of
the extracted information for process model generation. Based on a novel
prompting strategy, we show that LLMs are able to outperform state-of-the-art
machine learning approaches with absolute performance improvements of up to 8\%
$F_1$ score across three different datasets. We evaluate our prompting strategy
on eight different LLMs, showing it is universally applicable, while also
analyzing the impact of certain prompt parts on extraction quality. The number
of example texts, the specificity of definitions, and the rigour of format
instructions are identified as key for improving the accuracy of extracted
information. Our code, prompts, and data are publicly available.","[{'name': 'Julian Neuberger'}, {'name': 'Lars Ackermann'}, {'name': 'Han van der Aa'}, {'name': 'Stefan Jablonski'}]",2024-07-26T06:39:35Z
http://arxiv.org/abs/2407.18538v1,http://arxiv.org/abs/2407.18538v1,"Towards a Multidimensional Evaluation Framework for Empathetic
  Conversational Systems","Empathetic Conversational Systems (ECS) are built to respond empathetically
to the user's emotions and sentiments, regardless of the application domain.
Current ECS studies evaluation approaches are restricted to offline evaluation
experiments primarily for gold standard comparison & benchmarking, and user
evaluation studies for collecting human ratings on specific constructs. These
methods are inadequate in measuring the actual quality of empathy in
conversations. In this paper, we propose a multidimensional empathy evaluation
framework with three new methods for measuring empathy at (i) structural level
using three empathy-related dimensions, (ii) behavioral level using empathy
behavioral types, and (iii) overall level using an empathy lexicon, thereby
fortifying the evaluation process. Experiments were conducted with the
state-of-the-art ECS models and large language models (LLMs) to show the
framework's usefulness.","[{'name': 'Aravind Sesagiri Raamkumar'}, {'name': 'Siyuan Brandon Loh'}]",2024-07-26T06:34:55Z
http://arxiv.org/abs/2407.18525v1,http://arxiv.org/abs/2407.18525v1,"Is larger always better? Evaluating and prompting large language models
  for non-generative medical tasks","The use of Large Language Models (LLMs) in medicine is growing, but their
ability to handle both structured Electronic Health Record (EHR) data and
unstructured clinical notes is not well-studied. This study benchmarks various
models, including GPT-based LLMs, BERT-based models, and traditional clinical
predictive models, for non-generative medical tasks utilizing renowned
datasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7
traditional predictive models using the MIMIC dataset (ICU patient records) and
the TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality
and readmission prediction, disease hierarchy reconstruction, and biomedical
sentence matching, comparing both zero-shot and finetuned performance. Results
indicated that LLMs exhibited robust zero-shot predictive capabilities on
structured EHR data when using well-designed prompting strategies, frequently
surpassing traditional models. However, for unstructured medical texts, LLMs
did not outperform finetuned BERT models, which excelled in both supervised and
unsupervised tasks. Consequently, while LLMs are effective for zero-shot
learning on structured data, finetuned BERT models are more suitable for
unstructured texts, underscoring the importance of selecting models based on
specific task requirements and data characteristics to optimize the application
of NLP technology in healthcare.","[{'name': 'Yinghao Zhu'}, {'name': 'Junyi Gao'}, {'name': 'Zixiang Wang'}, {'name': 'Weibin Liao'}, {'name': 'Xiaochen Zheng'}, {'name': 'Lifang Liang'}, {'name': 'Yasha Wang'}, {'name': 'Chengwei Pan'}, {'name': 'Ewen M. Harrison'}, {'name': 'Liantao Ma'}]",2024-07-26T06:09:10Z
http://arxiv.org/abs/2407.18501v1,http://arxiv.org/abs/2407.18501v1,"The formation of perceptual space in early phonetic acquisition: a
  cross-linguistic modeling approach","This study investigates how learners organize perceptual space in early
phonetic acquisition by advancing previous studies in two key aspects. Firstly,
it examines the shape of the learned hidden representation as well as its
ability to categorize phonetic categories. Secondly, it explores the impact of
training models on context-free acoustic information, without involving
contextual cues, on phonetic acquisition, closely mimicking the early language
learning stage. Using a cross-linguistic modeling approach, autoencoder models
are trained on English and Mandarin and evaluated in both native and non-native
conditions, following experimental conditions used in infant language
perception studies. The results demonstrate that unsupervised bottom-up
training on context-free acoustic information leads to comparable learned
representations of perceptual space between native and non-native conditions
for both English and Mandarin, resembling the early stage of universal
listening in infants. These findings provide insights into the organization of
perceptual space during early phonetic acquisition and contribute to our
understanding of the formation and representation of phonetic categories.","[{'name': 'Frank Lihui Tan'}, {'name': 'Youngah Do'}]",2024-07-26T04:18:36Z
http://arxiv.org/abs/2407.18498v1,http://arxiv.org/abs/2407.18498v1,"A Reliable Common-Sense Reasoning Socialbot Built Using LLMs and
  Goal-Directed ASP","The development of large language models (LLMs), such as GPT, has enabled the
construction of several socialbots, like ChatGPT, that are receiving a lot of
attention for their ability to simulate a human conversation. However, the
conversation is not guided by a goal and is hard to control. In addition,
because LLMs rely more on pattern recognition than deductive reasoning, they
can give confusing answers and have difficulty integrating multiple topics into
a cohesive response. These limitations often lead the LLM to deviate from the
main topic to keep the conversation interesting. We propose AutoCompanion, a
socialbot that uses an LLM model to translate natural language into predicates
(and vice versa) and employs commonsense reasoning based on Answer Set
Programming (ASP) to hold a social conversation with a human. In particular, we
rely on s(CASP), a goal-directed implementation of ASP as the backend. This
paper presents the framework design and how an LLM is used to parse user
messages and generate a response from the s(CASP) engine output. To validate
our proposal, we describe (real) conversations in which the chatbot's goal is
to keep the user entertained by talking about movies and books, and s(CASP)
ensures (i) correctness of answers, (ii) coherence (and precision) during the
conversation, which it dynamically regulates to achieve its specific purpose,
and (iii) no deviation from the main topic.","[{'name': 'Yankai Zeng'}, {'name': 'Abhiramon Rajashekharan'}, {'name': 'Kinjal Basu'}, {'name': 'Huaduo Wang'}, {'name': 'Joaquín Arias'}, {'name': 'Gopal Gupta'}]",2024-07-26T04:13:43Z
http://arxiv.org/abs/2407.18496v1,http://arxiv.org/abs/2407.18496v1,"Towards More Accurate Prediction of Human Empathy and Emotion in Text
  and Multi-turn Conversations by Combining Advanced NLP, Transformers-based
  Networks, and Linguistic Methodologies","Based on the WASSA 2022 Shared Task on Empathy Detection and Emotion
Classification, we predict the level of empathic concern and personal distress
displayed in essays. For the first stage of this project we implemented a
Feed-Forward Neural Network using sentence-level embeddings as features. We
experimented with four different embedding models for generating the inputs to
the neural network. The subsequent stage builds upon the previous work and we
have implemented three types of revisions. The first revision focuses on the
enhancements to the model architecture and the training approach. The second
revision focuses on handling class imbalance using stratified data sampling.
The third revision focuses on leveraging lexical resources, where we apply four
different resources to enrich the features associated with the dataset. During
the final stage of this project, we have created the final end-to-end system
for the primary task using an ensemble of models to revise primary task
performance. Additionally, as part of the final stage, these approaches have
been adapted to the WASSA 2023 Shared Task on Empathy Emotion and Personality
Detection in Interactions, in which the empathic concern, emotion polarity, and
emotion intensity in dyadic text conversations are predicted.","[{'name': 'Manisha Singh'}, {'name': 'Divy Sharma'}, {'name': 'Alonso Ma'}, {'name': 'Nora Goldfine'}]",2024-07-26T04:01:27Z
http://arxiv.org/abs/2407.21059v1,http://arxiv.org/abs/2407.21059v1,"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable
  Frameworks","Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities
of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The
increasing demands of application scenarios have driven the evolution of RAG,
leading to the integration of advanced retrievers, LLMs and other complementary
technologies, which in turn has amplified the intricacy of RAG systems.
However, the rapid advancements are outpacing the foundational RAG paradigm,
with many methods struggling to be unified under the process of
""retrieve-then-generate"". In this context, this paper examines the limitations
of the existing RAG paradigm and introduces the modular RAG framework. By
decomposing complex RAG systems into independent modules and specialized
operators, it facilitates a highly reconfigurable framework. Modular RAG
transcends the traditional linear architecture, embracing a more advanced
design that integrates routing, scheduling, and fusion mechanisms. Drawing on
extensive research, this paper further identifies prevalent RAG
patterns-linear, conditional, branching, and looping-and offers a comprehensive
analysis of their respective implementation nuances. Modular RAG presents
innovative opportunities for the conceptualization and deployment of RAG
systems. Finally, the paper explores the potential emergence of new operators
and paradigms, establishing a solid theoretical foundation and a practical
roadmap for the continued evolution and practical deployment of RAG
technologies.","[{'name': 'Yunfan Gao'}, {'name': 'Yun Xiong'}, {'name': 'Meng Wang'}, {'name': 'Haofen Wang'}]",2024-07-26T03:45:30Z
http://arxiv.org/abs/2407.18483v4,http://arxiv.org/abs/2407.18483v4,"A Role-specific Guided Large Language Model for Ophthalmic Consultation
  Based on Stylistic Differentiation","Ophthalmology consultations are crucial for diagnosing, treating, and
preventing eye diseases. However, the growing demand for consultations exceeds
the availability of ophthalmologists. By leveraging large pre-trained language
models, we can design effective dialogues for specific scenarios, aiding in
consultations. Traditional fine-tuning strategies for question-answering tasks
are impractical due to increasing model size and often ignoring patient-doctor
role function during consultations. In this paper, we propose EyeDoctor, an
ophthalmic medical questioning large language model that enhances accuracy
through doctor-patient role perception guided and an augmented knowledge base
with external disease information. Experimental results show EyeDoctor achieves
higher question-answering precision in ophthalmology consultations. Notably,
EyeDoctor demonstrated a 7.25% improvement in Rouge-1 scores and a 10.16%
improvement in F1 scores on multi-round datasets compared to second best model
ChatGPT, highlighting the importance of doctor-patient role differentiation and
dynamic knowledge base expansion for intelligent medical consultations. EyeDoc
also serves as a free available web based service and souce code is available
at https://github.com/sperfu/EyeDoc.","[{'name': 'Laiyi Fu'}, {'name': 'Binbin Fan'}, {'name': 'Hongkai Du'}, {'name': 'Yanxiang Feng'}, {'name': 'Chunhua Li'}, {'name': 'Huping Song'}]",2024-07-26T03:23:31Z
http://arxiv.org/abs/2407.18479v1,http://arxiv.org/abs/2407.18479v1,Multi-turn Response Selection with Commonsense-enhanced Language Models,"As a branch of advanced artificial intelligence, dialogue systems are
prospering. Multi-turn response selection is a general research problem in
dialogue systems. With the assistance of background information and pre-trained
language models, the performance of state-of-the-art methods on this problem
gains impressive improvement. However, existing studies neglect the importance
of external commonsense knowledge. Hence, we design a Siamese network where a
pre-trained Language model merges with a Graph neural network (SinLG). SinLG
takes advantage of Pre-trained Language Models (PLMs) to catch the word
correlations in the context and response candidates and utilizes a Graph Neural
Network (GNN) to reason helpful common sense from an external knowledge graph.
The GNN aims to assist the PLM in fine-tuning, and arousing its related
memories to attain better performance. Specifically, we first extract related
concepts as nodes from an external knowledge graph to construct a subgraph with
the context response pair as a super node for each sample. Next, we learn two
representations for the context response pair via both the PLM and GNN. A
similarity loss between the two representations is utilized to transfer the
commonsense knowledge from the GNN to the PLM. Then only the PLM is used to
infer online so that efficiency can be guaranteed. Finally, we conduct
extensive experiments on two variants of the PERSONA-CHAT dataset, which proves
that our solution can not only improve the performance of the PLM but also
achieve an efficient inference.","[{'name': 'Yuandong Wang'}, {'name': 'Xuhui Ren'}, {'name': 'Tong Chen'}, {'name': 'Yuxiao Dong'}, {'name': 'Nguyen Quoc Viet Hung'}, {'name': 'Jie Tang'}]",2024-07-26T03:13:47Z
http://arxiv.org/abs/2407.18471v1,http://arxiv.org/abs/2407.18471v1,Constructing the CORD-19 Vaccine Dataset,"We introduce new dataset 'CORD-19-Vaccination' to cater to scientists
specifically looking into COVID-19 vaccine-related research. This dataset is
extracted from CORD-19 dataset [Wang et al., 2020] and augmented with new
columns for language detail, author demography, keywords, and topic per paper.
Facebook's fastText model is used to identify languages [Joulin et al., 2016].
To establish author demography (author affiliation, lab/institution location,
and lab/institution country columns) we processed the JSON file for each paper
and then further enhanced using Google's search API to determine country
values. 'Yake' was used to extract keywords from the title, abstract, and body
of each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to
add topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset,
we demonstrate a question-answering task like the one used in the CORD-19
Kaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential
sentence classification was performed on each paper's abstract using the model
from Dernoncourt et al. [2016]. We partially hand annotated the training
dataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination'
contains 30k research papers and can be immensely valuable for NLP research
such as text mining, information extraction, and question answering, specific
to the domain of COVID-19 vaccine research.","[{'name': 'Manisha Singh'}, {'name': 'Divy Sharma'}, {'name': 'Alonso Ma'}, {'name': 'Bridget Tyree'}, {'name': 'Margaret Mitchell'}]",2024-07-26T02:44:55Z
http://arxiv.org/abs/2407.18461v1,http://arxiv.org/abs/2407.18461v1,"Enhancing Dysarthric Speech Recognition for Unseen Speakers via
  Prototype-Based Adaptation","Dysarthric speech recognition (DSR) presents a formidable challenge due to
inherent inter-speaker variability, leading to severe performance degradation
when applying DSR models to new dysarthric speakers. Traditional speaker
adaptation methodologies typically involve fine-tuning models for each speaker,
but this strategy is cost-prohibitive and inconvenient for disabled users,
requiring substantial data collection. To address this issue, we introduce a
prototype-based approach that markedly improves DSR performance for unseen
dysarthric speakers without additional fine-tuning. Our method employs a
feature extractor trained with HuBERT to produce per-word prototypes that
encapsulate the characteristics of previously unseen speakers. These prototypes
serve as the basis for classification. Additionally, we incorporate supervised
contrastive learning to refine feature extraction. By enhancing representation
quality, we further improve DSR performance, enabling effective personalized
DSR. We release our code at https://github.com/NKU-HLT/PB-DSR.","[{'name': 'Shiyao Wang'}, {'name': 'Shiwan Zhao'}, {'name': 'Jiaming Zhou'}, {'name': 'Aobo Kong'}, {'name': 'Yong Qin'}]",2024-07-26T02:03:23Z
http://arxiv.org/abs/2407.18454v1,http://arxiv.org/abs/2407.18454v1,Fairness Definitions in Language Models Explained,"Language Models (LMs) have demonstrated exceptional performance across
various Natural Language Processing (NLP) tasks. Despite these advancements,
LMs can inherit and amplify societal biases related to sensitive attributes
such as gender and race, limiting their adoption in real-world applications.
Therefore, fairness has been extensively explored in LMs, leading to the
proposal of various fairness notions. However, the lack of clear agreement on
which fairness definition to apply in specific contexts (\textit{e.g.,}
medium-sized LMs versus large-sized LMs) and the complexity of understanding
the distinctions between these definitions can create confusion and impede
further progress. To this end, this paper proposes a systematic survey that
clarifies the definitions of fairness as they apply to LMs. Specifically, we
begin with a brief introduction to LMs and fairness in LMs, followed by a
comprehensive, up-to-date overview of existing fairness notions in LMs and the
introduction of a novel taxonomy that categorizes these concepts based on their
foundational principles and operational distinctions. We further illustrate
each definition through experiments, showcasing their practical implications
and outcomes. Finally, we discuss current research challenges and open
questions, aiming to foster innovative ideas and advance the field. The
implementation and additional resources are publicly available at
https://github.com/LavinWong/Fairness-in-Large-Language-Models/tree/main/definitions.","[{'name': 'Thang Viet Doan'}, {'name': 'Zhibo Chu'}, {'name': 'Zichong Wang'}, {'name': 'Wenbin Zhang'}]",2024-07-26T01:21:25Z
http://arxiv.org/abs/2407.18442v1,http://arxiv.org/abs/2407.18442v1,"Guidance-Based Prompt Data Augmentation in Specialized Domains for Named
  Entity Recognition","While the abundance of rich and vast datasets across numerous fields has
facilitated the advancement of natural language processing, sectors in need of
specialized data types continue to struggle with the challenge of finding
quality data. Our study introduces a novel guidance data augmentation technique
utilizing abstracted context and sentence structures to produce varied
sentences while maintaining context-entity relationships, addressing data
scarcity challenges. By fostering a closer relationship between context,
sentence structure, and role of entities, our method enhances data
augmentation's effectiveness. Consequently, by showcasing diversification in
both entity-related vocabulary and overall sentence structure, and
simultaneously improving the training performance of named entity recognition
task.","[{'name': 'Hyeonseok Kang'}, {'name': 'Hyein Seo'}, {'name': 'Jeesu Jung'}, {'name': 'Sangkeun Jung'}, {'name': 'Du-Seong Chang'}, {'name': 'Riwoo Chung'}]",2024-07-26T00:48:28Z
http://arxiv.org/abs/2407.21058v1,http://arxiv.org/abs/2407.21058v1,"Understanding the Interplay of Scale, Data, and Bias in Language Models:
  A Case Study with BERT","In the current landscape of language model research, larger models, larger
datasets and more compute seems to be the only way to advance towards
intelligence. While there have been extensive studies of scaling laws and
models' scaling behaviors, the effect of scale on a model's social biases and
stereotyping tendencies has received less attention. In this study, we explore
the influence of model scale and pre-training data on its learnt social biases.
We focus on BERT -- an extremely popular language model -- and investigate
biases as they show up during language modeling (upstream), as well as during
classification applications after fine-tuning (downstream). Our experiments on
four architecture sizes of BERT demonstrate that pre-training data
substantially influences how upstream biases evolve with model scale. With
increasing scale, models pre-trained on large internet scrapes like Common
Crawl exhibit higher toxicity, whereas models pre-trained on moderated data
sources like Wikipedia show greater gender stereotypes. However, downstream
biases generally decrease with increasing model scale, irrespective of the
pre-training data. Our results highlight the qualitative role of pre-training
data in the biased behavior of language models, an often overlooked aspect in
the study of scale. Through a detailed case study of BERT, we shed light on the
complex interplay of data and model scale, and investigate how it translates to
concrete biases.","[{'name': 'Muhammad Ali'}, {'name': 'Swetasudha Panda'}, {'name': 'Qinlan Shen'}, {'name': 'Michael Wick'}, {'name': 'Ari Kobren'}]",2024-07-25T23:09:33Z
http://arxiv.org/abs/2407.18421v1,http://arxiv.org/abs/2407.18421v1,Self-Directed Synthetic Dialogues and Revisions Technical Report,"Synthetic data has become an important tool in the fine-tuning of language
models to follow instructions and solve complex problems. Nevertheless, the
majority of open data to date is often lacking multi-turn data and collected on
closed models, limiting progress on advancing open fine-tuning methods. We
introduce Self Directed Synthetic Dialogues (SDSD), an experimental dataset
consisting of guided conversations of language models talking to themselves.
The dataset consists of multi-turn conversations generated with DBRX, Llama 2
70B, and Mistral Large, all instructed to follow a conversation plan generated
prior to the conversation. We also explore including principles from
Constitutional AI and other related works to create synthetic preference data
via revisions to the final conversation turn. We hope this work encourages
further exploration in multi-turn data and the use of open models for expanding
the impact of synthetic data.","[{'name': 'Nathan Lambert'}, {'name': 'Hailey Schoelkopf'}, {'name': 'Aaron Gokaslan'}, {'name': 'Luca Soldaini'}, {'name': 'Valentina Pyatkin'}, {'name': 'Louis Castricato'}]",2024-07-25T22:42:36Z
http://arxiv.org/abs/2407.18418v2,http://arxiv.org/abs/2407.18418v2,Know Your Limits: A Survey of Abstention in Large Language Models,"Abstention, the refusal of large language models (LLMs) to provide an answer,
is increasingly recognized for its potential to mitigate hallucinations and
enhance safety in LLM systems. In this survey, we introduce a framework to
examine abstention from three perspectives: the query, the model, and human
values. We organize the literature on abstention methods, benchmarks, and
evaluation metrics using this framework, and discuss merits and limitations of
prior work. We further identify and motivate areas for future work, centered
around whether abstention can be achieved as a meta-capability that transcends
specific tasks or domains, while still providing opportunities to optimize
abstention abilities based on context.","[{'name': 'Bingbing Wen'}, {'name': 'Jihan Yao'}, {'name': 'Shangbin Feng'}, {'name': 'Chenjun Xu'}, {'name': 'Yulia Tsvetkov'}, {'name': 'Bill Howe'}, {'name': 'Lucy Lu Wang'}]",2024-07-25T22:31:50Z
http://arxiv.org/abs/2407.18416v2,http://arxiv.org/abs/2407.18416v2,PersonaGym: Evaluating Persona Agents and LLMs,"Persona agents, which are LLM agents that act according to an assigned
persona, have demonstrated impressive contextual response capabilities across
various applications. These persona agents offer significant enhancements
across diverse sectors, such as education, healthcare, and entertainment, where
model developers can align agent responses to different user requirements
thereby broadening the scope of agent applications. However, evaluating persona
agent performance is incredibly challenging due to the complexity of assessing
persona adherence in free-form interactions across various environments that
are relevant to each persona agent. We introduce PersonaGym, the first dynamic
evaluation framework for assessing persona agents, and PersonaScore, the first
automated human-aligned metric grounded in decision theory for comprehensive
large-scale evaluation of persona agents. Our evaluation of 6 open and
closed-source LLMs, using a benchmark encompassing 200 personas and 10,000
questions, reveals significant opportunities for advancement in persona agent
capabilities across state-of-the-art models. For example, Claude 3.5 Sonnet
only has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite
being a much more advanced model. Importantly, we find that increased model
size and complexity do not necessarily imply enhanced persona agent
capabilities thereby highlighting the pressing need for algorithmic and
architectural invention towards faithful and performant persona agents.","[{'name': 'Vinay Samuel'}, {'name': 'Henry Peng Zou'}, {'name': 'Yue Zhou'}, {'name': 'Shreyas Chaudhari'}, {'name': 'Ashwin Kalyan'}, {'name': 'Tanmay Rajpurohit'}, {'name': 'Ameet Deshpande'}, {'name': 'Karthik Narasimhan'}, {'name': 'Vishvak Murahari'}]",2024-07-25T22:24:45Z
http://arxiv.org/abs/2407.18376v1,http://arxiv.org/abs/2407.18376v1,"Exploring Bengali Religious Dialect Biases in Large Language Models with
  Evaluation Perspectives","While Large Language Models (LLM) have created a massive technological impact
in the past decade, allowing for human-enabled applications, they can produce
output that contains stereotypes and biases, especially when using low-resource
languages. This can be of great ethical concern when dealing with sensitive
topics such as religion. As a means toward making LLMS more fair, we explore
bias from a religious perspective in Bengali, focusing specifically on two main
religious dialects: Hindu and Muslim-majority dialects. Here, we perform
different experiments and audit showing the comparative analysis of different
sentences using three commonly used LLMs: ChatGPT, Gemini, and Microsoft
Copilot, pertaining to the Hindu and Muslim dialects of specific words and
showcasing which ones catch the social biases and which do not. Furthermore, we
analyze our findings and relate them to potential reasons and evaluation
perspectives, considering their global impact with over 300 million speakers
worldwide. With this work, we hope to establish the rigor for creating more
fairness in LLMs, as these are widely used as creative writing agents.","[{'name': 'Azmine Toushik Wasi'}, {'name': 'Raima Islam'}, {'name': 'Mst Rafia Islam'}, {'name': 'Taki Hasan Rafi'}, {'name': 'Dong-Kyu Chae'}]",2024-07-25T20:19:29Z
http://arxiv.org/abs/2407.18370v1,http://arxiv.org/abs/2407.18370v1,"Trust or Escalate: LLM Judges with Provable Guarantees for Human
  Agreement","We present a principled approach to provide LLM-based evaluation with a
rigorous guarantee of human agreement. We first propose that a reliable
evaluation method should not uncritically rely on model preferences for
pairwise evaluation, but rather assess the confidence of judge models and
selectively decide when to trust its judgement. We then show that under this
selective evaluation framework, human agreement can be provably guaranteed --
such that the model evaluation aligns with that of humans to a user-specified
agreement level. As part of our framework, we also introduce Simulated
Annotators, a novel confidence estimation method that significantly improves
judge calibration and thus enables high coverage of evaluated instances.
Finally, we propose Cascaded Selective Evaluation, where we use cheaper models
as initial judges and escalate to stronger models only when necessary -- again,
while still providing a provable guarantee of human agreement. Experimental
results show that Cascaded Selective Evaluation guarantees strong alignment
with humans, far beyond what LLM judges could achieve without selective
evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never
achieves 80% human agreement, our method, even while employing substantially
cost-effective models such as Mistral-7B, guarantees over 80% human agreement
with almost 80% test coverage.","[{'name': 'Jaehun Jung'}, {'name': 'Faeze Brahman'}, {'name': 'Yejin Choi'}]",2024-07-25T20:04:59Z
http://arxiv.org/abs/2407.18367v1,http://arxiv.org/abs/2407.18367v1,Robust Claim Verification Through Fact Detection,"Claim verification can be a challenging task. In this paper, we present a
method to enhance the robustness and reasoning capabilities of automated claim
verification through the extraction of short facts from evidence. Our novel
approach, FactDetect, leverages Large Language Models (LLMs) to generate
concise factual statements from evidence and label these facts based on their
semantic relevance to the claim and evidence. The generated facts are then
combined with the claim and evidence. To train a lightweight supervised model,
we incorporate a fact-detection task into the claim verification process as a
multitasking approach to improve both performance and explainability. We also
show that augmenting FactDetect in the claim verification prompt enhances
performance in zero-shot claim verification using LLMs. Our method demonstrates
competitive results in the supervised claim verification model by 15% on the F1
score when evaluated for challenging scientific claim verification datasets. We
also demonstrate that FactDetect can be augmented with claim and evidence for
zero-shot prompting (AugFactDetect) in LLMs for verdict prediction. We show
that AugFactDetect outperforms the baseline with statistical significance on
three challenging scientific claim verification datasets with an average of
17.3% performance gain compared to the best performing baselines.","[{'name': 'Nazanin Jafari'}, {'name': 'James Allan'}]",2024-07-25T20:03:43Z
http://arxiv.org/abs/2407.18248v1,http://arxiv.org/abs/2407.18248v1,"Self-Training with Direct Preference Optimization Improves
  Chain-of-Thought Reasoning","Effective training of language models (LMs) for mathematical reasoning tasks
demands high-quality supervised fine-tuning data. Besides obtaining annotations
from human experts, a common alternative is sampling from larger and more
powerful LMs. However, this knowledge distillation approach can be costly and
unstable, particularly when relying on closed-source, proprietary LMs like
GPT-4, whose behaviors are often unpredictable. In this work, we demonstrate
that the reasoning abilities of small-scale LMs can be enhanced through
self-training, a process where models learn from their own outputs. We also
show that the conventional self-training can be further augmented by a
preference learning algorithm called Direct Preference Optimization (DPO). By
integrating DPO into self-training, we leverage preference data to guide LMs
towards more accurate and diverse chain-of-thought reasoning. We evaluate our
method across various mathematical reasoning tasks using different base models.
Our experiments show that this approach not only improves LMs' reasoning
performance but also offers a more cost-effective and scalable solution
compared to relying on large proprietary LMs.","[{'name': 'Tianduo Wang'}, {'name': 'Shichen Li'}, {'name': 'Wei Lu'}]",2024-07-25T17:59:16Z
http://arxiv.org/abs/2407.18242v1,http://arxiv.org/abs/2407.18242v1,LoRA-Pro: Are Low-Rank Adapters Properly Optimized?,"Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method
for parameter-efficient fine-tuning foundation models by re-parameterizing the
original matrix into the product of two low-rank matrices. Despite its
efficiency, LoRA often yields inferior performance compared to full
fine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.
Firstly, we delve into the optimization processes in LoRA and full fine-tuning.
We reveal that while LoRA employs low-rank approximation, it neglects to
approximate the optimization process of full fine-tuning. To address this, we
introduce a novel concept called the ""equivalent gradient."" This virtual
gradient makes the optimization process on the re-parameterized matrix
equivalent to LoRA, which can be used to quantify the differences between LoRA
and full fine-tuning. The equivalent gradient is derived from the gradients of
matrices $A$ and $B$. To narrow the performance gap, our approach minimizes the
differences between the equivalent gradient and the gradient obtained from full
fine-tuning during the optimization process. By solving this objective, we
derive optimal closed-form solutions for updating matrices $A$ and $B$. Our
method constrains the optimization process, shrinking the performance gap
between LoRA and full fine-tuning. Extensive experiments on natural language
processing tasks validate the effectiveness of our method.","[{'name': 'Zhengbo Wang'}, {'name': 'Jian Liang'}]",2024-07-25T17:57:12Z
http://arxiv.org/abs/2407.18219v2,http://arxiv.org/abs/2407.18219v2,"Recursive Introspection: Teaching Language Model Agents How to
  Self-Improve","A central piece in enabling intelligent agentic behavior in foundation models
is to make them capable of introspecting upon their behavior, reasoning, and
correcting their mistakes as more computation or interaction is available. Even
the strongest proprietary large language models (LLMs) do not quite exhibit the
ability of continually improving their responses sequentially, even in
scenarios where they are explicitly told that they are making a mistake. In
this paper, we develop RISE: Recursive IntroSpEction, an approach for
fine-tuning LLMs to introduce this capability, despite prior work hypothesizing
that this capability may not be possible to attain. Our approach prescribes an
iterative fine-tuning procedure, which attempts to teach the model how to alter
its response after having executed previously unsuccessful attempts to solve a
hard test-time problem, with optionally additional environment feedback. RISE
poses fine-tuning for a single-turn prompt as solving a multi-turn Markov
decision process (MDP), where the initial state is the prompt. Inspired by
principles in online imitation learning and reinforcement learning, we propose
strategies for multi-turn data collection and training so as to imbue an LLM
with the capability to recursively detect and correct its previous mistakes in
subsequent iterations. Our experiments show that RISE enables Llama2, Llama3,
and Mistral models to improve themselves with more turns on math reasoning
tasks, outperforming several single-turn strategies given an equal amount of
inference-time computation. We also find that RISE scales well, often attaining
larger benefits with more capable models. Our analysis shows that RISE makes
meaningful improvements to responses to arrive at the correct solution for
challenging prompts, without disrupting one-turn abilities as a result of
expressing more complex distributions.","[{'name': 'Yuxiao Qu'}, {'name': 'Tianjun Zhang'}, {'name': 'Naman Garg'}, {'name': 'Aviral Kumar'}]",2024-07-25T17:35:59Z
http://arxiv.org/abs/2407.18213v2,http://arxiv.org/abs/2407.18213v2,Exploring Scaling Trends in LLM Robustness,"Language model capabilities predictably improve from scaling a model's size
and training data. Motivated by this, increasingly large language models have
been trained, yielding an array of impressive capabilities. Yet these models
are vulnerable to adversarial prompts, such as ""jailbreaks"" that hijack models
to perform undesired behaviors, posing a significant risk of misuse. Prior work
indicates that computer vision models become more robust with model and data
scaling, raising the question: does language model robustness also improve with
scale? We study this question empirically, finding that larger models respond
substantially better to adversarial training, but there is little to no benefit
from model scale in the absence of explicit defenses.","[{'name': 'Nikolaus Howe'}, {'name': 'Michał Zajac'}, {'name': 'Ian McKenzie'}, {'name': 'Oskar Hollinsworth'}, {'name': 'Tom Tseng'}, {'name': 'Pierre-Luc Bacon'}, {'name': 'Adam Gleave'}]",2024-07-25T17:26:41Z
http://arxiv.org/abs/2407.18147v1,http://arxiv.org/abs/2407.18147v1,The FIGNEWS Shared Task on News Media Narratives,"We present an overview of the FIGNEWS shared task, organized as part of the
ArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses
bias and propaganda annotation in multilingual news posts. We focus on the
early days of the Israel War on Gaza as a case study. The task aims to foster
collaboration in developing annotation guidelines for subjective tasks by
creating frameworks for analyzing diverse narratives highlighting potential
bias and propaganda. In a spirit of fostering and encouraging diversity, we
address the problem from a multilingual perspective, namely within five
languages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams
participated in two annotation subtasks: bias (16 teams) and propaganda (6
teams). The teams competed in four evaluation tracks: guidelines development,
annotation quality, annotation quantity, and consistency. Collectively, the
teams produced 129,800 data points. Key findings and implications for the field
are discussed.","[{'name': 'Wajdi Zaghouani'}, {'name': 'Mustafa Jarrar'}, {'name': 'Nizar Habash'}, {'name': 'Houda Bouamor'}, {'name': 'Imed Zitouni'}, {'name': 'Mona Diab'}, {'name': 'Samhaa R. El-Beltagy'}, {'name': 'Muhammed AbuOdeh'}]",2024-07-25T15:58:19Z
http://arxiv.org/abs/2407.18129v2,http://arxiv.org/abs/2407.18129v2,Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic,"Recent advancements have significantly enhanced the capabilities of
Multimodal Large Language Models (MLLMs) in generating and understanding
image-to-text content. Despite these successes, progress is predominantly
limited to English due to the scarcity of high quality multimodal resources in
other languages. This limitation impedes the development of competitive models
in languages such as Arabic. To alleviate this situation, we introduce an
efficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced
language model based on LLaMA-2 to facilitate multimodal interactions. Dallah
demonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning
six Arabic dialects, Dallah showcases its capability to handle complex
dialectal interactions incorporating both textual and visual elements. The
model excels in two benchmark tests: one evaluating its performance on Modern
Standard Arabic (MSA) and another specifically designed to assess dialectal
responses. Beyond its robust performance in multimodal interaction tasks,
Dallah has the potential to pave the way for further development of
dialect-aware Arabic MLLMs.","[{'name': 'Fakhraddin Alwajih'}, {'name': 'Gagan Bhatia'}, {'name': 'Muhammad Abdul-Mageed'}]",2024-07-25T15:36:48Z
http://arxiv.org/abs/2407.18119v1,http://arxiv.org/abs/2407.18119v1,"Tracking linguistic information in transformer-based sentence embeddings
  through targeted sparsification","Analyses of transformer-based models have shown that they encode a variety of
linguistic information from their textual input. While these analyses have shed
a light on the relation between linguistic information on one side, and
internal architecture and parameters on the other, a question remains
unanswered: how is this linguistic information reflected in sentence
embeddings? Using datasets consisting of sentences with known structure, we
test to what degree information about chunks (in particular noun, verb or
prepositional phrases), such as grammatical number, or semantic role, can be
localized in sentence embeddings. Our results show that such information is not
distributed over the entire sentence embedding, but rather it is encoded in
specific regions. Understanding how the information from an input text is
compressed into sentence embeddings helps understand current transformer models
and help build future explainable neural models.","[{'name': 'Vivi Nastase'}, {'name': 'Paola Merlo'}]",2024-07-25T15:27:08Z
http://arxiv.org/abs/2407.18078v1,http://arxiv.org/abs/2407.18078v1,PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization,"The recent emergence of Large Language Models (LLMs) has heralded a new era
of human-AI interaction. These sophisticated models, exemplified by Chat-GPT
and its successors, have exhibited remarkable capabilities in language
understanding. However, as these LLMs have undergone exponential growth, a
crucial dimension that remains understudied is the personalization of these
models. Large foundation models such as GPT-3 etc. focus on creating a
universal model that serves a broad range of tasks and users. This approach
emphasizes the model's generalization capabilities, treating users as a
collective rather than as distinct individuals. While practical for many common
applications, this one-size-fits-all approach often fails to address the rich
tapestry of human diversity and individual needs. To explore this issue we
introduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP
models for user personalization. \datasetname{} consists of a series of
user-centered tasks containing diverse and individualized expressions where the
preferences of users can potentially differ for the same input. Using PEFT-U,
we explore the challenge of efficiently personalizing LLMs to accommodate
user-specific preferences in the context of diverse user-centered tasks.","[{'name': 'Christopher Clarke'}, {'name': 'Yuzhao Heng'}, {'name': 'Lingjia Tang'}, {'name': 'Jason Mars'}]",2024-07-25T14:36:18Z
http://arxiv.org/abs/2407.18061v1,http://arxiv.org/abs/2407.18061v1,Difficulty Estimation and Simplification of French Text Using LLMs,"We leverage generative large language models for language learning
applications, focusing on estimating the difficulty of foreign language texts
and simplifying them to lower difficulty levels. We frame both tasks as
prediction problems and develop a difficulty classification model using labeled
examples, transfer learning, and large language models, demonstrating superior
accuracy compared to previous approaches. For simplification, we evaluate the
trade-off between simplification quality and meaning preservation, comparing
zero-shot and fine-tuned performances of large language models. We show that
meaningful text simplifications can be obtained with limited fine-tuning. Our
experiments are conducted on French texts, but our methods are
language-agnostic and directly applicable to other foreign languages.","[{'name': 'Henri Jamet'}, {'name': 'Yash Raj Shrestha'}, {'name': 'Michalis Vlachos'}]",2024-07-25T14:16:08Z
http://arxiv.org/abs/2407.18058v1,http://arxiv.org/abs/2407.18058v1,"I can listen but cannot read: An evaluation of two-tower multimodal
  systems for instrument recognition","Music two-tower multimodal systems integrate audio and text modalities into a
joint audio-text space, enabling direct comparison between songs and their
corresponding labels. These systems enable new approaches for classification
and retrieval, leveraging both modalities. Despite the promising results they
have shown for zero-shot classification and retrieval tasks, closer inspection
of the embeddings is needed. This paper evaluates the inherent zero-shot
properties of joint audio-text spaces for the case-study of instrument
recognition. We present an evaluation and analysis of two-tower systems for
zero-shot instrument recognition and a detailed analysis of the properties of
the pre-joint and joint embeddings spaces. Our findings suggest that audio
encoders alone demonstrate good quality, while challenges remain within the
text encoder or joint space projection. Specifically, two-tower systems exhibit
sensitivity towards specific words, favoring generic prompts over musically
informed ones. Despite the large size of textual encoders, they do not yet
leverage additional textual context or infer instruments accurately from their
descriptions. Lastly, a novel approach for quantifying the semantic
meaningfulness of the textual space leveraging an instrument ontology is
proposed. This method reveals deficiencies in the systems' understanding of
instruments and provides evidence of the need for fine-tuning text encoders on
musical data.","[{'name': 'Yannis Vasilakis'}, {'name': 'Rachel Bittner'}, {'name': 'Johan Pauwels'}]",2024-07-25T14:15:05Z
http://arxiv.org/abs/2407.18035v1,http://arxiv.org/abs/2407.18035v1,"RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large
  Language Models","Natural images captured by mobile devices often suffer from multiple types of
degradation, such as noise, blur, and low light. Traditional image restoration
methods require manual selection of specific tasks, algorithms, and execution
sequences, which is time-consuming and may yield suboptimal results. All-in-one
models, though capable of handling multiple tasks, typically support only a
limited range and often produce overly smooth, low-fidelity outcomes due to
their broad data distribution fitting. To address these challenges, we first
define a new pipeline for restoring images with multiple degradations, and then
introduce RestoreAgent, an intelligent image restoration system leveraging
multimodal large language models. RestoreAgent autonomously assesses the type
and extent of degradation in input images and performs restoration through (1)
determining the appropriate restoration tasks, (2) optimizing the task
sequence, (3) selecting the most suitable models, and (4) executing the
restoration. Experimental results demonstrate the superior performance of
RestoreAgent in handling complex degradation, surpassing human experts.
Furthermore, the system modular design facilitates the fast integration of new
tasks and models, enhancing its flexibility and scalability for various
applications.","[{'name': 'Haoyu Chen'}, {'name': 'Wenbo Li'}, {'name': 'Jinjin Gu'}, {'name': 'Jingjing Ren'}, {'name': 'Sixiang Chen'}, {'name': 'Tian Ye'}, {'name': 'Renjing Pei'}, {'name': 'Kaiwen Zhou'}, {'name': 'Fenglong Song'}, {'name': 'Lei Zhu'}]",2024-07-25T13:29:37Z
http://arxiv.org/abs/2407.18008v1,http://arxiv.org/abs/2407.18008v1,"GermanPartiesQA: Benchmarking Commercial Large Language Models for
  Political Bias and Sycophancy","LLMs are changing the way humans create and interact with content,
potentially affecting citizens' political opinions and voting decisions. As
LLMs increasingly shape our digital information ecosystems, auditing to
evaluate biases, sycophancy, or steerability has emerged as an active field of
research. In this paper, we evaluate and compare the alignment of six LLMs by
OpenAI, Anthropic, and Cohere with German party positions and evaluate
sycophancy based on a prompt experiment. We contribute to evaluating political
bias and sycophancy in multi-party systems across major commercial LLMs. First,
we develop the benchmark dataset GermanPartiesQA based on the Voting Advice
Application Wahl-o-Mat covering 10 state and 1 national elections between 2021
and 2023. In our study, we find a left-green tendency across all examined LLMs.
We then conduct our prompt experiment for which we use the benchmark and
sociodemographic data of leading German parliamentarians to evaluate changes in
LLMs responses. To differentiate between sycophancy and steerabilty, we use 'I
am [politician X], ...' and 'You are [politician X], ...' prompts. Against our
expectations, we do not observe notable differences between prompting 'I am'
and 'You are'. While our findings underscore that LLM responses can be
ideologically steered with political personas, they suggest that observed
changes in LLM outputs could be better described as personalization to the
given context rather than sycophancy.","[{'name': 'Jan Batzner'}, {'name': 'Volker Stocker'}, {'name': 'Stefan Schmid'}, {'name': 'Gjergji Kasneci'}]",2024-07-25T13:04:25Z
http://arxiv.org/abs/2407.18003v3,http://arxiv.org/abs/2407.18003v3,"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache
  Consumption","Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,
have revolutionized various industries with their advanced language
comprehension. However, their efficiency is challenged by the Transformer
architecture' s struggle with handling long texts. KV-Cache has emerged as a
pivotal solution to this issue, converting the time complexity of token
generation from quadratic to linear, albeit with increased GPU memory overhead
proportional to conversation length. With the development of the LLM community
and academia, various KV-Cache compression methods have been proposed. In this
review, we dissect the various properties of KV-Cache and elaborate on various
methods currently used to optimize the KV-Cache space usage of LLMs. These
methods span the pre-training phase, deployment phase, and inference phase, and
we summarize the commonalities and differences among these methods.
Additionally, we list some metrics for evaluating the long-text capabilities of
large language models, from both efficiency and capability perspectives. Our
review thus sheds light on the evolving landscape of LLM optimization, offering
insights into future advancements in this dynamic field.","[{'name': 'Luohe Shi'}, {'name': 'Hongyi Zhang'}, {'name': 'Yao Yao'}, {'name': 'Zuchao Li'}, {'name': 'Hai Zhao'}]",2024-07-25T12:56:22Z
http://arxiv.org/abs/2407.17997v1,http://arxiv.org/abs/2407.17997v1,"On the Effect of Purely Synthetic Training Data for Different Automatic
  Speech Recognition Architectures","In this work we evaluate the utility of synthetic data for training automatic
speech recognition (ASR). We use the ASR training data to train a
text-to-speech (TTS) system similar to FastSpeech-2. With this TTS we reproduce
the original training data, training ASR systems solely on synthetic data. For
ASR, we use three different architectures, attention-based encoder-decoder,
hybrid deep neural network hidden Markov model and a Gaussian mixture hidden
Markov model, showing the different sensitivity of the models to synthetic data
generation. In order to extend previous work, we present a number of ablation
studies on the effectiveness of synthetic vs. real training data for ASR. In
particular we focus on how the gap between training on synthetic and real data
changes by varying the speaker embedding or by scaling the model size. For the
latter we show that the TTS models generalize well, even when training scores
indicate overfitting.","[{'name': 'Nick Rossenbach'}, {'name': 'Benedikt Hilmes'}, {'name': 'Ralf Schlüter'}]",2024-07-25T12:44:45Z
http://arxiv.org/abs/2407.17974v1,http://arxiv.org/abs/2407.17974v1,"What does Kiki look like? Cross-modal associations between speech sounds
  and visual shapes in vision-and-language models","Humans have clear cross-modal preferences when matching certain novel words
to visual shapes. Evidence suggests that these preferences play a prominent
role in our linguistic processing, language learning, and the origins of
signal-meaning mappings. With the rise of multimodal models in AI, such as
vision- and-language (VLM) models, it becomes increasingly important to uncover
the kinds of visio-linguistic associations these models encode and whether they
align with human representations. Informed by experiments with humans, we probe
and compare four VLMs for a well-known human cross-modal preference, the
bouba-kiki effect. We do not find conclusive evidence for this effect but
suggest that results may depend on features of the models, such as architecture
design, model size, and training details. Our findings inform discussions on
the origins of the bouba-kiki effect in human cognition and future developments
of VLMs that align well with human cross-modal associations.","[{'name': 'Tessa Verhoef'}, {'name': 'Kiana Shahrasbi'}, {'name': 'Tom Kouwenhoven'}]",2024-07-25T12:09:41Z
http://arxiv.org/abs/2407.18990v2,http://arxiv.org/abs/2407.18990v2,"Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM
  Tuning in Real-World Applications","Fine-tuning Large Language Models (LLMs) is an effective method to enhance
their performance on downstream tasks. However, choosing the appropriate
setting of tuning hyperparameters (HPs) is a labor-intensive and
computationally expensive process. Here, we provide recommended HP
configurations for practical use-cases that represent a better starting point
for practitioners, when considering two SOTA LLMs and two commonly used tuning
methods. We describe Coverage-based Search (CBS), a process for ranking HP
configurations based on an offline extensive grid search, such that the top
ranked configurations collectively provide a practical robust recommendation
for a wide range of datasets and domains. We focus our experiments on
Llama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a
total of > 10,000 tuning experiments. Our results suggest that, in general,
Llama-3-8B and LoRA should be preferred, when possible. Moreover, we show that
for both models and tuning methods, exploring only a few HP configurations, as
recommended by our analysis, can provide excellent results in practice, making
this work a valuable resource for practitioners.","[{'name': 'Alon Halfon'}, {'name': 'Shai Gretz'}, {'name': 'Ofir Arviv'}, {'name': 'Artem Spector'}, {'name': 'Orith Toledo-Ronen'}, {'name': 'Yoav Katz'}, {'name': 'Liat Ein-Dor'}, {'name': 'Michal Shmueli-Scheuer'}, {'name': 'Noam Slonim'}]",2024-07-25T12:07:55Z
http://arxiv.org/abs/2407.17960v1,http://arxiv.org/abs/2407.17960v1,"The Curious Case of Representational Alignment: Unravelling
  Visio-Linguistic Tasks in Emergent Communication","Natural language has the universal properties of being compositional and
grounded in reality. The emergence of linguistic properties is often
investigated through simulations of emergent communication in referential
games. However, these experiments have yielded mixed results compared to
similar experiments addressing linguistic properties of human language. Here we
address representational alignment as a potential contributing factor to these
results. Specifically, we assess the representational alignment between agent
image representations and between agent representations and input images. Doing
so, we confirm that the emergent language does not appear to encode human-like
conceptual visual features, since agent image representations drift away from
inputs whilst inter-agent alignment increases. We moreover identify a strong
relationship between inter-agent alignment and topographic similarity, a common
metric for compositionality, and address its consequences. To address these
issues, we introduce an alignment penalty that prevents representational drift
but interestingly does not improve performance on a compositional
discrimination task. Together, our findings emphasise the key role
representational alignment plays in simulations of language emergence.","[{'name': 'Tom Kouwenhoven'}, {'name': 'Max Peeperkorn'}, {'name': 'Bram van Dijk'}, {'name': 'Tessa Verhoef'}]",2024-07-25T11:29:27Z
http://arxiv.org/abs/2407.17940v2,http://arxiv.org/abs/2407.17940v2,Positive Text Reframing under Multi-strategy Optimization,"Differing from sentiment transfer, positive reframing seeks to substitute
negative perspectives with positive expressions while preserving the original
meaning. With the emergence of pre-trained language models (PLMs), it is
possible to achieve acceptable results by fine-tuning PLMs. Nevertheless,
generating fluent, diverse and task-constrained reframing text remains a
significant challenge. To tackle this issue, a \textbf{m}ulti-\textbf{s}trategy
\textbf{o}ptimization \textbf{f}ramework (MSOF) is proposed in this paper.
Starting from the objective of positive reframing, we first design positive
sentiment reward and content preservation reward to encourage the model to
transform the negative expressions of the original text while ensuring the
integrity and consistency of the semantics. Then, different decoding
optimization approaches are introduced to improve the quality of text
generation. Finally, based on the modeling formula of positive reframing, we
propose a multi-dimensional re-ranking method that further selects candidate
sentences from three dimensions: strategy consistency, text similarity and
fluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate
our framework achieves significant improvements on unconstrained and controlled
positive reframing tasks.","[{'name': 'Shutong Jia'}, {'name': 'Biwei Cao'}, {'name': 'Qingqing Gao'}, {'name': 'Jiuxin Cao'}, {'name': 'Bo Liu'}]",2024-07-25T10:58:42Z
http://arxiv.org/abs/2407.20274v1,http://arxiv.org/abs/2407.20274v1,"Exploring the Plausibility of Hate and Counter Speech Detectors with
  Explainable AI","In this paper we investigate the explainability of transformer models and
their plausibility for hate speech and counter speech detection. We compare
representatives of four different explainability approaches, i.e.,
gradient-based, perturbation-based, attention-based, and prototype-based
approaches, and analyze them quantitatively with an ablation study and
qualitatively in a user study. Results show that perturbation-based
explainability performs best, followed by gradient-based and attention-based
explainability. Prototypebased experiments did not yield useful results.
Overall, we observe that explainability strongly supports the users in better
understanding the model predictions.","[{'name': 'Adrian Jaques Böck'}, {'name': 'Djordje Slijepčević'}, {'name': 'Matthias Zeppelzauer'}]",2024-07-25T10:17:04Z
http://arxiv.org/abs/2407.17914v1,http://arxiv.org/abs/2407.17914v1,"Modelling Multimodal Integration in Human Concept Processing with
  Vision-and-Language Models","Representations from deep neural networks (DNNs) have proven remarkably
predictive of neural activity involved in both visual and linguistic
processing. Despite these successes, most studies to date concern unimodal
DNNs, encoding either visual or textual input but not both. Yet, there is
growing evidence that human meaning representations integrate linguistic and
sensory-motor information. Here we investigate whether the integration of
multimodal information operated by current vision-and-language DNN models
(VLMs) leads to representations that are more aligned with human brain activity
than those obtained by language-only and vision-only DNNs. We focus on fMRI
responses recorded while participants read concept words in the context of
either a full sentence or an accompanying picture. Our results reveal that VLM
representations correlate more strongly than language- and vision-only DNNs
with activations in brain areas functionally related to language processing. A
comparison between different types of visuo-linguistic architectures shows that
recent generative VLMs tend to be less brain-aligned than previous
architectures with lower performance on downstream applications. Moreover,
through an additional analysis comparing brain vs. behavioural alignment across
multiple VLMs, we show that -- with one remarkable exception -- representations
that strongly align with behavioural judgments do not correlate highly with
brain responses. This indicates that brain similarity does not go hand in hand
with behavioural similarity, and vice versa.","[{'name': 'Anna Bavaresco'}, {'name': 'Marianne de Heer Kloots'}, {'name': 'Sandro Pezzelle'}, {'name': 'Raquel Fernández'}]",2024-07-25T10:08:37Z
http://arxiv.org/abs/2407.17900v5,http://arxiv.org/abs/2407.17900v5,"The Power of Combining Data and Knowledge: GPT-4o is an Effective
  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of
  Lung Cancer","Lymph node metastasis (LNM) is a crucial factor in determining the initial
treatment for patients with lung cancer, yet accurate preoperative diagnosis of
LNM remains challenging. Recently, large language models (LLMs) have garnered
significant attention due to their remarkable text generation capabilities.
Leveraging the extensive medical knowledge learned from vast corpora, LLMs can
estimate probabilities for clinical problems, though their performance has
historically been inferior to data-driven machine learning models. In this
paper, we propose a novel ensemble method that combines the medical knowledge
acquired by LLMs with the latent patterns identified by machine learning models
to enhance LNM prediction performance. Initially, we developed machine learning
models using patient data. We then designed a prompt template to integrate the
patient data with the predicted probability from the machine learning model.
Subsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,
to estimate the likelihood of LNM based on patient data and then adjust the
estimate using the machine learning output. Finally, we collected three outputs
from the GPT-4o using the same prompt and ensembled these results as the final
prediction. Using the proposed method, our models achieved an AUC value of
0.778 and an AP value of 0.426 for LNM prediction, significantly improving
predictive performance compared to baseline machine learning models. The
experimental results indicate that GPT-4o can effectively leverage its medical
knowledge and the probabilities predicted by machine learning models to achieve
more accurate LNM predictions. These findings demonstrate that LLMs can perform
well in clinical risk prediction tasks, offering a new paradigm for integrating
medical knowledge and patient data in clinical predictions.","[{'name': 'Danqing Hu'}, {'name': 'Bing Liu'}, {'name': 'Xiaofeng Zhu'}, {'name': 'Nan Wu'}]",2024-07-25T09:42:24Z
http://arxiv.org/abs/2407.17876v1,http://arxiv.org/abs/2407.17876v1,"A Large-Scale Sensitivity Analysis on Latent Embeddings and
  Dimensionality Reductions for Text Spatializations","The semantic similarity between documents of a text corpus can be visualized
using map-like metaphors based on two-dimensional scatterplot layouts. These
layouts result from a dimensionality reduction on the document-term matrix or a
representation within a latent embedding, including topic models. Thereby, the
resulting layout depends on the input data and hyperparameters of the
dimensionality reduction and is therefore affected by changes in them.
Furthermore, the resulting layout is affected by changes in the input data and
hyperparameters of the dimensionality reduction. However, such changes to the
layout require additional cognitive efforts from the user. In this work, we
present a sensitivity study that analyzes the stability of these layouts
concerning (1) changes in the text corpora, (2) changes in the hyperparameter,
and (3) randomness in the initialization. Our approach has two stages: data
measurement and data analysis. First, we derived layouts for the combination of
three text corpora and six text embeddings and a grid-search-inspired
hyperparameter selection of the dimensionality reductions. Afterward, we
quantified the similarity of the layouts through ten metrics, concerning local
and global structures and class separation. Second, we analyzed the resulting
42817 tabular data points in a descriptive statistical analysis. From this, we
derived guidelines for informed decisions on the layout algorithm and highlight
specific hyperparameter settings. We provide our implementation as a Git
repository at
https://github.com/hpicgs/Topic-Models-and-Dimensionality-Reduction-Sensitivity-Study
and results as Zenodo archive at https://doi.org/10.5281/zenodo.12772898.","[{'name': 'Daniel Atzberger'}, {'name': 'Tim Cech'}, {'name': 'Willy Scheibel'}, {'name': 'Jürgen Döllner'}, {'name': 'Michael Behrisch'}, {'name': 'Tobias Schreck'}]",2024-07-25T08:46:49Z
http://arxiv.org/abs/2407.17874v1,http://arxiv.org/abs/2407.17874v1,Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions,"End-to-end automatic speech recognition (E2E ASR) systems have significantly
improved speech recognition through training on extensive datasets. Despite
these advancements, they still struggle to accurately recognize domain specific
words, such as proper nouns and technical terminologies. To address this
problem, we propose a method to utilize the state-of-the-art Whisper without
modifying its architecture, preserving its generalization performance while
enabling it to leverage descriptions effectively. Moreover, we propose two
additional training techniques to improve the domain specific ASR: decoder
fine-tuning, and context perturbation. We also propose a method to use a Large
Language Model (LLM) to generate descriptions with simple metadata, when
descriptions are unavailable. Our experiments demonstrate that proposed methods
notably enhance domain-specific ASR accuracy on real-life datasets, with
LLM-generated descriptions outperforming human-crafted ones in effectiveness.","[{'name': 'Jiwon Suh'}, {'name': 'Injae Na'}, {'name': 'Woohwan Jung'}]",2024-07-25T08:44:04Z
http://arxiv.org/abs/2407.17870v1,http://arxiv.org/abs/2407.17870v1,"Is the Digital Forensics and Incident Response Pipeline Ready for
  Text-Based Threats in LLM Era?","In the era of generative AI, the widespread adoption of Neural Text
Generators (NTGs) presents new cybersecurity challenges, particularly within
the realms of Digital Forensics and Incident Response (DFIR). These challenges
primarily involve the detection and attribution of sources behind advanced
attacks like spearphishing and disinformation campaigns. As NTGs evolve, the
task of distinguishing between human and NTG-authored texts becomes critically
complex. This paper rigorously evaluates the DFIR pipeline tailored for
text-based security systems, specifically focusing on the challenges of
detecting and attributing authorship of NTG-authored texts. By introducing a
novel human-NTG co-authorship text attack, termed CS-ACT, our study uncovers
significant vulnerabilities in traditional DFIR methodologies, highlighting
discrepancies between ideal scenarios and real-world conditions. Utilizing 14
diverse datasets and 43 unique NTGs, up to the latest GPT-4, our research
identifies substantial vulnerabilities in the forensic profiling phase,
particularly in attributing authorship to NTGs. Our comprehensive evaluation
points to factors such as model sophistication and the lack of distinctive
style within NTGs as significant contributors for these vulnerabilities. Our
findings underscore the necessity for more sophisticated and adaptable
strategies, such as incorporating adversarial learning, stylizing NTGs, and
implementing hierarchical attribution through the mapping of NTG lineages to
enhance source attribution. This sets the stage for future research and the
development of more resilient text-based security systems.","[{'name': 'Avanti Bhandarkar'}, {'name': 'Ronald Wilson'}, {'name': 'Anushka Swarup'}, {'name': 'Mengdi Zhu'}, {'name': 'Damon Woodard'}]",2024-07-25T08:42:53Z
http://arxiv.org/abs/2407.17866v1,http://arxiv.org/abs/2407.17866v1,Financial Statement Analysis with Large Language Models,"We investigate whether an LLM can successfully perform financial statement
analysis in a way similar to a professional human analyst. We provide
standardized and anonymous financial statements to GPT4 and instruct the model
to analyze them to determine the direction of future earnings. Even without any
narrative or industry-specific information, the LLM outperforms financial
analysts in its ability to predict earnings changes. The LLM exhibits a
relative advantage over human analysts in situations when the analysts tend to
struggle. Furthermore, we find that the prediction accuracy of the LLM is on
par with the performance of a narrowly trained state-of-the-art ML model. LLM
prediction does not stem from its training memory. Instead, we find that the
LLM generates useful narrative insights about a company's future performance.
Lastly, our trading strategies based on GPT's predictions yield a higher Sharpe
ratio and alphas than strategies based on other models. Taken together, our
results suggest that LLMs may take a central role in decision-making.","[{'name': 'Alex Kim'}, {'name': 'Maximilian Muhn'}, {'name': 'Valeri Nikolaev'}]",2024-07-25T08:36:58Z
http://arxiv.org/abs/2407.17863v1,http://arxiv.org/abs/2407.17863v1,factgenie: A Framework for Span-based Evaluation of Generated Texts,"We present factgenie: a framework for annotating and visualizing word spans
in textual model outputs. Annotations can capture various span-based phenomena
such as semantic inaccuracies or irrelevant text. With factgenie, the
annotations can be collected both from human crowdworkers and large language
models. Our framework consists of a web interface for data visualization and
gathering text annotations, powered by an easily extensible codebase.","[{'name': 'Zdeněk Kasner'}, {'name': 'Ondřej Plátek'}, {'name': 'Patrícia Schmidtová'}, {'name': 'Simone Balloccu'}, {'name': 'Ondřej Dušek'}]",2024-07-25T08:33:23Z
http://arxiv.org/abs/2407.17862v1,http://arxiv.org/abs/2407.17862v1,Exploring Description-Augmented Dataless Intent Classification,"In this work, we introduce several schemes to leverage description-augmented
embedding similarity for dataless intent classification using current
state-of-the-art (SOTA) text embedding models. We report results of our methods
on four commonly used intent classification datasets and compare against
previous works of a similar nature. Our work shows promising results for
dataless classification scaling to a large number of unseen intents. We show
competitive results and significant improvements (+6.12\% Avg.) over strong
zero-shot baselines, all without training on labelled or task-specific data.
Furthermore, we provide qualitative error analysis of the shortfalls of this
methodology to help guide future research in this area.","[{'name': 'Ruoyu Hu'}, {'name': 'Foaad Khosmood'}, {'name': 'Abbas Edalat'}]",2024-07-25T08:31:57Z
http://arxiv.org/abs/2407.17854v1,http://arxiv.org/abs/2407.17854v1,"Shapley Value-based Contrastive Alignment for Multimodal Information
  Extraction","The rise of social media and the exponential growth of multimodal
communication necessitates advanced techniques for Multimodal Information
Extraction (MIE). However, existing methodologies primarily rely on direct
Image-Text interactions, a paradigm that often faces significant challenges due
to semantic and modality gaps between images and text. In this paper, we
introduce a new paradigm of Image-Context-Text interaction, where large
multimodal models (LMMs) are utilized to generate descriptive textual context
to bridge these gaps. In line with this paradigm, we propose a novel Shapley
Value-based Contrastive Alignment (Shap-CA) method, which aligns both
context-text and context-image pairs. Shap-CA initially applies the Shapley
value concept from cooperative game theory to assess the individual
contribution of each element in the set of contexts, texts and images towards
total semantic and modality overlaps. Following this quantitative evaluation, a
contrastive learning strategy is employed to enhance the interactive
contribution within context-text/image pairs, while minimizing the influence
across these pairs. Furthermore, we design an adaptive fusion module for
selective cross-modal fusion. Extensive experiments across four MIE datasets
demonstrate that our method significantly outperforms existing state-of-the-art
methods.","[{'name': 'Wen Luo'}, {'name': 'Yu Xia'}, {'name': 'Shen Tianshu'}, {'name': 'Sujian Li'}]",2024-07-25T08:15:43Z
http://arxiv.org/abs/2407.17852v1,http://arxiv.org/abs/2407.17852v1,Scaling A Simple Approach to Zero-Shot Speech Recognition,"Despite rapid progress in increasing the language coverage of automatic
speech recognition, the field is still far from covering all languages with a
known writing script. Recent work showed promising results with a zero-shot
approach requiring only a small amount of text data, however, accuracy heavily
depends on the quality of the used phonemizer which is often weak for unseen
languages. In this paper, we present MMS Zero-shot a conceptually simpler
approach based on romanization and an acoustic model trained on data in 1,078
different languages or three orders of magnitude more than prior art. MMS
Zero-shot reduces the average character error rate by a relative 46% over 100
unseen languages compared to the best previous work. Moreover, the error rate
of our approach is only 2.5x higher compared to in-domain supervised baselines,
while our approach uses no labeled data for the evaluation languages at all.","[{'name': 'Jinming Zhao'}, {'name': 'Vineel Pratap'}, {'name': 'Michael Auli'}]",2024-07-25T08:08:55Z
http://arxiv.org/abs/2407.17844v1,http://arxiv.org/abs/2407.17844v1,"Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease
  Classification: A Systematic Review","Parkinson's disease (PD), the second most prevalent neurodegenerative
disorder worldwide, frequently presents with early-stage speech impairments.
Recent advancements in Artificial Intelligence (AI), particularly deep learning
(DL), have significantly enhanced PD diagnosis through the analysis of speech
data. Nevertheless, the progress of research is restricted by the limited
availability of publicly accessible speech-based PD datasets, primarily due to
privacy and ethical concerns. This review covers the latest DL-based AI
approaches for speech-based PD classification, focusing on performance,
available resources and associated challenges of 33 scientific works published
between 2020 and March 2024. These DL approaches are categorized into
end-to-end (E2E) learning, transfer learning (TL) and deep acoustic features
(DAF) extraction. Among E2E approaches, Convolutional Neural Networks (CNNs)
are prevalent, though Transformers are increasingly popular. E2E approaches
face challenges such as limited data and computational resources, especially
with Transformers. TL addresses these issues by providing more robust PD
diagnosis and better generalizability across languages. DAF extraction aims to
improve the explainability and interpretability of results by examining the
specific effects of deep features on both other DL approaches and more
traditional machine learning (ML) methods. However, it often underperforms
compared to E2E and TL approaches. This review also discusses unresolved issues
related to bias, explainability and privacy, highlighting the need for future
research.","[{'name': 'Lisanne van Gelderen'}, {'name': 'Cristian Tejedor-García'}]",2024-07-25T07:58:19Z
http://arxiv.org/abs/2407.20272v1,http://arxiv.org/abs/2407.20272v1,An Efficient Inference Framework for Early-exit Large Language Models,"Building efficient inference framework has gained increasing interests for
research community. Early-exit models, a variant of LLMs, improves the
inference efficiency of LLMs by skipping rest layers and directly generate
output tokens when they are confident enough. However, there is no work of LLM
inference framework that takes early-exit models into consideration. This is
non-trivial as prior art on LLM inference cannot be directly applied to
early-exit models. In this work, we solves two key challenges in building
efficient inference framework for early-exit models: (1) batch inference at
iteration-level granularity; and (2) KV cache management. For the former, we
propose to process the batch until all sequences surpass the early-exit
confidence threshold. For the latter, we propose to fill the KV cache of rest
layers before the iteration terminates. Our evaluation shows that, compared
with the original vLLM operating at full layers, our solution achieves up to
1.25x speed up.","[{'name': 'Ruijie Miao'}, {'name': 'Yihan Yan'}, {'name': 'Xinshuo Yao'}, {'name': 'Tong Yang'}]",2024-07-25T07:50:17Z
http://arxiv.org/abs/2407.17827v1,http://arxiv.org/abs/2407.17827v1,"Unified Lexical Representation for Interpretable Visual-Language
  Alignment","Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's
groundbreaking work. Although CLIP performs well, the typical direct latent
feature alignment lacks clarity in its representation and similarity scores. On
the other hand, lexical representation, a vector whose element represents the
similarity between the sample and a word from the vocabulary, is a natural
sparse representation and interpretable, providing exact matches for individual
words. However, lexical representations is difficult to learn due to no
ground-truth supervision and false-discovery issues, and thus requires complex
design to train effectively. In this paper, we introduce LexVLA, a more
interpretable VLA framework by learning a unified lexical representation for
both modalities without complex design. We use DINOv2 as our visual model for
its local-inclined features and Llama 2, a generative language model, to
leverage its in-context lexical prediction ability. To avoid the false
discovery, we propose an overuse penalty to refrain the lexical representation
from falsely frequently activating meaningless words. We demonstrate that these
two pre-trained uni-modal models can be well-aligned by fine-tuning on modest
multi-modal dataset and avoid intricate training configurations. On cross-modal
retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,
outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those
trained from scratch on even bigger datasets (e.g., 1.1B data, including
CC-12M). We conduct extensive experiments to analyze LexVLA.","[{'name': 'Yifan Li'}, {'name': 'Yikai Wang'}, {'name': 'Yanwei Fu'}, {'name': 'Dongyu Ru'}, {'name': 'Zheng Zhang'}, {'name': 'Tong He'}]",2024-07-25T07:35:27Z
http://arxiv.org/abs/2407.17817v1,http://arxiv.org/abs/2407.17817v1,Demystifying Verbatim Memorization in Large Language Models,"Large Language Models (LLMs) frequently memorize long sequences verbatim,
often with serious legal and privacy implications. Much prior work has studied
such verbatim memorization using observational data. To complement such work,
we develop a framework to study verbatim memorization in a controlled setting
by continuing pre-training from Pythia checkpoints with injected sequences. We
find that (1) non-trivial amounts of repetition are necessary for verbatim
memorization to happen; (2) later (and presumably better) checkpoints are more
likely to verbatim memorize sequences, even for out-of-distribution sequences;
(3) the generation of memorized sequences is triggered by distributed model
states that encode high-level features and makes important use of general
language modeling capabilities. Guided by these insights, we develop stress
tests to evaluate unlearning methods and find they often fail to remove the
verbatim memorized information, while also degrading the LM. Overall, these
findings challenge the hypothesis that verbatim memorization stems from
specific model weights or mechanisms. Rather, verbatim memorization is
intertwined with the LM's general capabilities and thus will be very difficult
to isolate and suppress without degrading model quality.","[{'name': 'Jing Huang'}, {'name': 'Diyi Yang'}, {'name': 'Christopher Potts'}]",2024-07-25T07:10:31Z
http://arxiv.org/abs/2407.20271v1,http://arxiv.org/abs/2407.20271v1,"Learn while Unlearn: An Iterative Unlearning Framework for Generative
  Language Models","Recent advancements in machine learning, especially in Natural Language
Processing (NLP), have led to the development of sophisticated models trained
on vast datasets, but this progress has raised concerns about potential
sensitive information leakage. In response, regulatory measures like the EU
General Data Protection Regulation (GDPR) have driven the exploration of
Machine Unlearning techniques, which aim to enable models to selectively forget
certain data entries. While early approaches focused on pre-processing methods,
recent research has shifted towards training-based machine unlearning methods.
However, many existing methods require access to original training data, posing
challenges in scenarios where such data is unavailable. Besides, directly
facilitating unlearning may undermine the language model's general expressive
ability. To this end, in this paper, we introduce the Iterative Contrastive
Unlearning (ICU) framework, which addresses these challenges by incorporating
three key components. We propose a Knowledge Unlearning Induction module for
unlearning specific target sequences and a Contrastive Learning Enhancement
module to prevent degrading in generation capacity. Additionally, an Iterative
Unlearning Refinement module is integrated to make the process more adaptive to
each target sample respectively. Experimental results demonstrate the efficacy
of ICU in maintaining performance while efficiently unlearning sensitive
information, offering a promising avenue for privacy-conscious machine learning
applications.","[{'name': 'Haoyu Tang'}, {'name': 'Ye Liu'}, {'name': 'Xukai Liu'}, {'name': 'Kai Zhang'}, {'name': 'Yanghai Zhang'}, {'name': 'Qi Liu'}, {'name': 'Enhong Chen'}]",2024-07-25T07:09:35Z
http://arxiv.org/abs/2408.00588v1,http://arxiv.org/abs/2408.00588v1,"Closing the gap between open-source and commercial large language models
  for medical evidence summarization","Large language models (LLMs) hold great promise in summarizing medical
evidence. Most recent studies focus on the application of proprietary LLMs.
Using proprietary LLMs introduces multiple risk factors, including a lack of
transparency and vendor dependency. While open-source LLMs allow better
transparency and customization, their performance falls short compared to
proprietary ones. In this study, we investigated to what extent fine-tuning
open-source LLMs can further improve their performance in summarizing medical
evidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs
of systematic reviews and summaries, we fine-tuned three broadly-used,
open-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned
LLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:
8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and
15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of
fine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,
smaller fine-tuned models sometimes even demonstrated superior performance
compared to larger zero-shot models. The above trends of improvement were also
manifested in both human and GPT4-simulated evaluations. Our results can be
applied to guide model selection for tasks demanding particular domain
knowledge, such as medical evidence summarization.","[{'name': 'Gongbo Zhang'}, {'name': 'Qiao Jin'}, {'name': 'Yiliang Zhou'}, {'name': 'Song Wang'}, {'name': 'Betina R. Idnay'}, {'name': 'Yiming Luo'}, {'name': 'Elizabeth Park'}, {'name': 'Jordan G. Nestor'}, {'name': 'Matthew E. Spotnitz'}, {'name': 'Ali Soroush'}, {'name': 'Thomas Campion'}, {'name': 'Zhiyong Lu'}, {'name': 'Chunhua Weng'}, {'name': 'Yifan Peng'}]",2024-07-25T05:03:01Z
http://arxiv.org/abs/2407.17773v1,http://arxiv.org/abs/2407.17773v1,KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models,"This paper investigates visual analogical reasoning in large multimodal
models (LMMs) compared to human adults and children. A ""visual analogy"" is an
abstract rule inferred from one image and applied to another. While benchmarks
exist for testing visual reasoning in LMMs, they require advanced skills and
omit basic visual analogies that even young children can make. Inspired by
developmental psychology, we propose a new benchmark of 1,400 visual
transformations of everyday objects to test LMMs on visual analogical reasoning
and compare them to children and adults. We structure the evaluation into three
stages: identifying what changed (e.g., color, number, etc.), how it changed
(e.g., added one object), and applying the rule to new scenarios. Our findings
show that while models like GPT-4V, LLaVA-1.5, and MANTIS identify the ""what""
effectively, they struggle with quantifying the ""how"" and extrapolating this
rule to new objects. In contrast, children and adults exhibit much stronger
analogical reasoning at all three stages. Additionally, the strongest tested
model, GPT-4V, performs better in tasks involving simple visual attributes like
color and size, correlating with quicker human adult response times.
Conversely, more complex tasks such as number, rotation, and reflection, which
necessitate extensive cognitive processing and understanding of the 3D physical
world, present more significant challenges. Altogether, these findings
highlight the limitations of training models on data that primarily consists of
2D images and text.","[{'name': 'Eunice Yiu'}, {'name': 'Maan Qraitem'}, {'name': 'Charlie Wong'}, {'name': 'Anisa Noor Majhi'}, {'name': 'Yutong Bai'}, {'name': 'Shiry Ginosar'}, {'name': 'Alison Gopnik'}, {'name': 'Kate Saenko'}]",2024-07-25T05:02:39Z
http://arxiv.org/abs/2407.17772v1,http://arxiv.org/abs/2407.17772v1,"ERIT Lightweight Multimodal Dataset for Elderly Emotion Recognition and
  Multimodal Fusion Evaluation","ERIT is a novel multimodal dataset designed to facilitate research in a
lightweight multimodal fusion. It contains text and image data collected from
videos of elderly individuals reacting to various situations, as well as seven
emotion labels for each data sample. Because of the use of labeled images of
elderly users reacting emotionally, it is also facilitating research on emotion
recognition in an underrepresented age group in machine learning visual emotion
recognition. The dataset is validated through comprehensive experiments
indicating its importance in neural multimodal fusion research.","[{'name': 'Rita Frieske'}, {'name': 'Bertram E. Shi'}]",2024-07-25T05:02:27Z
http://arxiv.org/abs/2407.17771v1,http://arxiv.org/abs/2407.17771v1,Banyan: Improved Representation Learning with Explicit Structure,"We present Banyan, an improved model to learn semantic representations by
inducing explicit structure over data. In contrast to prior approaches using
structure spanning single sentences, Banyan learns by resolving multiple
constituent structures into a shared one explicitly incorporating global
context. Combined with an improved message-passing scheme inspired by Griffin,
Banyan learns significantly better representations, avoids spurious false
negatives with contrastive learning, and drastically improves memory efficiency
in such explicit-structured models. Using the Self-StrAE framework, we show
that Banyan (a) outperforms baselines using sentential structure across various
settings (b) matches or outperforms unstructured baselines like GloVe
(+augmentations) and a RoBERTa medium (+simcse) pre-trained on 100M tokens,
despite having just a handful of (non-embedding) parameters, and (c) also
learns effective representations across several low resource (Asian and
African) languages as measured on SemRel tasks.","[{'name': 'Mattia Opper'}, {'name': 'N. Siddharth'}]",2024-07-25T04:58:08Z
http://arxiv.org/abs/2407.17770v1,http://arxiv.org/abs/2407.17770v1,BotEval: Facilitating Interactive Human Evaluation,"Following the rapid progress in natural language processing (NLP) models,
language models are applied to increasingly more complex interactive tasks such
as negotiations and conversation moderations. Having human evaluators directly
interact with these NLP models is essential for adequately evaluating the
performance on such interactive tasks. We develop BotEval, an easily
customizable, open-source, evaluation toolkit that focuses on enabling
human-bot interactions as part of the evaluation process, as opposed to human
evaluators making judgements for a static input. BotEval balances flexibility
for customization and user-friendliness by providing templates for common use
cases that span various degrees of complexity and built-in compatibility with
popular crowdsourcing platforms. We showcase the numerous useful features of
BotEval through a study that evaluates the performance of various chatbots on
their effectiveness for conversational moderation and discuss how BotEval
differs from other annotation tools.","[{'name': 'Hyundong Cho'}, {'name': 'Thamme Gowda'}, {'name': 'Yuyang Huang'}, {'name': 'Zixun Lu'}, {'name': 'Tianli Tong'}, {'name': 'Jonathan May'}]",2024-07-25T04:57:31Z
http://arxiv.org/abs/2407.17745v1,http://arxiv.org/abs/2407.17745v1,"Beyond Entity Alignment: Towards Complete Knowledge Graph Alignment via
  Entity-Relation Synergy","Knowledge Graph Alignment (KGA) aims to integrate knowledge from multiple
sources to address the limitations of individual Knowledge Graphs (KGs) in
terms of coverage and depth. However, current KGA models fall short in
achieving a ``complete'' knowledge graph alignment. Existing models primarily
emphasize the linkage of cross-graph entities but overlook aligning relations
across KGs, thereby providing only a partial solution to KGA. The semantic
correlations embedded in relations are largely overlooked, potentially
restricting a comprehensive understanding of cross-KG signals. In this paper,
we propose to conceptualize relation alignment as an independent task and
conduct KGA by decomposing it into two distinct but highly correlated
sub-tasks: entity alignment and relation alignment. To capture the mutually
reinforcing correlations between these objectives, we propose a novel
Expectation-Maximization-based model, EREM, which iteratively optimizes both
sub-tasks. Experimental results on real-world datasets demonstrate that EREM
consistently outperforms state-of-the-art models in both entity alignment and
relation alignment tasks.","[{'name': 'Xiaohan Fang'}, {'name': 'Chaozhuo Li'}, {'name': 'Yi Zhao'}, {'name': 'Qian Zang'}, {'name': 'Litian Zhang'}, {'name': 'Jiquan Peng'}, {'name': 'Xi Zhang'}, {'name': 'Jibing Gong'}]",2024-07-25T03:40:09Z
http://arxiv.org/abs/2407.17734v1,http://arxiv.org/abs/2407.17734v1,"Cost-effective Instruction Learning for Pathology Vision and Language
  Analysis","The advent of vision-language models fosters the interactive conversations
between AI-enabled models and humans. Yet applying these models into clinics
must deal with daunting challenges around large-scale training data, financial,
and computational resources. Here we propose a cost-effective instruction
learning framework for conversational pathology named as CLOVER. CLOVER only
trains a lightweight module and uses instruction tuning while freezing the
parameters of the large language model. Instead of using costly GPT-4, we
propose well-designed prompts on GPT-3.5 for building generation-based
instructions, emphasizing the utility of pathological knowledge derived from
the Internet source. To augment the use of instructions, we construct a
high-quality set of template-based instructions in the context of digital
pathology. From two benchmark datasets, our findings reveal the strength of
hybrid-form instructions in the visual question-answer in pathology. Extensive
results show the cost-effectiveness of CLOVER in answering both open-ended and
closed-ended questions, where CLOVER outperforms strong baselines that possess
37 times more training parameters and use instruction data generated from
GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot
learning in the external clinical dataset. These findings demonstrate that
cost-effective modeling of CLOVER could accelerate the adoption of rapid
conversational applications in the landscape of digital pathology.","[{'name': 'Kaitao Chen'}, {'name': 'Mianxin Liu'}, {'name': 'Fang Yan'}, {'name': 'Lei Ma'}, {'name': 'Xiaoming Shi'}, {'name': 'Lilong Wang'}, {'name': 'Xiaosong Wang'}, {'name': 'Lifeng Zhu'}, {'name': 'Zhe Wang'}, {'name': 'Mu Zhou'}, {'name': 'Shaoting Zhang'}]",2024-07-25T03:12:57Z
http://arxiv.org/abs/2407.17730v1,http://arxiv.org/abs/2407.17730v1,"Are Large Language Models Possible to Conduct Cognitive Behavioral
  Therapy?","In contemporary society, the issue of psychological health has become
increasingly prominent, characterized by the diversification, complexity, and
universality of mental disorders. Cognitive Behavioral Therapy (CBT), currently
the most influential and clinically effective psychological treatment method
with no side effects, has limited coverage and poor quality in most countries.
In recent years, researches on the recognition and intervention of emotional
disorders using large language models (LLMs) have been validated, providing new
possibilities for psychological assistance therapy. However, are LLMs truly
possible to conduct cognitive behavioral therapy? Many concerns have been
raised by mental health experts regarding the use of LLMs for therapy. Seeking
to answer this question, we collected real CBT corpus from online video
websites, designed and conducted a targeted automatic evaluation framework
involving the evaluation of emotion tendency of generated text, structured
dialogue pattern and proactive inquiry ability. For emotion tendency, we
calculate the emotion tendency score of the CBT dialogue text generated by each
model. For structured dialogue pattern, we use a diverse range of automatic
evaluation metrics to compare speaking style, the ability to maintain
consistency of topic and the use of technology in CBT between different models
. As for inquiring to guide the patient, we utilize PQA (Proactive Questioning
Ability) metric. We also evaluated the CBT ability of the LLM after integrating
a CBT knowledge base to explore the help of introducing additional knowledge to
enhance the model's CBT counseling ability. Four LLM variants with excellent
performance on natural language processing are evaluated, and the experimental
result shows the great potential of LLMs in psychological counseling realm,
especially after combining with other technological means.","[{'name': 'Hao Shen'}, {'name': 'Zihan Li'}, {'name': 'Minqiang Yang'}, {'name': 'Minghui Ni'}, {'name': 'Yongfeng Tao'}, {'name': 'Zhengyang Yu'}, {'name': 'Weihao Zheng'}, {'name': 'Chen Xu'}, {'name': 'Bin Hu'}]",2024-07-25T03:01:47Z
http://arxiv.org/abs/2407.21057v1,http://arxiv.org/abs/2407.21057v1,Multi-group Uncertainty Quantification for Long-form Text Generation,"While large language models are rapidly moving towards consumer-facing
applications, they are often still prone to factual errors and hallucinations.
In order to reduce the potential harms that may come from these errors, it is
important for users to know to what extent they can trust an LLM when it makes
a factual claim. To this end, we study the problem of uncertainty
quantification of factual correctness in long-form natural language generation.
Given some output from a large language model, we study both uncertainty at the
level of individual claims contained within the output (via calibration) and
uncertainty across the entire output itself (via conformal prediction).
Moreover, we invoke multicalibration and multivalid conformal prediction to
ensure that such uncertainty guarantees are valid both marginally and across
distinct groups of prompts. Using the task of biography generation, we
demonstrate empirically that having access to and making use of additional
group attributes for each prompt improves both overall and group-wise
performance. As the problems of calibration, conformal prediction, and their
multi-group counterparts have not been extensively explored previously in the
context of long-form text generation, we consider these empirical results to
form a benchmark for this setting.","[{'name': 'Terrance Liu'}, {'name': 'Zhiwei Steven Wu'}]",2024-07-25T02:59:52Z
http://arxiv.org/abs/2407.17716v1,http://arxiv.org/abs/2407.17716v1,"Describe Where You Are: Improving Noise-Robustness for Speech Emotion
  Recognition with Text Description of the Environment","Speech emotion recognition (SER) systems often struggle in real-world
environments, where ambient noise severely degrades their performance. This
paper explores a novel approach that exploits prior knowledge of testing
environments to maximize SER performance under noisy conditions. To address
this task, we propose a text-guided, environment-aware training where an SER
model is trained with contaminated speech samples and their paired noise
description. We use a pre-trained text encoder to extract the text-based
environment embedding and then fuse it to a transformer-based SER model during
training and inference. We demonstrate the effectiveness of our approach
through our experiment with the MSP-Podcast corpus and real-world additive
noise samples collected from the Freesound repository. Our experiment indicates
that the text-based environment descriptions processed by a large language
model (LLM) produce representations that improve the noise-robustness of the
SER system. In addition, our proposed approach with an LLM yields better
performance than our environment-agnostic baselines, especially in low
signal-to-noise ratio (SNR) conditions. When testing at -5dB SNR level, our
proposed method shows better performance than our best baseline model by 31.8 %
(arousal), 23.5% (dominance), and 9.5% (valence).","[{'name': 'Seong-Gyun Leem'}, {'name': 'Daniel Fulford'}, {'name': 'Jukka-Pekka Onnela'}, {'name': 'David Gard'}, {'name': 'Carlos Busso'}]",2024-07-25T02:30:40Z
http://arxiv.org/abs/2407.17695v1,http://arxiv.org/abs/2407.17695v1,Enhancing Agent Learning through World Dynamics Modeling,"While large language models (LLMs) have been increasingly deployed across
tasks in language understanding and interactive decision-making, their
impressive performance is largely due to the comprehensive and in-depth domain
knowledge embedded within them. However, the extent of this knowledge can vary
across different domains. Existing methods often assume that LLMs already
possess such comprehensive and in-depth knowledge of their environment,
overlooking potential gaps in their understanding of actual world dynamics. To
address this gap, we introduce Discover, Verify, and Evolve (DiVE), a framework
that discovers world dynamics from a small number of demonstrations, verifies
the correctness of these dynamics, and evolves new, advanced dynamics tailored
to the current situation. Through extensive evaluations, we analyze the impact
of each component on performance and compare the automatically generated
dynamics from DiVE with human-annotated world dynamics. Our results demonstrate
that LLMs guided by DiVE can make better decisions, achieving rewards
comparable to human players in the Crafter environment.","[{'name': 'Zhiyuan Sun'}, {'name': 'Haochen Shi'}, {'name': 'Marc-Alexandre Côté'}, {'name': 'Glen Berseth'}, {'name': 'Xingdi Yuan'}, {'name': 'Bang Liu'}]",2024-07-25T01:32:41Z
http://arxiv.org/abs/2407.17688v2,http://arxiv.org/abs/2407.17688v2,"Examining the Influence of Political Bias on Large Language Model
  Performance in Stance Classification","Large Language Models (LLMs) have demonstrated remarkable capabilities in
executing tasks based on natural language queries. However, these models,
trained on curated datasets, inherently embody biases ranging from racial to
national and gender biases. It remains uncertain whether these biases impact
the performance of LLMs for certain tasks. In this study, we investigate the
political biases of LLMs within the stance classification task, specifically
examining whether these models exhibit a tendency to more accurately classify
politically-charged stances. Utilizing three datasets, seven LLMs, and four
distinct prompting schemes, we analyze the performance of LLMs on politically
oriented statements and targets. Our findings reveal a statistically
significant difference in the performance of LLMs across various politically
oriented stance classification tasks. Furthermore, we observe that this
difference primarily manifests at the dataset level, with models and prompting
schemes showing statistically similar performances across different stance
classification datasets. Lastly, we observe that when there is greater
ambiguity in the target the statement is directed towards, LLMs have poorer
stance classification accuracy.
  Code & Dataset: http://doi.org/10.5281/zenodo.12938478","[{'name': 'Lynnette Hui Xian Ng'}, {'name': 'Iain Cruickshank'}, {'name': 'Roy Ka-Wei Lee'}]",2024-07-25T01:11:38Z
http://arxiv.org/abs/2407.17686v1,http://arxiv.org/abs/2407.17686v1,Transformers on Markov Data: Constant Depth Suffices,"Attention-based transformers have been remarkably successful at modeling
generative processes across various domains and modalities. In this paper, we
study the behavior of transformers on data drawn from \kth Markov processes,
where the conditional distribution of the next symbol in a sequence depends on
the previous $k$ symbols observed. We observe a surprising phenomenon
empirically which contradicts previous findings: when trained for sufficiently
long, a transformer with a fixed depth and $1$ head per layer is able to
achieve low test loss on sequences drawn from \kth Markov sources, even as $k$
grows. Furthermore, this low test loss is achieved by the transformer's ability
to represent and learn the in-context conditional empirical distribution. On
the theoretical side, our main result is that a transformer with a single head
and three layers can represent the in-context conditional empirical
distribution for \kth Markov sources, concurring with our empirical
observations. Along the way, we prove that \textit{attention-only} transformers
with $O(\log_2(k))$ layers can represent the in-context conditional empirical
distribution by composing induction heads to track the previous $k$ symbols in
the sequence. These results provide more insight into our current understanding
of the mechanisms by which transformers learn to capture context, by
understanding their behavior on Markov sources.","[{'name': 'Nived Rajaraman'}, {'name': 'Marco Bondaschi'}, {'name': 'Kannan Ramchandran'}, {'name': 'Michael Gastpar'}, {'name': 'Ashok Vardhan Makkuva'}]",2024-07-25T01:07:09Z
http://arxiv.org/abs/2407.17678v1,http://arxiv.org/abs/2407.17678v1,"Efficient LLM Training and Serving with Heterogeneous Context Sharding
  among Attention Heads","Existing LLM training and inference frameworks struggle in boosting
efficiency with sparsity while maintaining the integrity of context and model
architecture. Inspired by the sharding concept in database and the fact that
attention parallelizes over heads on accelerators, we propose Sparsely-Sharded
(S2) Attention, an attention algorithm that allocates heterogeneous context
partitions for different attention heads to divide and conquer. S2-Attention
enforces each attention head to only attend to a partition of contexts
following a strided sparsity pattern, while the full context is preserved as
the union of all the shards. As attention heads are processed in separate
thread blocks, the context reduction for each head can thus produce end-to-end
speed-up and memory reduction. At inference, LLMs trained with S2-Attention can
then take the KV cache reduction as free meals with guaranteed model quality
preserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X
wall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction
in end-to-end training time and 10X inference latency, (2) on-par model
training quality compared to default attention, (3)perfect needle retrieval
accuracy over 32K context window. On top of the algorithm, we build DKernel, an
LLM training and inference kernel library that allows users to customize
sparsity patterns for their own models. We open-sourced DKerneland make it
compatible with Megatron, Pytorch, and vLLM.","[{'name': 'Xihui Lin'}, {'name': 'Yunan Zhang'}, {'name': 'Suyu Ge'}, {'name': 'Barun Patra'}, {'name': 'Vishrav Chaudhary'}, {'name': 'Xia Song'}]",2024-07-25T00:27:07Z
http://arxiv.org/abs/2407.17638v2,http://arxiv.org/abs/2407.17638v2,Time Matters: Examine Temporal Effects on Biomedical Language Models,"Time roots in applying language models for biomedical applications: models
are trained on historical data and will be deployed for new or future data,
which may vary from training data. While increasing biomedical tasks have
employed state-of-the-art language models, there are very few studies have
examined temporal effects on biomedical models when data usually shifts across
development and deployment. This study fills the gap by statistically probing
relations between language model performance and data shifts across three
biomedical tasks. We deploy diverse metrics to evaluate model performance,
distance methods to measure data drifts, and statistical methods to quantify
temporal effects on biomedical language models. Our study shows that time
matters for deploying biomedical language models, while the degree of
performance degradation varies by biomedical tasks and statistical
quantification approaches. We believe this study can establish a solid
benchmark to evaluate and assess temporal effects on deploying biomedical
language models.","[{'name': 'Weisi Liu'}, {'name': 'Zhe He'}, {'name': 'Xiaolei Huang'}]",2024-07-24T21:06:40Z
http://arxiv.org/abs/2407.17636v1,http://arxiv.org/abs/2407.17636v1,"IgnitionInnovators at ""Discharge Me!"": Chain-of-Thought Instruction
  Finetuning Large Language Models for Discharge Summaries","This paper presents our proposed approach to the Discharge Me! shared task,
collocated with the 23th Workshop on Biomedical Natural Language Processing
(BioNLP). In this work, we develop an LLM-based framework for solving the
Discharge Summary Documentation (DSD) task, i.e., generating the two critical
target sections `Brief Hospital Course' and `Discharge Instructions' in the
discharge summary. By streamlining the recent instruction-finetuning process on
LLMs, we explore several prompting strategies for optimally adapting LLMs to
specific generation task of DSD. Experimental results show that providing a
clear output structure, complimented by a set of comprehensive
Chain-of-Thoughts (CoT) questions, effectively improves the model's reasoning
capability, and thereby, enhancing the structural correctness and faithfulness
of clinical information in the generated text. Source code is available at:
https://github.com/antangrocket1312/Discharge_LLM","[{'name': 'An Quang Tang'}, {'name': 'Xiuzhen Zhang'}, {'name': 'Minh Ngoc Dinh'}]",2024-07-24T21:02:53Z
http://arxiv.org/abs/2407.17629v2,http://arxiv.org/abs/2407.17629v2,"Papilusion at DAGPap24: Paper or Illusion? Detecting AI-generated
  Scientific Papers","This paper presents Papilusion, an AI-generated scientific text detector
developed within the DAGPap24 shared task on detecting automatically generated
scientific papers. We propose an ensemble-based approach and conduct ablation
studies to analyze the effect of the detector configurations on the
performance. Papilusion is ranked 6th on the leaderboard, and we improve our
performance after the competition ended, achieving 99.46 (+9.63) of the
F1-score on the official test set.","[{'name': 'Nikita Andreev'}, {'name': 'Alexander Shirnin'}, {'name': 'Vladislav Mikhailov'}, {'name': 'Ekaterina Artemova'}]",2024-07-24T20:38:13Z
http://arxiv.org/abs/2407.17624v1,http://arxiv.org/abs/2407.17624v1,"Traditional Methods Outperform Generative LLMs at Forecasting Credit
  Ratings","Large Language Models (LLMs) have been shown to perform well for many
downstream tasks. Transfer learning can enable LLMs to acquire skills that were
not targeted during pre-training. In financial contexts, LLMs can sometimes
beat well-established benchmarks. This paper investigates how well LLMs perform
in the task of forecasting corporate credit ratings. We show that while LLMs
are very good at encoding textual information, traditional methods are still
very competitive when it comes to encoding numeric and multimodal data. For our
task, current LLMs perform worse than a more traditional XGBoost architecture
that combines fundamental and macroeconomic data with high-density text-based
embedding features.","[{'name': 'Felix Drinkall'}, {'name': 'Janet B. Pierrehumbert'}, {'name': 'Stefan Zohren'}]",2024-07-24T20:30:55Z
http://arxiv.org/abs/2407.17605v1,http://arxiv.org/abs/2407.17605v1,Coupling Speech Encoders with Downstream Text Models,"We present a modular approach to building cascade speech translation (AST)
models that guarantees that the resulting model performs no worse than the
1-best cascade baseline while preserving state-of-the-art speech recognition
(ASR) and text translation (MT) performance for a given task. Our novel
contribution is the use of an ``exporter'' layer that is trained under L2-loss
to ensure a strong match between ASR embeddings and the MT token embeddings for
the 1-best sequence. The ``exporter'' output embeddings are fed directly to the
MT model in lieu of 1-best token embeddings, thus guaranteeing that the
resulting model performs no worse than the 1-best cascade baseline, while
allowing back-propagation gradient to flow from the MT model into the ASR
components. The matched-embeddings cascade architecture provide a significant
improvement over its 1-best counterpart in scenarios where incremental training
of the MT model is not an option and yet we seek to improve quality by
leveraging (speech, transcription, translated transcription) data provided with
the AST task. The gain disappears when the MT model is incrementally trained on
the parallel text data available with the AST task. The approach holds promise
for other scenarios that seek to couple ASR encoders and immutable text models,
such at large language models (LLM).","[{'name': 'Ciprian Chelba'}, {'name': 'Johan Schalkwyk'}]",2024-07-24T19:29:13Z
http://arxiv.org/abs/2407.17469v1,http://arxiv.org/abs/2407.17469v1,I Could've Asked That: Reformulating Unanswerable Questions,"When seeking information from unfamiliar documents, users frequently pose
questions that cannot be answered by the documents. While existing large
language models (LLMs) identify these unanswerable questions, they do not
assist users in reformulating their questions, thereby reducing their overall
utility. We curate CouldAsk, an evaluation benchmark composed of existing and
new datasets for document-grounded question answering, specifically designed to
study reformulating unanswerable questions. We evaluate state-of-the-art
open-source and proprietary LLMs on CouldAsk. The results demonstrate the
limited capabilities of these models in reformulating questions. Specifically,
GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the
time, respectively. Error analysis shows that 62% of the unsuccessful
reformulations stem from the models merely rephrasing the questions or even
generating identical questions. We publicly release the benchmark and the code
to reproduce the experiments.","[{'name': 'Wenting Zhao'}, {'name': 'Ge Gao'}, {'name': 'Claire Cardie'}, {'name': 'Alexander M. Rush'}]",2024-07-24T17:59:07Z
http://arxiv.org/abs/2407.17468v1,http://arxiv.org/abs/2407.17468v1,"WildHallucinations: Evaluating Long-form Factuality in LLMs with
  Real-World Entity Queries","While hallucinations of large language models (LLMs) prevail as a major
challenge, existing evaluation benchmarks on factuality do not cover the
diverse domains of knowledge that the real-world users of LLMs seek information
about. To bridge this gap, we introduce WildHallucinations, a benchmark that
evaluates factuality. It does so by prompting LLMs to generate information
about entities mined from user-chatbot conversations in the wild. These
generations are then automatically fact-checked against a systematically
curated knowledge source collected from web search. Notably, half of these
real-world entities do not have associated Wikipedia pages. We evaluate 118,785
generations from 15 LLMs on 7,919 entities. We find that LLMs consistently
hallucinate more on entities without Wikipedia pages and exhibit varying
hallucination rates across different domains. Finally, given the same base
models, adding a retrieval component only slightly reduces hallucinations but
does not eliminate hallucinations.","[{'name': 'Wenting Zhao'}, {'name': 'Tanya Goyal'}, {'name': 'Yu Ying Chiu'}, {'name': 'Liwei Jiang'}, {'name': 'Benjamin Newman'}, {'name': 'Abhilasha Ravichander'}, {'name': 'Khyathi Chandu'}, {'name': 'Ronan Le Bras'}, {'name': 'Claire Cardie'}, {'name': 'Yuntian Deng'}, {'name': 'Yejin Choi'}]",2024-07-24T17:59:05Z
http://arxiv.org/abs/2407.17467v1,http://arxiv.org/abs/2407.17467v1,"CMR Scaling Law: Predicting Critical Mixture Ratios for Continual
  Pre-training of Language Models","Large Language Models (LLMs) excel in diverse tasks but often underperform in
specialized fields due to limited domain-specific or proprietary corpus.
Continual pre-training (CPT) enhances LLM capabilities by imbuing new
domain-specific or proprietary knowledge while replaying general corpus to
prevent catastrophic forgetting. The data mixture ratio of general corpus and
domain-specific corpus, however, has been chosen heuristically, leading to
sub-optimal training efficiency in practice. In this context, we attempt to
re-visit the scaling behavior of LLMs under the hood of CPT, and discover a
power-law relationship between loss, mixture ratio, and training tokens scale.
We formalize the trade-off between general and domain-specific capabilities,
leading to a well-defined Critical Mixture Ratio (CMR) of general and domain
data. By striking the balance, CMR maintains the model's general ability and
achieves the desired domain transfer, ensuring the highest utilization of
available resources. Therefore, if we value the balance between efficiency and
effectiveness, CMR can be consider as the optimal mixture ratio.Through
extensive experiments, we ascertain the predictability of CMR, and propose CMR
scaling law and have substantiated its generalization. These findings offer
practical guidelines for optimizing LLM training in specialized domains,
ensuring both general and domain-specific performance while efficiently
managing training resources.","[{'name': 'Jiawei Gu'}, {'name': 'Zacc Yang'}, {'name': 'Chuanghao Ding'}, {'name': 'Rui Zhao'}, {'name': 'Fei Tan'}]",2024-07-24T17:59:02Z
http://arxiv.org/abs/2407.17546v1,http://arxiv.org/abs/2407.17546v1,"Exploring Domain Robust Lightweight Reward Models based on Router
  Mechanism","Recent advancements in large language models have heavily relied on the large
reward model from reinforcement learning from human feedback for fine-tuning.
However, the use of a single reward model across various domains may not always
be optimal, often requiring retraining from scratch when new domain data is
introduced. To address these challenges, we explore the utilization of small
language models operating in a domain-specific manner based on router
mechanisms. Our three approaches are: 1) utilize mixture of experts to form a
single reward model by modularizing an internal router and experts, 2)
employing external router to select the appropriate reward model from multiple
domain-specific models, and 3) the framework reduces parameter size by loading
reward models and router adapters onto a single small language model using
adapters. Experimental validation underscores the effectiveness of our
approach, demonstrating performance comparable to baseline methods while also
reducing the total parameter size.","[{'name': 'Hyuk Namgoong'}, {'name': 'Jeesu Jung'}, {'name': 'Sangkeun Jung'}, {'name': 'Yoonhyung Roh'}]",2024-07-24T17:25:12Z
http://arxiv.org/abs/2407.17447v1,http://arxiv.org/abs/2407.17447v1,Fluent Student-Teacher Redteaming,"Many publicly available language models have been safety tuned to reduce the
likelihood of toxic or liability-inducing text. Users or security analysts
attempt to jailbreak or redteam these models with adversarial prompts which
cause compliance with requests. One attack method is to apply discrete
optimization techniques to the prompt. However, the resulting attack strings
are often gibberish text, easily filtered by defenders due to high measured
perplexity, and may fail for unseen tasks and/or well-tuned models. In this
work, we improve existing algorithms (primarily GCG and BEAST) to develop
powerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our
technique centers around a new distillation-based approach that encourages the
victim model to emulate a toxified finetune, either in terms of output
probabilities or internal activations. To encourage human-fluent attacks, we
add a multi-model perplexity penalty and a repetition penalty to the objective.
We also enhance optimizer strength by allowing token insertions, token swaps,
and token deletions and by using longer attack sequences. The resulting process
is able to reliably jailbreak the most difficult target models with prompts
that appear similar to human-written prompts. On Advbench we achieve attack
success rates $>93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while
maintaining model-measured perplexity $<33$; we achieve $95$% attack success
for Phi-3, though with higher perplexity. We also find a universally-optimized
single fluent prompt that induces $>88$% compliance on previously unseen tasks
across Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box
models.","[{'name': 'T. Ben Thompson'}, {'name': 'Michael Sklar'}]",2024-07-24T17:23:18Z
http://arxiv.org/abs/2407.17406v1,http://arxiv.org/abs/2407.17406v1,"Dependency Transformer Grammars: Integrating Dependency Structures into
  Transformer Language Models","Syntactic Transformer language models aim to achieve better generalization
through simultaneously modeling syntax trees and sentences. While prior work
has been focusing on adding constituency-based structures to Transformers, we
introduce Dependency Transformer Grammars (DTGs), a new class of Transformer
language model with explicit dependency-based inductive bias. DTGs simulate
dependency transition systems with constrained attention patterns by modifying
attention masks, incorporate the stack information through relative positional
encoding, and augment dependency arc representation with a combination of token
embeddings and operation embeddings. When trained on a dataset of sentences
annotated with dependency trees, DTGs achieve better generalization while
maintaining comparable perplexity with Transformer language model baselines.
DTGs also outperform recent constituency-based models, showing that dependency
can better guide Transformer language models. Our code is released at
https://github.com/zhaoyd1/Dep_Transformer_Grammars.","[{'name': 'Yida Zhao'}, {'name': 'Chao Lou'}, {'name': 'Kewei Tu'}]",2024-07-24T16:38:38Z
http://arxiv.org/abs/2407.17545v1,http://arxiv.org/abs/2407.17545v1,"Large Language Models for Anomaly Detection in Computational Workflows:
  from Supervised Fine-Tuning to In-Context Learning","Anomaly detection in computational workflows is critical for ensuring system
reliability and security. However, traditional rule-based methods struggle to
detect novel anomalies. This paper leverages large language models (LLMs) for
workflow anomaly detection by exploiting their ability to learn complex data
patterns. Two approaches are investigated: 1) supervised fine-tuning (SFT),
where pre-trained LLMs are fine-tuned on labeled data for sentence
classification to identify anomalies, and 2) in-context learning (ICL) where
prompts containing task descriptions and examples guide LLMs in few-shot
anomaly detection without fine-tuning. The paper evaluates the performance,
efficiency, generalization of SFT models, and explores zero-shot and few-shot
ICL prompts and interpretability enhancement via chain-of-thought prompting.
Experiments across multiple workflow datasets demonstrate the promising
potential of LLMs for effective anomaly detection in complex executions.","[{'name': 'Hongwei Jin'}, {'name': 'George Papadimitriou'}, {'name': 'Krishnan Raghavan'}, {'name': 'Pawel Zuk'}, {'name': 'Prasanna Balaprakash'}, {'name': 'Cong Wang'}, {'name': 'Anirban Mandal'}, {'name': 'Ewa Deelman'}]",2024-07-24T16:33:04Z
http://arxiv.org/abs/2408.01453v1,http://arxiv.org/abs/2408.01453v1,"Reporting and Analysing the Environmental Impact of Language Models on
  the Example of Commonsense Question Answering with External Knowledge","Human-produced emissions are growing at an alarming rate, causing already
observable changes in the climate and environment in general. Each year global
carbon dioxide emissions hit a new record, and it is reported that 0.5% of
total US greenhouse gas emissions are attributed to data centres as of 2021.
The release of ChatGPT in late 2022 sparked social interest in Large Language
Models (LLMs), the new generation of Language Models with a large number of
parameters and trained on massive amounts of data. Currently, numerous
companies are releasing products featuring various LLMs, with many more models
in development and awaiting release. Deep Learning research is a competitive
field, with only models that reach top performance attracting attention and
being utilized. Hence, achieving better accuracy and results is often the first
priority, while the model's efficiency and the environmental impact of the
study are neglected. However, LLMs demand substantial computational resources
and are very costly to train, both financially and environmentally. It becomes
essential to raise awareness and promote conscious decisions about algorithmic
and hardware choices. Providing information on training time, the approximate
carbon dioxide emissions and power consumption would assist future studies in
making necessary adjustments and determining the compatibility of available
computational resources with model requirements. In this study, we infused T5
LLM with external knowledge and fine-tuned the model for Question-Answering
task. Furthermore, we calculated and reported the approximate environmental
impact for both steps. The findings demonstrate that the smaller models may not
always be sustainable options, and increased training does not always imply
better performance. The most optimal outcome is achieved by carefully
considering both performance and efficiency factors.","[{'name': 'Aida Usmanova'}, {'name': 'Junbo Huang'}, {'name': 'Debayan Banerjee'}, {'name': 'Ricardo Usbeck'}]",2024-07-24T16:16:16Z
http://arxiv.org/abs/2407.17390v1,http://arxiv.org/abs/2407.17390v1,CovScore: Evaluation of Multi-Document Abstractive Title Set Generation,"This paper introduces CovScore, an automatic reference-less methodology for
evaluating thematic title sets, extracted from a corpus of documents. While
such extraction methods are widely used, evaluating their effectiveness remains
an open question. Moreover, some existing practices heavily rely on slow and
laborious human annotation procedures. Inspired by recently introduced
LLM-based judge methods, we propose a novel methodology that decomposes quality
into five main metrics along different aspects of evaluation. This framing
simplifies and expedites the manual evaluation process and enables automatic
and independent LLM-based evaluation. As a test case, we apply our approach to
a corpus of Holocaust survivor testimonies, motivated both by its relevance to
title set extraction and by the moral significance of this pursuit. We validate
the methodology by experimenting with naturalistic and synthetic title set
generation systems and compare their performance with the methodology.","[{'name': 'Itamar Trainin'}, {'name': 'Omri Abend'}]",2024-07-24T16:14:15Z
http://arxiv.org/abs/2407.17387v1,http://arxiv.org/abs/2407.17387v1,PERSONA: A Reproducible Testbed for Pluralistic Alignment,"The rapid advancement of language models (LMs) necessitates robust alignment
with diverse user values. However, current preference optimization approaches
often fail to capture the plurality of user opinions, instead reinforcing
majority viewpoints and marginalizing minority perspectives. We introduce
PERSONA, a reproducible test bed designed to evaluate and improve pluralistic
alignment of LMs. We procedurally generate diverse user profiles from US census
data, resulting in 1,586 synthetic personas with varied demographic and
idiosyncratic attributes. We then generate a large-scale evaluation dataset
containing 3,868 prompts and 317,200 feedback pairs obtained from our synthetic
personas. Leveraging this dataset, we systematically evaluate LM capabilities
in role-playing diverse users, verified through human judges, and the
establishment of both a benchmark, PERSONA Bench, for pluralistic alignment
approaches as well as an extensive dataset to create new and future benchmarks.
The full dataset and benchmarks are available here:
https://www.synthlabs.ai/research/persona.","[{'name': 'Louis Castricato'}, {'name': 'Nathan Lile'}, {'name': 'Rafael Rafailov'}, {'name': 'Jan-Philipp Fränken'}, {'name': 'Chelsea Finn'}]",2024-07-24T16:11:39Z
http://arxiv.org/abs/2407.17383v1,http://arxiv.org/abs/2407.17383v1,"A Comprehensive Approach to Misspelling Correction with BERT and
  Levenshtein Distance","Writing, as an omnipresent form of human communication, permeates nearly
every aspect of contemporary life. Consequently, inaccuracies or errors in
written communication can lead to profound consequences, ranging from financial
losses to potentially life-threatening situations. Spelling mistakes, among the
most prevalent writing errors, are frequently encountered due to various
factors. This research aims to identify and rectify diverse spelling errors in
text using neural networks, specifically leveraging the Bidirectional Encoder
Representations from Transformers (BERT) masked language model. To achieve this
goal, we compiled a comprehensive dataset encompassing both non-real-word and
real-word errors after categorizing different types of spelling mistakes.
Subsequently, multiple pre-trained BERT models were employed. To ensure optimal
performance in correcting misspelling errors, we propose a combined approach
utilizing the BERT masked language model and Levenshtein distance. The results
from our evaluation data demonstrate that the system presented herein exhibits
remarkable capabilities in identifying and rectifying spelling mistakes, often
surpassing existing systems tailored for the Persian language.","[{'name': 'Amirreza Naziri'}, {'name': 'Hossein Zeinali'}]",2024-07-24T16:07:11Z
http://arxiv.org/abs/2407.17379v2,http://arxiv.org/abs/2407.17379v2,"MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image
  Relational Association Capabilities in Large Visual Language Models","Given the remarkable success that large visual language models (LVLMs) have
achieved in image perception tasks, the endeavor to make LVLMs perceive the
world like humans is drawing increasing attention. Current multi-modal
benchmarks primarily focus on facts or specific topic-related knowledge
contained within individual images. However, they often overlook the
associative relations between multiple images, which require the identification
and analysis of similarities among entities or content present in different
images. Therefore, we propose the multi-image relation association task and a
meticulously curated Multi-granularity Multi-image Relational Association
(MMRA) benchmark, comprising 1,024 samples. In order to systematically and
comprehensively evaluate current LVLMs, we establish an associational relation
system among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)
at two granularity levels (i.e., image and entity) according to the relations
in ConceptNet. Our experiments reveal that on the MMRA benchmark, current
multi-image LVLMs exhibit distinct advantages and disadvantages across various
subtasks. Notably, fine-grained, entity-level multi-image perception tasks pose
a greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs
perform poorly on spatial-related tasks, indicating that LVLMs still have
limited spatial awareness. Additionally, our findings indicate that while LVLMs
demonstrate a strong capability to perceive image details, enhancing their
ability to associate information across multiple images hinges on improving the
reasoning capabilities of their language model component. Moreover, we explored
the ability of LVLMs to perceive image sequences within the context of our
multi-image association task. Our experiments show that the majority of current
LVLMs do not adequately model image sequences during the pre-training process.","[{'name': 'Siwei Wu'}, {'name': 'Kang Zhu'}, {'name': 'Yu Bai'}, {'name': 'Yiming Liang'}, {'name': 'Yizhi Li'}, {'name': 'Haoning Wu'}, {'name': 'J. H. Liu'}, {'name': 'Ruibo Liu'}, {'name': 'Xingwei Qu'}, {'name': 'Xuxin Cheng'}, {'name': 'Ge Zhang'}, {'name': 'Wenhao Huang'}, {'name': 'Chenghua Lin'}]",2024-07-24T15:59:01Z
http://arxiv.org/abs/2407.17349v1,http://arxiv.org/abs/2407.17349v1,"Boosting Large Language Models with Socratic Method for Conversational
  Mathematics Teaching","With the introduction of large language models (LLMs), automatic math
reasoning has seen tremendous success. However, current methods primarily focus
on providing solutions or using techniques like Chain-of-Thought to enhance
problem-solving accuracy. In this paper, we focus on improving the capability
of mathematics teaching via a Socratic teaching-based LLM
(\texttt{SocraticLLM}), which guides learners toward profound thinking with
clarity and self-discovery via conversation. We collect and release a
high-quality mathematical teaching dataset, named \texttt{SocraticMATH}, which
provides Socratic-style conversations of problems with extra knowledge. Also,
we propose a knowledge-enhanced LLM as a strong baseline to generate reliable
responses with review, guidance/heuristic, rectification, and summarization.
Experimental results show the great advantages of \texttt{SocraticLLM} by
comparing it with several strong generative models. The codes and datasets are
available on \url{https://github.com/ECNU-ICALK/SocraticMath}.","[{'name': 'Yuyang Ding'}, {'name': 'Hanglei Hu'}, {'name': 'Jie Zhou'}, {'name': 'Qin Chen'}, {'name': 'Bo Jiang'}, {'name': 'Liang He'}]",2024-07-24T15:18:17Z
http://arxiv.org/abs/2407.17344v1,http://arxiv.org/abs/2407.17344v1,"Label Alignment and Reassignment with Generalist Large Language Model
  for Enhanced Cross-Domain Named Entity Recognition","Named entity recognition on the in-domain supervised and few-shot settings
have been extensively discussed in the NLP community and made significant
progress. However, cross-domain NER, a more common task in practical scenarios,
still poses a challenge for most NER methods. Previous research efforts in that
area primarily focus on knowledge transfer such as correlate label information
from source to target domains but few works pay attention to the problem of
label conflict. In this study, we introduce a label alignment and reassignment
approach, namely LAR, to address this issue for enhanced cross-domain named
entity recognition, which includes two core procedures: label alignment between
source and target domains and label reassignment for type inference. The
process of label reassignment can significantly be enhanced by integrating with
an advanced large-scale language model such as ChatGPT. We conduct an extensive
range of experiments on NER datasets involving both supervised and zero-shot
scenarios. Empirical experimental results demonstrate the validation of our
method with remarkable performance under the supervised and zero-shot
out-of-domain settings compared to SOTA methods.","[{'name': 'Ke Bao'}, {'name': 'Chonghuan Yang'}]",2024-07-24T15:13:12Z
http://arxiv.org/abs/2407.17291v1,http://arxiv.org/abs/2407.17291v1,How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?,"In this study, we address the growing issue of misleading charts, a prevalent
problem that undermines the integrity of information dissemination. Misleading
charts can distort the viewer's perception of data, leading to
misinterpretations and decisions based on false information. The development of
effective automatic detection methods for misleading charts is an urgent field
of research. The recent advancement of multimodal Large Language Models (LLMs)
has introduced a promising direction for addressing this challenge. We explored
the capabilities of these models in analyzing complex charts and assessing the
impact of different prompting strategies on the models' analyses. We utilized a
dataset of misleading charts collected from the internet by prior research and
crafted nine distinct prompts, ranging from simple to complex, to test the
ability of four different multimodal LLMs in detecting over 21 different chart
issues. Through three experiments--from initial exploration to detailed
analysis--we progressively gained insights into how to effectively prompt LLMs
to identify misleading charts and developed strategies to address the
scalability challenges encountered as we expanded our detection range from the
initial five issues to 21 issues in the final experiment. Our findings reveal
that multimodal LLMs possess a strong capability for chart comprehension and
critical thinking in data interpretation. There is significant potential in
employing multimodal LLMs to counter misleading information by supporting
critical thinking and enhancing visualization literacy. This study demonstrates
the applicability of LLMs in addressing the pressing concern of misleading
charts.","[{'name': 'Leo Yu-Ho Lo'}, {'name': 'Huamin Qu'}]",2024-07-24T14:02:20Z
http://arxiv.org/abs/2407.21056v1,http://arxiv.org/abs/2407.21056v1,"What Matters in Explanations: Towards Explainable Fake Review Detection
  Focusing on Transformers","Customers' reviews and feedback play crucial role on electronic
commerce~(E-commerce) platforms like Amazon, Zalando, and eBay in influencing
other customers' purchasing decisions. However, there is a prevailing concern
that sellers often post fake or spam reviews to deceive potential customers and
manipulate their opinions about a product. Over the past decade, there has been
considerable interest in using machine learning (ML) and deep learning (DL)
models to identify such fraudulent reviews. Unfortunately, the decisions made
by complex ML and DL models - which often function as \emph{black-boxes} - can
be surprising and difficult for general users to comprehend. In this paper, we
propose an explainable framework for detecting fake reviews with high precision
in identifying fraudulent content with explanations and investigate what
information matters most for explaining particular decisions by conducting
empirical user evaluation. Initially, we develop fake review detection models
using DL and transformer models including XLNet and DistilBERT. We then
introduce layer-wise relevance propagation (LRP) technique for generating
explanations that can map the contributions of words toward the predicted
class. The experimental results on two benchmark fake review detection datasets
demonstrate that our predictive models achieve state-of-the-art performance and
outperform several existing methods. Furthermore, the empirical user evaluation
of the generated explanations concludes which important information needs to be
considered in generating explanations in the context of fake review
identification.","[{'name': 'Md Shajalal'}, {'name': 'Md Atabuzzaman'}, {'name': 'Alexander Boden'}, {'name': 'Gunnar Stevens'}, {'name': 'Delong Du'}]",2024-07-24T13:26:02Z
http://arxiv.org/abs/2407.17230v1,http://arxiv.org/abs/2407.17230v1,"Improving ICD coding using Chapter based Named Entities and Attentional
  Models","Recent advancements in natural language processing (NLP) have led to
automation in various domains. However, clinical NLP often relies on benchmark
datasets that may not reflect real-world scenarios accurately. Automatic ICD
coding, a vital NLP task, typically uses outdated and imbalanced datasets like
MIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4
and 0.7 due to many false positives. Our research introduces an enhanced
approach to ICD coding that improves F1 scores by using chapter-based named
entities and attentional models. This method categorizes discharge summaries
into ICD-9 Chapters and develops attentional models with chapter-specific data,
eliminating the need to consider external data for code identification. For
categorization, we use Chapter-IV to de-bias and influence key entities and
weights without neural networks, creating accurate thresholds and providing
interpretability for human validation. Post-validation, we develop attentional
models for three frequent and three non-frequent codes from Chapter-IV using
Bidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with
Multi-head Attention architectures. The average Micro-F1 scores of 0.79 and
0.81 from these models demonstrate significant performance improvements in ICD
coding.","[{'name': 'Abhijith R. Beeravolu'}, {'name': 'Mirjam Jonkman'}, {'name': 'Sami Azam'}, {'name': 'Friso De Boer'}]",2024-07-24T12:34:23Z
http://arxiv.org/abs/2407.17227v1,http://arxiv.org/abs/2407.17227v1,"LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN
  prover","Recently, large language models have presented promising results in aiding
formal mathematical reasoning. However, their performance is restricted due to
the scarcity of formal theorem-proving data, which requires additional effort
to be extracted from raw formal language corpora. Meanwhile, a significant
amount of human-written formal language corpora remains underutilized. To
address this issue, we propose LEAN-GitHub, a dataset consisting of large-scale
formal data extracted from almost all Lean 4 repositories on GitHub. After
fine-tuning InternLM-math-plus on this dataset, our model achieved accuracies
of 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F
test, surpassing state-of-the-art method at 52%. And it also achieves
state-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting
different fields/levels of math. These results demonstrate that our proposed
dataset is beneficial for formal reasoning on a wide range of math topics. We
open-source our model at https://GitHub. com/InternLM/InternLM-Math and our
data at https://huggingface.co/ datasets/InternLM/Lean-GitHub","[{'name': 'Zijian Wu'}, {'name': 'Jiayu Wang'}, {'name': 'Dahua Lin'}, {'name': 'Kai Chen'}]",2024-07-24T12:28:03Z
http://arxiv.org/abs/2407.21055v1,http://arxiv.org/abs/2407.21055v1,"Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework
  for Medical Applications","Large Language Models (LLMs) have exhibited remarkable proficiency in natural
language understanding, prompting extensive exploration of their potential
applications across diverse domains. In the medical domain, open-source LLMs
have demonstrated moderate efficacy following domain-specific fine-tuning;
however, they remain substantially inferior to proprietary models such as GPT-4
and GPT-3.5. These open-source models encounter limitations in the
comprehensiveness of domain-specific knowledge and exhibit a propensity for
'hallucinations' during text generation. To mitigate these issues, researchers
have implemented the Retrieval-Augmented Generation (RAG) approach, which
augments LLMs with background information from external knowledge bases while
preserving the model's internal parameters. However, document noise can
adversely affect performance, and the application of RAG in the medical field
remains in its nascent stages. This study presents the Bailicai framework: a
novel integration of retrieval-augmented generation with large language models
optimized for the medical domain. The Bailicai framework augments the
performance of LLMs in medicine through the implementation of four sub-modules.
Experimental results demonstrate that the Bailicai approach surpasses existing
medical domain LLMs across multiple medical benchmarks and exceeds the
performance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates
the prevalent issue of hallucinations in medical applications of LLMs and
ameliorates the noise-related challenges associated with traditional RAG
techniques when processing irrelevant or pseudo-relevant documents.","[{'name': 'Cui Long'}, {'name': 'Yongbin Liu'}, {'name': 'Chunping Ouyang'}, {'name': 'Ying Yu'}]",2024-07-24T12:27:33Z
http://arxiv.org/abs/2407.21054v1,http://arxiv.org/abs/2407.21054v1,Sentiment Reasoning for Healthcare,"Transparency in AI decision-making is crucial in healthcare due to the severe
consequences of errors, and this is important for building trust among AI and
users in sentiment analysis task. Incorporating reasoning capabilities helps
Large Language Models (LLMs) understand human emotions within broader contexts,
handle nuanced and ambiguous language, and infer underlying sentiments that may
not be explicitly stated. In this work, we introduce a new task - Sentiment
Reasoning - for both speech and text modalities, along with our proposed
multimodal multitask framework and dataset. Our study showed that
rationale-augmented training enhances model performance in sentiment
classification across both human transcript and ASR settings. Also, we found
that the generated rationales typically exhibit different vocabularies compared
to human-generated rationales, but maintain similar semantics. All code, data
(English-translated and Vietnamese) and models are published online:
https://github.com/leduckhai/MultiMed","[{'name': 'Khai Le-Duc'}, {'name': 'Khai-Nguyen Nguyen'}, {'name': 'Bach Phan Tat'}, {'name': 'Duy Le'}, {'name': 'Jerry Ngo'}, {'name': 'Long Vo-Dang'}, {'name': 'Anh Totti Nguyen'}, {'name': 'Truong-Son Hy'}]",2024-07-24T12:07:54Z
http://arxiv.org/abs/2407.17174v1,http://arxiv.org/abs/2407.17174v1,"NarrationDep: Narratives on Social Media For Automatic Depression
  Detection","Social media posts provide valuable insight into the narrative of users and
their intentions, including providing an opportunity to automatically model
whether a social media user is depressed or not. The challenge lies in
faithfully modelling user narratives from their online social media posts,
which could potentially be useful in several different applications. We have
developed a novel and effective model called \texttt{NarrationDep}, which
focuses on detecting narratives associated with depression. By analyzing a
user's tweets, \texttt{NarrationDep} accurately identifies crucial narratives.
\texttt{NarrationDep} is a deep learning framework that jointly models
individual user tweet representations and clusters of users' tweets. As a
result, \texttt{NarrationDep} is characterized by a novel two-layer deep
learning model: the first layer models using social media text posts, and the
second layer learns semantic representations of tweets associated with a
cluster. To faithfully model these cluster representations, the second layer
incorporates a novel component that hierarchically learns from users' posts.
The results demonstrate that our framework outperforms other comparative models
including recently developed models on a variety of datasets.","[{'name': 'Hamad Zogan'}, {'name': 'Imran Razzak'}, {'name': 'Shoaib Jameel'}, {'name': 'Guandong Xu'}]",2024-07-24T11:24:25Z
http://arxiv.org/abs/2407.17172v1,http://arxiv.org/abs/2407.17172v1,Speech Editing -- a Summary,"With the rise of video production and social media, speech editing has become
crucial for creators to address issues like mispronunciations, missing words,
or stuttering in audio recordings. This paper explores text-based speech
editing methods that modify audio via text transcripts without manual waveform
editing. These approaches ensure edited audio is indistinguishable from the
original by altering the mel-spectrogram. Recent advancements, such as
context-aware prosody correction and advanced attention mechanisms, have
improved speech editing quality. This paper reviews state-of-the-art methods,
compares key metrics, and examines widely used datasets. The aim is to
highlight ongoing issues and inspire further research and innovation in speech
editing.","[{'name': 'Tobias Kässmann'}, {'name': 'Yining Liu'}, {'name': 'Danni Liu'}]",2024-07-24T11:22:57Z
http://arxiv.org/abs/2407.17167v1,http://arxiv.org/abs/2407.17167v1,"Zero-Shot vs. Few-Shot Multi-Speaker TTS Using Pre-trained Czech
  SpeechT5 Model","In this paper, we experimented with the SpeechT5 model pre-trained on
large-scale datasets. We pre-trained the foundation model from scratch and
fine-tuned it on a large-scale robust multi-speaker text-to-speech (TTS) task.
We tested the model capabilities in a zero- and few-shot scenario. Based on two
listening tests, we evaluated the synthetic audio quality and the similarity of
how synthetic voices resemble real voices. Our results showed that the SpeechT5
model can generate a synthetic voice for any speaker using only one minute of
the target speaker's data. We successfully demonstrated the high quality and
similarity of our synthetic voices on publicly known Czech politicians and
celebrities.","[{'name': 'Jan Lehečka'}, {'name': 'Zdeněk Hanzlíček'}, {'name': 'Jindřich Matoušek'}, {'name': 'Daniel Tihelka'}]",2024-07-24T11:14:06Z
http://arxiv.org/abs/2407.17160v1,http://arxiv.org/abs/2407.17160v1,"A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for
  Automatic Speech Recognition in Multilingual Oral History Archives","In this paper, we are comparing monolingual Wav2Vec 2.0 models with various
multilingual models to see whether we could improve speech recognition
performance on a unique oral history archive containing a lot of mixed-language
sentences. Our main goal is to push forward research on this unique dataset,
which is an extremely valuable part of our cultural heritage. Our results
suggest that monolingual speech recognition models are, in most cases, superior
to multilingual models, even when processing the oral history archive full of
mixed-language sentences from non-native speakers. We also performed the same
experiments on the public CommonVoice dataset to verify our results. We are
contributing to the research community by releasing our pre-trained models to
the public.","[{'name': 'Jan Lehečka'}, {'name': 'Josef V. Psutka'}, {'name': 'Luboš Šmídl'}, {'name': 'Pavel Ircing'}, {'name': 'Josef Psutka'}]",2024-07-24T11:03:47Z
http://arxiv.org/abs/2407.17150v2,http://arxiv.org/abs/2407.17150v2,SimCT: A Simple Consistency Test Protocol in LLMs Development Lifecycle,"In this work, we report our efforts to advance the standard operation
procedure of developing Large Language Models (LLMs) or LLMs-based systems or
services in industry. We introduce the concept of Large Language Model
Development Lifecycle (LDLC) and then highlight the importance of consistency
test in ensuring the delivery quality. The principled solution of consistency
test, however, is usually overlooked by industrial practitioners and not urgent
in academia, and current practical solutions are insufficiently rigours and
labor-intensive. We thus propose a simple yet effective consistency test
protocol, named SimCT. SimCT is mainly to proactively check the consistency
across different development stages of ""bare metal"" LLMs or associated services
without accessing the model artifacts, in an attempt to expedite the delivery
by reducing the back-and-forth alignment communications among multiple teams
involved in different development stages.
  Specifically, SimCT encompasses response-wise and model-wise tests. We
implement the protocol with LightGBM and Student's t-test for two components
respectively, and perform extensive experiments to substantiate the
effectiveness of SimCT and the involved components.","[{'name': 'Fufangchen Zhao'}, {'name': 'Guoqiang Jin'}, {'name': 'Rui Zhao'}, {'name': 'Jiangheng Huang'}, {'name': 'Fei Tan'}]",2024-07-24T10:49:19Z
http://arxiv.org/abs/2407.17126v1,http://arxiv.org/abs/2407.17126v1,"SDoH-GPT: Using Large Language Models to Extract Social Determinants of
  Health (SDoH)","Extracting social determinants of health (SDoH) from unstructured medical
notes depends heavily on labor-intensive annotations, which are typically
task-specific, hampering reusability and limiting sharing. In this study we
introduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)
method leveraging contrastive examples and concise instructions to extract SDoH
without relying on extensive medical annotations or costly human intervention.
It achieved tenfold and twentyfold reductions in time and cost respectively,
and superior consistency with human annotators measured by Cohen's kappa of up
to 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the
strengths of both, ensuring high accuracy and computational efficiency while
consistently maintaining 0.90+ AUROC scores. Testing across three distinct
datasets has confirmed its robustness and accuracy. This study highlights the
potential of leveraging LLMs to revolutionize medical note classification,
demonstrating their capability to achieve highly accurate classifications with
significantly reduced time and cost.","[{'name': 'Bernardo Consoli'}, {'name': 'Xizhi Wu'}, {'name': 'Song Wang'}, {'name': 'Xinyu Zhao'}, {'name': 'Yanshan Wang'}, {'name': 'Justin Rousseau'}, {'name': 'Tom Hartvigsen'}, {'name': 'Li Shen'}, {'name': 'Huanmei Wu'}, {'name': 'Yifan Peng'}, {'name': 'Qi Long'}, {'name': 'Tianlong Chen'}, {'name': 'Ying Ding'}]",2024-07-24T09:57:51Z
http://arxiv.org/abs/2407.17125v2,http://arxiv.org/abs/2407.17125v2,"Behavioral Testing: Can Large Language Models Implicitly Resolve
  Ambiguous Entities?","One of the major aspects contributing to the striking performance of large
language models (LLMs) is the vast amount of factual knowledge accumulated
during pre-training. Yet, many LLMs suffer from self-inconsistency, which
raises doubts about their trustworthiness and reliability. In this paper, we
focus on entity type ambiguity and analyze current state-of-the-art LLMs for
their proficiency and consistency in applying their factual knowledge when
prompted for entities under ambiguity. To do so, we propose an evaluation
protocol that disentangles knowing from applying knowledge, and test
state-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs perform
poorly with ambiguous prompts, achieving only 80% accuracy. Our results further
demonstrate systematic discrepancies in LLM behavior and their failure to
consistently apply information, indicating that the models can exhibit
knowledge without being able to utilize it, significant biases for preferred
readings, as well as self inconsistencies. Our study highlights the importance
of handling entity ambiguity in future for more trustworthy LLMs","[{'name': 'Anastasiia Sedova'}, {'name': 'Robert Litschko'}, {'name': 'Diego Frassinelli'}, {'name': 'Benjamin Roth'}, {'name': 'Barbara Plank'}]",2024-07-24T09:48:48Z
http://arxiv.org/abs/2407.17081v1,http://arxiv.org/abs/2407.17081v1,"A Survey Forest Diagram : Gain a Divergent Insight View on a Specific
  Research Topic","With the exponential growth in the number of papers and the trend of AI
research, the use of Generative AI for information retrieval and
question-answering has become popular for conducting research surveys. However,
novice researchers unfamiliar with a particular field may not significantly
improve their efficiency in interacting with Generative AI because they have
not developed divergent thinking in that field. This study aims to develop an
in-depth Survey Forest Diagram that guides novice researchers in divergent
thinking about the research topic by indicating the citation clues among
multiple papers, to help expand the survey perspective for novice researchers.","[{'name': 'Jinghong Li'}, {'name': 'Wen Gu'}, {'name': 'Koichi Ota'}, {'name': 'Shinobu Hasegawa'}]",2024-07-24T08:17:37Z
http://arxiv.org/abs/2407.17075v3,http://arxiv.org/abs/2407.17075v3,SAFETY-J: Evaluating Safety with Critique,"The deployment of Large Language Models (LLMs) in content generation raises
significant safety concerns, particularly regarding the transparency and
interpretability of content evaluations. Current methods, primarily focused on
binary safety classifications, lack mechanisms for detailed critique, limiting
their utility for model improvement and user trust. To address these
limitations, we introduce SAFETY-J, a bilingual generative safety evaluator for
English and Chinese with critique-based judgment. SAFETY-J utilizes a robust
training dataset that includes diverse dialogues and augmented query-response
pairs to assess safety across various scenarios comprehensively. We establish
an automated meta-evaluation benchmark that objectively assesses the quality of
critiques with minimal human intervention, facilitating scalable and continuous
improvement. Additionally, SAFETY-J employs an iterative preference learning
technique to dynamically refine safety assessments based on meta-evaluations
and critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced
and accurate safety evaluations, thereby enhancing both critique quality and
predictive reliability in complex content scenarios. To facilitate further
research and application, we open-source SAFETY-J's training protocols,
datasets, and code at https://github.com/GAIR-NLP/Safety-J.","[{'name': 'Yixiu Liu'}, {'name': 'Yuxiang Zheng'}, {'name': 'Shijie Xia'}, {'name': 'Jiajun Li'}, {'name': 'Yi Tu'}, {'name': 'Chaoling Song'}, {'name': 'Pengfei Liu'}]",2024-07-24T08:04:00Z
http://arxiv.org/abs/2407.17060v1,http://arxiv.org/abs/2407.17060v1,High Efficiency Image Compression for Large Visual-Language Models,"In recent years, large visual language models (LVLMs) have shown impressive
performance and promising generalization capability in multi-modal tasks, thus
replacing humans as receivers of visual information in various application
scenarios. In this paper, we pioneer to propose a variable bitrate image
compression framework consisting of a pre-editing module and an end-to-end
codec to achieve promising rate-accuracy performance for different LVLMs. In
particular, instead of optimizing an adaptive pre-editing network towards a
particular task or several representative tasks, we propose a new optimization
strategy tailored for LVLMs, which is designed based on the representation and
discrimination capability with token-level distortion and rank. The pre-editing
module and the variable bitrate end-to-end image codec are jointly trained by
the losses based on semantic tokens of the large model, which introduce
enhanced generalization capability for various data and tasks. {Experimental
results demonstrate that the proposed framework could efficiently achieve much
better rate-accuracy performance compared to the state-of-the-art coding
standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal
tasks have revealed the robustness and generalization capability of the
proposed framework.","[{'name': 'Binzhe Li'}, {'name': 'Shurun Wang'}, {'name': 'Shiqi Wang'}, {'name': 'Yan Ye'}]",2024-07-24T07:37:12Z
http://arxiv.org/abs/2407.17023v1,http://arxiv.org/abs/2407.17023v1,From Internal Conflict to Contextual Adaptation of Language Models,"Knowledge-intensive language understanding tasks require Language Models
(LMs) to integrate relevant context, mitigating their inherent weaknesses, such
as incomplete or outdated knowledge. Nevertheless, studies indicate that LMs
often ignore the provided context as it can conflict with the pre-existing LM's
memory learned during pre-training. Moreover, conflicting knowledge can already
be present in the LM's parameters, termed intra-memory conflict. Existing works
have studied the two types of knowledge conflicts only in isolation. We
conjecture that the (degree of) intra-memory conflicts can in turn affect LM's
handling of context-memory conflicts. To study this, we introduce the DYNAMICQA
dataset, which includes facts with a temporal dynamic nature where a fact can
change with a varying time frequency and disputable dynamic facts, which can
change depending on the viewpoint. DYNAMICQA is the first to include real-world
knowledge conflicts and provide context to study the link between the different
types of knowledge conflicts. With the proposed dataset, we assess the use of
uncertainty for measuring the intra-memory conflict and introduce a novel
Coherent Persuasion (CP) score to evaluate the context's ability to sway LM's
semantic output. Our extensive experiments reveal that static facts, which are
unlikely to change, are more easily updated with additional context, relative
to temporal and disputable facts.","[{'name': 'Sara Vera Marjanović'}, {'name': 'Haeun Yu'}, {'name': 'Pepa Atanasova'}, {'name': 'Maria Maistro'}, {'name': 'Christina Lioma'}, {'name': 'Isabelle Augenstein'}]",2024-07-24T06:06:07Z
http://arxiv.org/abs/2407.17022v1,http://arxiv.org/abs/2407.17022v1,"Can Language Models Evaluate Human Written Text? Case Study on Korean
  Student Writing for Education","Large language model (LLM)-based evaluation pipelines have demonstrated their
capability to robustly evaluate machine-generated text. Extending this
methodology to assess human-written text could significantly benefit
educational settings by providing direct feedback to enhance writing skills,
although this application is not straightforward. In this paper, we investigate
whether LLMs can effectively assess human-written text for educational
purposes. We collected 100 texts from 32 Korean students across 15 types of
writing and employed GPT-4-Turbo to evaluate them using grammaticality,
fluency, coherence, consistency, and relevance as criteria. Our analyses
indicate that LLM evaluators can reliably assess grammaticality and fluency, as
well as more objective types of writing, though they struggle with other
criteria and types of writing. We publicly release our dataset and feedback.","[{'name': 'Seungyoon Kim'}, {'name': 'Seungone Kim'}]",2024-07-24T06:02:57Z
http://arxiv.org/abs/2407.17011v1,http://arxiv.org/abs/2407.17011v1,"Unveiling In-Context Learning: A Coordinate System to Understand Its
  Working Mechanism","Large language models (LLMs) exhibit remarkable in-context learning (ICL)
capabilities. However, the underlying working mechanism of ICL remains poorly
understood. Recent research presents two conflicting views on ICL: One
attributes it to LLMs' inherent ability of task recognition, deeming label
correctness and shot numbers of demonstrations as not crucial; the other
emphasizes the impact of similar examples in the demonstrations, stressing the
need for label correctness and more shots. In this work, we provide a
Two-Dimensional Coordinate System that unifies both views into a systematic
framework. The framework explains the behavior of ICL through two orthogonal
variables: whether LLMs can recognize the task and whether similar examples are
presented in the demonstrations. We propose the peak inverse rank metric to
detect the task recognition ability of LLMs and study LLMs' reactions to
different definitions of similarity. Based on these, we conduct extensive
experiments to elucidate how ICL functions across each quadrant on multiple
representative classification tasks. Finally, we extend our analyses to
generation tasks, showing that our coordinate system can also be used to
interpret ICL for generation tasks effectively.","[{'name': 'Anhao Zhao'}, {'name': 'Fanghua Ye'}, {'name': 'Jinlan Fu'}, {'name': 'Xiaoyu Shen'}]",2024-07-24T05:26:52Z
http://arxiv.org/abs/2407.16997v1,http://arxiv.org/abs/2407.16997v1,"Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal
  Intervention Perspective","This paper investigates Who's Harry Potter (WHP), a pioneering yet
insufficiently understood method for LLM unlearning. We explore it in two
steps. First, we introduce a new task of LLM targeted unlearning, where given
an unlearning target (e.g., a person) and some unlearning documents, we aim to
unlearn only the information about the target, rather than everything in the
unlearning documents. We further argue that a successful unlearning should
satisfy criteria such as not outputting gibberish, not fabricating facts about
the unlearning target, and not releasing factual information under jailbreak
attacks. Second, we construct a causal intervention framework for targeted
unlearning, where the knowledge of the unlearning target is modeled as a
confounder between LLM input and output, and the unlearning process as a
deconfounding process. This framework justifies and extends WHP, deriving a
simple unlearning algorithm that includes WHP as a special case. Experiments on
existing and new datasets show that our approach, without explicitly optimizing
for the aforementioned criteria, achieves competitive performance in all of
them. Our code is available at
https://github.com/UCSB-NLP-Chang/causal_unlearn.git.","[{'name': 'Yujian Liu'}, {'name': 'Yang Zhang'}, {'name': 'Tommi Jaakkola'}, {'name': 'Shiyu Chang'}]",2024-07-24T04:39:24Z
http://arxiv.org/abs/2407.16994v1,http://arxiv.org/abs/2407.16994v1,"A Voter-Based Stochastic Rejection-Method Framework for Asymptotically
  Safe Language Model Outputs","This paper proposes a new method for preventing unsafe or otherwise low
quality large language model (LLM) outputs, by leveraging the stochasticity of
LLMs. We propose a system whereby LLM checkers vote on the acceptability of a
generated output, regenerating it if a threshold of disapproval is reached,
until sufficient checkers approve. We further propose estimators for cost and
failure rate, and based on those estimators and experimental data tailored to
the application, we propose an algorithm that achieves a desired failure rate
at the least possible cost. We demonstrate that, under these models, failure
rate decreases exponentially as a function of cost when voter count and
threshold are chosen according to the algorithm, and that the models reasonably
estimate the actual performance of such a system in action, even with limited
data.","[{'name': 'Jake R. Watts'}, {'name': 'Joel Sokol'}]",2024-07-24T04:27:55Z
http://arxiv.org/abs/2407.17532v1,http://arxiv.org/abs/2407.17532v1,"Generative artificial intelligence in dentistry: Current approaches and
  future challenges","Artificial intelligence (AI) has become a commodity for people because of the
advent of generative AI (GenAI) models that bridge the usability gap of AI by
providing a natural language interface to interact with complex models. These
GenAI models range from text generation - such as two-way chat systems - to the
generation of image or video from textual descriptions input by a user. These
advancements in AI have impacted Dentistry in multiple aspects. In dental
education, the student now has the opportunity to solve a plethora of questions
by only prompting a GenAI model and have the answer in a matter of seconds.
GenAI models can help us deliver better patient healthcare by helping
practitioners gather knowledge quickly and efficiently. Finally, GenAI can also
be used in dental research, where the applications range from new drug
discovery to assistance in academic writing. In this review, we first define
GenAI models and describe their multiple generation modalities; then, we
explain and discuss their current and potential applications in Dentistry; and
finally, we describe the challenges these new technologies impose in our area.","[{'name': 'Fabián Villena'}, {'name': 'Claudia Véliz'}, {'name': 'Rosario García-Huidobro'}, {'name': 'Sebastián Aguayo'}]",2024-07-24T03:33:47Z
http://arxiv.org/abs/2407.16970v1,http://arxiv.org/abs/2407.16970v1,Towards Aligning Language Models with Textual Feedback,"We present ALT (ALignment with Textual feedback), an approach that aligns
language models with user preferences expressed in text. We argue that text
offers greater expressiveness, enabling users to provide richer feedback than
simple comparative preferences and this richer feedback can lead to more
efficient and effective alignment. ALT aligns the model by conditioning its
generation on the textual feedback. Our method relies solely on language
modeling techniques and requires minimal hyper-parameter tuning, though it
still presents the main benefits of RL-based alignment algorithms and can
effectively learn from textual feedback. We explore the efficacy and efficiency
of textual feedback across different tasks such as toxicity reduction,
summarization, and dialog response generation. We find that ALT outperforms PPO
for the task of toxicity reduction while being able to match its performance on
summarization with only 20% of the samples. We also explore how ALT can be used
with feedback provided by an existing LLM where we explore an LLM providing
constrained and unconstrained textual feedback. We also outline future
directions to align models with natural language feedback.","[{'name': 'Saüc Abadal Lloret'}, {'name': 'Shehzaad Dhuliawala'}, {'name': 'Keerthiram Murugesan'}, {'name': 'Mrinmaya Sachan'}]",2024-07-24T03:32:05Z
http://arxiv.org/abs/2407.16951v1,http://arxiv.org/abs/2407.16951v1,"Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias
  Mitigation","Large language models (LLMs) often inherit biases from vast amounts of
training corpora. Traditional debiasing methods, while effective to some
extent, do not completely eliminate memorized biases and toxicity in LLMs. In
this paper, we study an unlearning-based approach to debiasing in LLMs by
performing gradient ascent on hate speech against minority groups, i.e.,
minimizing the likelihood of biased or toxic content. Specifically, we propose
a mask language modeling unlearning technique, which unlearns the harmful part
of the text. This method enables LLMs to selectively forget and disassociate
from biased and harmful content. Experimental results demonstrate the
effectiveness of our approach in diminishing bias while maintaining the
language modeling abilities. Surprisingly, the results also unveil an
unexpected potential for cross-domain transfer unlearning: debiasing in one
bias form (e.g. gender) may contribute to mitigating others (e.g. race and
religion).","[{'name': 'Huimin Lu'}, {'name': 'Masaru Isonuma'}, {'name': 'Junichiro Mori'}, {'name': 'Ichiro Sakata'}]",2024-07-24T02:37:42Z
http://arxiv.org/abs/2407.16939v1,http://arxiv.org/abs/2407.16939v1,"Early screening of potential breakthrough technologies with enhanced
  interpretability: A patent-specific hierarchical attention network model","Despite the usefulness of machine learning approaches for the early screening
of potential breakthrough technologies, their practicality is often hindered by
opaque models. To address this, we propose an interpretable machine learning
approach to predicting future citation counts from patent texts using a
patent-specific hierarchical attention network (PatentHAN) model. Central to
this approach are (1) a patent-specific pre-trained language model, capturing
the meanings of technical words in patent claims, (2) a hierarchical network
structure, enabling detailed analysis at the claim level, and (3) a claim-wise
self-attention mechanism, revealing pivotal claims during the screening
process. A case study of 35,376 pharmaceutical patents demonstrates the
effectiveness of our approach in early screening of potential breakthrough
technologies while ensuring interpretability. Furthermore, we conduct
additional analyses using different language models and claim types to examine
the robustness of the approach. It is expected that the proposed approach will
enhance expert-machine collaboration in identifying breakthrough technologies,
providing new insight derived from text mining into technological value.","[{'name': 'Jaewoong Choi'}, {'name': 'Janghyeok Yoon'}, {'name': 'Changyong Lee'}]",2024-07-24T02:17:10Z
http://arxiv.org/abs/2407.16931v1,http://arxiv.org/abs/2407.16931v1,"ScholarChemQA: Unveiling the Power of Language Models in Chemical
  Research Question Answering","Question Answering (QA) effectively evaluates language models' reasoning and
knowledge depth. While QA datasets are plentiful in areas like general domain
and biomedicine, academic chemistry is less explored. Chemical QA plays a
crucial role in both education and research by effectively translating complex
chemical information into readily understandable format. Addressing this gap,
we introduce ScholarChemQA, a large-scale QA dataset constructed from chemical
papers. This dataset reflects typical real-world challenges, including an
imbalanced data distribution and a substantial amount of unlabeled data that
can be potentially useful. Correspondingly, we introduce a QAMatch model,
specifically designed to effectively answer chemical questions by fully
leveraging our collected data. We first address the issue of imbalanced label
distribution by re-weighting the instance-wise loss based on the inverse
frequency of each class, ensuring minority classes are not dominated by
majority ones during optimization. Next, we utilize the unlabeled data to
enrich the learning process, generating a variety of augmentations based on a
SoftMix operation and ensuring their predictions align with the same target,
i.e., pseudo-labels. To ensure the quality of the pseudo-labels, we propose a
calibration procedure aimed at closely aligning the pseudo-label estimates of
individual samples with a desired ground truth distribution. Experiments show
that our QAMatch significantly outperforms the recent similar-scale baselines
and Large Language Models (LLMs) not only on our ScholarChemQA dataset but also
on four benchmark datasets. We hope our benchmark and model can facilitate and
promote more research on chemical QA.","[{'name': 'Xiuying Chen'}, {'name': 'Tairan Wang'}, {'name': 'Taicheng Guo'}, {'name': 'Kehan Guo'}, {'name': 'Juexiao Zhou'}, {'name': 'Haoyang Li'}, {'name': 'Mingchen Zhuge'}, {'name': 'Jürgen Schmidhuber'}, {'name': 'Xin Gao'}, {'name': 'Xiangliang Zhang'}]",2024-07-24T01:46:55Z
http://arxiv.org/abs/2407.16920v1,http://arxiv.org/abs/2407.16920v1,"Train-Attention: Meta-Learning Where to Focus in Continual Knowledge
  Learning","Previous studies on continual knowledge learning (CKL) in large language
models (LLMs) have predominantly focused on approaches such as regularization,
architectural modifications, and rehearsal techniques to mitigate catastrophic
forgetting. However, these methods naively inherit the inefficiencies of
standard training procedures, indiscriminately applying uniform weight across
all tokens, which can lead to unnecessary parameter updates and increased
forgetting. To address these shortcomings, we propose a novel CKL approach
termed Train-Attention-Augmented Language Model (TAALM), which enhances
learning efficiency by dynamically predicting and applying weights to tokens
based on their usefulness. This method employs a meta-learning framework that
optimizes token importance predictions, facilitating targeted knowledge updates
and minimizing forgetting. Also, we observe that existing benchmarks do not
clearly exhibit the trade-off between learning and retaining, therefore we
propose a new benchmark, \textsc{LAMA-ckl}, to address this issue. Through
experiments conducted on both newly introduced and established CKL benchmarks,
TAALM proves the state-of-the-art performance upon the baselines, and also
shows synergistic compatibility when integrated with previous CKL approaches.","[{'name': 'Yeongbin Seo'}, {'name': 'Dongha Lee'}, {'name': 'Jinyoung Yeo'}]",2024-07-24T01:04:34Z
http://arxiv.org/abs/2407.16908v1,http://arxiv.org/abs/2407.16908v1,Generation Constraint Scaling Can Mitigate Hallucination,"Addressing the issue of hallucinations in large language models (LLMs) is a
critical challenge. As the cognitive mechanisms of hallucination have been
related to memory, here we explore hallucination for LLM that is enabled with
explicit memory mechanisms. We empirically demonstrate that by simply scaling
the readout vector that constrains generation in a memory-augmented LLM
decoder, hallucination mitigation can be achieved in a training-free manner.
Our method is geometry-inspired and outperforms a state-of-the-art LLM editing
method on the task of generation of Wikipedia-like biography entries both in
terms of generation quality and runtime complexity.","[{'name': 'Georgios Kollias'}, {'name': 'Payel Das'}, {'name': 'Subhajit Chaudhury'}]",2024-07-23T23:58:19Z
http://arxiv.org/abs/2407.16860v1,http://arxiv.org/abs/2407.16860v1,"$\textit{BenchIE}^{FL}$ : A Manually Re-Annotated Fact-Based Open
  Information Extraction Benchmark","Open Information Extraction (OIE) is a field of natural language processing
that aims to present textual information in a format that allows it to be
organized, analyzed and reflected upon. Numerous OIE systems are developed,
claiming ever-increasing performance, marking the need for objective
benchmarks. BenchIE is the latest reference we know of. Despite being very well
thought out, we noticed a number of issues we believe are limiting. Therefore,
we propose $\textit{BenchIE}^{FL}$, a new OIE benchmark which fully enforces
the principles of BenchIE while containing fewer errors, omissions and
shortcomings when candidate facts are matched towards reference ones.
$\textit{BenchIE}^{FL}$ allows insightful conclusions to be drawn on the actual
performance of OIE extractors.","[{'name': 'Fabrice Lamarche'}, {'name': 'Philippe Langlais'}]",2024-07-23T22:04:04Z
http://arxiv.org/abs/2407.16837v1,http://arxiv.org/abs/2407.16837v1,CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs,"The ability to compare objects, scenes, or situations is crucial for
effective decision-making and problem-solving in everyday life. For instance,
comparing the freshness of apples enables better choices during grocery
shopping, while comparing sofa designs helps optimize the aesthetics of our
living space. Despite its significance, the comparative capability is largely
unexplored in artificial general intelligence (AGI). In this paper, we
introduce CompBench, a benchmark designed to evaluate the comparative reasoning
capability of multimodal large language models (MLLMs). CompBench mines and
pairs images through visually oriented questions covering eight dimensions of
relative comparison: visual attribute, existence, state, emotion, temporality,
spatiality, quantity, and quality. We curate a collection of around 40K image
pairs using metadata from diverse vision datasets and CLIP similarity scores.
These image pairs span a broad array of visual domains, including animals,
fashion, sports, and both outdoor and indoor scenes. The questions are
carefully crafted to discern relative characteristics between two images and
are labeled by human annotators for accuracy and relevance. We use CompBench to
evaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our
results reveal notable shortcomings in their comparative abilities. We believe
CompBench not only sheds light on these limitations but also establishes a
solid foundation for future enhancements in the comparative capability of
MLLMs.","[{'name': 'Jihyung Kil'}, {'name': 'Zheda Mai'}, {'name': 'Justin Lee'}, {'name': 'Zihe Wang'}, {'name': 'Kerrie Cheng'}, {'name': 'Lemeng Wang'}, {'name': 'Ye Liu'}, {'name': 'Arpita Chowdhury'}, {'name': 'Wei-Lun Chao'}]",2024-07-23T21:02:38Z
http://arxiv.org/abs/2407.16833v1,http://arxiv.org/abs/2407.16833v1,"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
  Study and Hybrid Approach","Retrieval Augmented Generation (RAG) has been a powerful tool for Large
Language Models (LLMs) to efficiently process overly lengthy contexts. However,
recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to
understand long contexts directly. We conduct a comprehensive comparison
between RAG and long-context (LC) LLMs, aiming to leverage the strengths of
both. We benchmark RAG and LC across various public datasets using three latest
LLMs. Results reveal that when resourced sufficiently, LC consistently
outperforms RAG in terms of average performance. However, RAG's significantly
lower cost remains a distinct advantage. Based on this observation, we propose
Self-Route, a simple yet effective method that routes queries to RAG or LC
based on model self-reflection. Self-Route significantly reduces the
computation cost while maintaining a comparable performance to LC. Our findings
provide a guideline for long-context applications of LLMs using RAG and LC.","[{'name': 'Zhuowan Li'}, {'name': 'Cheng Li'}, {'name': 'Mingyang Zhang'}, {'name': 'Qiaozhu Mei'}, {'name': 'Michael Bendersky'}]",2024-07-23T20:51:52Z
http://arxiv.org/abs/2407.16772v1,http://arxiv.org/abs/2407.16772v1,VisMin: Visual Minimal-Change Understanding,"Fine-grained understanding of objects, attributes, and relationships between
objects is crucial for visual-language models (VLMs). Existing benchmarks
primarily focus on evaluating VLMs' capability to distinguish between two very
similar \textit{captions} given an image. In this paper, we introduce a new,
challenging benchmark termed \textbf{Vis}ual \textbf{Min}imal-Change
Understanding (VisMin), which requires models to predict the correct
image-caption match given two images and two captions. The image pair and
caption pair contain minimal changes, i.e., only one aspect changes at a time
from among the following: \textit{object}, \textit{attribute}, \textit{count},
and \textit{spatial relation}. These changes test the models' understanding of
objects, attributes (such as color, material, shape), counts, and spatial
relationships between objects. We built an automatic framework using large
language models and diffusion models, followed by a rigorous 4-step
verification process by human annotators. Empirical experiments reveal that
current VLMs exhibit notable deficiencies in understanding spatial
relationships and counting abilities. We also generate a large-scale training
dataset to finetune CLIP and Idefics2, showing significant improvements in
fine-grained understanding across benchmarks and in CLIP's general image-text
alignment. We release all resources, including the benchmark, training data,
and finetuned model checkpoints, at \url{https://vismin.net/}.","[{'name': 'Rabiul Awal'}, {'name': 'Saba Ahmadi'}, {'name': 'Le Zhang'}, {'name': 'Aishwarya Agrawal'}]",2024-07-23T18:10:43Z
http://arxiv.org/abs/2407.16695v1,http://arxiv.org/abs/2407.16695v1,"Stress-Testing Long-Context Language Models with Lifelong ICL and Task
  Haystack","We introduce Lifelong ICL, a problem setting that challenges long-context
language models (LMs) to learn from a sequence of language tasks through
in-context learning (ICL). We further introduce Task Haystack, an evaluation
suite dedicated to assessing and diagnosing how long-context LMs utilizes
contexts in Lifelong ICL. When given a task instruction and test inputs,
long-context LMs are expected to leverage the relevant demonstrations in the
Lifelong ICL prompt, avoid distraction and interference from other tasks, and
achieve test accuracies that are not significantly worse than the Single-task
ICL baseline.
  Task Haystack draws inspiration from the widely-adopted
""needle-in-a-haystack"" (NIAH) evaluation, but presents new and unique
challenges. It demands that models (1) utilize the contexts with deeper
understanding, rather than resorting to simple copying and pasting; (2)
navigate through long streams of evolving topics and tasks, which closely
approximates the complexities of real-world usage of long-context LMs.
Additionally, Task Haystack inherits the controllability aspect of NIAH,
providing model developers with tools and visualizations to identify model
vulnerabilities effectively.
  We benchmark 12 long-context LMs using Task Haystack. We find that
state-of-the-art closed models such as GPT-4o still struggle in this setting,
failing 15% of the cases on average, while all open-weight models we evaluate
further lack behind by a large margin, failing up to 61% of the cases. In our
controlled analysis, we identify factors such as distraction and recency bias
as contributors to these failure cases. Further, we observe declines in
performance when task instructions are paraphrased at test time or when ICL
demonstrations are repeated excessively, raising concerns about the robustness,
instruction understanding, and true context utilization of current long-context
LMs.","[{'name': 'Xiaoyue Xu'}, {'name': 'Qinyuan Ye'}, {'name': 'Xiang Ren'}]",2024-07-23T17:57:41Z
http://arxiv.org/abs/2407.16693v1,http://arxiv.org/abs/2407.16693v1,Explanation Regularisation through the Lens of Attributions,"Explanation regularisation (ER) has been introduced as a way to guide models
to make their predictions in a manner more akin to humans, i.e., making their
attributions ""plausible"". This is achieved by introducing an auxiliary
explanation loss, that measures how well the output of an input attribution
technique for the model agrees with relevant human-annotated rationales. One
positive outcome of using ER appears to be improved performance in
out-of-domain (OOD) settings, presumably due to an increased reliance on
""plausible"" tokens. However, previous work has under-explored the impact of the
ER objective on model attributions, in particular when obtained with techniques
other than the one used to train ER. In this work, we contribute a study of
ER's effectiveness at informing classification decisions on plausible tokens,
and the relationship between increased plausibility and robustness to OOD
conditions. Through a series of analyses, we find that the connection between
ER and the ability of a classifier to rely on plausible features has been
overstated and that a stronger reliance on plausible tokens does not seem to be
the cause for any perceived OOD improvements.","[{'name': 'Pedro Ferreira'}, {'name': 'Wilker Aziz'}, {'name': 'Ivan Titov'}]",2024-07-23T17:56:32Z
http://arxiv.org/abs/2407.16686v1,http://arxiv.org/abs/2407.16686v1,Can Large Language Models Automatically Jailbreak GPT-4V?,"GPT-4V has attracted considerable attention due to its extraordinary capacity
for integrating and processing multimodal information. At the same time, its
ability of face recognition raises new safety concerns of privacy leakage.
Despite researchers' efforts in safety alignment through RLHF or preprocessing
filters, vulnerabilities might still be exploited. In our study, we introduce
AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt
optimization. We leverage Large Language Models (LLMs) for red-teaming to
refine the jailbreak prompt and employ weak-to-strong in-context learning
prompts to boost efficiency. Furthermore, we present an effective search method
that incorporates early stopping to minimize optimization time and token
expenditure. Our experiments demonstrate that AutoJailbreak significantly
surpasses conventional methods, achieving an Attack Success Rate (ASR)
exceeding 95.3\%. This research sheds light on strengthening GPT-4V security,
underscoring the potential for LLMs to be exploited in compromising GPT-4V
integrity.","[{'name': 'Yuanwei Wu'}, {'name': 'Yue Huang'}, {'name': 'Yixin Liu'}, {'name': 'Xiang Li'}, {'name': 'Pan Zhou'}, {'name': 'Lichao Sun'}]",2024-07-23T17:50:45Z
http://arxiv.org/abs/2407.16741v1,http://arxiv.org/abs/2407.16741v1,"OpenDevin: An Open Platform for AI Software Developers as Generalist
  Agents","Software is one of the most powerful tools that we humans have at our
disposal; it allows a skilled programmer to interact with the world in complex
and profound ways. At the same time, thanks to improvements in large language
models (LLMs), there has also been a rapid development in AI agents that
interact with and affect change in their surrounding environments. In this
paper, we introduce OpenDevin, a platform for the development of powerful and
flexible AI agents that interact with the world in similar ways to those of a
human developer: by writing code, interacting with a command line, and browsing
the web. We describe how the platform allows for the implementation of new
agents, safe interaction with sandboxed environments for code execution,
coordination between multiple agents, and incorporation of evaluation
benchmarks. Based on our currently incorporated benchmarks, we perform an
evaluation of agents over 15 challenging tasks, including software engineering
(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released
under the permissive MIT license, OpenDevin is a community project spanning
academia and industry with more than 1.3K contributions from over 160
contributors and will improve going forward.","[{'name': 'Xingyao Wang'}, {'name': 'Boxuan Li'}, {'name': 'Yufan Song'}, {'name': 'Frank F. Xu'}, {'name': 'Xiangru Tang'}, {'name': 'Mingchen Zhuge'}, {'name': 'Jiayi Pan'}, {'name': 'Yueqi Song'}, {'name': 'Bowen Li'}, {'name': 'Jaskirat Singh'}, {'name': 'Hoang H. Tran'}, {'name': 'Fuqiang Li'}, {'name': 'Ren Ma'}, {'name': 'Mingzhang Zheng'}, {'name': 'Bill Qian'}, {'name': 'Yanjun Shao'}, {'name': 'Niklas Muennighoff'}, {'name': 'Yizhe Zhang'}, {'name': 'Binyuan Hui'}, {'name': 'Junyang Lin'}, {'name': 'Robert Brennan'}, {'name': 'Hao Peng'}, {'name': 'Heng Ji'}, {'name': 'Graham Neubig'}]",2024-07-23T17:50:43Z
http://arxiv.org/abs/2407.16667v1,http://arxiv.org/abs/2407.16667v1,"RedAgent: Red Teaming Large Language Models with Context-aware
  Autonomous Language Agent","Recently, advanced Large Language Models (LLMs) such as GPT-4 have been
integrated into many real-world applications like Code Copilot. These
applications have significantly expanded the attack surface of LLMs, exposing
them to a variety of threats. Among them, jailbreak attacks that induce toxic
responses through jailbreak prompts have raised critical safety concerns. To
identify these threats, a growing number of red teaming approaches simulate
potential adversarial scenarios by crafting jailbreak prompts to test the
target LLM. However, existing red teaming methods do not consider the unique
vulnerabilities of LLM in different scenarios, making it difficult to adjust
the jailbreak prompts to find context-specific vulnerabilities. Meanwhile,
these methods are limited to refining jailbreak templates using a few mutation
operations, lacking the automation and scalability to adapt to different
scenarios. To enable context-aware and efficient red teaming, we abstract and
model existing attacks into a coherent concept called ""jailbreak strategy"" and
propose a multi-agent LLM system named RedAgent that leverages these strategies
to generate context-aware jailbreak prompts. By self-reflecting on contextual
feedback in an additional memory buffer, RedAgent continuously learns how to
leverage these strategies to achieve effective jailbreaks in specific contexts.
Extensive experiments demonstrate that our system can jailbreak most black-box
LLMs in just five queries, improving the efficiency of existing red teaming
methods by two times. Additionally, RedAgent can jailbreak customized LLM
applications more efficiently. By generating context-aware jailbreak prompts
towards applications on GPTs, we discover 60 severe vulnerabilities of these
real-world applications with only two queries per vulnerability. We have
reported all found issues and communicated with OpenAI and Meta for bug fixes.","[{'name': 'Huiyu Xu'}, {'name': 'Wenhui Zhang'}, {'name': 'Zhibo Wang'}, {'name': 'Feng Xiao'}, {'name': 'Rui Zheng'}, {'name': 'Yunhe Feng'}, {'name': 'Zhongjie Ba'}, {'name': 'Kui Ren'}]",2024-07-23T17:34:36Z
http://arxiv.org/abs/2407.16664v1,http://arxiv.org/abs/2407.16664v1,Towards scalable efficient on-device ASR with transfer learning,"Multilingual pretraining for transfer learning significantly boosts the
robustness of low-resource monolingual ASR models. This study systematically
investigates three main aspects: (a) the impact of transfer learning on model
performance during initial training or fine-tuning, (b) the influence of
transfer learning across dataset domains and languages, and (c) the effect on
rare-word recognition compared to non-rare words. Our finding suggests that
RNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word
Error Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across
languages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8%
compared to monolingual baselines for MLS and in-house datasets. Out-of-domain
pretraining leads to 28% higher WERR than in-domain pretraining. Both rare and
non-rare words benefit, with rare words showing greater improvements with
out-of-domain pretraining, and non-rare words with in-domain pretraining.","[{'name': 'Laxmi Pandey'}, {'name': 'Ke Li'}, {'name': 'Jinxi Guo'}, {'name': 'Debjyoti Paul'}, {'name': 'Arthur Guo'}, {'name': 'Jay Mahadeokar'}, {'name': 'Xuedong Zhang'}]",2024-07-23T17:29:02Z
http://arxiv.org/abs/2407.16737v1,http://arxiv.org/abs/2407.16737v1,A Survey of Text Style Transfer: Applications and Ethical Implications,"Text style transfer (TST) is an important task in controllable text
generation, which aims to control selected attributes of language use, such as
politeness, formality, or sentiment, without altering the style-independent
content of the text. The field has received considerable research attention in
recent years and has already been covered in several reviews, but the focus has
mostly been on the development of new algorithms and learning from different
types of data (supervised, unsupervised, out-of-domain, etc.) and not so much
on the application side. However, TST-related technologies are gradually
reaching a production- and deployment-ready level, and therefore, the inclusion
of the application perspective in TST research becomes crucial. Similarly, the
often overlooked ethical considerations of TST technology have become a
pressing issue. This paper presents a comprehensive review of TST applications
that have been researched over the years, using both traditional linguistic
approaches and more recent deep learning methods. We discuss current
challenges, future research directions, and ethical implications of TST
applications in text generation. By providing a holistic overview of the
landscape of TST applications, we hope to stimulate further research and
contribute to a better understanding of the potential as well as ethical
considerations associated with TST.","[{'name': 'Sourabrata Mukherjee'}, {'name': 'Mateusz Lango'}, {'name': 'Zdenek Kasner'}, {'name': 'Ondrej Dušek'}]",2024-07-23T17:15:23Z
http://arxiv.org/abs/2407.16637v1,http://arxiv.org/abs/2407.16637v1,Course-Correction: Safety Alignment Using Synthetic Preferences,"The risk of harmful content generated by large language models (LLMs) becomes
a critical concern. This paper presents a systematic study on assessing and
improving LLMs' capability to perform the task of \textbf{course-correction},
\ie, the model can steer away from generating harmful content autonomously. To
start with, we introduce the \textsc{C$^2$-Eval} benchmark for quantitative
assessment and analyze 10 popular LLMs, revealing varying proficiency of
current safety-tuned LLMs in course-correction. To improve, we propose
fine-tuning LLMs with preference learning, emphasizing the preference for
timely course-correction. Using an automated pipeline, we create
\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to
teach models the concept of timely course-correction through data-driven
preference learning. Experiments on 2 LLMs, \textsc{Llama2-Chat 7B} and
\textsc{Qwen2 7B}, show that our method effectively enhances course-correction
skills without affecting general performance. Additionally, it effectively
improves LLMs' safety, particularly in resisting jailbreak attacks.","[{'name': 'Rongwu Xu'}, {'name': 'Yishuo Cai'}, {'name': 'Zhenhong Zhou'}, {'name': 'Renjie Gu'}, {'name': 'Haiqin Weng'}, {'name': 'Yan Liu'}, {'name': 'Tianwei Zhang'}, {'name': 'Wei Xu'}, {'name': 'Han Qiu'}]",2024-07-23T16:54:28Z
http://arxiv.org/abs/2407.16624v1,http://arxiv.org/abs/2407.16624v1,Semantic Change Characterization with LLMs using Rhetorics,"Languages continually evolve in response to societal events, resulting in new
terms and shifts in meanings. These changes have significant implications for
computer applications, including automatic translation and chatbots, making it
essential to characterize them accurately. The recent development of LLMs has
notably advanced natural language understanding, particularly in sense
inference and reasoning. In this paper, we investigate the potential of LLMs in
characterizing three types of semantic change: dimension, relation, and
orientation. We achieve this by combining LLMs' Chain-of-Thought with
rhetorical devices and conducting an experimental assessment of our approach
using newly created datasets. Our results highlight the effectiveness of LLMs
in capturing and analyzing semantic changes, providing valuable insights to
improve computational linguistic applications.","[{'name': 'Jader Martins Camboim de Sá'}, {'name': 'Marcos Da Silveira'}, {'name': 'Cédric Pruski'}]",2024-07-23T16:32:49Z
http://arxiv.org/abs/2407.16615v1,http://arxiv.org/abs/2407.16615v1,Lawma: The Power of Specialization for Legal Tasks,"Annotation and classification of legal text are central components of
empirical legal research. Traditionally, these tasks are often delegated to
trained research assistants. Motivated by the advances in language modeling,
empirical legal scholars are increasingly turning to prompting commercial
models, hoping that it will alleviate the significant cost of human annotation.
Despite growing use, our understanding of how to best utilize large language
models for legal tasks remains limited. We conduct a comprehensive study of 260
legal text classification tasks, nearly all new to the machine learning
community. Starting from GPT-4 as a baseline, we show that it has non-trivial
but highly varied zero-shot accuracy, often exhibiting performance that may be
insufficient for legal work. We then demonstrate that a lightly fine-tuned
Llama 3 model vastly outperforms GPT-4 on almost all tasks, typically by
double-digit percentage points. We find that larger models respond better to
fine-tuning than smaller models. A few tens to hundreds of examples suffice to
achieve high classification accuracy. Notably, we can fine-tune a single model
on all 260 tasks simultaneously at a small loss in accuracy relative to having
a separate model for each task. Our work points to a viable alternative to the
predominant practice of prompting commercial models. For concrete legal tasks
with some available labeled data, researchers are better off using a fine-tuned
open-source model.","[{'name': 'Ricardo Dominguez-Olmedo'}, {'name': 'Vedant Nanda'}, {'name': 'Rediet Abebe'}, {'name': 'Stefan Bechtold'}, {'name': 'Christoph Engel'}, {'name': 'Jens Frankenreiter'}, {'name': 'Krishna Gummadi'}, {'name': 'Moritz Hardt'}, {'name': 'Michael Livermore'}]",2024-07-23T16:23:04Z
http://arxiv.org/abs/2407.16607v2,http://arxiv.org/abs/2407.16607v2,"Data Mixture Inference: What do BPE Tokenizers Reveal about their
  Training Data?","The pretraining data of today's strongest language models is opaque; in
particular, little is known about the proportions of various domains or
languages represented. In this work, we tackle a task which we call data
mixture inference, which aims to uncover the distributional make-up of training
data. We introduce a novel attack based on a previously overlooked source of
information -- byte-pair encoding (BPE) tokenizers, used by the vast majority
of modern language models. Our key insight is that the ordered list of merge
rules learned by a BPE tokenizer naturally reveals information about the token
frequencies in its training data: the first merge is the most common byte pair,
the second is the most common pair after merging the first token, and so on.
Given a tokenizer's merge list along with data samples for each category of
interest, we formulate a linear program that solves for the proportion of each
category in the tokenizer's training set. Importantly, to the extent to which
tokenizer training data is representative of the pretraining data, we
indirectly learn about pretraining data. In controlled experiments, we show
that our attack recovers mixture ratios with high precision for tokenizers
trained on known mixtures of natural languages, programming languages, and data
sources. We then apply our approach to off-the-shelf tokenizers released with
recent LMs. We confirm much publicly disclosed information about these models,
and also make several new inferences: GPT-4o's tokenizer is much more
multilingual than its predecessors, training on 39% non-English data; Llama3
extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and
Claude's tokenizers are trained on predominantly code (~60%). We hope our work
sheds light on current design practices for pretraining data, and inspires
continued research into data mixture inference for LMs.","[{'name': 'Jonathan Hayase'}, {'name': 'Alisa Liu'}, {'name': 'Yejin Choi'}, {'name': 'Sewoong Oh'}, {'name': 'Noah A. Smith'}]",2024-07-23T16:13:22Z
http://arxiv.org/abs/2407.16604v1,http://arxiv.org/abs/2407.16604v1,Shared Imagination: LLMs Hallucinate Alike,"Despite the recent proliferation of large language models (LLMs), their
training recipes -- model architecture, pre-training data and optimization
algorithm -- are often very similar. This naturally raises the question of the
similarity among the resulting models. In this paper, we propose a novel
setting, imaginary question answering (IQA), to better understand model
similarity. In IQA, we ask one model to generate purely imaginary questions
(e.g., on completely made-up concepts in physics) and prompt another model to
answer. Surprisingly, despite the total fictionality of these questions, all
models can answer each other's questions with remarkable success, suggesting a
""shared imagination space"" in which these models operate during such
hallucinations. We conduct a series of investigations into this phenomenon and
discuss implications on model homogeneity, hallucination, and computational
creativity.","[{'name': 'Yilun Zhou'}, {'name': 'Caiming Xiong'}, {'name': 'Silvio Savarese'}, {'name': 'Chien-Sheng Wu'}]",2024-07-23T16:06:22Z
http://arxiv.org/abs/2407.16593v1,http://arxiv.org/abs/2407.16593v1,"A Comparative Study on Patient Language across Therapeutic Domains for
  Effective Patient Voice Classification in Online Health Discussions","There exists an invisible barrier between healthcare professionals'
perception of a patient's clinical experience and the reality. This barrier may
be induced by the environment that hinders patients from sharing their
experiences openly with healthcare professionals. As patients are observed to
discuss and exchange knowledge more candidly on social media, valuable insights
can be leveraged from these platforms. However, the abundance of non-patient
posts on social media necessitates filtering out such irrelevant content to
distinguish the genuine voices of patients, a task we refer to as patient voice
classification. In this study, we analyse the importance of linguistic
characteristics in accurately classifying patient voices. Our findings
underscore the essential role of linguistic and statistical text similarity
analysis in identifying common patterns among patient groups. These results
allude to even starker differences in the way patients express themselves at a
disease level and across various therapeutic domains. Additionally, we
fine-tuned a pre-trained Language Model on the combined datasets with similar
linguistic patterns, resulting in a highly accurate automatic patient voice
classification. Being the pioneering study on the topic, our focus on
extracting authentic patient experiences from social media stands as a crucial
step towards advancing healthcare standards and fostering a patient-centric
approach.","[{'name': 'Giorgos Lysandrou'}, {'name': 'Roma English Owen'}, {'name': 'Vanja Popovic'}, {'name': 'Grant Le Brun'}, {'name': 'Aryo Pradipta Gema'}, {'name': 'Beatrice Alex'}, {'name': 'Elizabeth A. L. Fairley'}]",2024-07-23T15:51:46Z
http://arxiv.org/abs/2407.16574v1,http://arxiv.org/abs/2407.16574v1,"TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement
  Learning from Human Feedback","Reinforcement Learning from Human Feedback (RLHF) leverages human preference
data to train language models to align more closely with human essence. These
human preference data, however, are labeled at the sequence level, creating a
mismatch between sequence-level preference labels and tokens, which are
autoregressively generated from the language model. Although several recent
approaches have tried to provide token-level (i.e., dense) rewards for each
individual token, these typically rely on predefined discrete reward values
(e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying
degrees of preference inherent to each token. To address this limitation, we
introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a
discriminator trained to distinguish positive and negative tokens, and the
confidence of the discriminator is used to assign continuous rewards to each
token considering the context. Extensive experiments show that our proposed
TLCR leads to consistent performance improvements over previous sequence-level
or token-level discrete rewards on open-ended generation benchmarks.","[{'name': 'Eunseop Yoon'}, {'name': 'Hee Suk Yoon'}, {'name': 'SooHwan Eom'}, {'name': 'Gunsoo Han'}, {'name': 'Daniel Wontae Nam'}, {'name': 'Daejin Jo'}, {'name': 'Kyoung-Woon On'}, {'name': 'Mark A. Hasegawa-Johnson'}, {'name': 'Sungwoong Kim'}, {'name': 'Chang D. Yoo'}]",2024-07-23T15:27:37Z
http://arxiv.org/abs/2407.16565v1,http://arxiv.org/abs/2407.16565v1,"Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases
  Generation with Small Language Models","Recent surge in the accessibility of large language models (LLMs) to the
general population can lead to untrackable use of such models for
medical-related recommendations. Language generation via LLMs models has two
key problems: firstly, they are prone to hallucination and therefore, for any
medical purpose they require scientific and factual grounding; secondly, LLMs
pose tremendous challenge to computational resources due to their gigantic
model size. In this work, we introduce pRAGe, a pipeline for Retrieval
Augmented Generation and evaluation of medical paraphrases generation using
Small Language Models (SLM). We study the effectiveness of SLMs and the impact
of external knowledge base for medical paraphrase generation in French.","[{'name': 'Ioana Buhnila'}, {'name': 'Aman Sinha'}, {'name': 'Mathieu Constant'}]",2024-07-23T15:17:11Z
http://arxiv.org/abs/2407.16537v1,http://arxiv.org/abs/2407.16537v1,"Quantifying the Role of Textual Predictability in Automatic Speech
  Recognition","A long-standing question in automatic speech recognition research is how to
attribute errors to the ability of a model to model the acoustics, versus its
ability to leverage higher-order context (lexicon, morphology, syntax,
semantics). We validate a novel approach which models error rates as a function
of relative textual predictability, and yields a single number, $k$, which
measures the effect of textual predictability on the recognizer. We use this
method to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use
of textual context than a hybrid ASR model, in spite of not using an explicit
language model, and also use it to shed light on recent results demonstrating
poor performance of standard ASR systems on African-American English. We
demonstrate that these mostly represent failures of acoustic--phonetic
modelling. We show how this approach can be used straightforwardly in
diagnosing and improving ASR.","[{'name': 'Sean Robertson'}, {'name': 'Gerald Penn'}, {'name': 'Ewan Dunbar'}]",2024-07-23T14:47:25Z
http://arxiv.org/abs/2407.16526v1,http://arxiv.org/abs/2407.16526v1,"Imperfect Vision Encoders: Efficient and Robust Tuning for
  Vision-Language Models","Vision language models (VLMs) demonstrate impressive capabilities in visual
question answering and image captioning, acting as a crucial link between
visual and language models. However, existing open-source VLMs heavily rely on
pretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness
across diverse domains, it still exhibits non-negligible image understanding
errors. These errors propagate to the VLM responses, resulting in sub-optimal
performance. In our work, we propose an efficient and robust method for
updating vision encoders within VLMs. Our approach selectively and locally
updates encoders, leading to substantial performance improvements on data where
previous mistakes occurred, while maintaining overall robustness. Furthermore,
we demonstrate the effectiveness of our method during continual few-shot
updates. Theoretical grounding, generality, and computational efficiency
characterize our approach.","[{'name': 'Aristeidis Panos'}, {'name': 'Rahaf Aljundi'}, {'name': 'Daniel Olmeda Reino'}, {'name': 'Richard E Turner'}]",2024-07-23T14:39:40Z
http://arxiv.org/abs/2407.16521v2,http://arxiv.org/abs/2407.16521v2,"AMONGAGENTS: Evaluating Large Language Models in the Interactive
  Text-Based Social Deduction Game","Strategic social deduction games serve as valuable testbeds for evaluating
the understanding and inference skills of language models, offering crucial
insights into social science, artificial intelligence, and strategic gaming.
This paper focuses on creating proxies of human behavior in simulated
environments, with Among Us utilized as a tool for studying simulated human
behavior. The study introduces a text-based game environment, named
AmongAgents, that mirrors the dynamics of Among Us. Players act as crew members
aboard a spaceship, tasked with identifying impostors who are sabotaging the
ship and eliminating the crew. Within this environment, the behavior of
simulated language agents is analyzed. The experiments involve diverse game
sequences featuring different configurations of Crewmates and Impostor
personality archetypes. Our work demonstrates that state-of-the-art large
language models (LLMs) can effectively grasp the game rules and make decisions
based on the current context. This work aims to promote further exploration of
LLMs in goal-oriented games with incomplete information and complex action
spaces, as these settings offer valuable opportunities to assess language model
performance in socially driven scenarios.","[{'name': 'Yizhou Chi'}, {'name': 'Lingjun Mao'}, {'name': 'Zineng Tang'}]",2024-07-23T14:34:38Z
http://arxiv.org/abs/2407.16516v1,http://arxiv.org/abs/2407.16516v1,"Assessing In-context Learning and Fine-tuning for Topic Classification
  of German Web Data","Researchers in the political and social sciences often rely on classification
models to analyze trends in information consumption by examining browsing
histories of millions of webpages. Automated scalable methods are necessary due
to the impracticality of manual labeling. In this paper, we model the detection
of topic-related content as a binary classification task and compare the
accuracy of fine-tuned pre-trained encoder models against in-context learning
strategies. Using only a few hundred annotated data points per topic, we detect
content related to three German policies in a database of scraped webpages. We
compare multilingual and monolingual models, as well as zero and few-shot
approaches, and investigate the impact of negative sampling strategies and the
combination of URL & content-based features. Our results show that a small
sample of annotated data is sufficient to train an effective classifier.
Fine-tuning encoder-based models yields better results than in-context
learning. Classifiers using both URL & content-based features perform best,
while using URLs alone provides adequate results when content is unavailable.","[{'name': 'Julian Schelb'}, {'name': 'Roberto Ulloa'}, {'name': 'Andreas Spitz'}]",2024-07-23T14:31:59Z
http://arxiv.org/abs/2407.16470v2,http://arxiv.org/abs/2407.16470v2,"Machine Translation Hallucination Detection for Low and High Resource
  Languages using Large Language Models","Recent advancements in massively multilingual machine translation systems
have significantly enhanced translation accuracy; however, even the best
performing systems still generate hallucinations, severely impacting user
trust. Detecting hallucinations in Machine Translation (MT) remains a critical
challenge, particularly since existing methods excel with High-Resource
Languages (HRLs) but exhibit substantial limitations when applied to
Low-Resource Languages (LRLs). This paper evaluates hallucination detection
approaches using Large Language Models (LLMs) and semantic similarity within
massively multilingual embeddings. Our study spans 16 language directions,
covering HRLs, LRLs, with diverse scripts. We find that the choice of model is
essential for performance. On average, for HRLs, Llama3-70B outperforms the
previous state of the art by as much as 0.16 MCC (Matthews Correlation
Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other
LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can
achieve performance comparable or even better than previously proposed models,
despite not being explicitly trained for any machine translation task. However,
their advantage is less significant for LRLs.","[{'name': 'Kenza Benkirane'}, {'name': 'Laura Gongas'}, {'name': 'Shahar Pelles'}, {'name': 'Naomi Fuchs'}, {'name': 'Joshua Darmon'}, {'name': 'Pontus Stenetorp'}, {'name': 'David Ifeoluwa Adelani'}, {'name': 'Eduardo Sánchez'}]",2024-07-23T13:40:54Z
http://arxiv.org/abs/2407.16444v1,http://arxiv.org/abs/2407.16444v1,"Psychomatics -- A Multidisciplinary Framework for Understanding
  Artificial Minds","Although LLMs and other artificial intelligence systems demonstrate cognitive
skills similar to humans, like concept learning and language acquisition, the
way they process information fundamentally differs from biological cognition.
To better understand these differences this paper introduces Psychomatics, a
multidisciplinary framework bridging cognitive science, linguistics, and
computer science. It aims to better understand the high-level functioning of
LLMs, focusing specifically on how LLMs acquire, learn, remember, and use
information to produce their outputs. To achieve this goal, Psychomatics will
rely on a comparative methodology, starting from a theory-driven research
question - is the process of language development and use different in humans
and LLMs? - drawing parallels between LLMs and biological systems. Our analysis
shows how LLMs can map and manipulate complex linguistic patterns in their
training data. Moreover, LLMs can follow Grice's Cooperative Principle to
provide relevant and informative responses. However, human cognition draws from
multiple sources of meaning, including experiential, emotional, and imaginative
facets, which transcend mere language processing and are rooted in our social
and developmental trajectories. Moreover, current LLMs lack physical
embodiment, reducing their ability to make sense of the intricate interplay
between perception, action, and cognition that shapes human understanding and
expression. Ultimately, Psychomatics holds the potential to yield
transformative insights into the nature of language, cognition, and
intelligence, both artificial and biological. Moreover, by drawing parallels
between LLMs and human cognitive processes, Psychomatics can inform the
development of more robust and human-like AI systems.","[{'name': 'Giuseppe Riva'}, {'name': 'Fabrizia Mantovani'}, {'name': 'Brenda K. Wiederhold'}, {'name': 'Antonella Marchetti'}, {'name': 'Andrea Gaggioli'}]",2024-07-23T12:53:41Z
http://arxiv.org/abs/2407.16724v1,http://arxiv.org/abs/2407.16724v1,"Educating LLMs like Human Students: Structure-aware Injection of Domain
  Knowledge","This paper presents a pioneering methodology, termed StructTuning, to
efficiently transform foundation Large Language Models (LLMs) into domain
specialists. It significantly minimizes the training corpus requirement to a
mere 0.3% while achieving an impressive 50% of traditional knowledge injection
performance. Our method is inspired by the educational processes for human
students, particularly how structured domain knowledge from textbooks is
absorbed and then applied to tackle real-world challenges through specific
exercises. Based on this, we propose a novel two-stage knowledge injection
strategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware
Supervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data
into an auto-generated taxonomy of domain knowledge, enabling LLMs to
effectively memorize textual segments linked to specific expertise within the
taxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt
models to reveal the underlying knowledge structure in their outputs,
leveraging this structured domain insight to address practical problems
adeptly. Our ultimate method has undergone extensive evaluations across model
architectures and scales, using closed-book question-answering tasks on
LongBench and MMedBench datasets. Remarkably, our method matches 50% of the
improvement displayed by the state-of-the-art MMedLM2 on MMedBench, but with
only 0.3% quantity of the training corpus. This breakthrough showcases the
potential to scale up our StructTuning for stronger domain-specific LLMs. Code
will be made public soon.","[{'name': 'Kai Liu'}, {'name': 'Ze Chen'}, {'name': 'Zhihang Fu'}, {'name': 'Rongxin Jiang'}, {'name': 'Fan Zhou'}, {'name': 'Yaowu Chen'}, {'name': 'Yue Wu'}, {'name': 'Jieping Ye'}]",2024-07-23T12:38:48Z
http://arxiv.org/abs/2407.16434v1,http://arxiv.org/abs/2407.16434v1,Enhancing LLM's Cognition via Structurization,"When reading long-form text, human cognition is complex and structurized.
While large language models (LLMs) process input contexts through a causal and
sequential perspective, this approach can potentially limit their ability to
handle intricate and complex inputs effectively. To enhance LLM's cognition
capability, this paper presents a novel concept of context structurization.
Specifically, we transform the plain, unordered contextual sentences into
well-ordered and hierarchically structurized elements. By doing so, LLMs can
better grasp intricate and extended contexts through precise attention and
information-seeking along the organized structures. Extensive evaluations are
conducted across various model architectures and sizes (including several 7B-
to 72B-size auto-regressive LLMs as well as BERT-like masking models) on a
diverse set of NLP tasks (e.g., context-based question-answering, exhaustive
hallucination evaluation, and passage-level dense retrieval). Empirical results
show consistent and significant performance gains afforded by a single-round
structurization. In particular, we boost a 72B-parameter open-source model to
achieve comparable performance against GPT-3.5-Turbo as the hallucination
evaluator. Besides, we show the feasibility of distilling advanced LLMs'
language processing abilities to a smaller yet effective StruXGPT-7B to execute
structurization, addressing the practicality of our approach. Code will be made
public soon.","[{'name': 'Kai Liu'}, {'name': 'Zhihang Fu'}, {'name': 'Chao Chen'}, {'name': 'Wei Zhang'}, {'name': 'Rongxin Jiang'}, {'name': 'Fan Zhou'}, {'name': 'Yaowu Chen'}, {'name': 'Yue Wu'}, {'name': 'Jieping Ye'}]",2024-07-23T12:33:58Z
http://arxiv.org/abs/2407.16431v1,http://arxiv.org/abs/2407.16431v1,"FairFlow: An Automated Approach to Model-based Counterfactual Data
  Augmentation For NLP","Despite the evolution of language models, they continue to portray harmful
societal biases and stereotypes inadvertently learned from training data. These
inherent biases often result in detrimental effects in various applications.
Counterfactual Data Augmentation (CDA), which seeks to balance demographic
attributes in training data, has been a widely adopted approach to mitigate
bias in natural language processing. However, many existing CDA approaches rely
on word substitution techniques using manually compiled word-pair dictionaries.
These techniques often lead to out-of-context substitutions, resulting in
potential quality issues. The advancement of model-based techniques, on the
other hand, has been challenged by the need for parallel training data. Works
in this area resort to manually generated parallel data that are expensive to
collect and are consequently limited in scale. This paper proposes FairFlow, an
automated approach to generating parallel data for training counterfactual text
generator models that limits the need for human intervention. Furthermore, we
show that FairFlow significantly overcomes the limitations of dictionary-based
word-substitution approaches whilst maintaining good performance.","[{'name': 'Ewoenam Kwaku Tokpo'}, {'name': 'Toon Calders'}]",2024-07-23T12:29:37Z
http://arxiv.org/abs/2407.21053v1,http://arxiv.org/abs/2407.21053v1,"Knowledge Models for Cancer Clinical Practice Guidelines : Construction,
  Management and Usage in Question Answering","An automated knowledge modeling algorithm for Cancer Clinical Practice
Guidelines (CPGs) extracts the knowledge contained in the CPG documents and
transforms it into a programmatically interactable, easy-to-update structured
model with minimal human intervention. The existing automated algorithms have
minimal scope and cannot handle the varying complexity of the knowledge content
in the CPGs for different cancer types. This work proposes an improved
automated knowledge modeling algorithm to create knowledge models from the
National Comprehensive Cancer Network (NCCN) CPGs in Oncology for different
cancer types. The proposed algorithm has been evaluated with NCCN CPGs for four
different cancer types. We also proposed an algorithm to compare the knowledge
models for different versions of a guideline to discover the specific changes
introduced in the treatment protocol of a new version. We created a
question-answering (Q&A) framework with the guideline knowledge models as the
augmented knowledge base to study our ability to query the knowledge models. We
compiled a set of 32 question-answer pairs derived from two reliable data
sources for the treatment of Non-Small Cell Lung Cancer (NSCLC) to evaluate the
Q&A framework. The framework was evaluated against the question-answer pairs
from one data source, and it can generate the answers with 54.5% accuracy from
the treatment algorithm and 81.8% accuracy from the discussion part of the NCCN
NSCLC guideline knowledge model.","[{'name': 'Pralaypati Ta'}, {'name': 'Bhumika Gupta'}, {'name': 'Arihant Jain'}, {'name': 'Sneha Sree C'}, {'name': 'Keerthi Ram'}, {'name': 'Mohanasankar Sivaprakasam'}]",2024-07-23T11:26:40Z
http://arxiv.org/abs/2407.16382v1,http://arxiv.org/abs/2407.16382v1,TookaBERT: A Step Forward for Persian NLU,"The field of natural language processing (NLP) has seen remarkable
advancements, thanks to the power of deep learning and foundation models.
Language models, and specifically BERT, have been key players in this progress.
In this study, we trained and introduced two new BERT models using Persian
data. We put our models to the test, comparing them to seven existing models
across 14 diverse Persian natural language understanding (NLU) tasks. The
results speak for themselves: our larger model outperforms the competition,
showing an average improvement of at least +2.8 points. This highlights the
effectiveness and potential of our new BERT models for Persian NLU tasks.","[{'name': 'MohammadAli SadraeiJavaheri'}, {'name': 'Ali Moghaddaszadeh'}, {'name': 'Milad Molazadeh'}, {'name': 'Fariba Naeiji'}, {'name': 'Farnaz Aghababaloo'}, {'name': 'Hamideh Rafiee'}, {'name': 'Zahra Amirmahani'}, {'name': 'Tohid Abedini'}, {'name': 'Fatemeh Zahra Sheikhi'}, {'name': 'Amirmohammad Salehoof'}]",2024-07-23T11:12:47Z
http://arxiv.org/abs/2407.16370v1,http://arxiv.org/abs/2407.16370v1,Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction,"Building upon the strength of modern large language models (LLMs), generative
error correction (GEC) has emerged as a promising paradigm that can elevate the
performance of modern automatic speech recognition (ASR) systems. One
representative approach is to leverage in-context learning to prompt LLMs so
that a better hypothesis can be generated by the LLMs based on a
carefully-designed prompt and an $N$-best list of hypotheses produced by ASR
systems. However, it is yet unknown whether the existing prompts are the most
effective ones for the task of post-ASR error correction. In this context, this
paper first explores alternative prompts to identify an initial set of
effective prompts, and then proposes to employ an evolutionary prompt
optimization algorithm to refine the initial prompts. Evaluations results on
the CHiME-4 subset of the Task $1$ of the SLT $2024$ GenSEC challenge show the
effectiveness and potential of the proposed algorithms.","[{'name': 'Rithik Sachdev'}, {'name': 'Zhong-Qiu Wang'}, {'name': 'Chao-Han Huck Yang'}]",2024-07-23T10:38:49Z
http://arxiv.org/abs/2407.16347v1,http://arxiv.org/abs/2407.16347v1,FACTTRACK: Time-Aware World State Tracking in Story Outlines,"While accurately detecting and correcting factual contradictions in language
model outputs has become increasingly important as their capabilities improve,
doing so is highly challenging. We propose a novel method, FACTTRACK, for
tracking atomic facts and addressing factual contradictions. Crucially,
FACTTRACK also maintains time-aware validity intervals for each fact, allowing
for change over time. At a high level, FACTTRACK consists of a four-step
pipeline to update a world state data structure for each new event: (1)
decompose the event into directional atomic facts; (2) determine the validity
interval of each atomic fact using the world state; (3) detect contradictions
with existing facts in the world state; and finally (4) add new facts to the
world state and update existing atomic facts. When we apply FACTTRACK to
contradiction detection on structured story outlines, we find that FACTTRACK
using LLaMA2-7B-Chat substantially outperforms a fair baseline using
LLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline.
Moreover, when using GPT4, FACTTRACK significantly outperforms the GPT4
baseline.","[{'name': 'Zhiheng Lyu'}, {'name': 'Kevin Yang'}, {'name': 'Lingpeng Kong'}, {'name': 'Daniel Klein'}]",2024-07-23T09:50:14Z
http://arxiv.org/abs/2407.16318v1,http://arxiv.org/abs/2407.16318v1,PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing,"Deploying language models (LMs) necessitates outputs to be both high-quality
and compliant with safety guidelines. Although Inference-Time Guardrails (ITG)
offer solutions that shift model output distributions towards compliance, we
find that current methods struggle in balancing safety with helpfulness. ITG
Methods that safely address non-compliant queries exhibit lower helpfulness
while those that prioritize helpfulness compromise on safety. We refer to this
trade-off as the guardrail tax, analogous to the alignment tax. To address
this, we propose PrimeGuard, a novel ITG method that utilizes structured
control flow.
  PrimeGuard routes requests to different self-instantiations of the LM with
varying instructions, leveraging its inherent instruction-following
capabilities and in-context learning. Our tuning-free approach dynamically
compiles system-designer guidelines for each query. We construct and release
safe-eval, a diverse red-team safety benchmark. Extensive evaluations
demonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail tax
by (1) significantly increasing resistance to iterative jailbreak attacks and
(2) achieving state-of-the-art results in safety guardrailing while (3)
matching helpfulness scores of alignment-tuned models. Extensive evaluations
demonstrate that PrimeGuard, without fine-tuning, outperforms all competing
baselines and overcomes the guardrail tax by improving the fraction of safe
responses from 61% to 97% and increasing average helpfulness scores from 4.17
to 4.29 on the largest models, while reducing attack success rate from 100% to
8%.
  PrimeGuard implementation is available at
https://github.com/dynamofl/PrimeGuard and safe-eval dataset is available at
https://huggingface.co/datasets/dynamoai/safe_eval.","[{'name': 'Blazej Manczak'}, {'name': 'Eliott Zemour'}, {'name': 'Eric Lin'}, {'name': 'Vaikkunth Mugunthan'}]",2024-07-23T09:14:27Z
http://arxiv.org/abs/2407.21052v1,http://arxiv.org/abs/2407.21052v1,"Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet
  Extraction","Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract
fine-grained sentiment elements from target domain sentences by leveraging the
knowledge acquired from the source domain. Due to the absence of labeled data
in the target domain, recent studies tend to rely on pre-trained language
models to generate large amounts of synthetic data for training purposes.
However, these approaches entail additional computational costs associated with
the generation process. Different from them, we discover a striking resemblance
between table-filling methods in ASTE and two-stage Object Detection (OD) in
computer vision, which inspires us to revisit the cross-domain ASTE task and
approach it from an OD standpoint. This allows the model to benefit from the OD
extraction paradigm and region-level alignment. Building upon this premise, we
propose a novel method named \textbf{T}able-\textbf{F}illing via \textbf{M}ean
\textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the
sentence into a 2D table to detect word relations, while TFMT treats the table
as a feature map and utilizes a region consistency to enhance the quality of
those generated pseudo labels. Additionally, considering the existence of the
domain gap, a cross-domain consistency based on Maximum Mean Discrepancy is
designed to alleviate domain shift problems. Our method achieves
state-of-the-art performance with minimal parameters and computational costs,
making it a strong baseline for cross-domain ASTE.","[{'name': 'Kun Peng'}, {'name': 'Lei Jiang'}, {'name': 'Qian Li'}, {'name': 'Haoran Li'}, {'name': 'Xiaoyan Yu'}, {'name': 'Li Sun'}, {'name': 'Shuo Sun'}, {'name': 'Yanxian Bi'}, {'name': 'Hao Peng'}]",2024-07-23T09:04:08Z
http://arxiv.org/abs/2408.06345v1,http://arxiv.org/abs/2408.06345v1,"Deep Learning based Key Information Extraction from Business Documents:
  Systematic Literature Review","Extracting key information from documents represents a large portion of
business workloads and therefore offers a high potential for efficiency
improvements and process automation. With recent advances in deep learning, a
plethora of deep learning-based approaches for Key Information Extraction have
been proposed under the umbrella term Document Understanding that enable the
processing of complex business documents. The goal of this systematic
literature review is an in-depth analysis of existing approaches in this domain
and the identification of opportunities for further research. To this end, 96
approaches published between 2017 and 2023 are analyzed in this study.","[{'name': 'Alexander Rombach'}, {'name': 'Peter Fettke'}]",2024-07-23T08:15:55Z
http://arxiv.org/abs/2407.16245v1,http://arxiv.org/abs/2407.16245v1,"Exploring the Effectiveness and Consistency of Task Selection in
  Intermediate-Task Transfer Learning","Identifying beneficial tasks to transfer from is a critical step toward
successful intermediate-task transfer learning. In this work, we experiment
with 130 source-target task combinations and demonstrate that the transfer
performance exhibits severe variance across different source tasks and training
seeds, highlighting the crucial role of intermediate-task selection in a
broader context. We compare four representative task selection methods in a
unified setup, focusing on their effectiveness and consistency. Compared to
embedding-free methods and text embeddings, task embeddings constructed from
fine-tuned weights can better estimate task transferability by improving task
prediction scores from 2.59% to 3.96%. Despite their strong performance, we
observe that the task embeddings do not consistently demonstrate superiority
for tasks requiring reasoning abilities. Furthermore, we introduce a novel
method that measures pairwise token similarity using maximum inner product
search, leading to the highest performance in task prediction. Our findings
suggest that token-wise similarity is better predictive for predicting
transferability compared to averaging weights.","[{'name': 'Pin-Jie Lin'}, {'name': 'Miaoran Zhang'}, {'name': 'Marius Mosbach'}, {'name': 'Dietrich Klakow'}]",2024-07-23T07:31:43Z
http://arxiv.org/abs/2407.16234v1,http://arxiv.org/abs/2407.16234v1,"A Multi-view Mask Contrastive Learning Graph Convolutional Neural
  Network for Age Estimation","The age estimation task aims to use facial features to predict the age of
people and is widely used in public security, marketing, identification, and
other fields. However, the features are mainly concentrated in facial
keypoints, and existing CNN and Transformer-based methods have inflexibility
and redundancy for modeling complex irregular structures. Therefore, this paper
proposes a Multi-view Mask Contrastive Learning Graph Convolutional Neural
Network (MMCL-GCN) for age estimation. Specifically, the overall structure of
the MMCL-GCN network contains a feature extraction stage and an age estimation
stage. In the feature extraction stage, we introduce a graph structure to
construct face images as input and then design a Multi-view Mask Contrastive
Learning (MMCL) mechanism to learn complex structural and semantic information
about face images. The learning mechanism employs an asymmetric siamese network
architecture, which utilizes an online encoder-decoder structure to reconstruct
the missing information from the original graph and utilizes the target encoder
to learn latent representations for contrastive learning. Furthermore, to
promote the two learning mechanisms better compatible and complementary, we
adopt two augmentation strategies and optimize the joint losses. In the age
estimation stage, we design a Multi-layer Extreme Learning Machine (ML-IELM)
with identity mapping to fully use the features extracted by the online
encoder. Then, a classifier and a regressor were constructed based on ML-IELM,
which were used to identify the age grouping interval and accurately estimate
the final age. Extensive experiments show that MMCL-GCN can effectively reduce
the error of age estimation on benchmark datasets such as Adience, MORPH-II,
and LAP-2016.","[{'name': 'Yiping Zhang'}, {'name': 'Yuntao Shou'}, {'name': 'Tao Meng'}, {'name': 'Wei Ai'}, {'name': 'Keqin Li'}]",2024-07-23T07:17:46Z
http://arxiv.org/abs/2407.16222v1,http://arxiv.org/abs/2407.16222v1,"PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of
  Multilingual Alignment","Large language models demonstrate reasonable multilingual abilities, despite
predominantly English-centric pretraining. However, the spontaneous
multilingual alignment in these models is shown to be weak, leading to
unsatisfactory cross-lingual transfer and knowledge sharing. Previous works
attempt to address this issue by explicitly injecting multilingual alignment
information during or after pretraining. Thus for the early stage in
pretraining, the alignment is weak for sharing information or knowledge across
languages. In this paper, we propose PreAlign, a framework that establishes
multilingual alignment prior to language model pretraining. PreAlign injects
multilingual alignment by initializing the model to generate similar
representations of aligned words and preserves this alignment using a
code-switching strategy during pretraining. Extensive experiments in a
synthetic English to English-Clone setting demonstrate that PreAlign
significantly outperforms standard multilingual joint training in language
modeling, zero-shot cross-lingual transfer, and cross-lingual knowledge
application. Further experiments in real-world scenarios further validate
PreAlign's effectiveness across various model sizes.","[{'name': 'Jiahuan Li'}, {'name': 'Shujian Huang'}, {'name': 'Xinyu Dai'}, {'name': 'Jiajun Chen'}]",2024-07-23T06:59:53Z
http://arxiv.org/abs/2407.16221v1,http://arxiv.org/abs/2407.16221v1,"Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of
  Large Language Models","As Large Language Models (LLMs) achieve remarkable performance across various
NLP tasks, their reliability becomes essential for widespread adoption. This
paper focuses on Abstention Ability (AA), a critical yet under explored aspect
of reliability - the ability of LLMs to refrain from answering questions when
they are uncertain or when definitive answer is not possible, while maintaining
question-answering (QA) task performance. While previous works have focused on
understanding the recollection abilities of LLMs or their ability to identify
imponderable/unanswerable questions, we believe there is a need for an
effective AA evaluation method. Therefore, we propose a black-box evaluation
methodology to examine and understand the AA of LLMs across a variety of
multiple-choice QA tasks. We measure AA by rewarding models for abstaining from
answering when their predictions are incorrect or when the questions are
inherently unanswerable. We investigate three strategies, Strict Prompting,
Verbal Confidence Thresholding, and Chain-of-Thought (CoT), to understand their
impact on abstention across different LLMs. Our findings reveal that while even
state-of-the-art LLMs like GPT-4 struggle with abstention, strategic prompting
such as CoT, can significantly enhance this ability. Furthermore, we
demonstrate that improving AA also leads to better overall QA task performance,
underscoring the importance of evaluating AA in LLMs.","[{'name': 'Nishanth Madhusudhan'}, {'name': 'Sathwik Tejaswi Madhusudhan'}, {'name': 'Vikas Yadav'}, {'name': 'Masoud Hashemi'}]",2024-07-23T06:56:54Z
http://arxiv.org/abs/2407.16216v1,http://arxiv.org/abs/2407.16216v1,"A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO,
  DPO and More","With advancements in self-supervised learning, the availability of trillions
tokens in a pre-training corpus, instruction fine-tuning, and the development
of large Transformers with billions of parameters, large language models (LLMs)
are now capable of generating factual and coherent responses to human queries.
However, the mixed quality of training data can lead to the generation of
undesired responses, presenting a significant challenge. Over the past two
years, various methods have been proposed from different perspectives to
enhance LLMs, particularly in aligning them with human expectation. Despite
these efforts, there has not been a comprehensive survey paper that categorizes
and details these approaches. In this work, we aim to address this gap by
categorizing these papers into distinct topics and providing detailed
explanations of each alignment method, thereby helping readers gain a thorough
understanding of the current state of the field.","[{'name': 'Zhichao Wang'}, {'name': 'Bin Bi'}, {'name': 'Shiva Kumar Pentyala'}, {'name': 'Kiran Ramnath'}, {'name': 'Sougata Chaudhuri'}, {'name': 'Shubham Mehrotra'}, {'name': 'Zixu'}, {'name': 'Zhu'}, {'name': 'Xiang-Bo Mao'}, {'name': 'Sitaram Asur'}, {'name': 'Na'}, {'name': 'Cheng'}]",2024-07-23T06:45:52Z
http://arxiv.org/abs/2407.16207v1,http://arxiv.org/abs/2407.16207v1,Graph-Structured Speculative Decoding,"Speculative decoding has emerged as a promising technique to accelerate the
inference of Large Language Models (LLMs) by employing a small language model
to draft a hypothesis sequence, which is then validated by the LLM. The
effectiveness of this approach heavily relies on the balance between
performance and efficiency of the draft model. In our research, we focus on
enhancing the proportion of draft tokens that are accepted to the final output
by generating multiple hypotheses instead of just one. This allows the LLM more
options to choose from and select the longest sequence that meets its
standards. Our analysis reveals that hypotheses produced by the draft model
share many common token sequences, suggesting a potential for optimizing
computation. Leveraging this observation, we introduce an innovative approach
utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This
structure enables us to efficiently predict and merge recurring token
sequences, vastly reducing the computational demands of the draft model. We
term this approach Graph-structured Speculative Decoding (GSD). We apply GSD
across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and
observe a remarkable speedup of 1.73$\times$ to 1.96$\times$, significantly
surpassing standard speculative decoding.","[{'name': 'Zhuocheng Gong'}, {'name': 'Jiahao Liu'}, {'name': 'Ziyue Wang'}, {'name': 'Pengfei Wu'}, {'name': 'Jingang Wang'}, {'name': 'Xunliang Cai'}, {'name': 'Dongyan Zhao'}, {'name': 'Rui Yan'}]",2024-07-23T06:21:24Z
http://arxiv.org/abs/2407.16205v3,http://arxiv.org/abs/2407.16205v3,Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models,"The rapid development of Large Language Models (LLMs) has brought remarkable
generative capabilities across diverse tasks. However, despite the impressive
achievements, these LLMs still have numerous inherent vulnerabilities,
particularly when faced with jailbreak attacks. By investigating jailbreak
attacks, we can uncover hidden weaknesses in LLMs and inform the development of
more robust defense mechanisms to fortify their security. In this paper, we
further explore the boundary of jailbreak attacks on LLMs and propose
Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes
advantage of LLMs' growing analyzing and reasoning capability and reveals their
underlying vulnerabilities when facing analyzing-based tasks. We conduct a
detailed evaluation of ABJ across various open-source and closed-source LLMs,
which achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)
on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and
efficiency. Our research highlights the importance of prioritizing and
enhancing the safety of LLMs to mitigate the risks of misuse. The code is
publicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This
paper contains examples of LLMs that might be offensive or harmful.","[{'name': 'Shi Lin'}, {'name': 'Rongchang Li'}, {'name': 'Xun Wang'}, {'name': 'Changting Lin'}, {'name': 'Wenpeng Xing'}, {'name': 'Meng Han'}]",2024-07-23T06:14:41Z
http://arxiv.org/abs/2407.16192v1,http://arxiv.org/abs/2407.16192v1,"How to Leverage Personal Textual Knowledge for Personalized
  Conversational Information Retrieval","Personalized conversational information retrieval (CIR) combines
conversational and personalizable elements to satisfy various users' complex
information needs through multi-turn interaction based on their backgrounds.
The key promise is that the personal textual knowledge base (PTKB) can improve
the CIR effectiveness because the retrieval results can be more related to the
user's background. However, PTKB is noisy: not every piece of knowledge in PTKB
is relevant to the specific query at hand. In this paper, we explore and test
several ways to select knowledge from PTKB and use it for query reformulation
by using a large language model (LLM). The experimental results show the PTKB
might not always improve the search results when used alone, but LLM can help
generate a more appropriate personalized query when high-quality guidance is
provided.","[{'name': 'Fengran Mo'}, {'name': 'Longxiang Zhao'}, {'name': 'Kaiyu Huang'}, {'name': 'Yue Dong'}, {'name': 'Degen Huang'}, {'name': 'Jian-Yun Nie'}]",2024-07-23T05:34:41Z
http://arxiv.org/abs/2407.16190v2,http://arxiv.org/abs/2407.16190v2,Artificial Agency and Large Language Models,"The arrival of Large Language Models (LLMs) has stirred up philosophical
debates about the possibility of realizing agency in an artificial manner. In
this work we contribute to the debate by presenting a theoretical model that
can be used as a threshold conception for artificial agents. The model defines
agents as systems whose actions and goals are always influenced by a dynamic
framework of factors that consists of the agent's accessible history, its
adaptive repertoire and its external environment. This framework, in turn, is
influenced by the actions that the agent takes and the goals that it forms. We
show with the help of the model that state-of-the-art LLMs are not agents yet,
but that there are elements to them that suggest a way forward. The paper
argues that a combination of the agent architecture presented in Park et al.
(2023) together with the use of modules like the Coscientist in Boiko et al.
(2023) could potentially be a way to realize agency in an artificial manner. We
end the paper by reflecting on the obstacles one might face in building such an
artificial agent and by presenting possible directions for future research.","[{'name': 'Maud van Lier'}, {'name': 'Gorka Muñoz-Gil'}]",2024-07-23T05:32:00Z
http://arxiv.org/abs/2407.21051v1,http://arxiv.org/abs/2407.21051v1,"An Active Inference Strategy for Prompting Reliable Responses from Large
  Language Models in Medical Practice","Continuing advances in Large Language Models (LLMs) in artificial
intelligence offer important capacities in intuitively accessing and using
medical knowledge in many contexts, including education and training as well as
assessment and treatment. Most of the initial literature on LLMs in medicine
has emphasized that LLMs are unsuitable for medical use because they are
non-deterministic, may provide incorrect or harmful responses, and cannot be
regulated to assure quality control. If these issues could be corrected,
optimizing LLM technology could benefit patients and physicians by providing
affordable, point-of-care medical knowledge. Our proposed framework refines LLM
responses by restricting their primary knowledge base to domain-specific
datasets containing validated medical information. Additionally, we introduce
an actor-critic LLM prompting protocol based on active inference principles of
human cognition, where a Therapist agent initially responds to patient queries,
and a Supervisor agent evaluates and adjusts responses to ensure accuracy and
reliability. We conducted a validation study where expert cognitive behaviour
therapy for insomnia (CBT-I) therapists evaluated responses from the LLM in a
blind format. Experienced human CBT-I therapists assessed responses to 100
patient queries, comparing LLM-generated responses with appropriate and
inappropriate responses crafted by experienced CBT-I therapists. Results showed
that LLM responses received high ratings from the CBT-I therapists, often
exceeding those of therapist-generated appropriate responses. This structured
approach aims to integrate advanced LLM technology into medical applications,
meeting regulatory requirements for establishing the safe and effective use of
special purpose validated LLMs in medicine.","[{'name': 'Roma Shusterman'}, {'name': 'Allison C. Waters'}, {'name': 'Shannon O`Neill'}, {'name': 'Phan Luu'}, {'name': 'Don M. Tucker'}]",2024-07-23T05:00:18Z
http://arxiv.org/abs/2407.16181v1,http://arxiv.org/abs/2407.16181v1,"Structural Optimization Ambiguity and Simplicity Bias in Unsupervised
  Neural Grammar Induction","Neural parameterization has significantly advanced unsupervised grammar
induction. However, training these models with a traditional likelihood loss
for all possible parses exacerbates two issues: 1) $\textit{structural
optimization ambiguity}$ that arbitrarily selects one among structurally
ambiguous optimal grammars despite the specific preference of gold parses, and
2) $\textit{structural simplicity bias}$ that leads a model to underutilize
rules to compose parse trees. These challenges subject unsupervised neural
grammar induction (UNGI) to inevitable prediction errors, high variance, and
the necessity for extensive grammars to achieve accurate predictions. This
paper tackles these issues, offering a comprehensive analysis of their origins.
As a solution, we introduce $\textit{sentence-wise parse-focusing}$ to reduce
the parse pool per sentence for loss evaluation, using the structural bias from
pre-trained parsers on the same dataset. In unsupervised parsing benchmark
tests, our method significantly improves performance while effectively reducing
variance and bias toward overly simplistic parses. Our research promotes
learning more compact, accurate, and consistent explicit grammars, facilitating
better interpretability.","[{'name': 'Jinwook Park'}, {'name': 'Kangil Kim'}]",2024-07-23T04:57:03Z
http://arxiv.org/abs/2407.16168v1,http://arxiv.org/abs/2407.16168v1,Progressively Modality Freezing for Multi-Modal Entity Alignment,"Multi-Modal Entity Alignment aims to discover identical entities across
heterogeneous knowledge graphs. While recent studies have delved into fusion
paradigms to represent entities holistically, the elimination of features
irrelevant to alignment and modal inconsistencies is overlooked, which are
caused by inherent differences in multi-modal features. To address these
challenges, we propose a novel strategy of progressive modality freezing,
called PMF, that focuses on alignmentrelevant features and enhances multi-modal
feature fusion. Notably, our approach introduces a pioneering cross-modal
association loss to foster modal consistency. Empirical evaluations across nine
datasets confirm PMF's superiority, demonstrating stateof-the-art performance
and the rationale for freezing modalities. Our code is available at
https://github.com/ninibymilk/PMF-MMEA.","[{'name': 'Yani Huang'}, {'name': 'Xuefeng Zhang'}, {'name': 'Richong Zhang'}, {'name': 'Junfan Chen'}, {'name': 'Jaein Kim'}]",2024-07-23T04:22:30Z
http://arxiv.org/abs/2407.16166v1,http://arxiv.org/abs/2407.16166v1,"Robust Privacy Amidst Innovation with Large Language Models Through a
  Critical Assessment of the Risks","This study examines integrating EHRs and NLP with large language models
(LLMs) to improve healthcare data management and patient care. It focuses on
using advanced models to create secure, HIPAA-compliant synthetic patient notes
for biomedical research. The study used de-identified and re-identified MIMIC
III datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.
Text generation employed templates and keyword extraction for contextually
relevant notes, with one-shot generation for comparison. Privacy assessment
checked PHI occurrence, while text utility was tested using an ICD-9 coding
task. Text quality was evaluated with ROUGE and cosine similarity metrics to
measure semantic similarity with source notes. Analysis of PHI occurrence and
text utility via the ICD-9 coding task showed that the keyword-based method had
low risk and good performance. One-shot generation showed the highest PHI
exposure and PHI co-occurrence, especially in geographic location and date
categories. The Normalized One-shot method achieved the highest classification
accuracy. Privacy analysis revealed a critical balance between data utility and
privacy protection, influencing future data use and sharing. Re-identified data
consistently outperformed de-identified data. This study demonstrates the
effectiveness of keyword-based methods in generating privacy-protecting
synthetic clinical notes that retain data usability, potentially transforming
clinical data-sharing practices. The superior performance of re-identified over
de-identified data suggests a shift towards methods that enhance utility and
privacy by using dummy PHIs to perplex privacy attacks.","[{'name': 'Yao-Shun Chuang'}, {'name': 'Atiquer Rahman Sarkar'}, {'name': 'Noman Mohammed'}, {'name': 'Xiaoqian Jiang'}]",2024-07-23T04:20:14Z
http://arxiv.org/abs/2407.21050v2,http://arxiv.org/abs/2407.21050v2,"Artificial Intelligence in Extracting Diagnostic Data from Dental
  Records","This research addresses the issue of missing structured data in dental
records by extracting diagnostic information from unstructured text. The
updated periodontology classification system's complexity has increased
incomplete or missing structured diagnoses. To tackle this, we use advanced AI
and NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a
RoBERTa model. This significantly enhances the model's ability to understand
medical and dental language. We evaluated the model using 120 randomly selected
clinical notes from two datasets, demonstrating its improved diagnostic
extraction accuracy. The results showed high accuracy in diagnosing periodontal
status, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In
the subtype category, Site 2 achieved perfect scores, outperforming Site 1.
This method enhances extraction accuracy and broadens its use across dental
contexts. The study underscores AI and NLP's transformative impact on
healthcare delivery and management. Integrating AI and NLP technologies
enhances documentation and simplifies administrative tasks by precisely
extracting complex clinical information. This approach effectively addresses
challenges in dental diagnostics. Using synthetic training data from LLMs
optimizes the training process, improving accuracy and efficiency in
identifying periodontal diagnoses from clinical notes. This innovative method
holds promise for broader healthcare applications, potentially improving
patient care quality.","[{'name': 'Yao-Shun Chuang'}, {'name': 'Chun-Teh Lee'}, {'name': 'Oluwabunmi Tokede'}, {'name': 'Guo-Hao Lin'}, {'name': 'Ryan Brandon'}, {'name': 'Trung Duong Tran'}, {'name': 'Xiaoqian Jiang'}, {'name': 'Muhammad F. Walji'}]",2024-07-23T04:05:48Z
http://arxiv.org/abs/2407.16148v1,http://arxiv.org/abs/2407.16148v1,"CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for
  Literature Review Support","Literature review requires researchers to synthesize a large amount of
information and is increasingly challenging as the scientific literature
expands. In this work, we investigate the potential of LLMs for producing
hierarchical organizations of scientific studies to assist researchers with
literature review. We define hierarchical organizations as tree structures
where nodes refer to topical categories and every node is linked to the studies
assigned to that category. Our naive LLM-based pipeline for hierarchy
generation from a set of studies produces promising yet imperfect hierarchies,
motivating us to collect CHIME, an expert-curated dataset for this task focused
on biomedicine. Given the challenging and time-consuming nature of building
hierarchies from scratch, we use a human-in-the-loop process in which experts
correct errors (both links between categories and study assignment) in
LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies
covering 472 topics, and expert-corrected hierarchies for a subset of 100
topics. Expert corrections allow us to quantify LLM performance, and we find
that while they are quite good at generating and organizing categories, their
assignment of studies to categories could be improved. We attempt to train a
corrector model with human feedback which improves study assignment by 12.6 F1
points. We release our dataset and models to encourage research on developing
better assistive tools for literature review.","[{'name': 'Chao-Chun Hsu'}, {'name': 'Erin Bransom'}, {'name': 'Jenna Sparks'}, {'name': 'Bailey Kuehl'}, {'name': 'Chenhao Tan'}, {'name': 'David Wadden'}, {'name': 'Lucy Lu Wang'}, {'name': 'Aakanksha Naik'}]",2024-07-23T03:18:00Z
http://arxiv.org/abs/2407.21049v1,http://arxiv.org/abs/2407.21049v1,"Evaluating Long Range Dependency Handling in Code Generation Models
  using Multi-Step Key Retrieval","As language models support larger and larger context sizes, evaluating their
ability to make effective use of that context becomes increasingly important.
We analyze the ability of several code generation models to handle long range
dependencies using a suite of multi-step key retrieval tasks in context windows
up to 8k tokens in length. The tasks progressively increase in difficulty and
allow more nuanced evaluation of model capabilities than tests like the popular
needle-in-the-haystack test. We find that performance degrades significantly
(up to 2x) when a function references another function that is defined later in
the prompt. We also observe that models that use sliding window attention
mechanisms have difficulty handling references further than the size of a
single window. We perform simple prompt modifications using call graph
information to improve multi-step retrieval performance up to 3x. Our analysis
highlights different facets of long-context performance and is suggestive of
prompt construction strategies for code completion tools","[{'name': 'Yannick Assogba'}, {'name': 'Donghao Ren'}]",2024-07-23T02:45:22Z
http://arxiv.org/abs/2407.16127v1,http://arxiv.org/abs/2407.16127v1,"Finetuning Generative Large Language Models with Discrimination
  Instructions for Knowledge Graph Completion","Traditional knowledge graph (KG) completion models learn embeddings to
predict missing facts. Recent works attempt to complete KGs in a
text-generation manner with large language models (LLMs). However, they need to
ground the output of LLMs to KG entities, which inevitably brings errors. In
this paper, we present a finetuning framework, DIFT, aiming to unleash the KG
completion ability of LLMs and avoid grounding errors. Given an incomplete
fact, DIFT employs a lightweight model to obtain candidate entities and
finetunes an LLM with discrimination instructions to select the correct one
from the given candidates. To improve performance while reducing instruction
data, DIFT uses a truncated sampling method to select useful facts for
finetuning and injects KG embeddings into the LLM. Extensive experiments on
benchmark datasets demonstrate the effectiveness of our proposed framework.","[{'name': 'Yang Liu'}, {'name': 'Xiaobin Tian'}, {'name': 'Zequn Sun'}, {'name': 'Wei Hu'}]",2024-07-23T02:25:01Z
http://arxiv.org/abs/2407.21048v1,http://arxiv.org/abs/2407.21048v1,"APTNESS: Incorporating Appraisal Theory and Emotion Support Strategies
  for Empathetic Response Generation","Empathetic response generation is designed to comprehend the emotions of
others and select the most appropriate strategies to assist them in resolving
emotional challenges. Empathy can be categorized into cognitive empathy and
affective empathy. The former pertains to the ability to understand and discern
the emotional issues and situations of others, while the latter involves the
capacity to provide comfort. To enhance one's empathetic abilities, it is
essential to develop both these aspects. Therefore, we develop an innovative
framework that combines retrieval augmentation and emotional support strategy
integration. Our framework starts with the introduction of a comprehensive
emotional palette for empathy. We then apply appraisal theory to decompose this
palette and create a database of empathetic responses. This database serves as
an external resource and enhances the LLM's empathy by integrating semantic
retrieval mechanisms. Moreover, our framework places a strong emphasis on the
proper articulation of response strategies. By incorporating emotional support
strategies, we aim to enrich the model's capabilities in both cognitive and
affective empathy, leading to a more nuanced and comprehensive empathetic
response. Finally, we extract datasets ED and ET from the empathetic dialogue
dataset \textsc{EmpatheticDialogues} and ExTES based on dialogue length.
Experiments demonstrate that our framework can enhance the empathy ability of
LLMs from both cognitive and affective empathy perspectives. Our code is
released at https://github.com/CAS-SIAT-XinHai/APTNESS.","[{'name': 'Yuxuan Hu'}, {'name': 'Minghuan Tan'}, {'name': 'Chenwei Zhang'}, {'name': 'Zixuan Li'}, {'name': 'Xiaodan Liang'}, {'name': 'Min Yang'}, {'name': 'Chengming Li'}, {'name': 'Xiping Hu'}]",2024-07-23T02:23:37Z
http://arxiv.org/abs/2407.16110v3,http://arxiv.org/abs/2407.16110v3,Analyzing Polysemy Evolution Using Semantic Cells,"The senses of words evolve. The sense of the same word may change from today
to tomorrow, and multiple senses of the same word may be the result of the
evolution of each other, that is, they may be parents and children. If we view
Juba as an evolving ecosystem, the paradigm of learning the correct answer,
which does not move with the sense of a word, is no longer valid. This paper is
a case study that shows that word polysemy is an evolutionary consequence of
the modification of Semantic Cells, which has al-ready been presented by the
author, by introducing a small amount of diversity in its initial state as an
example of analyzing the current set of short sentences. In particular, the
analysis of a sentence sequence of 1000 sentences in some order for each of the
four senses of the word Spring, collected using Chat GPT, shows that the word
acquires the most polysemy monotonically in the analysis when the senses are
arranged in the order in which they have evolved. In other words, we present a
method for analyzing the dynamism of a word's acquiring polysemy with evolution
and, at the same time, a methodology for viewing polysemy from an evolutionary
framework rather than a learning-based one.","[{'name': 'Yukio Ohsawa'}, {'name': 'Dingming Xue'}, {'name': 'Kaira Sekiguchi'}]",2024-07-23T00:52:12Z
http://arxiv.org/abs/2407.16073v1,http://arxiv.org/abs/2407.16073v1,KaPQA: Knowledge-Augmented Product Question-Answering,"Question-answering for domain-specific applications has recently attracted
much interest due to the latest advancements in large language models (LLMs).
However, accurately assessing the performance of these applications remains a
challenge, mainly due to the lack of suitable benchmarks that effectively
simulate real-world scenarios. To address this challenge, we introduce two
product question-answering (QA) datasets focused on Adobe Acrobat and Photoshop
products to help evaluate the performance of existing models on domain-specific
product QA tasks. Additionally, we propose a novel knowledge-driven RAG-QA
framework to enhance the performance of the models in the product QA task. Our
experiments demonstrated that inducing domain knowledge through query
reformulation allowed for increased retrieval and generative performance when
compared to standard RAG-QA methods. This improvement, however, is slight, and
thus illustrates the challenge posed by the datasets introduced.","[{'name': 'Swetha Eppalapally'}, {'name': 'Daksh Dangi'}, {'name': 'Chaithra Bhat'}, {'name': 'Ankita Gupta'}, {'name': 'Ruiyi Zhang'}, {'name': 'Shubham Agarwal'}, {'name': 'Karishma Bagga'}, {'name': 'Seunghyun Yoon'}, {'name': 'Nedim Lipka'}, {'name': 'Ryan A. Rossi'}, {'name': 'Franck Dernoncourt'}]",2024-07-22T22:14:56Z
http://arxiv.org/abs/2407.16047v1,http://arxiv.org/abs/2407.16047v1,"Leveraging Large Language Models to Geolocate Linguistic Variations in
  Social Media Posts","Geolocalization of social media content is the task of determining the
geographical location of a user based on textual data, that may show linguistic
variations and informal language. In this project, we address the GeoLingIt
challenge of geolocalizing tweets written in Italian by leveraging large
language models (LLMs). GeoLingIt requires the prediction of both the region
and the precise coordinates of the tweet. Our approach involves fine-tuning
pre-trained LLMs to simultaneously predict these geolocalization aspects. By
integrating innovative methodologies, we enhance the models' ability to
understand the nuances of Italian social media text to improve the
state-of-the-art in this domain. This work is conducted as part of the Large
Language Models course at the Bertinoro International Spring School 2024. We
make our code publicly available on GitHub
https://github.com/dawoz/geolingit-biss2024.","[{'name': 'Davide Savarro'}, {'name': 'Davide Zago'}, {'name': 'Stefano Zoia'}]",2024-07-22T20:54:35Z
http://arxiv.org/abs/2408.00802v1,http://arxiv.org/abs/2408.00802v1,Leveraging LLM Reasoning Enhances Personalized Recommender Systems,"Recent advancements have showcased the potential of Large Language Models
(LLMs) in executing reasoning tasks, particularly facilitated by
Chain-of-Thought (CoT) prompting. While tasks like arithmetic reasoning involve
clear, definitive answers and logical chains of thought, the application of LLM
reasoning in recommendation systems (RecSys) presents a distinct challenge.
RecSys tasks revolve around subjectivity and personalized preferences, an
under-explored domain in utilizing LLMs' reasoning capabilities. Our study
explores several aspects to better understand reasoning for RecSys and
demonstrate how task quality improves by utilizing LLM reasoning in both
zero-shot and finetuning settings. Additionally, we propose RecSAVER
(Recommender Systems Automatic Verification and Evaluation of Reasoning) to
automatically assess the quality of LLM reasoning responses without the
requirement of curated gold references or human raters. We show that our
framework aligns with real human judgment on the coherence and faithfulness of
reasoning responses. Overall, our work shows that incorporating reasoning into
RecSys can improve personalized tasks, paving the way for further advancements
in recommender system methodologies.","[{'name': 'Alicia Y. Tsai'}, {'name': 'Adam Kraft'}, {'name': 'Long Jin'}, {'name': 'Chenwei Cai'}, {'name': 'Anahita Hosseini'}, {'name': 'Taibai Xu'}, {'name': 'Zemin Zhang'}, {'name': 'Lichan Hong'}, {'name': 'Ed H. Chi'}, {'name': 'Xinyang Yi'}]",2024-07-22T20:18:50Z
http://arxiv.org/abs/2407.16030v1,http://arxiv.org/abs/2407.16030v1,Enhancing Temporal Understanding in LLMs for Semi-structured Tables,"Temporal reasoning over tabular data presents substantial challenges for
large language models (LLMs), as evidenced by recent research. In this study,
we conduct a comprehensive analysis of temporal datasets to pinpoint the
specific limitations of LLMs. Our investigation leads to enhancements in
TempTabQA, a dataset specifically designed for tabular temporal question
answering. We provide critical insights for improving LLM performance in
temporal reasoning tasks with tabular data. Furthermore, we introduce a novel
approach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings
demonstrate that our method significantly improves evidence-based reasoning
across various models. Additionally, our experimental results reveal that
indirect supervision with auxiliary data substantially boosts model performance
in these tasks. This work contributes to a deeper understanding of LLMs'
temporal reasoning abilities over tabular data and promotes advancements in
their application across diverse fields.","[{'name': 'Irwin Deng'}, {'name': 'Kushagra Dixit'}, {'name': 'Vivek Gupta'}, {'name': 'Dan Roth'}]",2024-07-22T20:13:10Z
http://arxiv.org/abs/2407.16008v1,http://arxiv.org/abs/2407.16008v1,"Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic
  Data Generation","Reward models (RMs) are crucial for aligning large language models (LLMs)
with human preferences. They are trained using preference datasets where each
example consists of one input prompt, two responses, and a preference label. As
curating a high-quality human labeled preference dataset is both time-consuming
and expensive, people often rely on existing powerful LLMs for preference label
generation. This can potentially introduce noise and impede RM training. In
this work, we present RMBoost, a novel synthetic preference data generation
paradigm to boost reward model quality. Unlike traditional methods, which
generate two responses before obtaining the preference label, RMBoost first
generates one response and selects a preference label, followed by generating
the second more (or less) preferred response conditioned on the pre-selected
preference label and the first response. This approach offers two main
advantages. First, RMBoost reduces labeling noise since preference pairs are
constructed intentionally. Second, RMBoost facilitates the creation of more
diverse responses by incorporating various quality aspects (e.g., helpfulness,
relevance, completeness) into the prompts. We conduct extensive experiments
across three diverse datasets and demonstrate that RMBoost outperforms other
synthetic preference data generation techniques and significantly boosts the
performance of four distinct reward models.","[{'name': 'Jiaming Shen'}, {'name': 'Ran Xu'}, {'name': 'Yennie Jun'}, {'name': 'Zhen Qin'}, {'name': 'Tianqi Liu'}, {'name': 'Carl Yang'}, {'name': 'Yi Liang'}, {'name': 'Simon Baumgartner'}, {'name': 'Michael Bendersky'}]",2024-07-22T19:21:55Z
http://arxiv.org/abs/2407.16007v1,http://arxiv.org/abs/2407.16007v1,"SocialQuotes: Learning Contextual Roles of Social Media Quotes on the
  Web","Web authors frequently embed social media to support and enrich their
content, creating the potential to derive web-based, cross-platform social
media representations that can enable more effective social media retrieval
systems and richer scientific analyses. As step toward such capabilities, we
introduce a novel language modeling framework that enables automatic annotation
of roles that social media entities play in their embedded web context. Using
related communication theory, we liken social media embeddings to quotes,
formalize the page context as structured natural language signals, and identify
a taxonomy of roles for quotes within the page context. We release
SocialQuotes, a new data set built from the Common Crawl of over 32 million
social quotes, 8.3k of them with crowdsourced quote annotations. Using
SocialQuotes and the accompanying annotations, we provide a role classification
case study, showing reasonable performance with modern-day LLMs, and exposing
explainable aspects of our framework via page content ablations. We also
classify a large batch of un-annotated quotes, revealing interesting
cross-domain, cross-platform role distributions on the web.","[{'name': 'John Palowitch'}, {'name': 'Hamidreza Alvari'}, {'name': 'Mehran Kazemi'}, {'name': 'Tanvir Amin'}, {'name': 'Filip Radlinski'}]",2024-07-22T19:21:01Z
http://arxiv.org/abs/2407.15992v1,http://arxiv.org/abs/2407.15992v1,Multimodal Input Aids a Bayesian Model of Phonetic Learning,"One of the many tasks facing the typically-developing child language learner
is learning to discriminate between the distinctive sounds that make up words
in their native language. Here we investigate whether multimodal
information--specifically adult speech coupled with video frames of speakers'
faces--benefits a computational model of phonetic learning. We introduce a
method for creating high-quality synthetic videos of speakers' faces for an
existing audio corpus. Our learning model, when both trained and tested on
audiovisual inputs, achieves up to a 8.1% relative improvement on a phoneme
discrimination battery compared to a model trained and tested on audio-only
input. It also outperforms the audio model by up to 3.9% when both are tested
on audio-only data, suggesting that visual information facilitates the
acquisition of acoustic distinctions. Visual information is especially
beneficial in noisy audio environments, where an audiovisual model closes 67%
of the loss in discrimination performance of the audio model in noise relative
to a non-noisy environment. These results demonstrate that visual information
benefits an ideal learner and illustrate some of the ways that children might
be able to leverage visual cues when learning to discriminate speech sounds.","[{'name': 'Sophia Zhi'}, {'name': 'Roger P. Levy'}, {'name': 'Stephan C. Meylan'}]",2024-07-22T19:00:11Z
http://arxiv.org/abs/2407.16711v2,http://arxiv.org/abs/2407.16711v2,Benchmarks as Microscopes: A Call for Model Metrology,"Modern language models (LMs) pose a new challenge in capability assessment.
Static benchmarks inevitably saturate without providing confidence in the
deployment tolerances of LM-based systems, but developers nonetheless claim
that their models have generalized traits such as reasoning or open-domain
language understanding based on these flawed metrics. The science and practice
of LMs requires a new approach to benchmarking which measures specific
capabilities with dynamic assessments. To be confident in our metrics, we need
a new discipline of model metrology -- one which focuses on how to generate
benchmarks that predict performance under deployment. Motivated by our
evaluation criteria, we outline how building a community of model metrology
practitioners -- one focused on building tools and studying how to measure
system capabilities -- is the best way to meet these needs to and add clarity
to the AI discussion.","[{'name': 'Michael Saxon'}, {'name': 'Ari Holtzman'}, {'name': 'Peter West'}, {'name': 'William Yang Wang'}, {'name': 'Naomi Saphra'}]",2024-07-22T17:52:12Z
http://arxiv.org/abs/2407.15835v1,http://arxiv.org/abs/2407.15835v1,dMel: Speech Tokenization made Simple,"Large language models have revolutionized natural language processing by
leveraging self-supervised pretraining on vast textual data. Inspired by this
success, researchers have investigated complicated speech tokenization methods
to discretize continuous speech signals so that language modeling techniques
can be applied to speech data. However, existing approaches either model
semantic tokens, potentially losing acoustic information, or model acoustic
tokens, risking the loss of semantic information. Having multiple token types
also complicates the architecture and requires additional pretraining. Here we
show that discretizing mel-filterbank channels into discrete intensity bins
produces a simple representation (dMel), that performs better than other
existing speech tokenization methods. Using a transformer decoder-only
architecture for speech-text modeling, we comprehensively evaluate different
speech tokenization methods on speech recognition (ASR), speech synthesis
(TTS). Our results demonstrate the effectiveness of dMel in achieving high
performance on both tasks within a unified framework, paving the way for
efficient and effective joint modeling of speech and text.","[{'name': 'He Bai'}, {'name': 'Tatiana Likhomanenko'}, {'name': 'Ruixiang Zhang'}, {'name': 'Zijin Gu'}, {'name': 'Zakaria Aldeneh'}, {'name': 'Navdeep Jaitly'}]",2024-07-22T17:51:53Z
http://arxiv.org/abs/2407.15828v1,http://arxiv.org/abs/2407.15828v1,"J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue
  Language Modeling","Spoken dialogue plays a crucial role in human-AI interactions, necessitating
dialogue-oriented spoken language models (SLMs). To develop versatile SLMs,
large-scale and diverse speech datasets are essential. Additionally, to ensure
hiqh-quality speech generation, the data must be spontaneous like in-wild data
and must be acoustically clean with noise removed. Despite the critical need,
no open-source corpus meeting all these criteria has been available. This study
addresses this gap by constructing and releasing a large-scale spoken dialogue
corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly
accessible. Furthermore, this paper presents a language-independent method for
corpus construction and describes experiments on dialogue generation using SLMs
trained on J-CHAT. Experimental results indicate that the collected data from
multiple domains by our method improve the naturalness and meaningfulness of
dialogue generation.","[{'name': 'Wataru Nakata'}, {'name': 'Kentaro Seki'}, {'name': 'Hitomi Yanaka'}, {'name': 'Yuki Saito'}, {'name': 'Shinnosuke Takamichi'}, {'name': 'Hiroshi Saruwatari'}]",2024-07-22T17:46:50Z
http://arxiv.org/abs/2407.15814v1,http://arxiv.org/abs/2407.15814v1,Perceptions of Linguistic Uncertainty by Language Models and Humans,"Uncertainty expressions such as ``probably'' or ``highly unlikely'' are
pervasive in human language. While prior work has established that there is
population-level agreement in terms of how humans interpret these expressions,
there has been little inquiry into the abilities of language models to
interpret such expressions. In this paper, we investigate how language models
map linguistic expressions of uncertainty to numerical responses. Our approach
assesses whether language models can employ theory of mind in this setting:
understanding the uncertainty of another agent about a particular statement,
independently of the model's own certainty about that statement. We evaluate
both humans and 10 popular language models on a task created to assess these
abilities. Unexpectedly, we find that 8 out of 10 models are able to map
uncertainty expressions to probabilistic responses in a human-like manner.
However, we observe systematically different behavior depending on whether a
statement is actually true or false. This sensitivity indicates that language
models are substantially more susceptible to bias based on their prior
knowledge (as compared to humans). These findings raise important questions and
have broad implications for human-AI alignment and AI-AI communication.","[{'name': 'Catarina G Belem'}, {'name': 'Markelle Kelly'}, {'name': 'Mark Steyvers'}, {'name': 'Sameer Singh'}, {'name': 'Padhraic Smyth'}]",2024-07-22T17:26:12Z
http://arxiv.org/abs/2407.15806v1,http://arxiv.org/abs/2407.15806v1,"FSboard: Over 3 million characters of ASL fingerspelling collected via
  smartphones","Progress in machine understanding of sign languages has been slow and
hampered by limited data. In this paper, we present FSboard, an American Sign
Language fingerspelling dataset situated in a mobile text entry use case,
collected from 147 paid and consenting Deaf signers using Pixel 4A selfie
cameras in a variety of environments. Fingerspelling recognition is an
incomplete solution that is only one small part of sign language translation,
but it could provide some immediate benefit to Deaf/Hard of Hearing signers as
more broadly capable technology develops. At >3 million characters in length
and >250 hours in duration, FSboard is the largest fingerspelling recognition
dataset to date by a factor of >10x. As a simple baseline, we finetune 30 Hz
MediaPipe Holistic landmark inputs into ByT5-Small and achieve 11.1% Character
Error Rate (CER) on a test set with unique phrases and signers. This quality
degrades gracefully when decreasing frame rate and excluding face/body
landmarks: plausible optimizations to help models run on device in real time.","[{'name': 'Manfred Georg'}, {'name': 'Garrett Tanzer'}, {'name': 'Saad Hassan'}, {'name': 'Maximus Shengelia'}, {'name': 'Esha Uboweja'}, {'name': 'Sam Sepah'}, {'name': 'Sean Forbes'}, {'name': 'Thad Starner'}]",2024-07-22T17:20:22Z
http://arxiv.org/abs/2407.15788v1,http://arxiv.org/abs/2407.15788v1,"Extracting Structured Insights from Financial News: An Augmented LLM
  Driven Approach","Financial news plays a crucial role in decision-making processes across the
financial sector, yet the efficient processing of this information into a
structured format remains challenging. This paper presents a novel approach to
financial news processing that leverages Large Language Models (LLMs) to
overcome limitations that previously prevented the extraction of structured
data from unstructured financial news. We introduce a system that extracts
relevant company tickers from raw news article content, performs sentiment
analysis at the company level, and generates summaries, all without relying on
pre-structured data feeds. Our methodology combines the generative capabilities
of LLMs, and recent prompting techniques, with a robust validation framework
that uses a tailored string similarity approach. Evaluation on a dataset of
5530 financial news articles demonstrates the effectiveness of our approach,
with 90% of articles not missing any tickers compared with current data
providers, and 22% of articles having additional relevant tickers. In addition
to this paper, the methodology has been implemented at scale with the resulting
processed data made available through a live API endpoint, which is updated in
real-time with the latest news. To the best of our knowledge, we are the first
data provider to offer granular, per-company sentiment analysis from news
articles, enhancing the depth of information available to market participants.
We also release the evaluation dataset of 5530 processed articles as a static
file, which we hope will facilitate further research leveraging financial news.","[{'name': 'Rian Dolphin'}, {'name': 'Joe Dursun'}, {'name': 'Jonathan Chow'}, {'name': 'Jarrett Blankenship'}, {'name': 'Katie Adams'}, {'name': 'Quinton Pike'}]",2024-07-22T16:47:31Z
http://arxiv.org/abs/2407.15762v1,http://arxiv.org/abs/2407.15762v1,"Conditioned Language Policy: A General Framework for Steerable
  Multi-Objective Finetuning","Reward-based finetuning is crucial for aligning language policies with
intended behaviors (e.g., creativity and safety). A key challenge here is to
develop steerable language models that trade-off multiple (conflicting)
objectives in a flexible and efficient manner. This paper presents Conditioned
Language Policy (CLP), a general framework for finetuning language models on
multiple objectives. Building on techniques from multi-task training and
parameter-efficient finetuning, CLP can learn steerable models that effectively
trade-off conflicting objectives at inference time. Notably, this does not
require training or maintaining multiple models to achieve different trade-offs
between the objectives. Through an extensive set of experiments and ablations,
we show that the CLP framework learns steerable models that outperform and
Pareto-dominate the current state-of-the-art approaches for multi-objective
finetuning.","[{'name': 'Kaiwen Wang'}, {'name': 'Rahul Kidambi'}, {'name': 'Ryan Sullivan'}, {'name': 'Alekh Agarwal'}, {'name': 'Christoph Dann'}, {'name': 'Andrea Michi'}, {'name': 'Marco Gelmi'}, {'name': 'Yunxuan Li'}, {'name': 'Raghav Gupta'}, {'name': 'Avinava Dubey'}, {'name': 'Alexandre Ramé'}, {'name': 'Johan Ferret'}, {'name': 'Geoffrey Cideron'}, {'name': 'Le Hou'}, {'name': 'Hongkun Yu'}, {'name': 'Amr Ahmed'}, {'name': 'Aranyak Mehta'}, {'name': 'Léonard Hussenot'}, {'name': 'Olivier Bachem'}, {'name': 'Edouard Leurent'}]",2024-07-22T16:13:38Z
http://arxiv.org/abs/2407.15754v1,http://arxiv.org/abs/2407.15754v1,"LongVideoBench: A Benchmark for Long-context Interleaved Video-Language
  Understanding","Large multimodal models (LMMs) are processing increasingly longer and richer
inputs. Albeit the progress, few public benchmark is available to measure such
development. To mitigate this gap, we introduce LongVideoBench, a
question-answering benchmark that features video-language interleaved inputs up
to an hour long. Our benchmark includes 3,763 varying-length web-collected
videos with their subtitles across diverse themes, designed to comprehensively
evaluate LMMs on long-term multimodal understanding. To achieve this, we
interpret the primary challenge as to accurately retrieve and reason over
detailed multimodal information from long inputs. As such, we formulate a novel
video question-answering task termed referring reasoning. Specifically, as part
of the question, it contains a referring query that references related video
contexts, called referred context. The model is then required to reason over
relevant video details from the referred context. Following the paradigm of
referring reasoning, we curate 6,678 human-annotated multiple-choice questions
in 17 fine-grained categories, establishing one of the most comprehensive
benchmarks for long-form video understanding. Evaluations suggest that the
LongVideoBench presents significant challenges even for the most advanced
proprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their
open-source counterparts show an even larger performance gap. In addition, our
results indicate that model performance on the benchmark improves only when
they are capable of processing more frames, positioning LongVideoBench as a
valuable benchmark for evaluating future-generation long-context LMMs.","[{'name': 'Haoning Wu'}, {'name': 'Dongxu Li'}, {'name': 'Bei Chen'}, {'name': 'Junnan Li'}]",2024-07-22T16:00:55Z
http://arxiv.org/abs/2407.15736v1,http://arxiv.org/abs/2407.15736v1,"OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a
  German Migration Context","When immigrating to a new country, it is easy to feel overwhelmed by the need
to obtain information on financial support, housing, schooling, language
courses, and other issues. If relocation is rushed or even forced, the
necessity for high-quality answers to such questions is all the more urgent.
Official immigration counselors are usually overbooked, and online systems
could guide newcomers to the requested information or a suitable counseling
service.
  To this end, we present OMoS-QA, a dataset of German and English questions
paired with relevant trustworthy documents and manually annotated answers,
specifically tailored to this scenario. Questions are automatically generated
with an open-source large language model (LLM) and answer sentences are
selected by crowd workers with high agreement. With our data, we conduct a
comparison of 5 pretrained LLMs on the task of extractive question answering
(QA) in German and English. Across all models and both languages, we find high
precision and low-to-mid recall in selecting answer sentences, which is a
favorable trade-off to avoid misleading users. This performance even holds up
when the question language does not match the document language. When it comes
to identifying unanswerable questions given a context, there are larger
differences between the two languages.","[{'name': 'Steffen Kleinle'}, {'name': 'Jakob Prange'}, {'name': 'Annemarie Friedrich'}]",2024-07-22T15:40:17Z
http://arxiv.org/abs/2407.15723v1,http://arxiv.org/abs/2407.15723v1,"DStruct2Design: Data and Benchmarks for Data Structure Driven Generative
  Floor Plan Design","Text conditioned generative models for images have yielded impressive
results. Text conditioned floorplan generation as a special type of raster
image generation task also received particular attention. However there are
many use cases in floorpla generation where numerical properties of the
generated result are more important than the aesthetics. For instance, one
might want to specify sizes for certain rooms in a floorplan and compare the
generated floorplan with given specifications Current approaches, datasets and
commonly used evaluations do not support these kinds of constraints. As such,
an attractive strategy is to generate an intermediate data structure that
contains numerical properties of a floorplan which can be used to generate the
final floorplan image. To explore this setting we (1) construct a new dataset
for this data-structure to data-structure formulation of floorplan generation
using two popular image based floorplan datasets RPLAN and ProcTHOR-10k, and
provide the tools to convert further procedurally generated ProcTHOR floorplan
data into our format. (2) We explore the task of floorplan generation given a
partial or complete set of constraints and we design a series of metrics and
benchmarks to enable evaluating how well samples generated from models respect
the constraints. (3) We create multiple baselines by finetuning a large
language model (LLM), Llama3, and demonstrate the feasibility of using
floorplan data structure conditioned LLMs for the problem of floorplan
generation respecting numerical constraints. We hope that our new datasets and
benchmarks will encourage further research on different ways to improve the
performance of LLMs and other generative modelling techniques for generating
designs where quantitative constraints are only partially specified, but must
be respected.","[{'name': 'Zhi Hao Luo'}, {'name': 'Luis Lara'}, {'name': 'Ge Ya Luo'}, {'name': 'Florian Golemo'}, {'name': 'Christopher Beckham'}, {'name': 'Christopher Pal'}]",2024-07-22T15:27:55Z
http://arxiv.org/abs/2407.15720v2,http://arxiv.org/abs/2407.15720v2,"Do Large Language Models Have Compositional Ability? An Investigation
  into Limitations and Scalability","Large language models (LLMs) have emerged as powerful tools for many AI
problems and exhibit remarkable in-context learning (ICL) capabilities.
Compositional ability, solving unseen complex tasks that combine two or more
simple tasks, is an essential reasoning ability for Artificial General
Intelligence. Despite the tremendous success of LLMs, how they approach
composite tasks, especially those not encountered during the pretraining phase,
remains an open and largely underexplored question. In this study, we delve
into the ICL capabilities of LLMs on composite tasks, with only simple tasks as
in-context examples. We develop a test suite of composite tasks including
linguistic and logical challenges and perform empirical studies across
different LLM families. We observe that models exhibit divergent behaviors: (1)
For simpler composite tasks that apply distinct mapping mechanisms to different
input segments, the models demonstrate decent compositional ability, while
scaling up the model enhances this ability; (2) for more complex composite
tasks involving reasoning multiple steps, where each step represents one task,
models typically underperform, and scaling up generally provides no
improvements. We offer theoretical analysis in a simplified setting, explaining
that models exhibit compositional capability when the task handles different
input parts separately. We believe our work sheds new light on the capabilities
of LLMs in solving composite tasks regarding the nature of the tasks and model
scale. Our dataset and code are available at
{\url{https://github.com/OliverXUZY/LLM_Compose}}.","[{'name': 'Zhuoyan Xu'}, {'name': 'Zhenmei Shi'}, {'name': 'Yingyu Liang'}]",2024-07-22T15:22:34Z
http://arxiv.org/abs/2407.15711v1,http://arxiv.org/abs/2407.15711v1,AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?,"Language agents, built on top of language models (LMs), are systems that can
interact with complex environments, such as the open web. In this work, we
examine whether such agents can perform realistic and time-consuming tasks on
the web, e.g., monitoring real-estate markets or locating relevant nearby
businesses. We introduce AssistantBench, a challenging new benchmark consisting
of 214 realistic tasks that can be automatically evaluated, covering different
scenarios and domains. We find that AssistantBench exposes the limitations of
current systems, including language models and retrieval-augmented language
models, as no model reaches an accuracy of more than 25 points. While
closed-book LMs perform well, they exhibit low precision since they tend to
hallucinate facts. State-of-the-art web agents reach a score of near zero.
Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly
outperforms previous agents, and an ensemble of SPA and closed-book models
reaches the best overall performance. Moreover, we analyze failures of current
systems and highlight that web navigation remains a major challenge.","[{'name': 'Ori Yoran'}, {'name': 'Samuel Joseph Amouyal'}, {'name': 'Chaitanya Malaviya'}, {'name': 'Ben Bogin'}, {'name': 'Ofir Press'}, {'name': 'Jonathan Berant'}]",2024-07-22T15:18:45Z
http://arxiv.org/abs/2407.15695v1,http://arxiv.org/abs/2407.15695v1,Supporting the Digital Autonomy of Elders Through LLM Assistance,"The internet offers tremendous access to services, social connections, and
needed products. However, to those without sufficient experience, engaging with
businesses and friends across the internet can be daunting due to the ever
present danger of scammers and thieves, to say nothing of the myriad of
potential computer viruses. Like a forest rich with both edible and poisonous
plants, those familiar with the norms inhabit it safely with ease while
newcomers need a guide. However, reliance on a human digital guide can be
taxing and often impractical. We propose and pilot a simple but unexplored
idea: could an LLM provide the necessary support to help the elderly who are
separated by the digital divide safely achieve digital autonomy?","[{'name': 'Jesse Roberts'}, {'name': 'Lindsey Roberts'}, {'name': 'Alice Reed'}]",2024-07-22T15:01:45Z
http://arxiv.org/abs/2407.15694v1,http://arxiv.org/abs/2407.15694v1,"Counter Turing Test ($CT^2$): Investigating AI-Generated Text Detection
  for Hindi -- Ranking LLMs based on Hindi AI Detectability Index ($ADI_{hi}$)","The widespread adoption of large language models (LLMs) and awareness around
multilingual LLMs have raised concerns regarding the potential risks and
repercussions linked to the misapplication of AI-generated text, necessitating
increased vigilance. While these models are primarily trained for English,
their extensive training on vast datasets covering almost the entire web,
equips them with capabilities to perform well in numerous other languages.
AI-Generated Text Detection (AGTD) has emerged as a topic that has already
received immediate attention in research, with some initial methods having been
proposed, soon followed by the emergence of techniques to bypass detection. In
this paper, we report our investigation on AGTD for an indic language Hindi.
Our major contributions are in four folds: i) examined 26 LLMs to evaluate
their proficiency in generating Hindi text, ii) introducing the AI-generated
news article in Hindi ($AG_{hi}$) dataset, iii) evaluated the effectiveness of
five recently proposed AGTD techniques: ConDA, J-Guard, RADAR, RAIDAR and
Intrinsic Dimension Estimation for detecting AI-generated Hindi text, iv)
proposed Hindi AI Detectability Index ($ADI_{hi}$) which shows a spectrum to
understand the evolving landscape of eloquence of AI-generated text in Hindi.
We will make the codes and datasets available to encourage further research.","[{'name': 'Ishan Kavathekar'}, {'name': 'Anku Rani'}, {'name': 'Ashmit Chamoli'}, {'name': 'Ponnurangam Kumaraguru'}, {'name': 'Amit Sheth'}, {'name': 'Amitava Das'}]",2024-07-22T15:00:23Z
http://arxiv.org/abs/2407.15645v1,http://arxiv.org/abs/2407.15645v1,"Psychometric Alignment: Capturing Human Knowledge Distributions via
  Language Models","Language models (LMs) are increasingly used to simulate human-like responses
in scenarios where accurately mimicking a population's behavior can guide
decision-making, such as in developing educational materials and designing
public policies. The objective of these simulations is for LMs to capture the
variations in human responses, rather than merely providing the expected
correct answers. Prior work has shown that LMs often generate unrealistically
accurate responses, but there are no established metrics to quantify how
closely the knowledge distribution of LMs aligns with that of humans. To
address this, we introduce ""psychometric alignment,"" a metric that measures the
extent to which LMs reflect human knowledge distribution. Assessing this
alignment involves collecting responses from both LMs and humans to the same
set of test items and using Item Response Theory to analyze the differences in
item functioning between the groups. We demonstrate that our metric can capture
important variations in populations that traditional metrics, like differences
in accuracy, fail to capture. We apply this metric to assess existing LMs for
their alignment with human knowledge distributions across three real-world
domains. We find significant misalignment between LMs and human populations,
though using persona-based prompts can improve alignment. Interestingly,
smaller LMs tend to achieve greater psychometric alignment than larger LMs.
Further, training LMs on human response data from the target distribution
enhances their psychometric alignment on unseen test items, but the
effectiveness of such training varies across domains.","[{'name': 'Joy He-Yueya'}, {'name': 'Wanjing Anya Ma'}, {'name': 'Kanishk Gandhi'}, {'name': 'Benjamin W. Domingue'}, {'name': 'Emma Brunskill'}, {'name': 'Noah D. Goodman'}]",2024-07-22T14:02:59Z
http://arxiv.org/abs/2407.15621v1,http://arxiv.org/abs/2407.15621v1,"RadioRAG: Factual Large Language Models for Enhanced Diagnostics in
  Radiology Using Dynamic Retrieval Augmented Generation","Large language models (LLMs) have advanced the field of artificial
intelligence (AI) in medicine. However LLMs often generate outdated or
inaccurate information based on static training datasets. Retrieval augmented
generation (RAG) mitigates this by integrating outside data sources. While
previous RAG systems used pre-assembled, fixed databases with limited
flexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end
framework that retrieves data from authoritative radiologic online sources in
real-time. RadioRAG is evaluated using a dedicated radiologic
question-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of
various LLMs when answering radiology-specific questions with and without
access to additional online information via RAG. Using 80 questions from RSNA
Case Collection across radiologic subspecialties and 24 additional
expert-curated questions, for which the correct gold-standard answers were
available, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B
and 70B]) were prompted with and without RadioRAG. RadioRAG retrieved
context-specific information from www.radiopaedia.org in real-time and
incorporated them into its reply. RadioRAG consistently improved diagnostic
accuracy across all LLMs, with relative improvements ranging from 2% to 54%. It
matched or exceeded question answering without RAG across radiologic
subspecialties, particularly in breast imaging and emergency radiology.
However, degree of improvement varied among models; GPT-3.5-turbo and
Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2
showed no improvement, highlighting variability in its effectiveness. LLMs
benefit when provided access to domain-specific data beyond their training
data. For radiology, RadioRAG establishes a robust framework that substantially
improves diagnostic accuracy and factuality in radiological question answering.","[{'name': 'Soroosh Tayebi Arasteh'}, {'name': 'Mahshad Lotfinia'}, {'name': 'Keno Bressem'}, {'name': 'Robert Siepmann'}, {'name': 'Dyke Ferber'}, {'name': 'Christiane Kuhl'}, {'name': 'Jakob Nikolas Kather'}, {'name': 'Sven Nebelung'}, {'name': 'Daniel Truhn'}]",2024-07-22T13:29:56Z
http://arxiv.org/abs/2407.15612v2,http://arxiv.org/abs/2407.15612v2,Can GPT-4 learn to analyze moves in research article abstracts?,"One of the most powerful and enduring ideas in written discourse analysis is
that genres can be described in terms of the moves which structure a writer's
purpose. Considerable research has sought to identify these distinct
communicative acts, but analyses have been beset by problems of subjectivity,
reliability and the time-consuming need for multiple coders to confirm
analyses. In this paper we employ the affordances of GPT-4 to automate the
annotation process by using natural language prompts. Focusing on abstracts
from articles in four applied linguistics journals, we devise prompts which
enable the model to identify moves effectively. The annotated outputs of these
prompts were evaluated by two assessors with a third addressing disagreements.
The results show that an 8-shot prompt was more effective than one using two,
confirming that the inclusion of examples illustrating areas of variability can
enhance GPT-4's ability to recognize multiple moves in a single sentence and
reduce bias related to textual position. We suggest that GPT-4 offers
considerable potential in automating this annotation process, when human actors
with domain specific linguistic expertise inform the prompting process.","[{'name': 'Danni Yu'}, {'name': 'Marina Bondi'}, {'name': 'Ken Hyland'}]",2024-07-22T13:14:27Z
http://arxiv.org/abs/2407.15608v1,http://arxiv.org/abs/2407.15608v1,"StylusAI: Stylistic Adaptation for Robust German Handwritten Text
  Generation","In this study, we introduce StylusAI, a novel architecture leveraging
diffusion models in the domain of handwriting style generation. StylusAI is
specifically designed to adapt and integrate the stylistic nuances of one
language's handwriting into another, particularly focusing on blending English
handwriting styles into the context of the German writing system. This approach
enables the generation of German text in English handwriting styles and German
handwriting styles into English, enriching machine-generated handwriting
diversity while ensuring that the generated text remains legible across both
languages. To support the development and evaluation of StylusAI, we present
the \lq{Deutscher Handschriften-Datensatz}\rq~(DHSD), a comprehensive dataset
encompassing 37 distinct handwriting styles within the German language. This
dataset provides a fundamental resource for training and benchmarking in the
realm of handwritten text generation. Our results demonstrate that StylusAI not
only introduces a new method for style adaptation in handwritten text
generation but also surpasses existing models in generating handwriting samples
that improve both text quality and stylistic fidelity, evidenced by its
performance on the IAM database and our newly proposed DHSD. Thus, StylusAI
represents a significant advancement in the field of handwriting style
generation, offering promising avenues for future research and applications in
cross-linguistic style adaptation for languages with similar scripts.","[{'name': 'Nauman Riaz'}, {'name': 'Saifullah Saifullah'}, {'name': 'Stefan Agne'}, {'name': 'Andreas Dengel'}, {'name': 'Sheraz Ahmed'}]",2024-07-22T13:08:30Z
http://arxiv.org/abs/2407.21045v1,http://arxiv.org/abs/2407.21045v1,"Unlocking the Potential: Benchmarking Large Language Models in Water
  Engineering and Research","Recent advancements in Large Language Models (LLMs) have sparked interest in
their potential applications across various fields. This paper embarked on a
pivotal inquiry: Can existing LLMs effectively serve as ""water expert models""
for water engineering and research tasks? This study was the first to evaluate
LLMs' contributions across various water engineering and research tasks by
establishing a domain-specific benchmark suite, namely, WaterER. Herein, we
prepared 983 tasks related to water engineering and research, categorized into
""wastewater treatment"", ""environmental restoration"", ""drinking water treatment
and distribution"", ""sanitation"", ""anaerobic digestion"" and ""contaminants
assessment"". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5,
Gemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the
strengths of GPT-4 in handling diverse and complex tasks of water engineering
and water research, the specialized capabilities of Gemini in academic
contexts, Llama3's strongest capacity to answer Chinese water engineering
questions and the competitive performance of Chinese-oriented models like
GLM-4, ERNIE and QWEN in some water engineering tasks. More specifically,
current LLMs excelled particularly in generating precise research gaps for
papers on ""contaminants and related water quality monitoring and assessment"".
Additionally, they were more adept at creating appropriate titles for research
papers on ""treatment processes for wastewaters"", ""environmental restoration"",
and ""drinking water treatment"". Overall, this study pioneered evaluating LLMs
in water engineering and research by introducing the WaterER benchmark to
assess the trustworthiness of their predictions. This standardized evaluation
framework would also drive future advancements in LLM technology by using
targeting datasets, propelling these models towards becoming true ""water
expert"".","[{'name': 'Boyan Xu'}, {'name': 'Liang Wen'}, {'name': 'Zihao Li'}, {'name': 'Yuxing Yang'}, {'name': 'Guanlan Wu'}, {'name': 'Xiongpeng Tang'}, {'name': 'Yu Li'}, {'name': 'Zihao Wu'}, {'name': 'Qingxian Su'}, {'name': 'Xueqing Shi'}, {'name': 'Yue Yang'}, {'name': 'Rui Tong'}, {'name': 'How Yong Ng'}]",2024-07-22T12:32:22Z
http://arxiv.org/abs/2407.15588v2,http://arxiv.org/abs/2407.15588v2,"Unsupervised Robust Cross-Lingual Entity Alignment via Neighbor Triple
  Matching with Entity and Relation Texts","Cross-lingual entity alignment (EA) enables the integration of multiple
knowledge graphs (KGs) across different languages, providing users with
seamless access to diverse and comprehensive knowledge. Existing methods,
mostly supervised, face challenges in obtaining labeled entity pairs. To
address this, recent studies have shifted towards self-supervised and
unsupervised frameworks. Despite their effectiveness, these approaches have
limitations: (1) Relation passing: mainly focusing on the entity while
neglecting the semantic information of relations, (2) Isomorphic assumption:
assuming isomorphism between source and target graphs, which leads to noise and
reduced alignment accuracy, and (3) Noise vulnerability: susceptible to noise
in the textual features, especially when encountering inconsistent translations
or Out-Of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an
unsupervised and robust cross-lingual EA pipeline that jointly performs
Entity-level and Relation-level Alignment by neighbor triple matching strategy
using semantic textual features of relations and entities. Its refinement step
iteratively enhances results by fusing entity-level and relation-level
alignments based on neighbor triple matching. The additional verification step
examines the entities' neighbor triples as the linearized text. This
Align-then-Verify pipeline rigorously assesses alignment results, achieving
near-perfect alignment even in the presence of noisy textual features of
entities. Our extensive experiments demonstrate that the robustness and general
applicability of ERAlign improved the accuracy and effectiveness of EA tasks,
contributing significantly to knowledge-oriented applications.","[{'name': 'Soojin Yoon'}, {'name': 'Sungho Ko'}, {'name': 'Tongyoung Kim'}, {'name': 'SeongKu Kang'}, {'name': 'Jinyoung Yeo'}, {'name': 'Dongha Lee'}]",2024-07-22T12:25:48Z
http://arxiv.org/abs/2407.15508v2,http://arxiv.org/abs/2407.15508v2,"Compensate Quantization Errors+: Quantized Models Are Inquisitive
  Learners","Large Language Models (LLMs) showcase remarkable performance and robust
deductive capabilities, yet their expansive size complicates deployment and
raises environmental concerns due to substantial resource consumption. The
recent development of a quantization technique known as Learnable
Singular-value Increment (LSI) has addressed some of these quantization
challenges. Leveraging insights from LSI and our extensive research, we have
developed innovative methods that enhance the performance of quantized LLMs,
particularly in low-bit settings. Our methods consistently deliver
state-of-the-art results across various quantization scenarios and offer deep
theoretical insights into the quantization process, elucidating the potential
of quantized models for widespread application.","[{'name': 'Yifei Gao'}, {'name': 'Jie Ou'}, {'name': 'Lei Wang'}, {'name': 'Fanhua Shang'}, {'name': 'Jaji Wu'}, {'name': 'Jun Cheng'}]",2024-07-22T09:45:16Z
http://arxiv.org/abs/2407.15504v1,http://arxiv.org/abs/2407.15504v1,"Fundamental Limits of Prompt Compression: A Rate-Distortion Framework
  for Black-Box Language Models","We formalize the problem of prompt compression for large language models
(LLMs) and present a framework to unify token-level prompt compression methods
which create hard prompts for black-box models. We derive the distortion-rate
function for this setup as a linear program, and provide an efficient algorithm
to compute this fundamental limit via the dual of the linear program. Using the
distortion-rate function as the baseline, we study the performance of existing
compression schemes on a synthetic dataset consisting of prompts generated from
a Markov chain, natural language queries, and their respective answers. Our
empirical analysis demonstrates the criticality of query-aware prompt
compression, where the compressor has knowledge of the downstream task/query
for the black-box LLM. We show that there is a large gap between the
performance of current prompt compression methods and the optimal strategy, and
propose a query-aware, variable-rate adaptation of a prior work to close the
gap. We extend our experiments to a small natural language dataset to further
confirm our findings on our synthetic dataset.","[{'name': 'Adway Girish'}, {'name': 'Alliot Nagle'}, {'name': 'Marco Bondaschi'}, {'name': 'Michael Gastpar'}, {'name': 'Ashok Vardhan Makkuva'}, {'name': 'Hyeji Kim'}]",2024-07-22T09:40:13Z
http://arxiv.org/abs/2407.15498v1,http://arxiv.org/abs/2407.15498v1,"Refining Corpora from a Model Calibration Perspective for Chinese
  Spelling Correction","Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality
corpora, due to the labor-intensive labeling of spelling errors in real-life
human writing or typing scenarios. Two data augmentation methods are widely
adopted: (1) \textit{Random Replacement} with the guidance of confusion sets
and (2) \textit{OCR/ASR-based Generation} that simulates character misusing.
However, both methods inevitably introduce noisy data (e.g., false spelling
errors), potentially leading to over-correction. By carefully analyzing the two
types of corpora, we find that though the latter achieves more robust
generalization performance, the former yields better-calibrated CSC models. We
then provide a theoretical analysis of this empirical observation, based on
which a corpus refining strategy is proposed. Specifically, OCR/ASR-based data
samples are fed into a well-calibrated CSC model trained on random
replacement-based corpora and then filtered based on prediction confidence. By
learning a simple BERT-based model on the refined OCR/ASR-based corpus, we set
up impressive state-of-the-art performance on three widely-used benchmarks,
while significantly alleviating over-correction (e.g., lowering false positive
predictions).","[{'name': 'Dingyao Yu'}, {'name': 'Yang An'}, {'name': 'Wei Ye'}, {'name': 'Xiongfeng Xiao'}, {'name': 'Shaoguang Mao'}, {'name': 'Tao Ge'}, {'name': 'Shikun Zhang'}]",2024-07-22T09:26:35Z
http://arxiv.org/abs/2407.15489v1,http://arxiv.org/abs/2407.15489v1,"Two Stacks Are Better Than One: A Comparison of Language Modeling and
  Translation as Multilingual Pretraining Objectives","Pretrained language models (PLMs) display impressive performances and have
captured the attention of the NLP community. Establishing the best practices in
pretraining has therefore become a major point of focus for much of NLP
research -- especially since the insights developed for monolingual English
models need not carry to more complex multilingual. One significant caveat of
the current state of the art is that different works are rarely comparable:
they often discuss different parameter counts, training data, and evaluation
methodology.
  This paper proposes a comparison of multilingual pretraining objectives in a
controlled methodological environment. We ensure that training data and model
architectures are comparable, and discuss the downstream performances across 6
languages that we observe in probing and fine-tuning scenarios. We make two key
observations: (1) the architecture dictates which pretraining objective is
optimal; (2) multilingual translation is a very effective pre-training
objective under the right conditions. We make our code, data, and model weights
available at \texttt{\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.","[{'name': 'Zihao Li'}, {'name': 'Shaoxiong Ji'}, {'name': 'Timothee Mickus'}, {'name': 'Vincent Segonne'}, {'name': 'Jörg Tiedemann'}]",2024-07-22T09:16:30Z
http://arxiv.org/abs/2407.15459v1,http://arxiv.org/abs/2407.15459v1,"Text-to-Battery Recipe: A language modeling-based protocol for automatic
  battery recipe extraction and retrieval","Recent studies have increasingly applied natural language processing (NLP) to
automatically extract experimental research data from the extensive battery
materials literature. Despite the complex process involved in battery
manufacturing -- from material synthesis to cell assembly -- there has been no
comprehensive study systematically organizing this information. In response, we
propose a language modeling-based protocol, Text-to-Battery Recipe (T2BR), for
the automatic extraction of end-to-end battery recipes, validated using a case
study on batteries containing LiFePO4 cathode material. We report machine
learning-based paper filtering models, screening 2,174 relevant papers from the
keyword-based search results, and unsupervised topic models to identify 2,876
paragraphs related to cathode synthesis and 2,958 paragraphs related to cell
assembly. Then, focusing on the two topics, two deep learning-based named
entity recognition models are developed to extract a total of 30 entities --
including precursors, active materials, and synthesis methods -- achieving F1
scores of 88.18% and 94.61%. The accurate extraction of entities enables the
systematic generation of 165 end-toend recipes of LiFePO4 batteries. Our
protocol and results offer valuable insights into specific trends, such as
associations between precursor materials and synthesis methods, or combinations
between different precursor materials. We anticipate that our findings will
serve as a foundational knowledge base for facilitating battery-recipe
information retrieval. The proposed protocol will significantly accelerate the
review of battery material literature and catalyze innovations in battery
design and development.","[{'name': 'Daeun Lee'}, {'name': 'Jaewoong Choi'}, {'name': 'Hiroshi Mizuseki'}, {'name': 'Byungju Lee'}]",2024-07-22T08:15:02Z
http://arxiv.org/abs/2407.15441v1,http://arxiv.org/abs/2407.15441v1,"Developing a Reliable, General-Purpose Hallucination Detection and
  Mitigation Service: Insights and Lessons Learned","Hallucination, a phenomenon where large language models (LLMs) produce output
that is factually incorrect or unrelated to the input, is a major challenge for
LLM applications that require accuracy and dependability. In this paper, we
introduce a reliable and high-speed production system aimed at detecting and
rectifying the hallucination issue within LLMs. Our system encompasses named
entity recognition (NER), natural language inference (NLI), span-based
detection (SBD), and an intricate decision tree-based process to reliably
detect a wide range of hallucinations in LLM responses. Furthermore, our team
has crafted a rewriting mechanism that maintains an optimal mix of precision,
response time, and cost-effectiveness. We detail the core elements of our
framework and underscore the paramount challenges tied to response time,
availability, and performance metrics, which are crucial for real-world
deployment of these technologies. Our extensive evaluation, utilizing offline
data and live production traffic, confirms the efficacy of our proposed
framework and service.","[{'name': 'Song Wang'}, {'name': 'Xun Wang'}, {'name': 'Jie Mei'}, {'name': 'Yujia Xie'}, {'name': 'Sean Muarray'}, {'name': 'Zhang Li'}, {'name': 'Lingfeng Wu'}, {'name': 'Si-Qing Chen'}, {'name': 'Wayne Xiong'}]",2024-07-22T07:48:30Z
http://arxiv.org/abs/2407.15425v2,http://arxiv.org/abs/2407.15425v2,Empirical Capacity Model for Self-Attention Neural Networks,"Large pretrained self-attention neural networks, or transformers, have been
very successful in various tasks recently. The performance of a model on a
given task depends on its ability to memorize and generalize the training data.
Large transformer models, which may have billions of parameters, in theory have
a huge capacity to memorize content. However, the current algorithms for the
optimization fall short of the theoretical capacity, and the capacity is also
highly dependent on the content. In this paper, we focus on the memory capacity
of these models obtained using common training algorithms and synthetic
training data. Based on the results, we derive an empirical capacity model
(ECM) for a generic transformer. The ECM can be used to design task-specific
transformer models with an optimal number of parameters in cases where the
target memorization capability of the task can be defined.","[{'name': 'Aki Härmä'}, {'name': 'Marcin Pietrasik'}, {'name': 'Anna Wilbik'}]",2024-07-22T07:02:15Z
http://arxiv.org/abs/2407.15415v1,http://arxiv.org/abs/2407.15415v1,"LLaST: Improved End-to-end Speech Translation System Leveraged by Large
  Language Models","We introduces LLaST, a framework for building high-performance Large Language
model based Speech-to-text Translation systems. We address the limitations of
end-to-end speech translation(E2E ST) models by exploring model architecture
design and optimization techniques tailored for LLMs. Our approach includes
LLM-based speech translation architecture design, ASR-augmented training,
multilingual data augmentation, and dual-LoRA optimization. Our approach
demonstrates superior performance on the CoVoST-2 benchmark and showcases
exceptional scaling capabilities powered by LLMs. We believe this effective
method will serve as a strong baseline for speech translation and provide
insights for future improvements of the LLM-based speech translation framework.
We release the data, code and models in https://github.com/openaudiolab/LLaST.","[{'name': 'Xi Chen'}, {'name': 'Songyang Zhang'}, {'name': 'Qibing Bai'}, {'name': 'Kai Chen'}, {'name': 'Satoshi Nakamura'}]",2024-07-22T06:42:00Z
http://arxiv.org/abs/2408.03945v1,http://arxiv.org/abs/2408.03945v1,"Impacts of Anthropomorphizing Large Language Models in Learning
  Environments","Large Language Models (LLMs) are increasingly being used in learning
environments to support teaching-be it as learning companions or as tutors.
With our contribution, we aim to discuss the implications of the
anthropomorphization of LLMs in learning environments on educational theory to
build a foundation for more effective learning outcomes and understand their
emotional impact on learners. According to the media equation, people tend to
respond to media in the same way as they would respond to another person. A
study conducted by the Georgia Institute of Technology showed that chatbots can
be successfully implemented in learning environments. In this study, learners
in selected online courses were unable to distinguish the chatbot from a ""real""
teacher. As LLM-based chatbots such as OpenAI's GPT series are increasingly
used in educational tools, it is important to understand how the attribution
processes to LLM-based chatbots in terms of anthropomorphization affect
learners' emotions.","[{'name': 'Kristina Schaaff'}, {'name': 'Marc-André Heidelmann'}]",2024-07-22T06:28:54Z
http://arxiv.org/abs/2407.15017v2,http://arxiv.org/abs/2407.15017v2,Knowledge Mechanisms in Large Language Models: A Survey and Perspective,"Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial
for advancing towards trustworthy AGI. This paper reviews knowledge mechanism
analysis from a novel taxonomy including knowledge utilization and evolution.
Knowledge utilization delves into the mechanism of memorization, comprehension
and application, and creation. Knowledge evolution focuses on the dynamic
progression of knowledge within individual and group LLMs. Moreover, we discuss
what knowledge LLMs have learned, the reasons for the fragility of parametric
knowledge, and the potential dark knowledge (hypothesis) that will be
challenging to address. We hope this work can help understand knowledge in LLMs
and provide insights for future research.","[{'name': 'Mengru Wang'}, {'name': 'Yunzhi Yao'}, {'name': 'Ziwen Xu'}, {'name': 'Shuofei Qiao'}, {'name': 'Shumin Deng'}, {'name': 'Peng Wang'}, {'name': 'Xiang Chen'}, {'name': 'Jia-Chen Gu'}, {'name': 'Yong Jiang'}, {'name': 'Pengjun Xie'}, {'name': 'Fei Huang'}, {'name': 'Huajun Chen'}, {'name': 'Ningyu Zhang'}]",2024-07-22T06:15:59Z
http://arxiv.org/abs/2407.15399v1,http://arxiv.org/abs/2407.15399v1,"Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned
  Large Language Models","With the development of large language models (LLMs) like ChatGPT, both their
vast applications and potential vulnerabilities have come to the forefront.
While developers have integrated multiple safety mechanisms to mitigate their
misuse, a risk remains, particularly when models encounter adversarial inputs.
This study unveils an attack mechanism that capitalizes on human conversation
strategies to extract harmful information from LLMs. We delineate three pivotal
strategies: (i) decomposing malicious questions into seemingly innocent
sub-questions; (ii) rewriting overtly malicious questions into more covert,
benign-sounding ones; (iii) enhancing the harmfulness of responses by prompting
models for illustrative examples. Unlike conventional methods that target
explicit malicious responses, our approach delves deeper into the nature of the
information provided in responses. Through our experiments conducted on
GPT-3.5-turbo, GPT-4, and Llama2, our method has demonstrated a marked efficacy
compared to conventional attack methods. In summary, this work introduces a
novel attack method that outperforms previous approaches, raising an important
question: How to discern whether the ultimate intent in a dialogue is
malicious?","[{'name': 'Xiao Liu'}, {'name': 'Liangzhi Li'}, {'name': 'Tong Xiang'}, {'name': 'Fuying Ye'}, {'name': 'Lu Wei'}, {'name': 'Wangyue Li'}, {'name': 'Noa Garcia'}]",2024-07-22T06:04:29Z
http://arxiv.org/abs/2407.15390v1,http://arxiv.org/abs/2407.15390v1,ALLaM: Large Language Models for Arabic and English,"We present ALLaM: Arabic Large Language Model, a series of large language
models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is
carefully trained considering the values of language alignment and knowledge
transfer at scale. Our autoregressive decoder-only architecture models
demonstrate how second-language acquisition via vocabulary expansion and
pretraining on a mixture of Arabic and English text can steer a model towards a
new language (Arabic) without any catastrophic forgetting in the original
language (English). Furthermore, we highlight the effectiveness of using
parallel/translated data to aid the process of knowledge alignment between
languages. Finally, we show that extensive alignment with human preferences can
significantly enhance the performance of a language model compared to models of
a larger scale with lower quality alignment. ALLaM achieves state-of-the-art
performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and
Arabic Exams. Our aligned models improve both in Arabic and English from their
base aligned models.","[{'name': 'M Saiful Bari'}, {'name': 'Yazeed Alnumay'}, {'name': 'Norah A. Alzahrani'}, {'name': 'Nouf M. Alotaibi'}, {'name': 'Hisham A. Alyahya'}, {'name': 'Sultan AlRashed'}, {'name': 'Faisal A. Mirza'}, {'name': 'Shaykhah Z. Alsubaie'}, {'name': 'Hassan A. Alahmed'}, {'name': 'Ghadah Alabduljabbar'}, {'name': 'Raghad Alkhathran'}, {'name': 'Yousef Almushayqih'}, {'name': 'Raneem Alnajim'}, {'name': 'Salman Alsubaihi'}, {'name': 'Maryam Al Mansour'}, {'name': 'Majed Alrubaian'}, {'name': 'Ali Alammari'}, {'name': 'Zaki Alawami'}, {'name': 'Abdulmohsen Al-Thubaity'}, {'name': 'Ahmed Abdelali'}, {'name': 'Jeril Kuriakose'}, {'name': 'Abdalghani Abujabal'}, {'name': 'Nora Al-Twairesh'}, {'name': 'Areeb Alowisheq'}, {'name': 'Haidar Khan'}]",2024-07-22T05:35:17Z
http://arxiv.org/abs/2407.15375v1,http://arxiv.org/abs/2407.15375v1,"The Development of a Comprehensive Spanish Dictionary for Phonetic and
  Lexical Tagging in Socio-phonetic Research (ESPADA)","Pronunciation dictionaries are an important component in the process of
speech forced alignment. The accuracy of these dictionaries has a strong effect
on the aligned speech data since they help the mapping between orthographic
transcriptions and acoustic signals. In this paper, I present the creation of a
comprehensive pronunciation dictionary in Spanish (ESPADA) that can be used in
most of the dialect variants of Spanish data. Current dictionaries focus on
specific regional variants, but with the flexible nature of our tool, it can be
readily applied to capture the most common phonetic differences across major
dialectal variants. We propose improvements to current pronunciation
dictionaries as well as mapping other relevant annotations such as
morphological and lexical information. In terms of size, it is currently the
most complete dictionary with more than 628,000 entries, representing words
from 16 countries. All entries come with their corresponding pronunciations,
morphological and lexical tagging, and other relevant information for phonetic
analysis: stress patterns, phonotactics, IPA transcriptions, and more. This
aims to equip socio-phonetic researchers with a complete open-source tool that
enhances dialectal research within socio-phonetic frameworks in the Spanish
language.",[{'name': 'Simon Gonzalez'}],2024-07-22T04:51:33Z
http://arxiv.org/abs/2407.15374v1,http://arxiv.org/abs/2407.15374v1,"ILiAD: An Interactive Corpus for Linguistic Annotated Data from Twitter
  Posts","Social Media platforms have offered invaluable opportunities for linguistic
research. The availability of up-to-date data, coming from any part in the
world, and coming from natural contexts, has allowed researchers to study
language in real time. One of the fields that has made great use of social
media platforms is Corpus Linguistics. There is currently a wide range of
projects which have been able to successfully create corpora from social media.
In this paper, we present the development and deployment of a linguistic corpus
from Twitter posts in English, coming from 26 news agencies and 27 individuals.
The main goal was to create a fully annotated English corpus for linguistic
analysis. We include information on morphology and syntax, as well as NLP
features such as tokenization, lemmas, and n- grams. The information is
presented through a range of powerful visualisations for users to explore
linguistic patterns in the corpus. With this tool, we aim to contribute to the
area of language technologies applied to linguistic research.",[{'name': 'Simon Gonzalez'}],2024-07-22T04:48:04Z
http://arxiv.org/abs/2407.15359v1,http://arxiv.org/abs/2407.15359v1,"UF-HOBI at ""Discharge Me!"": A Hybrid Solution for Discharge Summary
  Generation Through Prompt-based Tuning of GatorTronGPT Models","Automatic generation of discharge summaries presents significant challenges
due to the length of clinical documentation, the dispersed nature of patient
information, and the diverse terminology used in healthcare. This paper
presents a hybrid solution for generating discharge summary sections as part of
our participation in the ""Discharge Me!"" Challenge at the BioNLP 2024 Shared
Task. We developed a two-stage generation method using both extractive and
abstractive techniques, in which we first apply name entity recognition (NER)
to extract key clinical concepts, which are then used as input for a
prompt-tuning-based GatorTronGPT model to generate coherent text for two
important sections including ""Brief Hospital Course"" and ""Discharge
Instructions"". Our system was ranked 5th in this challenge, achieving an
overall score of 0.284. The results demonstrate the effectiveness of our hybrid
solution in improving the quality of automated discharge section generation.","[{'name': 'Mengxian Lyu'}, {'name': 'Cheng Peng'}, {'name': 'Daniel Paredes'}, {'name': 'Ziyi Chen'}, {'name': 'Aokun Chen'}, {'name': 'Jiang Bian'}, {'name': 'Yonghui Wu'}]",2024-07-22T04:02:45Z
http://arxiv.org/abs/2407.15353v2,http://arxiv.org/abs/2407.15353v2,"Customized Retrieval Augmented Generation and Benchmarking for EDA Tool
  Documentation QA","Retrieval augmented generation (RAG) enhances the accuracy and reliability of
generative AI models by sourcing factual information from external databases,
which is extensively employed in document-grounded question-answering (QA)
tasks. Off-the-shelf RAG flows are well pretrained on general-purpose
documents, yet they encounter significant challenges when being applied to
knowledge-intensive vertical domains, such as electronic design automation
(EDA). This paper addresses such issue by proposing a customized RAG framework
along with three domain-specific techniques for EDA tool documentation QA,
including a contrastive learning scheme for text embedding model fine-tuning, a
reranker distilled from proprietary LLM, and a generative LLM fine-tuned with
high-quality domain corpus. Furthermore, we have developed and released a
documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced
RTL-to-GDSII design platform. Experimental results demonstrate that our
proposed RAG flow and techniques have achieved superior performance on ORD-QA
as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA
benchmark and the training dataset for our customized RAG flow are open-source
at https://github.com/lesliepy99/RAG-EDA.","[{'name': 'Yuan Pu'}, {'name': 'Zhuolun He'}, {'name': 'Tairu Qiu'}, {'name': 'Haoyuan Wu'}, {'name': 'Bei Yu'}]",2024-07-22T03:44:27Z
http://arxiv.org/abs/2407.15352v1,http://arxiv.org/abs/2407.15352v1,MAVEN-Fact: A Large-scale Event Factuality Detection Dataset,"Event Factuality Detection (EFD) task determines the factuality of textual
events, i.e., classifying whether an event is a fact, possibility, or
impossibility, which is essential for faithfully understanding and utilizing
event knowledge. However, due to the lack of high-quality large-scale data,
event factuality detection is under-explored in event understanding research,
which limits the development of EFD community. To address these issues and
provide faithful event understanding, we introduce MAVEN-Fact, a large-scale
and high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes
factuality annotations of 112,276 events, making it the largest EFD dataset.
Extensive experiments demonstrate that MAVEN-Fact is challenging for both
conventional fine-tuned models and large language models (LLMs). Thanks to the
comprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact
also supports some further analyses and we find that adopting event arguments
and relations helps in event factuality detection for fine-tuned models but
does not benefit LLMs. Furthermore, we preliminarily study an application case
of event factuality detection and find it helps in mitigating event-related
hallucination in LLMs. Our dataset and codes can be obtained from
\url{https://github.com/lcy2723/MAVEN-FACT}","[{'name': 'Chunyang Li'}, {'name': 'Hao Peng'}, {'name': 'Xiaozhi Wang'}, {'name': 'Yunjia Qi'}, {'name': 'Lei Hou'}, {'name': 'Bin Xu'}, {'name': 'Juanzi Li'}]",2024-07-22T03:43:46Z
http://arxiv.org/abs/2407.15351v2,http://arxiv.org/abs/2407.15351v2,"LLMExplainer: Large Language Model based Bayesian Inference for Graph
  Explanation Generation","Recent studies seek to provide Graph Neural Network (GNN) interpretability
via multiple unsupervised learning models. Due to the scarcity of datasets,
current methods easily suffer from learning bias. To solve this problem, we
embed a Large Language Model (LLM) as knowledge into the GNN explanation
network to avoid the learning bias problem. We inject LLM as a Bayesian
Inference (BI) module to mitigate learning bias. The efficacy of the BI module
has been proven both theoretically and experimentally. We conduct experiments
on both synthetic and real-world datasets. The innovation of our work lies in
two parts: 1. We provide a novel view of the possibility of an LLM functioning
as a Bayesian inference to improve the performance of existing algorithms; 2.
We are the first to discuss the learning bias issues in the GNN explanation
problem.","[{'name': 'Jiaxing Zhang'}, {'name': 'Jiayi Liu'}, {'name': 'Dongsheng Luo'}, {'name': 'Jennifer Neville'}, {'name': 'Hua Wei'}]",2024-07-22T03:36:38Z
http://arxiv.org/abs/2407.15346v1,http://arxiv.org/abs/2407.15346v1,"Knowledge Acquisition Disentanglement for Knowledge-based Visual
  Question Answering with Large Language Models","Knowledge-based Visual Question Answering (KVQA) requires both image and
world knowledge to answer questions. Current methods first retrieve knowledge
from the image and external knowledge base with the original complex question,
then generate answers with Large Language Models (LLMs). However, since the
original question contains complex elements that require knowledge from
different sources, acquiring different kinds of knowledge in a coupled manner
may confuse models and hinder them from retrieving precise knowledge.
Furthermore, the ``forward-only'' answering process fails to explicitly capture
the knowledge needs of LLMs, which can further hurt answering quality. To cope
with the above limitations, we propose DKA: Disentangled Knowledge Acquisition
from LLM feedback, a training-free framework that disentangles knowledge
acquisition to avoid confusion and uses LLM's feedback to specify the required
knowledge. Specifically, DKA requires LLMs to specify what knowledge they need
to answer the question and decompose the original complex question into two
simple sub-questions: Image-based sub-question and Knowledge-based
sub-question. Then we use the two sub-questions to retrieve knowledge from the
image and knowledge base, respectively. In this way, two knowledge acquisition
models can focus on the content that corresponds to them and avoid disturbance
of irrelevant elements in the original complex question, which can help to
provide more precise knowledge and better align the knowledge needs of LLMs to
yield correct answers. Experiments on benchmark datasets show that DKA
significantly outperforms SOTA models. To facilitate future research, our data
and code are available at \url{https://github.com/Lackel/DKA}.","[{'name': 'Wenbin An'}, {'name': 'Feng Tian'}, {'name': 'Jiahao Nie'}, {'name': 'Wenkai Shi'}, {'name': 'Haonan Lin'}, {'name': 'Yan Chen'}, {'name': 'QianYing Wang'}, {'name': 'Yaqiang Wu'}, {'name': 'Guang Dai'}, {'name': 'Ping Chen'}]",2024-07-22T03:05:32Z
http://arxiv.org/abs/2407.15343v1,http://arxiv.org/abs/2407.15343v1,Improving Minimum Bayes Risk Decoding with Multi-Prompt,"While instruction fine-tuned LLMs are effective text generators, sensitivity
to prompt construction makes performance unstable and sub-optimal in practice.
Relying on a single ""best"" prompt cannot capture all differing approaches to a
generation problem. Using this observation, we propose multi-prompt decoding,
where many candidate generations are decoded from a prompt bank at
inference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR)
decoding, which selects a final output using a trained value metric. We show
multi-prompt improves MBR across a comprehensive set of conditional generation
tasks, and show this is a result of estimating a more diverse and higher
quality candidate space than that of a single prompt. Further experiments
confirm multi-prompt improves generation across tasks, models and metrics.","[{'name': 'David Heineman'}, {'name': 'Yao Dou'}, {'name': 'Wei Xu'}]",2024-07-22T02:57:10Z
http://arxiv.org/abs/2407.15341v1,http://arxiv.org/abs/2407.15341v1,"ZZU-NLP at SIGHAN-2024 dimABSA Task: Aspect-Based Sentiment Analysis
  with Coarse-to-Fine In-context Learning","The DimABSA task requires fine-grained sentiment intensity prediction for
restaurant reviews, including scores for Valence and Arousal dimensions for
each Aspect Term. In this study, we propose a Coarse-to-Fine In-context
Learning(CFICL) method based on the Baichuan2-7B model for the DimABSA task in
the SIGHAN 2024 workshop. Our method improves prediction accuracy through a
two-stage optimization process. In the first stage, we use fixed in-context
examples and prompt templates to enhance the model's sentiment recognition
capability and provide initial predictions for the test data. In the second
stage, we encode the Opinion field using BERT and select the most similar
training data as new in-context examples based on similarity. These examples
include the Opinion field and its scores, as well as related opinion words and
their average scores. By filtering for sentiment polarity, we ensure that the
examples are consistent with the test data. Our method significantly improves
prediction accuracy and consistency by effectively utilizing training data and
optimizing in-context examples, as validated by experimental results.","[{'name': 'Senbin Zhu'}, {'name': 'Hanjie Zhao'}, {'name': 'Xingren Wang'}, {'name': 'Shanhong Liu'}, {'name': 'Yuxiang Jia'}, {'name': 'Hongying Zan'}]",2024-07-22T02:54:46Z
http://arxiv.org/abs/2407.15339v1,http://arxiv.org/abs/2407.15339v1,Deep Learning for Economists,"Deep learning provides powerful methods to impute structured information from
large-scale, unstructured text and image datasets. For example, economists
might wish to detect the presence of economic activity in satellite images, or
to measure the topics or entities mentioned in social media, the congressional
record, or firm filings. This review introduces deep neural networks, covering
methods such as classifiers, regression models, generative AI, and embedding
models. Applications include classification, document digitization, record
linkage, and methods for data exploration in massive scale text and image
corpora. When suitable methods are used, deep learning models can be cheap to
tune and can scale affordably to problems involving millions or billions of
data points.. The review is accompanied by a companion website, EconDL, with
user-friendly demo notebooks, software resources, and a knowledge base that
provides technical details and additional applications.",[{'name': 'Melissa Dell'}],2024-07-22T02:53:18Z
http://arxiv.org/abs/2407.15891v1,http://arxiv.org/abs/2407.15891v1,RazorAttention: Efficient KV Cache Compression Through Retrieval Heads,"The memory and computational demands of Key-Value (KV) cache present
significant challenges for deploying long-context language models. Previous
approaches attempt to mitigate this issue by selectively dropping tokens, which
irreversibly erases critical information that might be needed for future
queries. In this paper, we propose a novel compression technique for KV cache
that preserves all token information. Our investigation reveals that: i) Most
attention heads primarily focus on the local context; ii) Only a few heads,
denoted as retrieval heads, can essentially pay attention to all input tokens.
These key observations motivate us to use separate caching strategy for
attention heads. Therefore, we propose RazorAttention, a training-free KV cache
compression algorithm, which maintains a full cache for these crucial retrieval
heads and discards the remote tokens in non-retrieval heads. Furthermore, we
introduce a novel mechanism involving a ""compensation token"" to further recover
the information in the dropped tokens. Extensive evaluations across a diverse
set of large language models (LLMs) demonstrate that RazorAttention achieves a
reduction in KV cache size by over 70% without noticeable impacts on
performance. Additionally, RazorAttention is compatible with FlashAttention,
rendering it an efficient and plug-and-play solution that enhances LLM
inference efficiency without overhead or retraining of the original model.","[{'name': 'Hanlin Tang'}, {'name': 'Yang Lin'}, {'name': 'Jing Lin'}, {'name': 'Qingsen Han'}, {'name': 'Shikuan Hong'}, {'name': 'Yiwu Yao'}, {'name': 'Gongyi Wang'}]",2024-07-22T01:12:23Z
http://arxiv.org/abs/2407.15296v1,http://arxiv.org/abs/2407.15296v1,"Weak-to-Strong Compositional Learning from Generative Models for
  Language-based Object Detection","Vision-language (VL) models often exhibit a limited understanding of complex
expressions of visual objects (e.g., attributes, shapes, and their relations),
given complex and diverse language queries. Traditional approaches attempt to
improve VL models using hard negative synthetic text, but their effectiveness
is limited. In this paper, we harness the exceptional compositional
understanding capabilities of generative foundational models. We introduce a
novel method for structured synthetic data generation aimed at enhancing the
compositional understanding of VL models in language-based object detection.
Our framework generates densely paired positive and negative triplets (image,
text descriptions, and bounding boxes) in both image and text domains. By
leveraging these synthetic triplets, we transform 'weaker' VL models into
'stronger' models in terms of compositional understanding, a process we call
""Weak-to-Strong Compositional Learning"" (WSCL). To achieve this, we propose a
new compositional contrastive learning formulation that discovers semantics and
structures in complex descriptions from synthetic triplets. As a result, VL
models trained with our synthetic data generation exhibit a significant
performance boost in the Omnilabel benchmark by up to +5AP and the D3 benchmark
by +6.9AP upon existing baselines.","[{'name': 'Kwanyong Park'}, {'name': 'Kuniaki Saito'}, {'name': 'Donghyun Kim'}]",2024-07-21T23:43:24Z
http://arxiv.org/abs/2407.15286v2,http://arxiv.org/abs/2407.15286v2,"Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal
  Mechanisms and the Superficial Hypothesis","Large Language Models (LLMs) are capable of producing content that
perpetuates stereotypes, discrimination, and toxicity. The recently proposed
moral self-correction is a computationally efficient method for reducing
harmful content in the responses of LLMs. However, the process of how injecting
self-correction instructions can modify the behavior of LLMs remains
under-explored. In this paper, we explore the effectiveness of moral
self-correction by answering three research questions: (1) In what scenarios
does moral self-correction work? (2) What are the internal mechanisms of LLMs,
e.g., hidden states, that are influenced by moral self-correction instructions?
(3) Is intrinsic moral self-correction actually superficial? We argue that
self-correction can help LLMs find a shortcut to more morally correct output,
rather than truly reducing the immorality stored in hidden states. Through
empirical investigation with tasks of language generation and multi-choice
question answering, we conclude: (i) LLMs exhibit good performance across both
tasks, and self-correction instructions are particularly beneficial when the
correct answer is already top-ranked; (ii) The morality levels in intermediate
hidden states are strong indicators as to whether one instruction would be more
effective than another; (iii) Based on our analysis of intermediate hidden
states and task case studies of self-correction behaviors, we are first to
propose the hypothesis that intrinsic moral self-correction is in fact
superficial.","[{'name': 'Guangliang Liu'}, {'name': 'Haitao Mao'}, {'name': 'Jiliang Tang'}, {'name': 'Kristen Marie Johnson'}]",2024-07-21T22:50:11Z
http://arxiv.org/abs/2407.15281v1,http://arxiv.org/abs/2407.15281v1,"SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense
  Persona Knowledge Linking","Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.",[{'name': 'Kuan-Yen Lin'}],2024-07-21T22:07:14Z
http://arxiv.org/abs/2407.15268v1,http://arxiv.org/abs/2407.15268v1,"Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical
  Radiology Report Generation","Multimodal foundation models hold significant potential for automating
radiology report generation, thereby assisting clinicians in diagnosing cardiac
diseases. However, generated reports often suffer from serious factual
inaccuracy. In this paper, we introduce a fact-aware multimodal
retrieval-augmented pipeline in generating accurate radiology reports
(FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then
integrate factual knowledge to train a universal multimodal retriever. Given a
radiology image, our retriever can identify high-quality reference reports to
augment multimodal foundation models, thus enhancing the factual completeness
and correctness of report generation. Experiments on two benchmark datasets
show that our multimodal retriever outperforms state-of-the-art retrievers on
both language generation and radiology-specific metrics, up to 6.5% and 2%
score in F1CheXbert and F1RadGraph. Further analysis indicates that employing
our factually-informed training strategy imposes an effective supervision
signal, without relying on explicit diagnostic label guidance, and successfully
propagates fact-aware capabilities from the multimodal retriever to the
multimodal foundation model in radiology report generation.","[{'name': 'Liwen Sun'}, {'name': 'James Zhao'}, {'name': 'Megan Han'}, {'name': 'Chenyan Xiong'}]",2024-07-21T21:04:28Z
http://arxiv.org/abs/2407.21041v1,http://arxiv.org/abs/2407.21041v1,"They Look Like Each Other: Case-based Reasoning for Explainable
  Depression Detection on Twitter using Large Language Models","Depression is a common mental health issue that requires prompt diagnosis and
treatment. Despite the promise of social media data for depression detection,
the opacity of employed deep learning models hinders interpretability and
raises bias concerns. We address this challenge by introducing ProtoDep, a
novel, explainable framework for Twitter-based depression detection. ProtoDep
leverages prototype learning and the generative power of Large Language Models
to provide transparent explanations at three levels: (i) symptom-level
explanations for each tweet and user, (ii) case-based explanations comparing
the user to similar individuals, and (iii) transparent decision-making through
classification weights. Evaluated on five benchmark datasets, ProtoDep achieves
near state-of-the-art performance while learning meaningful prototypes. This
multi-faceted approach offers significant potential to enhance the reliability
and transparency of depression detection on social media, ultimately aiding
mental health professionals in delivering more informed care.","[{'name': 'Mohammad Saeid Mahdavinejad'}, {'name': 'Peyman Adibi'}, {'name': 'Amirhassan Monadjemi'}, {'name': 'Pascal Hitzler'}]",2024-07-21T20:13:50Z
http://arxiv.org/abs/2407.15248v1,http://arxiv.org/abs/2407.15248v1,"XAI meets LLMs: A Survey of the Relation between Explainable AI and
  Large Language Models","In this survey, we address the key challenges in Large Language Models (LLM)
research, focusing on the importance of interpretability. Driven by increasing
interest from AI and business sectors, we highlight the need for transparency
in LLMs. We examine the dual paths in current LLM research and eXplainable
Artificial Intelligence (XAI): enhancing performance through XAI and the
emerging focus on model interpretability. Our paper advocates for a balanced
approach that values interpretability equally with functional advancements.
Recognizing the rapid development in LLM research, our survey includes both
peer-reviewed and preprint (arXiv) papers, offering a comprehensive overview of
XAI's role in LLM research. We conclude by urging the research community to
advance both LLM and XAI fields together.","[{'name': 'Erik Cambria'}, {'name': 'Lorenzo Malandri'}, {'name': 'Fabio Mercorio'}, {'name': 'Navid Nobani'}, {'name': 'Andrea Seveso'}]",2024-07-21T19:23:45Z
http://arxiv.org/abs/2407.15237v1,http://arxiv.org/abs/2407.15237v1,"Two eyes, Two views, and finally, One summary! Towards Multi-modal
  Multi-tasking Knowledge-Infused Medical Dialogue Summarization","We often summarize a multi-party conversation in two stages: chunking with
homogeneous units and summarizing the chunks. Thus, we hypothesize that there
exists a correlation between homogeneous speaker chunking and overall
summarization tasks. In this work, we investigate the effectiveness of a
multi-faceted approach that simultaneously produces summaries of medical
concerns, doctor impressions, and an overall view. We introduce a multi-modal,
multi-tasking, knowledge-infused medical dialogue summary generation
(MMK-Summation) model, which is incorporated with adapter-based fine-tuning
through a gated mechanism for multi-modal information integration. The model,
MMK-Summation, takes dialogues as input, extracts pertinent external knowledge
based on the context, integrates the knowledge and visual cues from the
dialogues into the textual content, and ultimately generates concise summaries
encompassing medical concerns, doctor impressions, and a comprehensive
overview. The introduced model surpasses multiple baselines and traditional
summarization models across all evaluation metrics (including human
evaluation), which firmly demonstrates the efficacy of the knowledge-guided
multi-tasking, multimodal medical conversation summarization. The code is
available at https://github.com/NLP-RL/MMK-Summation.","[{'name': 'Anisha Saha'}, {'name': 'Abhisek Tiwari'}, {'name': 'Sai Ruthvik'}, {'name': 'Sriparna Saha'}]",2024-07-21T18:00:10Z
http://arxiv.org/abs/2407.15235v1,http://arxiv.org/abs/2407.15235v1,"TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for
  Instruction Tuning Data","Instruction tuning has achieved unprecedented success in NLP, turning large
language models into versatile chatbots. However, the increasing variety and
volume of instruction datasets demand significant computational resources. To
address this, it is essential to extract a small and highly informative subset
(i.e., Coreset) that achieves comparable performance to the full dataset.
Achieving this goal poses non-trivial challenges: 1) data selection requires
accurate data representations that reflect the training samples' quality, 2)
considering the diverse nature of instruction datasets, and 3) ensuring the
efficiency of the coreset selection algorithm for large models. To address
these challenges, we propose Task-Agnostic Gradient Clustered COreset Selection
(TAGCOS). Specifically, we leverage sample gradients as the data
representations, perform clustering to group similar data, and apply an
efficient greedy algorithm for coreset selection. Experimental results show
that our algorithm, selecting only 5% of the data, surpasses other unsupervised
methods and achieves performance close to that of the full dataset.","[{'name': 'Jipeng Zhang'}, {'name': 'Yaxuan Qin'}, {'name': 'Renjie Pi'}, {'name': 'Weizhong Zhang'}, {'name': 'Rui Pan'}, {'name': 'Tong Zhang'}]",2024-07-21T17:59:20Z
http://arxiv.org/abs/2407.15229v1,http://arxiv.org/abs/2407.15229v1,The Hitchhiker's Guide to Human Alignment with *PO,"With the growing utilization of large language models (LLMs) across domains,
alignment towards human preferences has become one of the most critical aspects
of training models. At the forefront of state-of-the-art human alignment
methods are preference optimization methods (*PO). However, prior research has
often concentrated on identifying the best-performing method, typically
involving a grid search over hyperparameters, which can be impractical for
general practitioners. In this paper, we aim to identify the algorithm that,
while being performant, is simultaneously more robust to varying
hyperparameters, thereby increasing the likelihood of achieving better results.
We focus on a realistic out-of-distribution (OOD) scenario that mirrors
real-world applications of human alignment, offering practical insights into
the strengths and weaknesses of these methods. Furthermore, to better
understand the shortcomings of generations from the different methods, we
analyze the model generations through the lens of KL divergence of the SFT
model and the response length statistics. Our analysis reveals that the widely
adopted DPO method consistently produces lengthy responses of inferior quality
that are very close to the SFT responses. Motivated by these findings, we
propose an embarrassingly simple extension to the DPO algorithm, LN-DPO,
resulting in more concise responses without sacrificing quality compared to the
policy obtained by vanilla DPO.","[{'name': 'Kian Ahrabian'}, {'name': 'Xihui Lin'}, {'name': 'Barun Patra'}, {'name': 'Vishrav Chaudhary'}, {'name': 'Alon Benhaim'}, {'name': 'Jay Pujara'}, {'name': 'Xia Song'}]",2024-07-21T17:35:20Z
http://arxiv.org/abs/2407.15227v1,http://arxiv.org/abs/2407.15227v1,"A Community-Centric Perspective for Characterizing and Detecting
  Anti-Asian Violence-Provoking Speech","Violence-provoking speech -- speech that implicitly or explicitly promotes
violence against the members of the targeted community, contributed to a
massive surge in anti-Asian crimes during the pandemic. While previous works
have characterized and built tools for detecting other forms of harmful speech,
like fear speech and hate speech, our work takes a community-centric approach
to studying anti-Asian violence-provoking speech. Using data from ~420k Twitter
posts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we
develop a codebook to characterize anti-Asian violence-provoking speech and
collect a community-crowdsourced dataset to facilitate its large-scale
detection using state-of-the-art classifiers. We contrast the capabilities of
natural language processing classifiers, ranging from BERT-based to LLM-based
classifiers, in detecting violence-provoking speech with their capabilities to
detect anti-Asian hateful speech. In contrast to prior work that has
demonstrated the effectiveness of such classifiers in detecting hateful speech
($F_1 = 0.89$), our work shows that accurate and reliable detection of
violence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the
implications of our findings, particularly the need for proactive interventions
to support Asian communities during public health crises. The resources related
to the study are available at
https://claws-lab.github.io/violence-provoking-speech/.","[{'name': 'Gaurav Verma'}, {'name': 'Rynaa Grover'}, {'name': 'Jiawei Zhou'}, {'name': 'Binny Mathew'}, {'name': 'Jordan Kraemer'}, {'name': 'Munmun De Choudhury'}, {'name': 'Srijan Kumar'}]",2024-07-21T17:27:17Z
http://arxiv.org/abs/2407.15211v1,http://arxiv.org/abs/2407.15211v1,"When Do Universal Image Jailbreaks Transfer Between Vision-Language
  Models?","The integration of new modalities into frontier AI systems offers exciting
capabilities, but also increases the possibility such systems can be
adversarially manipulated in undesirable ways. In this work, we focus on a
popular class of vision-language models (VLMs) that generate text outputs
conditioned on visual and textual inputs. We conducted a large-scale empirical
study to assess the transferability of gradient-based universal image
""jailbreaks"" using a diverse set of over 40 open-parameter VLMs, including 18
new VLMs that we publicly release. Overall, we find that transferable
gradient-based image jailbreaks are extremely difficult to obtain. When an
image jailbreak is optimized against a single VLM or against an ensemble of
VLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits
little-to-no transfer to any other VLMs; transfer is not affected by whether
the attacked and target VLMs possess matching vision backbones or language
models, whether the language model underwent instruction-following and/or
safety-alignment training, or many other factors. Only two settings display
partially successful transfer: between identically-pretrained and
identically-initialized VLMs with slightly different VLM training data, and
between different training checkpoints of a single VLM. Leveraging these
results, we then demonstrate that transfer can be significantly improved
against a specific target VLM by attacking larger ensembles of ""highly-similar""
VLMs. These results stand in stark contrast to existing evidence of universal
and transferable text jailbreaks against language models and transferable
adversarial attacks against image classifiers, suggesting that VLMs may be more
robust to gradient-based transfer attacks.","[{'name': 'Rylan Schaeffer'}, {'name': 'Dan Valentine'}, {'name': 'Luke Bailey'}, {'name': 'James Chua'}, {'name': 'Cristóbal Eyzaguirre'}, {'name': 'Zane Durante'}, {'name': 'Joe Benton'}, {'name': 'Brando Miranda'}, {'name': 'Henry Sleight'}, {'name': 'John Hughes'}, {'name': 'Rajashree Agrawal'}, {'name': 'Mrinank Sharma'}, {'name': 'Scott Emmons'}, {'name': 'Sanmi Koyejo'}, {'name': 'Ethan Perez'}]",2024-07-21T16:27:24Z
http://arxiv.org/abs/2407.15186v2,http://arxiv.org/abs/2407.15186v2,A Survey on Employing Large Language Models for Text-to-SQL Tasks,"The increasing volume of data stored in relational databases has led to the
need for efficient querying and utilization of this data in various sectors.
However, writing SQL queries requires specialized knowledge, which poses a
challenge for non-professional users trying to access and query databases.
Text-to-SQL parsing solves this issue by converting natural language queries
into SQL queries, thus making database access more accessible for non-expert
users. To take advantage of the recent developments in Large Language Models
(LLMs), a range of new methods have emerged, with a primary focus on prompt
engineering and fine-tuning. This survey provides a comprehensive overview of
LLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,
fine-tuning methods, and future research directions. We hope this review will
enable readers to gain a broader understanding of the recent advances in this
field and offer some insights into its future trajectory.","[{'name': 'Liang Shi'}, {'name': 'Zhengju Tang'}, {'name': 'Nan Zhang'}, {'name': 'Xiaotong Zhang'}, {'name': 'Zhi Yang'}]",2024-07-21T14:48:23Z
http://arxiv.org/abs/2407.15184v1,http://arxiv.org/abs/2407.15184v1,"Decoding Multilingual Moral Preferences: Unveiling LLM's Biases Through
  the Moral Machine Experiment","Large language models (LLMs) increasingly find their way into the most
diverse areas of our everyday lives. They indirectly influence people's
decisions or opinions through their daily use. Therefore, understanding how and
which moral judgements these LLMs make is crucial. However, morality is not
universal and depends on the cultural background. This raises the question of
whether these cultural preferences are also reflected in LLMs when prompted in
different languages or whether moral decision-making is consistent across
different languages. So far, most research has focused on investigating the
inherent values of LLMs in English. While a few works conduct multilingual
analyses of moral bias in LLMs in a multilingual setting, these analyses do not
go beyond atomic actions. To the best of our knowledge, a multilingual analysis
of moral bias in dilemmas has not yet been conducted.
  To address this, our paper builds on the moral machine experiment (MME) to
investigate the moral preferences of five LLMs, Falcon, Gemini, Llama, GPT, and
MPT, in a multilingual setting and compares them with the preferences collected
from humans belonging to different cultures. To accomplish this, we generate
6500 scenarios of the MME and prompt the models in ten languages on which
action to take. Our analysis reveals that all LLMs inhibit different moral
biases to some degree and that they not only differ from the human preferences
but also across multiple languages within the models themselves. Moreover, we
find that almost all models, particularly Llama 3, divert greatly from human
values and, for instance, prefer saving fewer people over saving more.","[{'name': 'Karina Vida'}, {'name': 'Fabian Damken'}, {'name': 'Anne Lauscher'}]",2024-07-21T14:48:13Z
http://arxiv.org/abs/2407.15176v1,http://arxiv.org/abs/2407.15176v1,"Farewell to Length Extrapolation, a Training-Free Infinite Context with
  Finite Attention Scope","The maximum supported context length is a critical bottleneck limiting the
practical application of the Large Language Model (LLM). Although existing
length extrapolation methods can extend the context of LLMs to millions of
tokens, these methods all have an explicit upper bound. In this work, we
propose LongCache, a training-free approach that enables LLM to support an
infinite context with finite context scope, through full-context cache
selection and training-free integration. This effectively frees LLMs from the
length extrapolation issue. We validate LongCache on the LongBench and L-Eval
and demonstrate its performance is on par with traditional full-attention
mechanisms. Furthermore, we have applied LongCache on mainstream LLMs,
including LLaMA3 and Mistral-v0.3, enabling them to support context lengths of
at least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of
LongCache by GPU-aware optimization soon.","[{'name': 'Xiaoran Liu'}, {'name': 'Qipeng Guo'}, {'name': 'Yuerong Song'}, {'name': 'Zhigeng Liu'}, {'name': 'Kai Lv'}, {'name': 'Hang Yan'}, {'name': 'Linlin Li'}, {'name': 'Qun Liu'}, {'name': 'Xipeng Qiu'}]",2024-07-21T14:23:37Z
http://arxiv.org/abs/2407.15160v1,http://arxiv.org/abs/2407.15160v1,When Can Transformers Count to n?,"Large language models based on the transformer architectures can solve highly
complex tasks. But are there simple tasks that such models cannot solve? Here
we focus on very simple counting tasks, that involve counting how many times a
token in the vocabulary have appeared in a string. We show that if the
dimension of the transformer state is linear in the context length, this task
can be solved. However, the solution we propose does not scale beyond this
limit, and we provide theoretical arguments for why it is likely impossible for
a size limited transformer to implement this task. Our empirical results
demonstrate the same phase-transition in performance, as anticipated by the
theoretical argument. Our results demonstrate the importance of understanding
how transformers can solve simple tasks.","[{'name': 'Gilad Yehudai'}, {'name': 'Haim Kaplan'}, {'name': 'Asma Ghandeharioun'}, {'name': 'Mor Geva'}, {'name': 'Amir Globerson'}]",2024-07-21T13:31:02Z
http://arxiv.org/abs/2407.15154v1,http://arxiv.org/abs/2407.15154v1,"Fine-grained Gender Control in Machine Translation with Large Language
  Models","In machine translation, the problem of ambiguously gendered input has been
pointed out, where the gender of an entity is not available in the source
sentence. To address this ambiguity issue, the task of controlled translation
that takes the gender of the ambiguous entity as additional input have been
proposed. However, most existing works have only considered a simplified setup
of one target gender for input. In this paper, we tackle controlled translation
in a more realistic setting of inputs with multiple entities and propose
Gender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs
the model with fine-grained entity-level gender information to translate with
correct gender inflections. By utilizing four evaluation benchmarks, we
investigate the controlled translation capability of LLMs in multiple
dimensions and find that LLMs reach state-of-the-art performance in controlled
translation. Furthermore, we discover an emergence of gender interference
phenomenon when controlling the gender of multiple entities. Finally, we
address the limitations of existing gender accuracy evaluation metrics and
propose leveraging LLMs as an evaluator for gender inflection in machine
translation.","[{'name': 'Minwoo Lee'}, {'name': 'Hyukhun Koh'}, {'name': 'Minsung Kim'}, {'name': 'Kyomin Jung'}]",2024-07-21T13:15:00Z
http://arxiv.org/abs/2407.15136v1,http://arxiv.org/abs/2407.15136v1,"A multi-level multi-label text classification dataset of 19th century
  Ottoman and Russian literary and critical texts","This paper introduces a multi-level, multi-label text classification dataset
comprising over 3000 documents. The dataset features literary and critical
texts from 19th-century Ottoman Turkish and Russian. It is the first study to
apply large language models (LLMs) to this dataset, sourced from prominent
literary periodicals of the era. The texts have been meticulously organized and
labeled. This was done according to a taxonomic framework that takes into
account both their structural and semantic attributes. Articles are categorized
and tagged with bibliometric metadata by human experts. We present baseline
classification results using a classical bag-of-words (BoW) naive Bayes model
and three modern LLMs: multilingual BERT, Falcon, and Llama-v2. We found that
in certain cases, Bag of Words (BoW) outperforms Large Language Models (LLMs),
emphasizing the need for additional research, especially in low-resource
language settings. This dataset is expected to be a valuable resource for
researchers in natural language processing and machine learning, especially for
historical and low-resource languages. The dataset is publicly available^1.","[{'name': 'Gokcen Gokceoglu'}, {'name': 'Devrim Cavusoglu'}, {'name': 'Emre Akbas'}, {'name': 'Özen Nergis Dolcerocca'}]",2024-07-21T12:14:45Z
http://arxiv.org/abs/2407.15130v2,http://arxiv.org/abs/2407.15130v2,"DOPRA: Decoding Over-accumulation Penalization and Re-allocation in
  Specific Weighting Layer","In this work, we introduce DOPRA, a novel approach designed to mitigate
hallucinations in multi-modal large language models (MLLMs). Unlike existing
solutions that typically involve costly supplementary training data or the
integration of external knowledge sources, DOPRA innovatively addresses
hallucinations by decoding specific weighted layer penalties and
redistribution, offering an economical and effective solution without
additional resources. DOPRA is grounded in unique insights into the intrinsic
mechanisms controlling hallucinations within MLLMs, especially the models'
tendency to over-rely on a subset of summary tokens in the self-attention
matrix, neglecting critical image-related information. This phenomenon is
particularly pronounced in certain strata. To counteract this over-reliance,
DOPRA employs a strategy of weighted overlay penalties and redistribution in
specific layers, such as the 12th layer, during the decoding process.
Furthermore, DOPRA includes a retrospective allocation process that re-examines
the sequence of generated tokens, allowing the algorithm to reallocate token
selection to better align with the actual image content, thereby reducing the
incidence of hallucinatory descriptions in auto-generated captions. Overall,
DOPRA represents a significant step forward in improving the output quality of
MLLMs by systematically reducing hallucinations through targeted adjustments
during the decoding process.","[{'name': 'Jinfeng Wei'}, {'name': 'Xiaofeng Zhang'}]",2024-07-21T11:54:49Z
http://arxiv.org/abs/2407.21040v1,http://arxiv.org/abs/2407.21040v1,"Towards Automated Data Sciences with Natural Language and SageCopilot:
  Practices and Lessons Learned","While the field of NL2SQL has made significant advancements in translating
natural language instructions into executable SQL scripts for data querying and
processing, achieving full automation within the broader data science pipeline
- encompassing data querying, analysis, visualization, and reporting - remains
a complex challenge. This study introduces SageCopilot, an advanced,
industry-grade system system that automates the data science pipeline by
integrating Large Language Models (LLMs), Autonomous Agents (AutoAgents), and
Language User Interfaces (LUIs). Specifically, SageCopilot incorporates a
two-phase design: an online component refining users' inputs into executable
scripts through In-Context Learning (ICL) and running the scripts for results
reporting & visualization, and an offline preparing demonstrations requested by
ICL in the online phase. A list of trending strategies such as Chain-of-Thought
and prompt-tuning have been used to augment SageCopilot for enhanced
performance. Through rigorous testing and comparative analysis against
prompt-based solutions, SageCopilot has been empirically validated to achieve
superior end-to-end performance in generating or executing scripts and offering
results with visualization, backed by real-world datasets. Our in-depth
ablation studies highlight the individual contributions of various components
and strategies used by SageCopilot to the end-to-end correctness for data
sciences.","[{'name': 'Yuan Liao'}, {'name': 'Jiang Bian'}, {'name': 'Yuhui Yun'}, {'name': 'Shuo Wang'}, {'name': 'Yubo Zhang'}, {'name': 'Jiaming Chu'}, {'name': 'Tao Wang'}, {'name': 'Kewei Li'}, {'name': 'Yuchen Li'}, {'name': 'Xuhong Li'}, {'name': 'Shilei Ji'}, {'name': 'Haoyi Xiong'}]",2024-07-21T08:58:18Z
http://arxiv.org/abs/2407.15073v1,http://arxiv.org/abs/2407.15073v1,Multi-Agent Causal Discovery Using Large Language Models,"Large Language Models (LLMs) have demonstrated significant potential in
causal discovery tasks by utilizing their vast expert knowledge from extensive
text corpora. However, the multi-agent capabilities of LLMs in causal discovery
remain underexplored. This paper introduces a general framework to investigate
this potential. The first is the Meta Agents Model, which relies exclusively on
reasoning and discussions among LLM agents to conduct causal discovery. The
second is the Coding Agents Model, which leverages the agents' ability to plan,
write, and execute code, utilizing advanced statistical libraries for causal
discovery. The third is the Hybrid Model, which integrates both the Meta Agents
Model and CodingAgents Model approaches, combining the statistical analysis and
reasoning skills of multiple agents. Our proposed framework shows promising
results by effectively utilizing LLMs expert knowledge, reasoning capabilities,
multi-agent cooperation, and statistical causal methods. By exploring the
multi-agent potential of LLMs, we aim to establish a foundation for further
research in utilizing LLMs multi-agent for solving causal-related problems.","[{'name': 'Hao Duong Le'}, {'name': 'Xin Xia'}, {'name': 'Zhang Chen'}]",2024-07-21T06:21:47Z
http://arxiv.org/abs/2407.15071v1,http://arxiv.org/abs/2407.15071v1,Relational Database Augmented Large Language Model,"Large language models (LLMs) excel in many natural language processing (NLP)
tasks. However, since LLMs can only incorporate new knowledge through training
or supervised fine-tuning processes, they are unsuitable for applications that
demand precise, up-to-date, and private information not available in the
training corpora. This precise, up-to-date, and private information is
typically stored in relational databases. Thus, a promising solution is to
augment LLMs with the inclusion of relational databases as external memory.
This can ensure the timeliness, correctness, and consistency of data, and
assist LLMs in performing complex arithmetic operations beyond their inherent
capabilities. However, bridging the gap between LLMs and relational databases
is challenging. It requires the awareness of databases and data values stored
in databases to select correct databases and issue correct SQL queries.
Besides, it is necessary for the external memory to be independent of the LLM
to meet the needs of real-world applications. We introduce a novel LLM-agnostic
memory architecture comprising a database selection memory, a data value
memory, and relational databases. And we design an elegant pipeline to retrieve
information from it. Besides, we carefully design the prompts to instruct the
LLM to maximize the framework's potential. To evaluate our method, we compose a
new dataset with various types of questions. Experimental results show that our
framework enables LLMs to effectively answer database-related questions, which
is beyond their direct ability.","[{'name': 'Zongyue Qin'}, {'name': 'Chen Luo'}, {'name': 'Zhengyang Wang'}, {'name': 'Haoming Jiang'}, {'name': 'Yizhou Sun'}]",2024-07-21T06:19:10Z
http://arxiv.org/abs/2407.15055v1,http://arxiv.org/abs/2407.15055v1,Natural Language Task-Oriented Dialog System 2.0,"Task-oriented dialog (TOD) systems play a crucial role in facilitating
efficient interactions between users and machines by focusing on achieving
specific goals through natural language communication. These systems
traditionally rely on manually annotated metadata, such as dialog states and
policy annotations, which is labor-intensive, expensive, inconsistent, and
prone to errors, thereby limiting the potential to leverage the vast amounts of
available conversational data. A critical aspect of TOD systems involves
accessing and integrating information from external sources to effectively
engage users. The process of determining when and how to query external
resources represents a fundamental challenge in system design, however existing
approaches expect this information to provided in the context. In this paper,
we introduce Natural Language Task Oriented Dialog System (NL-ToD), a novel
model that removes the dependency on manually annotated turn-wise data by
utilizing dialog history and domain schemas to create a Zero Shot Generalizable
TOD system. We also incorporate query generation as a core task of the system,
where the output of the system could be a response to the user or an API query
to communicate with an external resource. To achieve a more granular analysis
of the system output, we classify the output into multiple categories: slot
filling, retrieval, and query generation. Our analysis reveals that slot
filling is the most challenging TOD task for all models. Experimental results
on three popular TOD datasets (SGD, KETOD and BiToD) shows the effectiveness of
our approach as NL-ToD outperforms state-of-the-art approaches, particularly
with a \textbf{31.4\%} and \textbf{82.1\%} improvement in the BLEU-4 score on
the SGD and KETOD dataset.","[{'name': 'Adib Mosharrof'}, {'name': 'A. B. Siddique'}]",2024-07-21T04:52:38Z
http://arxiv.org/abs/2407.15047v2,http://arxiv.org/abs/2407.15047v2,"End-to-End Video Question Answering with Frame Scoring Mechanisms and
  Adaptive Sampling","Video Question Answering (VideoQA) has emerged as a challenging frontier in
the field of multimedia processing, requiring intricate interactions between
visual and textual modalities. Simply uniformly sampling frames or
indiscriminately aggregating frame-level visual features often falls short in
capturing the nuanced and relevant contexts of videos to well perform VideoQA.
To mitigate these issues, we propose VidF4, a novel VideoQA framework equipped
with tailored frame selection strategy for effective and efficient VideoQA. We
propose three frame-scoring mechanisms that consider both question relevance
and inter-frame similarity to evaluate the importance of each frame for a given
question on the video. Furthermore, we design a differentiable adaptive frame
sampling mechanism to facilitate end-to-end training for the frame selector and
answer generator. The experimental results across three widely adopted
benchmarks demonstrate that our model consistently outperforms existing VideoQA
methods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA
(+1.0%). Furthermore, through both quantitative and qualitative analyses, we
validate the effectiveness of each design choice.","[{'name': 'Jianxin Liang'}, {'name': 'Xiaojun Meng'}, {'name': 'Yueqian Wang'}, {'name': 'Chang Liu'}, {'name': 'Qun Liu'}, {'name': 'Dongyan Zhao'}]",2024-07-21T04:09:37Z
http://arxiv.org/abs/2407.15046v1,http://arxiv.org/abs/2407.15046v1,Audio-visual training for improved grounding in video-text LLMs,"Recent advances in multimodal LLMs, have led to several video-text models
being proposed for critical video-related tasks. However, most of the previous
works support visual input only, essentially muting the audio signal in the
video. Few models that support both audio and visual input, are not explicitly
trained on audio data. Hence, the effect of audio towards video understanding
is largely unexplored. To this end, we propose a model architecture that
handles audio-visual inputs explicitly. We train our model with both audio and
visual data from a video instruction-tuning dataset. Comparison with
vision-only baselines, and other audio-visual models showcase that training on
audio data indeed leads to improved grounding of responses. For better
evaluation of audio-visual models, we also release a human-annotated benchmark
dataset, with audio-aware question-answer pairs.","[{'name': 'Shivprasad Sagare'}, {'name': 'Hemachandran S'}, {'name': 'Kinshuk Sarabhai'}, {'name': 'Prashant Ullegaddi'}, {'name': 'Rajeshkumar SA'}]",2024-07-21T03:59:14Z
http://arxiv.org/abs/2407.15021v1,http://arxiv.org/abs/2407.15021v1,Enhancing Incremental Summarization with Structured Representations,"Large language models (LLMs) often struggle with processing extensive input
contexts, which can lead to redundant, inaccurate, or incoherent summaries.
Recent methods have used unstructured memory to incrementally process these
contexts, but they still suffer from information overload due to the volume of
unstructured data handled. In our study, we introduce structured knowledge
representations ($GU_{json}$), which significantly improve summarization
performance by 40% and 14% across two public datasets. Most notably, we propose
the Chain-of-Key strategy ($CoK_{json}$) that dynamically updates or augments
these representations with new information, rather than recreating the
structured memory for each new source. This method further enhances performance
by 7% and 4% on the datasets.","[{'name': 'EunJeong Hwang'}, {'name': 'Yichao Zhou'}, {'name': 'James Bradley Wendt'}, {'name': 'Beliz Gunel'}, {'name': 'Nguyen Vo'}, {'name': 'Jing Xie'}, {'name': 'Sandeep Tata'}]",2024-07-21T00:23:33Z
http://arxiv.org/abs/2407.15018v1,http://arxiv.org/abs/2407.15018v1,"Answer, Assemble, Ace: Understanding How Transformers Answer Multiple
  Choice Questions","Multiple-choice question answering (MCQA) is a key competence of performant
transformer language models that is tested by mainstream benchmarks. However,
recent evidence shows that models can have quite a range of performance,
particularly when the task format is diversified slightly (such as by shuffling
answer choice order). In this work we ask: how do successful models perform
formatted MCQA? We employ vocabulary projection and activation patching methods
to localize key hidden states that encode relevant information for predicting
the correct answer. We find that prediction of a specific answer symbol is
causally attributed to a single middle layer, and specifically its multi-head
self-attention mechanism. We show that subsequent layers increase the
probability of the predicted answer symbol in vocabulary space, and that this
probability increase is associated with a sparse set of attention heads with
unique roles. We additionally uncover differences in how different models
adjust to alternative symbols. Finally, we demonstrate that a synthetic task
can disentangle sources of model error to pinpoint when a model has learned
formatted MCQA, and show that an inability to separate answer symbol tokens in
vocabulary space is a property of models unable to perform formatted MCQA
tasks.","[{'name': 'Sarah Wiegreffe'}, {'name': 'Oyvind Tafjord'}, {'name': 'Yonatan Belinkov'}, {'name': 'Hannaneh Hajishirzi'}, {'name': 'Ashish Sabharwal'}]",2024-07-21T00:10:23Z
http://arxiv.org/abs/2407.14997v1,http://arxiv.org/abs/2407.14997v1,"Improving Citation Text Generation: Overcoming Limitations in Length
  Control","A key challenge in citation text generation is that the length of generated
text often differs from the length of the target, lowering the quality of the
generation. While prior works have investigated length-controlled generation,
their effectiveness depends on knowing the appropriate generation length. In
this work, we present an in-depth study of the limitations of predicting
scientific citation text length and explore the use of heuristic estimates of
desired length.","[{'name': 'Biswadip Mandal'}, {'name': 'Xiangci Li'}, {'name': 'Jessica Ouyang'}]",2024-07-20T22:10:37Z
http://arxiv.org/abs/2407.14985v1,http://arxiv.org/abs/2407.14985v1,"Generalization v.s. Memorization: Tracing Language Models' Capabilities
  Back to Pretraining Data","Despite the proven utility of large language models (LLMs) in real-world
applications, there remains a lack of understanding regarding how they leverage
their large-scale pretraining text corpora to achieve such capabilities. In
this work, we investigate the interplay between generalization and memorization
in pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their
training data. Our experiments focus on three general task types: translation,
question-answering, and multiple-choice reasoning. With various sizes of
open-source LLMs and their pretraining corpora, we observe that as the model
size increases, the task-relevant $n$-gram pair data becomes increasingly
important, leading to improved task performance, decreased memorization,
stronger generalization, and emergent abilities. Our results support the
hypothesis that LLMs' capabilities emerge from a delicate balance of
memorization and generalization with sufficient task-related pretraining data,
and point the way to larger-scale analyses that could further improve our
understanding of these models.","[{'name': 'Antonis Antoniades'}, {'name': 'Xinyi Wang'}, {'name': 'Yanai Elazar'}, {'name': 'Alfonso Amayuelas'}, {'name': 'Alon Albalak'}, {'name': 'Kexun Zhang'}, {'name': 'William Yang Wang'}]",2024-07-20T21:24:40Z
http://arxiv.org/abs/2407.14971v1,http://arxiv.org/abs/2407.14971v1,"Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and
  Semantically-Rich Vision-Language Models","Vision-language models (VLMs) have achieved significant strides in recent
times specially in multimodal tasks, yet they remain susceptible to adversarial
attacks on their vision components. To address this, we propose Sim-CLIP, an
unsupervised adversarial fine-tuning method that enhances the robustness of the
widely-used CLIP vision encoder against such attacks while maintaining semantic
richness and specificity. By employing a Siamese architecture with cosine
similarity loss, Sim-CLIP learns semantically meaningful and attack-resilient
visual representations without requiring large batch sizes or momentum
encoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned
CLIP encoder exhibit significantly enhanced robustness against adversarial
attacks, while preserving semantic meaning of the perturbed images. Notably,
Sim-CLIP does not require additional training or fine-tuning of the VLM itself;
replacing the original vision encoder with our fine-tuned Sim-CLIP suffices to
provide robustness. This work underscores the significance of reinforcing
foundational models like CLIP to safeguard the reliability of downstream VLM
applications, paving the way for more secure and effective multimodal systems.","[{'name': 'Md Zarif Hossain'}, {'name': 'Ahmed Imteaj'}]",2024-07-20T19:53:52Z
http://arxiv.org/abs/2407.14962v4,http://arxiv.org/abs/2407.14962v4,"Recent Advances in Generative AI and Large Language Models: Current
  Status, Challenges, and Perspectives","The emergence of Generative Artificial Intelligence (AI) and Large Language
Models (LLMs) has marked a new era of Natural Language Processing (NLP),
introducing unprecedented capabilities that are revolutionizing various
domains. This paper explores the current state of these cutting-edge
technologies, demonstrating their remarkable advancements and wide-ranging
applications. Our paper contributes to providing a holistic perspective on the
technical foundations, practical applications, and emerging challenges within
the evolving landscape of Generative AI and LLMs. We believe that understanding
the generative capabilities of AI systems and the specific context of LLMs is
crucial for researchers, practitioners, and policymakers to collaboratively
shape the responsible and ethical integration of these technologies into
various domains. Furthermore, we identify and address main research gaps,
providing valuable insights to guide future research endeavors within the AI
research community.","[{'name': 'Desta Haileselassie Hagos'}, {'name': 'Rick Battle'}, {'name': 'Danda B. Rawat'}]",2024-07-20T18:48:35Z
http://arxiv.org/abs/2407.17522v1,http://arxiv.org/abs/2407.17522v1,"Mapping the Technological Future: A Topic, Sentiment, and Emotion
  Analysis in Social Media Discourse","People worldwide are currently confronted with a number of technological
challenges, which act as a potent source of uncertainty. The uncertainty
arising from the volatility and unpredictability of technology (such as AI) and
its potential consequences is widely discussed on social media. This study uses
BERTopic modelling along with sentiment and emotion analysis on 1.5 million
tweets from 2021 to 2023 to identify anticipated tech-driven futures and
capture the emotions communicated by 400 key opinion leaders (KOLs). Findings
indicate positive sentiment significantly outweighs negative, with a prevailing
dominance of positive anticipatory emotions. Specifically, the 'Hope' score is
approximately 10.33\% higher than the median 'Anxiety' score. KOLs emphasize
'Optimism' and benefits over 'Pessimism' and challenges. The study emphasizes
the important role KOLs play in shaping future visions through anticipatory
discourse and emotional tone during times of technological uncertainty.","[{'name': 'Alina Landowska'}, {'name': 'Maciej Skorski'}, {'name': 'Krzysztof Rajda'}]",2024-07-20T18:15:30Z
http://arxiv.org/abs/2407.14940v1,http://arxiv.org/abs/2407.14940v1,"Conversational Rubert for Detecting Competitive Interruptions in
  ASR-Transcribed Dialogues","Interruption in a dialogue occurs when the listener begins their speech
before the current speaker finishes speaking. Interruptions can be broadly
divided into two groups: cooperative (when the listener wants to support the
speaker), and competitive (when the listener tries to take control of the
conversation against the speaker's will). A system that automatically
classifies interruptions can be used in call centers, specifically in the tasks
of customer satisfaction monitoring and agent monitoring. In this study, we
developed a text-based interruption classification model by preparing an
in-house dataset consisting of ASR-transcribed customer support telephone
dialogues in Russian. We fine-tuned Conversational RuBERT on our dataset and
optimized hyperparameters, and the model performed well. With further
improvements, the proposed model can be applied to automatic monitoring
systems.","[{'name': 'Dmitrii Galimzianov'}, {'name': 'Viacheslav Vyshegorodtsev'}]",2024-07-20T17:25:53Z
http://arxiv.org/abs/2407.14937v1,http://arxiv.org/abs/2407.14937v1,"Operationalizing a Threat Model for Red-Teaming Large Language Models
  (LLMs)","Creating secure and resilient applications with large language models (LLM)
requires anticipating, adjusting to, and countering unforeseen threats.
Red-teaming has emerged as a critical technique for identifying vulnerabilities
in real-world LLM implementations. This paper presents a detailed threat model
and provides a systematization of knowledge (SoK) of red-teaming attacks on
LLMs. We develop a taxonomy of attacks based on the stages of the LLM
development and deployment process and extract various insights from previous
research. In addition, we compile methods for defense and practical red-teaming
strategies for practitioners. By delineating prominent attack motifs and
shedding light on various entry points, this paper provides a framework for
improving the security and robustness of LLM-based systems.","[{'name': 'Apurv Verma'}, {'name': 'Satyapriya Krishna'}, {'name': 'Sebastian Gehrmann'}, {'name': 'Madhavan Seshadri'}, {'name': 'Anu Pradhan'}, {'name': 'Tom Ault'}, {'name': 'Leslie Barrett'}, {'name': 'David Rabinowitz'}, {'name': 'John Doucette'}, {'name': 'NhatHai Phan'}]",2024-07-20T17:05:04Z
http://arxiv.org/abs/2407.14904v1,http://arxiv.org/abs/2407.14904v1,"Large-vocabulary forensic pathological analyses via prototypical
  cross-modal contrastive learning","Forensic pathology is critical in determining the cause and manner of death
through post-mortem examinations, both macroscopic and microscopic. The field,
however, grapples with issues such as outcome variability, laborious processes,
and a scarcity of trained professionals. This paper presents SongCi, an
innovative visual-language model (VLM) designed specifically for forensic
pathology. SongCi utilizes advanced prototypical cross-modal self-supervised
contrastive learning to enhance the accuracy, efficiency, and generalizability
of forensic analyses. It was pre-trained and evaluated on a comprehensive
multi-center dataset, which includes over 16 million high-resolution image
patches, 2,228 vision-language pairs of post-mortem whole slide images (WSIs),
and corresponding gross key findings, along with 471 distinct diagnostic
outcomes. Our findings indicate that SongCi surpasses existing multi-modal AI
models in many forensic pathology tasks, performs comparably to experienced
forensic pathologists and significantly better than less experienced ones, and
provides detailed multi-modal explainability, offering critical assistance in
forensic investigations. To the best of our knowledge, SongCi is the first VLM
specifically developed for forensic pathological analysis and the first
large-vocabulary computational pathology (CPath) model that directly processes
gigapixel WSIs in forensic science.","[{'name': 'Chen Shen'}, {'name': 'Chunfeng Lian'}, {'name': 'Wanqing Zhang'}, {'name': 'Fan Wang'}, {'name': 'Jianhua Zhang'}, {'name': 'Shuanliang Fan'}, {'name': 'Xin Wei'}, {'name': 'Gongji Wang'}, {'name': 'Kehan Li'}, {'name': 'Hongshu Mu'}, {'name': 'Hao Wu'}, {'name': 'Xinggong Liang'}, {'name': 'Jianhua Ma'}, {'name': 'Zhenyuan Wang'}]",2024-07-20T15:34:52Z
http://arxiv.org/abs/2407.21039v1,http://arxiv.org/abs/2407.21039v1,"Mapping Patient Trajectories: Understanding and Visualizing Sepsis
  Prognostic Pathways from Patients Clinical Narratives","In recent years, healthcare professionals are increasingly emphasizing on
personalized and evidence-based patient care through the exploration of
prognostic pathways. To study this, structured clinical variables from
Electronic Health Records (EHRs) data have traditionally been employed by many
researchers. Presently, Natural Language Processing models have received great
attention in clinical research which expanded the possibilities of using
clinical narratives. In this paper, we propose a systematic methodology for
developing sepsis prognostic pathways derived from clinical notes, focusing on
diverse patient subgroups identified by exploring comorbidities associated with
sepsis and generating explanations of these subgroups using SHAP. The extracted
prognostic pathways of these subgroups provide valuable insights into the
dynamic trajectories of sepsis severity over time. Visualizing these pathways
sheds light on the likelihood and direction of disease progression across
various contexts and reveals patterns and pivotal factors or biomarkers
influencing the transition between sepsis stages, whether toward deterioration
or improvement. This empowers healthcare providers to implement more
personalized and effective healthcare strategies for individual patients.","[{'name': 'Sudeshna Jana'}, {'name': 'Tirthankar Dasgupta'}, {'name': 'Lipika Dey'}]",2024-07-20T14:45:55Z
http://arxiv.org/abs/2407.14885v1,http://arxiv.org/abs/2407.14885v1,Falcon2-11B Technical Report,"We introduce Falcon2-11B, a foundation model trained on over five trillion
tokens, and its multimodal counterpart, Falcon2-11B-vlm, which is a
vision-to-text model. We report our findings during the training of the
Falcon2-11B which follows a multi-stage approach where the early stages are
distinguished by their context length and a final stage where we use a curated,
high-quality dataset. Additionally, we report the effect of doubling the batch
size mid-training and how training loss spikes are affected by the learning
rate. The downstream performance of the foundation model is evaluated on
established benchmarks, including multilingual and code datasets. The
foundation model shows strong generalization across all the tasks which makes
it suitable for downstream finetuning use cases. For the vision language model,
we report the performance on several benchmarks and show that our model
achieves a higher average score compared to open-source models of similar size.
The model weights and code of both Falcon2-11B and Falcon2-11B-vlm are made
available under a permissive license.","[{'name': 'Quentin Malartic'}, {'name': 'Nilabhra Roy Chowdhury'}, {'name': 'Ruxandra Cojocaru'}, {'name': 'Mugariya Farooq'}, {'name': 'Giulia Campesan'}, {'name': 'Yasser Abdelaziz Dahou Djilali'}, {'name': 'Sanath Narayan'}, {'name': 'Ankit Singh'}, {'name': 'Maksim Velikanov'}, {'name': 'Basma El Amel Boussaha'}, {'name': 'Mohammed Al-Yafeai'}, {'name': 'Hamza Alobeidli'}, {'name': 'Leen Al Qadi'}, {'name': 'Mohamed El Amine Seddik'}, {'name': 'Kirill Fedyanin'}, {'name': 'Reda Alami'}, {'name': 'Hakim Hacid'}]",2024-07-20T14:23:15Z
http://arxiv.org/abs/2407.14878v1,http://arxiv.org/abs/2407.14878v1,"Modular Sentence Encoders: Separating Language Specialization from
  Cross-Lingual Alignment","Multilingual sentence encoders are commonly obtained by training multilingual
language models to map sentences from different languages into a shared
semantic space. As such, they are subject to curse of multilinguality, a loss
of monolingual representational accuracy due to parameter sharing. Another
limitation of multilingual sentence encoders is the trade-off between
monolingual and cross-lingual performance. Training for cross-lingual alignment
of sentence embeddings distorts the optimal monolingual structure of semantic
spaces of individual languages, harming the utility of sentence embeddings in
monolingual tasks. In this work, we address both issues by modular training of
sentence encoders, i.e., by separating monolingual specialization from
cross-lingual alignment. We first efficiently train language-specific sentence
encoders to avoid negative interference between languages (i.e., the curse). We
then align all non-English monolingual encoders to the English encoder by
training a cross-lingual alignment adapter on top of each, preventing
interference with monolingual specialization from the first step. In both
steps, we resort to contrastive learning on machine-translated paraphrase data.
Monolingual and cross-lingual evaluations on semantic text
similarity/relatedness and multiple-choice QA render our modular solution more
effective than multilingual sentence encoders, especially benefiting
low-resource languages.","[{'name': 'Yongxin Huang'}, {'name': 'Kexin Wang'}, {'name': 'Goran Glavaš'}, {'name': 'Iryna Gurevych'}]",2024-07-20T13:56:39Z
http://arxiv.org/abs/2407.14875v1,http://arxiv.org/abs/2407.14875v1,Seal: Advancing Speech Language Models to be Few-Shot Learners,"Existing auto-regressive language models have demonstrated a remarkable
capability to perform a new task with just a few examples in prompt, without
requiring any additional training. In order to extend this capability to a
multi-modal setting (i.e. speech and language), this paper introduces the Seal
model, an abbreviation for speech language model. It incorporates a novel
alignment method, in which Kullback-Leibler divergence loss is performed to
train a projector that bridges a frozen speech encoder with a frozen language
model decoder. The resulting Seal model exhibits robust performance as a
few-shot learner on two speech understanding tasks. Additionally, consistency
experiments are conducted to validate its robustness on different pre-trained
language models.","[{'name': 'Shuyu Lei'}, {'name': 'Lingen Liu'}, {'name': 'Jiaolong Yang'}, {'name': 'Yasen Jiao'}, {'name': 'Yuxiang Yang'}, {'name': 'Yushu Yang'}, {'name': 'Xiang Guo'}]",2024-07-20T13:28:12Z
http://arxiv.org/abs/2407.14845v1,http://arxiv.org/abs/2407.14845v1,"Understanding the Relationship between Prompts and Response Uncertainty
  in Large Language Models","Large language models (LLMs) are widely used in decision-making, but their
reliability, especially in critical tasks like healthcare, is not
well-established. Therefore, understanding how LLMs reason and make decisions
is crucial for their safe deployment. This paper investigates how the
uncertainty of responses generated by LLMs relates to the information provided
in the input prompt. Leveraging the insight that LLMs learn to infer latent
concepts during pretraining, we propose a prompt-response concept model that
explains how LLMs generate responses and helps understand the relationship
between prompts and response uncertainty. We show that the uncertainty
decreases as the prompt's informativeness increases, similar to epistemic
uncertainty. Our detailed experimental results on real datasets validate our
proposed model.","[{'name': 'Ze Yu Zhang'}, {'name': 'Arun Verma'}, {'name': 'Finale Doshi-Velez'}, {'name': 'Bryan Kian Hsiang Low'}]",2024-07-20T11:19:58Z
http://arxiv.org/abs/2407.14829v2,http://arxiv.org/abs/2407.14829v2,Overview of AI-Debater 2023: The Challenges of Argument Generation Tasks,"In this paper we present the results of the AI-Debater 2023 Challenge held by
the Chinese Conference on Affect Computing (CCAC 2023), and introduce the
related datasets. We organize two tracks to handle the argumentative generation
tasks in different scenarios, namely, Counter-Argument Generation (Track 1) and
Claim-based Argument Generation (Track 2). Each track is equipped with its
distinct dataset and baseline model respectively. In total, 32 competing teams
register for the challenge, from which we received 11 successful submissions.
In this paper, we will present the results of the challenge and a summary of
the systems, highlighting commonalities and innovations among participating
systems. Datasets and baseline models of the AI-Debater 2023 Challenge have
been already released and can be accessed through the official website of the
challenge.","[{'name': 'Jiayu Lin'}, {'name': 'Guanrong Chen'}, {'name': 'Bojun Jin'}, {'name': 'Chenyang Li'}, {'name': 'Shutong Jia'}, {'name': 'Wancong Lin'}, {'name': 'Yang Sun'}, {'name': 'Yuhang He'}, {'name': 'Caihua Yang'}, {'name': 'Jianzhu Bao'}, {'name': 'Jipeng Wu'}, {'name': 'Wen Su'}, {'name': 'Jinglu Chen'}, {'name': 'Xinyi Li'}, {'name': 'Tianyu Chen'}, {'name': 'Mingjie Han'}, {'name': 'Shuaiwen Du'}, {'name': 'Zijian Wang'}, {'name': 'Jiyin Li'}, {'name': 'Fuzhong Suo'}, {'name': 'Hao Wang'}, {'name': 'Nuanchen Lin'}, {'name': 'Xuanjing Huang'}, {'name': 'Changjian Jiang'}, {'name': 'RuiFeng Xu'}, {'name': 'Long Zhang'}, {'name': 'Jiuxin Cao'}, {'name': 'Ting Jin'}, {'name': 'Zhongyu Wei'}]",2024-07-20T10:13:54Z
http://arxiv.org/abs/2407.14822v1,http://arxiv.org/abs/2407.14822v1,Text Style Transfer: An Introductory Overview,"Text Style Transfer (TST) is a pivotal task in natural language generation to
manipulate text style attributes while preserving style-independent content.
The attributes targeted in TST can vary widely, including politeness,
authorship, mitigation of offensive language, modification of feelings, and
adjustment of text formality. TST has become a widely researched topic with
substantial advancements in recent years. This paper provides an introductory
overview of TST, addressing its challenges, existing approaches, datasets,
evaluation measures, subtasks, and applications. This fundamental overview
improves understanding of the background and fundamentals of text style
transfer.","[{'name': 'Sourabrata Mukherjee'}, {'name': 'Ondrej Dušek'}]",2024-07-20T09:54:55Z
http://arxiv.org/abs/2407.14795v1,http://arxiv.org/abs/2407.14795v1,Automatic Real-word Error Correction in Persian Text,"Automatic spelling correction stands as a pivotal challenge within the ambit
of natural language processing (NLP), demanding nuanced solutions. Traditional
spelling correction techniques are typically only capable of detecting and
correcting non-word errors, such as typos and misspellings. However,
context-sensitive errors, also known as real-word errors, are more challenging
to detect because they are valid words that are used incorrectly in a given
context. The Persian language, characterized by its rich morphology and complex
syntax, presents formidable challenges to automatic spelling correction
systems. Furthermore, the limited availability of Persian language resources
makes it difficult to train effective spelling correction models. This paper
introduces a cutting-edge approach for precise and efficient real-word error
correction in Persian text. Our methodology adopts a structured, multi-tiered
approach, employing semantic analysis, feature selection, and advanced
classifiers to enhance error detection and correction efficacy. The innovative
architecture discovers and stores semantic similarities between words and
phrases in Persian text. The classifiers accurately identify real-word errors,
while the semantic ranking algorithm determines the most probable corrections
for real-word errors, taking into account specific spelling correction and
context properties such as context, semantic similarity, and edit-distance
measures. Evaluations have demonstrated that our proposed method surpasses
previous Persian real-word error correction models. Our method achieves an
impressive F-measure of 96.6% in the detection phase and an accuracy of 99.1%
in the correction phase. These results clearly indicate that our approach is a
highly promising solution for automatic real-word error correction in Persian
text.","[{'name': 'Seyed Mohammad Sadegh Dashti'}, {'name': 'Amid Khatibi Bardsiri'}, {'name': 'Mehdi Jafari Shahbazzadeh'}]",2024-07-20T07:50:52Z
http://arxiv.org/abs/2407.14790v1,http://arxiv.org/abs/2407.14790v1,Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?,"Solving grid puzzles involves a significant amount of logical reasoning.
Hence, it is a good domain to evaluate the reasoning capability of a model
which can then guide us to improve the reasoning ability of models. However,
most existing works evaluate only the final predicted answer of a puzzle,
without delving into an in-depth analysis of the LLMs' reasoning chains (such
as where they falter) or providing any finer metrics to evaluate them. Since
LLMs may rely on simple heuristics or artifacts to predict the final answer, it
is crucial to evaluate the generated reasoning chain beyond overall correctness
measures, for accurately evaluating the reasoning abilities of LLMs. To this
end, we first develop GridPuzzle, an evaluation dataset comprising 274
grid-based puzzles with different complexities. Second, we propose a new error
taxonomy derived from manual analysis of reasoning chains from LLMs including
GPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop an LLM-based
framework for large-scale subjective evaluation (i.e., identifying errors) and
an objective metric, PuzzleEval, to evaluate the correctness of reasoning
chains. Evaluating reasoning chains from LLMs leads to several interesting
findings. We further show that existing prompting methods used for enhancing
models' reasoning abilities do not improve performance on GridPuzzle. This
highlights the importance of understanding fine-grained errors and presents a
challenge for future research to enhance LLMs' puzzle-solving abilities by
developing methods that address these errors. Data and source code are
available at https://github.com/Mihir3009/GridPuzzle.","[{'name': 'Nemika Tyagi'}, {'name': 'Mihir Parmar'}, {'name': 'Mohith Kulkarni'}, {'name': 'Aswin RRV'}, {'name': 'Nisarg Patel'}, {'name': 'Mutsumi Nakamura'}, {'name': 'Arindam Mitra'}, {'name': 'Chitta Baral'}]",2024-07-20T07:43:07Z
http://arxiv.org/abs/2407.14767v1,http://arxiv.org/abs/2407.14767v1,"I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case
  Study on Text-to-SQL Generation","In this study, we explore the proactive ability of LLMs to seek user support,
using text-to-SQL generation as a case study. We propose metrics to evaluate
the trade-off between performance improvements and user burden, and investigate
whether LLMs can determine when to request help and examine their performance
with varying levels of information availability. Our experiments reveal that
without external feedback, many LLMs struggle to recognize their need for
additional support. Our findings highlight the importance of external signals
and provide insights for future research on improving support-seeking
strategies.","[{'name': 'Cheng-Kuang Wu'}, {'name': 'Zhi Rui Tam'}, {'name': 'Chao-Chung Wu'}, {'name': 'Chieh-Yen Lin'}, {'name': 'Hung-yi Lee'}, {'name': 'Yun-Nung Chen'}]",2024-07-20T06:12:29Z
http://arxiv.org/abs/2408.00798v1,http://arxiv.org/abs/2408.00798v1,"Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation
  for Industrial Knowledge Base","This paper introduces Golden-Retriever, designed to efficiently navigate vast
industrial knowledge bases, overcoming challenges in traditional LLM
fine-tuning and RAG frameworks with domain-specific jargon and context
interpretation. Golden-Retriever incorporates a reflection-based question
augmentation step before document retrieval, which involves identifying jargon,
clarifying its meaning based on context, and augmenting the question
accordingly. Specifically, our method extracts and lists all jargon and
abbreviations in the input question, determines the context against a
pre-defined list, and queries a jargon dictionary for extended definitions and
descriptions. This comprehensive augmentation ensures the RAG framework
retrieves the most relevant documents by providing clear context and resolving
ambiguities, significantly improving retrieval accuracy. Evaluations using
three open-source LLMs on a domain-specific question-answer dataset demonstrate
Golden-Retriever's superior performance, providing a robust solution for
efficiently integrating and querying industrial knowledge bases.","[{'name': 'Zhiyu An'}, {'name': 'Xianzhong Ding'}, {'name': 'Yen-Chun Fu'}, {'name': 'Cheng-Chung Chu'}, {'name': 'Yan Li'}, {'name': 'Wan Du'}]",2024-07-20T06:10:46Z
http://arxiv.org/abs/2407.14701v1,http://arxiv.org/abs/2407.14701v1,"Contextual modulation of language comprehension in a dynamic neural
  model of lexical meaning","We propose and computationally implement a dynamic neural model of lexical
meaning, and experimentally test its behavioral predictions. We demonstrate the
architecture and behavior of the model using as a test case the English lexical
item 'have', focusing on its polysemous use. In the model, 'have' maps to a
semantic space defined by two continuous conceptual dimensions, connectedness
and control asymmetry, previously proposed to parameterize the conceptual
system for language. The mapping is modeled as coupling between a neural node
representing the lexical item and neural fields representing the conceptual
dimensions. While lexical knowledge is modeled as a stable coupling pattern,
real-time lexical meaning retrieval is modeled as the motion of neural
activation patterns between metastable states corresponding to semantic
interpretations or readings. Model simulations capture two previously reported
empirical observations: (1) contextual modulation of lexical semantic
interpretation, and (2) individual variation in the magnitude of this
modulation. Simulations also generate a novel prediction that the by-trial
relationship between sentence reading time and acceptability should be
contextually modulated. An experiment combining self-paced reading and
acceptability judgments replicates previous results and confirms the new model
prediction. Altogether, results support a novel perspective on lexical
polysemy: that the many related meanings of a word are metastable neural
activation states that arise from the nonlinear dynamics of neural populations
governing interpretation on continuous semantic dimensions.","[{'name': 'Michael C. Stern'}, {'name': 'Maria M. Piñango'}]",2024-07-19T23:28:55Z
http://arxiv.org/abs/2407.14679v1,http://arxiv.org/abs/2407.14679v1,Compact Language Models via Pruning and Knowledge Distillation,"Large language models (LLMs) targeting different deployment scales and sizes
are currently produced by training each variant from scratch; this is extremely
compute-intensive. In this paper, we investigate if pruning an existing LLM and
then re-training it with a fraction (<3%) of the original training data can be
a suitable alternative to repeated, full retraining. To this end, we develop a
set of practical and effective compression best practices for LLMs that combine
depth, width, attention and MLP pruning with knowledge distillation-based
retraining; we arrive at these best practices through a detailed empirical
exploration of pruning strategies for each axis, methods to combine axes,
distillation strategies, and search techniques for arriving at optimal
compressed architectures. We use this guide to compress the Nemotron-4 family
of LLMs by a factor of 2-4x, and compare their performance to similarly-sized
models on a variety of language modeling tasks. Deriving 8B and 4B models from
an already pretrained 15B model using our approach requires up to 40x fewer
training tokens per model compared to training from scratch; this results in
compute cost savings of 1.8x for training the full model family (15B, 8B, and
4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to
training from scratch, perform comparably to other community models such as
Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art
compression techniques from the literature. We have open-sourced Minitron model
weights on Huggingface, with corresponding supplementary material including
example code available on GitHub.","[{'name': 'Saurav Muralidharan'}, {'name': 'Sharath Turuvekere Sreenivas'}, {'name': 'Raviraj Joshi'}, {'name': 'Marcin Chochowski'}, {'name': 'Mostofa Patwary'}, {'name': 'Mohammad Shoeybi'}, {'name': 'Bryan Catanzaro'}, {'name': 'Jan Kautz'}, {'name': 'Pavlo Molchanov'}]",2024-07-19T21:47:57Z
http://arxiv.org/abs/2407.21038v1,http://arxiv.org/abs/2407.21038v1,"Advancing Chart Question Answering with Robust Chart Component
  Recognition","Chart comprehension presents significant challenges for machine learning
models due to the diverse and intricate shapes of charts. Existing multimodal
methods often overlook these visual features or fail to integrate them
effectively for chart question answering (ChartQA). To address this, we
introduce Chartformer, a unified framework that enhances chart component
recognition by accurately identifying and classifying components such as bars,
lines, pies, titles, legends, and axes. Additionally, we propose a novel
Question-guided Deformable Co-Attention (QDCAt) mechanism, which fuses chart
features encoded by Chartformer with the given question, leveraging the
question's guidance to ground the correct answer. Extensive experiments
demonstrate that the proposed approaches significantly outperform baseline
models in chart component recognition and ChartQA tasks, achieving improvements
of 3.2% in mAP and 15.4% in accuracy, respectively. These results underscore
the robustness of our solution for detailed visual data interpretation across
various applications.","[{'name': 'Hanwen Zheng'}, {'name': 'Sijia Wang'}, {'name': 'Chris Thomas'}, {'name': 'Lifu Huang'}]",2024-07-19T20:55:06Z
http://arxiv.org/abs/2407.14644v2,http://arxiv.org/abs/2407.14644v2,"Human-Interpretable Adversarial Prompt Attack on Large Language Models
  with Situational Context","Previous research on testing the vulnerabilities in Large Language Models
(LLMs) using adversarial attacks has primarily focused on nonsensical prompt
injections, which are easily detected upon manual or automated review (e.g.,
via byte entropy). However, the exploration of innocuous human-understandable
malicious prompts augmented with adversarial injections remains limited. In
this research, we explore converting a nonsensical suffix attack into a
sensible prompt via a situation-driven contextual re-writing. This allows us to
show suffix conversion without any gradients, using only LLMs to perform the
attacks, and thus better understand the scope of possible risks. We combine an
independent, meaningful adversarial insertion and situations derived from
movies to check if this can trick an LLM. The situations are extracted from the
IMDB dataset, and prompts are defined following a few-shot chain-of-thought
prompting. Our approach demonstrates that a successful situation-driven attack
can be executed on both open-source and proprietary LLMs. We find that across
many LLMs, as few as 1 attempt produces an attack and that these attacks
transfer between LLMs.","[{'name': 'Nilanjana Das'}, {'name': 'Edward Raff'}, {'name': 'Manas Gaur'}]",2024-07-19T19:47:26Z
http://arxiv.org/abs/2407.14640v1,http://arxiv.org/abs/2407.14640v1,"CVE-LLM : Automatic vulnerability evaluation in medical device industry
  using large language models","The healthcare industry is currently experiencing an unprecedented wave of
cybersecurity attacks, impacting millions of individuals. With the discovery of
thousands of vulnerabilities each month, there is a pressing need to drive the
automation of vulnerability assessment processes for medical devices,
facilitating rapid mitigation efforts. Generative AI systems have
revolutionized various industries, offering unparalleled opportunities for
automation and increased efficiency. This paper presents a solution leveraging
Large Language Models (LLMs) to learn from historical evaluations of
vulnerabilities for the automatic assessment of vulnerabilities in the medical
devices industry. This approach is applied within the portfolio of a single
manufacturer, taking into account device characteristics, including existing
security posture and controls. The primary contributions of this paper are
threefold. Firstly, it provides a detailed examination of the best practices
for training a vulnerability Language Model (LM) in an industrial context.
Secondly, it presents a comprehensive comparison and insightful analysis of the
effectiveness of Language Models in vulnerability assessment. Finally, it
proposes a new human-in-the-loop framework to expedite vulnerability evaluation
processes.","[{'name': 'Rikhiya Ghosh'}, {'name': 'Oladimeji Farri'}, {'name': 'Hans-Martin von Stockhausen'}, {'name': 'Martin Schmitt'}, {'name': 'George Marica Vasile'}]",2024-07-19T19:34:17Z
http://arxiv.org/abs/2407.14622v1,http://arxiv.org/abs/2407.14622v1,BOND: Aligning LLMs with Best-of-N Distillation,"Reinforcement learning from human feedback (RLHF) is a key driver of quality
and safety in state-of-the-art large language models. Yet, a surprisingly
simple and strong inference-time strategy is Best-of-N sampling that selects
the best generation among N candidates. In this paper, we propose Best-of-N
Distillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but
without its significant computational overhead at inference time. Specifically,
BOND is a distribution matching algorithm that forces the distribution of
generations from the policy to get closer to the Best-of-N distribution. We use
the Jeffreys divergence (a linear combination of forward and backward KL) to
balance between mode-covering and mode-seeking behavior, and derive an
iterative formulation that utilizes a moving anchor for efficiency. We
demonstrate the effectiveness of our approach and several design choices
through experiments on abstractive summarization and Gemma models. Aligning
Gemma policies with BOND outperforms other RLHF algorithms by improving results
on several benchmarks.","[{'name': 'Pier Giuseppe Sessa'}, {'name': 'Robert Dadashi'}, {'name': 'Léonard Hussenot'}, {'name': 'Johan Ferret'}, {'name': 'Nino Vieillard'}, {'name': 'Alexandre Ramé'}, {'name': 'Bobak Shariari'}, {'name': 'Sarah Perrin'}, {'name': 'Abe Friesen'}, {'name': 'Geoffrey Cideron'}, {'name': 'Sertan Girgin'}, {'name': 'Piotr Stanczyk'}, {'name': 'Andrea Michi'}, {'name': 'Danila Sinopalnikov'}, {'name': 'Sabela Ramos'}, {'name': 'Amélie Héliou'}, {'name': 'Aliaksei Severyn'}, {'name': 'Matt Hoffman'}, {'name': 'Nikola Momchev'}, {'name': 'Olivier Bachem'}]",2024-07-19T18:38:25Z
http://arxiv.org/abs/2407.14614v1,http://arxiv.org/abs/2407.14614v1,Evaluating language models as risk scores,"Current question-answering benchmarks predominantly focus on accuracy in
realizable prediction tasks. Conditioned on a question and answer-key, does the
most likely token match the ground truth? Such benchmarks necessarily fail to
evaluate language models' ability to quantify outcome uncertainty. In this
work, we focus on the use of language models as risk scores for unrealizable
prediction tasks. We introduce folktexts, a software package to systematically
generate risk scores using large language models, and evaluate them against
benchmark prediction tasks. Specifically, the package derives natural language
tasks from US Census data products, inspired by popular tabular data
benchmarks. A flexible API allows for any task to be constructed out of 28
census features whose values are mapped to prompt-completion pairs. We
demonstrate the utility of folktexts through a sweep of empirical insights on
16 recent large language models, inspecting risk scores, calibration curves,
and diverse evaluation metrics. We find that zero-shot risk sores have high
predictive signal while being widely miscalibrated: base models overestimate
outcome uncertainty, while instruction-tuned models underestimate uncertainty
and generate over-confident risk scores.","[{'name': 'André F. Cruz'}, {'name': 'Moritz Hardt'}, {'name': 'Celestine Mendler-Dünner'}]",2024-07-19T18:13:37Z
http://arxiv.org/abs/2407.14609v1,http://arxiv.org/abs/2407.14609v1,"Adversarial Databases Improve Success in Retrieval-based Large Language
  Models","Open-source LLMs have shown great potential as fine-tuned chatbots, and
demonstrate robust abilities in reasoning and surpass many existing benchmarks.
Retrieval-Augmented Generation (RAG) is a technique for improving the
performance of LLMs on tasks that the models weren't explicitly trained on, by
leveraging external knowledge databases. Numerous studies have demonstrated the
effectiveness of RAG to more successfully accomplish downstream tasks when
using vector datasets that consist of relevant background information. It has
been implicitly assumed by those in the field that if adversarial background
information is utilized in this context, that the success of using a RAG-based
approach would be nonexistent or even negatively impact the results. To address
this assumption, we tested several open-source LLMs on the ability of RAG to
improve their success in answering multiple-choice questions (MCQ) in the
medical subspecialty field of Nephrology. Unlike previous studies, we examined
the effect of RAG in utilizing both relevant and adversarial background
databases. We set up several open-source LLMs, including Llama 3, Phi-3,
Mixtral 8x7b, Zephyr$\beta$, and Gemma 7B Instruct, in a zero-shot RAG
pipeline. As adversarial sources of information, text from the Bible and a
Random Words generated database were used for comparison. Our data show that
most of the open-source LLMs improve their multiple-choice test-taking success
as expected when incorporating relevant information vector databases.
Surprisingly however, adversarial Bible text significantly improved the success
of many LLMs and even random word text improved test taking ability of some of
the models. In summary, our results demonstrate for the first time the
countertintuitive ability of adversarial information datasets to improve the
RAG-based LLM success.","[{'name': 'Sean Wu'}, {'name': 'Michael Koo'}, {'name': 'Li Yo Kao'}, {'name': 'Andy Black'}, {'name': 'Lesley Blum'}, {'name': 'Fabien Scalzo'}, {'name': 'Ira Kurtz'}]",2024-07-19T18:08:39Z
http://arxiv.org/abs/2407.14507v1,http://arxiv.org/abs/2407.14507v1,"Internal Consistency and Self-Feedback in Large Language Models: A
  Survey","Large language models (LLMs) are expected to respond accurately but often
exhibit deficient reasoning or generate hallucinatory content. To address
these, studies prefixed with ``Self-'' such as Self-Consistency, Self-Improve,
and Self-Refine have been initiated. They share a commonality: involving LLMs
evaluating and updating itself to mitigate the issues. Nonetheless, these
efforts lack a unified perspective on summarization, as existing surveys
predominantly focus on categorization without examining the motivations behind
these works.
  In this paper, we summarize a theoretical framework, termed Internal
Consistency, which offers unified explanations for phenomena such as the lack
of reasoning and the presence of hallucinations. Internal Consistency assesses
the coherence among LLMs' latent layer, decoding layer, and response layer
based on sampling methodologies. Expanding upon the Internal Consistency
framework, we introduce a streamlined yet effective theoretical framework
capable of mining Internal Consistency, named Self-Feedback. The Self-Feedback
framework consists of two modules: Self-Evaluation and Self-Update. This
framework has been employed in numerous studies.
  We systematically classify these studies by tasks and lines of work;
summarize relevant evaluation methods and benchmarks; and delve into the
concern, ``Does Self-Feedback Really Work?'' We propose several critical
viewpoints, including the ``Hourglass Evolution of Internal Consistency'',
``Consistency Is (Almost) Correctness'' hypothesis, and ``The Paradox of Latent
and Explicit Reasoning''. Furthermore, we outline promising directions for
future research. We have open-sourced the experimental code, reference list,
and statistical data, available at
\url{https://github.com/IAAR-Shanghai/ICSFSurvey}.","[{'name': 'Xun Liang'}, {'name': 'Shichao Song'}, {'name': 'Zifan Zheng'}, {'name': 'Hanyu Wang'}, {'name': 'Qingchen Yu'}, {'name': 'Xunkai Li'}, {'name': 'Rong-Hua Li'}, {'name': 'Feiyu Xiong'}, {'name': 'Zhiyu Li'}]",2024-07-19T17:59:03Z
http://arxiv.org/abs/2407.14506v2,http://arxiv.org/abs/2407.14506v2,"On Pre-training of Multimodal Language Models Customized for Chart
  Understanding","Recent studies customizing Multimodal Large Language Models (MLLMs) for
domain-specific tasks have yielded promising results, especially in the field
of scientific chart comprehension. These studies generally utilize visual
instruction tuning with specialized datasets to enhance question and answer
(QA) accuracy within the chart domain. However, they often neglect the
fundamental discrepancy between natural image-caption pre-training data and
digital chart image-QA data, particularly in the models' capacity to extract
underlying numeric values from charts. This paper tackles this oversight by
exploring the training processes necessary to improve MLLMs' comprehension of
charts. We present three key findings: (1) Incorporating raw data values in
alignment pre-training markedly improves comprehension of chart data. (2)
Replacing images with their textual representation randomly during end-to-end
fine-tuning transfer the language reasoning capability to chart interpretation
skills. (3) Requiring the model to first extract the underlying chart data and
then answer the question in the fine-tuning can further improve the accuracy.
Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart
comprehension. CHOPINLLM effectively interprets various types of charts,
including unannotated ones, while maintaining robust reasoning abilities.
Furthermore, we establish a new benchmark to evaluate MLLMs' understanding of
different chart types across various comprehension levels. Experimental results
show that CHOPINLLM exhibits strong performance in understanding both annotated
and unannotated charts across a wide range of types.","[{'name': 'Wan-Cyuan Fan'}, {'name': 'Yen-Chun Chen'}, {'name': 'Mengchen Liu'}, {'name': 'Lu Yuan'}, {'name': 'Leonid Sigal'}]",2024-07-19T17:58:36Z
http://arxiv.org/abs/2407.14487v1,http://arxiv.org/abs/2407.14487v1,Evaluating the Reliability of Self-Explanations in Large Language Models,"This paper investigates the reliability of explanations generated by large
language models (LLMs) when prompted to explain their previous output. We
evaluate two kinds of such self-explanations - extractive and counterfactual -
using three state-of-the-art LLMs (2B to 8B parameters) on two different
classification tasks (objective and subjective). Our findings reveal, that,
while these self-explanations can correlate with human judgement, they do not
fully and accurately follow the model's decision process, indicating a gap
between perceived and actual model reasoning. We show that this gap can be
bridged because prompting LLMs for counterfactual explanations can produce
faithful, informative, and easy-to-verify results. These counterfactuals offer
a promising alternative to traditional explainability methods (e.g. SHAP,
LIME), provided that prompts are tailored to specific tasks and checked for
validity.","[{'name': 'Korbinian Randl'}, {'name': 'John Pavlopoulos'}, {'name': 'Aron Henriksson'}, {'name': 'Tony Lindgren'}]",2024-07-19T17:41:08Z
http://arxiv.org/abs/2407.14482v1,http://arxiv.org/abs/2407.14482v1,"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG
  Capabilities","In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge
the gap between open-access LLMs and leading proprietary models (e.g.,
GPT-4-Turbo) in long-context understanding and retrieval-augmented generation
(RAG) capabilities. These two capabilities are essential for LLMs to process
large volumes of information that cannot fit into a single prompt and are
complementary to each other, depending on the downstream tasks and
computational budgets. We present a detailed continued training recipe to
extend the context window of Llama3-70B-base from 8K to 128K tokens, along with
a three-stage instruction tuning process to enhance the model's
instruction-following, RAG performance, and long-context understanding
capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model
achieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context
understanding tasks and surpasses it on the RAG benchmark. Interestingly, we
find that the state-of-the-art long-context retriever can alleviate the top-k
context fragmentation issue in RAG, further improving RAG-based results for
long-context understanding tasks. We also provide extensive comparisons between
RAG and long-context solutions using state-of-the-art long-context LLMs.","[{'name': 'Peng Xu'}, {'name': 'Wei Ping'}, {'name': 'Xianchao Wu'}, {'name': 'Zihan Liu'}, {'name': 'Mohammad Shoeybi'}, {'name': 'Bryan Catanzaro'}]",2024-07-19T17:35:47Z
http://arxiv.org/abs/2407.14467v1,http://arxiv.org/abs/2407.14467v1,Check-Eval: A Checklist-based Approach for Evaluating Text Quality,"Evaluating the quality of text generated by large language models (LLMs)
remains a significant challenge. Traditional metrics often fail to align well
with human judgments, particularly in tasks requiring creativity and nuance. In
this paper, we propose Check-Eval, a novel evaluation framework leveraging LLMs
to assess the quality of generated text through a checklist-based approach.
Check-Eval can be employed as both a reference-free and reference-dependent
evaluation method, providing a structured and interpretable assessment of text
quality. The framework consists of two main stages: checklist generation and
checklist evaluation. We validate Check-Eval on two benchmark datasets:
Portuguese Legal Semantic Textual Similarity and SummEval. Our results
demonstrate that Check-Eval achieves higher correlations with human judgments
compared to existing metrics, such as G-Eval and GPTScore, underscoring its
potential as a more reliable and effective evaluation framework for natural
language generation tasks. The code for our experiments is available at
https://anonymous.4open.science/r/check-eval-0DB4.","[{'name': 'Jayr Pereira'}, {'name': 'Roberto Lotufo'}]",2024-07-19T17:14:16Z
http://arxiv.org/abs/2407.14458v1,http://arxiv.org/abs/2407.14458v1,"AudioInsight: Detecting Social Contexts Relevant to Social Anxiety from
  Speech","During social interactions, understanding the intricacies of the context can
be vital, particularly for socially anxious individuals. While previous
research has found that the presence of a social interaction can be detected
from ambient audio, the nuances within social contexts, which influence how
anxiety provoking interactions are, remain largely unexplored. As an
alternative to traditional, burdensome methods like self-report, this study
presents a novel approach that harnesses ambient audio segments to detect
social threat contexts. We focus on two key dimensions: number of interaction
partners (dyadic vs. group) and degree of evaluative threat (explicitly
evaluative vs. not explicitly evaluative). Building on data from a Zoom-based
social interaction study (N=52 college students, of whom the majority N=45 are
socially anxious), we employ deep learning methods to achieve strong detection
performance. Under sample-wide 5-fold Cross Validation (CV), our model
distinguished dyadic from group interactions with 90\% accuracy and detected
evaluative threat at 83\%. Using a leave-one-group-out CV, accuracies were 82\%
and 77\%, respectively. While our data are based on virtual interactions due to
pandemic constraints, our method has the potential to extend to diverse
real-world settings. This research underscores the potential of passive sensing
and AI to differentiate intricate social contexts, and may ultimately advance
the ability of context-aware digital interventions to offer personalized mental
health support.","[{'name': 'Varun Reddy'}, {'name': 'Zhiyuan Wang'}, {'name': 'Emma Toner'}, {'name': 'Max Larrazabal'}, {'name': 'Mehdi Boukhechba'}, {'name': 'Bethany A. Teachman'}, {'name': 'Laura E. Barnes'}]",2024-07-19T17:01:12Z
http://arxiv.org/abs/2407.14371v1,http://arxiv.org/abs/2407.14371v1,Open Artificial Knowledge,"The tremendous success of chat-based AI systems like ChatGPT, Claude, and
Gemini stems from Large Language Models (LLMs) trained on vast amount of
datasets. However, acquiring high-quality, diverse, and ethically sourced
training data remains a significant challenge. We introduce the Open Artificial
Knowledge (OAK) dataset, a large-scale resource of over 500 million tokens (at
the moment of writing) designed to address this issue. OAK leverages an
ensemble of state-of-the-art LLMs, including GPT4o, LLaMa3-70B, LLaMa3-8B,
Mixtral-8x7B, Gemma-7B, and Gemma-2-9B , to generate high-quality text across
diverse domains, guided by Wikipedia's main categories. Our methodology ensures
broad knowledge coverage while maintaining coherence and factual accuracy. The
OAK dataset aims to foster the development of more capable and aligned language
models while addressing critical issues of data scarcity and privacy in LLM
training, and it is freely available on www.oakdataset.org.","[{'name': 'Vadim Borisov'}, {'name': 'Richard H. Schreiber'}]",2024-07-19T15:01:24Z
http://arxiv.org/abs/2407.14346v1,http://arxiv.org/abs/2407.14346v1,"Improving Retrieval in Sponsored Search by Leveraging Query Context
  Signals","Accurately retrieving relevant bid keywords for user queries is critical in
Sponsored Search but remains challenging, particularly for short, ambiguous
queries. Existing dense and generative retrieval models often fail to capture
nuanced user intent in these cases. To address this, we propose an approach to
enhance query understanding by augmenting queries with rich contextual signals
derived from web search results and large language models, stored in an online
cache. Specifically, we use web search titles and snippets to ground queries in
real-world information and utilize GPT-4 to generate query rewrites and
explanations that clarify user intent. These signals are efficiently integrated
through a Fusion-in-Decoder based Unity architecture, enabling both dense and
generative retrieval with serving costs on par with traditional context-free
models. To address scenarios where context is unavailable in the cache, we
introduce context glancing, a curriculum learning strategy that improves model
robustness and performance even without contextual signals during inference.
Extensive offline experiments demonstrate that our context-aware approach
substantially outperforms context-free models. Furthermore, online A/B testing
on a prominent search engine across 160+ countries shows significant
improvements in user engagement and revenue.","[{'name': 'Akash Kumar Mohankumar'}, {'name': 'Gururaj K'}, {'name': 'Gagan Madan'}, {'name': 'Amit Singh'}]",2024-07-19T14:28:53Z
http://arxiv.org/abs/2407.14344v1,http://arxiv.org/abs/2407.14344v1,"LLMs left, right, and center: Assessing GPT's capabilities to label
  political bias from web domains","This research investigates whether OpenAI's GPT-4, a state-of-the-art large
language model, can accurately classify the political bias of news sources
based solely on their URLs. Given the subjective nature of political labels,
third-party bias ratings like those from Ad Fontes Media, AllSides, and Media
Bias/Fact Check (MBFC) are often used in research to analyze news source
diversity. This study aims to determine if GPT-4 can replicate these human
ratings on a seven-degree scale (""far-left"" to ""far-right""). The analysis
compares GPT-4's classifications against MBFC's, and controls for website
popularity using Open PageRank scores. Findings reveal a high correlation
($\text{Spearman's } \rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and
MBFC's ratings, indicating the model's potential reliability. However, GPT-4
abstained from classifying approximately $\frac{2}{3}$ of the dataset,
particularly less popular and less biased sources. The study also identifies a
slight leftward skew in GPT-4's classifications compared to MBFC's. The
analysis suggests that while GPT-4 can be a scalable, cost-effective tool for
political bias classification of news websites, but its use should complement
human judgment to mitigate biases. Further research is recommended to explore
the model's performance across different settings, languages, and additional
datasets.",[{'name': 'Raphael Hernandes'}],2024-07-19T14:28:07Z
http://arxiv.org/abs/2407.14321v1,http://arxiv.org/abs/2407.14321v1,Multimodal Misinformation Detection using Large Vision-Language Models,"The increasing proliferation of misinformation and its alarming impact have
motivated both industry and academia to develop approaches for misinformation
detection and fact checking. Recent advances on large language models (LLMs)
have shown remarkable performance in various tasks, but whether and how LLMs
could help with misinformation detection remains relatively underexplored. Most
of existing state-of-the-art approaches either do not consider evidence and
solely focus on claim related features or assume the evidence to be provided.
Few approaches consider evidence retrieval as part of the misinformation
detection but rely on fine-tuning models. In this paper, we investigate the
potential of LLMs for misinformation detection in a zero-shot setting. We
incorporate an evidence retrieval component into the process as it is crucial
to gather pertinent information from various sources to detect the veracity of
claims. To this end, we propose a novel re-ranking approach for multimodal
evidence retrieval using both LLMs and large vision-language models (LVLM). The
retrieved evidence samples (images and texts) serve as the input for an
LVLM-based approach for multimodal fact verification (LVLM4FV). To enable a
fair evaluation, we address the issue of incomplete ground truth for evidence
samples in an existing evidence retrieval dataset by annotating a more complete
set of evidence samples for both image and text retrieval. Our experimental
results on two datasets demonstrate the superiority of the proposed approach in
both evidence retrieval and fact verification tasks and also better
generalization capability across dataset compared to the supervised baseline.","[{'name': 'Sahar Tahmasebi'}, {'name': 'Eric Müller-Budack'}, {'name': 'Ralph Ewerth'}]",2024-07-19T13:57:11Z
http://arxiv.org/abs/2407.14309v2,http://arxiv.org/abs/2407.14309v2,"How to Engage Your Readers? Generating Guiding Questions to Promote
  Active Reading","Using questions in written text is an effective strategy to enhance
readability. However, what makes an active reading question good, what the
linguistic role of these questions is, and what is their impact on human
reading remains understudied. We introduce GuidingQ, a dataset of 10K in-text
questions from textbooks and scientific articles. By analyzing the dataset, we
present a comprehensive understanding of the use, distribution, and linguistic
characteristics of these questions. Then, we explore various approaches to
generate such questions using language models. Our results highlight the
importance of capturing inter-question relationships and the challenge of
question position identification in generating these questions. Finally, we
conduct a human study to understand the implication of such questions on
reading comprehension. We find that the generated questions are of high quality
and are almost as effective as human-written questions in terms of improving
readers' memorization and comprehension.","[{'name': 'Peng Cui'}, {'name': 'Vilém Zouhar'}, {'name': 'Xiaoyu Zhang'}, {'name': 'Mrinmaya Sachan'}]",2024-07-19T13:42:56Z
http://arxiv.org/abs/2407.14296v2,http://arxiv.org/abs/2407.14296v2,Foundation Models for Autonomous Robots in Unstructured Environments,"Automating activities through robots in unstructured environments, such as
construction sites, has been a long-standing desire. However, the high degree
of unpredictable events in these settings has resulted in far less adoption
compared to more structured settings, such as manufacturing, where robots can
be hard-coded or trained on narrowly defined datasets. Recently, pretrained
foundation models, such as Large Language Models (LLMs), have demonstrated
superior generalization capabilities by providing zero-shot solutions for
problems do not present in the training data, proposing them as a potential
solution for introducing robots to unstructured environments. To this end, this
study investigates potential opportunities and challenges of pretrained
foundation models from a multi-dimensional perspective. The study
systematically reviews application of foundation models in two field of robotic
and unstructured environment and then synthesized them with deliberative acting
theory. Findings showed that linguistic capabilities of LLMs have been utilized
more than other features for improving perception in human-robot interactions.
On the other hand, findings showed that the use of LLMs demonstrated more
applications in project management and safety in construction, and natural
hazard detection in disaster management. Synthesizing these findings, we
located the current state-of-the-art in this field on a five-level scale of
automation, placing them at conditional automation. This assessment was then
used to envision future scenarios, challenges, and solutions toward autonomous
safe unstructured environments. Our study can be seen as a benchmark to track
our progress toward that future.","[{'name': 'Hossein Naderi'}, {'name': 'Alireza Shojaei'}, {'name': 'Lifu Huang'}]",2024-07-19T13:26:52Z
http://arxiv.org/abs/2407.14295v1,http://arxiv.org/abs/2407.14295v1,"CoVoSwitch: Machine Translation of Synthetic Code-Switched Text Based on
  Intonation Units","Multilingual code-switching research is often hindered by the lack and
linguistically biased status of available datasets. To expand language
representation, we synthesize code-switching data by replacing intonation units
detected through PSST, a speech segmentation model fine-tuned from OpenAI's
Whisper, using a speech-to-text translation dataset, CoVoST 2. With our
dataset, CoVoSwitch, spanning 13 languages, we evaluate the code-switching
translation performance of two multilingual translation models, M2M-100 418M
and NLLB-200 600M. We reveal that the inclusion of code-switching units results
in higher translation performance than monolingual settings and that models are
better at code-switching translation into English than non-English. Further,
low-resource languages gain most from integration of code-switched units when
translating into English but much less when translating into non-English.
Translations into low-resource languages also perform worse than even raw
code-switched inputs. We find that systems excel at copying English tokens but
struggle with non-English tokens, that the off-target problem in monolingual
settings is also relevant in code-switching settings, and that models
hallucinate in code-switching translation by introducing words absent in both
of the original source sentences. CoVoSwitch and code are available at
https://github.com/sophiayk20/covoswitch.",[{'name': 'Yeeun Kang'}],2024-07-19T13:26:35Z
http://arxiv.org/abs/2407.14259v1,http://arxiv.org/abs/2407.14259v1,Voices in a Crowd: Searching for Clusters of Unique Perspectives,"Language models have been shown to reproduce underlying biases existing in
their training data, which is the majority perspective by default. Proposed
solutions aim to capture minority perspectives by either modelling annotator
disagreements or grouping annotators based on shared metadata, both of which
face significant challenges. We propose a framework that trains models without
encoding annotator metadata, extracts latent embeddings informed by annotator
behaviour, and creates clusters of similar opinions, that we refer to as
voices. Resulting clusters are validated post-hoc via internal and external
quantitative metrics, as well a qualitative analysis to identify the type of
voice that each cluster represents. Our results demonstrate the strong
generalisation capability of our framework, indicated by resulting clusters
being adequately robust, while also capturing minority perspectives based on
different demographic factors throughout two distinct datasets.","[{'name': 'Nikolas Vitsakis'}, {'name': 'Amit Parekh'}, {'name': 'Ioannis Konstas'}]",2024-07-19T12:37:15Z
http://arxiv.org/abs/2407.14246v2,http://arxiv.org/abs/2407.14246v2,Unipa-GPT: Large Language Models for university-oriented QA in Italian,"This paper illustrates the architecture and training of Unipa-GPT, a chatbot
relying on a Large Language Model, developed for assisting students in choosing
a bachelor/master degree course at the University of Palermo. Unipa-GPT relies
on gpt-3.5-turbo, it was presented in the context of the European Researchers'
Night (SHARPER night). In our experiments we adopted both the Retrieval
Augmented Generation (RAG) approach and fine-tuning to develop the system. The
whole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned
systems are compared, and a brief discussion on their performance is reported.
Further comparison with other Large Language Models and the experimental
results during the SHARPER night are illustrated.","[{'name': 'Irene Siragusa'}, {'name': 'Roberto Pirrone'}]",2024-07-19T12:28:22Z
http://arxiv.org/abs/2407.14224v1,http://arxiv.org/abs/2407.14224v1,"Hierarchical Windowed Graph Attention Network and a Large Scale Dataset
  for Isolated Indian Sign Language Recognition","Automatic Sign Language (SL) recognition is an important task in the computer
vision community. To build a robust SL recognition system, we need a
considerable amount of data which is lacking particularly in Indian sign
language (ISL). In this paper, we propose a large-scale isolated ISL dataset
and a novel SL recognition model based on skeleton graph structure. The dataset
covers 2,002 daily used common words in the deaf community recorded by 20 (10
male and 10 female) deaf adult signers (contains 40033 videos). We propose a SL
recognition model namely Hierarchical Windowed Graph Attention Network (HWGAT)
by utilizing the human upper body skeleton graph structure. The HWGAT tries to
capture distinctive motions by giving attention to different body parts induced
by the human skeleton graph structure. The utility of the proposed dataset and
the usefulness of our model are evaluated through extensive experiments. We
pre-trained the proposed model on the proposed dataset and fine-tuned it across
different sign language datasets further boosting the performance of 1.10,
0.46, 0.78, and 6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL
respectively compared to the existing state-of-the-art skeleton-based models.","[{'name': 'Suvajit Patra'}, {'name': 'Arkadip Maitra'}, {'name': 'Megha Tiwari'}, {'name': 'K. Kumaran'}, {'name': 'Swathy Prabhu'}, {'name': 'Swami Punyeshwarananda'}, {'name': 'Soumitra Samanta'}]",2024-07-19T11:48:36Z
http://arxiv.org/abs/2407.14212v1,http://arxiv.org/abs/2407.14212v1,"Braille-to-Speech Generator: Audio Generation Based on Joint Fine-Tuning
  of CLIP and Fastspeech2","An increasing number of Chinese people are troubled by different degrees of
visual impairment, which has made the modal conversion between a single image
or video frame in the visual field and the audio expressing the same
information a research hotspot. Deep learning technologies such as OCR+Vocoder
and Im2Wav enable English audio synthesis or image-to-sound matching in a
self-supervised manner. However, the audio data used for training is limited
and English is not universal for visually impaired people with different
educational levels. Therefore, for the sake of solving the problems of data
volume and language applicability to improve the reading efficiency of visually
impaired people, a set of image-to-speech framework CLIP-KNN-Fastspeech2 based
on the Chinese context was constructed. The framework integrates multiple basic
models and adopts the strategy of independent pre-training and joint
fine-tuning. First, the Chinese CLIP and Fastspeech2 text-to-speech models were
pre-trained on two public datasets, MUGE and Baker, respectively, and their
convergence was verified. Subsequently, joint fine-tuning was performed using a
self-built Braille image dataset. Experimental results on multiple public
datasets such as VGGSound, Flickr8k, ImageHear, and the self-built Braille
dataset BIT-DP show that the model has improved objective indicators such as
BLEU4,FAD(Fr\'echet Audio Distance), WER(Word Error Ratio), and even inference
speed. This verifies that the constructed model still has the ability to
synthesize high-quality speech under limited data, and also proves the
effectiveness of the joint training strategy that integrates multiple basic
models.","[{'name': 'Chun Xu'}, {'name': 'En-Wei Sun'}]",2024-07-19T11:18:44Z
http://arxiv.org/abs/2407.14192v1,http://arxiv.org/abs/2407.14192v1,LeKUBE: A Legal Knowledge Update BEnchmark,"Recent advances in Large Language Models (LLMs) have significantly shaped the
applications of AI in multiple fields, including the studies of legal
intelligence. Trained on extensive legal texts, including statutes and legal
documents, the legal LLMs can capture important legal knowledge/concepts
effectively and provide important support for downstream legal applications
such as legal consultancy. Yet, the dynamic nature of legal statutes and
interpretations also poses new challenges to the use of LLMs in legal
applications. Particularly, how to update the legal knowledge of LLMs
effectively and efficiently has become an important research problem in
practice. Existing benchmarks for evaluating knowledge update methods are
mostly designed for the open domain and cannot address the specific challenges
of the legal domain, such as the nuanced application of new legal knowledge,
the complexity and lengthiness of legal regulations, and the intricate nature
of legal reasoning. To address this gap, we introduce the Legal Knowledge
Update BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for
legal LLMs across five dimensions. Specifically, we categorize the needs of
knowledge updates in the legal domain with the help of legal professionals, and
then hire annotators from law schools to create synthetic updates to the
Chinese Criminal and Civil Code as well as sets of questions of which the
answers would change after the updates. Through a comprehensive evaluation of
state-of-the-art knowledge update methods, we reveal a notable gap between
existing knowledge update methods and the unique needs of the legal domain,
emphasizing the need for further research and development of knowledge update
mechanisms tailored for legal LLMs.","[{'name': 'Changyue Wang'}, {'name': 'Weihang Su'}, {'name': 'Hu Yiran'}, {'name': 'Qingyao Ai'}, {'name': 'Yueyue Wu'}, {'name': 'Cheng Luo'}, {'name': 'Yiqun Liu'}, {'name': 'Min Zhang'}, {'name': 'Shaoping Ma'}]",2024-07-19T10:40:10Z
http://arxiv.org/abs/2407.14145v1,http://arxiv.org/abs/2407.14145v1,PassTSL: Modeling Human-Created Passwords through Two-Stage Learning,"Textual passwords are still the most widely used user authentication
mechanism. Due to the close connections between textual passwords and natural
languages, advanced technologies in natural language processing (NLP) and
machine learning (ML) could be used to model passwords for different purposes
such as studying human password-creation behaviors and developing more advanced
password cracking methods for informing better defence mechanisms. In this
paper, we propose PassTSL (modeling human-created Passwords through Two-Stage
Learning), inspired by the popular pretraining-finetuning framework in NLP and
deep learning (DL). We report how different pretraining settings affected
PassTSL and proved its effectiveness by applying it to six large leaked
password databases. Experimental results showed that it outperforms five
state-of-the-art (SOTA) password cracking methods on password guessing by a
significant margin ranging from 4.11% to 64.69% at the maximum point. Based on
PassTSL, we also implemented a password strength meter (PSM), and our
experiments showed that it was able to estimate password strength more
accurately, causing fewer unsafe errors (overestimating the password strength)
than two other SOTA PSMs when they produce the same rate of safe errors
(underestimating the password strength): a neural-network based method and
zxcvbn. Furthermore, we explored multiple finetuning settings, and our
evaluations showed that, even a small amount of additional training data, e.g.,
only 0.1% of the pretrained data, can lead to over 3% improvement in password
guessing on average. We also proposed a heuristic approach to selecting
finetuning passwords based on JS (Jensen-Shannon) divergence and experimental
results validated its usefulness. In summary, our contributions demonstrate the
potential and feasibility of applying advanced NLP and ML methods to password
modeling and cracking.","[{'name': 'Yangde Wang'}, {'name': 'Haozhang Li'}, {'name': 'Weidong Qiu'}, {'name': 'Shujun Li'}, {'name': 'Peng Tang'}]",2024-07-19T09:23:30Z
http://arxiv.org/abs/2407.14133v1,http://arxiv.org/abs/2407.14133v1,"I Know About ""Up""! Enhancing Spatial Reasoning in Visual Language Models
  Through 3D Reconstruction","Visual Language Models (VLMs) are essential for various tasks, particularly
visual reasoning tasks, due to their robust multi-modal information
integration, visual reasoning capabilities, and contextual awareness. However,
existing \VLMs{}' visual spatial reasoning capabilities are often inadequate,
struggling even with basic tasks such as distinguishing left from right. To
address this, we propose the \ours{} model, designed to enhance the visual
spatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D
reconstruction model for obtaining different views of the input images and
incorporates a prompting mechanism to further improve visual spatial reasoning.
Experimental results on four visual spatial reasoning datasets show that our
\ours{} achieves up to 19.48% accuracy improvement, which indicates the
effectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.","[{'name': 'Zaiqiao Meng'}, {'name': 'Hao Zhou'}, {'name': 'Yifang Chen'}]",2024-07-19T09:03:30Z
http://arxiv.org/abs/2407.14088v1,http://arxiv.org/abs/2407.14088v1,"Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
  Generation: A State-of-the-Art Investigation","Data-to-text (D2T) generation aims to generate human-readable text from
semi-structured data, such as tables and graphs. The recent success of D2T is
largely attributed to advancements in LLMs. Despite the success of LLMs, no
research has been conducted to illustrate the impact of model size on the
performance of fine-tuned LLMs for D2T tasks. D2T model performance is
typically assessed based on three key qualities: \textit{readability}
(indicates fluency and coherence), \textit{informativeness} (measures content
similarity), and \textit{faithfulness} (assesses consistency of factual
information). It is currently uncertain whether increasing the size of LLMs
effectively improves performance in D2T tasks across these three qualities. The
objective of this study is to investigate the performance of fine-tuned LLMs in
D2T tasks in terms of model size. Through extensive comparative analysis, we
aim to elucidate both the advantages and limitations of scaling model sizes
across five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and
WebNLG) and twelve state-of-the-art LLMs with varying sizes from five different
LLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all
the three essential qualities of D2T models, we incorporate six widely
recognized automatic metrics -- \textsc{BLEU}, \textsc{METEOR},
\textsc{BERTScore}, \textsc{MoverScore}, \textsc{Parent}, and
\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance
concerning model size in the presence of source-reference divergence, a
critical aspect of D2T tasks. Our investigation reveals that increasing LLM
size enhances \textit{readability} and \textit{informativeness} in D2T tasks,
but larger (in terms of size) LLMs may sacrifice \textit{faithfulness}.
Moreover, small-sized LLMs show more resilience than larger ones when
source-reference divergence is present.","[{'name': 'Joy Mahapatra'}, {'name': 'Utpal Garain'}]",2024-07-19T07:54:30Z
http://arxiv.org/abs/2407.14085v1,http://arxiv.org/abs/2407.14085v1,"An Improved Method for Class-specific Keyword Extraction: A Case Study
  in the German Business Registry","The task of $\textit{keyword extraction}$ is often an important initial step
in unsupervised information extraction, forming the basis for tasks such as
topic modeling or document classification. While recent methods have proven to
be quite effective in the extraction of keywords, the identification of
$\textit{class-specific}$ keywords, or only those pertaining to a predefined
class, remains challenging. In this work, we propose an improved method for
class-specific keyword extraction, which builds upon the popular
$\textbf{KeyBERT}$ library to identify only keywords related to a class
described by $\textit{seed keywords}$. We test this method using a dataset of
German business registry entries, where the goal is to classify each business
according to an economic sector. Our results reveal that our method greatly
improves upon previous approaches, setting a new standard for
$\textit{class-specific}$ keyword extraction.","[{'name': 'Stephen Meisenbacher'}, {'name': 'Tim Schopf'}, {'name': 'Weixin Yan'}, {'name': 'Patrick Holl'}, {'name': 'Florian Matthes'}]",2024-07-19T07:42:48Z
http://arxiv.org/abs/2407.14076v2,http://arxiv.org/abs/2407.14076v2,"Domain-Specific Pretraining of Language Models: A Comparative Study in
  the Medical Field","There are many cases where LLMs are used for specific tasks in a single
domain. These usually require less general, but more domain-specific knowledge.
Highly capable, general-purpose state-of-the-art language models like GPT-4 or
Claude-3-opus can often be used for such tasks, but they are very large and
cannot be run locally, even if they were not proprietary. This can be a problem
when working with sensitive data. This paper focuses on domain-specific and
mixed-domain pretraining as potentially more efficient methods than general
pretraining for specialized language models. We will take a look at work
related to domain-specific pretraining, specifically in the medical area, and
compare benchmark results of specialized language models to general-purpose
language models.",[{'name': 'Tobias Kerner'}],2024-07-19T07:12:43Z
http://arxiv.org/abs/2407.14057v1,http://arxiv.org/abs/2407.14057v1,LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference,"The inference of transformer-based large language models consists of two
sequential stages: 1) a prefilling stage to compute the KV cache of prompts and
generate the first token, and 2) a decoding stage to generate subsequent
tokens. For long prompts, the KV cache must be computed for all tokens during
the prefilling stage, which can significantly increase the time needed to
generate the first token. Consequently, the prefilling stage may become a
bottleneck in the generation process. An open question remains whether all
prompt tokens are essential for generating the first token. To answer this, we
introduce a novel method, LazyLLM, that selectively computes the KV for tokens
important for the next token prediction in both the prefilling and decoding
stages. Contrary to static pruning approaches that prune the prompt at once,
LazyLLM allows language models to dynamically select different subsets of
tokens from the context in different generation steps, even though they might
be pruned in previous steps. Extensive experiments on standard datasets across
various tasks demonstrate that LazyLLM is a generic method that can be
seamlessly integrated with existing language models to significantly accelerate
the generation without fine-tuning. For instance, in the multi-document
question-answering task, LazyLLM accelerates the prefilling stage of the LLama
2 7B model by 2.34x while maintaining accuracy.","[{'name': 'Qichen Fu'}, {'name': 'Minsik Cho'}, {'name': 'Thomas Merth'}, {'name': 'Sachin Mehta'}, {'name': 'Mohammad Rastegari'}, {'name': 'Mahyar Najibi'}]",2024-07-19T06:34:45Z
http://arxiv.org/abs/2407.14056v1,http://arxiv.org/abs/2407.14056v1,"Rasa: Building Expressive Speech Synthesis Systems for Indian Languages
  in Low-resource Settings","We release Rasa, the first multilingual expressive TTS dataset for any Indian
language, which contains 10 hours of neutral speech and 1-3 hours of expressive
speech for each of the 6 Ekman emotions covering 3 languages: Assamese,
Bengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and
30 minutes of expressive data can yield a Fair system as indicated by MUSHRA
scores. Increasing neutral data to 10 hours, with minimal expressive data,
significantly enhances expressiveness. This offers a practical recipe for
resource-constrained languages, prioritizing easily obtainable neutral data
alongside smaller amounts of expressive data. We show the importance of
syllabically balanced data and pooling emotions to enhance expressiveness. We
also highlight challenges in generating specific emotions, e.g., fear and
surprise.","[{'name': 'Praveen Srinivasa Varadhan'}, {'name': 'Ashwin Sankar'}, {'name': 'Giri Raju'}, {'name': 'Mitesh M. Khapra'}]",2024-07-19T06:33:10Z
http://arxiv.org/abs/2407.14049v1,http://arxiv.org/abs/2407.14049v1,Prompted Aspect Key Point Analysis for Quantitative Review Summarization,"Key Point Analysis (KPA) aims for quantitative summarization that provides
key points (KPs) as succinct textual summaries and quantities measuring their
prevalence. KPA studies for arguments and reviews have been reported in the
literature. A majority of KPA studies for reviews adopt supervised learning to
extract short sentences as KPs before matching KPs to review comments for
quantification of KP prevalence. Recent abstractive approaches still generate
KPs based on sentences, often leading to KPs with overlapping and hallucinated
opinions, and inaccurate quantification. In this paper, we propose Prompted
Aspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA
employs aspect sentiment analysis and prompted in-context learning with Large
Language Models (LLMs) to generate and quantify KPs grounded in aspects for
business entities, which achieves faithful KPs with accurate quantification,
and removes the need for large amounts of annotated data for supervised
training. Experiments on the popular review dataset Yelp and the
aspect-oriented review summarization dataset SPACE show that our framework
achieves state-of-the-art performance. Source code and data are available at:
https://github.com/antangrocket1312/PAKPA","[{'name': 'An Quang Tang'}, {'name': 'Xiuzhen Zhang'}, {'name': 'Minh Ngoc Dinh'}, {'name': 'Erik Cambria'}]",2024-07-19T06:07:32Z
http://arxiv.org/abs/2407.14568v1,http://arxiv.org/abs/2407.14568v1,"SQLfuse: Enhancing Text-to-SQL Performance through Comprehensive LLM
  Synergy","Text-to-SQL conversion is a critical innovation, simplifying the transition
from complex SQL to intuitive natural language queries, especially significant
given SQL's prevalence in the job market across various roles. The rise of
Large Language Models (LLMs) like GPT-3.5 and GPT-4 has greatly advanced this
field, offering improved natural language understanding and the ability to
generate nuanced SQL statements. However, the potential of open-source LLMs in
Text-to-SQL applications remains underexplored, with many frameworks failing to
leverage their full capabilities, particularly in handling complex database
queries and incorporating feedback for iterative refinement. Addressing these
limitations, this paper introduces SQLfuse, a robust system integrating
open-source LLMs with a suite of tools to enhance Text-to-SQL translation's
accuracy and usability. SQLfuse features four modules: schema mining, schema
linking, SQL generation, and a SQL critic module, to not only generate but also
continuously enhance SQL query quality. Demonstrated by its leading performance
on the Spider Leaderboard and deployment by Ant Group, SQLfuse showcases the
practical merits of open-source LLMs in diverse business contexts.","[{'name': 'Tingkai Zhang'}, {'name': 'Chaoyu Chen'}, {'name': 'Cong Liao'}, {'name': 'Jun Wang'}, {'name': 'Xudong Zhao'}, {'name': 'Hang Yu'}, {'name': 'Jianchao Wang'}, {'name': 'Jianguo Li'}, {'name': 'Wenhui Shi'}]",2024-07-19T06:01:57Z
http://arxiv.org/abs/2407.14044v1,http://arxiv.org/abs/2407.14044v1,"ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing
  Functional Correctness?","Although large language models (LLMs) have been largely successful in
generating functionally correct programs, conditioning models to produce
efficient solutions while ensuring correctness remains a challenge. Further,
unreliability in benchmarking code efficiency is a hurdle across varying
hardware specifications for popular interpreted languages such as Python. In
this paper, we present ECCO, a reproducible benchmark for evaluating program
efficiency via two paradigms: natural language (NL) based code generation and
history-based code editing. On ECCO, we adapt and thoroughly investigate the
three most promising existing LLM-based approaches: in-context learning,
iterative refinement with execution or NL feedback, and fine-tuning conditioned
on execution and editing history. While most methods degrade functional
correctness and moderately increase program efficiency, we find that adding
execution information often helps maintain functional correctness, and NL
feedback enhances more on efficiency. We release our benchmark to support
future work on LLM-based generation of efficient code.","[{'name': 'Siddhant Waghjale'}, {'name': 'Vishruth Veerendranath'}, {'name': 'Zora Zhiruo Wang'}, {'name': 'Daniel Fried'}]",2024-07-19T05:47:40Z
http://arxiv.org/abs/2407.14039v1,http://arxiv.org/abs/2407.14039v1,BERTer: The Efficient One,"We explore advanced fine-tuning techniques to boost BERT's performance in
sentiment analysis, paraphrase detection, and semantic textual similarity. Our
approach leverages SMART regularization to combat overfitting, improves
hyperparameter choices, employs a cross-embedding Siamese architecture for
improved sentence embeddings, and introduces innovative early exiting methods.
Our fine-tuning findings currently reveal substantial improvements in model
efficiency and effectiveness when combining multiple fine-tuning architectures,
achieving a state-of-the-art performance score of on the test set, surpassing
current benchmarks and highlighting BERT's adaptability in multifaceted
linguistic tasks.","[{'name': 'Pradyumna Saligram'}, {'name': 'Andrew Lanpouthakoun'}]",2024-07-19T05:33:09Z
http://arxiv.org/abs/2407.14030v1,http://arxiv.org/abs/2407.14030v1,"HeCiX: Integrating Knowledge Graphs and Large Language Models for
  Biomedical Research","Despite advancements in drug development strategies, 90% of clinical trials
fail. This suggests overlooked aspects in target validation and drug
optimization. In order to address this, we introduce HeCiX-KG,
Hetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from
ClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines
data on previously conducted clinical trials from ClinicalTrials.gov, and
domain expertise on diseases and genes from Hetionet. This offers a thorough
resource for clinical researchers. Further, we introduce HeCiX, a system that
uses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability.
HeCiX shows high performance during evaluation against a range of clinically
relevant issues, proving this model to be promising for enhancing the
effectiveness of clinical research. Thus, this approach provides a more
holistic view of clinical trials and existing biological data.","[{'name': 'Prerana Sanjay Kulkarni'}, {'name': 'Muskaan Jain'}, {'name': 'Disha Sheshanarayana'}, {'name': 'Srinivasan Parthiban'}]",2024-07-19T05:04:24Z
http://arxiv.org/abs/2407.14000v1,http://arxiv.org/abs/2407.14000v1,"Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by
  Direct Preference Optimization","Extractive question answering over clinical text is a crucial need to help
deal with the deluge of clinical text generated in hospitals. While encoder
models (e.g., BERT) have been popular for this reading comprehension task,
recently encoder-decoder models (e.g., T5) are on the rise. There is also the
emergence of preference optimization techniques to align decoder-only LLMs with
human preferences. In this paper, we combine encoder-decoder models with the
direct preference optimization (DPO) method to improve over prior state of the
art for the RadQA radiology question answering task by 12-15 F1 points. To the
best of our knowledge, this effort is the first to show that DPO method also
works for reading comprehension via novel heuristics to generate preference
data without human inputs.","[{'name': 'Md Sultan Al Nahian'}, {'name': 'Ramakanth Kavuluru'}]",2024-07-19T03:12:10Z
http://arxiv.org/abs/2407.13999v1,http://arxiv.org/abs/2407.13999v1,"NeLLCom-X: A Comprehensive Neural-Agent Framework to Simulate Language
  Learning and Group Communication","Recent advances in computational linguistics include simulating the emergence
of human-like languages with interacting neural network agents, starting from
sets of random symbols. The recently introduced NeLLCom framework (Lian et al.,
2023) allows agents to first learn an artificial language and then use it to
communicate, with the aim of studying the emergence of specific linguistics
properties. We extend this framework (NeLLCom-X) by introducing more realistic
role-alternating agents and group communication in order to investigate the
interplay between language learnability, communication pressures, and group
size effects. We validate NeLLCom-X by replicating key findings from prior
research simulating the emergence of a word-order/case-marking trade-off. Next,
we investigate how interaction affects linguistic convergence and emergence of
the trade-off. The novel framework facilitates future simulations of diverse
linguistic aspects, emphasizing the importance of interaction and group
dynamics in language evolution.","[{'name': 'Yuchen Lian'}, {'name': 'Tessa Verhoef'}, {'name': 'Arianna Bisazza'}]",2024-07-19T03:03:21Z
http://arxiv.org/abs/2407.13998v1,http://arxiv.org/abs/2407.13998v1,"RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval
  Augmented Question Answering","Question answering based on retrieval augmented generation (RAG-QA) is an
important research topic in NLP and has a wide range of real-world
applications. However, most existing datasets for this task are either
constructed using a single source corpus or consist of short extractive
answers, which fall short of evaluating large language model (LLM) based RAG-QA
systems on cross-domain generalization. To address these limitations, we create
Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form
answers that integrate short extractive answers from multiple documents into a
single, coherent narrative, covering 26K queries and large corpora across seven
different domains. We further propose RAG-QA Arena by directly comparing
model-generated answers against LFRQA's answers using LLMs as evaluators. We
show via extensive experiments that RAG-QA Arena and human judgments on answer
quality are highly correlated. Moreover, only 41.3% of the most competitive
LLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a
challenging evaluation platform for future research.","[{'name': 'Rujun Han'}, {'name': 'Yuhao Zhang'}, {'name': 'Peng Qi'}, {'name': 'Yumo Xu'}, {'name': 'Jenyuan Wang'}, {'name': 'Lan Liu'}, {'name': 'William Yang Wang'}, {'name': 'Bonan Min'}, {'name': 'Vittorio Castelli'}]",2024-07-19T03:02:51Z
http://arxiv.org/abs/2407.13982v1,http://arxiv.org/abs/2407.13982v1,"Reexamining Racial Disparities in Automatic Speech Recognition
  Performance: The Role of Confounding by Provenance","Automatic speech recognition (ASR) models trained on large amounts of audio
data are now widely used to convert speech to written text in a variety of
applications from video captioning to automated assistants used in healthcare
and other domains. As such, it is important that ASR models and their use is
fair and equitable. Prior work examining the performance of commercial ASR
systems on the Corpus of Regional African American Language (CORAAL)
demonstrated significantly worse ASR performance on African American English
(AAE). The current study seeks to understand the factors underlying this
disparity by examining the performance of the current state-of-the-art neural
network based ASR system (Whisper, OpenAI) on the CORAAL dataset. Two key
findings have been identified as a result of the current study. The first
confirms prior findings of significant dialectal variation even across
neighboring communities, and worse ASR performance on AAE that can be improved
to some extent with fine-tuning of ASR models. The second is a novel finding
not discussed in prior work on CORAAL: differences in audio recording practices
within the dataset have a significant impact on ASR accuracy resulting in a
``confounding by provenance'' effect in which both language use and recording
quality differ by study location. These findings highlight the need for further
systematic investigation to disentangle the effects of recording quality and
inherent linguistic diversity when examining the fairness and bias present in
neural ASR models, as any bias in ASR accuracy may have negative downstream
effects on disparities in various domains of life in which ASR technology is
used.","[{'name': 'Changye Li'}, {'name': 'Trevor Cohen'}, {'name': 'Serguei Pakhomov'}]",2024-07-19T02:14:17Z
http://arxiv.org/abs/2407.13945v1,http://arxiv.org/abs/2407.13945v1,"FANTAstic SEquences and Where to Find Them: Faithful and Efficient API
  Call Generation through State-tracked Constrained Decoding and Reranking","API call generation is the cornerstone of large language models' tool-using
ability that provides access to the larger world. However, existing supervised
and in-context learning approaches suffer from high training costs, poor data
efficiency, and generated API calls that can be unfaithful to the API
documentation and the user's request. To address these limitations, we propose
an output-side optimization approach called FANTASE. Two of the unique
contributions of FANTASE are its State-Tracked Constrained Decoding (SCD) and
Reranking components. SCD dynamically incorporates appropriate API constraints
in the form of Token Search Trie for efficient and guaranteed generation
faithfulness with respect to the API documentation. The Reranking component
efficiently brings in the supervised signal by leveraging a lightweight model
as the discriminator to rerank the beam-searched candidate generations of the
large language model. We demonstrate the superior performance of FANTASE in API
call generation accuracy, inference efficiency, and context efficiency with
DSTC8 and API Bank datasets.","[{'name': 'Zhuoer Wang'}, {'name': 'Leonardo F. R. Ribeiro'}, {'name': 'Alexandros Papangelis'}, {'name': 'Rohan Mukherjee'}, {'name': 'Tzu-Yen Wang'}, {'name': 'Xinyan Zhao'}, {'name': 'Arijit Biswas'}, {'name': 'James Caverlee'}, {'name': 'Angeliki Metallinou'}]",2024-07-18T23:44:02Z
http://arxiv.org/abs/2407.13943v1,http://arxiv.org/abs/2407.13943v1,Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction,"This paper introduces Werewolf Arena, a novel framework for evaluating large
language models (LLMs) through the lens of the classic social deduction game,
Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the
game's complex dynamics of deception, deduction, and persuasion. The framework
introduces a dynamic turn-taking system based on bidding, mirroring real-world
discussions where individuals strategically choose when to speak. We
demonstrate the framework's utility through an arena-style tournament featuring
Gemini and GPT models. Our results reveal distinct strengths and weaknesses in
the models' strategic reasoning and communication. These findings highlight
Werewolf Arena's potential as a challenging and scalable LLM benchmark.","[{'name': 'Suma Bailis'}, {'name': 'Jane Friedhoff'}, {'name': 'Feiyang Chen'}]",2024-07-18T23:41:05Z
http://arxiv.org/abs/2407.13928v1,http://arxiv.org/abs/2407.13928v1,"BiasDPO: Mitigating Bias in Language Models through Direct Preference
  Optimization","Large Language Models (LLMs) have become pivotal in advancing natural
language processing, yet their potential to perpetuate biases poses significant
concerns. This paper introduces a new framework employing Direct Preference
Optimization (DPO) to mitigate gender, racial, and religious biases in
LLM-generated English text. By developing a loss function that favors less
biased over biased completions, our approach cultivates a preference for
respectful and non-discriminatory language in LLMs. We also contribute a
manually designed dataset for training LLMs to recognize and correct biases.
This dataset encompasses a diverse range of prompts paired with both biased and
unbiased completions. Implementing this approach on the Microsoft Phi-2 model,
we demonstrate substantial reductions in biased outputs as our model
outperforms the baseline model on almost all bias benchmarks. Our model also
achieves better performance compared to other open-source models on most
benchmarks. By reducing biases in the language generated by the model, our
study marks a significant step towards developing more ethical and socially
responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.",[{'name': 'Ahmed Allam'}],2024-07-18T22:32:20Z
http://arxiv.org/abs/2407.13906v1,http://arxiv.org/abs/2407.13906v1,Crafting Efficient Fine-Tuning Strategies for Large Language Models,"This paper addresses the challenges of efficiently fine-tuning large language
models (LLMs) by exploring data efficiency and hyperparameter optimization. We
investigate the minimum data required for effective fine-tuning and propose a
novel hyperparameter optimization method that leverages early-stage model
performance. Our experiments demonstrate that fine-tuning with as few as 200
samples can improve model accuracy from 70\% to 88\% in a product attribute
extraction task. We identify a saturation point of approximately 6,500 samples,
beyond which additional data yields diminishing returns. Our proposed bayesian
hyperparameter optimization method, which evaluates models at 20\% of total
training time, correlates strongly with final model performance, with 4 out of
5 top early-stage models remaining in the top 5 at completion. This approach
led to a 2\% improvement in accuracy over baseline models when evaluated on an
independent test set. These findings offer actionable insights for
practitioners, potentially reducing computational load and dependency on
extensive datasets while enhancing overall performance of fine-tuned LLMs.","[{'name': 'Michael Oliver'}, {'name': 'Guan Wang'}]",2024-07-18T21:36:00Z
http://arxiv.org/abs/2408.00005v1,http://arxiv.org/abs/2408.00005v1,"Framework for Curating Speech Datasets and Evaluating ASR Systems: A
  Case Study for Polish","Speech datasets available in the public domain are often underutilized
because of challenges in discoverability and interoperability. A comprehensive
framework has been designed to survey, catalog, and curate available speech
datasets, which allows replicable evaluation of automatic speech recognition
(ASR) systems. A case study focused on the Polish language was conducted; the
framework was applied to curate more than 24 datasets and evaluate 25
combinations of ASR systems and models. This research constitutes the most
extensive comparison to date of both commercial and free ASR systems for the
Polish language. It draws insights from 600 system-model-test set evaluations,
marking a significant advancement in both scale and comprehensiveness. The
results of surveys and performance comparisons are available as interactive
dashboards (https://huggingface.co/spaces/amu-cai/pl-asr-leaderboard) along
with curated datasets (https://huggingface.co/datasets/amu-cai/pl-asr-bigos-v2,
https://huggingface.co/datasets/pelcra/pl-asr-pelcra-for-bigos) and the open
challenge call (https://poleval.pl/tasks/task3). Tools used for evaluation are
open-sourced (https://github.com/goodmike31/pl-asr-bigos-tools), facilitating
replication and adaptation for other languages, as well as continuous expansion
with new datasets and systems.",[{'name': 'Michał Junczyk'}],2024-07-18T21:32:12Z
http://arxiv.org/abs/2407.13891v1,http://arxiv.org/abs/2407.13891v1,"Uncovering Political Bias in Emotion Inference Models: Implications for
  sentiment analysis in social science research","This paper investigates the presence of political bias in emotion inference
models used for sentiment analysis (SA) in social science research. Machine
learning models often reflect biases in their training data, impacting the
validity of their outcomes. While previous research has highlighted gender and
race biases, our study focuses on political bias - an underexplored yet
pervasive issue that can skew the interpretation of text data across a wide
array of studies. We conducted a bias audit on a Polish sentiment analysis
model developed in our lab. By analyzing valence predictions for names and
sentences involving Polish politicians, we uncovered systematic differences
influenced by political affiliations. Our findings indicate that annotations by
human raters propagate political biases into the model's predictions. To
mitigate this, we pruned the training dataset of texts mentioning these
politicians and observed a reduction in bias, though not its complete
elimination. Given the significant implications of political bias in SA, our
study emphasizes caution in employing these models for social science research.
We recommend a critical examination of SA results and propose using
lexicon-based systems as a more ideologically neutral alternative. This paper
underscores the necessity for ongoing scrutiny and methodological adjustments
to ensure the reliability and impartiality of the use of machine learning in
academic and applied contexts.","[{'name': 'Hubert Plisiecki'}, {'name': 'Paweł Lenartowicz'}, {'name': 'Maria Flakus'}, {'name': 'Artur Pokropek'}]",2024-07-18T20:31:07Z
http://arxiv.org/abs/2407.13887v1,http://arxiv.org/abs/2407.13887v1,Learning Goal-Conditioned Representations for Language Reward Models,"Techniques that learn improved representations via offline data or
self-supervised objectives have shown impressive results in traditional
reinforcement learning (RL). Nevertheless, it is unclear how improved
representation learning can benefit reinforcement learning from human feedback
(RLHF) on language models (LMs). In this work, we propose training reward
models (RMs) in a contrastive, $\textit{goal-conditioned}$ fashion by
increasing the representation similarity of future states along sampled
preferred trajectories and decreasing the similarity along randomly sampled
dispreferred trajectories. This objective significantly improves RM performance
by up to 0.09 AUROC across challenging benchmarks, such as MATH and GSM8k.
These findings extend to general alignment as well -- on the Helpful-Harmless
dataset, we observe $2.3\%$ increase in accuracy. Beyond improving reward model
performance, we show this way of training RM representations enables improved
$\textit{steerability}$ because it allows us to evaluate the likelihood of an
action achieving a particular goal-state (e.g., whether a solution is correct
or helpful). Leveraging this insight, we find that we can filter up to $55\%$
of generated tokens during majority voting by discarding trajectories likely to
end up in an ""incorrect"" state, which leads to significant cost savings. We
additionally find that these representations can perform fine-grained control
by conditioning on desired future goal-states. For example, we show that
steering a Llama 3 model towards helpful generations with our approach improves
helpfulness by $9.6\%$ over a supervised-fine-tuning trained baseline.
Similarly, steering the model towards complex generations improves complexity
by $21.6\%$ over the baseline. Overall, we find that training RMs in this
contrastive, goal-conditioned fashion significantly improves performance and
enables model steerability.","[{'name': 'Vaskar Nath'}, {'name': 'Dylan Slack'}, {'name': 'Jeff Da'}, {'name': 'Yuntao Ma'}, {'name': 'Hugh Zhang'}, {'name': 'Spencer Whitehead'}, {'name': 'Sean Hendryx'}]",2024-07-18T20:23:11Z
http://arxiv.org/abs/2407.14562v2,http://arxiv.org/abs/2407.14562v2,"Thought-Like-Pro: Enhancing Reasoning of Large Language Models through
  Self-Driven Prolog-based Chain-of-Thought","Large language models (LLMs) have shown exceptional performance as
general-purpose assistants, excelling across a variety of reasoning tasks. This
achievement represents a significant step toward achieving artificial general
intelligence (AGI). Despite these advancements, the effectiveness of LLMs often
hinges on the specific prompting strategies employed, and there remains a lack
of a robust framework to facilitate learning and generalization across diverse
reasoning tasks. To address these challenges, we introduce a novel learning
framework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to
imitate the Chain-of-Thought (CoT) process which is verified and translated
from reasoning trajectories generated by a symbolic Prolog logic engine. This
framework proceeds in a self-driven manner, that enables LLMs to formulate
rules and statements from given instructions and leverage the symbolic Prolog
engine to derive results. Subsequently, LLMs convert Prolog-derived successive
reasoning trajectories into natural language CoT for imitation learning. Our
empirical findings indicate that our proposed approach substantially enhances
the reasoning abilities of LLMs and demonstrates robust generalization across
out-of-distribution reasoning tasks.","[{'name': 'Xiaoyu Tan'}, {'name': 'Yongxin Deng'}, {'name': 'Xihe Qiu'}, {'name': 'Weidi Xu'}, {'name': 'Chao Qu'}, {'name': 'Wei Chu'}, {'name': 'Yinghui Xu'}, {'name': 'Yuan Qi'}]",2024-07-18T18:52:10Z
http://arxiv.org/abs/2407.13833v1,http://arxiv.org/abs/2407.13833v1,"Phi-3 Safety Post-Training: Aligning Language Models with a ""Break-Fix""
  Cycle","Recent innovations in language model training have demonstrated that it is
possible to create highly performant models that are small enough to run on a
smartphone. As these models are deployed in an increasing number of domains, it
is critical to ensure that they are aligned with human preferences and safety
considerations. In this report, we present our methodology for safety aligning
the Phi-3 series of language models. We utilized a ""break-fix"" cycle,
performing multiple rounds of dataset curation, safety post-training,
benchmarking, red teaming, and vulnerability identification to cover a variety
of harm areas in both single and multi-turn scenarios. Our results indicate
that this approach iteratively improved the performance of the Phi-3 models
across a wide range of responsible AI benchmarks.","[{'name': 'Emman Haider'}, {'name': 'Daniel Perez-Becker'}, {'name': 'Thomas Portet'}, {'name': 'Piyush Madan'}, {'name': 'Amit Garg'}, {'name': 'David Majercak'}, {'name': 'Wen Wen'}, {'name': 'Dongwoo Kim'}, {'name': 'Ziyi Yang'}, {'name': 'Jianwen Zhang'}, {'name': 'Hiteshi Sharma'}, {'name': 'Blake Bullwinkel'}, {'name': 'Martin Pouliot'}, {'name': 'Amanda Minnich'}, {'name': 'Shiven Chawla'}, {'name': 'Solianna Herrera'}, {'name': 'Shahed Warreth'}, {'name': 'Maggie Engler'}, {'name': 'Gary Lopez'}, {'name': 'Nina Chikanov'}, {'name': 'Raja Sekhar Rao Dheekonda'}, {'name': 'Bolor-Erdene Jagdagdorj'}, {'name': 'Roman Lutz'}, {'name': 'Richard Lundeen'}, {'name': 'Tori Westerhoff'}, {'name': 'Pete Bryan'}, {'name': 'Christian Seifert'}, {'name': 'Ram Shankar Siva Kumar'}, {'name': 'Andrew Berkley'}, {'name': 'Alex Kessler'}]",2024-07-18T18:06:59Z
http://arxiv.org/abs/2407.13765v2,http://arxiv.org/abs/2407.13765v2,"Latent Causal Probing: A Formal Perspective on Probing with Causal
  Models of Data","As language models (LMs) deliver increasing performance on a range of NLP
tasks, probing classifiers have become an indispensable technique in the effort
to better understand their inner workings. A typical setup involves (1)
defining an auxiliary task consisting of a dataset of text annotated with
labels, then (2) supervising small classifiers to predict the labels from the
representations of a pretrained LM as it processed the dataset. A high probing
accuracy is interpreted as evidence that the LM has learned to perform the
auxiliary task as an unsupervised byproduct of its original pretraining
objective. Despite the widespread usage of probes, however, the robust design
and analysis of probing experiments remains a challenge. We develop a formal
perspective on probing using structural causal models (SCM). Specifically,
given an SCM which explains the distribution of tokens observed during
training, we frame the central hypothesis as whether the LM has learned to
represent the latent variables of the SCM. Empirically, we extend a recent
study of LMs in the context of a synthetic grid-world navigation task, where
having an exact model of the underlying causal structure allows us to draw
strong inferences from the result of probing experiments. Our techniques
provide robust empirical evidence for the ability of LMs to induce the latent
concepts underlying text.","[{'name': 'Charles Jin'}, {'name': 'Martin Rinard'}]",2024-07-18T17:59:27Z
http://arxiv.org/abs/2407.13757v1,http://arxiv.org/abs/2407.13757v1,"Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation
  of Large Language Models","Retrieval-Augmented Generation (RAG) is applied to solve hallucination
problems and real-time constraints of large language models, but it also
induces vulnerabilities against retrieval corruption attacks. Existing research
mainly explores the unreliability of RAG in white-box and closed-domain QA
tasks. In this paper, we aim to reveal the vulnerabilities of
Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks
for opinion manipulation. We explore the impact of such attacks on user
cognition and decision-making, providing new insight to enhance the reliability
and security of RAG models. We manipulate the ranking results of the retrieval
model in RAG with instruction and use these results as data to train a
surrogate model. By employing adversarial retrieval attack methods to the
surrogate model, black-box transfer attacks on RAG are further realized.
Experiments conducted on opinion datasets across multiple topics show that the
proposed attack strategy can significantly alter the opinion polarity of the
content generated by RAG. This demonstrates the model's vulnerability and, more
importantly, reveals the potential negative impact on user cognition and
decision-making, making it easier to mislead users into accepting incorrect or
biased information.","[{'name': 'Zhuo Chen'}, {'name': 'Jiawei Liu'}, {'name': 'Haotan Liu'}, {'name': 'Qikai Cheng'}, {'name': 'Fan Zhang'}, {'name': 'Wei Lu'}, {'name': 'Xiaozhong Liu'}]",2024-07-18T17:55:55Z
http://arxiv.org/abs/2407.13744v1,http://arxiv.org/abs/2407.13744v1,"LLMs as Function Approximators: Terminology, Taxonomy, and Questions for
  Evaluation","Natural Language Processing has moved rather quickly from modelling specific
tasks to taking more general pre-trained models and fine-tuning them for
specific tasks, to a point where we now have what appear to be inherently
generalist models. This paper argues that the resultant loss of clarity on what
these models model leads to metaphors like ""artificial general intelligences""
that are not helpful for evaluating their strengths and weaknesses. The
proposal is to see their generality, and their potential value, in their
ability to approximate specialist function, based on a natural language
specification. This framing brings to the fore questions of the quality of the
approximation, but beyond that, also questions of discoverability, stability,
and protectability of these functions. As the paper will show, this framing
hence brings together in one conceptual framework various aspects of
evaluation, both from a practical and a theoretical perspective, as well as
questions often relegated to a secondary status (such as ""prompt injection"" and
""jailbreaking"").",[{'name': 'David Schlangen'}],2024-07-18T17:49:56Z
http://arxiv.org/abs/2407.13709v1,http://arxiv.org/abs/2407.13709v1,Understanding Reference Policies in Direct Preference Optimization,"Direct Preference Optimization (DPO) has become a widely used training method
for the instruction fine-tuning of large language models (LLMs). In this work,
we explore an under-investigated aspect of DPO - its dependency on the
reference model or policy. Such reference policies, typically instantiated as
the model to be further fine-tuned, are important since they can impose an
upper limit on DPO's effectiveness. Therefore, we address three related
research questions in this work. First, we explore the optimal strength of the
KL-divergence constraint in DPO, which penalizes deviations from the reference
policy, and find that DPO is sensitive to this strength. Next, we examine the
necessity of reference policies for instruction fine-tuning by providing both
theoretical and empirical comparisons between DPO and related learning
objectives, demonstrating DPO's superiority. Additionally, we investigate
whether DPO benefits from stronger reference policies, finding that a stronger
reference policy can lead to improved performance, but only when it is similar
to the model being fine-tuned. Our findings highlight the confounding role of
reference policies in DPO and offer insights for best practices, while also
identifying open research questions for future studies.","[{'name': 'Yixin Liu'}, {'name': 'Pengfei Liu'}, {'name': 'Arman Cohan'}]",2024-07-18T17:08:10Z
http://arxiv.org/abs/2407.21037v1,http://arxiv.org/abs/2407.21037v1,"An Application of Large Language Models to Coding Negotiation
  Transcripts","In recent years, Large Language Models (LLM) have demonstrated impressive
capabilities in the field of natural language processing (NLP). This paper
explores the application of LLMs in negotiation transcript analysis by the
Vanderbilt AI Negotiation Lab. Starting in September 2022, we applied multiple
strategies using LLMs from zero shot learning to fine tuning models to
in-context learning). The final strategy we developed is explained, along with
how to access and use the model. This study provides a sense of both the
opportunities and roadblocks for the implementation of LLMs in real life
applications and offers a model for how LLMs can be applied to coding in other
fields.","[{'name': 'Ray Friedman'}, {'name': 'Jaewoo Cho'}, {'name': 'Jeanne Brett'}, {'name': 'Xuhui Zhan'}, {'name': 'Ningyu Han'}, {'name': 'Sriram Kannan'}, {'name': 'Yingxiang Ma'}, {'name': 'Jesse Spencer-Smith'}, {'name': 'Elisabeth Jäckel'}, {'name': 'Alfred Zerres'}, {'name': 'Madison Hooper'}, {'name': 'Katie Babbit'}, {'name': 'Manish Acharya'}, {'name': 'Wendi Adair'}, {'name': 'Soroush Aslani'}, {'name': 'Tayfun Aykaç'}, {'name': 'Chris Bauman'}, {'name': 'Rebecca Bennett'}, {'name': 'Garrett Brady'}, {'name': 'Peggy Briggs'}, {'name': 'Cheryl Dowie'}, {'name': 'Chase Eck'}, {'name': 'Igmar Geiger'}, {'name': 'Frank Jacob'}, {'name': 'Molly Kern'}, {'name': 'Sujin Lee'}, {'name': 'Leigh Anne Liu'}, {'name': 'Wu Liu'}, {'name': 'Jeffrey Loewenstein'}, {'name': 'Anne Lytle'}, {'name': 'Li Ma'}, {'name': 'Michel Mann'}, {'name': 'Alexandra Mislin'}, {'name': 'Tyree Mitchell'}, {'name': 'Hannah Martensen née Nagler'}, {'name': 'Amit Nandkeolyar'}, {'name': 'Mara Olekalns'}, {'name': 'Elena Paliakova'}, {'name': 'Jennifer Parlamis'}, {'name': 'Jason Pierce'}, {'name': 'Nancy Pierce'}, {'name': 'Robin Pinkley'}, {'name': 'Nathalie Prime'}, {'name': 'Jimena Ramirez-Marin'}, {'name': 'Kevin Rockmann'}, {'name': 'William Ross'}, {'name': 'Zhaleh Semnani-Azad'}, {'name': 'Juliana Schroeder'}, {'name': 'Philip Smith'}, {'name': 'Elena Stimmer'}, {'name': 'Roderick Swaab'}, {'name': 'Leigh Thompson'}, {'name': 'Cathy Tinsley'}, {'name': 'Ece Tuncel'}, {'name': 'Laurie Weingart'}, {'name': 'Robert Wilken'}, {'name': 'JingJing Yao'}, {'name': 'Zhi-Xue Zhang'}]",2024-07-18T17:05:59Z
http://arxiv.org/abs/2407.13702v1,http://arxiv.org/abs/2407.13702v1,"ANHALTEN: Cross-Lingual Transfer for German Token-Level Reference-Free
  Hallucination Detection","Research on token-level reference-free hallucination detection has
predominantly focused on English, primarily due to the scarcity of robust
datasets in other languages. This has hindered systematic investigations into
the effectiveness of cross-lingual transfer for this important NLP application.
To address this gap, we introduce ANHALTEN, a new evaluation dataset that
extends the English hallucination detection dataset to German. To the best of
our knowledge, this is the first work that explores cross-lingual transfer for
token-level reference-free hallucination detection. ANHALTEN contains gold
annotations in German that are parallel (i.e., directly comparable to the
original English instances). We benchmark several prominent cross-lingual
transfer approaches, demonstrating that larger context length leads to better
hallucination detection in German, even without succeeding context.
Importantly, we show that the sample-efficient few-shot transfer is the most
effective approach in most setups. This highlights the practical benefits of
minimal annotation effort in the target language for reference-free
hallucination detection. Aiming to catalyze future research on cross-lingual
token-level reference-free hallucination detection, we make ANHALTEN publicly
available: https://github.com/janekh24/anhalten","[{'name': 'Janek Herrlein'}, {'name': 'Chia-Chien Hung'}, {'name': 'Goran Glavaš'}]",2024-07-18T17:01:38Z
http://arxiv.org/abs/2407.13696v1,http://arxiv.org/abs/2407.13696v1,"Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark
  Evaluation","Recent advancements in Language Models (LMs) have catalyzed the creation of
multiple benchmarks, designed to assess these models' general capabilities. A
crucial task, however, is assessing the validity of the benchmarks themselves.
This is most commonly done via Benchmark Agreement Testing (BAT), where new
benchmarks are validated against established ones using some agreement metric
(e.g., rank correlation). Despite the crucial role of BAT for benchmark
builders and consumers, there are no standardized procedures for such agreement
testing. This deficiency can lead to invalid conclusions, fostering mistrust in
benchmarks and upending the ability to properly choose the appropriate
benchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how
some overlooked methodological choices can significantly influence BAT results,
potentially undermining the validity of conclusions. To address these
inconsistencies, we propose a set of best practices for BAT and demonstrate how
utilizing these methodologies greatly improves BAT robustness and validity. To
foster adoption and facilitate future research,, we introduce BenchBench, a
python package for BAT, and release the BenchBench-leaderboard, a
meta-benchmark designed to evaluate benchmarks using their peers. Our findings
underscore the necessity for standardized BAT, ensuring the robustness and
validity of benchmark evaluations in the evolving landscape of language model
research.
  BenchBench Package: https://github.com/IBM/BenchBench
  Leaderboard: https://huggingface.co/spaces/per/BenchBench","[{'name': 'Yotam Perlitz'}, {'name': 'Ariel Gera'}, {'name': 'Ofir Arviv'}, {'name': 'Asaf Yehudai'}, {'name': 'Elron Bandel'}, {'name': 'Eyal Shnarch'}, {'name': 'Michal Shmueli-Scheuer'}, {'name': 'Leshem Choshen'}]",2024-07-18T17:00:23Z
http://arxiv.org/abs/2407.13692v2,http://arxiv.org/abs/2407.13692v2,Prover-Verifier Games improve legibility of LLM outputs,"One way to increase confidence in the outputs of Large Language Models (LLMs)
is to support them with reasoning that is clear and easy to check -- a property
we call legibility. We study legibility in the context of solving grade-school
math problems and show that optimizing chain-of-thought solutions only for
answer correctness can make them less legible. To mitigate the loss in
legibility, we propose a training algorithm inspired by Prover-Verifier Game
from Anil et al. (2021). Our algorithm iteratively trains small verifiers to
predict solution correctness, ""helpful"" provers to produce correct solutions
that the verifier accepts, and ""sneaky"" provers to produce incorrect solutions
that fool the verifier. We find that the helpful prover's accuracy and the
verifier's robustness to adversarial attacks increase over the course of
training. Furthermore, we show that legibility training transfers to
time-constrained humans tasked with verifying solution correctness. Over course
of LLM training human accuracy increases when checking the helpful prover's
solutions, and decreases when checking the sneaky prover's solutions. Hence,
training for checkability by small verifiers is a plausible technique for
increasing output legibility. Our results suggest legibility training against
small verifiers as a practical avenue for increasing legibility of large LLMs
to humans, and thus could help with alignment of superhuman models.","[{'name': 'Jan Hendrik Kirchner'}, {'name': 'Yining Chen'}, {'name': 'Harri Edwards'}, {'name': 'Jan Leike'}, {'name': 'Nat McAleese'}, {'name': 'Yuri Burda'}]",2024-07-18T16:58:18Z
http://arxiv.org/abs/2407.13657v1,http://arxiv.org/abs/2407.13657v1,FuLG: 150B Romanian Corpus for Language Model Pretraining,"Research in the field of language models is rapidly evolving, with many open
models being released to the public. Openly available pretraining corpora
usually focus on only a handful of languages, with many others either missing
completely or extremely underrepresented. In this report, we introduce FuLG, a
hundred-fifty-billion-token Romanian corpus extracted from CommonCrawl. We
present our methodology for filtering FuLG and compare it via ablation studies
against existing Romanian corpora.","[{'name': 'Vlad-Andrei Bădoiu'}, {'name': 'Mihai-Valentin Dumitru'}, {'name': 'Alexandru M. Gherghescu'}, {'name': 'Alexandru Agache'}, {'name': 'Costin Raiciu'}]",2024-07-18T16:32:48Z
http://arxiv.org/abs/2407.13647v1,http://arxiv.org/abs/2407.13647v1,Weak-to-Strong Reasoning,"When large language models (LLMs) exceed human-level capabilities, it becomes
increasingly challenging to provide full-scale and accurate supervisions for
these models. Weak-to-strong learning, which leverages a less capable model to
unlock the latent abilities of a stronger model, proves valuable in this
context. Yet, the efficacy of this approach for complex reasoning tasks is
still untested. Furthermore, tackling reasoning tasks under the weak-to-strong
setting currently lacks efficient methods to avoid blindly imitating the weak
supervisor including its errors. In this paper, we introduce a progressive
learning framework that enables the strong model to autonomously refine its
training data, without requiring input from either a more advanced model or
human-annotated data. This framework begins with supervised fine-tuning on a
selective small but high-quality dataset, followed by preference optimization
on contrastive samples identified by the strong model itself. Extensive
experiments on the GSM8K and MATH datasets demonstrate that our method
significantly enhances the reasoning capabilities of Llama2-70b using three
separate weak models. This method is further validated in a forward-looking
experimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b
on the highly challenging OlympicArena dataset. This work paves the way for a
more scalable and sophisticated strategy to enhance AI reasoning powers. All
relevant code and resources are available in
\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.","[{'name': 'Yuqing Yang'}, {'name': 'Yan Ma'}, {'name': 'Pengfei Liu'}]",2024-07-18T16:25:17Z
http://arxiv.org/abs/2407.13638v1,http://arxiv.org/abs/2407.13638v1,"A Comparative Study on Automatic Coding of Medical Letters with
  Explainability","This study aims to explore the implementation of Natural Language Processing
(NLP) and machine learning (ML) techniques to automate the coding of medical
letters with visualised explainability and light-weighted local computer
settings. Currently in clinical settings, coding is a manual process that
involves assigning codes to each condition, procedure, and medication in a
patient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There
are preliminary research on automatic coding in this field using
state-of-the-art ML models; however, due to the complexity and size of the
models, the real-world deployment is not achieved. To further facilitate the
possibility of automatic coding practice, we explore some solutions in a local
computer setting; in addition, we explore the function of explainability for
transparency of AI models. We used the publicly available MIMIC-III database
and the HAN/HLAN network models for ICD code prediction purposes. We also
experimented with the mapping between ICD and SNOMED CT knowledge bases. In our
experiments, the models provided useful information for 97.98\% of codes. The
result of this investigation can shed some light on implementing automatic
clinical coding in practice, such as in hospital settings, on the local
computers used by clinicians , project page
\url{https://github.com/Glenj01/Medical-Coding}.","[{'name': 'Jamie Glen'}, {'name': 'Lifeng Han'}, {'name': 'Paul Rayson'}, {'name': 'Goran Nenadic'}]",2024-07-18T16:12:47Z
http://arxiv.org/abs/2407.13623v2,http://arxiv.org/abs/2407.13623v2,Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies,"Research on scaling large language models (LLMs) has primarily focused on
model parameters and training data size, overlooking the role of vocabulary
size. We investigate how vocabulary size impacts LLM scaling laws by training
models ranging from 33M to 3B parameters on up to 500B characters with various
vocabulary configurations. We propose three complementary approaches for
predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative
estimation, and parametric fit of the loss function. Our approaches converge on
the same result that the optimal vocabulary size depends on the available
compute budget and that larger models deserve larger vocabularies. However,
most LLMs use too small vocabulary sizes. For example, we predict that the
optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times
larger than its vocabulary of 32K. We validate our predictions empirically by
training models with 3B parameters across different FLOPs budgets. Adopting our
predicted optimal vocabulary size consistently improves downstream performance
over commonly used vocabulary sizes. By increasing the vocabulary size from the
conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to
32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly
considering model parameters and vocabulary size for efficient scaling.","[{'name': 'Chaofan Tao'}, {'name': 'Qian Liu'}, {'name': 'Longxu Dou'}, {'name': 'Niklas Muennighoff'}, {'name': 'Zhongwei Wan'}, {'name': 'Ping Luo'}, {'name': 'Min Lin'}, {'name': 'Ngai Wong'}]",2024-07-18T15:58:54Z
http://arxiv.org/abs/2407.13608v1,http://arxiv.org/abs/2407.13608v1,"dzNLP at NADI 2024 Shared Task: Multi-Classifier Ensemble with Weighted
  Voting and TF-IDF Features","This paper presents the contribution of our dzNLP team to the NADI 2024
shared task, specifically in Subtask 1 - Multi-label Country-level Dialect
Identification (MLDID) (Closed Track). We explored various configurations to
address the challenge: in Experiment 1, we utilized a union of n-gram analyzers
(word, character, character with word boundaries) with different n-gram values;
in Experiment 2, we combined a weighted union of Term Frequency-Inverse
Document Frequency (TF-IDF) features with various weights; and in Experiment 3,
we implemented a weighted major voting scheme using three classifiers: Linear
Support Vector Classifier (LSVC), Random Forest (RF), and K-Nearest Neighbors
(KNN).
  Our approach, despite its simplicity and reliance on traditional machine
learning techniques, demonstrated competitive performance in terms of F1-score
and precision. Notably, we achieved the highest precision score of 63.22% among
the participating teams. However, our overall F1 score was approximately 21%,
significantly impacted by a low recall rate of 12.87%. This indicates that
while our models were highly precise, they struggled to recall a broad range of
dialect labels, highlighting a critical area for improvement in handling
diverse dialectal variations.","[{'name': 'Mohamed Lichouri'}, {'name': 'Khaled Lounnas'}, {'name': 'Boualem Nadjib Zahaf'}, {'name': 'Mehdi Ayoub Rabiai'}]",2024-07-18T15:47:42Z
http://arxiv.org/abs/2407.13603v1,http://arxiv.org/abs/2407.13603v1,"dzStance at StanceEval2024: Arabic Stance Detection based on Sentence
  Transformers","This study compares Term Frequency-Inverse Document Frequency (TF-IDF)
features with Sentence Transformers for detecting writers' stances--favorable,
opposing, or neutral--towards three significant topics: COVID-19 vaccine,
digital transformation, and women empowerment. Through empirical evaluation, we
demonstrate that Sentence Transformers outperform TF-IDF features across
various experimental setups. Our team, dzStance, participated in a stance
detection competition, achieving the 13th position (74.91%) among 15 teams in
Women Empowerment, 10th (73.43%) in COVID Vaccine, and 12th (66.97%) in Digital
Transformation. Overall, our team's performance ranked 13th (71.77%) among all
participants. Notably, our approach achieved promising F1-scores, highlighting
its effectiveness in identifying writers' stances on diverse topics. These
results underscore the potential of Sentence Transformers to enhance stance
detection models for addressing critical societal issues.","[{'name': 'Mohamed Lichouri'}, {'name': 'Khaled Lounnas'}, {'name': 'Khelil Rafik Ouaras'}, {'name': 'Mohamed Abi'}, {'name': 'Anis Guechtouli'}]",2024-07-18T15:43:27Z
http://arxiv.org/abs/2407.13597v1,http://arxiv.org/abs/2407.13597v1,"PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like
  (PL) Tasks","Text summarization is a well-studied problem that deals with deriving
insights from unstructured text consumed by humans, and it has found extensive
business applications. However, many real-life tasks involve generating a
series of actions to achieve specific goals, such as workflows, recipes,
dialogs, and travel plans. We refer to them as planning-like (PL) tasks noting
that the main commonality they share is control flow information. which may be
partially specified. Their structure presents an opportunity to create more
practical summaries to help users make quick decisions. We investigate this
observation by introducing a novel plan summarization problem, presenting a
dataset, and providing a baseline method for generating PL summaries. Using
quantitative metrics and qualitative user studies to establish baselines, we
evaluate the plan summaries from our method and large language models. We
believe the novel problem and dataset can reinvigorate research in
summarization, which some consider as a solved problem.","[{'name': 'Vishal Pallagani'}, {'name': 'Biplav Srivastava'}, {'name': 'Nitin Gupta'}]",2024-07-18T15:36:02Z
http://arxiv.org/abs/2407.13579v1,http://arxiv.org/abs/2407.13579v1,Towards Zero-Shot Multimodal Machine Translation,"Current multimodal machine translation (MMT) systems rely on fully supervised
data (i.e models are trained on sentences with their translations and
accompanying images). However, this type of data is costly to collect, limiting
the extension of MMT to other language pairs for which such data does not
exist. In this work, we propose a method to bypass the need for fully
supervised data to train MMT systems, using multimodal English data only. Our
method, called ZeroMMT, consists in adapting a strong text-only machine
translation (MT) model by training it on a mixture of two objectives: visually
conditioned masked language modelling and the Kullback-Leibler divergence
between the original and new MMT outputs. We evaluate on standard MMT
benchmarks and the recently released CoMMuTE, a contrastive benchmark aiming to
evaluate how well models use images to disambiguate English sentences. We
obtain disambiguation performance close to state-of-the-art MMT models trained
additionally on fully supervised examples. To prove that our method generalizes
to languages with no fully supervised training data available, we extend the
CoMMuTE evaluation dataset to three new languages: Arabic, Russian and Chinese.
We further show that we can control the trade-off between disambiguation
capabilities and translation fidelity at inference time using classifier-free
guidance and without any additional data. Our code, data and trained models are
publicly accessible.","[{'name': 'Matthieu Futeral'}, {'name': 'Cordelia Schmid'}, {'name': 'Benoît Sagot'}, {'name': 'Rachel Bawden'}]",2024-07-18T15:20:31Z
http://arxiv.org/abs/2407.13578v1,http://arxiv.org/abs/2407.13578v1,Large Language Models as Reliable Knowledge Bases?,"The NLP community has recently shown a growing interest in leveraging Large
Language Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential
knowledge bases (KBs). However, the reliability and extent to which LLMs can
function as KBs remain underexplored. While previous studies suggest LLMs can
encode knowledge within their parameters, the amount of parametric knowledge
alone is not sufficient to evaluate their effectiveness as KBs. This study
defines criteria that a reliable LLM-as-KB should meet, focusing on factuality
and consistency, and covering both seen and unseen knowledge. We develop
several metrics based on these criteria and use them to evaluate 26 popular
LLMs, while providing a comprehensive analysis of the effects of model size,
instruction tuning, and in-context learning (ICL). Our results paint a worrying
picture. Even a high-performant model like GPT-3.5-turbo is not factual or
consistent, and strategies like ICL and fine-tuning are unsuccessful at making
LLMs better KBs.","[{'name': 'Danna Zheng'}, {'name': 'Mirella Lapata'}, {'name': 'Jeff Z. Pan'}]",2024-07-18T15:20:18Z
http://arxiv.org/abs/2407.13571v1,http://arxiv.org/abs/2407.13571v1,New Capability to Look Up an ASL Sign from a Video Example,"Looking up an unknown sign in an ASL dictionary can be difficult. Most ASL
dictionaries are organized based on English glosses, despite the fact that (1)
there is no convention for assigning English-based glosses to ASL signs; and
(2) there is no 1-1 correspondence between ASL signs and English words.
Furthermore, what if the user does not know either the meaning of the target
sign or its possible English translation(s)? Some ASL dictionaries enable
searching through specification of articulatory properties, such as handshapes,
locations, movement properties, etc. However, this is a cumbersome process and
does not always result in successful lookup. Here we describe a new system,
publicly shared on the Web, to enable lookup of a video of an ASL sign (e.g., a
webcam recording or a clip from a continuous signing video). The user submits a
video for analysis and is presented with the five most likely sign matches, in
decreasing order of likelihood, so that the user can confirm the selection and
then be taken to our ASLLRP Sign Bank entry for that sign. Furthermore, this
video lookup is also integrated into our newest version of SignStream(R)
software to facilitate linguistic annotation of ASL video data, enabling the
user to directly look up a sign in the video being annotated, and, upon
confirmation of the match, to directly enter into the annotation the gloss and
features of that sign, greatly increasing the efficiency and consistency of
linguistic annotations of ASL video data.","[{'name': 'Carol Neidle'}, {'name': 'Augustine Opoku'}, {'name': 'Carey Ballard'}, {'name': 'Yang Zhou'}, {'name': 'Xiaoxiao He'}, {'name': 'Gregory Dimitriadis'}, {'name': 'Dimitris Metaxas'}]",2024-07-18T15:14:35Z
http://arxiv.org/abs/2407.13565v1,http://arxiv.org/abs/2407.13565v1,"dzFinNlp at AraFinNLP: Improving Intent Detection in Financial
  Conversational Agents","In this paper, we present our dzFinNlp team's contribution for intent
detection in financial conversational agents, as part of the AraFinNLP shared
task. We experimented with various models and feature configurations, including
traditional machine learning methods like LinearSVC with TF-IDF, as well as
deep learning models like Long Short-Term Memory (LSTM). Additionally, we
explored the use of transformer-based models for this task. Our experiments
show promising results, with our best model achieving a micro F1-score of
93.02% and 67.21% on the ArBanking77 dataset, in the development and test sets,
respectively.","[{'name': 'Mohamed Lichouri'}, {'name': 'Khaled Lounnas'}, {'name': 'Mohamed Zakaria Amziane'}]",2024-07-18T14:37:20Z
http://arxiv.org/abs/2407.13561v1,http://arxiv.org/abs/2407.13561v1,"Research on Tibetan Tourism Viewpoints information generation system
  based on LLM","Tibet, ensconced within China's territorial expanse, is distinguished by its
labyrinthine and heterogeneous topography, a testament to its profound
historical heritage, and the cradle of a unique religious ethos. The very
essence of these attributes, however, has impeded the advancement of Tibet's
tourism service infrastructure, rendering existing smart tourism services
inadequate for the region's visitors. This study delves into the ramifications
of informational disparities at tourist sites on Tibetan tourism and addresses
the challenge of establishing the Large Language Model (LLM) evaluation
criteria. It introduces an innovative approach, the DualGen Bridge AI system,
employing supervised fine-tuning techniques to bolster model functionality and
enhance optimization processes. Furthermore, it pioneers a multi-structured
generative results assessment framework. Empirical validation confirms the
efficacy of this framework. The study also explores the application of the
supervised fine-tuning method within the proprietary DualGen Bridge AI, aimed
at refining the generation of tourist site information. The study's findings
offer valuable insights for optimizing system performance and provide support
and inspiration for the application of LLM technology in Tibet's tourism
services and beyond, potentially revolutionizing the smart tourism industry
with advanced, tailored information generation capabilities.","[{'name': 'Jinhu Qi'}, {'name': 'Shuai Yan'}, {'name': 'Wentao Zhang'}, {'name': 'Yibo Zhang'}, {'name': 'Zirui Liu'}, {'name': 'Ke Wang'}]",2024-07-18T14:31:53Z
http://arxiv.org/abs/2407.13559v1,http://arxiv.org/abs/2407.13559v1,"Qalam : A Multimodal LLM for Arabic Optical Character and Handwriting
  Recognition","Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)
pose unique challenges due to the cursive and context-sensitive nature of the
Arabic script. This study introduces Qalam, a novel foundation model designed
for Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder
architecture. Our model significantly outperforms existing methods, achieving a
Word Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We
train Qalam on a diverse dataset, including over 4.5 million images from Arabic
manuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,
Qalam demonstrates exceptional handling of Arabic diacritics, a critical
feature in Arabic scripts. Furthermore, it shows a remarkable ability to
process high-resolution inputs, addressing a common limitation in current OCR
systems. These advancements underscore Qalam's potential as a leading solution
for Arabic script recognition, offering a significant leap in accuracy and
efficiency.","[{'name': 'Gagan Bhatia'}, {'name': 'El Moatez Billah Nagoudi'}, {'name': 'Fakhraddin Alwajih'}, {'name': 'Muhammad Abdul-Mageed'}]",2024-07-18T14:31:09Z
http://arxiv.org/abs/2407.13511v1,http://arxiv.org/abs/2407.13511v1,"Can Open-Source LLMs Compete with Commercial Models? Exploring the
  Few-Shot Performance of Current GPT Models in Biomedical Tasks","Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT
and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)
benchmarks across different domains. New competing Open-Source alternatives
like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while
often offering higher throughput and being less costly to use. Open-Source LLMs
can also be self-hosted, which makes them interesting for enterprise and
clinical use cases where sensitive data should not be processed by third
parties. We participated in the 12th BioASQ challenge, which is a retrieval
augmented generation (RAG) setting, and explored the performance of current GPT
models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning
(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional
relevant knowledge from Wikipedia added to the context-window of the LLM might
improve their performance. Mixtral 8x7b was competitive in the 10-shot setting,
both with and without fine-tuning, but failed to produce usable results in the
zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to
measurable performance gains. Our results indicate that the performance gap
between commercial and open-source models in RAG setups exists mainly in the
zero-shot setting and can be closed by simply collecting few-shot examples for
domain-specific use cases. The code needed to rerun these experiments is
available through GitHub.","[{'name': 'Samy Ateia'}, {'name': 'Udo Kruschwitz'}]",2024-07-18T13:43:01Z
http://arxiv.org/abs/2407.13509v1,http://arxiv.org/abs/2407.13509v1,"Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous
  Behaviors Based on Language Models","Spontaneous style speech synthesis, which aims to generate human-like speech,
often encounters challenges due to the scarcity of high-quality data and
limitations in model capabilities. Recent language model-based TTS systems can
be trained on large, diverse, and low-quality speech datasets, resulting in
highly natural synthesized speech. However, they are limited by the difficulty
of simulating various spontaneous behaviors and capturing prosody variations in
spontaneous speech. In this paper, we propose a novel spontaneous speech
synthesis system based on language models. We systematically categorize and
uniformly model diverse spontaneous behaviors. Moreover, fine-grained prosody
modeling is introduced to enhance the model's ability to capture subtle prosody
variations in spontaneous speech.Experimental results show that our proposed
method significantly outperforms the baseline methods in terms of prosody
naturalness and spontaneous behavior naturalness.","[{'name': 'Weiqin Li'}, {'name': 'Peiji Yang'}, {'name': 'Yicheng Zhong'}, {'name': 'Yixuan Zhou'}, {'name': 'Zhisheng Wang'}, {'name': 'Zhiyong Wu'}, {'name': 'Xixin Wu'}, {'name': 'Helen Meng'}]",2024-07-18T13:42:38Z
http://arxiv.org/abs/2407.13481v1,http://arxiv.org/abs/2407.13481v1,"Attention Overflow: Language Model Input Blur during Long-Context
  Missing Items Recommendation","Large language models (LLMs) can suggest missing elements from items listed
in a prompt, which can be used for list completion or recommendations based on
users' history. However, their performance degrades when presented with too
many items, as they start to suggest items already included in the input list.
This occurs at around 100 items for mid-2024 flagship LLMs. We evaluate this
phenomenon on both synthetic problems (e.g., finding missing numbers in a given
range of shuffled integers) and realistic movie recommendation scenarios. We
refer to this issue as \textit{attention overflow}, as preventing repetition
requires attending to all items simultaneously. Although iterative loops can
mitigate this problem, their costs increase with the repetition rate, affecting
the language models' ability to derive novelty from lengthy inputs.",[{'name': 'Damien Sileo'}],2024-07-18T13:00:30Z
http://arxiv.org/abs/2407.13469v1,http://arxiv.org/abs/2407.13469v1,"Fixed and Adaptive Simultaneous Machine Translation Strategies Using
  Adapters","Simultaneous machine translation aims at solving the task of real-time
translation by starting to translate before consuming the full input, which
poses challenges in terms of balancing quality and latency of the translation.
The wait-$k$ policy offers a solution by starting to translate after consuming
$k$ words, where the choice of the number $k$ directly affects the latency and
quality. In applications where we seek to keep the choice over latency and
quality at inference, the wait-$k$ policy obliges us to train more than one
model. In this paper, we address the challenge of building one model that can
fulfil multiple latency levels and we achieve this by introducing lightweight
adapter modules into the decoder. The adapters are trained to be specialized
for different wait-$k$ values and compared to other techniques they offer more
flexibility to allow for reaping the benefits of parameter sharing and
minimizing interference. Additionally, we show that by combining with an
adaptive strategy, we can further improve the results. Experiments on two
language directions show that our method outperforms or competes with other
strong baselines on most latency values.","[{'name': 'Abderrahmane Issam'}, {'name': 'Yusuf Can Semerci'}, {'name': 'Jan Scholtes'}, {'name': 'Gerasimos Spanakis'}]",2024-07-18T12:42:45Z
http://arxiv.org/abs/2407.13435v1,http://arxiv.org/abs/2407.13435v1,"Enhancing Out-of-Vocabulary Performance of Indian TTS Systems for
  Practical Applications through Low-Effort Data Strategies","Publicly available TTS datasets for low-resource languages like Hindi and
Tamil typically contain 10-20 hours of data, leading to poor vocabulary
coverage. This limitation becomes evident in downstream applications where
domain-specific vocabulary coupled with frequent code-mixing with English,
results in many OOV words. To highlight this problem, we create a benchmark
containing OOV words from several real-world applications. Indeed,
state-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV
benchmark, as indicated by intelligibility tests. To improve the model's OOV
performance, we propose a low-effort and economically viable strategy to obtain
more training data. Specifically, we propose using volunteers as opposed to
high quality voice artists to record words containing character bigrams unseen
in the training data. We show that using such inexpensive data, the model's
performance improves on OOV words, while not affecting voice quality and
in-domain performance.","[{'name': 'Srija Anand'}, {'name': 'Praveen Srinivasa Varadhan'}, {'name': 'Ashwin Sankar'}, {'name': 'Giri Raju'}, {'name': 'Mitesh M. Khapra'}]",2024-07-18T12:03:14Z
http://arxiv.org/abs/2407.13419v1,http://arxiv.org/abs/2407.13419v1,From Words to Worlds: Compositionality for Cognitive Architectures,"Large language models (LLMs) are very performant connectionist systems, but
do they exhibit more compositionality? More importantly, is that part of why
they perform so well? We present empirical analyses across four LLM families
(12 models) and three task categories, including a novel task introduced below.
Our findings reveal a nuanced relationship in learning of compositional
strategies by LLMs -- while scaling enhances compositional abilities,
instruction tuning often has a reverse effect. Such disparity brings forth some
open issues regarding the development and improvement of large language models
in alignment with human cognitive capacities.","[{'name': 'Ruchira Dhar'}, {'name': 'Anders Søgaard'}]",2024-07-18T11:42:13Z
http://arxiv.org/abs/2407.13399v2,http://arxiv.org/abs/2407.13399v2,"Correcting the Mythos of KL-Regularization: Direct Alignment without
  Overoptimization via Chi-Squared Preference Optimization","Language model alignment methods, such as reinforcement learning from human
feedback (RLHF), have led to impressive advances in language model
capabilities, but existing techniques are limited by a widely observed
phenomenon known as overoptimization, where the quality of the language model
plateaus or degrades over the course of the alignment process. Overoptimization
is often attributed to overfitting to an inaccurate reward model, and while it
can be mitigated through online data collection, this is infeasible in many
settings. This raises a fundamental question: Do existing offline alignment
algorithms make the most of the data they have, or can their sample-efficiency
be improved further?
  We address this question with a new algorithm for offline alignment,
$\chi^2$-Preference Optimization ($\chi$PO). $\chi$PO is a one-line change to
Direct Preference Optimization (DPO; Rafailov et al., 2023), which only
involves modifying the logarithmic link function in the DPO objective. Despite
this minimal change, $\chi$PO implicitly implements the principle of pessimism
in the face of uncertainty via regularization with the $\chi^2$-divergence --
which quantifies uncertainty more effectively than KL-regularization -- and
provably alleviates overoptimization, achieving sample-complexity guarantees
based on single-policy concentrability -- the gold standard in offline
reinforcement learning. $\chi$PO's simplicity and strong guarantees make it the
first practical and general-purpose offline alignment algorithm that is
provably robust to overoptimization.","[{'name': 'Audrey Huang'}, {'name': 'Wenhao Zhan'}, {'name': 'Tengyang Xie'}, {'name': 'Jason D. Lee'}, {'name': 'Wen Sun'}, {'name': 'Akshay Krishnamurthy'}, {'name': 'Dylan J. Foster'}]",2024-07-18T11:08:40Z
http://arxiv.org/abs/2407.13377v1,http://arxiv.org/abs/2407.13377v1,Linear-Complexity Self-Supervised Learning for Speech Processing,"Self-supervised learning (SSL) models usually require weeks of pre-training
with dozens of high-end GPUs. These models typically have a multi-headed
self-attention (MHSA) context encoder. However, MHSA takes quadratic time and
space in the input length, contributing to the high pre-training cost.
Linear-complexity alternatives to MHSA have been proposed. For instance, in
supervised training, the SummaryMixing model is the first to outperform MHSA
across multiple speech processing tasks. However, these cheaper alternatives
have not been explored for SSL yet. This paper studies a linear-complexity
context encoder for SSL for the first time. With better or equivalent
performance for the downstream tasks of the MP3S benchmark, SummaryMixing
reduces the pre-training time and peak VRAM of wav2vec 2.0 model by 18% and by
23%, respectively, leading to the pre-training of a 155M wav2vec 2.0 model
finished within one week with 4 Tesla A100 GPUs. Code is available at
https://github.com/SamsungLabs/SummaryMixing.","[{'name': 'Shucong Zhang'}, {'name': 'Titouan Parcollet'}, {'name': 'Rogier van Dalen'}, {'name': 'Sourav Bhattacharya'}]",2024-07-18T10:34:33Z
http://arxiv.org/abs/2408.00004v1,http://arxiv.org/abs/2408.00004v1,Handling Numeric Expressions in Automatic Speech Recognition,"This paper addresses the problem of correctly formatting numeric expressions
in automatic speech recognition (ASR) transcripts. This is challenging since
the expected transcript format depends on the context, e.g., 1945 (year) vs.
19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize
and format numeric expression, such as years, timestamps, currency amounts, and
quantities. For the end-to-end approach we employed a data generation strategy
using a large language model (LLM) together with a text to speech (TTS) model
to generate adaptation data. The results on our test dataset show that while
approaches based on LLMs perform well on recognizing formatted numeric
expressions, adapted end-to-end models offer competitive performance with the
advantage of lower latency and inference cost.","[{'name': 'Christian Huber'}, {'name': 'Alexander Waibel'}]",2024-07-18T09:46:19Z
http://arxiv.org/abs/2407.13343v1,http://arxiv.org/abs/2407.13343v1,Learning-From-Mistakes Prompting for Indigenous Language Translation,"Using large language models, this paper presents techniques to improve
extremely low-resourced indigenous language translations. Our approaches are
grounded in the use of (1) the presence of a datastore consisting of a limited
number of parallel translation examples, (2) the inherent capabilities of LLMs
like GPT-3.5, and (3) a word-level translation dictionary. We harness the
potential of LLMs and in-context learning techniques in such a setting for
using LLMs as universal translators for extremely low-resourced languages. Our
methodology hinges on utilizing LLMs as language compilers for selected
language pairs, hypothesizing that they could internalize syntactic structures
to facilitate accurate translation. We introduce three techniques: KNNPrompting
with Retrieved Prompting Context, Chain-of-Thought Prompting and
Learningfrom-Mistakes Prompting, with the last method addressing past errors.
The evaluation results suggest that, even with limited corpora, LLMs can
effectively translate extremely low-resource languages when paired with proper
prompting.","[{'name': 'You-Cheng Liao'}, {'name': 'Chen-Jui Yu'}, {'name': 'Chi-Yi Lin'}, {'name': 'He-Feng Yun'}, {'name': 'Yen-Hsiang Wang'}, {'name': 'Hsiao-Min Li'}, {'name': 'Yao-Chung Fan'}]",2024-07-18T09:41:20Z
http://arxiv.org/abs/2407.13329v1,http://arxiv.org/abs/2407.13329v1,"Why do you cite? An investigation on citation intents and
  decision-making classification processes","Identifying the reason for which an author cites another work is essential to
understand the nature of scientific contributions and to assess their impact.
Citations are one of the pillars of scholarly communication and most metrics
employed to analyze these conceptual links are based on quantitative
observations. Behind the act of referencing another scholarly work there is a
whole world of meanings that needs to be proficiently and effectively revealed.
This study emphasizes the importance of trustfully classifying citation intents
to provide more comprehensive and insightful analyses in research assessment.
We address this task by presenting a study utilizing advanced Ensemble
Strategies for Citation Intent Classification (CIC) incorporating Language
Models (LMs) and employing Explainable AI (XAI) techniques to enhance the
interpretability and trustworthiness of models' predictions. Our approach
involves two ensemble classifiers that utilize fine-tuned SciBERT and XLNet LMs
as baselines. We further demonstrate the critical role of section titles as a
feature in improving models' performances. The study also introduces a web
application developed with Flask and currently available at
http://137.204.64.4:81/cic/classifier, aimed at classifying citation intents.
One of our models sets as a new state-of-the-art (SOTA) with an 89.46% Macro-F1
score on the SciCite benchmark. The integration of XAI techniques provides
insights into the decision-making processes, highlighting the contributions of
individual words for level-0 classifications, and of individual models for the
metaclassification. The findings suggest that the inclusion of section titles
significantly enhances classification performances in the CIC task. Our
contributions provide useful insights for developing more robust datasets and
methodologies, thus fostering a deeper understanding of scholarly
communication.","[{'name': 'Lorenzo Paolini'}, {'name': 'Sahar Vahdati'}, {'name': 'Angelo Di Iorio'}, {'name': 'Robert Wardenga'}, {'name': 'Ivan Heibi'}, {'name': 'Silvio Peroni'}]",2024-07-18T09:29:33Z
http://arxiv.org/abs/2407.13301v1,http://arxiv.org/abs/2407.13301v1,"CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis","The field of medical diagnosis has undergone a significant transformation
with the advent of large language models (LLMs), yet the challenges of
interpretability within these models remain largely unaddressed. This study
introduces Chain-of-Diagnosis (CoD) to enhance the interpretability of
LLM-based medical diagnostics. CoD transforms the diagnostic process into a
diagnostic chain that mirrors a physician's thought process, providing a
transparent reasoning pathway. Additionally, CoD outputs the disease confidence
distribution to ensure transparency in decision-making. This interpretability
makes model diagnostics controllable and aids in identifying critical symptoms
for inquiry through the entropy reduction of confidences. With CoD, we
developed DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental
results demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic
benchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring
controllability in diagnostic rigor.","[{'name': 'Junying Chen'}, {'name': 'Chi Gui'}, {'name': 'Anningzhe Gao'}, {'name': 'Ke Ji'}, {'name': 'Xidong Wang'}, {'name': 'Xiang Wan'}, {'name': 'Benyou Wang'}]",2024-07-18T09:06:27Z
http://arxiv.org/abs/2407.13292v1,http://arxiv.org/abs/2407.13292v1,"Low-Resourced Speech Recognition for Iu Mien Language via
  Weakly-Supervised Phoneme-based Multilingual Pre-training","The mainstream automatic speech recognition (ASR) technology usually requires
hundreds to thousands of hours of annotated speech data. Three approaches to
low-resourced ASR are phoneme or subword based supervised pre-training, and
self-supervised pre-training over multilingual data. The Iu Mien language is
the main ethnic language of the Yao ethnic group in China and is low-resourced
in the sense that the annotated speech is very limited. With less than 10 hours
of transcribed Iu Mien language, this paper investigates and compares the three
approaches for Iu Mien speech recognition. Our experiments are based on the
recently released, three backbone models pretrained over the 10 languages from
the CommonVoice dataset (CV-Lang10), which correspond to the three approaches
for low-resourced ASR. It is found that phoneme supervision can achieve better
results compared to subword supervision and self-supervision, thereby providing
higher data-efficiency. Particularly, the Whistle models, i.e., obtained by the
weakly-supervised phoneme-based multilingual pre-training, obtain the most
competitive results.","[{'name': 'Lukuan Dong'}, {'name': 'Donghong Qin'}, {'name': 'Fengbo Bai'}, {'name': 'Fanhua Song'}, {'name': 'Yan Liu'}, {'name': 'Chen Xu'}, {'name': 'Zhijian Ou'}]",2024-07-18T08:46:47Z
http://arxiv.org/abs/2407.13248v1,http://arxiv.org/abs/2407.13248v1,Are Large Language Models Capable of Generating Human-Level Narratives?,"This paper investigates the capability of LLMs in storytelling, focusing on
narrative development and plot progression. We introduce a novel computational
framework to analyze narratives through three discourse-level aspects: i) story
arcs, ii) turning points, and iii) affective dimensions, including arousal and
valence. By leveraging expert and automatic annotations, we uncover significant
discrepancies between the LLM- and human- written stories. While human-written
stories are suspenseful, arousing, and diverse in narrative structures, LLM
stories are homogeneously positive and lack tension. Next, we measure narrative
reasoning skills as a precursor to generative capacities, concluding that most
LLMs fall short of human abilities in discourse understanding. Finally, we show
that explicit integration of aforementioned discourse features can enhance
storytelling, as is demonstrated by over 40% improvement in neural storytelling
in terms of diversity, suspense, and arousal.","[{'name': 'Yufei Tian'}, {'name': 'Tenghao Huang'}, {'name': 'Miri Liu'}, {'name': 'Derek Jiang'}, {'name': 'Alexander Spangher'}, {'name': 'Muhao Chen'}, {'name': 'Jonathan May'}, {'name': 'Nanyun Peng'}]",2024-07-18T08:02:49Z
http://arxiv.org/abs/2407.13244v1,http://arxiv.org/abs/2407.13244v1,"PM-LLM-Benchmark: Evaluating Large Language Models on Process Mining
  Tasks","Large Language Models (LLMs) have the potential to semi-automate some process
mining (PM) analyses. While commercial models are already adequate for many
analytics tasks, the competitive level of open-source LLMs in PM tasks is
unknown. In this paper, we propose PM-LLM-Benchmark, the first comprehensive
benchmark for PM focusing on domain knowledge (process-mining-specific and
process-specific) and on different implementation strategies. We focus also on
the challenges in creating such a benchmark, related to the public availability
of the data and on evaluation biases by the LLMs. Overall, we observe that most
of the considered LLMs can perform some process mining tasks at a satisfactory
level, but tiny models that would run on edge devices are still inadequate. We
also conclude that while the proposed benchmark is useful for identifying LLMs
that are adequate for process mining tasks, further research is needed to
overcome the evaluation biases and perform a more thorough ranking of the
competitive LLMs.","[{'name': 'Alessandro Berti'}, {'name': 'Humam Kourani'}, {'name': 'Wil M. P. van der Aalst'}]",2024-07-18T07:57:31Z
http://arxiv.org/abs/2407.13228v1,http://arxiv.org/abs/2407.13228v1,"Evaluating Large Language Models for Anxiety and Depression
  Classification using Counseling and Psychotherapy Transcripts","We aim to evaluate the efficacy of traditional machine learning and large
language models (LLMs) in classifying anxiety and depression from long
conversational transcripts. We fine-tune both established transformer models
(BERT, RoBERTa, Longformer) and more recent large models (Mistral-7B), trained
a Support Vector Machine with feature engineering, and assessed GPT models
through prompting. We observe that state-of-the-art models fail to enhance
classification outcomes compared to traditional machine learning methods.","[{'name': 'Junwei Sun'}, {'name': 'Siqi Ma'}, {'name': 'Yiran Fan'}, {'name': 'Peter Washington'}]",2024-07-18T07:26:09Z
http://arxiv.org/abs/2407.13205v1,http://arxiv.org/abs/2407.13205v1,Transformer-based Single-Cell Language Model: A Survey,"The transformers have achieved significant accomplishments in the natural
language processing as its outstanding parallel processing capabilities and
highly flexible attention mechanism. In addition, increasing studies based on
transformers have been proposed to model single-cell data. In this review, we
attempt to systematically summarize the single-cell language models and
applications based on transformers. First, we provide a detailed introduction
about the structure and principles of transformers. Then, we review the
single-cell language models and large language models for single-cell data
analysis. Moreover, we explore the datasets and applications of single-cell
language models in downstream tasks such as batch correction, cell clustering,
cell type annotation, gene regulatory network inference and perturbation
response. Further, we discuss the challenges of single-cell language models and
provide promising research directions. We hope this review will serve as an
up-to-date reference for researchers interested in the direction of single-cell
language models.","[{'name': 'Wei Lan'}, {'name': 'Guohang He'}, {'name': 'Mingyang Liu'}, {'name': 'Qingfeng Chen'}, {'name': 'Junyue Cao'}, {'name': 'Wei Peng'}]",2024-07-18T06:43:12Z
http://arxiv.org/abs/2407.13193v2,http://arxiv.org/abs/2407.13193v2,Retrieval-Augmented Generation for Natural Language Processing: A Survey,"Large language models (LLMs) have demonstrated great success in various
fields, benefiting from their huge amount of parameters that store knowledge.
However, LLMs still suffer from several key issues, such as hallucination
problems, knowledge update issues, and lacking domain-specific expertise. The
appearance of retrieval-augmented generation (RAG), which leverages an external
knowledge database to augment LLMs, makes up those drawbacks of LLMs. This
paper reviews all significant techniques of RAG, especially in the retriever
and the retrieval fusions. Besides, tutorial codes are provided for
implementing the representative techniques in RAG. This paper further discusses
the RAG training, including RAG with/without datastore update. Then, we
introduce the application of RAG in representative natural language processing
tasks and industrial scenarios. Finally, this paper discusses the future
directions and challenges of RAG for promoting its development.","[{'name': 'Shangyu Wu'}, {'name': 'Ying Xiong'}, {'name': 'Yufei Cui'}, {'name': 'Haolun Wu'}, {'name': 'Can Chen'}, {'name': 'Ye Yuan'}, {'name': 'Lianming Huang'}, {'name': 'Xue Liu'}, {'name': 'Tei-Wei Kuo'}, {'name': 'Nan Guan'}, {'name': 'Chun Jason Xue'}]",2024-07-18T06:06:53Z
http://arxiv.org/abs/2407.13168v1,http://arxiv.org/abs/2407.13168v1,SciCode: A Research Coding Benchmark Curated by Scientists,"Since language models (LMs) now outperform average humans on many challenging
tasks, it has become increasingly difficult to develop challenging,
high-quality, and realistic evaluations. We address this issue by examining
LMs' capabilities to generate code for solving real scientific research
problems. Incorporating input from scientists and AI researchers in 16 diverse
natural science sub-fields, including mathematics, physics, chemistry, biology,
and materials science, we created a scientist-curated coding benchmark,
SciCode. The problems in SciCode naturally factorize into multiple subproblems,
each involving knowledge recall, reasoning, and code synthesis. In total,
SciCode contains 338 subproblems decomposed from 80 challenging main problems.
It offers optional descriptions specifying useful scientific background
information and scientist-annotated gold-standard solutions and test cases for
evaluation. Claude3.5-Sonnet, the best-performing model among those tested, can
solve only 4.6% of the problems in the most realistic setting. We believe that
SciCode demonstrates both contemporary LMs' progress towards becoming helpful
scientific assistants and sheds light on the development and evaluation of
scientific AI in the future.","[{'name': 'Minyang Tian'}, {'name': 'Luyu Gao'}, {'name': 'Shizhuo Dylan Zhang'}, {'name': 'Xinan Chen'}, {'name': 'Cunwei Fan'}, {'name': 'Xuefei Guo'}, {'name': 'Roland Haas'}, {'name': 'Pan Ji'}, {'name': 'Kittithat Krongchon'}, {'name': 'Yao Li'}, {'name': 'Shengyan Liu'}, {'name': 'Di Luo'}, {'name': 'Yutao Ma'}, {'name': 'Hao Tong'}, {'name': 'Kha Trinh'}, {'name': 'Chenyu Tian'}, {'name': 'Zihan Wang'}, {'name': 'Bohao Wu'}, {'name': 'Yanyu Xiong'}, {'name': 'Shengzhu Yin'}, {'name': 'Minhui Zhu'}, {'name': 'Kilian Lieret'}, {'name': 'Yanxin Lu'}, {'name': 'Genglin Liu'}, {'name': 'Yufeng Du'}, {'name': 'Tianhua Tao'}, {'name': 'Ofir Press'}, {'name': 'Jamie Callan'}, {'name': 'Eliu Huerta'}, {'name': 'Hao Peng'}]",2024-07-18T05:15:24Z
http://arxiv.org/abs/2407.13164v1,http://arxiv.org/abs/2407.13164v1,"Translate-and-Revise: Boosting Large Language Models for Constrained
  Translation","Imposing constraints on machine translation systems presents a challenging
issue because these systems are not trained to make use of constraints in
generating adequate, fluent translations. In this paper, we leverage the
capabilities of large language models (LLMs) for constrained translation, given
that LLMs can easily adapt to this task by taking translation instructions and
constraints as prompts. However, LLMs cannot always guarantee the adequacy of
translation, and, in some cases, ignore the given constraints. This is in part
because LLMs might be overly confident in their predictions, overriding the
influence of the constraints. To overcome this overiding behaviour, we propose
to add a revision process that encourages LLMs to correct the outputs by
prompting them about the constraints that have not yet been met. We evaluate
our approach on four constrained translation tasks, encompassing both lexical
and structural constraints in multiple constraint domains. Experiments show
15\% improvement in constraint-based translation accuracy over standard LLMs
and the approach also significantly outperforms neural machine translation
(NMT) state-of-the-art methods.","[{'name': 'Pengcheng Huang'}, {'name': 'Yongyu Mu'}, {'name': 'Yuzhang Wu'}, {'name': 'Bei Li'}, {'name': 'Chunyang Xiao'}, {'name': 'Tong Xiao'}, {'name': 'Jingbo Zhu'}]",2024-07-18T05:08:09Z
http://arxiv.org/abs/2407.13153v1,http://arxiv.org/abs/2407.13153v1,"Preset-Voice Matching for Privacy Regulated Speech-to-Speech Translation
  Systems","In recent years, there has been increased demand for speech-to-speech
translation (S2ST) systems in industry settings. Although successfully
commercialized, cloning-based S2ST systems expose their distributors to
liabilities when misused by individuals and can infringe on personality rights
when exploited by media organizations. This work proposes a regulated S2ST
framework called Preset-Voice Matching (PVM). PVM removes cross-lingual voice
cloning in S2ST by first matching the input voice to a similar prior consenting
speaker voice in the target-language. With this separation, PVM avoids cloning
the input speaker, ensuring PVM systems comply with regulations and reduce risk
of misuse. Our results demonstrate PVM can significantly improve S2ST system
run-time in multi-speaker settings and the naturalness of S2ST synthesized
speech. To our knowledge, PVM is the first explicitly regulated S2ST framework
leveraging similarly-matched preset-voices for dynamic S2ST tasks.","[{'name': 'Daniel Platnick'}, {'name': 'Bishoy Abdelnour'}, {'name': 'Eamon Earl'}, {'name': 'Rahul Kumar'}, {'name': 'Zahra Rezaei'}, {'name': 'Thomas Tsangaris'}, {'name': 'Faraj Lagum'}]",2024-07-18T04:42:01Z
http://arxiv.org/abs/2407.13142v1,http://arxiv.org/abs/2407.13142v1,"A light-weight and efficient punctuation and word casing prediction
  model for on-device streaming ASR","Punctuation and word casing prediction are necessary for automatic speech
recognition (ASR). With the popularity of on-device end-to-end streaming ASR
systems, the on-device punctuation and word casing prediction become a
necessity while we found little discussion on this. With the emergence of
Transformer, Transformer based models have been explored for this scenario.
However, Transformer based models are too large for on-device ASR systems. In
this paper, we propose a light-weight and efficient model that jointly predicts
punctuation and word casing in real time. The model is based on Convolutional
Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM).
Experimental results on the IWSLT2011 test set show that the proposed model
obtains 9% relative improvement compared to the best of non-Transformer models
on overall F1-score. Compared to the representative of Transformer based
models, the proposed model achieves comparable results to the representative
model while being only one-fortieth its size and 2.5 times faster in terms of
inference time. It is suitable for on-device streaming ASR systems. Our code is
publicly available.","[{'name': 'Jian You'}, {'name': 'Xiangfeng Li'}]",2024-07-18T04:01:12Z
http://arxiv.org/abs/2407.13115v1,http://arxiv.org/abs/2407.13115v1,"TrialEnroll: Predicting Clinical Trial Enrollment Success with Deep &
  Cross Network and Large Language Models","Clinical trials need to recruit a sufficient number of volunteer patients to
demonstrate the statistical power of the treatment (e.g., a new drug) in curing
a certain disease. Clinical trial recruitment has a significant impact on trial
success. Forecasting whether the recruitment process would be successful before
we run the trial would save many resources and time. This paper develops a
novel deep & cross network with large language model (LLM)-augmented text
feature that learns semantic information from trial eligibility criteria and
predicts enrollment success. The proposed method enables interpretability by
understanding which sentence/word in eligibility criteria contributes heavily
to prediction. We also demonstrate the empirical superiority of the proposed
method (0.7002 PR-AUC) over a bunch of well-established machine learning
methods. The code and curated dataset are publicly available at
https://anonymous.4open.science/r/TrialEnroll-7E12.","[{'name': 'Ling Yue'}, {'name': 'Sixue Xing'}, {'name': 'Jintai Chen'}, {'name': 'Tianfan Fu'}]",2024-07-18T02:50:40Z
http://arxiv.org/abs/2407.13101v1,http://arxiv.org/abs/2407.13101v1,"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with
  an Iterative Approach","Multi-hop question answering is a challenging task with distinct industrial
relevance, and Retrieval-Augmented Generation (RAG) methods based on large
language models (LLMs) have become a popular approach to tackle this task.
Owing to the potential inability to retrieve all necessary information in a
single iteration, a series of iterative RAG methods has been recently
developed, showing significant performance improvements. However, existing
methods still face two critical challenges: context overload resulting from
multiple rounds of retrieval, and over-planning and repetitive planning due to
the lack of a recorded retrieval trajectory. In this paper, we propose a novel
iterative RAG method called ReSP, equipped with a dual-function summarizer.
This summarizer compresses information from retrieved documents, targeting both
the overarching question and the current sub-question concurrently.
Experimental results on the multi-hop question-answering datasets HotpotQA and
2WikiMultihopQA demonstrate that our method significantly outperforms the
state-of-the-art, and exhibits excellent robustness concerning context length.","[{'name': 'Zhouyu Jiang'}, {'name': 'Mengshu Sun'}, {'name': 'Lei Liang'}, {'name': 'Zhiqiang Zhang'}]",2024-07-18T02:19:00Z
http://arxiv.org/abs/2407.13097v1,http://arxiv.org/abs/2407.13097v1,AlcLaM: Arabic Dialectal Language Model,"Pre-trained Language Models (PLMs) are integral to many modern natural
language processing (NLP) systems. Although multilingual models cover a wide
range of languages, they often grapple with challenges like high inference
costs and a lack of diverse non-English training data. Arabic-specific PLMs are
trained predominantly on modern standard Arabic, which compromises their
performance on regional dialects. To tackle this, we construct an Arabic
dialectal corpus comprising 3.4M sentences gathered from social media
platforms. We utilize this corpus to expand the vocabulary and retrain a
BERT-based model from scratch. Named AlcLaM, our model was trained using only
13 GB of text, which represents a fraction of the data used by existing models
such as CAMeL, MARBERT, and ArBERT, compared to 7.8%, 10.2%, and 21.3%,
respectively. Remarkably, AlcLaM demonstrates superior performance on a variety
of Arabic NLP tasks despite the limited training data. AlcLaM is available at
GitHub https://github.com/amurtadha/Alclam and HuggingFace
https://huggingface.co/rahbi.","[{'name': 'Murtadha Ahmed'}, {'name': 'Saghir Alfasly'}, {'name': 'Bo Wen'}, {'name': 'Jamaal Qasem'}, {'name': 'Mohammed Ahmed'}, {'name': 'Yunfeng Liu'}]",2024-07-18T02:13:50Z
http://arxiv.org/abs/2407.13089v1,http://arxiv.org/abs/2407.13089v1,"MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for
  Fact-Checking","Fact-checking real-world claims often requires reviewing multiple multimodal
documents to assess a claim's truthfulness, which is a highly laborious and
time-consuming task. In this paper, we present a summarization model designed
to generate claim-specific summaries useful for fact-checking from multimodal,
multi-document datasets. The model takes inputs in the form of documents,
images, and a claim, with the objective of assisting in fact-checking tasks. We
introduce a dynamic perceiver-based model that can handle inputs from multiple
modalities of arbitrary lengths. To train our model, we leverage a novel
reinforcement learning-based entailment objective to generate summaries that
provide evidence distinguishing between different truthfulness labels. To
assess the efficacy of our approach, we conduct experiments on both an existing
benchmark and a new dataset of multi-document claims that we contribute. Our
approach outperforms the SOTA approach by 4.6% in the claim verification task
on the MOCHEG dataset and demonstrates strong performance on our new
Multi-News-Fact-Checking dataset.","[{'name': 'Ting-Chih Chen'}, {'name': 'Chia-Wei Tang'}, {'name': 'Chris Thomas'}]",2024-07-18T01:33:20Z
http://arxiv.org/abs/2407.13069v1,http://arxiv.org/abs/2407.13069v1,"Dynamic Sentiment Analysis with Local Large Language Models using
  Majority Voting: A Study on Factors Affecting Restaurant Evaluation","User-generated contents (UGCs) on online platforms allow marketing
researchers to understand consumer preferences for products and services. With
the advance of large language models (LLMs), some studies utilized the models
for annotation and sentiment analysis. However, the relationship between the
accuracy and the hyper-parameters of LLMs is yet to be thoroughly examined. In
addition, the issues of variability and reproducibility of results from each
trial of LLMs have rarely been considered in existing literature. Since actual
human annotation uses majority voting to resolve disagreements among
annotators, this study introduces a majority voting mechanism to a sentiment
analysis model using local LLMs. By a series of three analyses of online
reviews on restaurant evaluations, we demonstrate that majority voting with
multiple attempts using a medium-sized model produces more robust results than
using a large model with a single attempt. Furthermore, we conducted further
analysis to investigate the effect of each aspect on the overall evaluation.",[{'name': 'Junichiro Niimi'}],2024-07-18T00:28:04Z
http://arxiv.org/abs/2407.13048v1,http://arxiv.org/abs/2407.13048v1,Establishing Knowledge Preference in Language Models,"Language models are known to encode a great amount of factual knowledge
through pretraining. However, such knowledge might be insufficient to cater to
user requests, requiring the model to integrate external knowledge sources and
adhere to user-provided specifications. When answering questions about ongoing
events, the model should use recent news articles to update its response; when
asked to provide recommendations, the model should prioritize user
specifications over retrieved product reviews; when some facts are edited in
the model, the updated facts should override all prior knowledge learned by the
model even if they are conflicting. In all of the cases above, the model faces
a decision between its own parametric knowledge, (retrieved) contextual
knowledge, and user instruction knowledge. In this paper, we (1) unify such
settings into the problem of knowledge preference and define a three-level
preference hierarchy over these knowledge sources; (2) compile a collection of
existing datasets IfQA, MQuAKE, and MRQA covering a combination of settings
(with/without user specifications, with/without context documents) to
systematically evaluate how well models obey the intended knowledge preference;
and (3) propose a dataset synthesis method that composes diverse
question-answer pairs with user assumptions and related context to directly
fine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a
7B model, fine-tuned on only a few thousand examples automatically generated by
our proposed method, effectively achieves superior performance (more than 18%
improvement across all evaluation benchmarks) in adhering to the desired
knowledge preference hierarchy.","[{'name': 'Sizhe Zhou'}, {'name': 'Sha Li'}, {'name': 'Yu Meng'}, {'name': 'Yizhu Jiao'}, {'name': 'Heng Ji'}, {'name': 'Jiawei Han'}]",2024-07-17T23:16:11Z
http://arxiv.org/abs/2407.20244v1,http://arxiv.org/abs/2407.20244v1,"Steamroller Problems: An Evaluation of LLM Reasoning Capability with
  Automated Theorem Prover Strategies","This study presents the first examination of the ability of Large Language
Models (LLMs) to follow reasoning strategies that are used to guide Automated
Theorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and
Google's recent Gemini model on problems from a steamroller domain. In addition
to determining accuracy we make use of the Natural Language Processing library
spaCy to explore new methods of investigating LLM's reasoning capabilities.
This led to one alarming result, the low correlation between correct reasoning
and correct answers for any of the tested models. We found that the models'
performance when using the ATP reasoning strategies was comparable to one-shot
chain of thought and observe that attention to uncertainty in the accuracy
results is critical when drawing conclusions about model performance.
Consistent with previous speculation we confirm that LLMs have a preference
for, and are best able to follow, bottom up reasoning processes. However, the
reasoning strategies can still be beneficial for deriving small and relevant
sets of formulas for external processing by a trusted inference engine.","[{'name': 'Lachlan McGinness'}, {'name': 'Peter Baumgartner'}]",2024-07-17T22:49:23Z
http://arxiv.org/abs/2407.13040v1,http://arxiv.org/abs/2407.13040v1,Turkish Delights: a Dataset on Turkish Euphemisms,"Euphemisms are a form of figurative language relatively understudied in
natural language processing. This research extends the current computational
work on potentially euphemistic terms (PETs) to Turkish. We introduce the
Turkish PET dataset, the first available of its kind in the field. By creating
a list of euphemisms in Turkish, collecting example contexts, and annotating
them, we provide both euphemistic and non-euphemistic examples of PETs in
Turkish. We describe the dataset and methodologies, and also experiment with
transformer-based models on Turkish euphemism detection by using our dataset
for binary classification. We compare performances across models using F1,
accuracy, and precision as evaluation metrics.","[{'name': 'Hasan Can Biyik'}, {'name': 'Patrick Lee'}, {'name': 'Anna Feldman'}]",2024-07-17T22:13:42Z
http://arxiv.org/abs/2407.12982v1,http://arxiv.org/abs/2407.12982v1,Retrieval-Enhanced Machine Learning: Synthesis and Opportunities,"In the field of language modeling, models augmented with retrieval components
have emerged as a promising solution to address several challenges faced in the
natural language processing (NLP) field, including knowledge grounding,
interpretability, and scalability. Despite the primary focus on NLP, we posit
that the paradigm of retrieval-enhancement can be extended to a broader
spectrum of machine learning (ML) such as computer vision, time series
prediction, and computational biology. Therefore, this work introduces a formal
framework of this paradigm, Retrieval-Enhanced Machine Learning (REML), by
synthesizing the literature in various domains in ML with consistent notations
which is missing from the current literature. Also, we found that while a
number of studies employ retrieval components to augment their models, there is
a lack of integration with foundational Information Retrieval (IR) research. We
bridge this gap between the seminal IR research and contemporary REML studies
by investigating each component that comprises the REML framework. Ultimately,
the goal of this work is to equip researchers across various disciplines with a
comprehensive, formally structured framework of retrieval-enhanced models,
thereby fostering interdisciplinary future research.","[{'name': 'To Eun Kim'}, {'name': 'Alireza Salemi'}, {'name': 'Andrew Drozdov'}, {'name': 'Fernando Diaz'}, {'name': 'Hamed Zamani'}]",2024-07-17T20:01:21Z
http://arxiv.org/abs/2407.13803v1,http://arxiv.org/abs/2407.13803v1,Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality,"With the widespread adoption of Large Language Models (LLMs), concerns about
potential misuse have emerged. To this end, watermarking has been adapted to
LLM, enabling a simple and effective way to detect and monitor generated text.
However, while the existing methods can differentiate between watermarked and
unwatermarked text with high accuracy, they often face a trade-off between the
quality of the generated text and the effectiveness of the watermarking
process. In this work, we present a novel type of LLM watermark, Sparse
Watermark, which aims to mitigate this trade-off by applying watermarks to a
small subset of generated tokens distributed across the text. The key strategy
involves anchoring watermarked tokens to words that have specific
Part-of-Speech (POS) tags. Our experimental results demonstrate that the
proposed watermarking scheme achieves high detectability while generating text
that outperforms previous LLM watermarking methods in quality across various
tasks","[{'name': 'Duy C. Hoang'}, {'name': 'Hung T. Q. Le'}, {'name': 'Rui Chu'}, {'name': 'Ping Li'}, {'name': 'Weijie Zhao'}, {'name': 'Yingjie Lao'}, {'name': 'Khoa D. Doan'}]",2024-07-17T18:52:12Z
http://arxiv.org/abs/2407.12943v1,http://arxiv.org/abs/2407.12943v1,Halu-J: Critique-Based Hallucination Judge,"Large language models (LLMs) frequently generate non-factual content, known
as hallucinations. Existing retrieval-augmented-based hallucination detection
approaches typically address this by framing it as a classification task,
evaluating hallucinations based on their consistency with retrieved evidence.
However, this approach usually lacks detailed explanations for these
evaluations and does not assess the reliability of these explanations.
Furthermore, deficiencies in retrieval systems can lead to irrelevant or
partially relevant evidence retrieval, impairing the detection process.
Moreover, while real-world hallucination detection requires analyzing multiple
pieces of evidence, current systems usually treat all evidence uniformly
without considering its relevance to the content. To address these challenges,
we introduce Halu-J, a critique-based hallucination judge with 7 billion
parameters. Halu-J enhances hallucination detection by selecting pertinent
evidence and providing detailed critiques. Our experiments indicate that Halu-J
outperforms GPT-4o in multiple-evidence hallucination detection and matches its
capability in critique generation and evidence selection. We also introduce
ME-FEVER, a new dataset designed for multiple-evidence hallucination detection.
Our code and dataset can be found in https://github.com/GAIR-NLP/factool .","[{'name': 'Binjie Wang'}, {'name': 'Steffi Chern'}, {'name': 'Ethan Chern'}, {'name': 'Pengfei Liu'}]",2024-07-17T18:21:01Z
http://arxiv.org/abs/2407.20243v1,http://arxiv.org/abs/2407.20243v1,"Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller
  Embedding Dimensions","Embeddings from Large Language Models (LLMs) have emerged as critical
components in various applications, particularly for information retrieval.
While high-dimensional embeddings generally demonstrate superior performance as
they contain more salient information, their practical application is
frequently hindered by elevated computational latency and the associated higher
cost. To address these challenges, we propose Matryoshka-Adaptor, a novel
tuning framework designed for the customization of LLM embeddings.
Matryoshka-Adaptor facilitates substantial dimensionality reduction while
maintaining comparable performance levels, thereby achieving a significant
enhancement in computational efficiency and cost-effectiveness. Our framework
directly modifies the embeddings from pre-trained LLMs which is designed to be
seamlessly integrated with any LLM architecture, encompassing those accessible
exclusively through black-box APIs. Also, it exhibits efficacy in both
unsupervised and supervised learning settings. A rigorous evaluation conducted
across a diverse corpus of English, multilingual, and multimodal datasets
consistently reveals substantial gains with Matryoshka-Adaptor. Notably, with
Google and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in
dimensionality ranging from two- to twelve-fold without compromising
performance across multiple BEIR datasets.","[{'name': 'Jinsung Yoon'}, {'name': 'Raj Sinha'}, {'name': 'Sercan O Arik'}, {'name': 'Tomas Pfister'}]",2024-07-17T18:03:29Z
http://arxiv.org/abs/2407.12772v1,http://arxiv.org/abs/2407.12772v1,LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models,"The advances of large foundation models necessitate wide-coverage, low-cost,
and zero-contamination benchmarks. Despite continuous exploration of language
model evaluations, comprehensive studies on the evaluation of Large Multi-modal
Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified
and standardized multimodal benchmark framework with over 50 tasks and more
than 10 models to promote transparent and reproducible evaluations. Although
LMMS-EVAL offers comprehensive coverage, we find it still falls short in
achieving low cost and zero contamination. To approach this evaluation
trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that
emphasizes both coverage and efficiency. Additionally, we present Multimodal
LIVEBENCH that utilizes continuously updating news and online forums to assess
models' generalization abilities in the wild, featuring a low-cost and
zero-contamination evaluation approach. In summary, our work highlights the
importance of considering the evaluation trilemma and provides practical
solutions to navigate the trade-offs in evaluating large multi-modal models,
paving the way for more effective and reliable benchmarking of LMMs. We
opensource our codebase and maintain leaderboard of LIVEBENCH at
https://github.com/EvolvingLMMs-Lab/lmms-eval and
https://huggingface.co/spaces/lmms-lab/LiveBench.","[{'name': 'Kaichen Zhang'}, {'name': 'Bo Li'}, {'name': 'Peiyuan Zhang'}, {'name': 'Fanyi Pu'}, {'name': 'Joshua Adrian Cahyono'}, {'name': 'Kairui Hu'}, {'name': 'Shuai Liu'}, {'name': 'Yuanhan Zhang'}, {'name': 'Jingkang Yang'}, {'name': 'Chunyuan Li'}, {'name': 'Ziwei Liu'}]",2024-07-17T17:51:53Z
http://arxiv.org/abs/2407.12771v1,http://arxiv.org/abs/2407.12771v1,The Role of Network and Identity in the Diffusion of Hashtags,"Although the spread of behaviors is influenced by many social factors,
existing literature tends to study the effects of single factors -- most often,
properties of the social network -- on the final cascade. In order to move
towards a more integrated view of cascades, this paper offers the first
comprehensive investigation into the role of two social factors in the
diffusion of 1,337 popular hashtags representing the production of novel
culture on Twitter: 1) the topology of the Twitter social network and 2)
performance of each user's probable demographic identity. Here, we show that
cascades are best modeled using a combination of network and identity, rather
than either factor alone. This combined model best reproduces a composite index
of ten cascade properties across all 1,337 hashtags. However, there is
important heterogeneity in what social factors are required to reproduce
different properties of hashtag cascades. For instance, while a combined
network+identity model best predicts the popularity of cascades, a network-only
model has better performance in predicting cascade growth and an identity-only
model in adopter composition. We are able to predict what type of hashtag is
best modeled by each combination of features and use this to further improve
performance. Additionally, consistent with prior literature on the combined
network+identity model most outperforms the single-factor counterfactuals among
hashtags used for expressing racial or regional identity, stance-taking,
talking about sports, or variants of existing cultural trends with very slow-
or fast-growing communicative need. In sum, our results imply the utility of
multi-factor models in predicting cascades, in order to account for the varied
ways in which network, identity, and other social factors play a role in the
diffusion of hashtags on Twitter.","[{'name': 'Aparna Ananthasubramaniam'}, {'name': 'Yufei Zhu'}, {'name': 'David Jurgens'}, {'name': 'Daniel Romero'}]",2024-07-17T17:51:49Z
http://arxiv.org/abs/2407.12749v1,http://arxiv.org/abs/2407.12749v1,HDLCopilot: Hardware Design Library Querying with Natural Language,"Hardware design engineers routinely work with multiple Process Design Kits
(PDKs) from various fabrication labs, each containing several standard cell
libraries, optimized for specific metric such as speed, power, or density.
These libraries include multiple views such as liberty files for timing
information, LEF files for abstract layout details, and technology LEF for
process design rules. Navigating this complex landscape to retrieve specific
information about gates or design rules is often time-consuming and
error-prone. To address this, we present HDLCopilot, an LLM-powered PDK query
system that allows engineers to streamline interactions with PDKs in natural
language format, making information retrieval accurate and more efficient.
HDLCopilot achieves an accuracy of 94.23\% on an evaluation set comprised of
diverse and complex natural language queries. HDLCopilot positions itself as a
powerful assistant in the hardware design process, enhancing productivity and
reducing potential human errors.","[{'name': 'Manar Abdelatty'}, {'name': 'Sherief Reda'}]",2024-07-17T17:11:13Z
http://arxiv.org/abs/2407.12734v1,http://arxiv.org/abs/2407.12734v1,A LLM Benchmark based on the Minecraft Builder Dialog Agent Task,"In this work we proposing adapting the Minecraft builder task into an LLM
benchmark suitable for evaluating LLM ability in spatially orientated tasks,
and informing builder agent design. Previous works have proposed corpora with
varying complex structures, and human written instructions. We instead attempt
to provide a comprehensive synthetic benchmark for testing builder agents over
a series of distinct tasks that comprise of common building operations. We
believe this approach allows us to probe specific strengths and weaknesses of
different agents, and test the ability of LLMs in the challenging area of
spatial reasoning and vector based math.","[{'name': 'Chris Madge'}, {'name': 'Massimo Poesio'}]",2024-07-17T16:52:23Z
http://arxiv.org/abs/2407.12725v1,http://arxiv.org/abs/2407.12725v1,"Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language
  Models?","Elaborating a series of intermediate reasoning steps significantly improves
the ability of large language models (LLMs) to solve complex problems, as such
steps would evoke LLMs to think sequentially. However, human sarcasm
understanding is often considered an intuitive and holistic cognitive process,
in which various linguistic, contextual, and emotional cues are integrated to
form a comprehensive understanding of the speaker's true intention, which is
argued not be limited to a step-by-step reasoning process. To verify this
argument, we introduce a new prompting framework called SarcasmCue, which
contains four prompting strategies, $viz.$ chain of contradiction (CoC), graph
of cues (GoC), bagging of cues (BoC) and tensor of cues (ToC), which elicits
LLMs to detect human sarcasm by considering sequential and non-sequential
prompting methods. Through a comprehensive empirical comparison on four
benchmarking datasets, we show that the proposed four prompting methods
outperforms standard IO prompting, CoT and ToT with a considerable margin, and
non-sequential prompting generally outperforms sequential prompting.","[{'name': 'Ben Yao'}, {'name': 'Yazhou Zhang'}, {'name': 'Qiuchi Li'}, {'name': 'Jing Qin'}]",2024-07-17T16:42:03Z
http://arxiv.org/abs/2407.12707v2,http://arxiv.org/abs/2407.12707v2,TTSDS -- Text-to-Speech Distribution Score,"Many recently published Text-to-Speech (TTS) systems produce audio close to
real speech. However, TTS evaluation needs to be revisited to make sense of the
results obtained with the new architectures, approaches and datasets. We
propose evaluating the quality of synthetic speech as a combination of multiple
factors such as prosody, speaker identity, and intelligibility. Our approach
assesses how well synthetic speech mirrors real speech by obtaining correlates
of each factor and measuring their distance from both real speech datasets and
noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and
show that our score computed as an unweighted average of factors strongly
correlates with the human evaluations from each time period.","[{'name': 'Christoph Minixhofer'}, {'name': 'Ondřej Klejch'}, {'name': 'Peter Bell'}]",2024-07-17T16:30:27Z
http://arxiv.org/abs/2407.12626v1,http://arxiv.org/abs/2407.12626v1,"Domain-specific or Uncertainty-aware models: Does it really make a
  difference for biomedical text classification?","The success of pretrained language models (PLMs) across a spate of use-cases
has led to significant investment from the NLP community towards building
domain-specific foundational models. On the other hand, in mission critical
settings such as biomedical applications, other aspects also factor in-chief of
which is a model's ability to produce reasonable estimates of its own
uncertainty. In the present study, we discuss these two desiderata through the
lens of how they shape the entropy of a model's output probability
distribution. We find that domain specificity and uncertainty awareness can
often be successfully combined, but the exact task at hand weighs in much more
strongly.","[{'name': 'Aman Sinha'}, {'name': 'Timothee Mickus'}, {'name': 'Marianne Clausel'}, {'name': 'Mathieu Constant'}, {'name': 'Xavier Coubez'}]",2024-07-17T14:52:46Z
http://arxiv.org/abs/2407.12620v2,http://arxiv.org/abs/2407.12620v2,"Harnessing the Power of Artificial Intelligence to Vitalize Endangered
  Indigenous Languages: Technologies and Experiences","Since 2022 we have been exploring application areas and technologies in which
Artificial Intelligence (AI) and modern Natural Language Processing (NLP), such
as Large Language Models (LLMs), can be employed to foster the usage and
facilitate the documentation of Indigenous languages which are in danger of
disappearing. We start by discussing the decreasing diversity of languages in
the world and how working with Indigenous languages poses unique ethical
challenges for AI and NLP. To address those challenges, we propose an
alternative development AI cycle based on community engagement and usage. Then,
we report encouraging results in the development of high-quality machine
learning translators for Indigenous languages by fine-tuning state-of-the-art
(SOTA) translators with tiny amounts of data and discuss how to avoid some
common pitfalls in the process. We also present prototypes we have built in
projects done in 2023 and 2024 with Indigenous communities in Brazil, aimed at
facilitating writing, and discuss the development of Indigenous Language Models
(ILMs) as a replicable and scalable way to create spell-checkers, next-word
predictors, and similar tools. Finally, we discuss how we envision a future for
language documentation where dying languages are preserved as interactive
language models.","[{'name': 'Claudio Pinhanez'}, {'name': 'Paulo Cavalin'}, {'name': 'Luciana Storto'}, {'name': 'Thomas Finbow'}, {'name': 'Alexander Cobbinah'}, {'name': 'Julio Nogima'}, {'name': 'Marisa Vasconcelos'}, {'name': 'Pedro Domingues'}, {'name': 'Priscila de Souza Mizukami'}, {'name': 'Nicole Grell'}, {'name': 'Majoí Gongora'}, {'name': 'Isabel Gonçalves'}]",2024-07-17T14:46:37Z
http://arxiv.org/abs/2407.12543v1,http://arxiv.org/abs/2407.12543v1,"Abstraction Alignment: Comparing Model and Human Conceptual
  Relationships","Abstraction -- the process of generalizing specific examples into broad
reusable patterns -- is central to how people efficiently process and store
information and apply their knowledge to new data. Promisingly, research has
shown that ML models learn representations that span levels of abstraction,
from specific concepts like ""bolo tie"" and ""car tire"" to more general concepts
like ""CEO"" and ""model"". However, existing techniques analyze these
representations in isolation, treating learned concepts as independent
artifacts rather than an interconnected web of abstraction. As a result,
although we can identify the concepts a model uses to produce its output, it is
difficult to assess if it has learned a human-aligned abstraction of the
concepts that will generalize to new data. To address this gap, we introduce
abstraction alignment, a methodology to measure the agreement between a model's
learned abstraction and the expected human abstraction. We quantify abstraction
alignment by comparing model outputs against a human abstraction graph, such as
linguistic relationships or medical disease hierarchies. In evaluation tasks
interpreting image models, benchmarking language models, and analyzing medical
datasets, abstraction alignment provides a deeper understanding of model
behavior and dataset content, differentiating errors based on their agreement
with human knowledge, expanding the verbosity of current model quality metrics,
and revealing ways to improve existing human abstractions.","[{'name': 'Angie Boggust'}, {'name': 'Hyemin Bang'}, {'name': 'Hendrik Strobelt'}, {'name': 'Arvind Satyanarayan'}]",2024-07-17T13:27:26Z
http://arxiv.org/abs/2407.12532v1,http://arxiv.org/abs/2407.12532v1,"Towards Collaborative Intelligence: Propagating Intentions and Reasoning
  for Multi-Agent Coordination with Large Language Models","Effective collaboration in multi-agent systems requires communicating goals
and intentions between agents. Current agent frameworks often suffer from
dependencies on single-agent execution and lack robust inter-module
communication, frequently leading to suboptimal multi-agent reinforcement
learning (MARL) policies and inadequate task coordination. To address these
challenges, we present a framework for training large language models (LLMs) as
collaborative agents to enable coordinated behaviors in cooperative MARL. Each
agent maintains a private intention consisting of its current goal and
associated sub-tasks. Agents broadcast their intentions periodically, allowing
other agents to infer coordination tasks. A propagation network transforms
broadcast intentions into teammate-specific communication messages, sharing
relevant goals with designated teammates. The architecture of our framework is
structured into planning, grounding, and execution modules. During execution,
multiple agents interact in a downstream environment and communicate intentions
to enable coordinated behaviors. The grounding module dynamically adapts
comprehension strategies based on emerging coordination patterns, while
feedback from execution agents influnces the planning module, enabling the
dynamic re-planning of sub-tasks. Results in collaborative environment
simulation demonstrate intention propagation reduces miscoordination errors by
aligning sub-task dependencies between agents. Agents learn when to communicate
intentions and which teammates require task details, resulting in emergent
coordinated behaviors. This demonstrates the efficacy of intention sharing for
cooperative multi-agent RL based on LLMs.","[{'name': 'Xihe Qiu'}, {'name': 'Haoyu Wang'}, {'name': 'Xiaoyu Tan'}, {'name': 'Chao Qu'}, {'name': 'Yujie Xiong'}, {'name': 'Yuan Cheng'}, {'name': 'Yinghui Xu'}, {'name': 'Wei Chu'}, {'name': 'Yuan Qi'}]",2024-07-17T13:14:00Z
http://arxiv.org/abs/2407.12514v1,http://arxiv.org/abs/2407.12514v1,On Initializing Transformers with Pre-trained Embeddings,"It has become common practice now to use random initialization schemes,
rather than the pre-trained embeddings, when training transformer based models
from scratch. Indeed, we find that pre-trained word embeddings from GloVe, and
some sub-word embeddings extracted from language models such as T5 and mT5 fare
much worse compared to random initialization. This is counter-intuitive given
the well-known representational and transfer-learning advantages of
pre-training. Interestingly, we also find that BERT and mBERT embeddings fare
better than random initialization, showing the advantages of pre-trained
representations. In this work, we posit two potential factors that contribute
to these mixed results: the model sensitivity to parameter distribution and the
embedding interactions with position encodings. We observe that pre-trained
GloVe, T5, and mT5 embeddings have a wider distribution of values. As argued in
the initialization studies, such large value initializations can lead to poor
training because of saturated outputs. Further, the larger embedding values
can, in effect, absorb the smaller position encoding values when added
together, thus losing position information. Standardizing the pre-trained
embeddings to a narrow range (e.g. as prescribed by Xavier) leads to
substantial gains for Glove, T5, and mT5 embeddings. On the other hand, BERT
pre-trained embeddings, while larger, are still relatively closer to Xavier
initialization range which may allow it to effectively transfer the pre-trained
knowledge.","[{'name': 'Ha Young Kim'}, {'name': 'Niranjan Balasubramanian'}, {'name': 'Byungkon Kang'}]",2024-07-17T11:57:10Z
http://arxiv.org/abs/2407.12512v1,http://arxiv.org/abs/2407.12512v1,"$\textit{GeoHard}$: Towards Measuring Class-wise Hardness through
  Modelling Class Semantics","Recent advances in measuring hardness-wise properties of data guide language
models in sample selection within low-resource scenarios. However,
class-specific properties are overlooked for task setup and learning. How will
these properties influence model learning and is it generalizable across
datasets? To answer this question, this work formally initiates the concept of
$\textit{class-wise hardness}$. Experiments across eight natural language
understanding (NLU) datasets demonstrate a consistent hardness distribution
across learning paradigms, models, and human judgment. Subsequent experiments
unveil a notable challenge in measuring such class-wise hardness with
instance-level metrics in previous works. To address this, we propose
$\textit{GeoHard}$ for class-wise hardness measurement by modeling class
geometry in the semantic embedding space. $\textit{GeoHard}$ surpasses
instance-level metrics by over 59 percent on $\textit{Pearson}$'s correlation
on measuring class-wise hardness. Our analysis theoretically and empirically
underscores the generality of $\textit{GeoHard}$ as a fresh perspective on data
diagnosis. Additionally, we showcase how understanding class-wise hardness can
practically aid in improving task learning.","[{'name': 'Fengyu Cai'}, {'name': 'Xinran Zhao'}, {'name': 'Hongming Zhang'}, {'name': 'Iryna Gurevych'}, {'name': 'Heinz Koeppl'}]",2024-07-17T11:53:39Z
http://arxiv.org/abs/2407.12508v1,http://arxiv.org/abs/2407.12508v1,"MERLIN: Multimodal Embedding Refinement via LLM-based Iterative
  Navigation for Text-Video Retrieval-Rerank Pipeline","The rapid expansion of multimedia content has made accurately retrieving
relevant videos from large collections increasingly challenging. Recent
advancements in text-video retrieval have focused on cross-modal interactions,
large-scale foundation model training, and probabilistic modeling, yet often
neglect the crucial user perspective, leading to discrepancies between user
queries and the content retrieved. To address this, we introduce MERLIN
(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,
training-free pipeline that leverages Large Language Models (LLMs) for
iterative feedback learning. MERLIN refines query embeddings from a user
perspective, enhancing alignment between queries and video content through a
dynamic question answering process. Experimental results on datasets like
MSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves
Recall@1, outperforming existing systems and confirming the benefits of
integrating LLMs into multimodal retrieval systems for more responsive and
context-aware multimedia retrieval.","[{'name': 'Donghoon Han'}, {'name': 'Eunhwan Park'}, {'name': 'Gisang Lee'}, {'name': 'Adam Lee'}, {'name': 'Nojun Kwak'}]",2024-07-17T11:45:02Z
http://arxiv.org/abs/2407.12504v1,http://arxiv.org/abs/2407.12504v1,Case2Code: Learning Inductive Reasoning with Synthetic Data,"Complex reasoning is an impressive ability shown by large language models
(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought
prompting or iterative tool-using to solve challenging tasks step-by-step. In
this paper, we hope to focus on evaluating and teaching LLMs to conduct
inductive reasoning, that is, LLMs are supposed to infer underlying rules by
observing examples or sequential transformations. However, collecting
large-scale and diverse human-generated inductive data is challenging. We focus
on data synthesis in the code domain and propose a \textbf{Case2Code} task by
exploiting the expressiveness and correctness of programs. Specifically, we
collect a diverse set of executable programs, synthesize input-output
transformations for each program, and force LLMs to infer the underlying code
implementations based on the synthetic I/O cases. We first evaluate
representative LLMs on the synthesized Case2Code task and demonstrate that the
Case-to-code induction is challenging for LLMs. Then, we synthesize large-scale
Case2Code training samples to train LLMs to perform inductive reasoning.
Experimental results show that such induction training benefits not only in
distribution Case2Code performance but also enhances various coding abilities
of trained LLMs, demonstrating the great potential of learning inductive
reasoning via synthetic data.","[{'name': 'Yunfan Shao'}, {'name': 'Linyang Li'}, {'name': 'Yichuan Ma'}, {'name': 'Peiji Li'}, {'name': 'Demin Song'}, {'name': 'Qinyuan Cheng'}, {'name': 'Shimin Li'}, {'name': 'Xiaonan Li'}, {'name': 'Pengyu Wang'}, {'name': 'Qipeng Guo'}, {'name': 'Hang Yan'}, {'name': 'Xipeng Qiu'}, {'name': 'Xuanjing Huang'}, {'name': 'Dahua Lin'}]",2024-07-17T11:35:00Z
http://arxiv.org/abs/2407.12500v2,http://arxiv.org/abs/2407.12500v2,"Automate or Assist? The Role of Computational Models in Identifying
  Gendered Discourse in US Capital Trial Transcripts","The language used by US courtroom actors in criminal trials has long been
studied for biases. However, systematic studies for bias in high-stakes court
trials have been difficult, due to the nuanced nature of bias and the legal
expertise required. Large language models offer the possibility to automate
annotation. But validating the computational approach requires both an
understanding of how automated methods fit in existing annotation workflows and
what they really offer. We present a case study of adding a computational model
to a complex and high-stakes problem: identifying gender-biased language in US
capital trials for women defendants. Our team of experienced death-penalty
lawyers and NLP technologists pursue a three-phase study: first annotating
manually, then training and evaluating computational models, and finally
comparing expert annotations to model predictions. Unlike many typical NLP
tasks, annotating for gender bias in months-long capital trials is complicated,
with many individual judgment calls. Contrary to standard arguments for
automation that are based on efficiency and scalability, legal experts find the
computational models most useful in providing opportunities to reflect on their
own bias in annotation and to build consensus on annotation rules. This
experience suggests that seeking to replace experts with computational models
for complex annotation is both unrealistic and undesirable. Rather,
computational models offer valuable opportunities to assist the legal experts
in annotation-based studies.","[{'name': 'Andrea W Wen-Yi'}, {'name': 'Kathryn Adamson'}, {'name': 'Nathalie Greenfield'}, {'name': 'Rachel Goldberg'}, {'name': 'Sandra Babcock'}, {'name': 'David Mimno'}, {'name': 'Allison Koenecke'}]",2024-07-17T11:30:04Z
http://arxiv.org/abs/2407.12498v1,http://arxiv.org/abs/2407.12498v1,"Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of
  Few-Shot Learning","The linguistic capabilities of Multimodal Large Language Models (MLLMs) are
critical for their effective application across diverse tasks. This study aims
to evaluate the performance of MLLMs on the VALSE benchmark, focusing on the
efficacy of few-shot In-Context Learning (ICL), and Chain-of-Thought (CoT)
prompting. We conducted a comprehensive assessment of state-of-the-art MLLMs,
varying in model size and pretraining datasets. The experimental results reveal
that ICL and CoT prompting significantly boost model performance, particularly
in tasks requiring complex reasoning and contextual understanding. Models
pretrained on captioning datasets show superior zero-shot performance, while
those trained on interleaved image-text data benefit from few-shot learning.
Our findings provide valuable insights into optimizing MLLMs for better
grounding of language in visual contexts, highlighting the importance of the
composition of pretraining data and the potential of few-shot learning
strategies to improve the reasoning abilities of MLLMs.","[{'name': 'Mustafa Dogan'}, {'name': 'Ilker Kesen'}, {'name': 'Iacer Calixto'}, {'name': 'Aykut Erdem'}, {'name': 'Erkut Erdem'}]",2024-07-17T11:26:47Z
http://arxiv.org/abs/2407.12481v1,http://arxiv.org/abs/2407.12481v1,Pretraining Data and Tokenizer for Indic LLM,"We present a novel approach to data preparation for developing multilingual
Indic large language model. Our meticulous data acquisition spans open-source
and proprietary sources, including Common Crawl, Indic books, news articles,
and Wikipedia, ensuring a diverse and rich linguistic representation. For each
Indic language, we design a custom preprocessing pipeline to effectively
eliminate redundant and low-quality text content. Additionally, we perform
deduplication on Common Crawl data to address the redundancy present in 70% of
the crawled web pages. This study focuses on developing high-quality data,
optimizing tokenization for our multilingual dataset for Indic large language
models with 3B and 7B parameters, engineered for superior performance in Indic
languages. We introduce a novel multilingual tokenizer training strategy,
demonstrating our custom-trained Indic tokenizer outperforms the
state-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word
ratio for Indic languages.","[{'name': 'Rahul Kumar'}, {'name': 'Shubham Kakde'}, {'name': 'Divyansh Rajput'}, {'name': 'Daud Ibrahim'}, {'name': 'Rishabh Nahata'}, {'name': 'Pidathala Sowjanya'}, {'name': 'Deepak Kumar'}]",2024-07-17T11:06:27Z
http://arxiv.org/abs/2407.12473v1,http://arxiv.org/abs/2407.12473v1,A Novel Dependency Framework for Enhancing Discourse Data Analysis,"The development of different theories of discourse structure has led to the
establishment of discourse corpora based on these theories. However, the
existence of discourse corpora established on different theoretical bases
creates challenges when it comes to exploring them in a consistent and cohesive
way. This study has as its primary focus the conversion of PDTB annotations
into dependency structures. It employs refined BERT-based discourse parsers to
test the validity of the dependency data derived from the PDTB-style corpora in
English, Chinese, and several other languages. By converting both PDTB and RST
annotations for the same texts into dependencies, this study also applies
``dependency distance'' metrics to examine the correlation between RST
dependencies and PDTB dependencies in English. The results show that the PDTB
dependency data is valid and that there is a strong correlation between the two
types of dependency distance. This study presents a comprehensive approach for
analyzing and evaluating discourse corpora by employing discourse dependencies
to achieve unified analysis. By applying dependency representations, we can
extract data from PDTB, RST, and SDRT corpora in a coherent and unified manner.
Moreover, the cross-linguistic validation establishes the framework's
generalizability beyond English. The establishment of this comprehensive
dependency framework overcomes limitations of existing discourse corpora,
supporting a diverse range of algorithms and facilitating further studies in
computational discourse analysis and language sciences.","[{'name': 'Kun Sun'}, {'name': 'Rong Wang'}]",2024-07-17T10:55:00Z
http://arxiv.org/abs/2407.12425v1,http://arxiv.org/abs/2407.12425v1,"Navigating the Noisy Crowd: Finding Key Information for Claim
  Verification","Claim verification is a task that involves assessing the truthfulness of a
given claim based on multiple evidence pieces. Using large language models
(LLMs) for claim verification is a promising way. However, simply feeding all
the evidence pieces to an LLM and asking if the claim is factual does not yield
good results. The challenge lies in the noisy nature of both the evidence and
the claim: evidence passages typically contain irrelevant information, with the
key facts hidden within the context, while claims often convey multiple aspects
simultaneously. To navigate this ""noisy crowd"" of information, we propose EACon
(Evidence Abstraction and Claim Deconstruction), a framework designed to find
key information within evidence and verify each aspect of a claim separately.
EACon first finds keywords from the claim and employs fuzzy matching to select
relevant keywords for each raw evidence piece. These keywords serve as a guide
to extract and summarize critical information into abstracted evidence.
Subsequently, EACon deconstructs the original claim into subclaims, which are
then verified against both abstracted and raw evidence individually. We
evaluate EACon using two open-source LLMs on two challenging datasets. Results
demonstrate that EACon consistently and substantially improve LLMs' performance
in claim verification.","[{'name': 'Haisong Gong'}, {'name': 'Huanhuan Ma'}, {'name': 'Qiang Liu'}, {'name': 'Shu Wu'}, {'name': 'Liang Wang'}]",2024-07-17T09:24:10Z
http://arxiv.org/abs/2407.12402v1,http://arxiv.org/abs/2407.12402v1,"TurkishMMLU: Measuring Massive Multitask Language Understanding in
  Turkish","Multiple choice question answering tasks evaluate the reasoning,
comprehension, and mathematical abilities of Large Language Models (LLMs).
While existing benchmarks employ automatic translation for multilingual
evaluation, this approach is error-prone and potentially introduces culturally
biased questions, especially in social sciences. We introduce the first
multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'
understanding of the Turkish language. TurkishMMLU includes over 10,000
questions, covering 9 different subjects from Turkish high-school education
curricula. These questions are written by curriculum experts, suitable for the
high-school curricula in Turkey, covering subjects ranging from natural
sciences and math questions to more culturally representative topics such as
Turkish Literature and the history of the Turkish Republic. We evaluate over 20
LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),
closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)
models. We provide an extensive evaluation, including zero-shot and few-shot
evaluation of LLMs, chain-of-thought reasoning, and question difficulty
analysis along with model performance. We provide an in-depth analysis of the
Turkish capabilities and limitations of current LLMs to provide insights for
future LLMs for the Turkish language. We publicly release our code for the
dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.","[{'name': 'Arda Yüksel'}, {'name': 'Abdullatif Köksal'}, {'name': 'Lütfi Kerem Şenel'}, {'name': 'Anna Korhonen'}, {'name': 'Hinrich Schütze'}]",2024-07-17T08:28:55Z
http://arxiv.org/abs/2407.12393v4,http://arxiv.org/abs/2407.12393v4,PersLLM: A Personified Training Approach for Large Language Models,"Large language models exhibit aspects of human-level intelligence that
catalyze their application as human-like agents in domains such as social
simulations, human-machine interactions, and collaborative multi-agent systems.
However, the absence of distinct personalities, such as displaying ingratiating
behaviors, inconsistent opinions, and uniform response patterns, diminish LLMs
utility in practical applications. Addressing this, the development of
personality traits in LLMs emerges as a crucial area of research to unlock
their latent potential. Existing methods to personify LLMs generally involve
strategies like employing stylized training data for instruction tuning or
using prompt engineering to simulate different personalities. These methods
only capture superficial linguistic styles instead of the core of personalities
and are therefore not stable. In this study, we propose PersLLM, integrating
psychology-grounded principles of personality: social practice, consistency,
and dynamic development, into a comprehensive training methodology. We
incorporate personality traits directly into the model parameters, enhancing
the model's resistance to induction, promoting consistency, and supporting the
dynamic evolution of personality. Single-agent evaluation validates our
method's superiority, as it produces responses more aligned with reference
personalities compared to other approaches. Case studies for multi-agent
communication highlight its benefits in enhancing opinion consistency within
individual agents and fostering collaborative creativity among multiple agents
in dialogue contexts, potentially benefiting human simulation and multi-agent
cooperation. Additionally, human-agent interaction evaluations indicate that
our personified models significantly enhance interactive experiences,
underscoring the practical implications of our research.","[{'name': 'Zheni Zeng'}, {'name': 'Jiayi Chen'}, {'name': 'Huimin Chen'}, {'name': 'Yukun Yan'}, {'name': 'Yuxuan Chen'}, {'name': 'Zhenghao Liu'}, {'name': 'Zhiyuan Liu'}, {'name': 'Maosong Sun'}]",2024-07-17T08:13:22Z
http://arxiv.org/abs/2407.12389v1,http://arxiv.org/abs/2407.12389v1,Morphosyntactic Analysis for CHILDES,"Language development researchers are interested in comparing the process of
language learning across languages. Unfortunately, it has been difficult to
construct a consistent quantitative framework for such comparisons. However,
recent advances in AI (Artificial Intelligence) and ML (Machine Learning) are
providing new methods for ASR (automatic speech recognition) and NLP (natural
language processing) that can be brought to bear on this problem. Using the
Batchalign2 program (Liu et al., 2023), we have been transcribing and linking
data for the CHILDES database and have applied the UD (Universal Dependencies)
framework to provide a consistent and comparable morphosyntactic analysis for
27 languages. These new resources open possibilities for deeper crosslinguistic
study of language learning.","[{'name': 'Houjun Liu'}, {'name': 'Brian MacWhinney'}]",2024-07-17T08:11:24Z
http://arxiv.org/abs/2407.12376v1,http://arxiv.org/abs/2407.12376v1,Deep Learning-based Sentiment Analysis of Olympics Tweets,"Sentiment analysis (SA), is an approach of natural language processing (NLP)
for determining a text's emotional tone by analyzing subjective information
such as views, feelings, and attitudes toward specific topics, products,
services, events, or experiences. This study attempts to develop an advanced
deep learning (DL) model for SA to understand global audience emotions through
tweets in the context of the Olympic Games. The findings represent global
attitudes around the Olympics and contribute to advancing the SA models. We
have used NLP for tweet pre-processing and sophisticated DL models for arguing
with SA, this research enhances the reliability and accuracy of sentiment
classification. The study focuses on data selection, preprocessing,
visualization, feature extraction, and model building, featuring a baseline
Na\""ive Bayes (NB) model and three advanced DL models: Convolutional Neural
Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Bidirectional
Encoder Representations from Transformers (BERT). The results of the
experiments show that the BERT model can efficiently classify sentiments
related to the Olympics, achieving the highest accuracy of 99.23%.","[{'name': 'Indranil Bandyopadhyay'}, {'name': 'Rahul Karmakar'}]",2024-07-17T07:55:04Z
http://arxiv.org/abs/2407.12366v1,http://arxiv.org/abs/2407.12366v1,"NavGPT-2: Unleashing Navigational Reasoning Capability for Large
  Vision-Language Models","Capitalizing on the remarkable advancements in Large Language Models (LLMs),
there is a burgeoning initiative to harness LLMs for instruction following
robotic navigation. Such a trend underscores the potential of LLMs to
generalize navigational reasoning and diverse language understanding. However,
a significant discrepancy in agent performance is observed when integrating
LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous
downstream specialist models. Furthermore, the inherent capacity of language to
interpret and facilitate communication in agent interactions is often
underutilized in these integrations. In this work, we strive to bridge the
divide between VLN-specialized models and LLM-based navigation paradigms, while
maintaining the interpretative prowess of LLMs in generating linguistic
navigational reasoning. By aligning visual content in a frozen LLM, we
encompass visual observation comprehension for LLMs and exploit a way to
incorporate LLMs and navigation policy networks for effective action
predictions and navigational reasoning. We demonstrate the data efficiency of
the proposed methods and eliminate the gap between LM-based agents and
state-of-the-art VLN specialists.","[{'name': 'Gengze Zhou'}, {'name': 'Yicong Hong'}, {'name': 'Zun Wang'}, {'name': 'Xin Eric Wang'}, {'name': 'Qi Wu'}]",2024-07-17T07:44:26Z
http://arxiv.org/abs/2407.12888v1,http://arxiv.org/abs/2407.12888v1,"Explainable Biomedical Hypothesis Generation via Retrieval Augmented
  Generation enabled Large Language Models","The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.","[{'name': 'Alexander R. Pelletier'}, {'name': 'Joseph Ramirez'}, {'name': 'Irsyad Adam'}, {'name': 'Simha Sankar'}, {'name': 'Yu Yan'}, {'name': 'Ding Wang'}, {'name': 'Dylan Steinecke'}, {'name': 'Wei Wang'}, {'name': 'Peipei Ping'}]",2024-07-17T07:44:18Z
http://arxiv.org/abs/2407.12363v1,http://arxiv.org/abs/2407.12363v1,"Conversational Query Reformulation with the Guidance of Retrieved
  Documents","Conversational search seeks to retrieve relevant passages for the given
questions in Conversational QA (ConvQA). Questions in ConvQA face challenges
such as omissions and coreferences, making it difficult to obtain desired
search results. Conversational Query Reformulation (CQR) transforms these
current queries into de-contextualized forms to resolve these issues. However,
existing CQR methods focus on rewriting human-friendly queries, which may not
always yield optimal search results for the retriever. To overcome this
challenge, we introduce GuideCQR, a framework that utilizes guided documents to
refine queries, ensuring that they are optimal for retrievers. Specifically, we
augment keywords, generate expected answers from the re-ranked documents, and
unify them with the filtering process. Experimental results show that queries
enhanced by guided documents outperform previous CQR methods. Especially,
GuideCQR surpasses the performance of Large Language Model (LLM) prompt-powered
approaches and demonstrates the importance of the guided documents in
formulating retriever-friendly queries across diverse setups.","[{'name': 'Jeonghyun Park'}, {'name': 'Hwanhee Lee'}]",2024-07-17T07:39:16Z
http://arxiv.org/abs/2407.12358v1,http://arxiv.org/abs/2407.12358v1,"ProcTag: Process Tagging for Assessing the Efficacy of Document
  Instruction Data","Recently, large language models (LLMs) and multimodal large language models
(MLLMs) have demonstrated promising results on document visual question
answering (VQA) task, particularly after training on document instruction
datasets. An effective evaluation method for document instruction data is
crucial in constructing instruction data with high efficacy, which, in turn,
facilitates the training of LLMs and MLLMs for document VQA. However, most
existing evaluation methods for instruction data are limited to the textual
content of the instructions themselves, thereby hindering the effective
assessment of document instruction datasets and constraining their
construction. In this paper, we propose ProcTag, a data-oriented method that
assesses the efficacy of document instruction data. ProcTag innovatively
performs tagging on the execution process of instructions rather than the
instruction text itself. By leveraging the diversity and complexity of these
tags to assess the efficacy of the given dataset, ProcTag enables selective
sampling or filtering of document instructions. Furthermore, DocLayPrompt, a
novel semi-structured layout-aware document prompting strategy, is proposed for
effectively representing documents. Experiments demonstrate that sampling
existing open-sourced and generated document VQA/instruction datasets with
ProcTag significantly outperforms current methods for evaluating instruction
data. Impressively, with ProcTag-based sampling in the generated document
datasets, only 30.5\% of the document instructions are required to achieve
100\% efficacy compared to the complete dataset. The code is publicly available
at
https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/ProcTag.","[{'name': 'Yufan Shen'}, {'name': 'Chuwei Luo'}, {'name': 'Zhaoqing Zhu'}, {'name': 'Yang Chen'}, {'name': 'Qi Zheng'}, {'name': 'Zhi Yu'}, {'name': 'Jiajun Bu'}, {'name': 'Cong Yao'}]",2024-07-17T07:29:59Z
http://arxiv.org/abs/2407.12344v1,http://arxiv.org/abs/2407.12344v1,"The Better Angels of Machine Personality: How Personality Relates to LLM
  Safety","Personality psychologists have analyzed the relationship between personality
and safety behaviors in human society. Although Large Language Models (LLMs)
demonstrate personality traits, the relationship between personality traits and
safety abilities in LLMs still remains a mystery. In this paper, we discover
that LLMs' personality traits are closely related to their safety abilities,
i.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.
Meanwhile, the safety alignment generally increases various LLMs' Extraversion,
Sensing, and Judging traits. According to such findings, we can edit LLMs'
personality traits and improve their safety performance, e.g., inducing
personality from ISTJ to ISTP resulted in a relative improvement of
approximately 43% and 10% in privacy and fairness performance, respectively.
Additionally, we find that LLMs with different personality traits are
differentially susceptible to jailbreak. This study pioneers the investigation
of LLM safety from a personality perspective, providing new insights into LLM
safety enhancement.","[{'name': 'Jie Zhang'}, {'name': 'Dongrui Liu'}, {'name': 'Chen Qian'}, {'name': 'Ziyue Gan'}, {'name': 'Yong Liu'}, {'name': 'Yu Qiao'}, {'name': 'Jing Shao'}]",2024-07-17T06:36:29Z
http://arxiv.org/abs/2407.12342v1,http://arxiv.org/abs/2407.12342v1,"Word Embedding Dimension Reduction via Weakly-Supervised Feature
  Selection","As a fundamental task in natural language processing, word embedding converts
each word into a representation in a vector space. A challenge with word
embedding is that as the vocabulary grows, the vector space's dimension
increases and it can lead to a vast model size. Storing and processing word
vectors are resource-demanding, especially for mobile edge-devices
applications. This paper explores word embedding dimension reduction. To
balance computational costs and performance, we propose an efficient and
effective weakly-supervised feature selection method, named WordFS. It has two
variants, each utilizing novel criteria for feature selection. Experiments
conducted on various tasks (e.g., word and sentence similarity and binary and
multi-class classification) indicate that the proposed WordFS model outperforms
other dimension reduction methods at lower computational costs.","[{'name': 'Jintang Xue'}, {'name': 'Yun-Cheng Wang'}, {'name': 'Chengwei Wei'}, {'name': 'C. -C. Jay Kuo'}]",2024-07-17T06:36:09Z
http://arxiv.org/abs/2407.12336v1,http://arxiv.org/abs/2407.12336v1,M2DS: Multilingual Dataset for Multi-document Summarisation,"In the rapidly evolving digital era, there is an increasing demand for
concise information as individuals seek to distil key insights from various
sources. Recent attention from researchers on Multi-document Summarisation
(MDS) has resulted in diverse datasets covering customer reviews, academic
papers, medical and legal documents, and news articles. However, the
English-centric nature of these datasets has created a conspicuous void for
multilingual datasets in today's globalised digital landscape, where linguistic
diversity is celebrated. Media platforms such as British Broadcasting
Corporation (BBC) have disseminated news in 20+ languages for decades. With
only 380 million people speaking English natively as their first language,
accounting for less than 5% of the global population, the vast majority
primarily relies on other languages. These facts underscore the need for
inclusivity in MDS research, utilising resources from diverse languages.
Recognising this gap, we present the Multilingual Dataset for Multi-document
Summarisation (M2DS), which, to the best of our knowledge, is the first dataset
of its kind. It includes document-summary pairs in five languages from BBC
articles published during the 2010-2023 period. This paper introduces M2DS,
emphasising its unique multilingual aspect, and includes baseline scores from
state-of-the-art MDS models evaluated on our dataset.","[{'name': 'Kushan Hewapathirana'}, {'name': 'Nisansa de Silva'}, {'name': 'C. D. Athuraliya'}]",2024-07-17T06:25:51Z
http://arxiv.org/abs/2407.12327v1,http://arxiv.org/abs/2407.12327v1,"Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language
  Models","Post-training quantization is the leading method for addressing
memory-related bottlenecks in LLM inference, but unfortunately, it suffers from
significant performance degradation below 4-bit precision. An alternative
approach involves training compressed models directly at a low bitwidth (e.g.,
binary or ternary models). However, the performance, training dynamics, and
scaling trends of such models are not yet well understood. To address this
issue, we train and openly release the Spectra LLM suite consisting of 54
language models ranging from 99M to 3.9B parameters, trained on 300B tokens.
Spectra includes FloatLMs, post-training quantized QuantLMs (3, 4, 6, and 8
bits), and ternary LLMs (TriLMs) - our improved architecture for ternary
language modeling, which significantly outperforms previously proposed ternary
models of a given size (in bits), matching half-precision models at scale. For
example, TriLM 3.9B is (bit-wise) smaller than the half-precision FloatLM 830M,
but matches half-precision FloatLM 3.9B in commonsense reasoning and knowledge
benchmarks. However, TriLM 3.9B is also as toxic and stereotyping as FloatLM
3.9B, a model six times larger in size. Additionally, TriLM 3.9B lags behind
FloatLM in perplexity on validation splits and web-based corpora but performs
better on less noisy datasets like Lambada and PennTreeBank.
  To enhance understanding of low-bitwidth models, we are releasing 500+
intermediate checkpoints of the Spectra suite at
\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.","[{'name': 'Ayush Kaushal'}, {'name': 'Tejas Pandey'}, {'name': 'Tejas Vaidhya'}, {'name': 'Aaryan Bhagat'}, {'name': 'Irina Rish'}]",2024-07-17T05:53:20Z
http://arxiv.org/abs/2407.21033v1,http://arxiv.org/abs/2407.21033v1,"Multi-Grained Query-Guided Set Prediction Network for Grounded
  Multimodal Named Entity Recognition","Grounded Multimodal Named Entity Recognition (GMNER) is an emerging
information extraction (IE) task, aiming to simultaneously extract entity
spans, types, and entity-matched bounding box groundings in images from given
sentence-image pairs data. Recent unified methods employing machine reading
comprehension (MRC-based) frameworks or sequence generation-based models face
challenges in understanding the relationships of multimodal entities. MRC-based
frameworks, utilizing human-designed queries, struggle to model intra-entity
connections. Meanwhile, sequence generation-based outputs excessively rely on
inter-entity dependencies due to pre-defined decoding order. To tackle these,
we propose a novel unified framework named Multi-grained Query-guided Set
Prediction Network (MQSPN) to learn appropriate relationships at intra-entity
and inter-entity levels. Specifically, MQSPN consists of a Multi-grained Query
Set (MQS) and a Multimodal Set Prediction Network (MSP). MQS combines specific
type-grained and learnable entity-grained queries to adaptively strengthen
intra-entity connections by explicitly aligning visual regions with textual
spans. Based on solid intra-entity modeling, MSP reformulates GMNER as a set
prediction, enabling the parallel prediction of multimodal entities in a
non-autoregressive manner, eliminating redundant dependencies from preceding
sequences, and guiding models to establish appropriate inter-entity
relationships from a global matching perspective. Additionally, to boost better
alignment of two-level relationships, we also incorporate a Query-guided Fusion
Net (QFNet) to work as a glue network between MQS and MSP. Extensive
experiments demonstrate that our approach achieves state-of-the-art
performances in widely used benchmarks. Notably, our method improves 2.83% F1
in the difficult fine-grained GMNER benchmark.","[{'name': 'Jielong Tang'}, {'name': 'Zhenxing Wang'}, {'name': 'Ziyang Gong'}, {'name': 'Jianxing Yu'}, {'name': 'Shuang Wang'}, {'name': 'Jian Yin'}]",2024-07-17T05:42:43Z
http://arxiv.org/abs/2407.12309v1,http://arxiv.org/abs/2407.12309v1,"MEDFuse: Multimodal EHR Data Fusion with Masked Lab-Test Modeling and
  Large Language Models","Electronic health records (EHRs) are multimodal by nature, consisting of
structured tabular features like lab tests and unstructured clinical notes. In
real-life clinical practice, doctors use complementary multimodal EHR data
sources to get a clearer picture of patients' health and support clinical
decision-making. However, most EHR predictive models do not reflect these
procedures, as they either focus on a single modality or overlook the
inter-modality interactions/redundancy. In this work, we propose MEDFuse, a
Multimodal EHR Data Fusion framework that incorporates masked lab-test modeling
and large language models (LLMs) to effectively integrate structured and
unstructured medical data. MEDFuse leverages multimodal embeddings extracted
from two sources: LLMs fine-tuned on free clinical text and masked tabular
transformers trained on structured lab test results. We design a disentangled
transformer module, optimized by a mutual information loss to 1) decouple
modality-specific and modality-shared information and 2) extract useful joint
representation from the noise and redundancy present in clinical notes. Through
comprehensive validation on the public MIMIC-III dataset and the in-house FEMH
dataset, MEDFuse demonstrates great potential in advancing clinical
predictions, achieving over 90% F1 score in the 10-disease multi-label
classification task.","[{'name': 'Thao Minh Nguyen Phan'}, {'name': 'Cong-Tinh Dao'}, {'name': 'Chenwei Wu'}, {'name': 'Jian-Zhe Wang'}, {'name': 'Shun Liu'}, {'name': 'Jun-En Ding'}, {'name': 'David Restrepo'}, {'name': 'Feng Liu'}, {'name': 'Fang-Ming Hung'}, {'name': 'Wen-Chih Peng'}]",2024-07-17T04:17:09Z
http://arxiv.org/abs/2407.12277v1,http://arxiv.org/abs/2407.12277v1,Multimodal Reranking for Knowledge-Intensive Visual Question Answering,"Knowledge-intensive visual question answering requires models to effectively
use external knowledge to help answer visual questions. A typical pipeline
includes a knowledge retriever and an answer generator. However, a retriever
that utilizes local information, such as an image patch, may not provide
reliable question-candidate relevance scores. Besides, the two-tower
architecture also limits the relevance score modeling of a retriever to select
top candidates for answer generator reasoning. In this paper, we introduce an
additional module, a multi-modal reranker, to improve the ranking quality of
knowledge candidates for answer generation. Our reranking module takes
multi-modal information from both candidates and questions and performs
cross-item interaction for better relevance score modeling. Experiments on
OK-VQA and A-OKVQA show that multi-modal reranker from distant supervision
provides consistent improvements. We also find a training-testing discrepancy
with reranking in answer generation, where performance improves if training
knowledge candidates are similar to or noisier than those used in testing.","[{'name': 'Haoyang Wen'}, {'name': 'Honglei Zhuang'}, {'name': 'Hamed Zamani'}, {'name': 'Alexander Hauptmann'}, {'name': 'Michael Bendersky'}]",2024-07-17T02:58:52Z
http://arxiv.org/abs/2407.12259v1,http://arxiv.org/abs/2407.12259v1,In-Context Probing Approximates Influence Function for Data Valuation,"Data valuation quantifies the value of training data, and is used for data
attribution (i.e., determining the contribution of training data towards model
predictions), and data selection; both of which are important for curating
high-quality datasets to train large language models. In our paper, we show
that data valuation through in-context probing (i.e., prompting a LLM)
approximates influence functions for selecting training data. We provide a
theoretical sketch on this connection based on transformer models performing
""implicit"" gradient descent on its in-context inputs. Our empirical findings
show that in-context probing and gradient-based influence frameworks are
similar in how they rank training data. Furthermore, fine-tuning experiments on
data selected by either method reveal similar model performance.","[{'name': 'Cathy Jiao'}, {'name': 'Gary Gao'}, {'name': 'Chenyan Xiong'}]",2024-07-17T02:06:56Z
http://arxiv.org/abs/2407.12247v1,http://arxiv.org/abs/2407.12247v1,"Lacuna Language Learning: Leveraging RNNs for Ranked Text Completion in
  Digitized Coptic Manuscripts","Ancient manuscripts are frequently damaged, containing gaps in the text known
as lacunae. In this paper, we present a bidirectional RNN model for character
prediction of Coptic characters in manuscript lacunae. Our best model performs
with 72% accuracy on single character reconstruction, but falls to 37% when
reconstructing lacunae of various lengths. While not suitable for definitive
manuscript reconstruction, we argue that our RNN model can help scholars rank
the likelihood of textual reconstructions. As evidence, we use our RNN model to
rank reconstructions in two early Coptic manuscripts. Our investigation shows
that neural models can augment traditional methods of textual restoration,
providing scholars with an additional tool to assess lacunae in Coptic
manuscripts.","[{'name': 'Lauren Levine'}, {'name': 'Cindy Tung Li'}, {'name': 'Lydia Bremer-McCollum'}, {'name': 'Nicholas Wagner'}, {'name': 'Amir Zeldes'}]",2024-07-17T01:28:12Z
http://arxiv.org/abs/2407.12220v1,http://arxiv.org/abs/2407.12220v1,Questionable practices in machine learning,"Evaluating modern ML models is hard. The strong incentive for researchers and
companies to report a state-of-the-art result on some metric often leads to
questionable research practices (QRPs): bad practices which fall short of
outright research fraud. We describe 43 such practices which can undermine
reported results, giving examples where possible. Our list emphasises the
evaluation of large language models (LLMs) on public benchmarks. We also
discuss ""irreproducible research practices"", i.e. decisions that make it
difficult or impossible for other researchers to reproduce, build on or audit
previous research.","[{'name': 'Gavin Leech'}, {'name': 'Juan J. Vazquez'}, {'name': 'Misha Yagudin'}, {'name': 'Niclas Kupper'}, {'name': 'Laurence Aitchison'}]",2024-07-17T00:06:30Z
http://arxiv.org/abs/2407.12886v1,http://arxiv.org/abs/2407.12886v1,Whitening Not Recommended for Classification Tasks in LLMs,"Sentence embedding is a cornerstone in NLP. Whitening has been claimed to be
an effective operation to improve embedding quality obtained from Large
Language Models (LLMs). However, we find that the efficacy of whitening is
model-dependent and task-dependent. In particular, whitening degenerates
embeddings for classification tasks. The conclusion is supported by extensive
experiments. We also explored a variety of whitening operations, including PCA,
ZCA, PCA-Cor, ZCA-Cor and Cholesky whitenings. A by-product of our research is
embedding evaluation platform for LLMs called SentEval+.","[{'name': 'Ali Forooghi'}, {'name': 'Shaghayegh Sadeghi'}, {'name': 'Jianguo Lu'}]",2024-07-16T22:48:30Z
http://arxiv.org/abs/2407.12206v1,http://arxiv.org/abs/2407.12206v1,A Language Modeling Approach to Diacritic-Free Hebrew TTS,"We tackle the task of text-to-speech (TTS) in Hebrew. Traditional Hebrew
contains Diacritics, which dictate the way individuals should pronounce given
words, however, modern Hebrew rarely uses them. The lack of diacritics in
modern Hebrew results in readers expected to conclude the correct pronunciation
and understand which phonemes to use based on the context. This imposes a
fundamental challenge on TTS systems to accurately map between text-to-speech.
In this work, we propose to adopt a language modeling Diacritics-Free approach,
for the task of Hebrew TTS. The model operates on discrete speech
representations and is conditioned on a word-piece tokenizer. We optimize the
proposed method using in-the-wild weakly supervised data and compare it to
several diacritic-based TTS systems. Results suggest the proposed method is
superior to the evaluated baselines considering both content preservation and
naturalness of the generated speech. Samples can be found under the following
link: pages.cs.huji.ac.il/adiyoss-lab/HebTTS/","[{'name': 'Amit Roth'}, {'name': 'Arnon Turetzky'}, {'name': 'Yossi Adi'}]",2024-07-16T22:43:49Z
http://arxiv.org/abs/2407.12196v1,http://arxiv.org/abs/2407.12196v1,MASIVE: Open-Ended Affective State Identification in English and Spanish,"In the field of emotion analysis, much NLP research focuses on identifying a
limited number of discrete emotion categories, often applied across languages.
These basic sets, however, are rarely designed with textual data in mind, and
culture, language, and dialect can influence how particular emotions are
interpreted. In this work, we broaden our scope to a practically unbounded set
of \textit{affective states}, which includes any terms that humans use to
describe their experiences of feeling. We collect and publish MASIVE, a dataset
of Reddit posts in English and Spanish containing over 1,000 unique affective
states each. We then define the new problem of \textit{affective state
identification} for language generation models framed as a masked span
prediction task. On this task, we find that smaller finetuned multilingual
models outperform much larger LLMs, even on region-specific Spanish affective
states. Additionally, we show that pretraining on MASIVE improves model
performance on existing emotion benchmarks. Finally, through machine
translation experiments, we find that native speaker-written data is vital to
good performance on this task.","[{'name': 'Nicholas Deas'}, {'name': 'Elsbeth Turcan'}, {'name': 'Iván Pérez Mejía'}, {'name': 'Kathleen McKeown'}]",2024-07-16T21:43:47Z
http://arxiv.org/abs/2407.12176v1,http://arxiv.org/abs/2407.12176v1,GPT-4V Cannot Generate Radiology Reports Yet,"GPT-4V's purported strong multimodal abilities raise interests in using it to
automate radiology report writing, but there lacks thorough evaluations. In
this work, we perform a systematic evaluation of GPT-4V in generating radiology
reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt
to directly generate reports using GPT-4V through different prompting
strategies and find that it fails terribly in both lexical metrics and clinical
efficacy metrics. To understand the low performance, we decompose the task into
two steps: 1) the medical image reasoning step of predicting medical condition
labels from images; and 2) the report synthesis step of generating reports from
(groundtruth) conditions. We show that GPT-4V's performance in image reasoning
is consistently low across different prompts. In fact, the distributions of
model-predicted labels remain constant regardless of which groundtruth
conditions are present on the image, suggesting that the model is not
interpreting chest X-rays meaningfully. Even when given groundtruth conditions
in report synthesis, its generated reports are less correct and less
natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt
on the viability of using GPT-4V in a radiology workflow.","[{'name': 'Yuyang Jiang'}, {'name': 'Chacha Chen'}, {'name': 'Dang Nguyen'}, {'name': 'Benjamin M. Mervak'}, {'name': 'Chenhao Tan'}]",2024-07-16T21:03:14Z
http://arxiv.org/abs/2407.13796v1,http://arxiv.org/abs/2407.13796v1,"Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large
  Language Models","Security concerns for large language models (LLMs) have recently escalated,
focusing on thwarting jailbreaking attempts in discrete prompts. However, the
exploration of jailbreak vulnerabilities arising from continuous embeddings has
been limited, as prior approaches primarily involved appending discrete or
continuous suffixes to inputs. Our study presents a novel channel for
conducting direct attacks on LLM inputs, eliminating the need for suffix
addition or specific questions provided that the desired output is predefined.
We additionally observe that extensive iterations often lead to overfitting,
characterized by repetition in the output. To counteract this, we propose a
simple yet effective strategy named CLIP. Our experiments show that for an
input length of 40 at iteration 1000, applying CLIP improves the ASR from 62%
to 83%","[{'name': 'Zihao Xu'}, {'name': 'Yi Liu'}, {'name': 'Gelei Deng'}, {'name': 'Kailong Wang'}, {'name': 'Yuekang Li'}, {'name': 'Ling Shi'}, {'name': 'Stjepan Picek'}]",2024-07-16T20:53:00Z
http://arxiv.org/abs/2407.12141v1,http://arxiv.org/abs/2407.12141v1,"Predicting Emotion Intensity in Polish Political Texts: Comparing
  Supervised Models and Large Language Models in a Resource-Poor Language","This study explores the use of large language models (LLMs) to predict
emotion intensity in Polish political texts, a resource-poor language context.
The research compares the performance of several LLMs against a supervised
model trained on an annotated corpus of 10,000 social media texts, evaluated
for the intensity of emotions by expert judges. The findings indicate that
while the supervised model generally outperforms LLMs, offering higher accuracy
and lower variance, LLMs present a viable alternative, especially given the
high costs associated with data annotation. The study highlights the potential
of LLMs in low-resource language settings and underscores the need for further
research on emotion intensity prediction and its application across different
languages and continuous features. The implications suggest a nuanced
decision-making process to choose the right approach to emotion prediction for
researchers and practitioners based on resource availability and the specific
requirements of their tasks.","[{'name': 'Hubert Plisiecki'}, {'name': 'Piotr Koc'}, {'name': 'Maria Flakus'}, {'name': 'Artur Pokropek'}]",2024-07-16T19:53:14Z
http://arxiv.org/abs/2407.12126v2,http://arxiv.org/abs/2407.12126v2,"LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text
  Translation","Machine translation is indispensable in healthcare for enabling the global
dissemination of medical knowledge across languages. However, complex medical
terminology poses unique challenges to achieving adequate translation quality
and accuracy. This study introduces a novel ""LLMs-in-the-loop"" approach to
develop supervised neural machine translation models optimized specifically for
medical texts. While large language models (LLMs) have demonstrated powerful
capabilities, this research shows that small, specialized models trained on
high-quality in-domain (mostly synthetic) data can outperform even vastly
larger LLMs.
  Custom parallel corpora in six languages were compiled from scientific
articles, synthetically generated clinical documents, and medical texts. Our
LLMs-in-the-loop methodology employs synthetic data generation, rigorous
evaluation, and agent orchestration to enhance performance. We developed small
medical translation models using the MarianMT base model. We introduce a new
medical translation test dataset to standardize evaluation in this domain.
Assessed using BLEU, METEOR, ROUGE, and BERT scores on this test set, our
MarianMT-based models outperform Google Translate, DeepL, and GPT-4-Turbo.
  Results demonstrate that our LLMs-in-the-loop approach, combined with
fine-tuning high-quality, domain-specific data, enables specialized models to
outperform general-purpose and some larger systems. This research, part of a
broader series on expert small models, paves the way for future
healthcare-related AI developments, including deidentification and bio-medical
entity extraction models. Our study underscores the potential of tailored
neural translation models and the LLMs-in-the-loop methodology to advance the
field through improved data generation, evaluation, agent, and modeling
techniques.","[{'name': 'Bunyamin Keles'}, {'name': 'Murat Gunay'}, {'name': 'Serdar I. Caglar'}]",2024-07-16T19:32:23Z
http://arxiv.org/abs/2407.12108v1,http://arxiv.org/abs/2407.12108v1,Private prediction for large-scale synthetic text generation,"We present an approach for generating differentially private synthetic text
using large language models (LLMs), via private prediction. In the private
prediction framework, we only require the output synthetic data to satisfy
differential privacy guarantees. This is in contrast to approaches that train a
generative model on potentially sensitive user-supplied source data and seek to
ensure the model itself is safe to release.
  We prompt a pretrained LLM with source data, but ensure that next-token
predictions are made with differential privacy guarantees. Previous work in
this paradigm reported generating a small number of examples (<10) at
reasonable privacy levels, an amount of data that is useful only for downstream
in-context learning or prompting. In contrast, we make changes that allow us to
generate thousands of high-quality synthetic data points, greatly expanding the
set of potential applications. Our improvements come from an improved privacy
analysis and a better private selection mechanism, which makes use of the
equivalence between the softmax layer for sampling tokens in LLMs and the
exponential mechanism. Furthermore, we introduce a novel use of public
predictions via the sparse vector technique, in which we do not pay privacy
costs for tokens that are predictable without sensitive data; we find this to
be particularly effective for structured data.","[{'name': 'Kareem Amin'}, {'name': 'Alex Bie'}, {'name': 'Weiwei Kong'}, {'name': 'Alexey Kurakin'}, {'name': 'Natalia Ponomareva'}, {'name': 'Umar Syed'}, {'name': 'Andreas Terzis'}, {'name': 'Sergei Vassilvitskii'}]",2024-07-16T18:28:40Z
http://arxiv.org/abs/2407.12101v1,http://arxiv.org/abs/2407.12101v1,Better RAG using Relevant Information Gain,"A common way to extend the memory of large language models (LLMs) is by
retrieval augmented generation (RAG), which inserts text retrieved from a
larger memory into an LLM's context window. However, the context window is
typically limited to several thousand tokens, which limits the number of
retrieved passages that can inform a model's response. For this reason, it's
important to avoid occupying context window space with redundant information by
ensuring a degree of diversity among retrieved passages. At the same time, the
information should also be relevant to the current task. Most prior methods
that encourage diversity among retrieved results, such as Maximal Marginal
Relevance (MMR), do so by incorporating an objective that explicitly trades off
diversity and relevance. We propose a novel simple optimization metric based on
relevant information gain, a probabilistic measure of the total information
relevant to a query for a set of retrieved results. By optimizing this metric,
diversity organically emerges from our system. When used as a drop-in
replacement for the retrieval component of a RAG system, this method yields
state-of-the-art performance on question answering tasks from the Retrieval
Augmented Generation Benchmark (RGB), outperforming existing metrics that
directly optimize for relevance and diversity.","[{'name': 'Marc Pickett'}, {'name': 'Jeremy Hartman'}, {'name': 'Ayan Kumar Bhowmick'}, {'name': 'Raquib-ul Alam'}, {'name': 'Aditya Vempaty'}]",2024-07-16T18:09:21Z
http://arxiv.org/abs/2407.12094v1,http://arxiv.org/abs/2407.12094v1,"Identifying Speakers in Dialogue Transcripts: A Text-based Approach
  Using Pretrained Language Models","We introduce an approach to identifying speaker names in dialogue
transcripts, a crucial task for enhancing content accessibility and
searchability in digital media archives. Despite the advancements in speech
recognition, the task of text-based speaker identification (SpeakerID) has
received limited attention, lacking large-scale, diverse datasets for effective
model training. Addressing these gaps, we present a novel, large-scale dataset
derived from the MediaSum corpus, encompassing transcripts from a wide range of
media sources. We propose novel transformer-based models tailored for
SpeakerID, leveraging contextual cues within dialogues to accurately attribute
speaker names. Through extensive experiments, our best model achieves a great
precision of 80.3\%, setting a new benchmark for SpeakerID. The data and code
are publicly available here:
\url{https://github.com/adobe-research/speaker-identification}","[{'name': 'Minh Nguyen'}, {'name': 'Franck Dernoncourt'}, {'name': 'Seunghyun Yoon'}, {'name': 'Hanieh Deilamsalehy'}, {'name': 'Hao Tan'}, {'name': 'Ryan Rossi'}, {'name': 'Quan Hung Tran'}, {'name': 'Trung Bui'}, {'name': 'Thien Huu Nguyen'}]",2024-07-16T18:03:58Z
http://arxiv.org/abs/2407.12077v1,http://arxiv.org/abs/2407.12077v1,"GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill
  and Extreme KV-Cache Compression","We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model
that uses a new technique to efficiently generate a highly compressed and
reusable KV-Cache in linear time and space with respect to sequence length.
GoldFinch stacks our new GOLD transformer on top of an enhanced version of the
Finch (RWKV-6) architecture. We train up to 1.5B parameter class models of the
Finch, Llama, and GoldFinch architectures, and find dramatically improved
modeling performance relative to both Finch and Llama. Our cache size savings
increase linearly with model layer count, ranging from 756-2550 times smaller
than the traditional transformer cache for common sizes, enabling inference of
extremely large context lengths even on limited hardware. Although
autoregressive generation has O(n) time complexity per token because of
attention, pre-fill computation of the entire initial cache state for a
submitted context costs only O(1) time per token due to the use of a recurrent
neural network (RNN) to generate this cache. We release our trained weights and
training code under the Apache 2.0 license for community use.","[{'name': 'Daniel Goldstein'}, {'name': 'Fares Obeid'}, {'name': 'Eric Alcaide'}, {'name': 'Guangyu Song'}, {'name': 'Eugene Cheah'}]",2024-07-16T18:00:00Z
http://arxiv.org/abs/2407.11969v2,http://arxiv.org/abs/2407.11969v2,Does Refusal Training in LLMs Generalize to the Past Tense?,"Refusal training is widely used to prevent LLMs from generating harmful,
undesirable, or illegal outputs. We reveal a curious generalization gap in the
current refusal training approaches: simply reformulating a harmful request in
the past tense (e.g., ""How to make a Molotov cocktail?"" to ""How did people make
a Molotov cocktail?"") is often sufficient to jailbreak many state-of-the-art
LLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,
GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, and R2D2 models
using GPT-3.5 Turbo as a reformulation model. For example, the success rate of
this simple attack on GPT-4o increases from 1% using direct requests to 88%
using 20 past tense reformulation attempts on harmful requests from
JailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find
that reformulations in the future tense are less effective, suggesting that
refusal guardrails tend to consider past historical questions more benign than
hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5
Turbo show that defending against past reformulations is feasible when past
tense examples are explicitly included in the fine-tuning data. Overall, our
findings highlight that the widely used alignment techniques -- such as SFT,
RLHF, and adversarial training -- employed to align the studied models can be
brittle and do not always generalize as intended. We provide code and jailbreak
artifacts at https://github.com/tml-epfl/llm-past-tense.","[{'name': 'Maksym Andriushchenko'}, {'name': 'Nicolas Flammarion'}]",2024-07-16T17:59:55Z
http://arxiv.org/abs/2407.11963v1,http://arxiv.org/abs/2407.11963v1,"NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context
  Window?","In evaluating the long-context capabilities of large language models (LLMs),
identifying content relevant to a user's query from original long documents is
a crucial prerequisite for any LLM to answer questions based on long text. We
present NeedleBench, a framework consisting of a series of progressively more
challenging tasks for assessing bilingual long-context capabilities, spanning
multiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and
different depth ranges, allowing the strategic insertion of critical data
points in different text depth zones to rigorously test the retrieval and
reasoning capabilities of models in diverse contexts. We use the NeedleBench
framework to assess how well the leading open-source models can identify key
information relevant to the question and apply that information to reasoning in
bilingual long texts. Furthermore, we propose the Ancestral Trace Challenge
(ATC) to mimic the complexity of logical reasoning challenges that are likely
to be present in real-world long-context tasks, providing a simple method for
evaluating LLMs in dealing with complex long-context situations. Our results
suggest that current LLMs have significant room for improvement in practical
long-context applications, as they struggle with the complexity of logical
reasoning challenges that are likely to be present in real-world long-context
tasks. All codes and resources are available at OpenCompass:
https://github.com/open-compass/opencompass.","[{'name': 'Mo Li'}, {'name': 'Songyang Zhang'}, {'name': 'Yunxin Liu'}, {'name': 'Kai Chen'}]",2024-07-16T17:59:06Z
http://arxiv.org/abs/2407.12883v1,http://arxiv.org/abs/2407.12883v1,"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive
  Retrieval","Existing retrieval benchmarks primarily consist of information-seeking
queries (e.g., aggregated questions from search engines) where keyword or
semantic-based retrieval is usually sufficient. However, many complex
real-world queries require in-depth reasoning to identify relevant documents
that go beyond surface form matching. For example, finding documentation for a
coding question requires understanding the logic and syntax of the functions
involved. To better benchmark retrieval on such challenging queries, we
introduce BRIGHT, the first text retrieval benchmark that requires intensive
reasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398
real-world queries collected from diverse domains (such as economics,
psychology, robotics, software engineering, earth sciences, etc.), sourced from
naturally occurring or carefully curated human data. Extensive evaluation
reveals that even state-of-the-art retrieval models perform poorly on BRIGHT.
The leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0
nDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate
that augmenting queries with Chain-of-Thought reasoning generated by large
language models (LLMs) improves performance by up to 12.2 points. Moreover,
BRIGHT is robust against data leakage during pretraining of the benchmarked
models as we validate by showing similar performance even when documents from
the benchmark are included in the training data. We believe that BRIGHT paves
the way for future research on retrieval systems in more realistic and
challenging settings. Our code and data are available at
https://brightbenchmark.github.io.","[{'name': 'Hongjin Su'}, {'name': 'Howard Yen'}, {'name': 'Mengzhou Xia'}, {'name': 'Weijia Shi'}, {'name': 'Niklas Muennighoff'}, {'name': 'Han-yu Wang'}, {'name': 'Haisu Liu'}, {'name': 'Quan Shi'}, {'name': 'Zachary S. Siegel'}, {'name': 'Michael Tang'}, {'name': 'Ruoxi Sun'}, {'name': 'Jinsung Yoon'}, {'name': 'Sercan O. Arik'}, {'name': 'Danqi Chen'}, {'name': 'Tao Yu'}]",2024-07-16T17:58:27Z
http://arxiv.org/abs/2407.11948v1,http://arxiv.org/abs/2407.11948v1,"Rethinking Transformer-based Multi-document Summarization: An Empirical
  Investigation","The utilization of Transformer-based models prospers the growth of
multi-document summarization (MDS). Given the huge impact and widespread
adoption of Transformer-based models in various natural language processing
tasks, investigating their performance and behaviors in the context of MDS
becomes crucial for advancing the field and enhancing the quality of summary.
To thoroughly examine the behaviours of Transformer-based MDS models, this
paper presents five empirical studies on (1) measuring the impact of document
boundary separators quantitatively; (2) exploring the effectiveness of
different mainstream Transformer structures; (3) examining the sensitivity of
the encoder and decoder; (4) discussing different training strategies; and (5)
discovering the repetition in a summary generation. The experimental results on
prevalent MDS datasets and eleven evaluation metrics show the influence of
document boundary separators, the granularity of different level features and
different model training strategies. The results also reveal that the decoder
exhibits greater sensitivity to noises compared to the encoder. This
underscores the important role played by the decoder, suggesting a potential
direction for future research in MDS. Furthermore, the experimental results
indicate that the repetition problem in the generated summaries has
correlations with the high uncertainty scores.","[{'name': 'Congbo Ma'}, {'name': 'Wei Emma Zhang'}, {'name': 'Dileepa Pitawela'}, {'name': 'Haojie Zhuang'}, {'name': 'Yanfeng Shu'}]",2024-07-16T17:42:37Z
http://arxiv.org/abs/2407.11857v1,http://arxiv.org/abs/2407.11857v1,"Evaluating Task-Oriented Dialogue Consistency through Constraint
  Satisfaction","Task-oriented dialogues must maintain consistency both within the dialogue
itself, ensuring logical coherence across turns, and with the conversational
domain, accurately reflecting external knowledge. We propose to conceptualize
dialogue consistency as a Constraint Satisfaction Problem (CSP), wherein
variables represent segments of the dialogue referencing the conversational
domain, and constraints among variables reflect dialogue properties, including
linguistic, conversational, and domain-based aspects. To demonstrate the
feasibility of the approach, we utilize a CSP solver to detect inconsistencies
in dialogues re-lexicalized by an LLM. Our findings indicate that: (i) CSP is
effective to detect dialogue inconsistencies; and (ii) consistent dialogue
re-lexicalization is challenging for state-of-the-art LLMs, achieving only a
0.15 accuracy rate when compared to a CSP solver. Furthermore, through an
ablation study, we reveal that constraints derived from domain knowledge pose
the greatest difficulty in being respected. We argue that CSP captures core
properties of dialogue consistency that have been poorly considered by
approaches based on component pipelines.","[{'name': 'Tiziano Labruna'}, {'name': 'Bernardo Magnini'}]",2024-07-16T15:38:41Z
http://arxiv.org/abs/2407.11855v1,http://arxiv.org/abs/2407.11855v1,Scaling Sign Language Translation,"Sign language translation (SLT) addresses the problem of translating
information from a sign language in video to a spoken language in text.
Existing studies, while showing progress, are often limited to narrow domains
and/or few sign languages and struggle with open-domain tasks. In this paper,
we push forward the frontier of SLT by scaling pretraining data, model size,
and number of translation directions. We perform large-scale SLT pretraining on
different data including 1) noisy multilingual YouTube SLT data, 2) parallel
text corpora, and 3) SLT data augmented by translating video captions to other
languages with off-the-shelf machine translation models. We unify different
pretraining tasks with task-specific prompts under the encoder-decoder
architecture, and initialize the SLT model with pretrained (m/By)T5 models
across model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL
to 42 spoken languages) demonstrate the significance of data/model scaling and
cross-lingual cross-modal transfer, as well as the feasibility of zero-shot
SLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT
benchmarks covering 5 sign languages. Experiments show substantial quality
improvements over the vanilla baselines, surpassing the previous
state-of-the-art (SOTA) by wide margins.","[{'name': 'Biao Zhang'}, {'name': 'Garrett Tanzer'}, {'name': 'Orhan Firat'}]",2024-07-16T15:36:58Z
http://arxiv.org/abs/2407.11854v1,http://arxiv.org/abs/2407.11854v1,"Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in
  Grammatical Error Detection","Grammatical Error Detection (GED) methods rely heavily on human annotated
error corpora. However, these annotations are unavailable in many low-resource
languages. In this paper, we investigate GED in this context. Leveraging the
zero-shot cross-lingual transfer capabilities of multilingual pre-trained
language models, we train a model using data from a diverse set of languages to
generate synthetic errors in other languages. These synthetic error corpora are
then used to train a GED model. Specifically we propose a two-stage fine-tuning
pipeline where the GED model is first fine-tuned on multilingual synthetic data
from target languages followed by fine-tuning on human-annotated GED corpora
from source languages. This approach outperforms current state-of-the-art
annotation-free GED methods. We also analyse the errors produced by our method
and other strong baselines, finding that our approach produces errors that are
more diverse and more similar to human errors.","[{'name': 'Gaetan Lopez Latouche'}, {'name': 'Marc-André Carbonneau'}, {'name': 'Ben Swanson'}]",2024-07-16T15:35:15Z
http://arxiv.org/abs/2407.11843v1,http://arxiv.org/abs/2407.11843v1,"InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive
  Evaluation and Human Feedback","A crucial requirement for deploying LLM-based agents in real-life
applications is robustness against risky or irreversible mistakes. However,
existing research lacks a focus on the preemptive evaluation of reasoning
trajectories performed by LLM agents, leading to a gap in ensuring safe and
reliable operations. To explore better solutions, this paper introduces
InferAct, a novel approach that leverages the Theory-of-Mind capability of LLMs
to proactively detect potential errors before critical actions are executed
(e.g., ""buy-now"" in automatic online trading or web shopping). InferAct is also
capable of integrating human feedback to prevent irreversible risks and enhance
the actor agent's decision-making process. Experiments on three widely used
tasks demonstrate the effectiveness of InferAct. The proposed solution presents
a novel approach and concrete contributions toward developing LLM agents that
can be safely deployed in different environments involving critical
decision-making.","[{'name': 'Haishuo Fang'}, {'name': 'Xiaodan Zhu'}, {'name': 'Iryna Gurevych'}]",2024-07-16T15:24:44Z
http://arxiv.org/abs/2407.11833v1,http://arxiv.org/abs/2407.11833v1,LoFTI: Localization and Factuality Transfer to Indian Locales,"Large language models (LLMs) encode vast amounts of world knowledge acquired
via training on large web-scale datasets crawled from the internet. However,
these datasets typically exhibit a geographical bias towards English-speaking
Western countries. This results in LLMs producing biased or hallucinated
responses to queries that require answers localized to other geographical
regions. In this work, we introduce a new benchmark named LoFTI (Localization
and Factuality Transfer to Indian Locales) that can be used to evaluate an
LLM's localization and factual text transfer capabilities. LoFTI consists of
factual statements about entities in source and target locations; the source
locations are spread across the globe and the target locations are all within
India with varying degrees of hyperlocality (country, states, cities). The
entities span a wide variety of categories. We use LoFTI to evaluate Mixtral,
GPT-4 and two other Mixtral-based approaches well-suited to the task of
localized factual transfer. We demonstrate that LoFTI is a high-quality
evaluation benchmark and all the models, including GPT-4, produce skewed
results across varying levels of hyperlocality.","[{'name': 'Sona Elza Simon'}, {'name': 'Soumen Kumar Mondal'}, {'name': 'Abhishek Singhania'}, {'name': 'Sayambhu Sen'}, {'name': 'Preethi Jyothi'}]",2024-07-16T15:20:43Z
http://arxiv.org/abs/2407.11827v1,http://arxiv.org/abs/2407.11827v1,"GPT Assisted Annotation of Rhetorical and Linguistic Features for
  Interpretable Propaganda Technique Detection in News Text","While the use of machine learning for the detection of propaganda techniques
in text has garnered considerable attention, most approaches focus on
""black-box"" solutions with opaque inner workings. Interpretable approaches
provide a solution, however, they depend on careful feature engineering and
costly expert annotated data. Additionally, language features specific to
propagandistic text are generally the focus of rhetoricians or linguists, and
there is no data set labeled with such features suitable for machine learning.
This study codifies 22 rhetorical and linguistic features identified in
literature related to the language of persuasion for the purpose of annotating
an existing data set labeled with propaganda techniques. To help human experts
annotate natural language sentences with these features, RhetAnn, a web
application, was specifically designed to minimize an otherwise considerable
mental effort. Finally, a small set of annotated data was used to fine-tune
GPT-3.5, a generative large language model (LLM), to annotate the remaining
data while optimizing for financial cost and classification accuracy. This
study demonstrates how combining a small number of human annotated examples
with GPT can be an effective strategy for scaling the annotation process at a
fraction of the cost of traditional annotation relying solely on human experts.
The results are on par with the best performing model at the time of writing,
namely GPT-4, at 10x less the cost. Our contribution is a set of features,
their properties, definitions, and examples in a machine-readable format, along
with the code for RhetAnn and the GPT prompts and fine-tuning procedures for
advancing state-of-the-art interpretable propaganda technique detection.","[{'name': 'Kyle Hamilton'}, {'name': 'Luca Longo'}, {'name': 'Bojan Bozic'}]",2024-07-16T15:15:39Z
http://arxiv.org/abs/2407.12881v1,http://arxiv.org/abs/2407.12881v1,BinaryAlign: Word Alignment as Binary Sequence Labeling,"Real world deployments of word alignment are almost certain to cover both
high and low resource languages. However, the state-of-the-art for this task
recommends a different model class depending on the availability of gold
alignment training data for a particular language pair. We propose BinaryAlign,
a novel word alignment technique based on binary sequence labeling that
outperforms existing approaches in both scenarios, offering a unifying approach
to the task. Additionally, we vary the specific choice of multilingual
foundation model, perform stratified error analysis over alignment error type,
and explore the performance of BinaryAlign on non-English language pairs. We
make our source code publicly available.","[{'name': 'Gaetan Lopez Latouche'}, {'name': 'Marc-André Carbonneau'}, {'name': 'Ben Swanson'}]",2024-07-16T15:11:06Z
http://arxiv.org/abs/2407.11798v1,http://arxiv.org/abs/2407.11798v1,"PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined
  Speculation","Inference of Large Language Models (LLMs) across computer clusters has become
a focal point of research in recent times, with many acceleration techniques
taking inspiration from CPU speculative execution. These techniques reduce
bottlenecks associated with memory bandwidth, but also increase end-to-end
latency per inference run, requiring high speculation acceptance rates to
improve performance. Combined with a variable rate of acceptance across tasks,
speculative inference techniques can result in reduced performance.
Additionally, pipeline-parallel designs require many user requests to maintain
maximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative
acceleration technique to reduce inter-token latency and improve system
utilization for single-request scenarios while also improving tolerance to low
speculation acceptance rates and low-bandwidth interconnects. PipeInfer
exhibits up to a 2.15$\times$ improvement in generation speed over standard
speculative inference. PipeInfer achieves its improvement through Continuous
Asynchronous Speculation and Early Inference Cancellation, the former improving
latency and generation speed by running single-token inference simultaneously
with several speculative runs, while the latter improves speed and latency by
skipping the computation of invalidated runs, even in the middle of inference.","[{'name': 'Branden Butler'}, {'name': 'Sixing Yu'}, {'name': 'Arya Mazaheri'}, {'name': 'Ali Jannesari'}]",2024-07-16T14:52:02Z
http://arxiv.org/abs/2407.11789v1,http://arxiv.org/abs/2407.11789v1,Large Language Models as Misleading Assistants in Conversation,"Large Language Models (LLMs) are able to provide assistance on a wide range
of information-seeking tasks. However, model outputs may be misleading, whether
unintentionally or in cases of intentional deception. We investigate the
ability of LLMs to be deceptive in the context of providing assistance on a
reading comprehension task, using LLMs as proxies for human users. We compare
outcomes of (1) when the model is prompted to provide truthful assistance, (2)
when it is prompted to be subtly misleading, and (3) when it is prompted to
argue for an incorrect answer. Our experiments show that GPT-4 can effectively
mislead both GPT-3.5-Turbo and GPT-4, with deceptive assistants resulting in up
to a 23% drop in accuracy on the task compared to when a truthful assistant is
used. We also find that providing the user model with additional context from
the passage partially mitigates the influence of the deceptive model. This work
highlights the ability of LLMs to produce misleading information and the
effects this may have in real-world situations.","[{'name': 'Betty Li Hou'}, {'name': 'Kejian Shi'}, {'name': 'Jason Phang'}, {'name': 'James Aung'}, {'name': 'Steven Adler'}, {'name': 'Rosie Campbell'}]",2024-07-16T14:45:22Z
http://arxiv.org/abs/2407.11780v1,http://arxiv.org/abs/2407.11780v1,"SwitchCIT: Switching for Continual Instruction Tuning of Large Language
  Models","Large language models (LLMs) have exhibited impressive capabilities in
various domains, particularly in general language understanding. However these
models, trained on massive text data, may not be finely optimized for specific
tasks triggered by instructions. Continual instruction tuning is crucial to
adapt LLMs to evolving tasks and domains, ensuring their effectiveness and
relevance across a wide range of applications. In the context of continual
instruction tuning, where models are sequentially trained on different tasks,
catastrophic forgetting can occur, leading to performance degradation on
previously learned tasks. This work addresses the catastrophic forgetting in
continual instruction learning for LLMs through a switching mechanism for
routing computations to parameter-efficient tuned models. We demonstrate the
effectiveness of our method through experiments on continual instruction tuning
of different natural language generation tasks.","[{'name': 'Xinbo Wu'}, {'name': 'Max Hartman'}, {'name': 'Vidhata Arjun Jayaraman'}, {'name': 'Lav R. Varshney'}]",2024-07-16T14:37:33Z
http://arxiv.org/abs/2407.11774v1,http://arxiv.org/abs/2407.11774v1,"Sharif-MGTD at SemEval-2024 Task 8: A Transformer-Based Approach to
  Detect Machine Generated Text","Detecting Machine-Generated Text (MGT) has emerged as a significant area of
study within Natural Language Processing. While language models generate text,
they often leave discernible traces, which can be scrutinized using either
traditional feature-based methods or more advanced neural language models. In
this research, we explore the effectiveness of fine-tuning a RoBERTa-base
transformer, a powerful neural architecture, to address MGT detection as a
binary classification task. Focusing specifically on Subtask A
(Monolingual-English) within the SemEval-2024 competition framework, our
proposed system achieves an accuracy of 78.9% on the test dataset, positioning
us at 57th among participants. Our study addresses this challenge while
considering the limited hardware resources, resulting in a system that excels
at identifying human-written texts but encounters challenges in accurately
discerning MGTs.","[{'name': 'Seyedeh Fatemeh Ebrahimi'}, {'name': 'Karim Akhavan Azari'}, {'name': 'Amirmasoud Iravani'}, {'name': 'Arian Qazvini'}, {'name': 'Pouya Sadeghi'}, {'name': 'Zeinab Sadat Taghavi'}, {'name': 'Hossein Sameti'}]",2024-07-16T14:33:01Z
http://arxiv.org/abs/2407.11773v1,http://arxiv.org/abs/2407.11773v1,"Educational Personalized Learning Path Planning with Large Language
  Models","Educational Personalized Learning Path Planning (PLPP) aims to tailor
learning experiences to individual learners' needs, enhancing learning
efficiency and engagement. Despite its potential, traditional PLPP systems
often lack adaptability, interactivity, and transparency. This paper proposes a
novel approach integrating Large Language Models (LLMs) with prompt engineering
to address these challenges. By designing prompts that incorporate
learner-specific information, our method guides LLMs like LLama-2-70B and GPT-4
to generate personalized, coherent, and pedagogically sound learning paths. We
conducted experiments comparing our method with a baseline approach across
various metrics, including accuracy, user satisfaction, and the quality of
learning paths. The results show significant improvements in all areas,
particularly with GPT-4, demonstrating the effectiveness of prompt engineering
in enhancing PLPP. Additional long-term impact analysis further validates our
method's potential to improve learner performance and retention. This research
highlights the promise of LLMs and prompt engineering in advancing personalized
education.","[{'name': 'Chee Ng'}, {'name': 'Yuen Fung'}]",2024-07-16T14:32:56Z
http://arxiv.org/abs/2407.11770v1,http://arxiv.org/abs/2407.11770v1,"Robust Utility-Preserving Text Anonymization Based on Large Language
  Models","Text anonymization is crucial for sharing sensitive data while maintaining
privacy. Existing techniques face the emerging challenges of re-identification
attack ability of Large Language Models (LLMs), which have shown advanced
capability in memorizing detailed information and patterns as well as
connecting disparate pieces of information. In defending against LLM-based
re-identification attacks, anonymization could jeopardize the utility of the
resulting anonymized data in downstream tasks -- the trade-off between privacy
and data utility requires deeper understanding within the context of LLMs. This
paper proposes a framework composed of three LLM-based components -- a privacy
evaluator, a utility evaluator, and an optimization component, which work
collaboratively to perform anonymization. To provide a practical model for
large-scale and real-time environments, we distill the anonymization
capabilities into a lightweight model using Direct Preference Optimization
(DPO). Extensive experiments demonstrate that the proposed models outperform
baseline models, showing robustness in reducing the risk of re-identification
while preserving greater data utility in downstream tasks. Our code and dataset
are available at https://github.com/UKPLab/arxiv2024-rupta.","[{'name': 'Tianyu Yang'}, {'name': 'Xiaodan Zhu'}, {'name': 'Iryna Gurevych'}]",2024-07-16T14:28:56Z
http://arxiv.org/abs/2407.11766v1,http://arxiv.org/abs/2407.11766v1,Vectoring Languages,"Recent breakthroughs in large language models (LLM) have stirred up global
attention, and the research has been accelerating non-stop since then.
Philosophers and psychologists have also been researching the structure of
language for decades, but they are having a hard time finding a theory that
directly benefits from the breakthroughs of LLMs. In this article, we propose a
novel structure of language that reflects well on the mechanisms behind
language models and go on to show that this structure is also better at
capturing the diverse nature of language compared to previous methods. An
analogy of linear algebra is adapted to strengthen the basis of this
perspective. We further argue about the difference between this perspective and
the design philosophy for current language models. Lastly, we discuss how this
perspective can lead us to research directions that may accelerate the
improvements of science fastest.",[{'name': 'Joseph Chen'}],2024-07-16T14:25:55Z
http://arxiv.org/abs/2407.11733v2,http://arxiv.org/abs/2407.11733v2,"How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine
  Studies","With the widespread availability of LLMs since the release of ChatGPT and
increased public scrutiny, commercial model development appears to have focused
their efforts on 'safety' training concerning legal liabilities at the expense
of social impact evaluation. This mimics a similar trend which we could observe
for search engine autocompletion some years prior. We draw on scholarship from
NLP and search engine auditing and present a novel evaluation task in the style
of autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by
using four metrics, namely refusal rates, toxicity, sentiment and regard, with
and without safety system prompts. Our findings indicate an improvement to
stereotyping outputs with the system prompt, but overall a lack of attention by
LLMs under study to certain harms classified as toxic, particularly for prompts
about peoples/ethnicities and sexual orientation. Mentions of intersectional
identities trigger a disproportionate amount of stereotyping. Finally, we
discuss the implications of these findings about stereotyping harms in light of
the coming intermingling of LLMs and search and the choice of stereotyping
mitigation policy to adopt. We address model builders, academics, NLP
practitioners and policy makers, calling for accountability and awareness
concerning stereotyping harms, be it for training data curation, leader board
design and usage, or social impact measurement.","[{'name': 'Alina Leidinger'}, {'name': 'Richard Rogers'}]",2024-07-16T14:04:35Z
http://arxiv.org/abs/2407.11686v3,http://arxiv.org/abs/2407.11686v3,CCoE: A Compact LLM with Collaboration of Experts,"In the domain of Large Language Model (LLM), LLMs demonstrate significant
capabilities in natural language understanding and generation. With the growing
needs of applying LLMs on various domains, it is a research question that how
to efficiently train and build a model that has expertise in different domains
but with a low training cost. We propose CCoE architecture, a framework of
easily coupling multiple strong domain experts together to fuse into a big LLM,
provides a collective way of utilizing the different domain expert LLMs.
Besides, training a large collaborative of multiple expert LLMs requires a high
requirements on training sources. CCoE bypasses this problem through isolating
other experts and train each expert separately. The design of CCoE assembles
multiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoE
layer could have one or more expert LLMs. Expert LLMs have different number of
layers and have been well-trained for different domain tasks. Each expert is
fine-tuned to be able to achieve the comparable results with SOTA domain LLMs.
We start from 5 experts in the domain of Code, Math, Law, text-to-SQL and
Medical. The results indicate that our CCoE framework can easily and
efficiently boost nearly 10%-20% performance on original base model in
different domains but using less resources on training, as well as inference.","[{'name': 'Shaomang Huang'}, {'name': 'Jianfeng Pan'}, {'name': 'Hanzhong Zheng'}]",2024-07-16T13:03:58Z
http://arxiv.org/abs/2407.11681v1,http://arxiv.org/abs/2407.11681v1,MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models,"As Large Language Models (LLMs) grow dramatically in size, there is an
increasing trend in compressing and speeding up these models. Previous studies
have highlighted the usefulness of gradients for importance scoring in neural
network compressing, especially in pruning medium-size networks. However, the
substantial memory requirements involved in calculating gradients with
backpropagation impede the utilization of gradients in guiding LLM pruning. As
a result, most pruning strategies for LLMs rely on gradient-free criteria, such
as weight magnitudes or a mix of magnitudes and activations. In this paper, we
devise a hybrid pruning criterion, which appropriately integrates magnitude,
activation, and gradient to capitalize on feature map sensitivity for pruning
LLMs. To overcome memory requirement barriers, we estimate gradients using only
forward passes. Based on this, we propose a Memory-effIcieNt structured prunIng
procedure for LLMs (MINI-LLM) to remove no-critical channels and
multi-attention heads. Experimental results demonstrate the superior
performance of MINI-LLM over existing gradient-free methods on three LLMs:
LLaMA, BLOOM, and OPT across various downstream tasks (classification,
multiple-choice, and generation), while MINI-LLM maintains a GPU memory
footprint akin to gradient-free methods.","[{'name': 'Hongrong Cheng'}, {'name': 'Miao Zhang'}, {'name': 'Javen Qinfeng Shi'}]",2024-07-16T12:59:44Z
http://arxiv.org/abs/2407.11660v1,http://arxiv.org/abs/2407.11660v1,ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues,"Despite being heralded as the new standard for dialogue evaluation, the
closed-source nature of GPT-4 poses challenges for the community. Motivated by
the need for lightweight, open source, and multilingual dialogue evaluators,
this paper introduces GenResCoh (Generated Responses targeting Coherence).
GenResCoh is a novel LLM generated dataset comprising over 130k negative and
positive responses and accompanying explanations seeded from XDailyDialog and
XPersona covering English, French, German, Italian, and Chinese. Leveraging
GenResCoh, we propose ECoh (Evaluation of Coherence), a family of evaluators
trained to assess response coherence across multiple languages. Experimental
results demonstrate that ECoh achieves multilingual detection capabilities
superior to the teacher model (GPT-3.5-Turbo) on GenResCoh, despite being based
on a much smaller architecture. Furthermore, the explanations provided by ECoh
closely align in terms of quality with those generated by the teacher model.","[{'name': 'John Mendonça'}, {'name': 'Isabel Trancoso'}, {'name': 'Alon Lavie'}]",2024-07-16T12:28:30Z
http://arxiv.org/abs/2407.11638v1,http://arxiv.org/abs/2407.11638v1,"A Comprehensive Evaluation of Large Language Models on Temporal Event
  Forecasting","Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation(RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast,
incorporating raw texts in specific complex events and fine-tuning LLMs
significantly improves performance. Moreover, enhanced with retrieval modules,
LLM can effectively capture temporal relational patterns hidden in historical
events. Meanwhile, issues such as popularity bias and the long-tail problem
still persist in LLMs, particularly in the RAG-based method. These findings not
only deepen our understanding of LLM-based event forecasting methods but also
highlight several promising research directions.We consider that this
comprehensive evaluation, along with the identified research opportunities,
will significantly contribute to future research on temporal event forecasting
through LLMs.","[{'name': 'He Chang'}, {'name': 'Chenchen Ye'}, {'name': 'Zhulin Tao'}, {'name': 'Jie Wu'}, {'name': 'Zhengmao Yang'}, {'name': 'Yunshan Ma'}, {'name': 'Xianglin Huang'}, {'name': 'Tat-Seng Chua'}]",2024-07-16T11:58:54Z
http://arxiv.org/abs/2407.11606v2,http://arxiv.org/abs/2407.11606v2,The Foundations of Tokenization: Statistical and Computational Concerns,"Tokenization - the practice of converting strings of characters over an
alphabet into sequences of tokens over a vocabulary - is a critical yet
under-theorized step in the NLP pipeline. Notably, it remains the only major
step not fully integrated into widely used end-to-end neural models. This paper
aims to address this theoretical gap by laying the foundations of tokenization
from a formal perspective. By articulating and extending basic properties about
the category of stochastic maps, we propose a unified framework for
representing and analyzing tokenizer models. This framework allows us to
establish general conditions for the use of tokenizers. In particular, we
formally establish the necessary and sufficient conditions for a tokenizer
model to preserve the consistency of statistical estimators. Additionally, we
discuss statistical and computational concerns crucial for the design and
implementation of tokenizer models. The framework and results advanced in this
paper represent a step toward a robust theoretical foundation for neural
language modeling.","[{'name': 'Juan Luis Gastaldi'}, {'name': 'John Terilla'}, {'name': 'Luca Malagutti'}, {'name': 'Brian DuSell'}, {'name': 'Tim Vieira'}, {'name': 'Ryan Cotterell'}]",2024-07-16T11:12:28Z
http://arxiv.org/abs/2407.11591v2,http://arxiv.org/abs/2407.11591v2,"AdaptEval: Evaluating Large Language Models on Domain Adaptation for
  Text Summarization","Despite the advances in the abstractive summarization task using Large
Language Models (LLM), there is a lack of research that asses their abilities
to easily adapt to different domains. We evaluate the domain adaptation
abilities of a wide range of LLMs on the summarization task across various
domains in both fine-tuning and in-context learning settings. We also present
AdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a
domain benchmark and a set of metrics to facilitate the analysis of domain
adaptation. Our results demonstrate that LLMs exhibit comparable performance in
the in-context learning setting, regardless of their parameter scale.","[{'name': 'Anum Afzal'}, {'name': 'Ribin Chalumattu'}, {'name': 'Florian Matthes'}, {'name': 'Laura Mascarell'}]",2024-07-16T10:50:39Z
http://arxiv.org/abs/2407.11550v3,http://arxiv.org/abs/2407.11550v3,"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for
  Efficient LLM Inference","Large Language Models have excelled in various fields but encounter
challenges in memory and time efficiency due to the expanding Key-Value (KV)
cache required for long-sequence inference. Recent efforts try to reduce KV
cache size to a given memory budget by evicting vast non-critical cache
elements during runtime, while preserving generation quality. Our revisiting of
current eviction methods reveals that they fundamentally minimize an upper
bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of
multi-head self-attention mechanisms. Moreover, our analysis indicates that the
common practices of uniformly assigning budgets across attention heads harm
their post-eviction generation quality. In light of these findings, we propose
a simple yet effective adaptive budget allocation algorithm. This algorithm not
only optimizes the theoretical loss upper bound but also reduces the $L_1$
eviction loss in practice by aligning with the varied characteristics across
different heads. By integrating this algorithm into two state-of-the-art
methods, we demonstrate the effectiveness of using adaptive budget allocation
to optimize KV cache eviction. Extensive evaluations on 16 datasets and the
Needle-in-a-Haystack test confirm significant performance improvements across
various tasks.","[{'name': 'Yuan Feng'}, {'name': 'Junlin Lv'}, {'name': 'Yukun Cao'}, {'name': 'Xike Xie'}, {'name': 'S. Kevin Zhou'}]",2024-07-16T09:53:32Z
http://arxiv.org/abs/2407.12880v1,http://arxiv.org/abs/2407.12880v1,Cross-Modal Augmentation for Few-Shot Multimodal Fake News Detection,"The nascent topic of fake news requires automatic detection methods to
quickly learn from limited annotated samples. Therefore, the capacity to
rapidly acquire proficiency in a new task with limited guidance, also known as
few-shot learning, is critical for detecting fake news in its early stages.
Existing approaches either involve fine-tuning pre-trained language models
which come with a large number of parameters, or training a complex neural
network from scratch with large-scale annotated datasets. This paper presents a
multimodal fake news detection model which augments multimodal features using
unimodal features. For this purpose, we introduce Cross-Modal Augmentation
(CMA), a simple approach for enhancing few-shot multimodal fake news detection
by transforming n-shot classification into a more robust (n $\times$ z)-shot
problem, where z represents the number of supplementary features. The proposed
CMA achieves SOTA results over three benchmark datasets, utilizing a
surprisingly simple linear probing method to classify multimodal fake news with
only a few training samples. Furthermore, our method is significantly more
lightweight than prior approaches, particularly in terms of the number of
trainable parameters and epoch times. The code is available here:
\url{https://github.com/zgjiangtoby/FND_fewshot}","[{'name': 'Ye Jiang'}, {'name': 'Taihang Wang'}, {'name': 'Xiaoman Xu'}, {'name': 'Yimin Wang'}, {'name': 'Xingyi Song'}, {'name': 'Diana Maynard'}]",2024-07-16T09:32:11Z
http://arxiv.org/abs/2407.12879v1,http://arxiv.org/abs/2407.12879v1,"Large Visual-Language Models Are Also Good Classifiers: A Study of
  In-Context Multimodal Fake News Detection","Large visual-language models (LVLMs) exhibit exceptional performance in
visual-language reasoning across diverse cross-modal benchmarks. Despite these
advances, recent research indicates that Large Language Models (LLMs), like
GPT-3.5-turbo, underachieve compared to well-trained smaller models, such as
BERT, in Fake News Detection (FND), prompting inquiries into LVLMs' efficacy in
FND tasks. Although performance could improve through fine-tuning LVLMs, the
substantial parameters and requisite pre-trained weights render it a
resource-heavy endeavor for FND applications. This paper initially assesses the
FND capabilities of two notable LVLMs, CogVLM and GPT4V, in comparison to a
smaller yet adeptly trained CLIP model in a zero-shot context. The findings
demonstrate that LVLMs can attain performance competitive with that of the
smaller model. Next, we integrate standard in-context learning (ICL) with
LVLMs, noting improvements in FND performance, though limited in scope and
consistency. To address this, we introduce the \textbf{I}n-context
\textbf{M}ultimodal \textbf{F}ake \textbf{N}ews \textbf{D}etection (IMFND)
framework, enriching in-context examples and test inputs with predictions and
corresponding probabilities from a well-trained smaller model. This strategic
integration directs the LVLMs' focus towards news segments associated with
higher probabilities, thereby improving their analytical accuracy. The
experimental results suggest that the IMFND framework significantly boosts the
FND efficiency of LVLMs, achieving enhanced accuracy over the standard ICL
approach across three publicly available FND datasets.","[{'name': 'Ye Jiang'}, {'name': 'Yimin Wang'}]",2024-07-16T09:28:23Z
http://arxiv.org/abs/2407.11492v1,http://arxiv.org/abs/2407.11492v1,MMSD-Net: Towards Multi-modal Stuttering Detection,"Stuttering is a common speech impediment that is caused by irregular
disruptions in speech production, affecting over 70 million people across the
world. Standard automatic speech processing tools do not take speech ailments
into account and are thereby not able to generate meaningful results when
presented with stuttered speech as input. The automatic detection of stuttering
is an integral step towards building efficient, context-aware speech processing
systems. While previous approaches explore both statistical and neural
approaches for stuttering detection, all of these methods are uni-modal in
nature. This paper presents MMSD-Net, the first multi-modal neural framework
for stuttering detection. Experiments and results demonstrate that
incorporating the visual signal significantly aids stuttering detection, and
our model yields an improvement of 2-17% in the F1-score over existing
state-of-the-art uni-modal approaches.","[{'name': 'Liangyu Nie'}, {'name': 'Sudarsana Reddy Kadiri'}, {'name': 'Ruchit Agrawal'}]",2024-07-16T08:26:59Z
http://arxiv.org/abs/2407.12877v1,http://arxiv.org/abs/2407.12877v1,"Review-Feedback-Reason (ReFeR): A Novel Framework for NLG Evaluation and
  Reasoning","Assessing the quality of Natural Language Generation (NLG) outputs, such as
those produced by large language models (LLMs), poses significant challenges.
Traditional approaches involve either resource-intensive human evaluations or
automatic metrics, which often exhibit a low correlation with human judgment.
In this study, we propose Review-Feedback-Reason (ReFeR), a novel evaluation
framework for NLG using LLM agents. We rigorously test ReFeR using two
pre-existing benchmark datasets on diverse NLG tasks. The proposed framework
not only enhances the accuracy of NLG evaluation, surpassing previous
benchmarks by $\sim$20\%, but also generates constructive feedback and
significantly improves collective reasoning. This feedback is then leveraged
for the creation of instruction-tuning datasets, which, when used to fine-tune
smaller models like Mistral-7B, makes them extremely good evaluators, yielding
a better correlation with human evaluations and performance nearly on par with
GPT-3.5. We highlight the effectiveness of our methodology through its
application on three reasoning benchmarks, where it outperforms most of the
state-of-the-art methods, and also outperforms the reasoning capabilities of
models like GPT-3.5 Turbo by $\sim$11.67\% and GPT-4 by $\sim$1\% on an
average.","[{'name': 'Yaswanth Narsupalli'}, {'name': 'Abhranil Chandra'}, {'name': 'Sreevatsa Muppirala'}, {'name': 'Manish Gupta'}, {'name': 'Pawan Goyal'}]",2024-07-16T08:25:26Z
http://arxiv.org/abs/2407.11485v1,http://arxiv.org/abs/2407.11485v1,Scientific QA System with Verifiable Answers,"In this paper, we introduce the VerifAI project, a pioneering open-source
scientific question-answering system, designed to provide answers that are not
only referenced but also automatically vetted and verifiable. The components of
the system are (1) an Information Retrieval system combining semantic and
lexical search techniques over scientific papers (PubMed), (2) a
Retrieval-Augmented Generation (RAG) module using fine-tuned generative model
(Mistral 7B) and retrieved articles to generate claims with references to the
articles from which it was derived, and (3) a Verification engine, based on a
fine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task
using SciFACT dataset. The verification engine cross-checks the generated claim
and the article from which the claim was derived, verifying whether there may
have been any hallucinations in generating the claim. By leveraging the
Information Retrieval and RAG modules, Verif.ai excels in generating factual
information from a vast array of scientific sources. At the same time, the
Verification engine rigorously double-checks this output, ensuring its accuracy
and reliability. This dual-stage process plays a crucial role in acquiring and
confirming factual information, significantly enhancing the information
landscape. Our methodology could significantly enhance scientists'
productivity, concurrently fostering trust in applying generative language
models within scientific domains, where hallucinations and misinformation are
unacceptable.","[{'name': 'Adela Ljajić'}, {'name': 'Miloš Košprdić'}, {'name': 'Bojana Bašaragin'}, {'name': 'Darija Medvecki'}, {'name': 'Lorenzo Cassano'}, {'name': 'Nikola Milošević'}]",2024-07-16T08:21:02Z
http://arxiv.org/abs/2407.11484v4,http://arxiv.org/abs/2407.11484v4,The Oscars of AI Theater: A Survey on Role-Playing with Language Models,"This survey explores the burgeoning field of role-playing with language
models, focusing on their development from early persona-based models to
advanced character-driven simulations facilitated by Large Language Models
(LLMs). Initially confined to simple persona consistency due to limited model
capabilities, role-playing tasks have now expanded to embrace complex character
portrayals involving character consistency, behavioral alignment, and overall
attractiveness. We provide a comprehensive taxonomy of the critical components
in designing these systems, including data, models and alignment, agent
architecture and evaluation. This survey not only outlines the current
methodologies and challenges, such as managing dynamic personal profiles and
achieving high-level persona consistency but also suggests avenues for future
research in improving the depth and realism of role-playing applications. The
goal is to guide future research by offering a structured overview of current
methodologies and identifying potential areas for improvement. Related
resources and papers are available at
https://github.com/nuochenpku/Awesome-Role-Play-Papers.","[{'name': 'Nuo Chen'}, {'name': 'Yang Deng'}, {'name': 'Jia Li'}]",2024-07-16T08:20:39Z
http://arxiv.org/abs/2407.11470v1,http://arxiv.org/abs/2407.11470v1,"Beyond Correctness: Benchmarking Multi-dimensional Code Generation for
  Large Language Models","In recent years, researchers have proposed numerous benchmarks to evaluate
the impressive coding capabilities of large language models (LLMs). However,
existing benchmarks primarily focus on assessing the correctness of code
generated by LLMs, while neglecting other critical dimensions that also
significantly impact code quality. Therefore, this paper proposes the RACE
benchmark, which comprehensively evaluates the quality of code generated by
LLMs across 4 dimensions: Readability, mAintainability, Correctness, and
Efficiency. Specifically, considering the demand-dependent nature of dimensions
beyond correctness, we design various types of user requirements for each
dimension to assess the model's ability to generate correct code that also
meets user demands. We evaluate 18 representative LLMs on RACE and find that:
1) the current LLMs' ability to generate high-quality code on demand does not
yet meet the requirements of software development; 2) readability serves as a
critical indicator of the overall quality of generated code; 3) most LLMs
exhibit an inherent preference for specific coding style. These findings can
help researchers gain a deeper understanding of the coding capabilities of
current LLMs and shed light on future directions for model improvement.","[{'name': 'Jiasheng Zheng'}, {'name': 'Boxi Cao'}, {'name': 'Zhengzhao Ma'}, {'name': 'Ruotong Pan'}, {'name': 'Hongyu Lin'}, {'name': 'Yaojie Lu'}, {'name': 'Xianpei Han'}, {'name': 'Le Sun'}]",2024-07-16T08:08:48Z
http://arxiv.org/abs/2407.11438v2,http://arxiv.org/abs/2407.11438v2,"Trust No Bot: Discovering Personal Disclosures in Human-LLM
  Conversations in the Wild","Measuring personal disclosures made in human-chatbot interactions can provide
a better understanding of users' AI literacy and facilitate privacy research
for large language models (LLMs). We run an extensive, fine-grained analysis on
the personal disclosures made by real users to commercial GPT models,
investigating the leakage of personally identifiable and sensitive information.
To understand the contexts in which users disclose to chatbots, we develop a
taxonomy of tasks and sensitive topics, based on qualitative and quantitative
analysis of naturally occurring conversations. We discuss these potential
privacy harms and observe that: (1) personally identifiable information (PII)
appears in unexpected contexts such as in translation or code editing (48% and
16% of the time, respectively) and (2) PII detection alone is insufficient to
capture the sensitive topics that are common in human-chatbot interactions,
such as detailed sexual preferences or specific drug use habits. We believe
that these high disclosure rates are of significant importance for researchers
and data curators, and we call for the design of appropriate nudging mechanisms
to help users moderate their interactions.","[{'name': 'Niloofar Mireshghallah'}, {'name': 'Maria Antoniak'}, {'name': 'Yash More'}, {'name': 'Yejin Choi'}, {'name': 'Golnoosh Farnadi'}]",2024-07-16T07:05:31Z
http://arxiv.org/abs/2407.11417v1,http://arxiv.org/abs/2407.11417v1,"SPINACH: SPARQL-Based Information Navigation for Challenging Real-World
  Questions","Recent work integrating Large Language Models (LLMs) has led to significant
improvements in the Knowledge Base Question Answering (KBQA) task. However, we
posit that existing KBQA datasets that either have simple questions, use
synthetically generated logical forms, or are based on small knowledge base
(KB) schemas, do not capture the true complexity of KBQA tasks.
  To address this, we introduce the SPINACH dataset, an expert-annotated KBQA
dataset collected from forum discussions on Wikidata's ""Request a Query"" forum
with 320 decontextualized question-SPARQL pairs. Much more complex than
existing datasets, SPINACH calls for strong KBQA systems that do not rely on
training data to learn the KB schema, but can dynamically explore large and
often incomplete schemas and reason about them.
  Along with the dataset, we introduce the SPINACH agent, a new KBQA approach
that mimics how a human expert would write SPARQLs for such challenging
questions. Experiments on existing datasets show SPINACH's capability in KBQA,
achieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10
datasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6%
of the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH
dataset, SPINACH agent outperforms all baselines, including the best
GPT-4-based KBQA agent, by 38.1% in F1.","[{'name': 'Shicheng Liu'}, {'name': 'Sina J. Semnani'}, {'name': 'Harold Triedman'}, {'name': 'Jialiang Xu'}, {'name': 'Isaac Dan Zhao'}, {'name': 'Monica S. Lam'}]",2024-07-16T06:18:21Z
http://arxiv.org/abs/2407.12876v2,http://arxiv.org/abs/2407.12876v2,Exploring the Use of Abusive Generative AI Models on Civitai,"The rise of generative AI is transforming the landscape of digital imagery,
and exerting a significant influence on online creative communities. This has
led to the emergence of AI-Generated Content (AIGC) social platforms, such as
Civitai. These distinctive social platforms allow users to build and share
their own generative AI models, thereby enhancing the potential for more
diverse artistic expression. Designed in the vein of social networks, they also
provide artists with the means to showcase their creations (generated from the
models), engage in discussions, and obtain feedback, thus nurturing a sense of
community. Yet, this openness also raises concerns about the abuse of such
platforms, e.g., using models to disseminate deceptive deepfakes or infringe
upon copyrights. To explore this, we conduct the first comprehensive empirical
study of an AIGC social platform, focusing on its use for generating abusive
content. As an exemplar, we construct a comprehensive dataset covering Civitai,
the largest available AIGC social platform. Based on this dataset of 87K models
and 2M images, we explore the characteristics of content and discuss strategies
for moderation to better govern these platforms.","[{'name': 'Yiluo Wei'}, {'name': 'Yiming Zhu'}, {'name': 'Pan Hui'}, {'name': 'Gareth Tyson'}]",2024-07-16T06:18:03Z
http://arxiv.org/abs/2407.11409v1,http://arxiv.org/abs/2407.11409v1,"Representation Bias in Political Sample Simulations with Large Language
  Models","This study seeks to identify and quantify biases in simulating political
samples with Large Language Models, specifically focusing on vote choice and
public opinion. Using the GPT-3.5-Turbo model, we leverage data from the
American National Election Studies, German Longitudinal Election Study, Zuobiao
Dataset, and China Family Panel Studies to simulate voting behaviors and public
opinions. This methodology enables us to examine three types of representation
bias: disparities based on the the country's language, demographic groups, and
political regime types. The findings reveal that simulation performance is
generally better for vote choice than for public opinions, more accurate in
English-speaking countries, more effective in bipartisan systems than in
multi-partisan systems, and stronger in democratic settings than in
authoritarian regimes. These results contribute to enhancing our understanding
and developing strategies to mitigate biases in AI applications within the
field of computational social science.","[{'name': 'Weihong Qi'}, {'name': 'Hanjia Lyu'}, {'name': 'Jiebo Luo'}]",2024-07-16T05:52:26Z
http://arxiv.org/abs/2407.11406v1,http://arxiv.org/abs/2407.11406v1,Revisiting the Impact of Pursuing Modularity for Code Generation,"Modular programming, which aims to construct the final program by integrating
smaller, independent building blocks, has been regarded as a desirable practice
in software development. However, with the rise of recent code generation
agents built upon large language models (LLMs), a question emerges: is this
traditional practice equally effective for these new tools? In this work, we
assess the impact of modularity in code generation by introducing a novel
metric for its quantitative measurement. Surprisingly, unlike conventional
wisdom on the topic, we find that modularity is not a core factor for improving
the performance of code generation models. We also explore potential
explanations for why LLMs do not exhibit a preference for modular code compared
to non-modular code.","[{'name': 'Deokyeong Kang'}, {'name': 'Ki Jung Seo'}, {'name': 'Taeuk Kim'}]",2024-07-16T05:48:24Z
http://arxiv.org/abs/2407.11393v2,http://arxiv.org/abs/2407.11393v2,"CIC-BART-SSA: Controllable Image Captioning with Structured Semantic
  Augmentation","Controllable Image Captioning (CIC) aims at generating natural language
descriptions for an image, conditioned on information provided by end users,
e.g., regions, entities or events of interest. However, available
image-language datasets mainly contain captions that describe the entirety of
an image, making them ineffective for training CIC models that can potentially
attend to any subset of regions or relationships. To tackle this challenge, we
propose a novel, fully automatic method to sample additional focused and
visually grounded captions using a unified structured semantic representation
built on top of the existing set of captions associated with an image. We
leverage Abstract Meaning Representation (AMR), a cross-lingual graph-based
semantic formalism, to encode all possible spatio-semantic relations between
entities, beyond the typical spatial-relations-only focus of current methods.
We use this Structured Semantic Augmentation (SSA) framework to augment
existing image-caption datasets with the grounded controlled captions,
increasing their spatial and semantic diversity and focal coverage. We then
develop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that
sources its control signals from SSA-diversified datasets. We empirically show
that, compared to SOTA CIC models, CIC-BART-SSA generates captions that are
superior in diversity and text quality, are competitive in controllability,
and, importantly, minimize the gap between broad and highly focused controlled
captioning performance by efficiently generalizing to the challenging highly
focused scenarios. Code is available at
https://github.com/SamsungLabs/CIC-BART-SSA.","[{'name': 'Kalliopi Basioti'}, {'name': 'Mohamed A. Abdelsalam'}, {'name': 'Federico Fancellu'}, {'name': 'Vladimir Pavlovic'}, {'name': 'Afsaneh Fazly'}]",2024-07-16T05:26:12Z
http://arxiv.org/abs/2407.11384v1,http://arxiv.org/abs/2407.11384v1,"InvAgent: A Large Language Model based Multi-Agent System for Inventory
  Management in Supply Chains","Supply chain management (SCM) involves coordinating the flow of goods,
information, and finances across various entities to deliver products
efficiently. Effective inventory management is crucial in today's volatile,
uncertain, complex, and ambiguous (VUCA) world. Previous research has
demonstrated the superiority of heuristic methods and reinforcement learning
applications in inventory management. However, the application of large
language models (LLMs) as autonomous agents in multi-agent systems for
inventory management remains underexplored. This study introduces a novel
approach using LLMs to manage multi-agent inventory systems. Leveraging their
zero-shot learning capabilities, our model, InvAgent, enhances resilience and
improves efficiency across the supply chain network. Our contributions include
utilizing LLMs for zero-shot learning to enable adaptive and informed
decision-making without prior training, providing significant explainability
and clarity through Chain-of-Thought (CoT), and demonstrating dynamic
adaptability to varying demand scenarios while minimizing costs and avoiding
stockouts. Extensive evaluations across different scenarios highlight the
efficiency of our model in SCM.","[{'name': 'Yinzhu Quan'}, {'name': 'Zefang Liu'}]",2024-07-16T04:55:17Z
http://arxiv.org/abs/2407.11368v1,http://arxiv.org/abs/2407.11368v1,"Ancient Korean Archive Translation: Comparison Analysis on Statistical
  phrase alignment, LLM in-context learning, and inter-methodological approach","This study aims to compare three methods for translating ancient texts with
sparse corpora: (1) the traditional statistical translation method of phrase
alignment, (2) in-context LLM learning, and (3) proposed inter methodological
approach - statistical machine translation method using sentence piece tokens
derived from unified set of source-target corpus. The performance of the
proposed approach in this study is 36.71 in BLEU score, surpassing the scores
of SOLAR-10.7B context learning and the best existing Seq2Seq model. Further
analysis and discussion are presented.","[{'name': 'Sojung Lucia Kim'}, {'name': 'Taehong Jang'}, {'name': 'Joonmo Ahn'}]",2024-07-16T04:26:21Z
http://arxiv.org/abs/2407.15862v1,http://arxiv.org/abs/2407.15862v1,"Performance Evaluation of Lightweight Open-source Large Language Models
  in Pediatric Consultations: A Comparative Analysis","Large language models (LLMs) have demonstrated potential applications in
medicine, yet data privacy and computational burden limit their deployment in
healthcare institutions. Open-source and lightweight versions of LLMs emerge as
potential solutions, but their performance, particularly in pediatric settings
remains underexplored. In this cross-sectional study, 250 patient consultation
questions were randomly selected from a public online medical forum, with 10
questions from each of 25 pediatric departments, spanning from December 1,
2022, to October 30, 2023. Two lightweight open-source LLMs, ChatGLM3-6B and
Vicuna-7B, along with a larger-scale model, Vicuna-13B, and the widely-used
proprietary ChatGPT-3.5, independently answered these questions in Chinese
between November 1, 2023, and November 7, 2023. To assess reproducibility, each
inquiry was replicated once. We found that ChatGLM3-6B demonstrated higher
accuracy and completeness than Vicuna-13B and Vicuna-7B (P < .001), but all
were outperformed by ChatGPT-3.5. ChatGPT-3.5 received the highest ratings in
accuracy (65.2%) compared to ChatGLM3-6B (41.2%), Vicuna-13B (11.2%), and
Vicuna-7B (4.4%). Similarly, in completeness, ChatGPT-3.5 led (78.4%), followed
by ChatGLM3-6B (76.0%), Vicuna-13B (34.8%), and Vicuna-7B (22.0%) in highest
ratings. ChatGLM3-6B matched ChatGPT-3.5 in readability, both outperforming
Vicuna models (P < .001). In terms of empathy, ChatGPT-3.5 outperformed the
lightweight LLMs (P < .001). In safety, all models performed comparably well (P
> .05), with over 98.4% of responses being rated as safe. Repetition of
inquiries confirmed these findings. In conclusion, Lightweight LLMs demonstrate
promising application in pediatric healthcare. However, the observed gap
between lightweight and large-scale proprietary LLMs underscores the need for
continued development efforts.","[{'name': 'Qiuhong Wei'}, {'name': 'Ying Cui'}, {'name': 'Mengwei Ding'}, {'name': 'Yanqin Wang'}, {'name': 'Lingling Xiang'}, {'name': 'Zhengxiong Yao'}, {'name': 'Ceran Chen'}, {'name': 'Ying Long'}, {'name': 'Zhezhen Jin'}, {'name': 'Ximing Xu'}]",2024-07-16T03:35:09Z
http://arxiv.org/abs/2407.11345v1,http://arxiv.org/abs/2407.11345v1,"Beyond Binary: Multiclass Paraphasia Detection with Generative
  Pretrained Transformers and End-to-End Models","Aphasia is a language disorder that can lead to speech errors known as
paraphasias, which involve the misuse, substitution, or invention of words.
Automatic paraphasia detection can help those with Aphasia by facilitating
clinical assessment and treatment planning options. However, most automatic
paraphasia detection works have focused solely on binary detection, which
involves recognizing only the presence or absence of a paraphasia. Multiclass
paraphasia detection represents an unexplored area of research that focuses on
identifying multiple types of paraphasias and where they occur in a given
speech segment. We present novel approaches that use a generative pretrained
transformer (GPT) to identify paraphasias from transcripts as well as two
end-to-end approaches that focus on modeling both automatic speech recognition
(ASR) and paraphasia classification as multiple sequences vs. a single
sequence. We demonstrate that a single sequence model outperforms GPT baselines
for multiclass paraphasia detection.","[{'name': 'Matthew Perez'}, {'name': 'Aneesha Sampath'}, {'name': 'Minxue Niu'}, {'name': 'Emily Mower Provost'}]",2024-07-16T03:24:51Z
http://arxiv.org/abs/2407.12064v1,http://arxiv.org/abs/2407.12064v1,"LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization
  and Classification Task","Vision-language models have been extensively explored across a wide range of
tasks, achieving satisfactory performance; however, their application in
medical imaging remains underexplored. In this work, we propose a unified
framework - LiteGPT - for the medical imaging. We leverage multiple pre-trained
visual encoders to enrich information and enhance the performance of
vision-language models. To the best of our knowledge, this is the first study
to utilize vision-language models for the novel task of joint localization and
classification in medical images. Besides, we are pioneers in providing
baselines for disease localization in chest X-rays. Finally, we set new
state-of-the-art performance in the image classification task on the
well-benchmarked VinDr-CXR dataset. All code and models are publicly available
online: https://github.com/leduckhai/LiteGPT","[{'name': 'Khai Le-Duc'}, {'name': 'Ryan Zhang'}, {'name': 'Ngoc Son Nguyen'}, {'name': 'Tan-Hanh Pham'}, {'name': 'Anh Dao'}, {'name': 'Ba Hung Ngo'}, {'name': 'Anh Totti Nguyen'}, {'name': 'Truong-Son Hy'}]",2024-07-16T02:19:02Z
http://arxiv.org/abs/2407.11282v3,http://arxiv.org/abs/2407.11282v3,"Uncertainty is Fragile: Manipulating Uncertainty in Large Language
  Models","Large Language Models (LLMs) are employed across various high-stakes domains,
where the reliability of their outputs is crucial. One commonly used method to
assess the reliability of LLMs' responses is uncertainty estimation, which
gauges the likelihood of their answers being correct. While many studies focus
on improving the accuracy of uncertainty estimations for LLMs, our research
investigates the fragility of uncertainty estimation and explores potential
attacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,
when activated by a specific trigger in the input, manipulates the model's
uncertainty without affecting the final output. Specifically, the proposed
backdoor attack method can alter an LLM's output probability distribution,
causing the probability distribution to converge towards an attacker-predefined
distribution while ensuring that the top-1 prediction remains unchanged. Our
experimental results demonstrate that this attack effectively undermines the
model's self-evaluation reliability in multiple-choice questions. For instance,
we achieved a 100 attack success rate (ASR) across three different triggering
strategies in four models. Further, we investigate whether this manipulation
generalizes across different prompts and domains. This work highlights a
significant threat to the reliability of LLMs and underscores the need for
future defenses against such attacks. The code is available at
https://github.com/qcznlp/uncertainty_attack.","[{'name': 'Qingcheng Zeng'}, {'name': 'Mingyu Jin'}, {'name': 'Qinkai Yu'}, {'name': 'Zhenting Wang'}, {'name': 'Wenyue Hua'}, {'name': 'Zihao Zhou'}, {'name': 'Guangyan Sun'}, {'name': 'Yanda Meng'}, {'name': 'Shiqing Ma'}, {'name': 'Qifan Wang'}, {'name': 'Felix Juefei-Xu'}, {'name': 'Kaize Ding'}, {'name': 'Fan Yang'}, {'name': 'Ruixiang Tang'}, {'name': 'Yongfeng Zhang'}]",2024-07-15T23:41:11Z
http://arxiv.org/abs/2407.11277v2,http://arxiv.org/abs/2407.11277v2,"Target conversation extraction: Source separation using turn-taking
  dynamics","Extracting the speech of participants in a conversation amidst interfering
speakers and noise presents a challenging problem. In this paper, we introduce
the novel task of target conversation extraction, where the goal is to extract
the audio of a target conversation based on the speaker embedding of one of its
participants. To accomplish this, we propose leveraging temporal patterns
inherent in human conversations, particularly turn-taking dynamics, which
uniquely characterize speakers engaged in conversation and distinguish them
from interfering speakers and noise. Using neural networks, we show the
feasibility of our approach on English and Mandarin conversation datasets. In
the presence of interfering speakers, our results show an 8.19 dB improvement
in signal-to-noise ratio for 2-speaker conversations and a 7.92 dB improvement
for 2-4-speaker conversations. Code, dataset available at
https://github.com/chentuochao/Target-Conversation-Extraction.","[{'name': 'Tuochao Chen'}, {'name': 'Qirui Wang'}, {'name': 'Bohan Wu'}, {'name': 'Malek Itani'}, {'name': 'Sefik Emre Eskimez'}, {'name': 'Takuya Yoshioka'}, {'name': 'Shyamnath Gollakota'}]",2024-07-15T22:55:27Z
http://arxiv.org/abs/2407.11242v1,http://arxiv.org/abs/2407.11242v1,"OmniGenome: Aligning RNA Sequences with Secondary Structures in Genomic
  Foundation Models","The structures of RNA sequences play a vital role in various cellular
processes, while existing genomic foundation models (FMs) have struggled with
precise sequence-structure alignment, due to the complexity of exponential
combinations of nucleotide bases. In this study, we introduce OmniGenome, a
foundation model that addresses this critical challenge of sequence-structure
alignment in RNA FMs. OmniGenome bridges the sequences with secondary
structures using structure-contextualized modeling, enabling hard in-silico
genomic tasks that existing FMs cannot handle, e.g., RNA design tasks. The
results on two comprehensive genomic benchmarks show that OmniGenome achieves
state-of-the-art performance on complex RNA subtasks. For example, OmniGenome
solved 74% of complex puzzles, compared to SpliceBERT which solved only 3% of
the puzzles. Besides, OmniGenome solves most of the puzzles within $1$ hour,
while the existing methods usually allocate $24$ hours for each puzzle.
Overall, OmniGenome establishes wide genomic application cases and offers
profound insights into biological mechanisms from the perspective of
sequence-structure alignment.","[{'name': 'Heng Yang'}, {'name': 'Ke Li'}]",2024-07-15T21:10:40Z
http://arxiv.org/abs/2407.11240v1,http://arxiv.org/abs/2407.11240v1,"Making New Connections: LLMs as Puzzle Generators for The New York
  Times' Connections Word Game","The Connections puzzle is a word association game published daily by The New
York Times (NYT). In this game, players are asked to find groups of four words
that are connected by a common theme. While solving a given Connections puzzle
requires both semantic knowledge and abstract reasoning, generating novel
puzzles additionally requires a form of metacognition: generators must be able
to accurately model the downstream reasoning of potential solvers. In this
paper, we investigate the ability of the GPT family of Large Language Models
(LLMs) to generate challenging and creative word games for human players. We
start with an analysis of the word game Connections and the unique challenges
it poses as a Procedural Content Generation (PCG) domain. We then propose a
method for generating Connections puzzles using LLMs by adapting a Tree of
Thoughts (ToT) prompting approach. We evaluate this method by conducting a user
study, asking human players to compare AI-generated puzzles against published
Connections puzzles. Our findings show that LLMs are capable puzzle creators,
and can generate diverse sets of enjoyable, challenging, and creative
Connections puzzles as judged by human users.","[{'name': 'Tim Merino'}, {'name': 'Sam Earle'}, {'name': 'Ryan Sudhakaran'}, {'name': 'Shyam Sudhakaran'}, {'name': 'Julian Togelius'}]",2024-07-15T21:05:25Z
http://arxiv.org/abs/2407.11214v1,http://arxiv.org/abs/2407.11214v1,"PutnamBench: Evaluating Neural Theorem-Provers on the Putnam
  Mathematical Competition","We present PutnamBench, a new multilingual benchmark for evaluating the
ability of neural theorem-provers to solve competition mathematics problems.
PutnamBench consists of 1697 hand-constructed formalizations of 640 theorems
sourced from the William Lowell Putnam Mathematical Competition, the premier
undergraduate-level mathematics competition in North America. All the theorems
have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq
formalizations. Proving the theorems requires significant problem-solving
ability and proficiency in a broad range of topics taught in undergraduate
mathematics courses. We use PutnamBench to evaluate several established neural
and symbolic theorem-provers. These approaches can only solve a handful of the
PutnamBench problems, establishing the benchmark as a difficult open challenge
for research on neural theorem-proving. PutnamBench is available at
https://github.com/trishullab/PutnamBench.","[{'name': 'George Tsoukalas'}, {'name': 'Jasper Lee'}, {'name': 'John Jennings'}, {'name': 'Jimmy Xin'}, {'name': 'Michelle Ding'}, {'name': 'Michael Jennings'}, {'name': 'Amitayush Thakur'}, {'name': 'Swarat Chaudhuri'}]",2024-07-15T19:57:15Z
http://arxiv.org/abs/2407.11212v1,http://arxiv.org/abs/2407.11212v1,"Automated essay scoring in Arabic: a dataset and analysis of a
  BERT-based system","Automated Essay Scoring (AES) holds significant promise in the field of
education, helping educators to mark larger volumes of essays and provide
timely feedback. However, Arabic AES research has been limited by the lack of
publicly available essay data. This study introduces AR-AES, an Arabic AES
benchmark dataset comprising 2046 undergraduate essays, including gender
information, scores, and transparent rubric-based evaluation guidelines,
providing comprehensive insights into the scoring process. These essays come
from four diverse courses, covering both traditional and online exams.
Additionally, we pioneer the use of AraBERT for AES, exploring its performance
on different question types. We find encouraging results, particularly for
Environmental Chemistry and source-dependent essay questions. For the first
time, we examine the scale of errors made by a BERT-based AES system, observing
that 96.15 percent of the errors are within one point of the first human
marker's prediction, on a scale of one to five, with 79.49 percent of
predictions matching exactly. In contrast, additional human markers did not
exceed 30 percent exact matches with the first marker, with 62.9 percent within
one mark. These findings highlight the subjectivity inherent in essay grading,
and underscore the potential for current AES technology to assist human markers
to grade consistently across large classes.","[{'name': 'Rayed Ghazawi'}, {'name': 'Edwin Simpson'}]",2024-07-15T19:55:37Z
http://arxiv.org/abs/2407.11194v1,http://arxiv.org/abs/2407.11194v1,AstroMLab 1: Who Wins Astronomy Jeopardy!?,"We present a comprehensive evaluation of proprietary and open-weights large
language models using the first astronomy-specific benchmarking dataset. This
dataset comprises 4,425 multiple-choice questions curated from the Annual
Review of Astronomy and Astrophysics, covering a broad range of astrophysical
topics. Our analysis examines model performance across various astronomical
subfields and assesses response calibration, crucial for potential deployment
in research environments. Claude-3.5-Sonnet outperforms competitors by up to
4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we
observed a universal reduction in cost every 3-to-12 months to achieve similar
score in this particular astronomy benchmark. Open-source models have rapidly
improved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with
some of the best proprietary models. We identify performance variations across
topics, with non-English-focused models generally struggling more in
exoplanet-related fields, stellar astrophysics, and instrumentation related
questions. These challenges likely stem from less abundant training data,
limited historical context, and rapid recent developments in these areas. This
pattern is observed across both open-weights and proprietary models, with
regional dependencies evident, highlighting the impact of training data
diversity on model performance in specialized scientific domains.
Top-performing models demonstrate well-calibrated confidence, with correlations
above 0.9 between confidence and correctness, though they tend to be slightly
underconfident. The development for fast, low-cost inference of open-weights
models presents new opportunities for affordable deployment in astronomy. The
rapid progress observed suggests that LLM-driven research in astronomy may
become feasible in the near future.","[{'name': 'Yuan-Sen Ting'}, {'name': 'Tuan Dung Nguyen'}, {'name': 'Tirthankar Ghosal'}, {'name': 'Rui Pan'}, {'name': 'Hardik Arora'}, {'name': 'Zechang Sun'}, {'name': 'Tijmen de Haan'}, {'name': 'Nesar Ramachandra'}, {'name': 'Azton Wells'}, {'name': 'Sandeep Madireddy'}, {'name': 'Alberto Accomazzi'}]",2024-07-15T19:28:14Z
http://arxiv.org/abs/2407.11186v2,http://arxiv.org/abs/2407.11186v2,"FarsInstruct: Empowering Large Language Models for Persian Instruction
  Understanding","Instruction-tuned large language models, such as T0, have demonstrated
remarkable capabilities in following instructions across various domains.
However, their proficiency remains notably deficient in many low-resource
languages. To address this challenge, we introduce FarsInstruct: a
comprehensive instruction dataset designed to enhance the instruction-following
ability of large language models specifically for the Persian language, a
significant yet underrepresented language globally. FarsInstruct encompasses a
wide range of task types and datasets, each containing a mix of straightforward
to complex manual written instructions, as well as translations from Public
Pool of Prompts, ensuring a rich linguistic and cultural representation.
Furthermore, we introduce Co-CoLA, a framework designed to enhance the
multi-task adaptability of LoRA-tuned models. Through extensive experimental
analyses, our study showcases the effectiveness of FarsInstruct dataset coupled
with training by Co-CoLA framework, in improving the performance of large
language models within the Persian context. As of the current writing,
FarsInstruct comprises more than 200 templates across 21 distinct datasets, and
we intend to update it consistently, thus augmenting its applicability.","[{'name': 'Hojjat Mokhtarabadi'}, {'name': 'Ziba Zamani'}, {'name': 'Abbas Maazallahi'}, {'name': 'Hossein Manshaei'}]",2024-07-15T19:17:31Z
http://arxiv.org/abs/2408.00769v1,http://arxiv.org/abs/2408.00769v1,"Decoding AI and Human Authorship: Nuances Revealed Through NLP and
  Statistical Analysis","This research explores the nuanced differences in texts produced by AI and
those written by humans, aiming to elucidate how language is expressed
differently by AI and humans. Through comprehensive statistical data analysis,
the study investigates various linguistic traits, patterns of creativity, and
potential biases inherent in human-written and AI- generated texts. The
significance of this research lies in its contribution to understanding AI's
creative capabilities and its impact on literature, communication, and societal
frameworks. By examining a meticulously curated dataset comprising 500K essays
spanning diverse topics and genres, generated by LLMs, or written by humans,
the study uncovers the deeper layers of linguistic expression and provides
insights into the cognitive processes underlying both AI and human-driven
textual compositions. The analysis revealed that human-authored essays tend to
have a higher total word count on average than AI-generated essays but have a
shorter average word length compared to AI- generated essays, and while both
groups exhibit high levels of fluency, the vocabulary diversity of Human
authored content is higher than AI generated content. However, AI- generated
essays show a slightly higher level of novelty, suggesting the potential for
generating more original content through AI systems. The paper addresses
challenges in assessing the language generation capabilities of AI models and
emphasizes the importance of datasets that reflect the complexities of human-AI
collaborative writing. Through systematic preprocessing and rigorous
statistical analysis, this study offers valuable insights into the evolving
landscape of AI-generated content and informs future developments in natural
language processing (NLP).","[{'name': 'Mayowa Akinwande'}, {'name': 'Oluwaseyi Adeliyi'}, {'name': 'Toyyibat Yussuph'}]",2024-07-15T18:09:03Z
http://arxiv.org/abs/2407.11144v1,http://arxiv.org/abs/2407.11144v1,"YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language
  Parallel Corpus","Even for better-studied sign languages like American Sign Language (ASL),
data is the bottleneck for machine learning research. The situation is worse
yet for the many other sign languages used by Deaf/Hard of Hearing communities
around the world. In this paper, we present YouTube-SL-25, a large-scale,
open-domain multilingual corpus of sign language videos with seemingly
well-aligned captions drawn from YouTube. With >3000 hours of videos across >25
sign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest
parallel sign language dataset to date, and c) the first or largest parallel
dataset for many of its component languages. We provide baselines for
sign-to-text tasks using a unified multilingual multitask model based on T5 and
report scores on benchmarks across 4 sign languages. The results demonstrate
that multilingual transfer benefits both higher- and lower-resource sign
languages within YouTube-SL-25.","[{'name': 'Garrett Tanzer'}, {'name': 'Biao Zhang'}]",2024-07-15T18:08:34Z
http://arxiv.org/abs/2407.10969v3,http://arxiv.org/abs/2407.10969v3,Q-Sparse: All Large Language Models can be Fully Sparsely-Activated,"We introduce, Q-Sparse, a simple yet effective approach to training
sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity
of activations in LLMs which can bring significant efficiency gains in
inference. This is achieved by applying top-K sparsification to the activations
and the straight-through-estimator to the training. We also introduce Block
Q-Sparse for batch training and inference. The key results from this work are,
(1) Q-Sparse can achieve results comparable to those of baseline LLMs while
being much more efficient at inference time; (2) We present an
inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is
effective in different settings, including training-from-scratch,
continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for
both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the
synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the
cornerstone and a clear path to revolutionize the efficiency, including cost
and energy consumption, of future LLMs.","[{'name': 'Hongyu Wang'}, {'name': 'Shuming Ma'}, {'name': 'Ruiping Wang'}, {'name': 'Furu Wei'}]",2024-07-15T17:59:29Z
http://arxiv.org/abs/2407.10964v1,http://arxiv.org/abs/2407.10964v1,"No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen
  Representations","This paper introduces FUNGI, Features from UNsupervised GradIents, a method
to enhance the features of vision encoders by leveraging self-supervised
gradients. Our method is simple: given any pretrained model, we first compute
gradients from various self-supervised objectives for each input. These are
projected to a lower dimension and then concatenated with the model's
embedding. The resulting features are evaluated on k-nearest neighbor
classification over 11 datasets from vision, 5 from natural language
processing, and 2 from audio. Across backbones spanning various sizes and
pretraining strategies, FUNGI features provide consistent performance
improvements over the embeddings. We also show that using FUNGI features can
benefit linear classification and image retrieval, and that they significantly
improve the retrieval-based in-context scene understanding abilities of
pretrained models, for example improving upon DINO by +17% for semantic
segmentation - without any training.","[{'name': 'Walter Simoncini'}, {'name': 'Spyros Gidaris'}, {'name': 'Andrei Bursuc'}, {'name': 'Yuki M. Asano'}]",2024-07-15T17:58:42Z
http://arxiv.org/abs/2407.10960v1,http://arxiv.org/abs/2407.10960v1,Fast Matrix Multiplications for Lookup Table-Quantized LLMs,"The deployment of large language models (LLMs) is often constrained by memory
bandwidth, where the primary bottleneck is the cost of transferring model
parameters from the GPU's global memory to its registers. When coupled with
custom kernels that fuse the dequantization and matmul operations, weight-only
quantization can thus enable faster inference by reducing the amount of memory
movement. However, developing high-performance kernels for weight-quantized
LLMs presents substantial challenges, especially when the weights are
compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,
lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup
table engine for LUT-quantized LLMs, which uses offline restructuring of the
quantized weight matrix to minimize bit manipulations associated with
unpacking, and vectorization and duplication of the lookup table to mitigate
shared memory bandwidth constraints. At batch sizes < 32 and quantization group
size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster
than existing GEMM kernels. As an application of FLUTE, we explore a simple
extension to lookup table-based NormalFloat quantization and apply it to
quantize LLaMA3 to various configurations, obtaining competitive quantization
performance against strong baselines while obtaining an end-to-end throughput
increase of 1.5 to 2 times.","[{'name': 'Han Guo'}, {'name': 'William Brandon'}, {'name': 'Radostin Cholakov'}, {'name': 'Jonathan Ragan-Kelley'}, {'name': 'Eric P. Xing'}, {'name': 'Yoon Kim'}]",2024-07-15T17:55:42Z
http://arxiv.org/abs/2407.10956v1,http://arxiv.org/abs/2407.10956v1,"Spider2-V: How Far Are Multimodal Agents From Automating Data Science
  and Engineering Workflows?","Data science and engineering workflows often span multiple stages, from
warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As
vision language models (VLMs) advance in multimodal understanding and code
generation, VLM-based agents could potentially automate these workflows by
generating SQL queries, Python code, and GUI operations. This automation can
improve the productivity of experts while democratizing access to large-scale
data analysis. In this paper, we introduce Spider2-V, the first multimodal
agent benchmark focusing on professional data science and engineering
workflows, featuring 494 real-world tasks in authentic computer environments
and incorporating 20 enterprise-level professional applications. These tasks,
derived from real-world use cases, evaluate the ability of a multimodal agent
to perform data-related tasks by writing code and managing the GUI in
enterprise data software systems. To balance realistic simulation with
evaluation simplicity, we devote significant effort to developing automatic
configurations for task setup and carefully crafting evaluation metrics for
each task. Furthermore, we supplement multimodal agents with comprehensive
documents of these enterprise data software systems. Our empirical evaluation
reveals that existing state-of-the-art LLM/VLM-based agents do not reliably
automate full data workflows (14.0% success). Even with step-by-step guidance,
these agents still underperform in tasks that require fine-grained,
knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted
workspaces (10.6%). We hope that Spider2-V paves the way for autonomous
multimodal agents to transform the automation of data science and engineering
workflow. Our code and data are available at https://spider2-v.github.io.","[{'name': 'Ruisheng Cao'}, {'name': 'Fangyu Lei'}, {'name': 'Haoyuan Wu'}, {'name': 'Jixuan Chen'}, {'name': 'Yeqiao Fu'}, {'name': 'Hongcheng Gao'}, {'name': 'Xinzhuang Xiong'}, {'name': 'Hanchong Zhang'}, {'name': 'Yuchen Mao'}, {'name': 'Wenjing Hu'}, {'name': 'Tianbao Xie'}, {'name': 'Hongshen Xu'}, {'name': 'Danyang Zhang'}, {'name': 'Sida Wang'}, {'name': 'Ruoxi Sun'}, {'name': 'Pengcheng Yin'}, {'name': 'Caiming Xiong'}, {'name': 'Ansong Ni'}, {'name': 'Qian Liu'}, {'name': 'Victor Zhong'}, {'name': 'Lu Chen'}, {'name': 'Kai Yu'}, {'name': 'Tao Yu'}]",2024-07-15T17:54:37Z
http://arxiv.org/abs/2407.10953v2,http://arxiv.org/abs/2407.10953v2,"MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with
  Open-domain Information Extraction Large Language Models","The Mutual Reinforcement Effect (MRE) represents a promising avenue in
information extraction and multitasking research. Nevertheless, its
applicability has been constrained due to the exclusive availability of MRE mix
datasets in Japanese, thereby limiting comprehensive exploration by the global
research community. To address this limitation, we introduce a Multilingual MRE
mix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, and
Chinese. In this paper, we also propose a method for dataset translation
assisted by Large Language Models (LLMs), which significantly reduces the
manual annotation time required for dataset construction by leveraging LLMs to
translate the original Japanese datasets. Additionally, we have enriched the
dataset by incorporating open-domain Named Entity Recognition (NER) and
sentence classification tasks. Utilizing this expanded dataset, we developed a
unified input-output framework to train an Open-domain Information Extraction
Large Language Model (OIELLM). The OIELLM model demonstrates the capability to
effectively process novel MMM datasets, exhibiting significant improvements in
performance.","[{'name': 'Chengguang Gan'}, {'name': 'Qingyu Yin'}, {'name': 'Xinyang He'}, {'name': 'Hanjun Wei'}, {'name': 'Yunhao Liang'}, {'name': 'Younghun Lim'}, {'name': 'Shijian Wang'}, {'name': 'Hexiang Huang'}, {'name': 'Qinghao Zhang'}, {'name': 'Shiwen Ni'}, {'name': 'Tatsunori Mori'}]",2024-07-15T17:50:43Z
http://arxiv.org/abs/2407.10949v1,http://arxiv.org/abs/2407.10949v1,Representing Rule-based Chatbots with Transformers,"Transformer-based chatbots can conduct fluent, natural-sounding
conversations, but we have limited understanding of the mechanisms underlying
their behavior. Prior work has taken a bottom-up approach to understanding
Transformers by constructing Transformers for various synthetic and formal
language tasks, such as regular expressions and Dyck languages. However, it is
not obvious how to extend this approach to understand more naturalistic
conversational agents. In this work, we take a step in this direction by
constructing a Transformer that implements the ELIZA program, a classic,
rule-based chatbot. ELIZA illustrates some of the distinctive challenges of the
conversational setting, including both local pattern matching and long-term
dialog state tracking. We build on constructions from prior work -- in
particular, for simulating finite-state automata -- showing how simpler
constructions can be composed and extended to give rise to more sophisticated
behavior. Next, we train Transformers on a dataset of synthetically generated
ELIZA conversations and investigate the mechanisms the models learn. Our
analysis illustrates the kinds of mechanisms these models tend to prefer -- for
example, models favor an induction head mechanism over a more precise, position
based copying mechanism; and using intermediate generations to simulate
recurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing
an explicit connection between neural chatbots and interpretable, symbolic
mechanisms, our results offer a new setting for mechanistic analysis of
conversational agents.","[{'name': 'Dan Friedman'}, {'name': 'Abhishek Panigrahi'}, {'name': 'Danqi Chen'}]",2024-07-15T17:45:53Z
http://arxiv.org/abs/2407.10944v1,http://arxiv.org/abs/2407.10944v1,Learning from Naturally Occurring Feedback,"Human feedback data is a critical component in developing language models.
However, collecting this feedback is costly and ultimately not scalable. We
propose a scalable method for extracting feedback that users naturally include
when interacting with chat models, and leveraging it for model training. We are
further motivated by previous work that showed there are also qualitative
advantages to using naturalistic (rather than auto-generated) feedback, such as
less hallucinations and biases. We manually annotated conversation data to
confirm the presence of naturally occurring feedback in a standard corpus,
finding that as much as 30% of the chats include explicit feedback. We apply
our method to over 1M conversations to obtain hundreds of thousands of feedback
samples. Training with the extracted feedback shows significant performance
improvements over baseline models, demonstrating the efficacy of our approach
in enhancing model alignment to human preferences.","[{'name': 'Shachar Don-Yehiya'}, {'name': 'Leshem Choshen'}, {'name': 'Omri Abend'}]",2024-07-15T17:41:34Z
http://arxiv.org/abs/2407.12873v1,http://arxiv.org/abs/2407.12873v1,Evaluation of RAG Metrics for Question Answering in the Telecom Domain,"Retrieval Augmented Generation (RAG) is widely used to enable Large Language
Models (LLMs) perform Question Answering (QA) tasks in various domains.
However, RAG based on open-source LLM for specialized domains has challenges of
evaluating generated responses. A popular framework in the literature is the
RAG Assessment (RAGAS), a publicly available library which uses LLMs for
evaluation. One disadvantage of RAGAS is the lack of details of derivation of
numerical value of the evaluation metrics. One of the outcomes of this work is
a modified version of this package for few metrics (faithfulness, context
relevance, answer relevance, answer correctness, answer similarity and factual
correctness) through which we provide the intermediate outputs of the prompts
by using any LLMs. Next, we analyse the expert evaluations of the output of the
modified RAGAS package and observe the challenges of using it in the telecom
domain. We also study the effect of the metrics under correct vs. wrong
retrieval and observe that few of the metrics have higher values for correct
retrieval. We also study for differences in metrics between base embeddings and
those domain adapted via pre-training and fine-tuning. Finally, we comment on
the suitability and challenges of using these metrics for in-the-wild telecom
QA task.","[{'name': 'Sujoy Roychowdhury'}, {'name': 'Sumit Soman'}, {'name': 'H G Ranjani'}, {'name': 'Neeraj Gunda'}, {'name': 'Vansh Chhabra'}, {'name': 'Sai Krishna Bala'}]",2024-07-15T17:40:15Z
http://arxiv.org/abs/2407.10899v1,http://arxiv.org/abs/2407.10899v1,Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis,"Effective educational measurement relies heavily on the curation of
well-designed item pools (i.e., possessing the right psychometric properties).
However, item calibration is time-consuming and costly, requiring a sufficient
number of respondents for the response process. We explore using six different
LLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)
and various combinations of them using sampling methods to produce responses
with psychometric properties similar to human answers. Results show that some
LLMs have comparable or higher proficiency in College Algebra than college
students. No single LLM mimics human respondents due to narrow proficiency
distributions, but an ensemble of LLMs can better resemble college students'
ability distribution. The item parameters calibrated by LLM-Respondents have
high correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated
counterparts, and closely resemble the parameters of the human subset (e.g.
0.02 Spearman correlation difference). Several augmentation strategies are
evaluated for their relative performance, with resampling methods proving most
effective, enhancing the Spearman correlation from 0.89 (human only) to 0.93
(augmented human).","[{'name': 'Yunting Liu'}, {'name': 'Shreya Bhandari'}, {'name': 'Zachary A. Pardos'}]",2024-07-15T16:49:26Z
http://arxiv.org/abs/2407.10855v1,http://arxiv.org/abs/2407.10855v1,Weighted Grouped Query Attention in Transformers,"The attention mechanism forms the foundational blocks for transformer
language models. Recent approaches show that scaling the model achieves
human-level performance. However, with increasing demands for scaling and
constraints on hardware memory, the inference costs of these models remain
high. To reduce the inference time, Multi-Query Attention (MQA) and
Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet
al., 2023) respectively. In this paper, we propose a variation of Grouped-Query
Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new
learnable parameters for each key and value head in the T5 decoder attention
blocks, enabling the model to take a weighted average during finetuning. Our
model achieves an average of 0.53% improvement over GQA, and the performance
converges to traditional Multi-head attention (MHA) with no additional overhead
during inference. We evaluated the introduction of these parameters and
subsequent finetuning informs the model about the grouping mechanism during
training, thereby enhancing performance. Additionally, we demonstrate the
scaling laws in our analysis by comparing the results between T5-small and
T5-base architecture.","[{'name': 'Sai Sena Chinnakonduru'}, {'name': 'Astarag Mohapatra'}]",2024-07-15T16:07:13Z
http://arxiv.org/abs/2407.10853v2,http://arxiv.org/abs/2407.10853v2,"An Actionable Framework for Assessing Bias and Fairness in Large
  Language Model Use Cases","Large language models (LLMs) can exhibit bias in a variety of ways. Such
biases can create or exacerbate unfair outcomes for certain groups within a
protected attribute, including, but not limited to sex, race, sexual
orientation, or age. This paper aims to provide a technical guide for
practitioners to assess bias and fairness risks in LLM use cases. The main
contribution of this work is a decision framework that allows practitioners to
determine which metrics to use for a specific LLM use case. To achieve this,
this study categorizes LLM bias and fairness risks, maps those risks to a
taxonomy of LLM use cases, and then formally defines various metrics to assess
each type of risk. As part of this work, several new bias and fairness metrics
are introduced, including innovative counterfactual metrics as well as metrics
based on stereotype classifiers. Instead of focusing solely on the model
itself, the sensitivity of both prompt-risk and model-risk are taken into
account by defining evaluations at the level of an LLM use case, characterized
by a model and a population of prompts. Furthermore, because all of the
evaluation metrics are calculated solely using the LLM output, the proposed
framework is highly practical and easily actionable for practitioners.",[{'name': 'Dylan Bouchard'}],2024-07-15T16:04:44Z
http://arxiv.org/abs/2407.10829v1,http://arxiv.org/abs/2407.10829v1,"BiasScanner: Automatic Detection and Classification of News Bias to
  Strengthen Democracy","The increasing consumption of news online in the 21st century coincided with
increased publication of disinformation, biased reporting, hate speech and
other unwanted Web content. We describe BiasScanner, an application that aims
to strengthen democracy by supporting news consumers with scrutinizing news
articles they are reading online. BiasScanner contains a server-side
pre-trained large language model to identify biased sentences of news articles
and a front-end Web browser plug-in. At the time of writing, BiasScanner can
identify and classify more than two dozen types of media bias at the sentence
level, making it the most fine-grained model and only deployed application
(automatic system in use) of its kind. It was implemented in a light-weight and
privacy-respecting manner, and in addition to highlighting likely biased
sentence it also provides explanations for each classification decision as well
as a summary analysis for each news article. While prior research has addressed
news bias detection, we are not aware of any work that resulted in a deployed
browser plug-in (c.f. also biasscanner.org for a Web demo).","[{'name': 'Tim Menzner'}, {'name': 'Jochen L. Leidner'}]",2024-07-15T15:42:22Z
http://arxiv.org/abs/2407.10807v1,http://arxiv.org/abs/2407.10807v1,"Employing Sentence Space Embedding for Classification of Data Stream
  from Fake News Domain","Tabular data is considered the last unconquered castle of deep learning, yet
the task of data stream classification is stated to be an equally important and
demanding research area. Due to the temporal constraints, it is assumed that
deep learning methods are not the optimal solution for application in this
field. However, excluding the entire -- and prevalent -- group of methods seems
rather rash given the progress that has been made in recent years in its
development. For this reason, the following paper is the first to present an
approach to natural language data stream classification using the sentence
space method, which allows for encoding text into the form of a discrete
digital signal. This allows the use of convolutional deep networks dedicated to
image classification to solve the task of recognizing fake news based on text
data. Based on the real-life Fakeddit dataset, the proposed approach was
compared with state-of-the-art algorithms for data stream classification based
on generalization ability and time complexity.","[{'name': 'Paweł Zyblewski'}, {'name': 'Jakub Klikowski'}, {'name': 'Weronika Borek-Marciniec'}, {'name': 'Paweł Ksieniewicz'}]",2024-07-15T15:23:21Z
http://arxiv.org/abs/2407.10805v3,http://arxiv.org/abs/2407.10805v3,"Think-on-Graph 2.0: Deep and Interpretable Large Language Model
  Reasoning with Knowledge Graph-guided Retrieval","Retrieval-augmented generation (RAG) has significantly advanced large
language models (LLMs) by enabling dynamic information retrieval to mitigate
knowledge gaps and hallucinations in generated content. However, these systems
often falter with complex reasoning and consistency across diverse queries. In
this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns
questions with the knowledge graph and uses it as a navigational tool, which
deepens and refines the RAG paradigm for information collection and
integration. The KG-guided navigation fosters deep and long-range associations
to uphold logical consistency and optimize the scope of retrieval for precision
and interoperability. In conjunction, factual consistency can be better ensured
through semantic similarity guided by precise directives. ToG${2.0}$ not only
improves the accuracy and reliability of LLMs' responses but also demonstrates
the potential of hybrid structured knowledge systems to significantly advance
LLM reasoning, aligning it closer to human-like performance. We conducted
extensive experiments on four public datasets to demonstrate the advantages of
our method compared to the baseline.","[{'name': 'Shengjie Ma'}, {'name': 'Chengjin Xu'}, {'name': 'Xuhui Jiang'}, {'name': 'Muzhi Li'}, {'name': 'Huaren Qu'}, {'name': 'Jian Guo'}]",2024-07-15T15:20:40Z
http://arxiv.org/abs/2407.10804v1,http://arxiv.org/abs/2407.10804v1,"Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning
  and Format Alignment","Adapting general large language models (LLMs) to specialized domains presents
great challenges due to varied data distributions. This adaptation typically
requires continual pre-training on massive domain-specific corpora to
facilitate knowledge memorization, followed by training to apply this knowledge
following human instructions and preferences. However, this method may result
in inefficient knowledge memorization due to a lack of awareness of knowledge
utilization and imposes substantial demands on LLMs to simultaneously learn
knowledge utilization and format alignment with limited training samples. To
facilitate the domain adaptation of LLM, we revise this process and propose a
new domain adaptation framework including domain knowledge learning and general
format alignment, called Mix-CPT. Specifically, we first conduct a knowledge
mixture continual pre-training that concurrently focuses on knowledge
memorization and utilization, allowing for mutual reinforcement. To avoid
catastrophic forgetting during the continual pre-training process, we further
incorporate a logit swap self-distillation constraint. Subsequently, leveraging
the knowledge and capabilities acquired during continual pre-training, we
efficiently perform instruction tuning and alignment with a few general
training samples to achieve format alignment. Extensive experiments demonstrate
that our proposed Mix-CPT framework can simultaneously improve the task-solving
capabilities of LLMs on the target and general domains compared to the
traditional adaptation methods.","[{'name': 'Jinhao Jiang'}, {'name': 'Junyi Li'}, {'name': 'Wayne Xin Zhao'}, {'name': 'Yang Song'}, {'name': 'Tao Zhang'}, {'name': 'Ji-Rong Wen'}]",2024-07-15T15:20:13Z
http://arxiv.org/abs/2407.10795v1,http://arxiv.org/abs/2407.10795v1,Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping,"Decoding by contrasting layers (DoLa), is designed to improve the generation
quality of large language models (LLMs) by contrasting the prediction
probabilities between an early exit output (amateur logits) and the final
output (expert logits). However, we find that this approach does not work well
on non-English tasks. Inspired by previous interpretability work on language
transition during the model's forward pass, we discover that this issue arises
from a language mismatch between early exit output and final output. In this
work, we propose an improved contrastive decoding algorithm that is effective
for diverse languages beyond English. To obtain more helpful amateur logits, we
devise two strategies to skip a set of bottom, language-agnostic layers based
on our preliminary analysis. Experimental results on multilingual reasoning
benchmarks demonstrate that our proposed method outperforms previous
contrastive decoding baselines and substantially improves LLM's
chain-of-thought reasoning accuracy across 11 languages. The project will be
available at: https://github.com/NJUNLP/SkipLayerCD.","[{'name': 'Wenhao Zhu'}, {'name': 'Sizhe Liu'}, {'name': 'Shujian Huang'}, {'name': 'Shuaijie She'}, {'name': 'Chris Wendler'}, {'name': 'Jiajun Chen'}]",2024-07-15T15:14:01Z
http://arxiv.org/abs/2407.10794v1,http://arxiv.org/abs/2407.10794v1,"Graphusion: Leveraging Large Language Models for Scientific Knowledge
  Graph Fusion and Construction in NLP Education","Knowledge graphs (KGs) are crucial in the field of artificial intelligence
and are widely applied in downstream tasks, such as enhancing Question
Answering (QA) systems. The construction of KGs typically requires significant
effort from domain experts. Recently, Large Language Models (LLMs) have been
used for knowledge graph construction (KGC), however, most existing approaches
focus on a local perspective, extracting knowledge triplets from individual
sentences or documents. In this work, we introduce Graphusion, a zero-shot KGC
framework from free text. The core fusion module provides a global view of
triplets, incorporating entity merging, conflict resolution, and novel triplet
discovery. We showcase how Graphusion could be applied to the natural language
processing (NLP) domain and validate it in the educational scenario.
Specifically, we introduce TutorQA, a new expert-verified benchmark for graph
reasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our
evaluation demonstrates that Graphusion surpasses supervised baselines by up to
10% in accuracy on link prediction. Additionally, it achieves average scores of
2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and
relation recognition, respectively.","[{'name': 'Rui Yang'}, {'name': 'Boming Yang'}, {'name': 'Sixun Ouyang'}, {'name': 'Tianwei She'}, {'name': 'Aosong Feng'}, {'name': 'Yuang Jiang'}, {'name': 'Freddy Lecue'}, {'name': 'Jinghui Lu'}, {'name': 'Irene Li'}]",2024-07-15T15:13:49Z
http://arxiv.org/abs/2407.10793v1,http://arxiv.org/abs/2407.10793v1,"GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation
  Framework","Methods to evaluate Large Language Model (LLM) responses and detect
inconsistencies, also known as hallucinations, with respect to the provided
knowledge, are becoming increasingly important for LLM applications. Current
metrics fall short in their ability to provide explainable decisions,
systematically check all pieces of information in the response, and are often
too computationally expensive to be used in practice. We present GraphEval: a
hallucination evaluation framework based on representing information in
Knowledge Graph (KG) structures. Our method identifies the specific triples in
the KG that are prone to hallucinations and hence provides more insight into
where in the response a hallucination has occurred, if at all, than previous
methods. Furthermore, using our approach in conjunction with state-of-the-art
natural language inference (NLI) models leads to an improvement in balanced
accuracy on various hallucination benchmarks, compared to using the raw NLI
models. Lastly, we explore the use of GraphEval for hallucination correction by
leveraging the structure of the KG, a method we name GraphCorrect, and
demonstrate that the majority of hallucinations can indeed be rectified.","[{'name': 'Hannah Sansford'}, {'name': 'Nicholas Richardson'}, {'name': 'Hermina Petric Maretic'}, {'name': 'Juba Nait Saada'}]",2024-07-15T15:11:16Z
http://arxiv.org/abs/2407.10759v1,http://arxiv.org/abs/2407.10759v1,Qwen2-Audio Technical Report,"We introduce the latest progress of Qwen-Audio, a large-scale audio-language
model called Qwen2-Audio, which is capable of accepting various audio signal
inputs and performing audio analysis or direct textual responses with regard to
speech instructions. In contrast to complex hierarchical tags, we have
simplified the pre-training process by utilizing natural language prompts for
different data and tasks, and have further expanded the data volume. We have
boosted the instruction-following capability of Qwen2-Audio and implemented two
distinct audio interaction modes for voice chat and audio analysis. In the
voice chat mode, users can freely engage in voice interactions with Qwen2-Audio
without text input. In the audio analysis mode, users could provide audio and
text instructions for analysis during the interaction. Note that we do not use
any system prompts to switch between voice chat and audio analysis modes.
Qwen2-Audio is capable of intelligently comprehending the content within audio
and following voice commands to respond appropriately. For instance, in an
audio segment that simultaneously contains sounds, multi-speaker conversations,
and a voice command, Qwen2-Audio can directly understand the command and
provide an interpretation and response to the audio. Additionally, DPO has
optimized the model's performance in terms of factuality and adherence to
desired behavior. According to the evaluation results from AIR-Bench,
Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests
focused on audio-centric instruction-following capabilities. Qwen2-Audio is
open-sourced with the aim of fostering the advancement of the multi-modal
language community.","[{'name': 'Yunfei Chu'}, {'name': 'Jin Xu'}, {'name': 'Qian Yang'}, {'name': 'Haojie Wei'}, {'name': 'Xipin Wei'}, {'name': 'Zhifang Guo'}, {'name': 'Yichong Leng'}, {'name': 'Yuanjun Lv'}, {'name': 'Jinzheng He'}, {'name': 'Junyang Lin'}, {'name': 'Chang Zhou'}, {'name': 'Jingren Zhou'}]",2024-07-15T14:38:09Z
http://arxiv.org/abs/2407.10747v1,http://arxiv.org/abs/2407.10747v1,"Codebook LLMs: Adapting Political Science Codebooks for LLM Use and
  Adapting LLMs to Follow Codebooks","Codebooks -- documents that operationalize constructs and outline annotation
procedures -- are used almost universally by social scientists when coding
unstructured political texts. Recently, to reduce manual annotation costs,
political scientists have looked to generative large language models (LLMs) to
label and analyze text data. However, previous work using LLMs for
classification has implicitly relied on the universal label assumption --
correct classification of documents is possible using only a class label or
minimal definition and the information that the LLM inductively learns during
its pre-training. In contrast, we argue that political scientists who care
about valid measurement should instead make a codebook-construct label
assumption -- an LLM should follow the definition and exclusion criteria of a
construct/label provided in a codebook. In this work, we collect and curate
three political science datasets and their original codebooks and conduct a set
of experiments to understand whether LLMs comply with codebook instructions,
whether rewriting codebooks improves performance, and whether
instruction-tuning LLMs on codebook-document-label tuples improves performance
over zero-shot classification. Using Mistral 7B Instruct as our LLM, we find
re-structuring the original codebooks gives modest gains in zero-shot
performance but the model still struggles to comply with the constraints of the
codebooks. Optimistically, instruction-tuning Mistral on one of our datasets
gives significant gains over zero-shot inference (0.76 versus 0.53 micro F1).
We hope our conceptualization of the codebook-specific task, assumptions, and
instruction-tuning pipeline as well our semi-structured LLM codebook format
will help political scientists readily adapt to the LLM era.","[{'name': 'Andrew Halterman'}, {'name': 'Katherine A. Keith'}]",2024-07-15T14:20:09Z
http://arxiv.org/abs/2407.10745v1,http://arxiv.org/abs/2407.10745v1,"What distinguishes conspiracy from critical narratives? A computational
  analysis of oppositional discourse","The current prevalence of conspiracy theories on the internet is a
significant issue, tackled by many computational approaches. However, these
approaches fail to recognize the relevance of distinguishing between texts
which contain a conspiracy theory and texts which are simply critical and
oppose mainstream narratives. Furthermore, little attention is usually paid to
the role of inter-group conflict in oppositional narratives. We contribute by
proposing a novel topic-agnostic annotation scheme that differentiates between
conspiracies and critical texts, and that defines span-level categories of
inter-group conflict. We also contribute with the multilingual
XAI-DisInfodemics corpus (English and Spanish), which contains a high-quality
annotation of Telegram messages related to COVID-19 (5,000 messages per
language). We also demonstrate the feasibility of an NLP-based automatization
by performing a range of experiments that yield strong baseline solutions.
Finally, we perform an analysis which demonstrates that the promotion of
intergroup conflict and the presence of violence and anger are key aspects to
distinguish between the two types of oppositional narratives, i.e., conspiracy
vs. critical.","[{'name': 'Damir Korenčić'}, {'name': 'Berta Chulvi'}, {'name': 'Xavier Bonet Casals'}, {'name': 'Alejandro Toselli'}, {'name': 'Mariona Taulé'}, {'name': 'Paolo Rosso'}]",2024-07-15T14:18:47Z
http://arxiv.org/abs/2407.10735v2,http://arxiv.org/abs/2407.10735v2,Transforming Agency. On the mode of existence of Large Language Models,"This paper investigates the ontological characterization of Large Language
Models (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we
pay special attention to their status as agents. This requires explaining in
detail the architecture, processing, and training procedures that enable LLMs
to display their capacities, and the extensions used to turn LLMs into
agent-like systems. After a systematic analysis we conclude that a LLM fails to
meet necessary and sufficient conditions for autonomous agency in the light of
embodied theories of mind: the individuality condition (it is not the product
of its own activity, it is not even directly affected by it), the normativity
condition (it does not generate its own norms or goals), and, partially the
interactional asymmetry condition (it is not the origin and sustained source of
its interaction with the environment). If not agents, then ... what are LLMs?
We argue that ChatGPT should be characterized as an interlocutor or linguistic
automaton, a library-that-talks, devoid of (autonomous) agency, but capable to
engage performatively on non-purposeful yet purpose-structured and
purpose-bounded tasks. When interacting with humans, a ""ghostly"" component of
the human-machine interaction makes it possible to enact genuine conversational
experiences with LLMs. Despite their lack of sensorimotor and biological
embodiment, LLMs textual embodiment (the training corpus) and resource-hungry
computational embodiment, significantly transform existing forms of human
agency. Beyond assisted and extended agency, the LLM-human coupling can produce
midtended forms of agency, closer to the production of intentional agency than
to the extended instrumentality of any previous technologies.","[{'name': 'Xabier E. Barandiaran'}, {'name': 'Lola S. Almendros'}]",2024-07-15T14:01:35Z
http://arxiv.org/abs/2407.10725v1,http://arxiv.org/abs/2407.10725v1,"CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated
  Responses","The rapid progress in Large Language Models (LLMs) poses potential risks such
as generating unethical content. Assessing LLMs' values can help expose their
misalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or
close-source ones like GPT-4, to identify values reflected in generated
responses. Nevertheless, these evaluators face two challenges in open-ended
value evaluation: they should align with changing human value definitions with
minimal annotation, against their own bias (adaptability), and detect varying
value expressions and scenarios robustly (generalizability). To handle these
challenges, we introduce CLAVE, a novel framework which integrates two
complementary LLMs, a large one to extract high-level value concepts from a few
human labels, leveraging its extensive knowledge and generalizability, and a
smaller one fine-tuned on such concepts to better align with human value
understanding. This dual-model approach enables calibration with any value
systems using <100 human-labeled samples per value type. Then we present
ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples
across diverse domains, covering three major value systems. We benchmark the
capabilities of 12+ popular LLM evaluators and analyze their strengths and
weaknesses. Our findings reveal that combining fine-tuned small models and
prompt-based large ones serves as a superior balance in value evaluation.","[{'name': 'Jing Yao'}, {'name': 'Xiaoyuan Yi'}, {'name': 'Xing Xie'}]",2024-07-15T13:51:37Z
http://arxiv.org/abs/2407.10718v2,http://arxiv.org/abs/2407.10718v2,"Sibyl: Simple yet Effective Agent Framework for Complex Real-world
  Reasoning","Existing agents based on large language models (LLMs) demonstrate robust
problem-solving capabilities by integrating LLMs' inherent knowledge, strong
in-context learning and zero-shot capabilities, and the use of tools combined
with intricately designed LLM invocation workflows by humans. However, these
agents still exhibit shortcomings in long-term reasoning and under-use the
potential of existing tools, leading to noticeable deficiencies in complex
real-world reasoning scenarios. To address these limitations, we introduce
Sibyl, a simple yet powerful LLM-based agent framework designed to tackle
complex reasoning tasks by efficiently leveraging a minimal set of tools.
Drawing inspiration from Global Workspace Theory, Sibyl incorporates a global
workspace to enhance the management and sharing of knowledge and conversation
history throughout the system. Furthermore, guided by Society of Mind Theory,
Sibyl implements a multi-agent debate-based jury to self-refine the final
answers, ensuring a comprehensive and balanced approach. This approach aims to
reduce system complexity while expanding the scope of problems solvable-from
matters typically resolved by humans in minutes to those requiring hours or
even days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl
has been designed with a focus on scalability and ease of debugging by
incorporating the concept of reentrancy from functional programming from its
inception, with the aim of seamless and low effort integration in other LLM
applications to improve capabilities. Our experimental results on the GAIA
benchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves
state-of-the-art performance with an average score of 34.55%, compared to other
agents based on GPT-4. We hope that Sibyl can inspire more reliable and
reusable LLM-based agent solutions to address complex real-world reasoning
tasks.","[{'name': 'Yulong Wang'}, {'name': 'Tianhao Shen'}, {'name': 'Lifeng Liu'}, {'name': 'Jian Xie'}]",2024-07-15T13:45:40Z
http://arxiv.org/abs/2407.10701v1,http://arxiv.org/abs/2407.10701v1,DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems,"Recently, there has been a growing interest among large language model (LLM)
developers in LLM-based document reading systems, which enable users to upload
their own documents and pose questions related to the document contents, going
beyond simple reading comprehension tasks. Consequently, these systems have
been carefully designed to tackle challenges such as file parsing, metadata
extraction, multi-modal information understanding and long-context reading.
However, no current benchmark exists to evaluate their performance in such
scenarios, where a raw file and questions are provided as input, and a
corresponding response is expected as output. In this paper, we introduce
DocBench, a new benchmark designed to evaluate LLM-based document reading
systems. Our benchmark involves a meticulously crafted process, including the
recruitment of human annotators and the generation of synthetic questions. It
includes 229 real documents and 1,102 questions, spanning across five different
domains and four major types of questions. We evaluate both proprietary
LLM-based systems accessible via web interfaces or APIs, and a parse-then-read
pipeline employing open-source LLMs. Our evaluations reveal noticeable gaps
between existing LLM-based document reading systems and human performance,
underscoring the challenges of developing proficient systems. To summarize,
DocBench aims to establish a standardized benchmark for evaluating LLM-based
document reading systems under diverse real-world scenarios, thereby guiding
future advancements in this research area.","[{'name': 'Anni Zou'}, {'name': 'Wenhao Yu'}, {'name': 'Hongming Zhang'}, {'name': 'Kaixin Ma'}, {'name': 'Deng Cai'}, {'name': 'Zhuosheng Zhang'}, {'name': 'Hai Zhao'}, {'name': 'Dong Yu'}]",2024-07-15T13:17:42Z
http://arxiv.org/abs/2407.10691v1,http://arxiv.org/abs/2407.10691v1,"$\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific
  Domain through Complementary Granularity","Recent studies show the growing significance of document retrieval in the
generation of LLMs, i.e., RAG, within the scientific domain by bridging their
knowledge gap. However, dense retrievers often struggle with domain-specific
retrieval and complex query-document relationships, particularly when query
segments correspond to various parts of a document. To alleviate such prevalent
challenges, this paper introduces $\texttt{MixGR}$, which improves dense
retrievers' awareness of query-document matching across various levels of
granularity in queries and documents using a zero-shot approach.
$\texttt{MixGR}$ fuses various metrics based on these granularities to a united
score that reflects a comprehensive query-document similarity. Our experiments
demonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by
24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,
respectively, averaged on queries containing multiple subqueries from five
scientific retrieval datasets. Moreover, the efficacy of two downstream
scientific question-answering tasks highlights the advantage of
$\texttt{MixGR}$to boost the application of LLMs in the scientific domain.","[{'name': 'Fengyu Cai'}, {'name': 'Xinran Zhao'}, {'name': 'Tong Chen'}, {'name': 'Sihao Chen'}, {'name': 'Hongming Zhang'}, {'name': 'Iryna Gurevych'}, {'name': 'Heinz Koeppl'}]",2024-07-15T13:04:09Z
http://arxiv.org/abs/2407.10645v1,http://arxiv.org/abs/2407.10645v1,"Prompt Selection Matters: Enhancing Text Annotations for Social Sciences
  with Large Language Models","Large Language Models have recently been applied to text annotation tasks
from social sciences, equalling or surpassing the performance of human workers
at a fraction of the cost. However, no inquiry has yet been made on the impact
of prompt selection on labelling accuracy. In this study, we show that
performance greatly varies between prompts, and we apply the method of
automatic prompt optimization to systematically craft high quality prompts. We
also provide the community with a simple, browser-based implementation of the
method at https://prompt-ultra.github.io/ .","[{'name': 'Louis Abraham'}, {'name': 'Charles Arnal'}, {'name': 'Antoine Marie'}]",2024-07-15T12:04:32Z
http://arxiv.org/abs/2407.10629v1,http://arxiv.org/abs/2407.10629v1,Balancing the Scales: Reinforcement Learning for Fair Classification,"Fairness in classification tasks has traditionally focused on bias removal
from neural representations, but recent trends favor algorithmic methods that
embed fairness into the training process. These methods steer models towards
fair performance, preventing potential elimination of valuable information that
arises from representation manipulation. Reinforcement Learning (RL), with its
capacity for learning through interaction and adjusting reward functions to
encourage desired behaviors, emerges as a promising tool in this domain. In
this paper, we explore the usage of RL to address bias in imbalanced
classification by scaling the reward function to mitigate bias. We employ the
contextual multi-armed bandit framework and adapt three popular RL algorithms
to suit our objectives, demonstrating a novel approach to mitigating bias.","[{'name': 'Leon Eshuijs'}, {'name': 'Shihan Wang'}, {'name': 'Antske Fokkens'}]",2024-07-15T11:28:16Z
http://arxiv.org/abs/2407.10627v1,http://arxiv.org/abs/2407.10627v1,"Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated
  Chatbot Arena","Assessing the effectiveness of large language models (LLMs) presents
substantial challenges. The method of conducting human-annotated battles in an
online Chatbot Arena is a highly effective evaluative technique. However, this
approach is limited by the costs and time required for human annotation. In
this paper, we introduce Arena Learning, an innovative offline strategy
designed to simulate these arena battles using AI-driven annotations to
evaluate battle outcomes, thus facilitating the continuous improvement of the
target model through both supervised fine-tuning and reinforcement learning.
Arena Learning comprises two key elements. First, it ensures precise
evaluations and maintains consistency between offline simulations and online
competitions via WizardArena, a pipeline developed to accurately predict the
Elo rankings of various models using a meticulously designed offline test set.
Our results demonstrate that WizardArena's predictions closely align with those
from the online Arena. Second, it involves the continuous improvement of
training data based on the battle results and the refined model. We establish a
data flywheel to iteratively update the training data by highlighting the
weaknesses of the target model based on its battle results, enabling it to
learn from the strengths of multiple different models. We apply Arena Learning
to train our target model, WizardLM-$\beta$, and demonstrate significant
performance enhancements across various metrics. This fully automated training
and evaluation pipeline sets the stage for continuous advancements in various
LLMs via post-training. Notably, Arena Learning plays a pivotal role in the
success of WizardLM-2, and this paper serves both as an exploration of its
efficacy and a foundational study for future discussions related to WizardLM-2
and its derivatives.","[{'name': 'Haipeng Luo'}, {'name': 'Qingfeng Sun'}, {'name': 'Can Xu'}, {'name': 'Pu Zhao'}, {'name': 'Qingwei Lin'}, {'name': 'Jianguang Lou'}, {'name': 'Shifeng Chen'}, {'name': 'Yansong Tang'}, {'name': 'Weizhu Chen'}]",2024-07-15T11:26:07Z
http://arxiv.org/abs/2407.10626v2,http://arxiv.org/abs/2407.10626v2,"NoviCode: Generating Programs from Natural Language Utterances by
  Novices","Current Text-to-Code models demonstrate impressive capabilities in generating
executable code from natural language snippets. However, current studies focus
on technical instructions and programmer-oriented language, and it is an open
question whether these models can effectively translate natural language
descriptions given by non-technical users and express complex goals, to an
executable program that contains an intricate flow - composed of API access and
control structures as loops, conditions, and sequences. To unlock the challenge
of generating a complete program from a plain non-technical description we
present NoviCode, a novel NL Programming task, which takes as input an API and
a natural language description by a novice non-programmer and provides an
executable program as output. To assess the efficacy of models on this task, we
provide a novel benchmark accompanied by test suites wherein the generated
program code is assessed not according to their form, but according to their
functional execution. Our experiments show that, first, NoviCode is indeed a
challenging task in the code synthesis domain, and that generating complex code
from non-technical instructions goes beyond the current Text-to-Code paradigm.
Second, we show that a novel approach wherein we align the NL utterances with
the compositional hierarchical structure of the code, greatly enhances the
performance of LLMs on this task, compared with the end-to-end Text-to-Code
counterparts.","[{'name': 'Asaf Achi Mordechai'}, {'name': 'Yoav Goldberg'}, {'name': 'Reut Tsarfaty'}]",2024-07-15T11:26:03Z
http://arxiv.org/abs/2407.10603v1,http://arxiv.org/abs/2407.10603v1,"Leave No Knowledge Behind During Knowledge Distillation: Towards
  Practical and Effective Knowledge Distillation for Code-Switching ASR Using
  Realistic Data","Recent advances in automatic speech recognition (ASR) often rely on large
speech foundation models for generating high-quality transcriptions. However,
these models can be impractical due to limited computing resources. The
situation is even more severe in terms of more realistic or difficult
scenarios, such as code-switching ASR (CS-ASR). To address this, we present a
framework for developing more efficient models for CS-ASR through knowledge
distillation using realistic speech-only data. Our proposed method, Leave No
Knowledge Behind During Knowledge Distillation (K$^2$D), leverages both the
teacher model's knowledge and additional insights from a small auxiliary model.
We evaluate our approach on two in-domain and two out-domain datasets,
demonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled
realistic data, we have successfully obtained a 2-time smaller model with
5-time faster generation speed while outperforming the baseline methods and the
teacher model on all the testing sets. We have made our model publicly
available on Hugging Face
(https://huggingface.co/andybi7676/k2d-whisper.zh-en).","[{'name': 'Liang-Hsuan Tseng'}, {'name': 'Zih-Ching Chen'}, {'name': 'Wei-Shun Chang'}, {'name': 'Cheng-Kuang Lee'}, {'name': 'Tsung-Ren Huang'}, {'name': 'Hung-yi Lee'}]",2024-07-15T10:25:14Z
http://arxiv.org/abs/2407.12871v1,http://arxiv.org/abs/2407.12871v1,"MetaTool: Facilitating Large Language Models to Master Tools with
  Meta-task Augmentation","Utilizing complex tools with Large Language Models (LLMs) is a critical
component for grounding AI agents in various real-world scenarios. The core
challenge of manipulating tools lies in understanding their usage and
functionality. The prevailing approach involves few-shot prompting with
demonstrations or fine-tuning on expert trajectories. However, for complex
tools and tasks, mere in-context demonstrations may fail to cover sufficient
knowledge. Training-based methods are also constrained by the high cost of
dataset construction and limited generalizability. In this paper, we introduce
a new tool learning methodology (MetaTool) that is generalizable for mastering
any reusable toolset. Our approach includes a self-supervised data augmentation
technique that enables LLMs to gain a comprehensive understanding of various
tools, thereby improving their ability to complete tasks effectively. We
develop a series of meta-tasks that involve predicting masked factors of tool
execution. These self-supervised tasks enable the automatic generation of
high-quality QA data concerning tool comprehension. By incorporating meta-task
data into the instruction tuning process, the proposed MetaTool model achieves
significant superiority to open-source models and is comparable to
GPT-4/GPT-3.5 on multiple tool-oriented tasks.","[{'name': 'Xiaohan Wang'}, {'name': 'Dian Li'}, {'name': 'Yilin Zhao'}, {'name': 'Sinbadliu'}, {'name': 'Hui Wang'}]",2024-07-15T10:15:41Z
http://arxiv.org/abs/2407.10582v1,http://arxiv.org/abs/2407.10582v1,"Boosting Zero-Shot Crosslingual Performance using LLM-Based
  Augmentations with Effective Data Selection","Large language models (LLMs) are very proficient text generators. We leverage
this capability of LLMs to generate task-specific data via zero-shot prompting
and promote cross-lingual transfer for low-resource target languages. Given
task-specific data in a source language and a teacher model trained on this
data, we propose using this teacher to label LLM generations and employ a set
of simple data selection strategies that use the teacher's label probabilities.
Our data selection strategies help us identify a representative subset of
diverse generations that help boost zero-shot accuracies while being efficient,
in comparison to using all the LLM generations (without any subset selection).
We also highlight other important design choices that affect cross-lingual
performance such as the use of translations of source data and what labels are
best to use for the LLM generations. We observe significant performance gains
across sentiment analysis and natural language inference tasks (of up to a
maximum of 7.13 absolute points and 1.5 absolute points on average) across a
number of target languages (Hindi, Marathi, Urdu, Swahili) and domains.","[{'name': 'Barah Fazili'}, {'name': 'Ashish Sunil Agrawal'}, {'name': 'Preethi Jyothi'}]",2024-07-15T10:00:22Z
http://arxiv.org/abs/2407.10554v1,http://arxiv.org/abs/2407.10554v1,"Beyond Generative Artificial Intelligence: Roadmap for Natural Language
  Generation","Generative Artificial Intelligence has grown exponentially as a result of
Large Language Models (LLMs). This has been possible because of the impressive
performance of deep learning methods created within the field of Natural
Language Processing (NLP) and its subfield Natural Language Generation (NLG),
which is the focus of this paper. Within the growing LLM family are the popular
GPT-4, Bard and more specifically, tools such as ChatGPT have become a
benchmark for other LLMs when solving most of the tasks involved in NLG
research. This scenario poses new questions about the next steps for NLG and
how the field can adapt and evolve to deal with new challenges in the era of
LLMs. To address this, the present paper conducts a review of a representative
sample of surveys recently published in NLG. By doing so, we aim to provide the
scientific community with a research roadmap to identify which NLG aspects are
still not suitably addressed by LLMs, as well as suggest future lines of
research that should be addressed going forward.","[{'name': 'María Miró Maestre'}, {'name': 'Iván Martínez-Murillo'}, {'name': 'Tania J. Martin'}, {'name': 'Borja Navarro-Colorado'}, {'name': 'Antonio Ferrández'}, {'name': 'Armando Suárez Cueto'}, {'name': 'Elena Lloret'}]",2024-07-15T09:07:07Z
http://arxiv.org/abs/2407.10510v1,http://arxiv.org/abs/2407.10510v1,"TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription
  Prediction","Traditional Chinese medicine (TCM) relies on specific combinations of herbs
in prescriptions to treat symptoms and signs, a practice that spans thousands
of years. Predicting TCM prescriptions presents a fascinating technical
challenge with practical implications. However, this task faces limitations due
to the scarcity of high-quality clinical datasets and the intricate
relationship between symptoms and herbs. To address these issues, we introduce
DigestDS, a new dataset containing practical medical records from experienced
experts in digestive system diseases. We also propose a method, TCM-FTP (TCM
Fine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs)
through supervised fine-tuning on DigestDS. Additionally, we enhance
computational efficiency using a low-rank adaptation technique. TCM-FTP also
incorporates data augmentation by permuting herbs within prescriptions,
capitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves
an F1-score of 0.8031, surpassing previous methods significantly. Furthermore,
it demonstrates remarkable accuracy in dosage prediction, achieving a
normalized mean square error of 0.0604. In contrast, LLMs without fine-tuning
perform poorly. Although LLMs have shown capabilities on a wide range of tasks,
this work illustrates the importance of fine-tuning for TCM prescription
prediction, and we have proposed an effective way to do that.","[{'name': 'Xingzhi Zhou'}, {'name': 'Xin Dong'}, {'name': 'Chunhao Li'}, {'name': 'Yuning Bai'}, {'name': 'Yulong Xu'}, {'name': 'Ka Chun Cheung'}, {'name': 'Simon See'}, {'name': 'Xinpeng Song'}, {'name': 'Runshun Zhang'}, {'name': 'Xuezhong Zhou'}, {'name': 'Nevin L. Zhang'}]",2024-07-15T08:06:37Z
http://arxiv.org/abs/2407.10499v2,http://arxiv.org/abs/2407.10499v2,CIBench: Evaluating Your LLMs with a Code Interpreter Plugin,"While LLM-Based agents, which use external tools to solve complex problems,
have made significant progress, benchmarking their ability is challenging,
thereby hindering a clear understanding of their limitations. In this paper, we
propose an interactive evaluation framework, named CIBench, to comprehensively
assess LLMs' ability to utilize code interpreters for data science tasks. Our
evaluation framework includes an evaluation dataset and two evaluation modes.
The evaluation dataset is constructed using an LLM-human cooperative approach
and simulates an authentic workflow by leveraging consecutive and interactive
IPython sessions. The two evaluation modes assess LLMs' ability with and
without human assistance. We conduct extensive experiments to analyze the
ability of 24 LLMs on CIBench and provide valuable insights for future LLMs in
code interpreter utilization.","[{'name': 'Songyang Zhang'}, {'name': 'Chuyu Zhang'}, {'name': 'Yingfan Hu'}, {'name': 'Haowen Shen'}, {'name': 'Kuikun Liu'}, {'name': 'Zerun Ma'}, {'name': 'Fengzhe Zhou'}, {'name': 'Wenwei Zhang'}, {'name': 'Xuming He'}, {'name': 'Dahua Lin'}, {'name': 'Kai Chen'}]",2024-07-15T07:43:55Z
http://arxiv.org/abs/2407.10490v1,http://arxiv.org/abs/2407.10490v1,Learning Dynamics of LLM Finetuning,"Learning dynamics, which describes how the learning of specific training
examples influences the model's prediction of other examples, give us a
powerful tool for understanding the behavior of deep learning systems. We study
the learning dynamics of large language models during finetuning, by analyzing
the step-wise decomposition and accumulated influence among different
responses. Our framework allows a uniform interpretation of many interesting
observations about the training of popular algorithms for both instruction
tuning and preference tuning. The analysis not only explains where the benefits
of these methods come from but also inspires a simple, effective method to
further improve the alignment performance. Code for experiments is available at
https://github.com/Joshua-Ren/Learning_dynamics_LLM.","[{'name': 'Yi Ren'}, {'name': 'Danica J. Sutherland'}]",2024-07-15T07:30:28Z
http://arxiv.org/abs/2407.10488v1,http://arxiv.org/abs/2407.10488v1,How and where does CLIP process negation?,"Various benchmarks have been proposed to test linguistic understanding in
pre-trained vision \& language (VL) models. Here we build on the existence task
from the VALSE benchmark (Parcalabescu et al, 2022) which we use to test
models' understanding of negation, a particularly interesting issue for
multimodal models. However, while such VL benchmarks are useful for measuring
model performance, they do not reveal anything about the internal processes
through which these models arrive at their outputs in such visio-linguistic
tasks. We take inspiration from the growing literature on model
interpretability to explain the behaviour of VL models on the understanding of
negation. Specifically, we approach these questions through an in-depth
analysis of the text encoder in CLIP (Radford et al, 2021), a highly
influential VL model. We localise parts of the encoder that process negation
and analyse the role of attention heads in this task. Our contributions are
threefold. We demonstrate how methods from the language model interpretability
literature (such as causal tracing) can be translated to multimodal models and
tasks; we provide concrete insights into how CLIP processes negation on the
VALSE existence task; and we highlight inherent limitations in the VALSE
dataset as a benchmark for linguistic understanding.","[{'name': 'Vincent Quantmeyer'}, {'name': 'Pablo Mosteiro'}, {'name': 'Albert Gatt'}]",2024-07-15T07:20:06Z
http://arxiv.org/abs/2407.11100v3,http://arxiv.org/abs/2407.11100v3,"Building Intelligence Identification System via Large Language Model
  Watermarking: A Survey and Beyond","Large Language Models (LLMs) are increasingly integrated into diverse
industries, posing substantial security risks due to unauthorized replication
and misuse. To mitigate these concerns, robust identification mechanisms are
widely acknowledged as an effective strategy. Identification systems for LLMs
now rely heavily on watermarking technology to manage and protect intellectual
property and ensure data security. However, previous studies have primarily
concentrated on the basic principles of algorithms and lacked a comprehensive
analysis of watermarking theory and practice from the perspective of
intelligent identification. To bridge this gap, firstly, we explore how a
robust identity recognition system can be effectively implemented and managed
within LLMs by various participants using watermarking technology. Secondly, we
propose a mathematical framework based on mutual information theory, which
systematizes the identification process to achieve more precise and customized
watermarking. Additionally, we present a comprehensive evaluation of
performance metrics for LLM watermarking, reflecting participant preferences
and advancing discussions on its identification applications. Lastly, we
outline the existing challenges in current watermarking technologies and
theoretical frameworks, and provide directional guidance to address these
challenges. Our systematic classification and detailed exposition aim to
enhance the comparison and evaluation of various methods, fostering further
research and development toward a transparent, secure, and equitable LLM
ecosystem.","[{'name': 'Xuhong Wang'}, {'name': 'Haoyu Jiang'}, {'name': 'Yi Yu'}, {'name': 'Jingru Yu'}, {'name': 'Yilun Lin'}, {'name': 'Ping Yi'}, {'name': 'Yingchun Wang'}, {'name': 'Yu Qiao'}, {'name': 'Li Li'}, {'name': 'Fei-Yue Wang'}]",2024-07-15T07:20:02Z
http://arxiv.org/abs/2407.10486v1,http://arxiv.org/abs/2407.10486v1,"IDEAL: Leveraging Infinite and Dynamic Characterizations of Large
  Language Models for Query-focused Summarization","Query-focused summarization (QFS) aims to produce summaries that answer
particular questions of interest, enabling greater user control and
personalization. With the advent of large language models (LLMs), shows their
impressive capability of textual understanding through large-scale pretraining,
which implies the great potential of extractive snippet generation. In this
paper, we systematically investigated two indispensable characteristics that
the LLMs-based QFS models should be harnessed, Lengthy Document Summarization
and Efficiently Fine-grained Query-LLM Alignment, respectively.
Correspondingly, we propose two modules called Query-aware HyperExpert and
Query-focused Infini-attention to access the aforementioned characteristics.
These innovations pave the way for broader application and accessibility in the
field of QFS technology. Extensive experiments conducted on existing QFS
benchmarks indicate the effectiveness and generalizability of the proposed
approach. Our code is publicly available at
https://github.com/DCDmllm/IDEAL_Summary.","[{'name': 'Jie Cao'}, {'name': 'Dian Jiao'}, {'name': 'Qiang Yan'}, {'name': 'Wenqiao Zhang'}, {'name': 'Siliang Tang'}, {'name': 'Yueting Zhuang'}]",2024-07-15T07:14:56Z
http://arxiv.org/abs/2407.10481v1,http://arxiv.org/abs/2407.10481v1,"SuperPADL: Scaling Language-Directed Physics-Based Control with
  Progressive Supervised Distillation","Physically-simulated models for human motion can generate high-quality
responsive character animations, often in real-time. Natural language serves as
a flexible interface for controlling these models, allowing expert and
non-expert users to quickly create and edit their animations. Many recent
physics-based animation methods, including those that use text interfaces,
train control policies using reinforcement learning (RL). However, scaling
these methods beyond several hundred motions has remained challenging.
Meanwhile, kinematic animation models are able to successfully learn from
thousands of diverse motions by leveraging supervised learning methods.
Inspired by these successes, in this work we introduce SuperPADL, a scalable
framework for physics-based text-to-motion that leverages both RL and
supervised learning to train controllers on thousands of diverse motion clips.
SuperPADL is trained in stages using progressive distillation, starting with a
large number of specialized experts using RL. These experts are then
iteratively distilled into larger, more robust policies using a combination of
reinforcement learning and supervised learning. Our final SuperPADL controller
is trained on a dataset containing over 5000 skills and runs in real time on a
consumer GPU. Moreover, our policy can naturally transition between skills,
allowing for users to interactively craft multi-stage animations. We
experimentally demonstrate that SuperPADL significantly outperforms RL-based
baselines at this large data scale.","[{'name': 'Jordan Juravsky'}, {'name': 'Yunrong Guo'}, {'name': 'Sanja Fidler'}, {'name': 'Xue Bin Peng'}]",2024-07-15T07:07:11Z
http://arxiv.org/abs/2407.10457v1,http://arxiv.org/abs/2407.10457v1,"The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore
  Non-Determinism","Current evaluations of large language models (LLMs) often overlook
non-determinism, typically focusing on a single output per example. This limits
our understanding of LLM performance variability in real-world applications.
Our study addresses this issue by exploring key questions about the performance
differences between greedy decoding and sampling, identifying benchmarks'
consistency regarding non-determinism, and examining unique model behaviors.
Through extensive experiments, we observe that greedy decoding generally
outperforms sampling methods for most evaluated tasks. We also observe
consistent performance across different LLM sizes and alignment methods, noting
that alignment can reduce sampling variance. Moreover, our best-of-N sampling
approach demonstrates that smaller LLMs can match or surpass larger models such
as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This
research shows the importance of considering non-determinism in LLM evaluations
and provides insights for future LLM development and evaluation.","[{'name': 'Yifan Song'}, {'name': 'Guoyin Wang'}, {'name': 'Sujian Li'}, {'name': 'Bill Yuchen Lin'}]",2024-07-15T06:12:17Z
http://arxiv.org/abs/2407.10430v1,http://arxiv.org/abs/2407.10430v1,"Expanding the Scope: Inductive Knowledge Graph Reasoning with
  Multi-Starting Progressive Propagation","Knowledge graphs (KGs) are widely acknowledged as incomplete, and new
entities are constantly emerging in the real world. Inductive KG reasoning aims
to predict missing facts for these new entities. Among existing models, graph
neural networks (GNNs) based ones have shown promising performance for this
task. However, they are still challenged by inefficient message propagation due
to the distance and scalability issues. In this paper, we propose a new
inductive KG reasoning model, MStar, by leveraging conditional message passing
neural networks (C-MPNNs). Our key insight is to select multiple query-specific
starting entities to expand the scope of progressive propagation. To propagate
query-related messages to a farther area within limited steps, we subsequently
design a highway layer to propagate information toward these selected starting
entities. Moreover, we introduce a training strategy called LinkVerify to
mitigate the impact of noisy training samples. Experimental results validate
that MStar achieves superior performance compared with state-of-the-art models,
especially for distant entities.","[{'name': 'Zhoutian Shao'}, {'name': 'Yuanning Cui'}, {'name': 'Wei Hu'}]",2024-07-15T04:16:20Z
http://arxiv.org/abs/2408.00767v1,http://arxiv.org/abs/2408.00767v1,"Quantification and Validation for Degree of Understanding in M2M
  Semantic Communications","With the development of Artificial Intelligence (AI) and Internet of Things
(IoT) technologies, network communications based on the Shannon-Nyquist theorem
gradually reveal their limitations due to the neglect of semantic information
in the transmitted content. Semantic communication (SemCom) provides a solution
for extracting information meanings from the transmitted content. The semantic
information can be successfully interpreted by a receiver with the help of a
shared knowledge base (KB). This paper proposes a two-stage hierarchical
qualification and validation model for natural language-based
machine-to-machine (M2M) SemCom. The approach can be applied in various
applications, such as autonomous driving and edge computing. In the proposed
model, we quantitatively measure the degree of understanding (DoU) between two
communication parties at the word and sentence levels. The DoU is validated and
ensured at each level before moving to the next step. The model's effectiveness
is verified through a series of experiments, and the results show that the
quantification and validation method proposed in this paper can significantly
improve the DoU of inter-machine SemCom.","[{'name': 'Linhan Xia'}, {'name': 'Jiaxin Cai'}, {'name': 'Ricky Yuen-Tan Hou'}, {'name': 'Seon-Phil Jeong'}]",2024-07-15T03:37:42Z
http://arxiv.org/abs/2407.10385v1,http://arxiv.org/abs/2407.10385v1,"By My Eyes: Grounding Multimodal Large Language Models with Sensor Data
  via Visual Prompting","Large language models (LLMs) have demonstrated exceptional abilities across
various domains. However, utilizing LLMs for ubiquitous sensing applications
remains challenging as existing text-prompt methods show significant
performance degradation when handling long sensor data sequences. We propose a
visual prompting approach for sensor data using multimodal LLMs (MLLMs). We
design a visual prompt that directs MLLMs to utilize visualized sensor data
alongside the target sensory task descriptions. Additionally, we introduce a
visualization generator that automates the creation of optimal visualizations
tailored to a given sensory task, eliminating the need for prior task-specific
knowledge. We evaluated our approach on nine sensory tasks involving four
sensing modalities, achieving an average of 10% higher accuracy than text-based
prompts and reducing token costs by 15.8x. Our findings highlight the
effectiveness and cost-efficiency of visual prompts with MLLMs for various
sensory tasks.","[{'name': 'Hyungjun Yoon'}, {'name': 'Biniyam Aschalew Tolera'}, {'name': 'Taesik Gong'}, {'name': 'Kimin Lee'}, {'name': 'Sung-Ju Lee'}]",2024-07-15T01:33:54Z
http://arxiv.org/abs/2407.10380v1,http://arxiv.org/abs/2407.10380v1,NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models,"Cognitive textual and visual reasoning tasks, such as puzzles, series, and
analogies, demand the ability to quickly reason, decipher, and evaluate
patterns both textually and spatially. While LLMs and VLMs, through extensive
training on large amounts of human-curated data, have attained a high level of
pseudo-human intelligence in some common sense reasoning tasks, they still
struggle with more complex reasoning tasks that require cognitive
understanding. In this work, we introduce a new dataset, NTSEBench, designed to
evaluate the cognitive multi-modal reasoning and problem-solving skills of
large models. The dataset comprises 2,728 multiple-choice questions comprising
of a total of 4,642 images across 26 categories sampled from the NTSE
examination conducted nationwide in India, featuring both visual and textual
general aptitude questions that do not rely on rote learning. We establish
baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a
comparison between open source and propriety models, we propose four distinct
modeling strategies to handle different modalities (text and images) in the
dataset instances.","[{'name': 'Pranshu Pandya'}, {'name': 'Agney S Talwarr'}, {'name': 'Vatsal Gupta'}, {'name': 'Tushar Kataria'}, {'name': 'Vivek Gupta'}, {'name': 'Dan Roth'}]",2024-07-15T01:21:56Z
http://arxiv.org/abs/2407.10376v1,http://arxiv.org/abs/2407.10376v1,"Large Language Model-based FMRI Encoding of Language Functions for
  Subjects with Neurocognitive Disorder","Functional magnetic resonance imaging (fMRI) is essential for developing
encoding models that identify functional changes in language-related brain
areas of individuals with Neurocognitive Disorders (NCD). While large language
model (LLM)-based fMRI encoding has shown promise, existing studies
predominantly focus on healthy, young adults, overlooking older NCD populations
and cognitive level correlations. This paper explores language-related
functional changes in older NCD adults using LLM-based fMRI encoding and brain
scores, addressing current limitations. We analyze the correlation between
brain scores and cognitive scores at both whole-brain and language-related ROI
levels. Our findings reveal that higher cognitive abilities correspond to
better brain scores, with correlations peaking in the middle temporal gyrus.
This study highlights the potential of fMRI encoding models and brain scores
for detecting early functional changes in NCD patients.","[{'name': 'Yuejiao Wang'}, {'name': 'Xianmin Gong'}, {'name': 'Lingwei Meng'}, {'name': 'Xixin Wu'}, {'name': 'Helen Meng'}]",2024-07-15T01:09:08Z
http://arxiv.org/abs/2407.10351v1,http://arxiv.org/abs/2407.10351v1,"Comparing Complex Concepts with Transformers: Matching Patent Claims
  Against Natural Language Text","A key capability in managing patent applications or a patent portfolio is
comparing claims to other text, e.g. a patent specification. Because the
language of claims is different from language used elsewhere in the patent
application or in non-patent text, this has been challenging for computer based
natural language processing. We test two new LLM-based approaches and find that
both provide substantially better performance than previously published values.
The ability to match dense information from one domain against much more
distributed information expressed in a different vocabulary may also be useful
beyond the intellectual property space.","[{'name': 'Matthias Blume'}, {'name': 'Ghobad Heidari'}, {'name': 'Christoph Hewel'}]",2024-07-14T22:31:07Z
http://arxiv.org/abs/2407.10301v1,http://arxiv.org/abs/2407.10301v1,"Does Burrows' Delta really confirm that Rowling and Galbraith are the
  same author?","The stylo package includes a frequency table that can be used to calculate
distances between texts and thus independently solve the problem of attribution
of The Cuckoo's Calling, a novel that J.K. Rowling said she wrote. However, the
set of texts for this table is very vulnerable to criticism. The authors there
are not modern, they wrote in a different genre. I set out to test the
performance of the method on texts that are more relevant to the research
question.",[{'name': 'Boris Orekhov'}],2024-07-14T19:28:48Z
http://arxiv.org/abs/2407.10275v1,http://arxiv.org/abs/2407.10275v1,"Cross-Lingual Multi-Hop Knowledge Editing -- Benchmarks, Analysis and a
  Simple Contrastive Learning based Approach","Large language models are often expected to constantly adapt to new sources
of knowledge and knowledge editing techniques aim to efficiently patch the
outdated model knowledge, with minimal modification. Most prior works focus on
monolingual knowledge editing in English, even though new information can
emerge in any language from any part of the world. We propose the Cross-Lingual
Multi-Hop Knowledge Editing paradigm, for measuring and analyzing the
performance of various SoTA knowledge editing techniques in a cross-lingual
setup. Specifically, we create a parallel cross-lingual benchmark,
CROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive
analysis over various knowledge editing techniques uncover significant gaps in
performance between the cross-lingual and English-centric setting. Following
this, we propose a significantly improved system for cross-lingual multi-hop
knowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and
generate knowledge editing framework, where a retriever is formulated to recall
edited facts and support an LLM to adhere to knowledge edits. We develop
language-aware and hard-negative based contrastive objectives for improving the
cross-lingual and fine-grained fact retrieval and verification process used in
this framework. Extensive experiments on three LLMs, eight languages, and two
datasets show CLEVER-CKE's significant gains of up to 30% over prior methods.","[{'name': 'Aditi Khandelwal'}, {'name': 'Harman Singh'}, {'name': 'Hengrui Gu'}, {'name': 'Tianlong Chen'}, {'name': 'Kaixiong Zhou'}]",2024-07-14T17:18:16Z
http://arxiv.org/abs/2407.10266v2,http://arxiv.org/abs/2407.10266v2,"psifx -- Psychological and Social Interactions Feature Extraction
  Package","psifx is a plug-and-play multi-modal feature extraction toolkit, aiming to
facilitate and democratize the use of state-of-the-art machine learning
techniques for human sciences research. It is motivated by a need (a) to
automate and standardize data annotation processes, otherwise involving
expensive, lengthy, and inconsistent human labor, such as the transcription or
coding of behavior changes from audio and video sources; (b) to develop and
distribute open-source community-driven psychology research software; and (c)
to enable large-scale access and ease of use to non-expert users. The framework
contains an array of tools for tasks, such as speaker diarization,
closed-caption transcription and translation from audio, as well as body, hand,
and facial pose estimation and gaze tracking from video. The package has been
designed with a modular and task-oriented approach, enabling the community to
add or update new tools easily. We strongly hope that this package will provide
psychologists a simple and practical solution for efficiently a range of audio,
linguistic, and visual features from audio and video, thereby creating new
opportunities for in-depth study of real-time behavioral phenomena.","[{'name': 'Guillaume Rochette'}, {'name': 'Matthew J. Vowels'}]",2024-07-14T16:20:42Z
http://arxiv.org/abs/2407.10264v2,http://arxiv.org/abs/2407.10264v2,What Makes and Breaks Safety Fine-tuning? A Mechanistic Study,"Safety fine-tuning helps align Large Language Models (LLMs) with human
preferences for their safe deployment. To better understand the underlying
factors that make models safe via safety fine-tuning, we design a synthetic
data generation framework that captures salient aspects of an unsafe input by
modeling the interaction between the task the model is asked to perform (e.g.,
""design"") versus the specific concepts the task is asked to be performed upon
(e.g., a ""cycle"" vs. a ""bomb""). Using this, we investigate three well-known
safety fine-tuning methods -- supervised safety fine-tuning, direct preference
optimization, and unlearning -- and provide significant evidence demonstrating
that these methods minimally transform MLP weights to specifically align unsafe
inputs into its weights' null space. This yields a clustering of inputs based
on whether the model deems them safe or not. Correspondingly, when an
adversarial input (e.g., a jailbreak) is provided, its activations are closer
to safer samples, leading to the model processing such an input as if it were
safe. We validate our findings, wherever possible, on real-world models --
specifically, Llama-2 7B and Llama-3 8B.","[{'name': 'Samyak Jain'}, {'name': 'Ekdeep Singh Lubana'}, {'name': 'Kemal Oksuz'}, {'name': 'Tom Joy'}, {'name': 'Philip H. S. Torr'}, {'name': 'Amartya Sanyal'}, {'name': 'Puneet K. Dokania'}]",2024-07-14T16:12:57Z
http://arxiv.org/abs/2407.10252v1,http://arxiv.org/abs/2407.10252v1,"Nullpointer at CheckThat! 2024: Identifying Subjectivity from
  Multilingual Text Sequence","This study addresses a binary classification task to determine whether a text
sequence, either a sentence or paragraph, is subjective or objective. The task
spans five languages: Arabic, Bulgarian, English, German, and Italian, along
with a multilingual category. Our approach involved several key techniques.
Initially, we preprocessed the data through parts of speech (POS) tagging,
identification of question marks, and application of attention masks. We
fine-tuned the sentiment-based Transformer model
'MarieAngeA13/Sentiment-Analysis-BERT' on our dataset. Given the imbalance with
more objective data, we implemented a custom classifier that assigned greater
weight to objective data. Additionally, we translated non-English data into
English to maintain consistency across the dataset. Our model achieved notable
results, scoring top marks for the multilingual dataset (Macro F1=0.7121) and
German (Macro F1=0.7908). It ranked second for Arabic (Macro F1=0.4908) and
Bulgarian (Macro F1=0.7169), third for Italian (Macro F1=0.7430), and ninth for
English (Macro F1=0.6893).","[{'name': 'Md. Rafiul Biswas'}, {'name': 'Abrar Tasneem Abir'}, {'name': 'Wajdi Zaghouani'}]",2024-07-14T15:37:28Z
http://arxiv.org/abs/2407.10245v1,http://arxiv.org/abs/2407.10245v1,"GenSco: Can Question Decomposition based Passage Alignment improve
  Question Answering?","Retrieval augmented generation (RAG) with large language models (LLMs) for
Question Answering (QA) entails furnishing relevant context within the prompt
to facilitate the LLM in answer generation. During the generation, inaccuracies
or hallucinations frequently occur due to two primary factors: inadequate or
distracting context in the prompts, and the inability of LLMs to effectively
reason through the facts. In this paper, we investigate whether providing
aligned context via a carefully selected passage sequence leads to better
answer generation by the LLM for multi-hop QA. We introduce, ""GenSco"", a novel
approach of selecting passages based on the predicted decomposition of the
multi-hop questions}. The framework consists of two distinct LLMs: (i)
Generator LLM, which is used for question decomposition and final answer
generation; (ii) an auxiliary open-sourced LLM, used as the scorer, to
semantically guide the Generator for passage selection. The generator is
invoked only once for the answer generation, resulting in a cost-effective and
efficient approach. We evaluate on three broadly established multi-hop question
answering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve
an absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect
to the best performing baselines over MuSiQue and 2WikiMultiHop respectively.","[{'name': 'Barah Fazili'}, {'name': 'Koustava Goswami'}, {'name': 'Natwar Modani'}, {'name': 'Inderjeet Nair'}]",2024-07-14T15:25:08Z
http://arxiv.org/abs/2407.10153v1,http://arxiv.org/abs/2407.10153v1,"Look Within, Why LLMs Hallucinate: A Causal Perspective","The emergence of large language models (LLMs) is a milestone in generative
artificial intelligence, achieving significant success in text comprehension
and generation tasks. Despite the tremendous success of LLMs in many downstream
tasks, they suffer from severe hallucination problems, posing significant
challenges to the practical applications of LLMs. Most of the works about LLMs'
hallucinations focus on data quality. Self-attention is a core module in
transformer-based LLMs, while its potential relationship with LLMs'
hallucination has been hardly investigated. To fill this gap, we study this
problem from a causal perspective. We propose a method to intervene in LLMs'
self-attention layers and maintain their structures and sizes intact.
Specifically, we disable different self-attention layers in several popular
open-source LLMs and then compare their degrees of hallucination with the
original ones. We evaluate the intervened LLMs on hallucination assessment
benchmarks and conclude that disabling some specific self-attention layers in
the front or tail of the LLMs can alleviate hallucination issues. The study
paves a new way for understanding and mitigating LLMs' hallucinations.","[{'name': 'He Li'}, {'name': 'Haoang Chi'}, {'name': 'Mingyu Liu'}, {'name': 'Wenjing Yang'}]",2024-07-14T10:47:44Z
http://arxiv.org/abs/2407.10152v1,http://arxiv.org/abs/2407.10152v1,"Mitigating Translationese in Low-resource Languages: The Storyboard
  Approach","Low-resource languages often face challenges in acquiring high-quality
language data due to the reliance on translation-based methods, which can
introduce the translationese effect. This phenomenon results in translated
sentences that lack fluency and naturalness in the target language. In this
paper, we propose a novel approach for data collection by leveraging
storyboards to elicit more fluent and natural sentences. Our method involves
presenting native speakers with visual stimuli in the form of storyboards and
collecting their descriptions without direct exposure to the source text. We
conducted a comprehensive evaluation comparing our storyboard-based approach
with traditional text translation-based methods in terms of accuracy and
fluency. Human annotators and quantitative metrics were used to assess
translation quality. The results indicate a preference for text translation in
terms of accuracy, while our method demonstrates worse accuracy but better
fluency in the language focused.","[{'name': 'Garry Kuwanto'}, {'name': 'Eno-Abasi E. Urua'}, {'name': 'Priscilla Amondi Amuok'}, {'name': 'Shamsuddeen Hassan Muhammad'}, {'name': 'Anuoluwapo Aremu'}, {'name': 'Verrah Otiende'}, {'name': 'Loice Emma Nanyanga'}, {'name': 'Teresiah W. Nyoike'}, {'name': 'Aniefon D. Akpan'}, {'name': 'Nsima Ab Udouboh'}, {'name': 'Idongesit Udeme Archibong'}, {'name': 'Idara Effiong Moses'}, {'name': 'Ifeoluwatayo A. Ige'}, {'name': 'Benjamin Ajibade'}, {'name': 'Olumide Benjamin Awokoya'}, {'name': 'Idris Abdulmumin'}, {'name': 'Saminu Mohammad Aliyu'}, {'name': 'Ruqayya Nasir Iro'}, {'name': 'Ibrahim Said Ahmad'}, {'name': 'Deontae Smith'}, {'name': 'Praise-EL Michaels'}, {'name': 'David Ifeoluwa Adelani'}, {'name': 'Derry Tanti Wijaya'}, {'name': 'Anietie Andy'}]",2024-07-14T10:47:03Z
http://arxiv.org/abs/2407.10118v1,http://arxiv.org/abs/2407.10118v1,Textless Dependency Parsing by Labeled Sequence Prediction,"Traditional spoken language processing involves cascading an automatic speech
recognition (ASR) system into text processing models. In contrast, ""textless""
methods process speech representations without ASR systems, enabling the direct
use of acoustic speech features. Although their effectiveness is shown in
capturing acoustic features, it is unclear in capturing lexical knowledge. This
paper proposes a textless method for dependency parsing, examining its
effectiveness and limitations. Our proposed method predicts a dependency tree
from a speech signal without transcribing, representing the tree as a labeled
sequence. scading method outperforms the textless method in overall parsing
accuracy, the latter excels in instances with important acoustic features. Our
findings highlight the importance of fusing word-level representations and
sentence-level prosody for enhanced parsing performance. The code and models
are made publicly available: https://github.com/mynlp/SpeechParser.","[{'name': 'Shunsuke Kando'}, {'name': 'Yusuke Miyao'}, {'name': 'Jason Naradowsky'}, {'name': 'Shinnosuke Takamichi'}]",2024-07-14T08:38:14Z
http://arxiv.org/abs/2407.10114v2,http://arxiv.org/abs/2407.10114v2,"TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley
  Value Estimation","As large language models (LLMs) become increasingly prevalent in critical
applications, the need for interpretable AI has grown. We introduce TokenSHAP,
a novel method for interpreting LLMs by attributing importance to individual
tokens or substrings within input prompts. This approach adapts Shapley values
from cooperative game theory to natural language processing, offering a
rigorous framework for understanding how different parts of an input contribute
to a model's response. TokenSHAP leverages Monte Carlo sampling for
computational efficiency, providing interpretable, quantitative measures of
token importance. We demonstrate its efficacy across diverse prompts and LLM
architectures, showing consistent improvements over existing baselines in
alignment with human judgments, faithfulness to model behavior, and
consistency.
  Our method's ability to capture nuanced interactions between tokens provides
valuable insights into LLM behavior, enhancing model transparency, improving
prompt engineering, and aiding in the development of more reliable AI systems.
TokenSHAP represents a significant step towards the necessary interpretability
for responsible AI deployment, contributing to the broader goal of creating
more transparent, accountable, and trustworthy AI systems.","[{'name': 'Roni Goldshmidt'}, {'name': 'Miriam Horovicz'}]",2024-07-14T08:07:50Z
http://arxiv.org/abs/2407.10091v1,http://arxiv.org/abs/2407.10091v1,"Enhancing Emotion Prediction in News Headlines: Insights from ChatGPT
  and Seq2Seq Models for Free-Text Generation","Predicting emotions elicited by news headlines can be challenging as the task
is largely influenced by the varying nature of people's interpretations and
backgrounds. Previous works have explored classifying discrete emotions
directly from news headlines. We provide a different approach to tackling this
problem by utilizing people's explanations of their emotion, written in
free-text, on how they feel after reading a news headline. Using the dataset
BU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the
free-text explanations have a strong correlation with the dominant emotion
elicited by the headlines. The free-text explanations also contain more
sentimental context than the news headlines alone and can serve as a better
input to emotion classification models. Therefore, in this work we explored
generating emotion explanations from headlines by training a
sequence-to-sequence transformer model and by using pretrained large language
model, ChatGPT (GPT-4). We then used the generated emotion explanations for
emotion classification. In addition, we also experimented with training the
pretrained T5 model for the intermediate task of explanation generation before
fine-tuning it for emotion classification. Using McNemar's significance test,
methods that incorporate GPT-generated free-text emotion explanations
demonstrated significant improvement (P-value < 0.05) in emotion classification
from headlines, compared to methods that only use headlines. This underscores
the value of using intermediate free-text explanations for emotion prediction
tasks with headlines.","[{'name': 'Ge Gao'}, {'name': 'Jongin Kim'}, {'name': 'Sejin Paik'}, {'name': 'Ekaterina Novozhilova'}, {'name': 'Yi Liu'}, {'name': 'Sarah T. Bonna'}, {'name': 'Margrit Betke'}, {'name': 'Derry Tanti Wijaya'}]",2024-07-14T06:04:11Z
http://arxiv.org/abs/2407.10086v2,http://arxiv.org/abs/2407.10086v2,"Rapid Biomedical Research Classification: The Pandemic PACT Advanced
  Categorisation Engine","This paper introduces the Pandemic PACT Advanced Categorisation Engine
(PPACE) along with its associated dataset. PPACE is a fine-tuned model
developed to automatically classify research abstracts from funded biomedical
projects according to WHO-aligned research priorities. This task is crucial for
monitoring research trends and identifying gaps in global health preparedness
and response. Our approach builds on human-annotated projects, which are
allocated one or more categories from a predefined list. A large language model
is then used to generate `rationales' explaining the reasoning behind these
annotations. This augmented data, comprising expert annotations and rationales,
is subsequently used to fine-tune a smaller, more efficient model. Developed as
part of the Pandemic PACT project, which aims to track and analyse research
funding and clinical evidence for a wide range of diseases with outbreak
potential, PPACE supports informed decision-making by research funders,
policymakers, and independent researchers. We introduce and release both the
trained model and the instruction-based dataset used for its training. Our
evaluation shows that PPACE significantly outperforms its baselines. The
release of PPACE and its associated dataset offers valuable resources for
researchers in multilabel biomedical document classification and supports
advancements in aligning biomedical research with key global health priorities.","[{'name': 'Omid Rohanian'}, {'name': 'Mohammadmahdi Nouriborji'}, {'name': 'Olena Seminog'}, {'name': 'Rodrigo Furst'}, {'name': 'Thomas Mendy'}, {'name': 'Shanthi Levanita'}, {'name': 'Zaharat Kadri-Alabi'}, {'name': 'Nusrat Jabin'}, {'name': 'Daniela Toale'}, {'name': 'Georgina Humphreys'}, {'name': 'Emilia Antonio'}, {'name': 'Adrian Bucher'}, {'name': 'Alice Norton'}, {'name': 'David A. Clifton'}]",2024-07-14T05:22:53Z
http://arxiv.org/abs/2407.10049v1,http://arxiv.org/abs/2407.10049v1,AutoGRAMS: Autonomous Graphical Agent Modeling Software,"We introduce the AutoGRAMS framework for programming multi-step interactions
with language models. AutoGRAMS represents AI agents as a graph, where each
node can execute either a language modeling instruction or traditional code.
Likewise, transitions in the graph can be governed by either language modeling
decisions or traditional branch logic. AutoGRAMS supports using variables as
memory and allows nodes to call other AutoGRAMS graphs as functions. We show
how AutoGRAMS can be used to design highly sophisticated agents, including
self-referential agents that can modify their own graph. AutoGRAMS's
graph-centric approach aids interpretability, controllability, and safety
during the design, development, and deployment of AI agents. We provide our
framework as open source at https://github.com/autograms/autograms .","[{'name': 'Ben Krause'}, {'name': 'Lucia Chen'}, {'name': 'Emmanuel Kahembwe'}]",2024-07-14T02:25:45Z
http://arxiv.org/abs/2407.10021v1,http://arxiv.org/abs/2407.10021v1,"Document-level Clinical Entity and Relation Extraction via Knowledge
  Base-Guided Generation","Generative pre-trained transformer (GPT) models have shown promise in
clinical entity and relation extraction tasks because of their precise
extraction and contextual understanding capability. In this work, we further
leverage the Unified Medical Language System (UMLS) knowledge base to
accurately identify medical concepts and improve clinical entity and relation
extraction at the document level. Our framework selects UMLS concepts relevant
to the text and combines them with prompts to guide language models in
extracting entities. Our experiments demonstrate that this initial concept
mapping and the inclusion of these mapped concepts in the prompts improves
extraction results compared to few-shot extraction tasks on generic language
models that do not leverage UMLS. Further, our results show that this approach
is more effective than the standard Retrieval Augmented Generation (RAG)
technique, where retrieved data is compared with prompt embeddings to generate
results. Overall, we find that integrating UMLS concepts with GPT models
significantly improves entity and relation identification, outperforming the
baseline and RAG models. By combining the precise concept mapping capability of
knowledge-based approaches like UMLS with the contextual understanding
capability of GPT, our method highlights the potential of these approaches in
specialized domains like healthcare.","[{'name': 'Kriti Bhattarai'}, {'name': 'Inez Y. Oh'}, {'name': 'Zachary B. Abrams'}, {'name': 'Albert M. Lai'}]",2024-07-13T22:45:46Z
http://arxiv.org/abs/2407.10020v1,http://arxiv.org/abs/2407.10020v1,"Causality extraction from medical text using Large Language Models
  (LLMs)","This study explores the potential of natural language models, including large
language models, to extract causal relations from medical texts, specifically
from Clinical Practice Guidelines (CPGs). The outcomes causality extraction
from Clinical Practice Guidelines for gestational diabetes are presented,
marking a first in the field. We report on a set of experiments using variants
of BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs),
namely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better
than other models, including the Large Language Models, with an average
F1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less
consistency. We also release the code and an annotated a corpus of causal
statements within the Clinical Practice Guidelines for gestational diabetes.","[{'name': 'Seethalakshmi Gopalakrishnan'}, {'name': 'Luciana Garbayo'}, {'name': 'Wlodek Zadrozny'}]",2024-07-13T22:33:29Z
http://arxiv.org/abs/2407.10005v1,http://arxiv.org/abs/2407.10005v1,"Fine-grained Analysis of In-context Linear Estimation: Data,
  Architecture, and Beyond","Recent research has shown that Transformers with linear attention are capable
of in-context learning (ICL) by implementing a linear estimator through
gradient descent steps. However, the existing results on the optimization
landscape apply under stylized settings where task and feature vectors are
assumed to be IID and the attention weights are fully parameterized. In this
work, we develop a stronger characterization of the optimization and
generalization landscape of ICL through contributions on architectures,
low-rank parameterization, and correlated designs: (1) We study the landscape
of 1-layer linear attention and 1-layer H3, a state-space model. Under a
suitable correlated design assumption, we prove that both implement 1-step
preconditioned gradient descent. We show that thanks to its native convolution
filters, H3 also has the advantage of implementing sample weighting and
outperforming linear attention in suitable settings. (2) By studying correlated
designs, we provide new risk bounds for retrieval augmented generation (RAG)
and task-feature alignment which reveal how ICL sample complexity benefits from
distributional alignment. (3) We derive the optimal risk for low-rank
parameterized attention weights in terms of covariance spectrum. Through this,
we also shed light on how LoRA can adapt to a new distribution by capturing the
shift between task covariances. Experimental results corroborate our
theoretical findings. Overall, this work explores the optimization and risk
landscape of ICL in practically meaningful settings and contributes to a more
thorough understanding of its mechanics.","[{'name': 'Yingcong Li'}, {'name': 'Ankit Singh Rawat'}, {'name': 'Samet Oymak'}]",2024-07-13T21:13:55Z
http://arxiv.org/abs/2407.09893v1,http://arxiv.org/abs/2407.09893v1,"Synergistic Multi-Agent Framework with Trajectory Learning for
  Knowledge-Intensive Tasks","Recent advancements in Large Language Models (LLMs) have led to significant
breakthroughs in various natural language processing tasks. However, generating
factually consistent responses in knowledge-intensive scenarios remains a
challenge due to issues such as hallucination, difficulty in acquiring
long-tailed knowledge, and limited memory expansion. This paper introduces
SMART, a novel multi-agent framework that leverages external knowledge to
enhance the interpretability and factual consistency of LLM-generated
responses. SMART comprises four specialized agents, each performing a specific
sub-trajectory action to navigate complex knowledge-intensive tasks. We propose
a multi-agent co-training paradigm, Long- and Short-Trajectory Learning, which
ensures synergistic collaboration among agents while maintaining fine-grained
execution by each agent. Extensive experiments on 5 tasks demonstrate SMART's
superior performance compared to previous widely adopted methods.","[{'name': 'Shengbin Yue'}, {'name': 'Siyuan Wang'}, {'name': 'Wei Chen'}, {'name': 'Xuanjing Huang'}, {'name': 'Zhongyu Wei'}]",2024-07-13T13:58:24Z
http://arxiv.org/abs/2407.09888v1,http://arxiv.org/abs/2407.09888v1,"FarFetched: Entity-centric Reasoning and Claim Validation for the Greek
  Language based on Textually Represented Environments","Our collective attention span is shortened by the flood of online
information. With \textit{FarFetched}, we address the need for automated claim
validation based on the aggregated evidence derived from multiple online news
sources. We introduce an entity-centric reasoning framework in which latent
connections between events, actions, or statements are revealed via entity
mentions and represented in a graph database. Using entity linking and semantic
similarity, we offer a way for collecting and combining information from
diverse sources in order to generate evidence relevant to the user's claim.
Then, we leverage textual entailment recognition to quantitatively determine
whether this assertion is credible, based on the created evidence. Our approach
tries to fill the gap in automated claim validation for less-resourced
languages and is showcased on the Greek language, complemented by the training
of relevant semantic textual similarity (STS) and natural language inference
(NLI) models that are evaluated on translated versions of common benchmarks.","[{'name': 'Dimitris Papadopoulos'}, {'name': 'Katerina Metropoulou'}, {'name': 'Nikolaos Matsatsinis'}, {'name': 'Nikolaos Papadakis'}]",2024-07-13T13:30:20Z
http://arxiv.org/abs/2407.09886v1,http://arxiv.org/abs/2407.09886v1,"Speech-Copilot: Leveraging Large Language Models for Speech Processing
  via Task Decomposition, Modularization, and Program Generation","In this work, we introduce Speech-Copilot, a modular framework for
instruction-oriented speech-processing tasks that minimizes human effort in
toolset construction. Unlike end-to-end methods using large audio-language
models, Speech-Copilot builds speech processing-specific toolsets by analyzing
pre-collected task instructions and breaking tasks into manageable sub-tasks.
It features a flexible agent based on large language models that performs tasks
through program generation. Our approach achieves state-of-the-art performance
on the Dynamic-SUPERB benchmark, demonstrating its effectiveness across diverse
speech-processing tasks. Key contributions include: 1) developing an innovative
framework for speech processing-specific toolset construction, 2) establishing
a high-performing agent based on large language models, and 3) offering a new
perspective on addressing challenging instruction-oriented speech-processing
tasks. Without additional training processes required by end-to-end approaches,
our method provides a flexible and extendable solution for a wide range of
speech-processing applications.","[{'name': 'Chun-Yi Kuan'}, {'name': 'Chih-Kai Yang'}, {'name': 'Wei-Ping Huang'}, {'name': 'Ke-Han Lu'}, {'name': 'Hung-yi Lee'}]",2024-07-13T13:26:43Z
http://arxiv.org/abs/2407.09879v2,http://arxiv.org/abs/2407.09879v2,"sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through
  N-shot Guided Prompting","Despite the remarkable success of LLMs in English, there is a significant gap
in performance in non-English languages. In order to address this, we introduce
a novel recipe for creating a multilingual synthetic instruction tuning
dataset, sPhinX, which is created by selectively translating instruction
response pairs from English into 50 languages. We test the effectiveness of
sPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and
Mistral-7B and then evaluating them across a comprehensive suite of
multilingual benchmarks that test reasoning, question answering, and reading
comprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with
sPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared
to the baselines. We also devise a strategy to incorporate N-shot examples in
each fine-tuning sample which further boosts the performance of these models by
3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other
multilingual instruction tuning datasets on the same benchmarks along with
being sample efficient and diverse, thereby reducing dataset creation costs.
Additionally, instruction tuning with sPhinX does not lead to regression on
most standard LLM benchmarks.","[{'name': 'Sanchit Ahuja'}, {'name': 'Kumar Tanmay'}, {'name': 'Hardik Hansrajbhai Chauhan'}, {'name': 'Barun Patra'}, {'name': 'Kriti Aggarwal'}, {'name': 'Luciano Del Corro'}, {'name': 'Arindam Mitra'}, {'name': 'Tejas Indulal Dhamecha'}, {'name': 'Ahmed Awadallah'}, {'name': 'Monojit Choudhary'}, {'name': 'Vishrav Chaudhary'}, {'name': 'Sunayana Sitaram'}]",2024-07-13T13:03:45Z
http://arxiv.org/abs/2407.09849v1,http://arxiv.org/abs/2407.09849v1,Text-Based Detection of On-Hold Scripts in Contact Center Calls,"Average hold time is a concern for call centers because it affects customer
satisfaction. Contact centers should instruct their agents to use special
on-hold scripts to maintain positive interactions with clients. This study
presents a natural language processing model that detects on-hold phrases in
customer service calls transcribed by automatic speech recognition technology.
The task of finding hold scripts in dialogue was formulated as a multiclass
text classification problem with three mutually exclusive classes: scripts for
putting a client on hold, scripts for returning to a client, and phrases
irrelevant to on-hold scripts. We collected an in-house dataset of calls and
labeled each dialogue turn in each call. We fine-tuned RuBERT on the dataset by
exploring various hyperparameter sets and achieved high model performance. The
developed model can help agent monitoring by providing a way to check whether
an agent follows predefined on-hold scripts.","[{'name': 'Dmitrii Galimzianov'}, {'name': 'Viacheslav Vyshegorodtsev'}]",2024-07-13T11:11:41Z
http://arxiv.org/abs/2407.09835v2,http://arxiv.org/abs/2407.09835v2,"Investigating Low-Rank Training in Transformer Language Models:
  Efficiency and Scaling Analysis","State-of-the-art LLMs often rely on scale with high computational costs,
which has sparked a research agenda to reduce parameter counts and costs
without significantly impacting performance. Our study focuses on
Transformer-based LLMs, specifically applying low-rank parametrization to the
computationally intensive feedforward networks (FFNs), which are less studied
than attention blocks. In contrast to previous works, (i) we explore low-rank
parametrization at scale, up to 1.3B parameters; (ii) within Transformer
language models rather than convolutional architectures; and (iii) starting
from training from scratch. Experiments on the large RefinedWeb dataset show
that low-rank parametrization is both efficient (e.g., 2.6$\times$ FFN speed-up
with 32\% parameters) and effective during training. Interestingly, these
structured FFNs exhibit steeper scaling curves than the original models.
Motivated by this finding, we develop the wide and structured networks
surpassing the current medium-sized and large-sized Transformer in perplexity
and throughput performance. Our code is available at
https://github.com/CLAIRE-Labo/StructuredFFN/tree/main.","[{'name': 'Xiuying Wei'}, {'name': 'Skander Moalla'}, {'name': 'Razvan Pascanu'}, {'name': 'Caglar Gulcehre'}]",2024-07-13T10:08:55Z
http://arxiv.org/abs/2407.09817v1,http://arxiv.org/abs/2407.09817v1,"Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech
  Recognition System","Multi-talker speech recognition and target-talker speech recognition, both
involve transcription in multi-talker contexts, remain significant challenges.
However, existing methods rarely attempt to simultaneously address both tasks.
In this study, we propose a pioneering approach to empower Whisper, which is a
speech foundation model, to tackle joint multi-talker and target-talker speech
recognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar
separator into its encoder to separate mixed embedding for multiple talkers;
(ii) a Target Talker Identifier is introduced to identify the embedding flow of
the target talker on the fly, requiring only three-second enrollment speech as
a cue; (iii) soft prompt tuning for decoder is explored for better task
adaptation. Our method outperforms previous methods on two- and three-talker
LibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable
zero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.","[{'name': 'Lingwei Meng'}, {'name': 'Jiawen Kang'}, {'name': 'Yuejiao Wang'}, {'name': 'Zengrui Jin'}, {'name': 'Xixin Wu'}, {'name': 'Xunying Liu'}, {'name': 'Helen Meng'}]",2024-07-13T09:28:24Z
http://arxiv.org/abs/2407.09816v3,http://arxiv.org/abs/2407.09816v3,"MaskMoE: Boosting Token-Level Learning via Routing Mask in
  Mixture-of-Experts","Scaling the size of a model enhances its capabilities but significantly
increases computation complexity. Mixture-of-Experts models (MoE) address the
issue by allowing model size to scale up without substantially increasing
training or inference costs. In MoE, there is an important module called the
router, which is used to distribute each token to the experts. Currently, the
mainstream routing methods include dynamic routing and fixed routing. Despite
their promising results, MoE models encounter several challenges. Primarily,
for dynamic routing methods, the dispersion of training tokens across multiple
experts can lead to underfitting, particularly for infrequent tokens.
Additionally, though fixed routing methods can mitigate that issue, they
compromise on the diversity of representations. In this paper, we propose
\textbf{MaskMoE}, a method designed to enhance token-level learning by
employing a routing \textbf{mask}ing technique within the
\textbf{M}ixture-\textbf{o}f-\textbf{E}xperts model. MaskMoE is capable of
maintaining representation diversity while achieving more comprehensive
training. Experimental results demonstrate that our method outperforms previous
dominant Mixture-of-Experts models in terms of both perplexity (PPL) and
downstream task performance.","[{'name': 'Zhenpeng Su'}, {'name': 'Zijia Lin'}, {'name': 'Xue Bai'}, {'name': 'Xing Wu'}, {'name': 'Yizhe Xiong'}, {'name': 'Haoran Lian'}, {'name': 'Guangyuan Ma'}, {'name': 'Hui Chen'}, {'name': 'Guiguang Ding'}, {'name': 'Wei Zhou'}, {'name': 'Songlin Hu'}]",2024-07-13T09:22:33Z
http://arxiv.org/abs/2407.09801v1,http://arxiv.org/abs/2407.09801v1,IoT-LM: Large Multisensory Language Models for the Internet of Things,"The Internet of Things (IoT) network integrating billions of smart physical
devices embedded with sensors, software, and communication technologies is a
critical and rapidly expanding component of our modern world. The IoT ecosystem
provides a rich source of real-world modalities such as motion, thermal,
geolocation, imaging, depth, sensors, and audio to recognize the states of
humans and physical objects. Machine learning presents a rich opportunity to
automatically process IoT data at scale, enabling efficient inference for
understanding human wellbeing, controlling physical devices, and
interconnecting smart cities. To realize this potential, we introduce IoT-LM,
an open-source large multisensory language model tailored for the IoT
ecosystem. IoT-LM is enabled by two technical contributions: the first is
MultiIoT, the most expansive unified IoT dataset to date, encompassing over
1.15 million samples from 12 modalities and 8 tasks prepared for multisensory
pre-training and instruction-tuning. The second is a new multisensory multitask
adapter layer to condition pre-trained large language models on multisensory
IoT data. Not only does IoT-LM yield substantial improvements on 8 supervised
IoT classification tasks, but it also demonstrates new interactive
question-answering, reasoning, and dialog capabilities conditioned on IoT
sensors. We release IoT-LM's data sources and new multisensory language
modeling framework.","[{'name': 'Shentong Mo'}, {'name': 'Russ Salakhutdinov'}, {'name': 'Louis-Philippe Morency'}, {'name': 'Paul Pu Liang'}]",2024-07-13T08:20:37Z
http://arxiv.org/abs/2407.12866v1,http://arxiv.org/abs/2407.12866v1,Beyond KV Caching: Shared Attention for Efficient LLMs,"The efficiency of large language models (LLMs) remains a critical challenge,
particularly in contexts where computational resources are limited. Traditional
attention mechanisms in these models, while powerful, require significant
computational and memory resources due to the necessity of recalculating and
storing attention weights across different layers. This paper introduces a
novel Shared Attention (SA) mechanism, designed to enhance the efficiency of
LLMs by directly sharing computed attention weights across multiple layers.
Unlike previous methods that focus on sharing intermediate Key-Value (KV)
caches, our approach utilizes the isotropic tendencies of attention
distributions observed in advanced LLMs post-pretraining to reduce both the
computational flops and the size of the KV cache required during inference. We
empirically demonstrate that implementing SA across various LLMs results in
minimal accuracy loss on standard benchmarks. Our findings suggest that SA not
only conserves computational resources but also maintains robust model
performance, thereby facilitating the deployment of more efficient LLMs in
resource-constrained environments.","[{'name': 'Bingli Liao'}, {'name': 'Danilo Vasconcellos Vargas'}]",2024-07-13T07:23:07Z
http://arxiv.org/abs/2407.09756v1,http://arxiv.org/abs/2407.09756v1,"LLM-Collaboration on Automatic Science Journalism for the General
  Audience","Science journalism reports current scientific discoveries to non-specialists,
aiming to enable public comprehension of the state of the art. However, this
task can be challenging as the audience often lacks specific knowledge about
the presented research. To address this challenge, we propose a framework that
integrates three LLMs mimicking the real-world
writing-reading-feedback-revision workflow, with one LLM acting as the
journalist, a smaller LLM as the general public reader, and the third LLM as an
editor. The journalist's writing is iteratively refined by feedback from the
reader and suggestions from the editor. Our experiments demonstrate that by
leveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can
generate articles that are more accessible than those generated by existing
methods, including advanced models such as GPT-4.","[{'name': 'Gongyao Jiang'}, {'name': 'Xinran Shi'}, {'name': 'Qiong Luo'}]",2024-07-13T03:31:35Z
http://arxiv.org/abs/2407.09726v1,http://arxiv.org/abs/2407.09726v1,On Mitigating Code LLM Hallucinations with API Documentation,"In this study, we address the issue of API hallucinations in various software
engineering contexts. We introduce CloudAPIBench, a new benchmark designed to
measure API hallucination occurrences. CloudAPIBench also provides annotations
for frequencies of API occurrences in the public domain, allowing us to study
API hallucinations at various frequency levels. Our findings reveal that Code
LLMs struggle with low frequency APIs: for e.g., GPT-4o achieves only 38.58%
valid low frequency API invocations. We demonstrate that Documentation
Augmented Generation (DAG) significantly improves performance for low frequency
APIs (increase to 47.94% with DAG) but negatively impacts high frequency APIs
when using sub-optimal retrievers (a 39.02% absolute drop). To mitigate this,
we propose to intelligently trigger DAG where we check against an API index or
leverage Code LLMs' confidence scores to retrieve only when needed. We
demonstrate that our proposed methods enhance the balance between low and high
frequency API performance, resulting in more reliable API invocations (8.20%
absolute improvement on CloudAPIBench for GPT-4o).","[{'name': 'Nihal Jain'}, {'name': 'Robert Kwiatkowski'}, {'name': 'Baishakhi Ray'}, {'name': 'Murali Krishna Ramanathan'}, {'name': 'Varun Kumar'}]",2024-07-13T00:16:26Z
http://arxiv.org/abs/2407.09722v1,http://arxiv.org/abs/2407.09722v1,"Multi-Token Joint Speculative Decoding for Accelerating Large Language
  Model Inference","Transformer-based Large language models (LLMs) have demonstrated their power
in various tasks, but their inference incurs significant time and energy costs.
To accelerate LLM inference, speculative decoding uses a smaller model to
propose one sequence of tokens, which are subsequently validated in batch by
the target large model. Compared with autoregressive decoding, speculative
decoding generates the same number of tokens with fewer runs of the large
model, hence accelerating the overall inference by $1$-$2\times$. However,
greedy decoding is not the optimal decoding algorithm in terms of output
perplexity, which is a direct measurement of the effectiveness of a decoding
algorithm. An algorithm that has better output perplexity and even better
efficiency than speculative decoding can be more useful in practice. To achieve
this seemingly contradictory goal, we first introduce multi-token joint greedy
decoding (MJGD), which greedily generates multiple tokens at each step based on
their joint perplexity. We show that it leads to better perplexity for the
whole output. But the computation cost of MJGD is infeasible in practice. So we
further propose multi-token joint speculative decoding (MJSD), which
approximates and accelerates the MJGD from two aspects: it approximates the
joint distribution of the large model with that of a small model, and uses a
verification step to guarantee the accuracy of approximation; then it uses beam
decoding to accelerate the sequence generation from the joint distribution.
Compared with vanilla speculative decoding, MJSD has two advantages: (1) it is
an approximation of MJGD, thus achieving better output perplexity; (2)
verification with joint likelihood allows it to accept the longest prefix
sub-sequence of the draft tokens with valid perplexity, leading to better
efficiency...","[{'name': 'Zongyue Qin'}, {'name': 'Ziniu Hu'}, {'name': 'Zifan He'}, {'name': 'Neha Prakriya'}, {'name': 'Jason Cong'}, {'name': 'Yizhou Sun'}]",2024-07-12T23:29:54Z
http://arxiv.org/abs/2407.09709v1,http://arxiv.org/abs/2407.09709v1,GOFA: A Generative One-For-All Model for Joint Graph Language Modeling,"Foundation models, such as Large Language Models (LLMs) or Large Vision
Models (LVMs), have emerged as one of the most powerful tools in the respective
fields. However, unlike text and image data, graph data do not have a
definitive structure, posing great challenges to developing a Graph Foundation
Model (GFM). For example, current attempts at designing general graph models
either transform graph data into a language format for LLM-based prediction or
still train a GNN model with LLM as an assistant. The former can handle
unlimited tasks, while the latter captures graph structure much better -- yet,
no existing work can achieve both simultaneously. In this paper, we identify
three key desirable properties of a GFM: self-supervised pretraining, fluidity
in tasks, and graph awareness. To account for these properties, we extend the
conventional language modeling to the graph domain and propose a novel
generative graph language model GOFA to solve the problem. The model
interleaves randomly initialized GNN layers into a frozen pre-trained LLM so
that the semantic and structural modeling abilities are organically combined.
GOFA is pre-trained on newly proposed graph-level next-word prediction,
question-answering, and structural tasks to obtain the above GFM properties.
The pre-trained model is further fine-tuned on downstream tasks to obtain
task-solving ability. The fine-tuned model is evaluated on various downstream
tasks, demonstrating a strong ability to solve structural and contextual
problems in zero-shot scenarios. The code is available at
https://github.com/JiaruiFeng/GOFA.","[{'name': 'Lecheng Kong'}, {'name': 'Jiarui Feng'}, {'name': 'Hao Liu'}, {'name': 'Chengsong Huang'}, {'name': 'Jiaxin Huang'}, {'name': 'Yixin Chen'}, {'name': 'Muhan Zhang'}]",2024-07-12T22:23:51Z
http://arxiv.org/abs/2407.09704v1,http://arxiv.org/abs/2407.09704v1,"What an Elegant Bridge: Multilingual LLMs are Biased Similarly in
  Different Languages","This paper investigates biases of Large Language Models (LLMs) through the
lens of grammatical gender. Drawing inspiration from seminal works in
psycholinguistics, particularly the study of gender's influence on language
perception, we leverage multilingual LLMs to revisit and expand upon the
foundational experiments of Boroditsky (2003). Employing LLMs as a novel method
for examining psycholinguistic biases related to grammatical gender, we prompt
a model to describe nouns with adjectives in various languages, focusing
specifically on languages with grammatical gender. In particular, we look at
adjective co-occurrences across gender and languages, and train a binary
classifier to predict grammatical gender given adjectives an LLM uses to
describe a noun. Surprisingly, we find that a simple classifier can not only
predict noun gender above chance but also exhibit cross-language
transferability. We show that while LLMs may describe words differently in
different languages, they are biased similarly.","[{'name': 'Viktor Mihaylov'}, {'name': 'Aleksandar Shtedritski'}]",2024-07-12T22:10:16Z
http://arxiv.org/abs/2407.09688v1,http://arxiv.org/abs/2407.09688v1,"Large Language Models for Integrating Social Determinant of Health Data:
  A Case Study on Heart Failure 30-Day Readmission Prediction","Social determinants of health (SDOH) $-$ the myriad of circumstances in which
people live, grow, and age $-$ play an important role in health outcomes.
However, existing outcome prediction models often only use proxies of SDOH as
features. Recent open data initiatives present an opportunity to construct a
more comprehensive view of SDOH, but manually integrating the most relevant
data for individual patients becomes increasingly challenging as the volume and
diversity of public SDOH data grows. Large language models (LLMs) have shown
promise at automatically annotating structured data. Here, we conduct an
end-to-end case study evaluating the feasibility of using LLMs to integrate
SDOH data, and the utility of these SDOH features for clinical prediction. We
first manually label 700+ variables from two publicly-accessible SDOH data
sources to one of five semantic SDOH categories. Then, we benchmark performance
of 9 open-source LLMs on this classification task. Finally, we train ML models
to predict 30-day hospital readmission among 39k heart failure (HF) patients,
and we compare the prediction performance of the categorized SDOH variables
with standard clinical variables. Additionally, we investigate the impact of
few-shot LLM prompting on LLM annotation performance, and perform a metadata
ablation study on prompts to evaluate which information helps LLMs accurately
annotate these variables. We find that some open-source LLMs can effectively,
accurately annotate SDOH variables with zero-shot prompting without the need
for fine-tuning. Crucially, when combined with standard clinical features, the
LLM-annotated Neighborhood and Built Environment subset of the SDOH variables
shows the best performance predicting 30-day readmission of HF patients.","[{'name': 'Chase Fensore'}, {'name': 'Rodrigo M. Carrillo-Larco'}, {'name': 'Shivani A. Patel'}, {'name': 'Alanna A. Morris'}, {'name': 'Joyce C. Ho'}]",2024-07-12T21:14:06Z
http://arxiv.org/abs/2407.09652v1,http://arxiv.org/abs/2407.09652v1,"How Chinese are Chinese Language Models? The Puzzling Lack of Language
  Policy in China's LLMs","Contemporary language models are increasingly multilingual, but Chinese LLM
developers must navigate complex political and business considerations of
language diversity. Language policy in China aims at influencing the public
discourse and governing a multi-ethnic society, and has gradually transitioned
from a pluralist to a more assimilationist approach since 1949. We explore the
impact of these influences on current language technology. We evaluate six
open-source multilingual LLMs pre-trained by Chinese companies on 18 languages,
spanning a wide range of Chinese, Asian, and Anglo-European languages. Our
experiments show Chinese LLMs performance on diverse languages is
indistinguishable from international LLMs. Similarly, the models' technical
reports also show lack of consideration for pretraining data language coverage
except for English and Mandarin Chinese. Examining Chinese AI policy, model
experiments, and technical reports, we find no sign of any consistent policy,
either for or against, language diversity in China's LLM development. This
leaves a puzzling fact that while China regulates both the languages people use
daily as well as language model development, they do not seem to have any
policy on the languages in language models.","[{'name': 'Andrea W Wen-Yi'}, {'name': 'Unso Eun Seo Jo'}, {'name': 'Lu Jia Lin'}, {'name': 'David Mimno'}]",2024-07-12T19:21:40Z
http://arxiv.org/abs/2407.12865v1,http://arxiv.org/abs/2407.12865v1,"GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt
  Engineering","Prompt engineering for large language models (LLMs) is often a manual
time-intensive process that involves generating, evaluating, and refining
prompts iteratively to ensure high-quality outputs. While there has been work
on automating prompt engineering, the solutions generally are either tuned to
specific tasks with given answers or are quite costly. We introduce GRAD-SUM, a
scalable and flexible method for automatic prompt engineering that builds on
gradient-based optimization techniques. Our approach incorporates user-defined
task descriptions and evaluation criteria, and features a novel gradient
summarization module to generalize feedback effectively. Our results
demonstrate that GRAD-SUM consistently outperforms existing methods across
various benchmarks, highlighting its versatility and effectiveness in automatic
prompt optimization.","[{'name': 'Derek Austin'}, {'name': 'Elliott Chartock'}]",2024-07-12T19:11:21Z
http://arxiv.org/abs/2407.09453v1,http://arxiv.org/abs/2407.09453v1,"Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators","Nowadays, increasingly larger Deep Neural Networks (DNNs) are being
developed, trained, and utilized. These networks require significant
computational resources, putting a strain on both advanced and limited devices.
Our solution is to implement {\em weight block sparsity}, which is a structured
sparsity that is friendly to hardware. By zeroing certain sections of the
convolution and fully connected layers parameters of pre-trained DNN models, we
can efficiently speed up the DNN's inference process. This results in a smaller
memory footprint, faster communication, and fewer operations.
  Our work presents a vertical system that allows for the training of
convolution and matrix multiplication weights to exploit 8x8 block sparsity on
a single GPU within a reasonable amount of time. Compilers recognize this
sparsity and use it for both data compaction and computation splitting into
threads. Blocks like these take full advantage of both spatial and temporal
locality, paving the way for fast vector operations and memory reuse. By using
this system on a Resnet50 model, we were able to reduce the weight by half with
minimal accuracy loss, resulting in a two-times faster inference speed. We will
present performance estimates using accurate and complete code generation for
AIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, and
VGG16 to demonstrate the necessary synergy between hardware overlay designs and
software stacks for compiling and executing machine learning applications.","[{'name': ""Paolo D'Alberto""}, {'name': 'Taehee Jeong'}, {'name': 'Akshai Jain'}, {'name': 'Shreyas Manjunath'}, {'name': 'Mrinal Sarmah'}, {'name': 'Samuel Hsu'}, {'name': 'Yaswanth Raparti'}, {'name': 'Nitesh Pipralia'}]",2024-07-12T17:37:49Z
http://arxiv.org/abs/2407.09450v1,http://arxiv.org/abs/2407.09450v1,Human-like Episodic Memory for Infinite Context LLMs,"Large language models (LLMs) have shown remarkable capabilities, but still
struggle with processing extensive contexts, limiting their ability to maintain
coherence and accuracy over long sequences. In contrast, the human brain excels
at organising and retrieving episodic experiences across vast temporal scales,
spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that
integrates key aspects of human episodic memory and event cognition into LLMs,
enabling them to effectively handle practically infinite context lengths while
maintaining computational efficiency. EM-LLM organises sequences of tokens into
coherent episodic events using a combination of Bayesian surprise and
graph-theoretic boundary refinement in an on-line fashion. When needed, these
events are retrieved through a two-stage memory process, combining
similarity-based and temporally contiguous retrieval for efficient and
human-like access to relevant information. Experiments on the LongBench dataset
demonstrate EM-LLM's superior performance, outperforming the state-of-the-art
InfLLM model with an overall relative improvement of 4.3% across various tasks,
including a 33% improvement on the PassageRetrieval task. Furthermore, our
analysis reveals strong correlations between EM-LLM's event segmentation and
human-perceived events, suggesting a bridge between this artificial system and
its biological counterpart. This work not only advances LLM capabilities in
processing extended contexts but also provides a computational framework for
exploring human memory mechanisms, opening new avenues for interdisciplinary
research in AI and cognitive science.","[{'name': 'Zafeirios Fountas'}, {'name': 'Martin A Benfeghoul'}, {'name': 'Adnan Oomerjee'}, {'name': 'Fenia Christopoulou'}, {'name': 'Gerasimos Lampouras'}, {'name': 'Haitham Bou-Ammar'}, {'name': 'Jun Wang'}]",2024-07-12T17:34:03Z
http://arxiv.org/abs/2407.09429v1,http://arxiv.org/abs/2407.09429v1,Open (Clinical) LLMs are Sensitive to Instruction Phrasings,"Instruction-tuned Large Language Models (LLMs) can perform a wide range of
tasks given natural language instructions to do so, but they are sensitive to
how such instructions are phrased. This issue is especially concerning in
healthcare, as clinicians are unlikely to be experienced prompt engineers and
the potential consequences of inaccurate outputs are heightened in this domain.
  This raises a practical question: How robust are instruction-tuned LLMs to
natural variations in the instructions provided for clinical NLP tasks? We
collect prompts from medical doctors across a range of tasks and quantify the
sensitivity of seven LLMs -- some general, others specialized -- to natural
(i.e., non-adversarial) instruction phrasings. We find that performance varies
substantially across all models, and that -- perhaps surprisingly --
domain-specific models explicitly trained on clinical data are especially
brittle, compared to their general domain counterparts. Further, arbitrary
phrasing differences can affect fairness, e.g., valid but distinct instructions
for mortality prediction yield a range both in overall performance, and in
terms of differences between demographic groups.","[{'name': 'Alberto Mario Ceballos Arroyo'}, {'name': 'Monica Munnangi'}, {'name': 'Jiuding Sun'}, {'name': 'Karen Y. C. Zhang'}, {'name': 'Denis Jered McInerney'}, {'name': 'Byron C. Wallace'}, {'name': 'Silvio Amir'}]",2024-07-12T17:00:44Z
http://arxiv.org/abs/2407.09417v2,http://arxiv.org/abs/2407.09417v2,Mitigating Entity-Level Hallucination in Large Language Models,"The emergence of Large Language Models (LLMs) has revolutionized how users
access information, shifting from traditional search engines to direct
question-and-answer interactions with LLMs. However, the widespread adoption of
LLMs has revealed a significant challenge known as hallucination, wherein LLMs
generate coherent yet factually inaccurate responses. This hallucination
phenomenon has led to users' distrust in information retrieval systems based on
LLMs. To tackle this challenge, this paper proposes Dynamic Retrieval
Augmentation based on hallucination Detection (DRAD) as a novel method to
detect and mitigate hallucinations in LLMs. DRAD improves upon traditional
retrieval augmentation by dynamically adapting the retrieval process based on
real-time hallucination detection. It features two main components: Real-time
Hallucination Detection (RHD) for identifying potential hallucinations without
external models, and Self-correction based on External Knowledge (SEK) for
correcting these errors using external knowledge. Experiment results show that
DRAD demonstrates superior performance in both detecting and mitigating
hallucinations in LLMs. All of our code and data are open-sourced at
https://github.com/oneal2000/EntityHallucination.","[{'name': 'Weihang Su'}, {'name': 'Yichen Tang'}, {'name': 'Qingyao Ai'}, {'name': 'Changyue Wang'}, {'name': 'Zhijing Wu'}, {'name': 'Yiqun Liu'}]",2024-07-12T16:47:34Z
http://arxiv.org/abs/2407.09413v1,http://arxiv.org/abs/2407.09413v1,SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers,"Seeking answers to questions within long scientific research articles is a
crucial area of study that aids readers in quickly addressing their inquiries.
However, existing question-answering (QA) datasets based on scientific papers
are limited in scale and focus solely on textual content. To address this
limitation, we introduce SPIQA (Scientific Paper Image Question Answering), the
first large-scale QA dataset specifically designed to interpret complex figures
and tables within the context of scientific research articles across various
domains of computer science. Leveraging the breadth of expertise and ability of
multimodal large language models (MLLMs) to understand figures, we employ
automatic and manual curation to create the dataset. We craft an
information-seeking task involving multiple images that cover a wide variety of
plots, charts, tables, schematic diagrams, and result visualizations. SPIQA
comprises 270K questions divided into training, validation, and three different
evaluation splits. Through extensive experiments with 12 prominent foundational
models, we evaluate the ability of current multimodal systems to comprehend the
nuanced aspects of research articles. Additionally, we propose a
Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that
allows fine-grained, step-by-step assessment and improves model performance. We
further explore the upper bounds of performance enhancement with additional
textual information, highlighting its promising potential for future research
and the dataset's impact on revolutionizing how we interact with scientific
literature.","[{'name': 'Shraman Pramanick'}, {'name': 'Rama Chellappa'}, {'name': 'Subhashini Venugopalan'}]",2024-07-12T16:37:59Z
http://arxiv.org/abs/2407.09395v1,http://arxiv.org/abs/2407.09395v1,"Deep Bag-of-Words Model: An Efficient and Interpretable Relevance
  Architecture for Chinese E-Commerce","Text relevance or text matching of query and product is an essential
technique for the e-commerce search system to ensure that the displayed
products can match the intent of the query. Many studies focus on improving the
performance of the relevance model in search system. Recently, pre-trained
language models like BERT have achieved promising performance on the text
relevance task. While these models perform well on the offline test dataset,
there are still obstacles to deploy the pre-trained language model to the
online system as their high latency. The two-tower model is extensively
employed in industrial scenarios, owing to its ability to harmonize performance
with computational efficiency. Regrettably, such models present an opaque
``black box'' nature, which prevents developers from making special
optimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an
efficient and interpretable relevance architecture for Chinese e-commerce. Our
approach proposes to encode the query and the product into the sparse BoW
representation, which is a set of word-weight pairs. The weight means the
important or the relevant score between the corresponding word and the raw
text. The relevance score is measured by the accumulation of the matched word
between the sparse BoW representation of the query and the product. Compared to
popular dense distributed representation that usually suffers from the drawback
of black-box, the most advantage of the proposed representation model is highly
explainable and interventionable, which is a superior advantage to the
deployment and operation of online search engines. Moreover, the online
efficiency of the proposed model is even better than the most efficient inner
product form of dense representation ...","[{'name': 'Zhe Lin'}, {'name': 'Jiwei Tan'}, {'name': 'Dan Ou'}, {'name': 'Xi Chen'}, {'name': 'Shaowei Yao'}, {'name': 'Bo Zheng'}]",2024-07-12T16:18:05Z
http://arxiv.org/abs/2407.09311v1,http://arxiv.org/abs/2407.09311v1,"Scalability of Bayesian Network Structure Elicitation with Large
  Language Models: a Novel Methodology and Comparative Analysis","In this work, we propose a novel method for Bayesian Networks (BNs) structure
elicitation that is based on the initialization of several LLMs with different
experiences, independently querying them to create a structure of the BN, and
further obtaining the final structure by majority voting. We compare the method
with one alternative method on various widely and not widely known BNs of
different sizes and study the scalability of both methods on them. We also
propose an approach to check the contamination of BNs in LLM, which shows that
some widely known BNs are inapplicable for testing the LLM usage for BNs
structure elicitation. We also show that some BNs may be inapplicable for such
experiments because their node names are indistinguishable. The experiments on
the other BNs show that our method performs better than the existing method
with one of the three studied LLMs; however, the performance of both methods
significantly decreases with the increase in BN size.","[{'name': 'Nikolay Babakov'}, {'name': 'Ehud Reiter'}, {'name': 'Alberto Bugarin'}]",2024-07-12T14:52:13Z
http://arxiv.org/abs/2407.09298v2,http://arxiv.org/abs/2407.09298v2,Transformer Layers as Painters,"Despite their nearly universal adoption for large language models, the
internal workings of transformers are not well understood. We aim to better
understand the impact of removing or reorganizing information throughout the
layers of a pretrained transformer. Such an understanding could both yield
better usage of existing models as well as to make architectural improvements
to produce new variants. We present a series of empirical studies on frozen
models that show that the lower and final layers of pretrained transformers
differ from middle layers, but that middle layers have a surprising amount of
uniformity. We further show that some classes of problems have robustness to
skipping layers, running the layers in an order different from how they were
trained, or running the layers in parallel. Our observations suggest that even
frozen pretrained models may gracefully trade accuracy for latency by skipping
layers or running layers in parallel.","[{'name': 'Qi Sun'}, {'name': 'Marc Pickett'}, {'name': 'Aakash Kumar Nain'}, {'name': 'Llion Jones'}]",2024-07-12T14:31:05Z
http://arxiv.org/abs/2407.11068v3,http://arxiv.org/abs/2407.11068v3,"Show, Don't Tell: Evaluating Large Language Models Beyond Textual
  Understanding with ChildPlay","We explore the hypothesis that LLMs, such as GPT-3.5 and GPT-4, possess
broader cognitive functions, particularly in non-linguistic domains. Our
approach extends beyond standard linguistic benchmarks by incorporating games
like Tic-Tac-Toe, Connect Four, and Battleship, encoded via ASCII, to assess
strategic thinking and decision-making. To evaluate the models' ability to
generalize beyond their training data, we introduce two additional games. The
first game, LEGO Connect Language (LCL), tests the models' capacity to
understand spatial logic and follow assembly instructions. The second game, the
game of shapes, challenges the models to identify shapes represented by 1s
within a matrix of zeros, further testing their spatial reasoning skills. This
""show, don't tell"" strategy uses games instead of simply querying the models.
Our results show that despite their proficiency on standard benchmarks, GPT-3.5
and GPT-4's abilities to play and reason about fully observable games without
pre-training is mediocre. Both models fail to anticipate losing moves in
Tic-Tac-Toe and Connect Four, and they are unable to play Battleship correctly.
While GPT-4 shows some success in the game of shapes, both models fail at the
assembly tasks presented in the LCL game. These results suggest that while GPT
models can emulate conversational proficiency and basic rule comprehension,
their performance in strategic gameplay and spatial reasoning tasks is very
limited. Importantly, this reveals a blind spot in current LLM benchmarks that
we highlight with our gameplay benchmark suite ChildPlay
(https://github.com/child-play-neurips/child-play). Our findings provide a
cautionary tale about claims of emergent intelligence and reasoning
capabilities of LLMs that are roughly the size of GPT-3.5 and GPT-4.","[{'name': 'Gonçalo Hora de Carvalho'}, {'name': 'Oscar Knap'}, {'name': 'Robert Pollice'}]",2024-07-12T14:17:26Z
http://arxiv.org/abs/2407.09283v1,http://arxiv.org/abs/2407.09283v1,DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection,"Semantic role labeling (SRL) enriches many downstream applications, e.g.,
machine translation, question answering, summarization, and stance/belief
detection. However, building multilingual SRL models is challenging due to the
scarcity of semantically annotated corpora for multiple languages. Moreover,
state-of-the-art SRL projection (XSRL) based on large language models (LLMs)
yields output that is riddled with spurious role labels. Remediation of such
hallucinations is not straightforward due to the lack of explainability of
LLMs. We show that hallucinated role labels are related to naturally occurring
divergence types that interfere with initial alignments. We implement
Divergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging
linguistically-informed alignment remediation followed by greedy First-Come
First-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL
projection without additional transformer-based machinery, beating XSRL in both
human and automatic comparisons, and advancing beyond headwords to accommodate
phrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our
ground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%
(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%
(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our
approach to other language pairs (e.g., English-Tagalog).","[{'name': 'Sangpil Youm'}, {'name': 'Brodie Mather'}, {'name': 'Chathuri Jayaweera'}, {'name': 'Juliana Prada'}, {'name': 'Bonnie Dorr'}]",2024-07-12T14:13:59Z
http://arxiv.org/abs/2407.09276v1,http://arxiv.org/abs/2407.09276v1,H2O-Danube3 Technical Report,"We present H2O-Danube3, a series of small language models consisting of
H2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T
tokens. Our models are pre-trained on high quality Web data consisting of
primarily English tokens in three stages with different data mixes before final
supervised tuning for chat version. The models exhibit highly competitive
metrics across a multitude of academic, chat, and fine-tuning benchmarks.
Thanks to its compact architecture, H2O-Danube3 can be efficiently run on a
modern smartphone, enabling local inference and rapid processing capabilities
even on mobile devices. We make all models openly available under Apache 2.0
license further democratizing LLMs to a wider audience economically.","[{'name': 'Pascal Pfeiffer'}, {'name': 'Philipp Singer'}, {'name': 'Yauhen Babakhin'}, {'name': 'Gabor Fodor'}, {'name': 'Nischay Dhankhar'}, {'name': 'Sri Satish Ambati'}]",2024-07-12T14:09:40Z
http://arxiv.org/abs/2407.09252v2,http://arxiv.org/abs/2407.09252v2,Context Embeddings for Efficient Answer Generation in RAG,"Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge
of LLMs by extending the input with external information. As a consequence, the
contextual inputs to the model become much longer which slows down decoding
time directly translating to the time a user has to wait for an answer. We
address this challenge by presenting COCOM, an effective context compression
method, reducing long contexts to only a handful of Context Embeddings speeding
up the generation time by a large margin. Our method allows for different
compression rates trading off decoding time for answer quality. Compared to
earlier methods, COCOM allows for handling multiple contexts more effectively,
significantly reducing decoding time for long inputs. Our method demonstrates a
speed-up of up to 5.69 $\times$ while achieving higher performance compared to
existing efficient context compression methods.","[{'name': 'David Rau'}, {'name': 'Shuai Wang'}, {'name': 'Hervé Déjean'}, {'name': 'Stéphane Clinchant'}]",2024-07-12T13:30:44Z
http://arxiv.org/abs/2407.12863v1,http://arxiv.org/abs/2407.12863v1,"Token-Supervised Value Models for Enhancing Mathematical Reasoning
  Capabilities of Large Language Models","Large Language Models (LLMs) have demonstrated impressive problem-solving
capabilities in mathematics through step-by-step reasoning chains. However,
they are susceptible to reasoning errors that impact the quality of subsequent
reasoning chains and the final answer due to language models' autoregressive
token-by-token generating nature. Recent works have proposed adopting external
verifiers to guide the generation of reasoning paths, but existing works
utilize models that have been trained with step-by-step labels to assess the
correctness of token-by-token reasoning chains. Consequently, they struggle to
recognize discriminative details of tokens within a reasoning path and lack the
ability to evaluate whether an intermediate reasoning path is on a promising
track toward the correct final answer. To amend the lack of sound and
token-grained math-verification signals, we devise a novel training scheme for
verifiers that apply token-level supervision with the expected cumulative
reward (i.e., value). Furthermore, we propose a practical formulation of the
cumulative reward by reducing it to finding the probability of future
correctness of the final answer and thereby enabling the empirical estimation
of the value. Experimental results on mathematical reasoning benchmarks show
that Token-Supervised Value Model (TVM) can outperform step-by-step verifiers
on GSM8K and MATH with Mistral and Llama.","[{'name': 'Jung Hyun Lee'}, {'name': 'June Yong Yang'}, {'name': 'Byeongho Heo'}, {'name': 'Dongyoon Han'}, {'name': 'Kang Min Yoo'}]",2024-07-12T13:16:50Z
http://arxiv.org/abs/2407.09241v1,http://arxiv.org/abs/2407.09241v1,The Sociolinguistic Foundations of Language Modeling,"In this paper, we introduce a sociolinguistic perspective on language
modeling. We claim that large language models are inherently models of
varieties of language, and we consider how this insight can inform the
development and deployment of large language models. We begin by presenting a
technical definition of the concept of a variety of language as developed in
sociolinguistics. We then discuss how this perspective can help address five
basic challenges in language modeling: social bias, domain adaptation,
alignment, language change, and scale. Ultimately, we argue that it is crucial
to carefully define and compile training corpora that accurately represent the
specific varieties of language being modeled to maximize the performance and
societal value of large language models.","[{'name': 'Jack Grieve'}, {'name': 'Sara Bartl'}, {'name': 'Matteo Fuoli'}, {'name': 'Jason Grafmiller'}, {'name': 'Weihang Huang'}, {'name': 'Alejandro Jawerbaum'}, {'name': 'Akira Murakami'}, {'name': 'Marcus Perlman'}, {'name': 'Dana Roemling'}, {'name': 'Bodo Winter'}]",2024-07-12T13:12:55Z
http://arxiv.org/abs/2407.09187v1,http://arxiv.org/abs/2407.09187v1,"Enhancing Depressive Post Detection in Bangla: A Comparative Study of
  TF-IDF, BERT and FastText Embeddings","Due to massive adoption of social media, detection of users' depression
through social media analytics bears significant importance, particularly for
underrepresented languages, such as Bangla. This study introduces a
well-grounded approach to identify depressive social media posts in Bangla, by
employing advanced natural language processing techniques. The dataset used in
this work, annotated by domain experts, includes both depressive and
non-depressive posts, ensuring high-quality data for model training and
evaluation. To address the prevalent issue of class imbalance, we utilised
random oversampling for the minority class, thereby enhancing the model's
ability to accurately detect depressive posts. We explored various numerical
representation techniques, including Term Frequency-Inverse Document Frequency
(TF-IDF), Bidirectional Encoder Representations from Transformers (BERT)
embedding and FastText embedding, by integrating them with a deep
learning-based Convolutional Neural Network-Bidirectional Long Short-Term
Memory (CNN-BiLSTM) model. The results obtained through extensive
experimentation, indicate that the BERT approach performed better the others,
achieving a F1-score of 84%. This indicates that BERT, in combination with the
CNN-BiLSTM architecture, effectively recognises the nuances of Bangla texts
relevant to depressive contents. Comparative analysis with the existing
state-of-the-art methods demonstrates that our approach with BERT embedding
performs better than others in terms of evaluation metrics and the reliability
of dataset annotations. Our research significantly contribution to the
development of reliable tools for detecting depressive posts in the Bangla
language. By highlighting the efficacy of different embedding techniques and
deep learning models, this study paves the way for improved mental health
monitoring through social media platforms.","[{'name': 'Saad Ahmed Sazan'}, {'name': 'Mahdi H. Miraz'}, {'name': 'A B M Muntasir Rahman'}]",2024-07-12T11:40:17Z
http://arxiv.org/abs/2407.09184v1,http://arxiv.org/abs/2407.09184v1,"Does Incomplete Syntax Influence Korean Language Model? Focusing on Word
  Order and Case Markers","Syntactic elements, such as word order and case markers, are fundamental in
natural language processing. Recent studies show that syntactic information
boosts language model performance and offers clues for people to understand
their learning mechanisms. Unlike languages with a fixed word order such as
English, Korean allows for varied word sequences, despite its canonical
structure, due to case markers that indicate the functions of sentence
components. This study explores whether Korean language models can accurately
capture this flexibility. We note that incomplete word orders and omitted case
markers frequently appear in ordinary Korean communication. To investigate this
further, we introduce the Syntactically Incomplete Korean (SIKO) dataset.
Through SIKO, we assessed Korean language models' flexibility with incomplete
syntax and confirmed the dataset's training value. Results indicate these
models reflect Korean's inherent flexibility, accurately handling incomplete
inputs. Moreover, fine-tuning with SIKO enhances the ability to handle common
incomplete Korean syntactic forms. The dataset's simple construction process,
coupled with significant performance enhancements, solidifies its standing as
an effective data augmentation technique.","[{'name': 'Jong Myoung Kim'}, {'name': 'Young-Jun Lee'}, {'name': 'Yong-jin Han'}, {'name': 'Sangkeun Jung'}, {'name': 'Ho-Jin Choi'}]",2024-07-12T11:33:41Z
http://arxiv.org/abs/2407.13787v2,http://arxiv.org/abs/2407.13787v2,"The Honorific Effect: Exploring the Impact of Japanese Linguistic
  Formalities on AI-Generated Physics Explanations","This study investigates the influence of Japanese honorifics on the responses
of large language models (LLMs) when explaining the law of conservation of
momentum. We analyzed the outputs of six state-of-the-art AI models, including
variations of ChatGPT, Coral, and Gemini, using 14 different honorific forms.
Our findings reveal that honorifics significantly affect the quality,
consistency, and formality of AI-generated responses, demonstrating LLMs'
ability to interpret and adapt to social context cues embedded in language.
Notable variations were observed across different models, with some emphasizing
historical context and derivations, while others focused on intuitive
explanations. The study highlights the potential for using honorifics to adjust
the depth and complexity of AI-generated explanations in educational contexts.
Furthermore, the responsiveness of AI models to cultural linguistic elements
underscores the importance of considering cultural factors in AI development
for educational applications. These results open new avenues for research in
AI-assisted education and cultural adaptation in AI systems, with significant
implications for personalizing learning experiences and developing culturally
sensitive AI tools for global education.",[{'name': 'Keisuke Sato'}],2024-07-12T11:31:00Z
http://arxiv.org/abs/2407.09181v1,http://arxiv.org/abs/2407.09181v1,Exploring the Effectiveness of Methods for Persona Extraction,"The paper presents a study of methods for extracting information about
dialogue participants and evaluating their performance in Russian. To train
models for this task, the Multi-Session Chat dataset was translated into
Russian using multiple translation models, resulting in improved data quality.
A metric based on the F-score concept is presented to evaluate the
effectiveness of the extraction models. The metric uses a trained classifier to
identify the dialogue participant to whom the persona belongs. Experiments were
conducted on MBart, FRED-T5, Starling-7B, which is based on the Mistral, and
Encoder2Encoder models. The results demonstrated that all models exhibited an
insufficient level of recall in the persona extraction task. The incorporation
of the NCE Loss improved the model's precision at the expense of its recall.
Furthermore, increasing the model's size led to enhanced extraction of
personas.",[{'name': 'Konstantin Zaitsev'}],2024-07-12T11:30:10Z
http://arxiv.org/abs/2407.09152v1,http://arxiv.org/abs/2407.09152v1,"The Two Sides of the Coin: Hallucination Generation and Detection with
  LLMs as Evaluators for LLMs","Hallucination detection in Large Language Models (LLMs) is crucial for
ensuring their reliability. This work presents our participation in the CLEF
ELOQUENT HalluciGen shared task, where the goal is to develop evaluators for
both generating and detecting hallucinated content. We explored the
capabilities of four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, for this
purpose. We also employed ensemble majority voting to incorporate all four
models for the detection task. The results provide valuable insights into the
strengths and weaknesses of these LLMs in handling hallucination generation and
detection tasks.","[{'name': 'Anh Thu Maria Bui'}, {'name': 'Saskia Felizitas Brech'}, {'name': 'Natalie Hußfeldt'}, {'name': 'Tobias Jennert'}, {'name': 'Melanie Ullrich'}, {'name': 'Timo Breuer'}, {'name': 'Narjes Nikzad Khasmakhi'}, {'name': 'Philipp Schaer'}]",2024-07-12T10:34:46Z
http://arxiv.org/abs/2407.09137v1,http://arxiv.org/abs/2407.09137v1,"A Look Into News Avoidance Through AWRS: An Avoidance-Aware Recommender
  System","In recent years, journalists have expressed concerns about the increasing
trend of news article avoidance, especially within specific domains. This issue
has been exacerbated by the rise of recommender systems. Our research indicates
that recommender systems should consider avoidance as a fundamental factor. We
argue that news articles can be characterized by three principal elements:
exposure, relevance, and avoidance, all of which are closely interconnected. To
address these challenges, we introduce AWRS, an Avoidance-Aware Recommender
System. This framework incorporates avoidance awareness when recommending news,
based on the premise that news article avoidance conveys significant
information about user preferences. Evaluation results on three news datasets
in different languages (English, Norwegian, and Japanese) demonstrate that our
method outperforms existing approaches.","[{'name': 'Igor L. R. Azevedo'}, {'name': 'Toyotaro Suzumura'}, {'name': 'Yuichiro Yasui'}]",2024-07-12T10:16:03Z
http://arxiv.org/abs/2407.09136v1,http://arxiv.org/abs/2407.09136v1,"Stepwise Verification and Remediation of Student Reasoning Errors with
  Large Language Model Tutors","Large language models (LLMs) present an opportunity to scale high-quality
personalized education to all. A promising approach towards this means is to
build dialog tutoring models that scaffold students' problem-solving. However,
even though existing LLMs perform well in solving reasoning questions, they
struggle to precisely detect student's errors and tailor their feedback to
these errors. Inspired by real-world teaching practice where teachers identify
student errors and customize their response based on them, we focus on
verifying student solutions and show how grounding to such verification
improves the overall quality of tutor response generation. We collect a dataset
of 1K stepwise math reasoning chains with the first error step annotated by
teachers. We show empirically that finding the mistake in a student solution is
challenging for current models. We propose and evaluate several verifiers for
detecting these errors. Using both automatic and human evaluation we show that
the student solution verifiers steer the generation model towards highly
targeted responses to student errors which are more often correct with less
hallucinations compared to existing baselines.","[{'name': 'Nico Daheim'}, {'name': 'Jakub Macina'}, {'name': 'Manu Kapur'}, {'name': 'Iryna Gurevych'}, {'name': 'Mrinmaya Sachan'}]",2024-07-12T10:11:40Z
http://arxiv.org/abs/2407.09121v1,http://arxiv.org/abs/2407.09121v1,"Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled
  Refusal Training","This study addresses a critical gap in safety tuning practices for Large
Language Models (LLMs) by identifying and tackling a refusal position bias
within safety tuning data, which compromises the models' ability to
appropriately refuse generating unsafe content. We introduce a novel approach,
Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse
compliance to harmful prompts at any response position, significantly enhancing
their safety capabilities. DeRTa incorporates two novel components: (1) Maximum
Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models
to recognize and avoid unsafe content by appending a segment of harmful
response to the beginning of a safe response, and (2) Reinforced Transition
Optimization (RTO), which equips models with the ability to transition from
potential harm to safety refusal consistently throughout the harmful response
sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model
families across six attack scenarios, demonstrates that our method not only
improves model safety without compromising performance but also surpasses
well-known models such as GPT-4 in defending against attacks. Importantly, our
approach successfully defends recent advanced attack methods (e.g., CodeAttack)
that have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be
found at https://github.com/RobustNLP/DeRTa.","[{'name': 'Youliang Yuan'}, {'name': 'Wenxiang Jiao'}, {'name': 'Wenxuan Wang'}, {'name': 'Jen-tse Huang'}, {'name': 'Jiahao Xu'}, {'name': 'Tian Liang'}, {'name': 'Pinjia He'}, {'name': 'Zhaopeng Tu'}]",2024-07-12T09:36:33Z
http://arxiv.org/abs/2407.09120v1,http://arxiv.org/abs/2407.09120v1,"URRL-IMVC: Unified and Robust Representation Learning for Incomplete
  Multi-View Clustering","Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that
are only partially available. This poses two main challenges: effectively
leveraging multi-view information and mitigating the impact of missing views.
Prevailing solutions employ cross-view contrastive learning and missing view
recovery techniques. However, they either neglect valuable complementary
information by focusing only on consensus between views or provide unreliable
recovered views due to the absence of supervision. To address these
limitations, we propose a novel Unified and Robust Representation Learning for
Incomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a
unified embedding that is robust to view missing conditions by integrating
information from multiple views and neighboring samples. Firstly, to overcome
the limitations of cross-view contrastive learning, URRL-IMVC incorporates an
attention-based auto-encoder framework to fuse multi-view information and
generate unified embeddings. Secondly, URRL-IMVC directly enhances the
robustness of the unified embedding against view-missing conditions through KNN
imputation and data augmentation techniques, eliminating the need for explicit
missing view recovery. Finally, incremental improvements are introduced to
further enhance the overall performance, such as the Clustering Module and the
customization of the Encoder. We extensively evaluate the proposed URRL-IMVC
framework on various benchmark datasets, demonstrating its state-of-the-art
performance. Furthermore, comprehensive ablation studies are performed to
validate the effectiveness of our design.","[{'name': 'Ge Teng'}, {'name': 'Ting Mao'}, {'name': 'Chen Shen'}, {'name': 'Xiang Tian'}, {'name': 'Xuesong Liu'}, {'name': 'Yaowu Chen'}, {'name': 'Jieping Ye'}]",2024-07-12T09:35:25Z
http://arxiv.org/abs/2407.09072v1,http://arxiv.org/abs/2407.09072v1,New Desiderata for Direct Preference Optimization,"Large language models in the past have typically relied on some form of
reinforcement learning with human feedback (RLHF) to better align model
responses with human preferences. However, because of oft-observed
instabilities when implementing these RLHF pipelines, various
reparameterization techniques have recently been introduced to sidestep the
need for separately learning an RL reward model. Instead, directly fine-tuning
for human preferences is achieved via the minimization of a single closed-form
training objective, a process originally referred to as direct preference
optimization (DPO) and followed by several notable descendants. Although
effective in certain real-world settings, we introduce new evaluation criteria
that serve to highlight unresolved shortcomings in the ability of existing DPO
methods to interpolate between a pre-trained reference model and empirical
measures of human preferences, as well as unavoidable trade-offs in how low-
and high-quality responses are regularized and constraints are handled. Our
insights then motivate an alternative DPO-like loss that provably mitigates
these limitations. Empirical results serve to corroborate notable aspects of
our analyses.","[{'name': 'Xiangkun Hu'}, {'name': 'Tong He'}, {'name': 'David Wipf'}]",2024-07-12T07:52:32Z
http://arxiv.org/abs/2407.09020v3,http://arxiv.org/abs/2407.09020v3,"3M-Health: Multimodal Multi-Teacher Knowledge Distillation for Mental
  Health Detection","The significance of mental health classification is paramount in contemporary
society, where digital platforms serve as crucial sources for monitoring
individuals' well-being. However, existing social media mental health datasets
primarily consist of text-only samples, potentially limiting the efficacy of
models trained on such data. Recognising that humans utilise cross-modal
information to comprehend complex situations or issues, we present a novel
approach to address the limitations of current methodologies. In this work, we
introduce a Multimodal and Multi-Teacher Knowledge Distillation model for
Mental Health Classification, leveraging insights from cross-modal human
understanding. Unlike conventional approaches that often rely on simple
concatenation to integrate diverse features, our model addresses the challenge
of appropriately representing inputs of varying natures (e.g., texts and
sounds). To mitigate the computational complexity associated with integrating
all features into a single model, we employ a multimodal and multi-teacher
architecture. By distributing the learning process across multiple teachers,
each specialising in a particular feature extraction aspect, we enhance the
overall mental health classification performance. Through experimental
validation, we demonstrate the efficacy of our model in achieving improved
performance.","[{'name': 'Rina Carines Cabral'}, {'name': 'Siwen Luo'}, {'name': 'Josiah Poon'}, {'name': 'Soyeon Caren Han'}]",2024-07-12T06:22:45Z
http://arxiv.org/abs/2407.09014v2,http://arxiv.org/abs/2407.09014v2,CompAct: Compressing Retrieved Documents Actively for Question Answering,"Retrieval-augmented generation supports language models to strengthen their
factual groundings by providing external contexts. However, language models
often face challenges when given extensive information, diminishing their
effectiveness in solving questions. Context compression tackles this issue by
filtering out irrelevant information, but current methods still struggle in
realistic scenarios where crucial information cannot be captured with a
single-step approach. To overcome this limitation, we introduce CompAct, a
novel framework that employs an active strategy to condense extensive documents
without losing key information. Our experiments demonstrate that CompAct brings
significant improvements in both performance and compression rate on multi-hop
question-answering (QA) benchmarks. CompAct flexibly operates as a
cost-efficient plug-in module with various off-the-shelf retrievers or readers,
achieving exceptionally high compression rates (47x).","[{'name': 'Chanwoong Yoon'}, {'name': 'Taewhoo Lee'}, {'name': 'Hyeon Hwang'}, {'name': 'Minbyul Jeong'}, {'name': 'Jaewoo Kang'}]",2024-07-12T06:06:54Z
http://arxiv.org/abs/2407.09011v1,http://arxiv.org/abs/2407.09011v1,"One Stone, Four Birds: A Comprehensive Solution for QA System Using
  Supervised Contrastive Learning","This paper presents a novel and comprehensive solution to enhance both the
robustness and efficiency of question answering (QA) systems through supervised
contrastive learning (SCL). Training a high-performance QA system has become
straightforward with pre-trained language models, requiring only a small amount
of data and simple fine-tuning. However, despite recent advances, existing QA
systems still exhibit significant deficiencies in functionality and training
efficiency. We address the functionality issue by defining four key tasks: user
input intent classification, out-of-domain input detection, new intent
discovery, and continual learning. We then leverage a unified SCL-based
representation learning method to efficiently build an intra-class compact and
inter-class scattered feature space, facilitating both known intent
classification and unknown intent detection and discovery. Consequently, with
minimal additional tuning on downstream tasks, our approach significantly
improves model efficiency and achieves new state-of-the-art performance across
all tasks.","[{'name': 'Bo Wang'}, {'name': 'Tsunenori Mine'}]",2024-07-12T06:01:51Z
http://arxiv.org/abs/2407.09007v1,http://arxiv.org/abs/2407.09007v1,Benchmarking Language Model Creativity: A Case Study on Code Generation,"As LLMs become increasingly prevalent, it is interesting to consider how
``creative'' these models can be. From cognitive science, creativity consists
of at least two key characteristics: \emph{convergent} thinking (purposefulness
to achieve a given goal) and \emph{divergent} thinking (adaptability to new
environments or constraints) \citep{runco2003critical}. In this work, we
introduce a framework for quantifying LLM creativity that incorporates the two
characteristics. This is achieved by (1) Denial Prompting pushes LLMs to come
up with more creative solutions to a given problem by incrementally imposing
new constraints on the previous solution, compelling LLMs to adopt new
strategies, and (2) defining and computing the NeoGauge metric which examines
both convergent and divergent thinking in the generated creative responses by
LLMs. We apply the proposed framework on Codeforces problems, a natural data
source for collecting human coding solutions. We quantify NeoGauge for various
proprietary and open-source models and find that even the most creative model,
GPT-4, still falls short of demonstrating human-like creativity. We also
experiment with advanced reasoning strategies (MCTS, self-correction, etc.) and
observe no significant improvement in creativity. As a by-product of our
analysis, we release NeoCoder dataset for reproducing our results on future
models.","[{'name': 'Yining Lu'}, {'name': 'Dixuan Wang'}, {'name': 'Tianjian Li'}, {'name': 'Dongwei Jiang'}, {'name': 'Daniel Khashabi'}]",2024-07-12T05:55:22Z
http://arxiv.org/abs/2407.08995v1,http://arxiv.org/abs/2407.08995v1,Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs,"Recent advancements in LLMs have showcased their remarkable role-playing
capabilities, able to accurately simulate the dialogue styles and cognitive
processes of various roles based on different instructions and contexts.
Studies indicate that assigning LLMs the roles of experts, a strategy known as
role-play prompting, can enhance their performance in the corresponding
domains. However, the prompt needs to be manually designed for the given
problem, requiring certain expertise and iterative modifications. To this end,
we propose self-prompt tuning, making LLMs themselves generate role-play
prompts through fine-tuning. Leveraging the LIMA dataset as our foundational
corpus, we employ GPT-4 to annotate role-play prompts for each data points,
resulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like
Llama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned
LLMs can automatically generate expert role prompts for any given question. We
extensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and
open-ended question test. Our empirical results illustrate that self-prompt
tuned LLMs outperform standard instruction tuned baselines across most
datasets. This highlights the great potential of utilizing fine-tuning to
enable LLMs to self-prompt, thereby automating complex prompting strategies. We
release the dataset, models, and code at this
\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.","[{'name': 'Aobo Kong'}, {'name': 'Shiwan Zhao'}, {'name': 'Hao Chen'}, {'name': 'Qicheng Li'}, {'name': 'Yong Qin'}, {'name': 'Ruiqi Sun'}, {'name': 'Xin Zhou'}, {'name': 'Jiaming Zhou'}, {'name': 'Haoqin Sun'}]",2024-07-12T05:26:24Z
http://arxiv.org/abs/2407.08989v1,http://arxiv.org/abs/2407.08989v1,Robustness of LLMs to Perturbations in Text,"Having a clean dataset has been the foundational assumption of most natural
language processing (NLP) systems. However, properly written text is rarely
found in real-world scenarios and hence, oftentimes invalidates the
aforementioned foundational assumption. Recently, Large language models (LLMs)
have shown impressive performance, but can they handle the inevitable noise in
real-world data? This work tackles this critical question by investigating
LLMs' resilience against morphological variations in text. To that end, we
artificially introduce varying levels of noise into a diverse set of datasets
and systematically evaluate LLMs' robustness against the corrupt variations of
the original text. Our findings show that contrary to popular beliefs,
generative LLMs are quiet robust to noisy perturbations in text. This is a
departure from pre-trained models like BERT or RoBERTa whose performance has
been shown to be sensitive to deteriorating noisy text. Additionally, we test
LLMs' resilience on multiple real-world benchmarks that closely mimic commonly
found errors in the wild. With minimal prompting, LLMs achieve a new
state-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and
Lexical Semantic Change (LSC). To empower future research, we also release a
dataset annotated by humans stating their preference for LLM vs.
human-corrected outputs along with the code to reproduce our results.","[{'name': 'Ayush Singh'}, {'name': 'Navpreet Singh'}, {'name': 'Shubham Vatsal'}]",2024-07-12T04:50:17Z
http://arxiv.org/abs/2407.08959v1,http://arxiv.org/abs/2407.08959v1,"Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for
  Few-shot Hierarchical Text Classification","Recently, various pre-trained language models (PLMs) have been proposed to
prove their impressive performances on a wide range of few-shot tasks. However,
limited by the unstructured prior knowledge in PLMs, it is difficult to
maintain consistent performance on complex structured scenarios, such as
hierarchical text classification (HTC), especially when the downstream data is
extremely scarce. The main challenge is how to transfer the unstructured
semantic space in PLMs to the downstream domain hierarchy. Unlike previous work
on HTC which directly performs multi-label classification or uses graph neural
network (GNN) to inject label hierarchy, in this work, we study the HTC problem
under a few-shot setting to adapt knowledge in PLMs from an unstructured manner
to the downstream hierarchy. Technically, we design a simple yet effective
method named Hierarchical Iterative Conditional Random Field (HierICRF) to
search the most domain-challenging directions and exquisitely crafts
domain-hierarchy adaptation as a hierarchical iterative language modeling
problem, and then it encourages the model to make hierarchical consistency
self-correction during the inference, thereby achieving knowledge transfer with
hierarchical consistency preservation. We perform HierICRF on various
architectures, and extensive experiments on two popular HTC datasets
demonstrate that prompt with HierICRF significantly boosts the few-shot HTC
performance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%
to 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot
settings, while remaining SOTA hierarchical consistency performance.","[{'name': 'Ke Ji'}, {'name': 'Peng Wang'}, {'name': 'Wenjun Ke'}, {'name': 'Guozheng Li'}, {'name': 'Jiajun Liu'}, {'name': 'Jingsheng Gao'}, {'name': 'Ziyu Shang'}]",2024-07-12T03:21:57Z
http://arxiv.org/abs/2407.08952v1,http://arxiv.org/abs/2407.08952v1,"Detect, Investigate, Judge and Determine: A Novel LLM-based Framework
  for Few-shot Fake News Detection","Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news
from real ones in extremely low-resource scenarios. This task has garnered
increased attention due to the widespread dissemination and harmful impact of
fake news on social media. Large Language Models (LLMs) have demonstrated
competitive performance with the help of their rich prior knowledge and
excellent in-context learning abilities. However, existing methods face
significant limitations, such as the Understanding Ambiguity and Information
Scarcity, which significantly undermine the potential of LLMs. To address these
shortcomings, we propose a Dual-perspective Augmented Fake News Detection
(DAFND) model, designed to enhance LLMs from both inside and outside
perspectives. Specifically, DAFND first identifies the keywords of each news
article through a Detection Module. Subsequently, DAFND creatively designs an
Investigation Module to retrieve inside and outside valuable information
concerning to the current news, followed by another Judge Module to derive its
respective two prediction results. Finally, a Determination Module further
integrates these two predictions and derives the final result. Extensive
experiments on two publicly available datasets show the efficacy of our
proposed method, particularly in low-resource settings.","[{'name': 'Ye Liu'}, {'name': 'Jiajun Zhu'}, {'name': 'Kai Zhang'}, {'name': 'Haoyu Tang'}, {'name': 'Yanghai Zhang'}, {'name': 'Xukai Liu'}, {'name': 'Qi Liu'}, {'name': 'Enhong Chen'}]",2024-07-12T03:15:01Z
http://arxiv.org/abs/2407.08940v2,http://arxiv.org/abs/2407.08940v2,"Large Language Models as Biomedical Hypothesis Generators: A
  Comprehensive Evaluation","The rapid growth of biomedical knowledge has outpaced our ability to
efficiently extract insights and generate novel hypotheses. Large language
models (LLMs) have emerged as a promising tool to revolutionize knowledge
interaction and potentially accelerate biomedical discovery. In this paper, we
present a comprehensive evaluation of LLMs as biomedical hypothesis generators.
We construct a dataset of background-hypothesis pairs from biomedical
literature, carefully partitioned into training, seen, and unseen test sets
based on publication date to mitigate data contamination. Using this dataset,
we assess the hypothesis generation capabilities of top-tier instructed models
in zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of
uncertainty, a crucial aspect of scientific discovery, we incorporate tool use
and multi-agent interactions in our evaluation framework. Furthermore, we
propose four novel metrics grounded in extensive literature review to evaluate
the quality of generated hypotheses, considering both LLM-based and human
assessments. Our experiments yield two key findings: 1) LLMs can generate novel
and validated hypotheses, even when tested on literature unseen during
training, and 2) Increasing uncertainty through multi-agent interactions and
tool use can facilitate diverse candidate generation and improve zero-shot
hypothesis generation performance. However, we also observe that the
integration of additional knowledge through few-shot learning and tool use may
not always lead to performance gains, highlighting the need for careful
consideration of the type and scope of external knowledge incorporated. These
findings underscore the potential of LLMs as powerful aids in biomedical
hypothesis generation and provide valuable insights to guide further research
in this area.","[{'name': 'Biqing Qi'}, {'name': 'Kaiyan Zhang'}, {'name': 'Kai Tian'}, {'name': 'Haoxiang Li'}, {'name': 'Zhang-Ren Chen'}, {'name': 'Sihang Zeng'}, {'name': 'Ermo Hua'}, {'name': 'Hu Jinfang'}, {'name': 'Bowen Zhou'}]",2024-07-12T02:55:13Z
http://arxiv.org/abs/2407.08937v1,http://arxiv.org/abs/2407.08937v1,Self-Evolving GPT: A Lifelong Autonomous Experiential Learner,"To improve the performance of large language models (LLMs), researchers have
explored providing LLMs with textual task-solving experience via prompts.
However, they rely on manual efforts to acquire and apply such experience for
each task, which is not feasible for the growing demand for LLMs and the
variety of user questions. To address this issue, we design a lifelong
autonomous experiential learning framework based on LLMs to explore whether
LLMs can imitate human ability for learning and utilizing experience. It
autonomously learns and accumulates experience through experience transfer and
induction, categorizing the types of input questions to select which
accumulated experience to employ for them. Experimental results on six widely
used NLP datasets show that our framework performs reliably in each
intermediate step and effectively improves the performance of GPT-3.5 and
GPT-4. This validates the feasibility of using LLMs to mimic human experiential
learning and application capabilities. Additionally, we provide a detailed
analysis of the behavior of our framework at each step.","[{'name': 'Jinglong Gao'}, {'name': 'Xiao Ding'}, {'name': 'Yiming Cui'}, {'name': 'Jianbai Zhao'}, {'name': 'Hepeng Wang'}, {'name': 'Ting Liu'}, {'name': 'Bing Qin'}]",2024-07-12T02:49:13Z
http://arxiv.org/abs/2407.08898v1,http://arxiv.org/abs/2407.08898v1,"IDAT: A Multi-Modal Dataset and Toolkit for Building and Evaluating
  Interactive Task-Solving Agents","Seamless interaction between AI agents and humans using natural language
remains a key goal in AI research. This paper addresses the challenges of
developing interactive agents capable of understanding and executing grounded
natural language instructions through the IGLU competition at NeurIPS. Despite
advancements, challenges such as a scarcity of appropriate datasets and the
need for effective evaluation platforms persist. We introduce a scalable data
collection tool for gathering interactive grounded language instructions within
a Minecraft-like environment, resulting in a Multi-Modal dataset with around
9,000 utterances and over 1,000 clarification questions. Additionally, we
present a Human-in-the-Loop interactive evaluation platform for qualitative
analysis and comparison of agent performance through multi-turn communication
with human annotators. We offer to the community these assets referred to as
IDAT (IGLU Dataset And Toolkit) which aim to advance the development of
intelligent, interactive AI agents and provide essential resources for further
research.","[{'name': 'Shrestha Mohanty'}, {'name': 'Negar Arabzadeh'}, {'name': 'Andrea Tupini'}, {'name': 'Yuxuan Sun'}, {'name': 'Alexey Skrynnik'}, {'name': 'Artem Zholus'}, {'name': 'Marc-Alexandre Côté'}, {'name': 'Julia Kiseleva'}]",2024-07-12T00:07:43Z
http://arxiv.org/abs/2407.08892v1,http://arxiv.org/abs/2407.08892v1,Characterizing Prompt Compression Methods for Long Context Inference,"Long context inference presents challenges at the system level with increased
compute and memory requirements, as well as from an accuracy perspective in
being able to reason over long contexts. Recently, several methods have been
proposed to compress the prompt to reduce the context length. However, there
has been little work on comparing the different proposed methods across
different tasks through a standardized analysis. This has led to conflicting
results. To address this, here we perform a comprehensive characterization and
evaluation of different prompt compression methods. In particular, we analyze
extractive compression, summarization-based abstractive compression, and token
pruning methods. Surprisingly, we find that extractive compression often
outperforms all the other approaches, and enables up to 10x compression with
minimal accuracy degradation. Interestingly, we also find that despite several
recent claims, token pruning methods often lag behind extractive compression.
We only found marginal improvements on summarization tasks.","[{'name': 'Siddharth Jha'}, {'name': 'Lutfi Eren Erdogan'}, {'name': 'Sehoon Kim'}, {'name': 'Kurt Keutzer'}, {'name': 'Amir Gholami'}]",2024-07-11T23:34:32Z
http://arxiv.org/abs/2407.08887v1,http://arxiv.org/abs/2407.08887v1,"Automatic Pruning of Fine-tuning Datasets for Transformer-based Language
  Models","Transformer-based language models have shown state-of-the-art performance on
a variety of natural language understanding tasks. To achieve this performance,
these models are first pre-trained on general corpus and then fine-tuned on
downstream tasks. Previous work studied the effect of pruning the training set
of the downstream tasks on the performance of the model on its evaluation set.
In this work, we propose an automatic dataset pruning method for the training
set of fine-tuning tasks. Our method is based on the model's success rate in
correctly classifying each training data point. Unlike previous work which
relies on user feedback to determine subset size, our method automatically
extracts training subsets that are adapted for each pair of model and
fine-tuning task. Our method provides multiple subsets for use in dataset
pruning that navigate the trade-off between subset size and evaluation
accuracy. Our largest subset, which we also refer to as the winning ticket
subset, is on average $3 \times$ smaller than the original training set of the
fine-tuning task. Our experiments on 5 downstream tasks and 2 language models
show that, on average, fine-tuning on the winning ticket subsets results in a
$0.1 \%$ increase in the evaluation performance of the model.","[{'name': 'Mohammadreza Tayaranian'}, {'name': 'Seyyed Hasan Mozafari'}, {'name': 'Brett H. Meyer'}, {'name': 'James J. Clark'}, {'name': 'Warren J. Gross'}]",2024-07-11T22:46:18Z
http://arxiv.org/abs/2407.08853v1,http://arxiv.org/abs/2407.08853v1,"GPT-4 is judged more human than humans in displaced and inverted Turing
  tests","Everyday AI detection requires differentiating between people and AI in
informal, online conversations. In many cases, people will not interact
directly with AI systems but instead read conversations between AI systems and
other people. We measured how well people and large language models can
discriminate using two modified versions of the Turing test: inverted and
displaced. GPT-3.5, GPT-4, and displaced human adjudicators judged whether an
agent was human or AI on the basis of a Turing test transcript. We found that
both AI and displaced human judges were less accurate than interactive
interrogators, with below chance accuracy overall. Moreover, all three judged
the best-performing GPT-4 witness to be human more often than human witnesses.
This suggests that both humans and current LLMs struggle to distinguish between
the two when they are not actively interrogating the person, underscoring an
urgent need for more accurate tools to detect AI in conversations.","[{'name': 'Ishika Rathi'}, {'name': 'Sydney Taylor'}, {'name': 'Benjamin K. Bergen'}, {'name': 'Cameron R. Jones'}]",2024-07-11T20:28:24Z
http://arxiv.org/abs/2407.08842v1,http://arxiv.org/abs/2407.08842v1,Evaluating Nuanced Bias in Large Language Model Free Response Answers,"Pre-trained large language models (LLMs) can now be easily adapted for
specific business purposes using custom prompts or fine tuning. These
customizations are often iteratively re-engineered to improve some aspect of
performance, but after each change businesses want to ensure that there has
been no negative impact on the system's behavior around such critical issues as
bias. Prior methods of benchmarking bias use techniques such as word masking
and multiple choice questions to assess bias at scale, but these do not capture
all of the nuanced types of bias that can occur in free response answers, the
types of answers typically generated by LLM systems. In this paper, we identify
several kinds of nuanced bias in free text that cannot be similarly identified
by multiple choice tests. We describe these as: confidence bias, implied bias,
inclusion bias and erasure bias. We present a semi-automated pipeline for
detecting these types of bias by first eliminating answers that can be
automatically classified as unbiased and then co-evaluating name reversed pairs
using crowd workers. We believe that the nuanced classifications our method
generates can be used to give better feedback to LLMs, especially as LLM
reasoning capabilities become more advanced.","[{'name': 'Jennifer Healey'}, {'name': 'Laurie Byrum'}, {'name': 'Md Nadeem Akhtar'}, {'name': 'Moumita Sinha'}]",2024-07-11T19:58:13Z
http://arxiv.org/abs/2407.08836v1,http://arxiv.org/abs/2407.08836v1,Fault Diagnosis in Power Grids with Large Language Model,"Power grid fault diagnosis is a critical task for ensuring the reliability
and stability of electrical infrastructure. Traditional diagnostic systems
often struggle with the complexity and variability of power grid data. This
paper proposes a novel approach that leverages Large Language Models (LLMs),
specifically ChatGPT and GPT-4, combined with advanced prompt engineering to
enhance fault diagnosis accuracy and explainability. We designed comprehensive,
context-aware prompts to guide the LLMs in interpreting complex data and
providing detailed, actionable insights. Our method was evaluated against
baseline techniques, including standard prompting, Chain-of-Thought (CoT), and
Tree-of-Thought (ToT) methods, using a newly constructed dataset comprising
real-time sensor data, historical fault records, and component descriptions.
Experimental results demonstrate significant improvements in diagnostic
accuracy, explainability quality, response coherence, and contextual
understanding, underscoring the effectiveness of our approach. These findings
suggest that prompt-engineered LLMs offer a promising solution for robust and
reliable power grid fault diagnosis.","[{'name': 'Liu Jing'}, {'name': 'Amirul Rahman'}]",2024-07-11T19:44:18Z
http://arxiv.org/abs/2407.08818v1,http://arxiv.org/abs/2407.08818v1,"MAGNET: Improving the Multilingual Fairness of Language Models with
  Adaptive Gradient-Based Tokenization","In multilingual settings, non-Latin scripts and low-resource languages are
usually disadvantaged in terms of language models' utility, efficiency, and
cost. Specifically, previous studies have reported multiple modeling biases
that the current tokenization algorithms introduce to non-Latin script
languages, the main one being over-segmentation. In this work, we propose
MAGNET; multilingual adaptive gradient-based tokenization to reduce
over-segmentation via adaptive gradient-based subword tokenization. MAGNET
learns to predict segment boundaries between byte tokens in a sequence via
sub-modules within the model, which act as internal boundary predictors
(tokenizers). Previous gradient-based tokenization methods aimed for uniform
compression across sequences by integrating a single boundary predictor during
training and optimizing it end-to-end through stochastic reparameterization
alongside the next token prediction objective. However, this approach still
results in over-segmentation for non-Latin script languages in multilingual
settings. In contrast, MAGNET offers a customizable architecture where
byte-level sequences are routed through language-script-specific predictors,
each optimized for its respective language script. This modularity enforces
equitable segmentation granularity across different language scripts compared
to previous methods. Through extensive experiments, we demonstrate that in
addition to reducing segmentation disparities, MAGNET also enables faster
language modelling and improves downstream utility.","[{'name': 'Orevaoghene Ahia'}, {'name': 'Sachin Kumar'}, {'name': 'Hila Gonen'}, {'name': 'Valentin Hoffman'}, {'name': 'Tomasz Limisiewicz'}, {'name': 'Yulia Tsvetkov'}, {'name': 'Noah A. Smith'}]",2024-07-11T18:59:21Z
http://arxiv.org/abs/2408.01911v1,http://arxiv.org/abs/2408.01911v1,"Brief state of the art in social information mining: Practical
  application in analysis of trends in French legislative 2024","The analysis of social media information has undergone significant evolution
in the last decade due to advancements in artificial intelligence (AI) and
machine learning (ML). This paper provides an overview of the state-of-the-art
techniques in social media mining, with a practical application in analyzing
trends in the 2024 French legislative elections. We leverage natural language
processing (NLP) tools to gauge public opinion by extracting and analyzing
comments and reactions from the AgoraVox platform. The study reveals that the
National Rally party, led by Marine Le Pen, maintains a high level of
engagement on social media, outperforming traditional parties. This trend is
corroborated by user interactions, indicating a strong digital presence. The
results highlight the utility of advanced AI models, such as transformers and
large language models (LLMs), in capturing nuanced public sentiments and
predicting political leanings, demonstrating their potential in real-time
reputation management and crisis response.",[{'name': 'Jose A. Garcia Gutierrez'}],2024-07-11T18:22:58Z
http://arxiv.org/abs/2407.08790v1,http://arxiv.org/abs/2407.08790v1,"Large Models of What? Mistaking Engineering Achievements for Human
  Linguistic Agency","In this paper we argue that key, often sensational and misleading, claims
regarding linguistic capabilities of Large Language Models (LLMs) are based on
at least two unfounded assumptions; the assumption of language completeness and
the assumption of data completeness. Language completeness assumes that a
distinct and complete thing such as `a natural language' exists, the essential
characteristics of which can be effectively and comprehensively modelled by an
LLM. The assumption of data completeness relies on the belief that a language
can be quantified and wholly captured by data. Work within the enactive
approach to cognitive science makes clear that, rather than a distinct and
complete thing, language is a means or way of acting. Languaging is not the
kind of thing that can admit of a complete or comprehensive modelling. From an
enactive perspective we identify three key characteristics of enacted language;
embodiment, participation, and precariousness, that are absent in LLMs, and
likely incompatible in principle with current architectures. We argue that
these absences imply that LLMs are not now and cannot in their present form be
linguistic agents the way humans are. We illustrate the point in particular
through the phenomenon of `algospeak', a recently described pattern of high
stakes human language activity in heavily controlled online environments. On
the basis of these points, we conclude that sensational and misleading claims
about LLM agency and capabilities emerge from a deep misconception of both what
human language is and what LLMs are.","[{'name': 'Abeba Birhane'}, {'name': 'Marek McGann'}]",2024-07-11T18:06:01Z
http://arxiv.org/abs/2407.08734v1,http://arxiv.org/abs/2407.08734v1,Transformer Circuit Faithfulness Metrics are not Robust,"Mechanistic interpretability work attempts to reverse engineer the learned
algorithms present inside neural networks. One focus of this work has been to
discover 'circuits' -- subgraphs of the full model that explain behaviour on
specific tasks. But how do we measure the performance of such circuits? Prior
work has attempted to measure circuit 'faithfulness' -- the degree to which the
circuit replicates the performance of the full model. In this work, we survey
many considerations for designing experiments that measure circuit faithfulness
by ablating portions of the model's computation. Concerningly, we find existing
methods are highly sensitive to seemingly insignificant changes in the ablation
methodology. We conclude that existing circuit faithfulness scores reflect both
the methodological choices of researchers as well as the actual components of
the circuit - the task a circuit is required to perform depends on the ablation
used to test it. The ultimate goal of mechanistic interpretability work is to
understand neural networks, so we emphasize the need for more clarity in the
precise claims being made about circuits. We open source a library at
https://github.com/UFO-101/auto-circuit that includes highly efficient
implementations of a wide range of ablation methodologies and circuit discovery
algorithms.","[{'name': 'Joseph Miller'}, {'name': 'Bilal Chughtai'}, {'name': 'William Saunders'}]",2024-07-11T17:59:00Z
http://arxiv.org/abs/2407.08733v1,http://arxiv.org/abs/2407.08733v1,"Is Your Model Really A Good Math Reasoner? Evaluating Mathematical
  Reasoning with Checklist","Exceptional mathematical reasoning ability is one of the key features that
demonstrate the power of large language models (LLMs). How to comprehensively
define and evaluate the mathematical abilities of LLMs, and even reflect the
user experience in real-world scenarios, has emerged as a critical issue.
Current benchmarks predominantly concentrate on problem-solving capabilities,
which presents a substantial risk of model overfitting and fails to accurately
represent genuine mathematical reasoning abilities. In this paper, we argue
that if a model really understands a problem, it should be robustly and readily
applied across a diverse array of tasks. Motivated by this, we introduce
MATHCHECK, a well-designed checklist for testing task generalization and
reasoning robustness, as well as an automatic tool to generate checklists
efficiently. MATHCHECK includes multiple mathematical reasoning tasks and
robustness test types to facilitate a comprehensive evaluation of both
mathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we
develop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual
reasoning and multi-modal reasoning capabilities, respectively, serving as
upgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K.
We adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs,
assessing their comprehensive mathematical reasoning abilities. Our results
demonstrate that while frontier LLMs like GPT-4o continue to excel in various
abilities on the checklist, many other model families exhibit a significant
decline. Further experiments indicate that, compared to traditional math
benchmarks, MATHCHECK better reflects true mathematical abilities and
represents mathematical intelligence more linearly, thereby supporting our
design. On our MATHCHECK, we can easily conduct detailed behavior analysis to
deeply investigate models.","[{'name': 'Zihao Zhou'}, {'name': 'Shudong Liu'}, {'name': 'Maizhen Ning'}, {'name': 'Wei Liu'}, {'name': 'Jindong Wang'}, {'name': 'Derek F. Wong'}, {'name': 'Xiaowei Huang'}, {'name': 'Qiufeng Wang'}, {'name': 'Kaizhu Huang'}]",2024-07-11T17:58:58Z
http://arxiv.org/abs/2407.08716v1,http://arxiv.org/abs/2407.08716v1,A Taxonomy for Data Contamination in Large Language Models,"Large language models pretrained on extensive web corpora demonstrate
remarkable performance across a wide range of downstream tasks. However, a
growing concern is data contamination, where evaluation datasets may be
contained in the pretraining corpus, inflating model performance.
Decontamination, the process of detecting and removing such data, is a
potential solution; yet these contaminants may originate from altered versions
of the test set, evading detection during decontamination. How different types
of contamination impact the performance of language models on downstream tasks
is not fully understood. We present a taxonomy that categorizes the various
types of contamination encountered by LLMs during the pretraining phase and
identify which types pose the highest risk. We analyze the impact of
contamination on two key NLP tasks -- summarization and question answering --
revealing how different types of contamination influence task performance
during evaluation.","[{'name': 'Medha Palavalli'}, {'name': 'Amanda Bertsch'}, {'name': 'Matthew R. Gormley'}]",2024-07-11T17:50:34Z
http://arxiv.org/abs/2407.08713v1,http://arxiv.org/abs/2407.08713v1,GTA: A Benchmark for General Tool Agents,"Significant focus has been placed on integrating large language models (LLMs)
with various tools in developing general-purpose agents. This poses a challenge
to LLMs' tool-use capabilities. However, there are evident gaps between
existing tool-use evaluations and real-world scenarios. Current evaluations
often use AI-generated queries, single-step tasks, dummy tools, and text-only
interactions, failing to reveal the agents' real-world problem-solving
abilities effectively. To address this, we propose GTA, a benchmark for General
Tool Agents, featuring three main aspects: (i) Real user queries: human-written
queries with simple real-world objectives but implicit tool-use, requiring the
LLM to reason the suitable tools and plan the solution steps. (ii) Real
deployed tools: an evaluation platform equipped with tools across perception,
operation, logic, and creativity categories to evaluate the agents' actual task
execution performance. (iii) Real multimodal inputs: authentic image files,
such as spatial scenes, web page screenshots, tables, code snippets, and
printed/handwritten materials, used as the query contexts to align with
real-world scenarios closely. We design 229 real-world tasks and executable
tool chains to evaluate mainstream LLMs. Our findings show that real-world user
queries are challenging for existing LLMs, with GPT-4 completing less than 50%
of the tasks and most LLMs achieving below 25%. This evaluation reveals the
bottlenecks in the tool-use capabilities of current LLMs in real-world
scenarios, which provides future direction for advancing general-purpose tool
agents. The code and dataset are available at
https://github.com/open-compass/GTA.","[{'name': 'Jize Wang'}, {'name': 'Zerun Ma'}, {'name': 'Yining Li'}, {'name': 'Songyang Zhang'}, {'name': 'Cailian Chen'}, {'name': 'Kai Chen'}, {'name': 'Xinyi Le'}]",2024-07-11T17:50:09Z
http://arxiv.org/abs/2407.08662v1,http://arxiv.org/abs/2407.08662v1,"Uncertainty Estimation of Large Language Models in Medical Question
  Answering","Large Language Models (LLMs) show promise for natural language generation in
healthcare, but risk hallucinating factually incorrect information. Deploying
LLMs for medical question answering necessitates reliable uncertainty
estimation (UE) methods to detect hallucinations. In this work, we benchmark
popular UE methods with different model sizes on medical question-answering
datasets. Our results show that current approaches generally perform poorly in
this domain, highlighting the challenge of UE for medical applications. We also
observe that larger models tend to yield better results, suggesting a
correlation between model size and the reliability of UE. To address these
challenges, we propose Two-phase Verification, a probability-free Uncertainty
Estimation approach. First, an LLM generates a step-by-step explanation
alongside its initial answer, followed by formulating verification questions to
check the factual claims in the explanation. The model then answers these
questions twice: first independently, and then referencing the explanation.
Inconsistencies between the two sets of answers measure the uncertainty in the
original response. We evaluate our approach on three biomedical
question-answering datasets using Llama 2 Chat models and compare it against
the benchmarked baseline methods. The results show that our Two-phase
Verification method achieves the best overall accuracy and stability across
various datasets and model sizes, and its performance scales as the model size
increases.","[{'name': 'Jiaxin Wu'}, {'name': 'Yizhou Yu'}, {'name': 'Hong-Yu Zhou'}]",2024-07-11T16:51:33Z
http://arxiv.org/abs/2407.08607v1,http://arxiv.org/abs/2407.08607v1,Turn-Level Empathy Prediction Using Psychological Indicators,"For the WASSA 2024 Empathy and Personality Prediction Shared Task, we propose
a novel turn-level empathy detection method that decomposes empathy into six
psychological indicators: Emotional Language, Perspective-Taking, Sympathy and
Compassion, Extroversion, Openness, and Agreeableness. A pipeline of text
enrichment using a Large Language Model (LLM) followed by DeBERTA fine-tuning
demonstrates a significant improvement in the Pearson Correlation Coefficient
and F1 scores for empathy detection, highlighting the effectiveness of our
approach. Our system officially ranked 7th at the CONV-turn track.","[{'name': 'Shaz Furniturewala'}, {'name': 'Kokil Jaidka'}]",2024-07-11T15:43:27Z
http://arxiv.org/abs/2407.08582v1,http://arxiv.org/abs/2407.08582v1,On the Universal Truthfulness Hyperplane Inside LLMs,"While large language models (LLMs) have demonstrated remarkable abilities
across various fields, hallucination remains a significant challenge. Recent
studies have explored hallucinations through the lens of internal
representations, proposing mechanisms to decipher LLMs' adherence to facts.
However, these approaches often fail to generalize to out-of-distribution data,
leading to concerns about whether internal representation patterns reflect
fundamental factual awareness, or only overfit spurious correlations on the
specific datasets. In this work, we investigate whether a universal
truthfulness hyperplane that distinguishes the model's factually correct and
incorrect outputs exists within the model. To this end, we scale up the number
of training datasets and conduct an extensive evaluation -- we train the
truthfulness hyperplane on a diverse collection of over 40 datasets and examine
its cross-task, cross-domain, and in-domain generalization. Our results
indicate that increasing the diversity of the training datasets significantly
enhances the performance in all scenarios, while the volume of data samples
plays a less critical role. This finding supports the optimistic hypothesis
that a universal truthfulness hyperplane may indeed exist within the model,
offering promising directions for future research.","[{'name': 'Junteng Liu'}, {'name': 'Shiqi Chen'}, {'name': 'Yu Cheng'}, {'name': 'Junxian He'}]",2024-07-11T15:07:26Z
http://arxiv.org/abs/2407.08551v1,http://arxiv.org/abs/2407.08551v1,Autoregressive Speech Synthesis without Vector Quantization,"We present MELLE, a novel continuous-valued tokens based language modeling
approach for text to speech synthesis (TTS). MELLE autoregressively generates
continuous mel-spectrogram frames directly from text condition, bypassing the
need for vector quantization, which are originally designed for audio
compression and sacrifice fidelity compared to mel-spectrograms. Specifically,
(i) instead of cross-entropy loss, we apply regression loss with a proposed
spectrogram flux loss function to model the probability distribution of the
continuous-valued tokens. (ii) we have incorporated variational inference into
MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity
and model robustness. Experiments demonstrate that, compared to the two-stage
codec language models VALL-E and its variants, the single-stage MELLE mitigates
robustness issues by avoiding the inherent flaws of sampling discrete codes,
achieves superior performance across multiple metrics, and, most importantly,
offers a more streamlined paradigm. See https://aka.ms/melle for demos of our
work.","[{'name': 'Lingwei Meng'}, {'name': 'Long Zhou'}, {'name': 'Shujie Liu'}, {'name': 'Sanyuan Chen'}, {'name': 'Bing Han'}, {'name': 'Shujie Hu'}, {'name': 'Yanqing Liu'}, {'name': 'Jinyu Li'}, {'name': 'Sheng Zhao'}, {'name': 'Xixin Wu'}, {'name': 'Helen Meng'}, {'name': 'Furu Wei'}]",2024-07-11T14:36:53Z
http://arxiv.org/abs/2407.08521v2,http://arxiv.org/abs/2407.08521v2,Emergent Visual-Semantic Hierarchies in Image-Text Representations,"While recent vision-and-language models (VLMs) like CLIP are a powerful tool
for analyzing text and images in a shared semantic space, they do not
explicitly model the hierarchical nature of the set of texts which may describe
an image. Conversely, existing multimodal hierarchical representation learning
methods require costly training from scratch, failing to leverage the knowledge
encoded by state-of-the-art multimodal foundation models. In this work, we
study the knowledge of existing foundation models, finding that they exhibit
emergent understanding of visual-semantic hierarchies despite not being
directly trained for this purpose. We propose the Radial Embedding (RE)
framework for probing and optimizing hierarchical understanding, and contribute
the HierarCaps dataset, a benchmark facilitating the study of hierarchical
knowledge in image--text representations, constructed automatically via large
language models. Our results show that foundation VLMs exhibit zero-shot
hierarchical understanding, surpassing the performance of prior models
explicitly designed for this purpose. Furthermore, we show that foundation
models may be better aligned to hierarchical reasoning via a text-only
fine-tuning phase, while retaining pretraining knowledge.","[{'name': 'Morris Alper'}, {'name': 'Hadar Averbuch-Elor'}]",2024-07-11T14:09:42Z
http://arxiv.org/abs/2407.08495v1,http://arxiv.org/abs/2407.08495v1,"Investigating LLMs as Voting Assistants via Contextual Augmentation: A
  Case Study on the European Parliament Elections 2024","Instruction-finetuned Large Language Models exhibit unprecedented Natural
Language Understanding capabilities. Recent work has been exploring political
biases and political reasoning capabilities in LLMs, mainly scoped in the US
context. In light of the recent 2024 European Parliament elections, we are
investigating if LLMs can be used as Voting Advice Applications (VAAs). We
audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the
stance of political parties based on the latest ""EU and I"" voting assistance
questionnaire. Furthermore, we explore alternatives to improve models'
performance by augmenting the input context via Retrieval-Augmented Generation
(RAG) relying on web search, and Self-Reflection using staged conversations
that aim to re-collect relevant content from the model's internal memory. We
find that MIXTRAL is highly accurate with an 82% accuracy on average.
Augmenting the input context with expert-curated information can lead to a
significant boost of approx. 9%, which remains an open challenge for automated
approaches.",[{'name': 'Ilias Chalkidis'}],2024-07-11T13:29:28Z
http://arxiv.org/abs/2407.08488v2,http://arxiv.org/abs/2407.08488v2,Lynx: An Open Source Hallucination Evaluation Model,"Retrieval Augmented Generation (RAG) techniques aim to mitigate
hallucinations in Large Language Models (LLMs). However, LLMs can still produce
information that is unsupported or contradictory to the retrieved contexts. We
introduce LYNX, a SOTA hallucination detection LLM that is capable of advanced
reasoning on challenging real-world hallucination scenarios. To evaluate LYNX,
we present HaluBench, a comprehensive hallucination evaluation benchmark,
consisting of 15k samples sourced from various real-world domains. Our
experiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and
closed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,
HaluBench and our evaluation code for public access.","[{'name': 'Selvan Sunitha Ravi'}, {'name': 'Bartosz Mielczarek'}, {'name': 'Anand Kannappan'}, {'name': 'Douwe Kiela'}, {'name': 'Rebecca Qian'}]",2024-07-11T13:22:17Z
http://arxiv.org/abs/2407.08441v1,http://arxiv.org/abs/2407.08441v1,"Are Large Language Models Really Bias-Free? Jailbreak Prompts for
  Assessing Adversarial Robustness to Bias Elicitation","Large Language Models (LLMs) have revolutionized artificial intelligence,
demonstrating remarkable computational power and linguistic capabilities.
However, these models are inherently prone to various biases stemming from
their training data. These include selection, linguistic, and confirmation
biases, along with common stereotypes related to gender, ethnicity, sexual
orientation, religion, socioeconomic status, disability, and age. This study
explores the presence of these biases within the responses given by the most
recent LLMs, analyzing the impact on their fairness and reliability. We also
investigate how known prompt engineering techniques can be exploited to
effectively reveal hidden biases of LLMs, testing their adversarial robustness
against jailbreak prompts specially crafted for bias elicitation. Extensive
experiments are conducted using the most widespread LLMs at different scales,
confirming that LLMs can still be manipulated to produce biased or
inappropriate responses, despite their advanced capabilities and sophisticated
alignment processes. Our findings underscore the importance of enhancing
mitigation techniques to address these safety issues, toward a more sustainable
and inclusive artificial intelligence.","[{'name': 'Riccardo Cantini'}, {'name': 'Giada Cosenza'}, {'name': 'Alessio Orsino'}, {'name': 'Domenico Talia'}]",2024-07-11T12:30:19Z
http://arxiv.org/abs/2407.08440v2,http://arxiv.org/abs/2407.08440v2,"Beyond Instruction Following: Evaluating Inferential Rule Following of
  Large Language Models","Although Large Language Models (LLMs) have demonstrated strong
instruction-following ability, they are further supposed to be controlled and
guided by rules in real-world scenarios to be safe, accurate, and intelligent.
This demands the possession of inferential rule-following capability of LLMs.
However, few works have made a clear evaluation of the inferential
rule-following capability of LLMs. Previous studies that try to evaluate the
inferential rule-following capability of LLMs fail to distinguish the
inferential rule-following scenarios from the instruction-following scenarios.
Therefore, this paper first clarifies the concept of inferential rule-following
and proposes a comprehensive benchmark, RuleBench, to evaluate a diversified
range of inferential rule-following abilities. Our experimental results on a
variety of LLMs show that they are still limited in following rules. Our
analysis based on the evaluation results provides insights into the
improvements for LLMs toward a better inferential rule-following intelligent
agent. We further propose Inferential Rule-Following Tuning (IRFT), which
outperforms IFT in helping LLMs solve RuleBench. The data and code can be found
at: https://anonymous.4open.science/r/llm-rule-following-B3E3/","[{'name': 'Wangtao Sun'}, {'name': 'Chenxiang Zhang'}, {'name': 'Xueyou Zhang'}, {'name': 'Ziyang Huang'}, {'name': 'Haotian Xu'}, {'name': 'Pei Chen'}, {'name': 'Shizhu He'}, {'name': 'Jun Zhao'}, {'name': 'Kang Liu'}]",2024-07-11T12:26:55Z
http://arxiv.org/abs/2407.08400v1,http://arxiv.org/abs/2407.08400v1,Self-training Language Models for Arithmetic Reasoning,"Language models achieve impressive results in tasks involving complex
multistep reasoning, but scaling these capabilities further traditionally
requires expensive collection of more annotated data. In this work, we explore
the potential of improving the capabilities of language models without new
data, merely using automated feedback to the validity of their predictions in
arithmetic reasoning (self-training). We find that models can substantially
improve in both single-round (offline) and online self-training. In the offline
setting, supervised methods are able to deliver gains comparable to preference
optimization, but in online self-training, preference optimization shows to
largely outperform supervised training thanks to superior stability and
robustness on unseen types of problems.","[{'name': 'Marek Kadlčík'}, {'name': 'Michal Štefánik'}]",2024-07-11T11:06:05Z
http://arxiv.org/abs/2407.08388v1,http://arxiv.org/abs/2407.08388v1,On the attribution of confidence to large language models,"Credences are mental states corresponding to degrees of confidence in
propositions. Attribution of credences to Large Language Models (LLMs) is
commonplace in the empirical literature on LLM evaluation. Yet the theoretical
basis for LLM credence attribution is unclear. We defend three claims. First,
our semantic claim is that LLM credence attributions are (at least in general)
correctly interpreted literally, as expressing truth-apt beliefs on the part of
scientists that purport to describe facts about LLM credences. Second, our
metaphysical claim is that the existence of LLM credences is at least
plausible, although current evidence is inconclusive. Third, our epistemic
claim is that LLM credence attributions made in the empirical literature on LLM
evaluation are subject to non-trivial sceptical concerns. It is a distinct
possibility that even if LLMs have credences, LLM credence attributions are
generally false because the experimental techniques used to assess LLM
credences are not truth-tracking.","[{'name': 'Geoff Keeling'}, {'name': 'Winnie Street'}]",2024-07-11T10:51:06Z
http://arxiv.org/abs/2407.08351v1,http://arxiv.org/abs/2407.08351v1,"AutoBencher: Creating Salient, Novel, Difficult Datasets for Language
  Models","Evaluation is critical for assessing capabilities, tracking scientific
progress, and informing model selection. In this paper, we present three
desiderata for a good benchmark for language models: (i) salience (e.g.,
knowledge about World War II is more salient than a random day in history),
(ii) novelty (i.e., the benchmark reveals new trends in model rankings not
shown by previous benchmarks), and (iii) difficulty (i.e., the benchmark should
be difficult for existing models, leaving headroom for future improvement). We
operationalize these three desiderata and cast benchmark creation as a search
problem, that of finding benchmarks that that satisfy all three desiderata. To
tackle this search problem, we present AutoBencher, which uses a language model
to automatically search for datasets that meet the three desiderata.
AutoBencher uses privileged information (e.g. relevant documents) to construct
reliable datasets, and adaptivity with reranking to optimize for the search
objective. We use AutoBencher to create datasets for math, multilingual, and
knowledge-intensive question answering. The scalability of AutoBencher allows
it to test fine-grained categories and tail knowledge, creating datasets that
are on average 27% more novel and 22% more difficult than existing benchmarks.
A closer investigation of our constructed datasets shows that we can identify
specific gaps in LM knowledge in language models that are not captured by
existing benchmarks, such as Gemini Pro performing much worse on question
answering about the Permian Extinction and Fordism, while OpenAGI-7B performing
surprisingly well on QA about COVID-19.","[{'name': 'Xiang Lisa Li'}, {'name': 'Evan Zheran Liu'}, {'name': 'Percy Liang'}, {'name': 'Tatsunori Hashimoto'}]",2024-07-11T10:03:47Z
http://arxiv.org/abs/2407.08348v2,http://arxiv.org/abs/2407.08348v2,"Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large
  Language Models -- The Story Goes On","In this paper, we investigate the underlying factors that potentially enhance
the mathematical reasoning capabilities of large language models (LLMs). We
argue that the data scaling law for math reasoning capabilities in modern LLMs
is far from being saturated, highlighting how the model's quality improves with
increases in data quantity. To support this claim, we introduce the
Skywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using
our proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved
impressive accuracies of 51.2% on the competition-level MATH benchmark and
83.9% on the GSM8K benchmark using only SFT data, outperforming an early
version of GPT-4 on MATH. The superior performance of Skywork-Math models
contributes to our novel two-stage data synthesis and model SFT pipelines,
which include three different augmentation methods and a diverse seed problem
set, ensuring both the quantity and quality of Skywork-MathQA dataset across
varying difficulty levels. Most importantly, we provide several practical
takeaways to enhance math reasoning abilities in LLMs for both research and
industry applications.","[{'name': 'Liang Zeng'}, {'name': 'Liangjun Zhong'}, {'name': 'Liang Zhao'}, {'name': 'Tianwen Wei'}, {'name': 'Liu Yang'}, {'name': 'Jujie He'}, {'name': 'Cheng Cheng'}, {'name': 'Rui Hu'}, {'name': 'Yang Liu'}, {'name': 'Shuicheng Yan'}, {'name': 'Han Fang'}, {'name': 'Yahui Zhou'}]",2024-07-11T09:56:51Z
http://arxiv.org/abs/2407.08269v1,http://arxiv.org/abs/2407.08269v1,LLMs' morphological analyses of complex FST-generated Finnish words,"Rule-based language processing systems have been overshadowed by neural
systems in terms of utility, but it remains unclear whether neural NLP systems,
in practice, learn the grammar rules that humans use. This work aims to shed
light on the issue by evaluating state-of-the-art LLMs in a task of
morphological analysis of complex Finnish noun forms. We generate the forms
using an FST tool, and they are unlikely to have occurred in the training sets
of the LLMs, therefore requiring morphological generalisation capacity. We find
that GPT-4-turbo has some difficulties in the task while GPT-3.5-turbo
struggles and smaller models Llama2-70B and Poro-34B fail nearly completely.","[{'name': 'Anssi Moisio'}, {'name': 'Mathias Creutz'}, {'name': 'Mikko Kurimo'}]",2024-07-11T08:12:28Z
http://arxiv.org/abs/2407.08223v1,http://arxiv.org/abs/2407.08223v1,"Speculative RAG: Enhancing Retrieval Augmented Generation through
  Drafting","Retrieval augmented generation (RAG) combines the generative abilities of
large language models (LLMs) with external knowledge sources to provide more
accurate and up-to-date responses. Recent RAG advancements focus on improving
retrieval outcomes through iterative LLM refinement or self-critique
capabilities acquired through additional instruction tuning of LLMs. In this
work, we introduce Speculative RAG - a framework that leverages a larger
generalist LM to efficiently verify multiple RAG drafts produced in parallel by
a smaller, distilled specialist LM. Each draft is generated from a distinct
subset of retrieved documents, offering diverse perspectives on the evidence
while reducing input token counts per draft. This approach enhances
comprehension of each subset and mitigates potential position bias over long
context. Our method accelerates RAG by delegating drafting to the smaller
specialist LM, with the larger generalist LM performing a single verification
pass over the drafts. Extensive experiments demonstrate that Speculative RAG
achieves state-of-the-art performance with reduced latency on TriviaQA,
MuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy
by up to 12.97% while reducing latency by 51% compared to conventional RAG
systems on PubHealth.","[{'name': 'Zilong Wang'}, {'name': 'Zifeng Wang'}, {'name': 'Long Le'}, {'name': 'Huaixiu Steven Zheng'}, {'name': 'Swaroop Mishra'}, {'name': 'Vincent Perot'}, {'name': 'Yuwei Zhang'}, {'name': 'Anush Mattapalli'}, {'name': 'Ankur Taly'}, {'name': 'Jingbo Shang'}, {'name': 'Chen-Yu Lee'}, {'name': 'Tomas Pfister'}]",2024-07-11T06:50:19Z
http://arxiv.org/abs/2407.08219v1,http://arxiv.org/abs/2407.08219v1,"Generating Contextually-Relevant Navigation Instructions for Blind and
  Low Vision People","Navigating unfamiliar environments presents significant challenges for blind
and low-vision (BLV) individuals. In this work, we construct a dataset of
images and goals across different scenarios such as searching through kitchens
or navigating outdoors. We then investigate how grounded instruction generation
methods can provide contextually-relevant navigational guidance to users in
these instances. Through a sighted user study, we demonstrate that large
pretrained language models can produce correct and useful instructions
perceived as beneficial for BLV users. We also conduct a survey and interview
with 4 BLV users and observe useful insights on preferences for different
instructions based on the scenario.","[{'name': 'Zain Merchant'}, {'name': 'Abrar Anwar'}, {'name': 'Emily Wang'}, {'name': 'Souti Chattopadhyay'}, {'name': 'Jesse Thomason'}]",2024-07-11T06:40:36Z
http://arxiv.org/abs/2407.08206v1,http://arxiv.org/abs/2407.08206v1,"System Report for CCL24-Eval Task 7: Multi-Error Modeling and
  Fluency-Targeted Pre-training for Chinese Essay Evaluation","This system report presents our approaches and results for the Chinese Essay
Fluency Evaluation (CEFE) task at CCL-2024. For Track 1, we optimized
predictions for challenging fine-grained error types using binary
classification models and trained coarse-grained models on the Chinese Learner
4W corpus. In Track 2, we enhanced performance by constructing a pseudo-dataset
with multiple error types per sentence. For Track 3, where we achieved first
place, we generated fluency-rated pseudo-data via back-translation for
pre-training and used an NSP-based strategy with Symmetric Cross Entropy loss
to capture context and mitigate long dependencies. Our methods effectively
address key challenges in Chinese Essay Fluency Evaluation.","[{'name': 'Jingshen Zhang'}, {'name': 'Xiangyu Yang'}, {'name': 'Xinkai Su'}, {'name': 'Xinglu Chen'}, {'name': 'Tianyou Huang'}, {'name': 'Xinying Qiu'}]",2024-07-11T06:17:08Z
http://arxiv.org/abs/2407.08195v1,http://arxiv.org/abs/2407.08195v1,A Text-to-Game Engine for UGC-Based Role-Playing Games,"The shift from professionally generated content (PGC) to user-generated
content (UGC) has revolutionized various media formats, from text to video.
With the rapid advancements in generative AI, a similar shift is set to
transform the game industry, particularly in the realm of role-playing games
(RPGs). This paper introduces a new framework for a text-to-game engine that
utilizes foundation models to convert simple textual inputs into complex,
interactive RPG experiences. The engine dynamically renders the game story in a
multi-modal format and adjusts the game character, environment, and mechanics
in real-time in response to player actions. Using this framework, we developed
the ""Zagii"" game engine, which has successfully supported hundreds of RPG games
across a diverse range of genres and facilitated tens of thousands of online
user gameplay instances. This validates the effectiveness of our frame-work.
Our work showcases the potential for a more open and democratized gaming
paradigm, highlighting the transformative impact of generative AI on the game
life cycle.","[{'name': 'Lei Zhang'}, {'name': 'Xuezheng Peng'}, {'name': 'Shuyi Yang'}, {'name': 'Feiyang Wang'}]",2024-07-11T05:33:19Z
http://arxiv.org/abs/2407.08189v1,http://arxiv.org/abs/2407.08189v1,"fairBERTs: Erasing Sensitive Information Through Semantic and
  Fairness-aware Perturbations","Pre-trained language models (PLMs) have revolutionized both the natural
language processing research and applications. However, stereotypical biases
(e.g., gender and racial discrimination) encoded in PLMs have raised negative
ethical implications for PLMs, which critically limits their broader
applications. To address the aforementioned unfairness issues, we present
fairBERTs, a general framework for learning fair fine-tuned BERT series models
by erasing the protected sensitive information via semantic and fairness-aware
perturbations generated by a generative adversarial network. Through extensive
qualitative and quantitative experiments on two real-world tasks, we
demonstrate the great superiority of fairBERTs in mitigating unfairness while
maintaining the model utility. We also verify the feasibility of transferring
adversarial components in fairBERTs to other conventionally trained BERT-like
models for yielding fairness improvements. Our findings may shed light on
further research on building fairer fine-tuned PLMs.","[{'name': 'Jinfeng Li'}, {'name': 'Yuefeng Chen'}, {'name': 'Xiangyu Liu'}, {'name': 'Longtao Huang'}, {'name': 'Rong Zhang'}, {'name': 'Hui Xue'}]",2024-07-11T05:13:38Z
http://arxiv.org/abs/2407.08182v1,http://arxiv.org/abs/2407.08182v1,"Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal
  Theory for Post-Purchase Intention Analysis","Supervised machine-learning models for predicting user behavior offer a
challenging classification problem with lower average prediction performance
scores than other text classification tasks. This study evaluates multi-task
learning frameworks grounded in Cognitive Appraisal Theory to predict user
behavior as a function of users' self-expression and psychological attributes.
Our experiments show that users' language and traits improve predictions above
and beyond models predicting only from text. Our findings highlight the
importance of integrating psychological constructs into NLP to enhance the
understanding and prediction of user actions. We close with a discussion of the
implications for future applications of large language models for computational
psychology.","[{'name': 'Gerard Christopher Yeo'}, {'name': 'Shaz Furniturewala'}, {'name': 'Kokil Jaidka'}]",2024-07-11T04:57:52Z
http://arxiv.org/abs/2407.08152v1,http://arxiv.org/abs/2407.08152v1,"Privacy-Preserving Data Deduplication for Enhancing Federated Learning
  of Language Models","Deduplication is a vital preprocessing step that enhances machine learning
model performance and saves training time and energy. However, enhancing
federated learning through deduplication poses challenges, especially regarding
scalability and potential privacy violations if deduplication involves sharing
all clients' data. In this paper, we address the problem of deduplication in a
federated setup by introducing a pioneering protocol, Efficient
Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes
duplicates from multiple clients' datasets without compromising data privacy.
EP-MPD is constructed in a modular fashion, utilizing two novel variants of the
Private Set Intersection protocol. Our extensive experiments demonstrate the
significant benefits of deduplication in federated learning of large language
models. For instance, we observe up to 19.61% improvement in perplexity and up
to 27.95% reduction in running time. EP-MPD effectively balances privacy and
performance in federated learning, making it a valuable solution for
large-scale applications.","[{'name': 'Aydin Abadi'}, {'name': 'Vishnu Asutosh Dasu'}, {'name': 'Sumanta Sarkar'}]",2024-07-11T03:10:27Z
http://arxiv.org/abs/2407.08147v1,http://arxiv.org/abs/2407.08147v1,"Looks can be Deceptive: Distinguishing Repetition Disfluency from
  Reduplication","Reduplication and repetition, though similar in form, serve distinct
linguistic purposes. Reduplication is a deliberate morphological process used
to express grammatical, semantic, or pragmatic nuances, while repetition is
often unintentional and indicative of disfluency. This paper presents the first
large-scale study of reduplication and repetition in speech using computational
linguistics. We introduce IndicRedRep, a new publicly available dataset
containing Hindi, Telugu, and Marathi text annotated with reduplication and
repetition at the word level. We evaluate transformer-based models for
multi-class reduplication and repetition token classification, utilizing the
Reparandum-Interregnum-Repair structure to distinguish between the two
phenomena. Our models achieve macro F1 scores of up to 85.62% in Hindi, 83.95%
in Telugu, and 84.82% in Marathi for reduplication-repetition classification.","[{'name': 'Arif Ahmad'}, {'name': 'Mothika Gayathri Khyathi'}, {'name': 'Pushpak Bhattacharyya'}]",2024-07-11T03:00:14Z
http://arxiv.org/abs/2407.08112v2,http://arxiv.org/abs/2407.08112v2,"How Well Can a Long Sequence Model Model Long Sequences? Comparing
  Architechtural Inductive Biases on Long-Context Abilities","Long sequences occur in abundance within real-world scenarios, hence properly
modelling them opens numerous down-stream use-cases. Deep neural networks,
however, have often struggled with these for a variety of reasons. Recent
advances, both in system engineering as well as model design, have enabled the
scaling up of model that are purported to support extended context length. In
particular, the state-space and linear recurrent neural network families of
models hypothetically can entend to infinite sequence lenth. However, is this
too good to be true? We conduct an evaluation to show that while such claims
may be sound theoretically, there remain large practical gaps that are
empirically observed. In particular, recurrent models still suffer in the same
settings as long-context LLMs with attention. We further show that different
inductive biases have inconsistent extrapolation capabilities, highlighting the
need to further study such paradigms and investigate why long-context models
seemingly fail to behave as one might expect.",[{'name': 'Jerry Huang'}],2024-07-11T01:08:39Z
http://arxiv.org/abs/2407.08095v1,http://arxiv.org/abs/2407.08095v1,"Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered
  Motivational Interviewing","We introduce a novel application of large language models (LLMs) in
developing a virtual counselor capable of conducting motivational interviewing
(MI) for alcohol use counseling. Access to effective counseling remains
limited, particularly for substance abuse, and virtual agents offer a promising
solution by leveraging LLM capabilities to simulate nuanced communication
techniques inherent in MI. Our approach combines prompt engineering and
integration into a user-friendly virtual platform to facilitate realistic,
empathetic interactions. We evaluate the effectiveness of our virtual agent
through a series of studies focusing on replicating MI techniques and human
counselor dialog. Initial findings suggest that our LLM-powered virtual agent
matches human counselors' empathetic and adaptive conversational skills,
presenting a significant step forward in virtual health counseling and
providing insights into the design and implementation of LLM-based therapeutic
interactions.","[{'name': 'Ian Steenstra'}, {'name': 'Farnaz Nouraei'}, {'name': 'Mehdi Arjmand'}, {'name': 'Timothy W. Bickmore'}]",2024-07-10T23:50:08Z
http://arxiv.org/abs/2407.08044v1,http://arxiv.org/abs/2407.08044v1,"RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
  Weight-Activation Quantization","Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient
Fine-Tuning (PEFT)method, significantly enhances the training efficiency by
updating only a small portion of the weights in Large Language Models (LLMs).
Recently, weight-only quantization techniques have also been applied to LoRA
methods to reduce the memory footprint of fine-tuning. However, applying
weight-activation quantization to the LoRA pipeline is under-explored, and we
observe substantial performance degradation primarily due to the presence of
activation outliers. In this work, we propose RoLoRA, the first LoRA-based
scheme for effective weight-activation quantization. RoLoRA utilizes rotation
for outlier elimination and proposes rotation-aware fine-tuning to preserve the
outlier-free characteristics in rotated LLMs. Experimental results show RoLoRA
consistently improves low-bit LoRA convergence and post-training quantization
robustness in weight-activation settings. We evaluate RoLoRA across
LLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain
of 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks
compared to LoRA baseline. We further demonstrate its effectiveness on Large
Multimodal Models (LLaVA-1.5-7B). Codes are available at
https://github.com/HuangOwen/RoLoRA","[{'name': 'Xijie Huang'}, {'name': 'Zechun Liu'}, {'name': 'Shih-Yang Liu'}, {'name': 'Kwang-Ting Cheng'}]",2024-07-10T20:52:18Z
http://arxiv.org/abs/2407.08029v1,http://arxiv.org/abs/2407.08029v1,"A Critical Review of Causal Reasoning Benchmarks for Large Language
  Models","Numerous benchmarks aim to evaluate the capabilities of Large Language Models
(LLMs) for causal inference and reasoning. However, many of them can likely be
solved through the retrieval of domain knowledge, questioning whether they
achieve their purpose. In this review, we present a comprehensive overview of
LLM benchmarks for causality. We highlight how recent benchmarks move towards a
more thorough definition of causal reasoning by incorporating interventional or
counterfactual reasoning. We derive a set of criteria that a useful benchmark
or set of benchmarks should aim to satisfy. We hope this work will pave the way
towards a general framework for the assessment of causal understanding in LLMs
and the design of novel benchmarks.","[{'name': 'Linying Yang'}, {'name': 'Vik Shirvaikar'}, {'name': 'Oscar Clivio'}, {'name': 'Fabian Falck'}]",2024-07-10T20:11:51Z
http://arxiv.org/abs/2407.08008v1,http://arxiv.org/abs/2407.08008v1,DS@GT eRisk 2024: Sentence Transformers for Social Media Risk Assessment,"We present working notes for DS@GT team in the eRisk 2024 for Tasks 1 and 3.
We propose a ranking system for Task 1 that predicts symptoms of depression
based on the Beck Depression Inventory (BDI-II) questionnaire using binary
classifiers trained on question relevancy as a proxy for ranking. We find that
binary classifiers are not well calibrated for ranking, and perform poorly
during evaluation. For Task 3, we use embeddings from BERT to predict the
severity of eating disorder symptoms based on user post history. We find that
classical machine learning models perform well on the task, and end up
competitive with the baseline models. Representation of text data is crucial in
both tasks, and we find that sentence transformers are a powerful tool for
downstream modeling. Source code and models are available at
\url{https://github.com/dsgt-kaggle-clef/erisk-2024}.","[{'name': 'David Guecha'}, {'name': 'Aaryan Potdar'}, {'name': 'Anthony Miyaguchi'}]",2024-07-10T19:30:16Z
http://arxiv.org/abs/2407.08001v1,http://arxiv.org/abs/2407.08001v1,Automated Neural Patent Landscaping in the Small Data Regime,"Patent landscaping is the process of identifying all patents related to a
particular technological area, and is important for assessing various aspects
of the intellectual property context. Traditionally, constructing patent
landscapes is intensely laborious and expensive, and the rapid expansion of
patenting activity in recent decades has driven an increasing need for
efficient and effective automated patent landscaping approaches. In particular,
it is critical that we be able to construct patent landscapes using a minimal
number of labeled examples, as labeling patents for a narrow technology area
requires highly specialized (and hence expensive) technical knowledge. We
present an automated neural patent landscaping system that demonstrates
significantly improved performance on difficult examples (0.69 $F_1$ on 'hard'
examples, versus 0.6 for previously reported systems), and also significant
improvements with much less training data (overall 0.75 $F_1$ on as few as 24
examples). Furthermore, in evaluating such automated landscaping systems,
acquiring good data is challenge; we demonstrate a higher-quality training data
generation procedure by merging Abood and Feltenberger's (2018)
""seed/anti-seed"" approach with active learning to collect difficult labeled
examples near the decision boundary. Using this procedure we created a new
dataset of labeled AI patents for training and testing. As in prior work we
compare our approach with a number of baseline systems, and we release our code
and data for others to build upon.","[{'name': 'Tisa Islam Erana'}, {'name': 'Mark A. Finlayson'}]",2024-07-10T19:13:37Z
http://arxiv.org/abs/2407.07950v1,http://arxiv.org/abs/2407.07950v1,"Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM
  Reliance","The reconfiguration of human-LM interactions from simple sentence completions
to complex, multi-domain, humanlike engagements necessitates new methodologies
to understand how humans choose to rely on LMs. In our work, we contend that
reliance is influenced by numerous factors within the interactional context of
a generation, a departure from prior work that used verbalized confidence
(e.g., ""I'm certain the answer is..."") as the key determinant of reliance.
Here, we introduce Rel-A.I., an in situ, system-level evaluation approach to
measure human reliance on LM-generated epistemic markers (e.g., ""I think
it's.."", ""Undoubtedly it's...""). Using this methodology, we measure reliance
rates in three emergent human-LM interaction settings: long-term interactions,
anthropomorphic generations, and variable subject matter. Our findings reveal
that reliance is not solely based on verbalized confidence but is significantly
affected by other features of the interaction context. Prior interactions,
anthropomorphic cues, and subject domain all contribute to reliance
variability. An expression such as, ""I'm pretty sure it's..."", can vary up to
20% in reliance frequency depending on its interactional context. Our work
underscores the importance of context in understanding human reliance and
offers future designers and researchers with a methodology to conduct such
measurements.","[{'name': 'Kaitlyn Zhou'}, {'name': 'Jena D. Hwang'}, {'name': 'Xiang Ren'}, {'name': 'Nouha Dziri'}, {'name': 'Dan Jurafsky'}, {'name': 'Maarten Sap'}]",2024-07-10T18:00:05Z
http://arxiv.org/abs/2407.18940v1,http://arxiv.org/abs/2407.18940v1,LitSearch: A Retrieval Benchmark for Scientific Literature Search,"Literature search questions, such as ""where can I find research on the
evaluation of consistency in generated summaries?"" pose significant challenges
for modern search engines and retrieval systems. These questions often require
a deep understanding of research concepts and the ability to reason over entire
articles. In this work, we introduce LitSearch, a retrieval benchmark
comprising 597 realistic literature search queries about recent ML and NLP
papers. LitSearch is constructed using a combination of (1) questions generated
by GPT-4 based on paragraphs containing inline citations from research papers
and (2) questions about recently published papers, manually written by their
authors. All LitSearch questions were manually examined or edited by experts to
ensure high quality. We extensively benchmark state-of-the-art retrieval models
and also evaluate two LLM-based reranking pipelines. We find a significant
performance gap between BM25 and state-of-the-art dense retrievers, with a
24.8% difference in absolute recall@5. The LLM-based reranking strategies
further improve the best-performing dense retriever by 4.4%. Additionally,
commercial search engines and research tools like Google Search perform poorly
on LitSearch, lagging behind the best dense retriever by 32 points. Taken
together, these results show that LitSearch is an informative new testbed for
retrieval systems while catering to a real-world use case.","[{'name': 'Anirudh Ajith'}, {'name': 'Mengzhou Xia'}, {'name': 'Alexis Chevalier'}, {'name': 'Tanya Goyal'}, {'name': 'Danqi Chen'}, {'name': 'Tianyu Gao'}]",2024-07-10T18:00:03Z
http://arxiv.org/abs/2407.07895v2,http://arxiv.org/abs/2407.07895v2,"LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large
  Multimodal Models","Visual instruction tuning has made considerable strides in enhancing the
capabilities of Large Multimodal Models (LMMs). However, existing open LMMs
largely focus on single-image tasks, their applications to multi-image
scenarios remains less explored. Additionally, prior LMM research separately
tackles different scenarios, leaving it impossible to generalize cross
scenarios with new emerging capabilities. To this end, we introduce
LLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame
(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To
enable these capabilities, we regard the interleaved data format as a general
template and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4
primary domains with 14 tasks and 41 datasets. We also curate the
LLaVA-Interleave Bench to comprehensively evaluate the multi-image performance
of LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading
results in multi-image, video, and 3D benchmarks, while maintaining the
performance of single-image tasks. Besides, our model also exhibits several
emerging capabilities, e.g., transferring tasks across different settings and
modalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT","[{'name': 'Feng Li'}, {'name': 'Renrui Zhang'}, {'name': 'Hao Zhang'}, {'name': 'Yuanhan Zhang'}, {'name': 'Bo Li'}, {'name': 'Wei Li'}, {'name': 'Zejun Ma'}, {'name': 'Chunyuan Li'}]",2024-07-10T17:59:43Z
http://arxiv.org/abs/2407.07880v1,http://arxiv.org/abs/2407.07880v1,"Towards Robust Alignment of Language Models: Distributionally
  Robustifying Direct Preference Optimization","This study addresses the challenge of noise in training datasets for Direct
Preference Optimization (DPO), a method for aligning Large Language Models
(LLMs) with human preferences. We categorize noise into pointwise noise, which
includes low-quality data points, and pairwise noise, which encompasses
erroneous data pair associations that affect preference rankings. Utilizing
Distributionally Robust Optimization (DRO), we enhance DPO's resilience to
these types of noise. Our theoretical insights reveal that DPO inherently
embeds DRO principles, conferring robustness to pointwise noise, with the
regularization coefficient $\beta$ playing a critical role in its noise
resistance. Extending this framework, we introduce Distributionally
Robustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing
against worst-case pairwise scenarios. The novel hyperparameter $\beta'$ in Dr.
DPO allows for fine-tuned control over data pair reliability, providing a
strategic balance between exploration and exploitation in noisy training
environments. Empirical evaluations demonstrate that Dr. DPO substantially
improves the quality of generated text and response accuracy in preference
datasets, showcasing enhanced performance in both noisy and noise-free
settings. The code is available at https://github.com/junkangwu/Dr_DPO.","[{'name': 'Junkang Wu'}, {'name': 'Yuexiang Xie'}, {'name': 'Zhengyi Yang'}, {'name': 'Jiancan Wu'}, {'name': 'Jiawei Chen'}, {'name': 'Jinyang Gao'}, {'name': 'Bolin Ding'}, {'name': 'Xiang Wang'}, {'name': 'Xiangnan He'}]",2024-07-10T17:48:25Z
http://arxiv.org/abs/2407.07875v1,http://arxiv.org/abs/2407.07875v1,Generative Image as Action Models,"Image-generation diffusion models have been fine-tuned to unlock new
capabilities such as image-editing and novel view synthesis. Can we similarly
unlock image-generation models for visuomotor control? We present GENIMA, a
behavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'
as targets on RGB images. These images are fed into a controller that maps the
visual targets into a sequence of joint-positions. We study GENIMA on 25
RLBench and 9 real-world manipulation tasks. We find that, by lifting actions
into image-space, internet pre-trained diffusion models can generate policies
that outperform state-of-the-art visuomotor approaches, especially in
robustness to scene perturbations and generalizing to novel objects. Our method
is also competitive with 3D agents, despite lacking priors such as depth,
keypoints, or motion-planners.","[{'name': 'Mohit Shridhar'}, {'name': 'Yat Long Lo'}, {'name': 'Stephen James'}]",2024-07-10T17:41:10Z
http://arxiv.org/abs/2407.07858v1,http://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key
applications to enhance employee productivity. Retrieval Augmented Generation
(RAG), Large Language Models (LLMs), and orchestration frameworks like
Langchain and Llamaindex are crucial for building these chatbots. However,
creating effective enterprise chatbots is challenging and requires meticulous
RAG pipeline engineering. This includes fine-tuning embeddings and LLMs,
extracting documents from vector databases, rephrasing queries, reranking
results, designing prompts, honoring document access controls, providing
concise responses, including references, safeguarding personal information, and
building orchestration agents. We present a framework for building RAG-based
chatbots based on our experience with three NVIDIA chatbots: for IT/HR
benefits, financial earnings, and general content. Our contributions are
three-fold: introducing the FACTS framework (Freshness, Architectures, Cost,
Testing, Security), presenting fifteen RAG pipeline control points, and
providing empirical results on accuracy-latency tradeoffs between large and
small LLMs. To the best of our knowledge, this is the first paper of its kind
that provides a holistic view of the factors as well as solutions for building
secure enterprise-grade chatbots.""","[{'name': 'Rama Akkiraju'}, {'name': 'Anbang Xu'}, {'name': 'Deepak Bora'}, {'name': 'Tan Yu'}, {'name': 'Lu An'}, {'name': 'Vishal Seth'}, {'name': 'Aaditya Shukla'}, {'name': 'Pritam Gundecha'}, {'name': 'Hridhay Mehta'}, {'name': 'Ashwin Jha'}, {'name': 'Prithvi Raj'}, {'name': 'Abhinav Balasubramanian'}, {'name': 'Murali Maram'}, {'name': 'Guru Muthusamy'}, {'name': 'Shivakesh Reddy Annepally'}, {'name': 'Sidney Knowles'}, {'name': 'Min Du'}, {'name': 'Nick Burnett'}, {'name': 'Sean Javiya'}, {'name': 'Ashok Marannan'}, {'name': 'Mamta Kumari'}, {'name': 'Surbhi Jha'}, {'name': 'Ethan Dereszenski'}, {'name': 'Anupam Chakraborty'}, {'name': 'Subhash Ranjan'}, {'name': 'Amina Terfai'}, {'name': 'Anoop Surya'}, {'name': 'Tracey Mercer'}, {'name': 'Vinodh Kumar Thanigachalam'}, {'name': 'Tamar Bar'}, {'name': 'Sanjana Krishnan'}, {'name': 'Samy Kilaru'}, {'name': 'Jasmine Jaksic'}, {'name': 'Nave Algarici'}, {'name': 'Jacob Liberman'}, {'name': 'Joey Conway'}, {'name': 'Sonu Nayyar'}, {'name': 'Justin Boitano'}]",2024-07-10T17:20:59Z
http://arxiv.org/abs/2407.07840v2,http://arxiv.org/abs/2407.07840v2,"Decompose and Compare Consistency: Measuring VLMs' Answer Reliability
  via Task-Decomposition Consistency Comparison","Despite tremendous advancements, current state-of-the-art Vision-Language
Models (VLMs) are still far from perfect. They tend to hallucinate and may
generate biased responses. In such circumstances, having a way to assess the
reliability of a given response generated by a VLM is quite useful. Existing
methods, such as estimating uncertainty using answer likelihoods or
prompt-based confidence generation, often suffer from overconfidence. Other
methods use self-consistency comparison but are affected by confirmation
biases. To alleviate these, we propose \textbf{De}compose and \textbf{C}ompare
\textbf{C}onsistency (\texttt{DeCC}) for reliability measurement. By comparing
the consistency between the direct answer generated using the VLM's internal
reasoning process, and the indirect answers obtained by decomposing the
question into sub-questions and reasoning over the sub-answers produced by the
VLM, \texttt{DeCC} measures the reliability of VLM's direct answer. Experiments
across six vision-language tasks with three VLMs show \texttt{DeCC}'s
reliability estimation achieves better correlation with task accuracy compared
to the existing methods.","[{'name': 'Qian Yang'}, {'name': 'Weixiang Yan'}, {'name': 'Aishwarya Agrawal'}]",2024-07-10T17:00:29Z
http://arxiv.org/abs/2407.07810v1,http://arxiv.org/abs/2407.07810v1,Transformer Alignment in Large Language Models,"Large Language Models (LLMs) have made significant strides in natural
language processing, and a precise understanding of the internal mechanisms
driving their success is essential. We regard LLMs as transforming embeddings
via a discrete, coupled, nonlinear, dynamical system in high dimensions. This
perspective motivates tracing the trajectories of individual tokens as they
pass through transformer blocks, and linearizing the system along these
trajectories through their Jacobian matrices. In our analysis of 38 openly
available LLMs, we uncover the alignment of top left and right singular vectors
of Residual Jacobians, as well as the emergence of linearity and layer-wise
exponential growth. Notably, we discover that increased alignment
$\textit{positively correlates}$ with model performance. Metrics evaluated
post-training show significant improvement in comparison to measurements made
with randomly initialized weights, highlighting the significant effects of
training in transformers. These findings reveal a remarkable level of
regularity that has previously been overlooked, reinforcing the dynamical
interpretation and paving the way for deeper understanding and optimization of
LLM architectures.","[{'name': 'Murdock Aubry'}, {'name': 'Haoming Meng'}, {'name': 'Anton Sugolov'}, {'name': 'Vardan Papyan'}]",2024-07-10T16:30:27Z
http://arxiv.org/abs/2407.07802v1,http://arxiv.org/abs/2407.07802v1,ROSA: Random Subspace Adaptation for Efficient Fine-Tuning,"Model training requires significantly more memory, compared with inference.
Parameter efficient fine-tuning (PEFT) methods provide a means of adapting
large models to downstream tasks using less memory. However, existing methods
such as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce
latency overhead at inference time or achieve subpar downstream performance
compared with full fine-tuning. In this work we propose Random Subspace
Adaptation (ROSA), a method that outperforms previous PEFT methods by a
significant margin, while maintaining a zero latency overhead during inference
time. In contrast to previous methods, ROSA is able to adapt subspaces of
arbitrarily large dimension, better approximating full-finetuning. We
demonstrate both theoretically and experimentally that this makes ROSA strictly
more expressive than LoRA, without consuming additional memory during runtime.
As PEFT methods are especially useful in the natural language processing
domain, where models operate on scales that make full fine-tuning very
expensive, we evaluate ROSA in two common NLP scenarios: natural language
generation (NLG) and natural language understanding (NLU) with GPT-2 and
RoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms
LoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our
code is available at https://github.com/rosa-paper/rosa","[{'name': 'Marawan Gamal Abdel Hameed'}, {'name': 'Aristides Milios'}, {'name': 'Siva Reddy'}, {'name': 'Guillaume Rabusseau'}]",2024-07-10T16:20:53Z
http://arxiv.org/abs/2407.07801v2,http://arxiv.org/abs/2407.07801v2,AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning,"In recent years, advancements in representation learning and language models
have propelled Automated Captioning (AC) to new heights, enabling the
generation of human-level descriptions. Leveraging these advancements, we
propose AVCap, an Audio-Visual Captioning framework, a simple yet powerful
baseline approach applicable to audio-visual captioning. AVCap utilizes
audio-visual features as text tokens, which has many advantages not only in
performance but also in the extensibility and scalability of the model. AVCap
is designed around three pivotal dimensions: the exploration of optimal
audio-visual encoder architectures, the adaptation of pre-trained models
according to the characteristics of generated text, and the investigation into
the efficacy of modality fusion in captioning. Our method outperforms existing
audio-visual captioning methods across all metrics and the code is available on
https://github.com/JongSuk1/AVCap","[{'name': 'Jongsuk Kim'}, {'name': 'Jiwon Shin'}, {'name': 'Junmo Kim'}]",2024-07-10T16:17:49Z
http://arxiv.org/abs/2407.07799v1,http://arxiv.org/abs/2407.07799v1,Attribute or Abstain: Large Language Models as Long Document Assistants,"LLMs can help humans working with long documents, but are known to
hallucinate. Attribution can increase trust in LLM responses: The LLM provides
evidence that supports its response, which enhances verifiability. Existing
approaches to attribution have only been evaluated in RAG settings, where the
initial retrieval confounds LLM performance. This is crucially different from
the long document setting, where retrieval is not needed, but could help. Thus,
a long document specific evaluation of attribution is missing. To fill this
gap, we present LAB, a benchmark of 6 diverse long document tasks with
attribution, and experiment with different approaches to attribution on 4 LLMs
of different sizes, both prompted and fine-tuned. We find that citation, i.e.
response generation and evidence extraction in one step, mostly performs best.
We investigate whether the ``Lost in the Middle'' phenomenon exists for
attribution, but do not find this. We also find that evidence quality can
predict response quality on datasets with simple responses, but not so for
complex responses, as models struggle with providing evidence for complex
claims. We release code and data for further investigation.","[{'name': 'Jan Buchmann'}, {'name': 'Xiao Liu'}, {'name': 'Iryna Gurevych'}]",2024-07-10T16:16:02Z
http://arxiv.org/abs/2407.07796v2,http://arxiv.org/abs/2407.07796v2,"Evaluating Large Language Models with Grid-Based Game Competitions: An
  Extensible LLM Benchmark and Leaderboard","We introduce a novel and extensible benchmark for large language models
(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.
The open-source game simulation code, available on GitHub, allows LLMs to
compete and generates detailed data files in JSON, CSV, TXT, and PNG formats
for leaderboard rankings and further analysis. We present the results of games
among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by
Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and
GPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of
results from other LLMs. In total, we simulated 2,310 matches (5 sessions for
each pair among 7 LLMs and a random player) across three types of games, using
three distinct prompt types: list, illustration, and image. The results
revealed significant variations in LLM performance across different games and
prompt types, with analysis covering win and disqualification rates, missed
opportunity analysis, and invalid move analysis. The details of the leaderboard
and result matrix data are available as open-access data on GitHub. This study
enhances our understanding of LLMs' capabilities in playing games they were not
specifically trained for, helping to assess their rule comprehension and
strategic thinking. On the path to Artificial General Intelligence (AGI), this
study lays the groundwork for future exploration into their utility in complex
decision-making scenarios, illuminating their strategic thinking abilities and
offering directions for further inquiry into the limits of LLMs within
game-based frameworks.","[{'name': 'Oguzhan Topsakal'}, {'name': 'Colby Jacob Edell'}, {'name': 'Jackson Bailey Harper'}]",2024-07-10T16:14:34Z
http://arxiv.org/abs/2407.07791v2,http://arxiv.org/abs/2407.07791v2,"Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent
  Communities","The rapid adoption of large language models (LLMs) in multi-agent systems has
highlighted their impressive capabilities in various applications, such as
collaborative problem-solving and autonomous negotiation. However, the security
implications of these LLM-based multi-agent systems have not been thoroughly
investigated, particularly concerning the spread of manipulated knowledge. In
this paper, we investigate this critical issue by constructing a detailed
threat model and a comprehensive simulation environment that mirrors real-world
multi-agent deployments in a trusted platform. Subsequently, we propose a novel
two-stage attack method involving Persuasiveness Injection and Manipulated
Knowledge Injection to systematically explore the potential for manipulated
knowledge (i.e., counterfactual and toxic knowledge) spread without explicit
prompt manipulation.
  Our method leverages the inherent vulnerabilities of LLMs in handling world
knowledge, which can be exploited by attackers to unconsciously spread
fabricated information. Through extensive experiments, we demonstrate that our
attack method can successfully induce LLM-based agents to spread both
counterfactual and toxic knowledge without degrading their foundational
capabilities during agent communication. Furthermore, we show that these
manipulations can persist through popular retrieval-augmented generation
frameworks, where several benign agents store and retrieve manipulated chat
histories for future interactions. This persistence indicates that even after
the interaction has ended, the benign agents may continue to be influenced by
manipulated knowledge. Our findings reveal significant security risks in
LLM-based multi-agent systems, emphasizing the imperative need for robust
defenses against manipulated knowledge spread, such as introducing ``guardian''
agents and advanced fact-checking tools.","[{'name': 'Tianjie Ju'}, {'name': 'Yiting Wang'}, {'name': 'Xinbei Ma'}, {'name': 'Pengzhou Cheng'}, {'name': 'Haodong Zhao'}, {'name': 'Yulong Wang'}, {'name': 'Lifeng Liu'}, {'name': 'Jian Xie'}, {'name': 'Zhuosheng Zhang'}, {'name': 'Gongshen Liu'}]",2024-07-10T16:08:46Z
http://arxiv.org/abs/2407.07771v1,http://arxiv.org/abs/2407.07771v1,Multi-task Prompt Words Learning for Social Media Content Generation,"The rapid development of the Internet has profoundly changed human life.
Humans are increasingly expressing themselves and interacting with others on
social media platforms. However, although artificial intelligence technology
has been widely used in many aspects of life, its application in social media
content creation is still blank. To solve this problem, we propose a new prompt
word generation framework based on multi-modal information fusion, which
combines multiple tasks including topic classification, sentiment analysis,
scene recognition and keyword extraction to generate more comprehensive prompt
words. Subsequently, we use a template containing a set of prompt words to
guide ChatGPT to generate high-quality tweets. Furthermore, in the absence of
effective and objective evaluation criteria in the field of content generation,
we use the ChatGPT tool to evaluate the results generated by the algorithm,
making large-scale evaluation of content generation algorithms possible.
Evaluation results on extensive content generation demonstrate that our cue
word generation framework generates higher quality content compared to manual
methods and other cueing techniques, while topic classification, sentiment
analysis, and scene recognition significantly enhance content clarity and its
consistency with the image.","[{'name': 'Haochen Xue'}, {'name': 'Chong Zhang'}, {'name': 'Chengzhi Liu'}, {'name': 'Fangyu Wu'}, {'name': 'Xiaobo Jin'}]",2024-07-10T15:46:32Z
http://arxiv.org/abs/2407.07737v1,http://arxiv.org/abs/2407.07737v1,Fine-Tuning Large Language Models with User-Level Differential Privacy,"We investigate practical and scalable algorithms for training large language
models (LLMs) with user-level differential privacy (DP) in order to provably
safeguard all the examples contributed by each user. We study two variants of
DP-SGD with: (1) example-level sampling (ELS) and per-example gradient
clipping, and (2) user-level sampling (ULS) and per-user gradient clipping. We
derive a novel user-level DP accountant that allows us to compute provably
tight privacy guarantees for ELS. Using this, we show that while ELS can
outperform ULS in specific settings, ULS generally yields better results when
each user has a diverse collection of examples. We validate our findings
through experiments in synthetic mean estimation and LLM fine-tuning tasks
under fixed compute budgets. We find that ULS is significantly better in
settings where either (1) strong privacy guarantees are required, or (2) the
compute budget is large. Notably, our focus on LLM-compatible training
algorithms allows us to scale to models with hundreds of millions of parameters
and datasets with hundreds of thousands of users.","[{'name': 'Zachary Charles'}, {'name': 'Arun Ganesh'}, {'name': 'Ryan McKenna'}, {'name': 'H. Brendan McMahan'}, {'name': 'Nicole Mitchell'}, {'name': 'Krishna Pillutla'}, {'name': 'Keith Rush'}]",2024-07-10T15:07:58Z
http://arxiv.org/abs/2407.07726v1,http://arxiv.org/abs/2407.07726v1,PaliGemma: A versatile 3B VLM for transfer,"PaliGemma is an open Vision-Language Model (VLM) that is based on the
SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to
be a versatile and broadly knowledgeable base model that is effective to
transfer. It achieves strong performance on a wide variety of open-world tasks.
We evaluate PaliGemma on almost 40 diverse tasks including standard VLM
benchmarks, but also more specialized tasks such as remote-sensing and
segmentation.","[{'name': 'Lucas Beyer'}, {'name': 'Andreas Steiner'}, {'name': 'André Susano Pinto'}, {'name': 'Alexander Kolesnikov'}, {'name': 'Xiao Wang'}, {'name': 'Daniel Salz'}, {'name': 'Maxim Neumann'}, {'name': 'Ibrahim Alabdulmohsin'}, {'name': 'Michael Tschannen'}, {'name': 'Emanuele Bugliarello'}, {'name': 'Thomas Unterthiner'}, {'name': 'Daniel Keysers'}, {'name': 'Skanda Koppula'}, {'name': 'Fangyu Liu'}, {'name': 'Adam Grycner'}, {'name': 'Alexey Gritsenko'}, {'name': 'Neil Houlsby'}, {'name': 'Manoj Kumar'}, {'name': 'Keran Rong'}, {'name': 'Julian Eisenschlos'}, {'name': 'Rishabh Kabra'}, {'name': 'Matthias Bauer'}, {'name': 'Matko Bošnjak'}, {'name': 'Xi Chen'}, {'name': 'Matthias Minderer'}, {'name': 'Paul Voigtlaender'}, {'name': 'Ioana Bica'}, {'name': 'Ivana Balazevic'}, {'name': 'Joan Puigcerver'}, {'name': 'Pinelopi Papalampidi'}, {'name': 'Olivier Henaff'}, {'name': 'Xi Xiong'}, {'name': 'Radu Soricut'}, {'name': 'Jeremiah Harmsen'}, {'name': 'Xiaohua Zhai'}]",2024-07-10T14:57:46Z
http://arxiv.org/abs/2407.07683v1,http://arxiv.org/abs/2407.07683v1,"The Language of Weather: Social Media Reactions to Weather Accounting
  for Climatic and Linguistic Baselines","This study explores how different weather conditions influence public
sentiment on social media, focusing on Twitter data from the UK. By considering
climate and linguistic baselines, we improve the accuracy of weather-related
sentiment analysis. Our findings show that emotional responses to weather are
complex, influenced by combinations of weather variables and regional language
differences. The results highlight the importance of context-sensitive methods
for better understanding public mood in response to weather, which can enhance
impact-based forecasting and risk communication in the context of climate
change.","[{'name': 'James C. Young'}, {'name': 'Rudy Arthur'}, {'name': 'Hywel T. P. Williams'}]",2024-07-10T14:08:24Z
http://arxiv.org/abs/2407.07666v1,http://arxiv.org/abs/2407.07666v1,"A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models :
  Safety, Consensus, Objectivity, Reproducibility and Explainability","A comprehensive qualitative evaluation framework for large language models
(LLM) in healthcare that expands beyond traditional accuracy and quantitative
metrics needed. We propose 5 key aspects for evaluation of LLMs: Safety,
Consensus, Objectivity, Reproducibility and Explainability (S.C.O.R.E.). We
suggest that S.C.O.R.E. may form the basis for an evaluation framework for
future LLM-based models that are safe, reliable, trustworthy, and ethical for
healthcare and clinical applications.","[{'name': 'Ting Fang Tan'}, {'name': 'Kabilan Elangovan'}, {'name': 'Jasmine Ong'}, {'name': 'Nigam Shah'}, {'name': 'Joseph Sung'}, {'name': 'Tien Yin Wong'}, {'name': 'Lan Xue'}, {'name': 'Nan Liu'}, {'name': 'Haibo Wang'}, {'name': 'Chang Fu Kuo'}, {'name': 'Simon Chesterman'}, {'name': 'Zee Kin Yeong'}, {'name': 'Daniel SW Ting'}]",2024-07-10T13:45:16Z
http://arxiv.org/abs/2407.07630v1,http://arxiv.org/abs/2407.07630v1,"A Review of the Challenges with Massive Web-mined Corpora Used in Large
  Language Models Pre-Training","This article presents a comprehensive review of the challenges associated
with using massive web-mined corpora for the pre-training of large language
models (LLMs). This review identifies key challenges in this domain, including
challenges such as noise (irrelevant or misleading information), duplication of
content, the presence of low-quality or incorrect information, biases, and the
inclusion of sensitive or personal information in web-mined corpora. Addressing
these issues is crucial for the development of accurate, reliable, and
ethically responsible language models. Through an examination of current
methodologies for data cleaning, pre-processing, bias detection and mitigation,
we highlight the gaps in existing approaches and suggest directions for future
research. Our discussion aims to catalyze advancements in developing more
sophisticated and ethically responsible LLMs.","[{'name': 'Michał Perełkiewicz'}, {'name': 'Rafał Poświata'}]",2024-07-10T13:09:23Z
http://arxiv.org/abs/2407.07617v1,http://arxiv.org/abs/2407.07617v1,"Psycho-linguistic Experiment on Universal Semantic Components of Verbal
  Humor: System Description and Annotation","Objective criteria for universal semantic components that distinguish a
humorous utterance from a non-humorous one are presently under debate. In this
article, we give an in-depth observation of our system of self-paced reading
for annotation of humor, that collects readers' annotations while they open a
text word by word. The system registers keys that readers press to open the
next word, choose a class (humorous versus non-humorous texts), change their
choice. We also touch upon our psycho-linguistic experiment conducted with the
system and the data collected during it.","[{'name': 'Elena Mikhalkova'}, {'name': 'Nadezhda Ganzherli'}, {'name': 'Julia Murzina'}]",2024-07-10T12:56:17Z
http://arxiv.org/abs/2407.07612v1,http://arxiv.org/abs/2407.07612v1,Teaching Transformers Causal Reasoning through Axiomatic Training,"For text-based AI systems to interact in the real world, causal reasoning is
an essential skill. Since interventional data is costly to generate, we study
to what extent an agent can learn causal reasoning from passive data.
Specifically, we consider an axiomatic training setup where an agent learns
from multiple demonstrations of a causal axiom (or rule), rather than
incorporating the axiom as an inductive bias or inferring it from data values.
A key question is whether the agent would learn to generalize from the axiom
demonstrations to new scenarios. For example, if a transformer model is trained
on demonstrations of the causal transitivity axiom over small graphs, would it
generalize to applying the transitivity axiom over large graphs? Our results,
based on a novel axiomatic training scheme, indicate that such generalization
is possible. We consider the task of inferring whether a variable causes
another variable, given a causal graph structure. We find that a 67 million
parameter transformer model, when trained on linear causal chains (along with
some noisy variations) can generalize well to new kinds of graphs, including
longer causal chains, causal chains with reversed order, and graphs with
branching; even when it is not explicitly trained for such settings. Our model
performs at par (or even better) than many larger language models such as
GPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework
provides a new paradigm of learning causal reasoning from passive data that can
be used to learn arbitrary axioms, as long as sufficient demonstrations can be
generated.","[{'name': 'Aniket Vashishtha'}, {'name': 'Abhinav Kumar'}, {'name': 'Abbavaram Gowtham Reddy'}, {'name': 'Vineeth N Balasubramanian'}, {'name': 'Amit Sharma'}]",2024-07-10T12:50:44Z
http://arxiv.org/abs/2407.07606v1,http://arxiv.org/abs/2407.07606v1,"The Computational Learning of Construction Grammars: State of the Art
  and Prospective Roadmap","This paper documents and reviews the state of the art concerning
computational models of construction grammar learning. It brings together prior
work on the computational learning of form-meaning pairings, which has so far
been studied in several distinct areas of research. The goal of this paper is
threefold. First of all, it aims to synthesise the variety of methodologies
that have been proposed to date and the results that have been obtained.
Second, it aims to identify those parts of the challenge that have been
successfully tackled and reveal those that require further research. Finally,
it aims to provide a roadmap which can help to boost and streamline future
research efforts on the computational learning of large-scale, usage-based
construction grammars.","[{'name': 'Jonas Doumen'}, {'name': 'Veronica Juliana Schmalz'}, {'name': 'Katrien Beuls'}, {'name': 'Paul Van Eecke'}]",2024-07-10T12:45:02Z
http://arxiv.org/abs/2407.07566v1,http://arxiv.org/abs/2407.07566v1,HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing,"We present HebDB, a weakly supervised dataset for spoken language processing
in the Hebrew language. HebDB offers roughly 2500 hours of natural and
spontaneous speech recordings in the Hebrew language, consisting of a large
variety of speakers and topics. We provide raw recordings together with a
pre-processed, weakly supervised, and filtered version. The goal of HebDB is to
further enhance research and development of spoken language processing tools
for the Hebrew language. Hence, we additionally provide two baseline systems
for Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a
fully supervised model. We present the performance of these two methods
optimized on HebDB and compare them to current multi-lingual ASR alternatives.
Results suggest the proposed method reaches better results than the evaluated
baselines considering similar model sizes. Dataset, code, and models are
publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.","[{'name': 'Arnon Turetzky'}, {'name': 'Or Tal'}, {'name': 'Yael Segal-Feldman'}, {'name': 'Yehoshua Dissen'}, {'name': 'Ella Zeldes'}, {'name': 'Amit Roth'}, {'name': 'Eyal Cohen'}, {'name': 'Yosi Shrem'}, {'name': 'Bronya R. Chernyak'}, {'name': 'Olga Seleznova'}, {'name': 'Joseph Keshet'}, {'name': 'Yossi Adi'}]",2024-07-10T11:51:26Z
http://arxiv.org/abs/2407.07531v1,http://arxiv.org/abs/2407.07531v1,"Beyond Benchmarking: A New Paradigm for Evaluation and Assessment of
  Large Language Models","In current benchmarks for evaluating large language models (LLMs), there are
issues such as evaluation content restriction, untimely updates, and lack of
optimization guidance. In this paper, we propose a new paradigm for the
measurement of LLMs: Benchmarking-Evaluation-Assessment. Our paradigm shifts
the ""location"" of LLM evaluation from the ""examination room"" to the ""hospital"".
Through conducting a ""physical examination"" on LLMs, it utilizes specific
task-solving as the evaluation content, performs deep attribution of existing
problems within LLMs, and provides recommendation for optimization.","[{'name': 'Jin Liu'}, {'name': 'Qingquan Li'}, {'name': 'Wenlong Du'}]",2024-07-10T10:42:02Z
http://arxiv.org/abs/2407.07495v1,http://arxiv.org/abs/2407.07495v1,Bucket Pre-training is All You Need,"Large language models (LLMs) have demonstrated exceptional performance across
various natural language processing tasks. However, the conventional
fixed-length data composition strategy for pretraining, which involves
concatenating and splitting documents, can introduce noise and limit the
model's ability to capture long-range dependencies. To address this, we first
introduce three metrics for evaluating data composition quality: padding ratio,
truncation ratio, and concatenation ratio. We further propose a multi-bucket
data composition method that moves beyond the fixed-length paradigm, offering a
more flexible and efficient approach to pretraining. Extensive experiments
demonstrate that our proposed method could significantly improving both the
efficiency and efficacy of LLMs pretraining. Our approach not only reduces
noise and preserves context but also accelerates training, making it a
promising solution for LLMs pretraining.","[{'name': 'Hongtao Liu'}, {'name': 'Qiyao Peng'}, {'name': 'Qing Yang'}, {'name': 'Kai Liu'}, {'name': 'Hongyan Xu'}]",2024-07-10T09:27:23Z
http://arxiv.org/abs/2407.07487v1,http://arxiv.org/abs/2407.07487v1,"Review-LLM: Harnessing Large Language Models for Personalized Review
  Generation","Product review generation is an important task in recommender systems, which
could provide explanation and persuasiveness for the recommendation. Recently,
Large Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling
and generating ability, which could be applied in review generation. However,
directly applying the LLMs for generating reviews might be troubled by the
``polite'' phenomenon of the LLMs and could not generate personalized reviews
(e.g., negative reviews). In this paper, we propose Review-LLM that customizes
LLMs for personalized review generation. Firstly, we construct the prompt input
by aggregating user historical behaviors, which include corresponding item
titles and reviews. This enables the LLMs to capture user interest features and
review writing style. Secondly, we incorporate ratings as indicators of
satisfaction into the prompt, which could further improve the model's
understanding of user preferences and the sentiment tendency control of
generated reviews. Finally, we feed the prompt text into LLMs, and use
Supervised Fine-Tuning (SFT) to make the model generate personalized reviews
for the given user and target item. Experimental results on the real-world
dataset show that our fine-tuned model could achieve better review generation
performance than existing close-source LLMs.","[{'name': 'Qiyao Peng'}, {'name': 'Hongtao Liu'}, {'name': 'Hongyan Xu'}, {'name': 'Qing Yang'}, {'name': 'Minglai Shao'}, {'name': 'Wenjun Wang'}]",2024-07-10T09:22:19Z
http://arxiv.org/abs/2407.12860v1,http://arxiv.org/abs/2407.12860v1,"STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained
  LLMs","We present Simplified Text-Attributed Graph Embeddings (STAGE), a
straightforward yet effective method for enhancing node features in Graph
Neural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our
approach leverages Large-Language Models (LLMs) to generate embeddings for
textual attributes. STAGE achieves competitive results on various node
classification benchmarks while also maintaining a simplicity in implementation
relative to current state-of-the-art (SoTA) techniques. We show that utilizing
pre-trained LLMs as embedding generators provides robust features for ensemble
GNN training, enabling pipelines that are simpler than current SoTA approaches
which require multiple expensive training and prompting stages. We also
implement diffusion-pattern GNNs in an effort to make this pipeline scalable to
graphs beyond academic benchmarks.","[{'name': 'Aaron Zolnai-Lucas'}, {'name': 'Jack Boylan'}, {'name': 'Chris Hokamp'}, {'name': 'Parsa Ghaffari'}]",2024-07-10T08:50:25Z
http://arxiv.org/abs/2407.18930v1,http://arxiv.org/abs/2407.18930v1,"Dynamic Encoder Size Based on Data-Driven Layer-wise Pruning for Speech
  Recognition","Varying-size models are often required to deploy ASR systems under different
hardware and/or application constraints such as memory and latency. To avoid
redundant training and optimization efforts for individual models of different
sizes, we present the dynamic encoder size approach, which jointly trains
multiple performant models within one supernet from scratch. These subnets of
various sizes are layer-wise pruned from the supernet, and thus, enjoy full
parameter sharing. By combining score-based pruning with supernet training, we
propose two novel methods, Simple-Top-k and Iterative-Zero-Out, to
automatically select the best-performing subnets in a data-driven manner,
avoiding resource-intensive search efforts. Our experiments using CTC on both
Librispeech and TED-LIUM-v2 corpora show that our methods can achieve on-par
performance as individually trained models of each size category. Also, our
approach consistently brings small performance improvements for the full-size
supernet.","[{'name': 'Jingjing Xu'}, {'name': 'Wei Zhou'}, {'name': 'Zijian Yang'}, {'name': 'Eugen Beck'}, {'name': 'Ralf Schlueter'}]",2024-07-10T08:35:21Z
http://arxiv.org/abs/2407.07457v2,http://arxiv.org/abs/2407.07457v2,GLBench: A Comprehensive Benchmark for Graph with Large Language Models,"The emergence of large language models (LLMs) has revolutionized the way we
interact with graphs, leading to a new paradigm called GraphLLM. Despite the
rapid development of GraphLLM methods in recent years, the progress and
understanding of this field remain unclear due to the lack of a benchmark with
consistent experimental protocols. To bridge this gap, we introduce GLBench,
the first comprehensive benchmark for evaluating GraphLLM methods in both
supervised and zero-shot scenarios. GLBench provides a fair and thorough
evaluation of different categories of GraphLLM methods, along with traditional
baselines such as graph neural networks. Through extensive experiments on a
collection of real-world datasets with consistent data processing and splitting
strategies, we have uncovered several key findings. Firstly, GraphLLM methods
outperform traditional baselines in supervised settings, with LLM-as-enhancers
showing the most robust performance. However, using LLMs as predictors is less
effective and often leads to uncontrollable output issues. We also notice that
no clear scaling laws exist for current GraphLLM methods. In addition, both
structures and semantics are crucial for effective zero-shot transfer, and our
proposed simple baseline can even outperform several models tailored for
zero-shot scenarios. The data and code of the benchmark can be found at
https://github.com/NineAbyss/GLBench.","[{'name': 'Yuhan Li'}, {'name': 'Peisong Wang'}, {'name': 'Xiao Zhu'}, {'name': 'Aochuan Chen'}, {'name': 'Haiyun Jiang'}, {'name': 'Deng Cai'}, {'name': 'Victor Wai Kin Chan'}, {'name': 'Jia Li'}]",2024-07-10T08:20:47Z
http://arxiv.org/abs/2407.12859v1,http://arxiv.org/abs/2407.12859v1,"Automated Question Generation on Tabular Data for Conversational Data
  Exploration","Exploratory data analysis (EDA) is an essential step for analyzing a dataset
to derive insights. Several EDA techniques have been explored in the
literature. Many of them leverage visualizations through various plots. But it
is not easy to interpret them for a non-technical user, and producing
appropriate visualizations is also tough when there are a large number of
columns. Few other works provide a view of some interesting slices of data but
it is still difficult for the user to draw relevant insights from them. Of
late, conversational data exploration is gaining a lot of traction among
non-technical users. It helps the user to explore the dataset without having
deep technical knowledge about the data. Towards this, we propose a system that
recommends interesting questions in natural language based on relevant slices
of a dataset in a conversational setting. Specifically, given a dataset, we
pick a select set of interesting columns and identify interesting slices of
such columns and column combinations based on few interestingness measures. We
use our own fine-tuned variation of a pre-trained language model(T5) to
generate natural language questions in a specific manner. We then slot-fill
values in the generated questions and rank them for recommendations. We show
the utility of our proposed system in a coversational setting with a collection
of real datasets.","[{'name': 'Ritwik Chaudhuri'}, {'name': 'Rajmohan C'}, {'name': 'Kirushikesh DB'}, {'name': 'Arvind Agarwal'}]",2024-07-10T08:07:05Z
http://arxiv.org/abs/2407.17416v1,http://arxiv.org/abs/2407.17416v1,"Explaining Spectrograms in Machine Learning: A Study on Neural Networks
  for Speech Classification","This study investigates discriminative patterns learned by neural networks
for accurate speech classification, with a specific focus on vowel
classification tasks. By examining the activations and features of neural
networks for vowel classification, we gain insights into what the networks
""see"" in spectrograms. Through the use of class activation mapping, we identify
the frequencies that contribute to vowel classification and compare these
findings with linguistic knowledge. Experiments on a American English dataset
of vowels showcases the explainability of neural networks and provides valuable
insights into the causes of misclassifications and their characteristics when
differentiating them from unvoiced speech. This study not only enhances our
understanding of the underlying acoustic cues in vowel classification but also
offers opportunities for improving speech recognition by bridging the gap
between abstract representations in neural networks and established linguistic
knowledge","[{'name': 'Jesin James'}, {'name': 'Balamurali B. T.'}, {'name': 'Binu Abeysinghe'}, {'name': 'Junchen Liu'}]",2024-07-10T07:37:18Z
http://arxiv.org/abs/2407.07425v1,http://arxiv.org/abs/2407.07425v1,Out-of-distribution generalisation in spoken language understanding,"Test data is said to be out-of-distribution (OOD) when it unexpectedly
differs from the training data, a common challenge in real-world use cases of
machine learning. Although OOD generalisation has gained interest in recent
years, few works have focused on OOD generalisation in spoken language
understanding (SLU) tasks. To facilitate research on this topic, we introduce a
modified version of the popular SLU dataset SLURP, featuring data splits for
testing OOD generalisation in the SLU task. We call our modified dataset SLURP
For OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find
end-to-end SLU models to have limited capacity for generalisation. Furthermore,
by employing model interpretability techniques, we shed light on the factors
contributing to the generalisation difficulties of the models. To improve the
generalisation, we experiment with two techniques, which improve the results on
some, but not all the splits, emphasising the need for new techniques.","[{'name': 'Dejan Porjazovski'}, {'name': 'Anssi Moisio'}, {'name': 'Mikko Kurimo'}]",2024-07-10T07:27:38Z
http://arxiv.org/abs/2407.07931v1,http://arxiv.org/abs/2407.07931v1,"Search, Examine and Early-Termination: Fake News Detection with
  Annotation-Free Evidences","Pioneer researches recognize evidences as crucial elements in fake news
detection apart from patterns. Existing evidence-aware methods either require
laborious pre-processing procedures to assure relevant and high-quality
evidence data, or incorporate the entire spectrum of available evidences in all
news cases, regardless of the quality and quantity of the retrieved data. In
this paper, we propose an approach named \textbf{SEE} that retrieves useful
information from web-searched annotation-free evidences with an
early-termination mechanism. The proposed SEE is constructed by three main
phases: \textbf{S}earching online materials using the news as a query and
directly using their titles as evidences without any annotating or filtering
procedure, sequentially \textbf{E}xamining the news alongside with each piece
of evidence via attention mechanisms to produce new hidden states with
retrieved information, and allowing \textbf{E}arly-termination within the
examining loop by assessing whether there is adequate confidence for producing
a correct prediction. We have conducted extensive experiments on datasets with
unprocessed evidences, i.e., Weibo21, GossipCop, and pre-processed evidences,
namely Snopes and PolitiFact. The experimental results demonstrate that the
proposed method outperforms state-of-the-art approaches.","[{'name': 'Yuzhou Yang'}, {'name': 'Yangming Zhou'}, {'name': 'Qichao Ying'}, {'name': 'Zhenxing Qian'}, {'name': 'Xinpeng Zhang'}]",2024-07-10T07:22:30Z
http://arxiv.org/abs/2407.07370v1,http://arxiv.org/abs/2407.07370v1,LokiLM: Technical Report,"In this work, we introduce LokiLM, a 1.4B parameter large language model
trained on 500B tokens. Our model performs strongly in natural language
reasoning tasks and achieves state-of-the-art performance among models with
1.5B parameters or less. LokiLM is trained using multi-teacher knowledge
distillation and high-quality training data to achieve benchmark results
competitive with larger models trained on significantly more tokens. We support
these findings by introducing steps to avoid benchmark contamination and
overfitting throughout our development process. Despite its promising
performance, LokiLM exhibits a concerning amount of hallucinations and scores
poorly on the TruthfulQA benchmark, so we do not release the model publicly.","[{'name': 'Justin Kiefel'}, {'name': 'Shrey Shah'}]",2024-07-10T05:05:47Z
http://arxiv.org/abs/2407.07342v1,http://arxiv.org/abs/2407.07342v1,"Multilingual Blending: LLM Safety Alignment Evaluation with Language
  Mixture","As safety remains a crucial concern throughout the development lifecycle of
Large Language Models (LLMs), researchers and industrial practitioners have
increasingly focused on safeguarding and aligning LLM behaviors with human
preferences and ethical standards. LLMs, trained on extensive multilingual
corpora, exhibit powerful generalization abilities across diverse languages and
domains. However, current safety alignment practices predominantly focus on
single-language scenarios, which leaves their effectiveness in complex
multilingual contexts, especially for those complex mixed-language formats,
largely unexplored. In this study, we introduce Multilingual Blending, a
mixed-language query-response scheme designed to evaluate the safety alignment
of various state-of-the-art LLMs (e.g., GPT-4o, GPT-3.5, Llama3) under
sophisticated, multilingual conditions. We further investigate language
patterns such as language availability, morphology, and language family that
could impact the effectiveness of Multilingual Blending in compromising the
safeguards of LLMs. Our experimental results show that, without meticulously
crafted prompt templates, Multilingual Blending significantly amplifies the
detriment of malicious queries, leading to dramatically increased bypass rates
in LLM safety alignment (67.23% on GPT-3.5 and 40.34% on GPT-4o), far exceeding
those of single-language baselines. Moreover, the performance of Multilingual
Blending varies notably based on intrinsic linguistic properties, with
languages of different morphology and from diverse families being more prone to
evading safety alignments. These findings underscore the necessity of
evaluating LLMs and developing corresponding safety alignment strategies in a
complex, multilingual context to align with their superior cross-language
generalization capabilities.","[{'name': 'Jiayang Song'}, {'name': 'Yuheng Huang'}, {'name': 'Zhehua Zhou'}, {'name': 'Lei Ma'}]",2024-07-10T03:26:15Z
http://arxiv.org/abs/2407.07329v1,http://arxiv.org/abs/2407.07329v1,"Probability of Differentiation Reveals Brittleness of Homogeneity Bias
  in Large Language Models","Homogeneity bias in Large Language Models (LLMs) refers to their tendency to
homogenize the representations of some groups compared to others. Previous
studies documenting this bias have predominantly used encoder models, which may
have inadvertently introduced biases. To address this limitation, we prompted
GPT-4 to generate single word/expression completions associated with 18
situation cues - specific, measurable elements of environments that influence
how individuals perceive situations and compared the variability of these
completions using probability of differentiation. This approach directly
assessed homogeneity bias from the model's outputs, bypassing encoder models.
Across five studies, we find that homogeneity bias is highly volatile across
situation cues and writing prompts, suggesting that the bias observed in past
work may reflect those within encoder models rather than LLMs. Furthermore,
these results suggest that homogeneity bias in LLMs is brittle, as even minor
and arbitrary changes in prompts can significantly alter the expression of
biases. Future work should further explore how variations in syntactic features
and topic choices in longer text generations influence homogeneity bias in
LLMs.","[{'name': 'Messi H. J. Lee'}, {'name': 'Calvin K. Lai'}]",2024-07-10T02:56:55Z
http://arxiv.org/abs/2407.07325v2,http://arxiv.org/abs/2407.07325v2,HiLight: Technical Report on the Motern AI Video Language Model,"This technical report presents the implementation of a state-of-the-art video
encoder for video-text modal alignment and a video conversation framework
called HiLight, which features dual visual towers. The work is divided into two
main parts: 1.alignment of video and text modalities; 2.convenient and
efficient way to interact with users. Our goal is to address the task of video
comprehension in the context of billiards. The report includes a discussion of
the concepts and the final solution developed during the task's implementation.","[{'name': 'Zhiting Wang'}, {'name': 'Qiangong Zhou'}, {'name': 'Kangjie Yang'}, {'name': 'Zongyang Liu'}, {'name': 'Xin Mao'}]",2024-07-10T02:43:18Z
http://arxiv.org/abs/2407.07321v1,http://arxiv.org/abs/2407.07321v1,"RAG vs. Long Context: Examining Frontier Large Language Models for
  Environmental Review Document Comprehension","Large Language Models (LLMs) have been applied to many research problems
across various domains. One of the applications of LLMs is providing
question-answering systems that cater to users from different fields. The
effectiveness of LLM-based question-answering systems has already been
established at an acceptable level for users posing questions in popular and
public domains such as trivia and literature. However, it has not often been
established in niche domains that traditionally require specialized expertise.
To this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performance
of three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answering
questions originating from Environmental Impact Statements prepared by U.S.
federal government agencies in accordance with the National Environmental
Environmental Act (NEPA). We specifically measure the ability of LLMs to
understand the nuances of legal, technical, and compliance-related information
present in NEPA documents in different contextual scenarios. For example, we
test the LLMs' internal prior NEPA knowledge by providing questions without any
context, as well as assess how LLMs synthesize the contextual information
present in long NEPA documents to facilitate the question/answering task. We
compare the performance of the long context LLMs and RAG powered models in
handling different types of questions (e.g., problem-solving, divergent). Our
results suggest that RAG powered models significantly outperform the long
context models in the answer accuracy regardless of the choice of the frontier
LLM. Our further analysis reveals that many models perform better answering
closed questions than divergent and problem-solving questions.","[{'name': 'Hung Phan'}, {'name': 'Anurag Acharya'}, {'name': 'Sarthak Chaturvedi'}, {'name': 'Shivam Sharma'}, {'name': 'Mike Parker'}, {'name': 'Dan Nally'}, {'name': 'Ali Jannesari'}, {'name': 'Karl Pazdernik'}, {'name': 'Mahantesh Halappanavar'}, {'name': 'Sai Munikoti'}, {'name': 'Sameera Horawalavithana'}]",2024-07-10T02:33:09Z
http://arxiv.org/abs/2407.07313v1,http://arxiv.org/abs/2407.07313v1,"ESM+: Modern Insights into Perspective on Text-to-SQL Evaluation in the
  Age of Large Language Models","The task of Text-to-SQL enables anyone to retrieve information from SQL
databases using natural language. Despite several challenges, recent models
have made remarkable advancements in this task using large language models
(LLMs). Interestingly, we find that LLM-based models without fine-tuning
exhibit distinct natures compared to their fine-tuned counterparts, leading to
inadequacies in current evaluation metrics to accurately convey their
performance. Thus, we analyze the two primary metrics, Test Suite Execution
Accuracy (EXE) and Exact Set Matching Accuracy (ESM), to examine their
robustness for this task and address shortcomings. We compare the performance
of 9 LLM-based models using EXE, the original ESM, and our improved ESM (called
ESM+). Our results show that EXE and ESM have high false positive and negative
rates of 11.3% and 13.9%, while ESM+ gives those of 0.1% and 2.6% respectively,
providing a significantly more stable evaluation. We release the ESM+ script as
open-source for the community to contribute, while enjoying a more reliable
assessment of Text-to-SQL.","[{'name': 'Benjamin Ascoli'}, {'name': 'Ram Kandikonda'}, {'name': 'Jinho D. Choi'}]",2024-07-10T02:20:19Z
http://arxiv.org/abs/2407.12858v1,http://arxiv.org/abs/2407.12858v1,"Grounding and Evaluation for Large Language Models: Practical Challenges
  and Lessons Learned (Survey)","With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems
in high-stakes domains, ensuring the trustworthiness, safety, and observability
of these systems has become crucial. It is essential to evaluate and monitor AI
systems not only for accuracy and quality-related metrics but also for
robustness, bias, security, interpretability, and other responsible AI
dimensions. We focus on large language models (LLMs) and other generative AI
models, which present additional challenges such as hallucinations, harmful and
manipulative content, and copyright infringement. In this survey article
accompanying our KDD 2024 tutorial, we highlight a wide range of harms
associated with generative AI systems, and survey state of the art approaches
(along with open challenges) to address these harms.","[{'name': 'Krishnaram Kenthapadi'}, {'name': 'Mehrnoosh Sameki'}, {'name': 'Ankur Taly'}]",2024-07-10T01:23:10Z
http://arxiv.org/abs/2407.07275v1,http://arxiv.org/abs/2407.07275v1,"Remastering Divide and Remaster: A Cinematic Audio Source Separation
  Dataset with Multilingual Support","Cinematic audio source separation (CASS) is a relatively new subtask of audio
source separation, concerned with the separation of a mixture into the
dialogue, music, and effects stems. To date, only one publicly available
dataset exists for CASS, that is, the Divide and Remaster (DnR) dataset, which
is currently at version 2. While DnR v2 has been an incredibly useful resource
for CASS, several areas of improvement have been identified, particularly
through its use in the 2023 Sound Demixing Challenge. In this work, we develop
version 3 of the DnR dataset, addressing issues relating to vocal content in
non-dialogue stems, loudness distributions, mastering process, and linguistic
diversity. In particular, the dialogue stem of DnR v3 includes speech content
from more than 30 languages from multiple families including but not limited to
the Germanic, Romance, Indo-Aryan, Dravidian, Malayo-Polynesian, and Bantu
families. Benchmark results using the Bandit model indicated that training on
multilingual data yields significant generalizability to the model even in
languages with low data availability. Even in languages with high data
availability, the multilingual model often performs on par or better than
dedicated models trained on monolingual CASS datasets.","[{'name': 'Karn N. Watcharasupat'}, {'name': 'Chih-Wei Wu'}, {'name': 'Iroro Orife'}]",2024-07-09T23:39:37Z
http://arxiv.org/abs/2407.07263v1,http://arxiv.org/abs/2407.07263v1,"Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language
  Models","As language models have scaled both their number of parameters and
pretraining dataset sizes, the computational cost for pretraining has become
intractable except for the most well-resourced teams. This increasing cost
makes it ever more important to be able to reuse a model after it has completed
pretraining; allowing for a model's abilities to further improve without
needing to train from scratch. In this work, we detail a set of guidelines that
cover how to design efficacious data distributions and learning rate schedules
for continued pretraining of language models. When applying these findings
within a continued pretraining run on top of a well-trained 15B parameter
model, we show an improvement of 9\% in average model accuracy compared to the
baseline of continued training on the pretraining set. The resulting recipe
provides a practical starting point with which to begin developing language
models through reuse rather than retraining.","[{'name': 'Jupinder Parmar'}, {'name': 'Sanjev Satheesh'}, {'name': 'Mostofa Patwary'}, {'name': 'Mohammad Shoeybi'}, {'name': 'Bryan Catanzaro'}]",2024-07-09T22:37:59Z
http://arxiv.org/abs/2407.07258v1,http://arxiv.org/abs/2407.07258v1,"Identification of emotions on Twitter during the 2022 electoral process
  in Colombia","The study of Twitter as a means for analyzing social phenomena has gained
interest in recent years due to the availability of large amounts of data in a
relatively spontaneous environment. Within opinion-mining tasks, emotion
detection is specially relevant, as it allows for the identification of
people's subjective responses to different social events in a more granular way
than traditional sentiment analysis based on polarity. In the particular case
of political events, the analysis of emotions in social networks can provide
valuable information on the perception of candidates, proposals, and other
important aspects of the public debate. In spite of this importance, there are
few studies on emotion detection in Spanish and, to the best of our knowledge,
few resources are public for opinion mining in Colombian Spanish, highlighting
the need for generating resources addressing the specific cultural
characteristics of this variety. In this work, we present a small corpus of
tweets in Spanish related to the 2022 Colombian presidential elections,
manually labeled with emotions using a fine-grained taxonomy. We perform
classification experiments using supervised state-of-the-art models (BERT
models) and compare them with GPT-3.5 in few-shot learning settings. We make
our dataset and code publicly available for research purposes.","[{'name': 'Juan Jose Iguaran Fernandez'}, {'name': 'Juan Manuel Perez'}, {'name': 'German Rosati'}]",2024-07-09T22:26:42Z
http://arxiv.org/abs/2407.07225v1,http://arxiv.org/abs/2407.07225v1,ConvNLP: Image-based AI Text Detection,"The potentials of Generative-AI technologies like Large Language models
(LLMs) to revolutionize education are undermined by ethical considerations
around their misuse which worsens the problem of academic dishonesty. LLMs like
GPT-4 and Llama 2 are becoming increasingly powerful in generating
sophisticated content and answering questions, from writing academic essays to
solving complex math problems. Students are relying on these LLMs to complete
their assignments and thus compromising academic integrity. Solutions to detect
LLM-generated text are compute-intensive and often lack generalization. This
paper presents a novel approach for detecting LLM-generated AI-text using a
visual representation of word embedding. We have formulated a novel
Convolutional Neural Network called ZigZag ResNet, as well as a scheduler for
improving generalization, named ZigZag Scheduler. Through extensive evaluation
using datasets of text generated by six different state-of-the-art LLMs, our
model demonstrates strong intra-domain and inter-domain generalization
capabilities. Our best model detects AI-generated text with an impressive
average detection rate (over inter- and intra-domain test data) of 88.35%.
Through an exhaustive ablation study, our ZigZag ResNet and ZigZag Scheduler
provide a performance improvement of nearly 4% over the vanilla ResNet. The
end-to-end inference latency of our model is below 2.5ms per sentence. Our
solution offers a lightweight, computationally efficient, and faster
alternative to existing tools for AI-generated text detection, with better
generalization performance. It can help academic institutions in their fight
against the misuse of LLMs in academic settings. Through this work, we aim to
contribute to safeguarding the principles of academic integrity and ensuring
the trustworthiness of student work in the era of advanced LLMs.","[{'name': 'Suriya Prakash Jambunathan'}, {'name': 'Ashwath Shankarnarayan'}, {'name': 'Parijat Dube'}]",2024-07-09T20:44:40Z
http://arxiv.org/abs/2407.07094v1,http://arxiv.org/abs/2407.07094v1,AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning,"The pervasive deployment of Large Language Models-LLMs in various sectors
often neglects the nuanced requirements of individuals and small organizations,
who benefit more from models precisely tailored to their specific business
contexts rather than those with broadly superior general capabilities. This
work introduces \textbf{AnyTaskTune}, a novel fine-tuning methodology coined as
\textbf{Task-Fine-Tune}, specifically developed to elevate model performance on
a diverse array of domain-specific tasks. This method involves a meticulous
process to identify and define targeted sub-tasks within a domain, followed by
the creation of specialized enhancement datasets for fine-tuning, thereby
optimizing task-specific model performance. We conducted comprehensive
fine-tuning experiments not only in the legal domain for tasks such as keyword
extraction and sentence prediction but across over twenty different sub-tasks
derived from the domains of finance, healthcare, law, psychology, consumer
services, and human resources. To substantiate our approach and facilitate
community engagement, we will open-source these bilingual task datasets. Our
findings demonstrate that models fine-tuned using the \textbf{Task-Fine-Tune}
methodology not only achieve superior performance on these specific tasks but
also significantly outperform models with higher general capabilities in their
respective domains. Our work is publicly available at
\url{https://github.com/PandaVT/DataTager}.","[{'name': 'Jiaxi Cui'}, {'name': 'Wentao Zhang'}, {'name': 'Jing Tang'}, {'name': 'Xudong Tong'}, {'name': 'Zhenwei Zhang'}, {'name': 'Amie'}, {'name': 'Jing Wen'}, {'name': 'Rongsheng Wang'}, {'name': 'Pengfei Wu'}]",2024-07-09T17:59:56Z
http://arxiv.org/abs/2407.07093v1,http://arxiv.org/abs/2407.07093v1,"FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive
  Distillation","This work presents a Fully BInarized Large Language Model (FBI-LLM),
demonstrating for the first time how to train a large-scale binary language
model from scratch (not the partial binary or ternary LLM like BitNet b1.58) to
match the performance of its full-precision counterparts (e.g., FP16 or BF16)
in transformer-based LLMs. It achieves this by employing an autoregressive
distillation (AD) loss with maintaining equivalent model dimensions (130M,
1.3B, 7B) and training data volume as regular LLM pretraining, while delivering
competitive results in terms of perplexity and task-specific effectiveness.
Intriguingly, by analyzing the training trajectory, we find that the pretrained
weight is not necessary for training binarized LLMs from scratch. This research
encourages a new computational framework and may facilitate the future design
of specialized hardware tailored for fully 1-bit LLMs. We make all models,
code, and training dataset fully accessible and transparent to support further
research (Code: https://github.com/LiqunMa/FBI-LLM. Model:
https://huggingface.co/LiqunMa/).","[{'name': 'Liqun Ma'}, {'name': 'Mingjie Sun'}, {'name': 'Zhiqiang Shen'}]",2024-07-09T17:59:48Z
http://arxiv.org/abs/2407.07087v1,http://arxiv.org/abs/2407.07087v1,"CopyBench: Measuring Literal and Non-Literal Reproduction of
  Copyright-Protected Text in Language Model Generation","Evaluating the degree of reproduction of copyright-protected content by
language models (LMs) is of significant interest to the AI and legal
communities. Although both literal and non-literal similarities are considered
by courts when assessing the degree of reproduction, prior research has focused
only on literal similarities. To bridge this gap, we introduce CopyBench, a
benchmark designed to measure both literal and non-literal copying in LM
generations. Using copyrighted fiction books as text sources, we provide
automatic evaluation protocols to assess literal and non-literal copying,
balanced against the model utility in terms of the ability to recall facts from
the copyrighted works and generate fluent completions. We find that, although
literal copying is relatively rare, two types of non-literal copying -- event
copying and character copying -- occur even in models as small as 7B
parameters. Larger models demonstrate significantly more copying, with literal
copying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3%
to 6.9% when comparing Llama3-8B and 70B models, respectively. We further
evaluate the effectiveness of current strategies for mitigating copying and
show that (1) training-time alignment can reduce literal copying but may
increase non-literal copying, and (2) current inference-time mitigation methods
primarily reduce literal but not non-literal copying.","[{'name': 'Tong Chen'}, {'name': 'Akari Asai'}, {'name': 'Niloofar Mireshghallah'}, {'name': 'Sewon Min'}, {'name': 'James Grimmelmann'}, {'name': 'Yejin Choi'}, {'name': 'Hannaneh Hajishirzi'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Pang Wei Koh'}]",2024-07-09T17:58:18Z
http://arxiv.org/abs/2407.07080v1,http://arxiv.org/abs/2407.07080v1,"Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary
  and Instruction Capabilities","Training large language models (LLMs) in low-resource languages such as
Hebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and
DictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a
substantial corpus of approximately 200 billion tokens in both Hebrew and
English. Adapting a pre-trained model to a new language involves specialized
techniques that differ significantly from training a model from scratch or
further training existing models on well-resourced languages such as English.
We outline these novel training methodologies, which facilitate effective
learning and adaptation to the linguistic properties of Hebrew. Additionally,
we fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to
enhance its performance on task-specific instructions. To rigorously evaluate
our models, we introduce a new benchmark suite for Hebrew LLM evaluation,
covering a diverse set of tasks including Question Answering, Sentiment
Analysis, Winograd Schema Challenge, Translation, and Summarization. Our work
not only addresses the intricacies of training LLMs in low-resource languages
but also proposes a framework that can be leveraged for adapting other LLMs to
various non-English languages, contributing to the broader field of
multilingual NLP.","[{'name': 'Shaltiel Shmidman'}, {'name': 'Avi Shmidman'}, {'name': 'Amir DN Cohen'}, {'name': 'Moshe Koppel'}]",2024-07-09T17:51:37Z
http://arxiv.org/abs/2407.07071v1,http://arxiv.org/abs/2407.07071v1,"Lookback Lens: Detecting and Mitigating Contextual Hallucinations in
  Large Language Models Using Only Attention Maps","When asked to summarize articles or answer questions given a passage, large
language models (LLMs) can hallucinate details and respond with unsubstantiated
answers that are inaccurate with respect to the input context. This paper
describes a simple approach for detecting such contextual hallucinations. We
hypothesize that contextual hallucinations are related to the extent to which
an LLM attends to information in the provided context versus its own
generations. Based on this intuition, we propose a simple hallucination
detection model whose input features are given by the ratio of attention
weights on the context versus newly generated tokens (for each attention head).
We find that a linear classifier based on these lookback ratio features is as
effective as a richer detector that utilizes the entire hidden states of an LLM
or a text-based entailment model. The lookback ratio-based detector -- Lookback
Lens -- is found to transfer across tasks and even models, allowing a detector
that is trained on a 7B model to be applied (without retraining) to a larger
13B model. We further apply this detector to mitigate contextual
hallucinations, and find that a simple classifier-guided decoding approach is
able to reduce the amount of hallucination, for example by 9.6% in the XSum
summarization task.","[{'name': 'Yung-Sung Chuang'}, {'name': 'Linlu Qiu'}, {'name': 'Cheng-Yu Hsieh'}, {'name': 'Ranjay Krishna'}, {'name': 'Yoon Kim'}, {'name': 'James Glass'}]",2024-07-09T17:44:34Z
http://arxiv.org/abs/2407.07035v1,http://arxiv.org/abs/2407.07035v1,"Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era
  of Foundation Models","Vision-and-Language Navigation (VLN) has gained increasing attention over
recent years and many approaches have emerged to advance their development. The
remarkable achievements of foundation models have shaped the challenges and
proposed methods for VLN research. In this survey, we provide a top-down review
that adopts a principled framework for embodied planning and reasoning, and
emphasizes the current methods and future opportunities leveraging foundation
models to address VLN challenges. We hope our in-depth discussions could
provide valuable resources and insights: on one hand, to milestone the progress
and explore opportunities and potential roles for foundation models in this
field, and on the other, to organize different challenges and solutions in VLN
to foundation model researchers.","[{'name': 'Yue Zhang'}, {'name': 'Ziqiao Ma'}, {'name': 'Jialu Li'}, {'name': 'Yanyuan Qiao'}, {'name': 'Zun Wang'}, {'name': 'Joyce Chai'}, {'name': 'Qi Wu'}, {'name': 'Mohit Bansal'}, {'name': 'Parisa Kordjamshidi'}]",2024-07-09T16:53:36Z
http://arxiv.org/abs/2407.07026v1,http://arxiv.org/abs/2407.07026v1,"Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via
  Semantics Completion and Decomposition","With the proliferation of social media posts in recent years, the need to
detect sentiments in multimodal (image-text) content has grown rapidly. Since
posts are user-generated, the image and text from the same post can express
different or even contradictory sentiments, leading to potential
\textbf{sentiment discrepancy}. However, existing works mainly adopt a
single-branch fusion structure that primarily captures the consistent sentiment
between image and text. The ignorance or implicit modeling of discrepant
sentiment results in compromised unimodal encoding and limited performances. In
this paper, we propose a semantics Completion and Decomposition (CoDe) network
to resolve the above issue. In the semantics completion module, we complement
image and text representations with the semantics of the OCR text embedded in
the image, helping bridge the sentiment gap. In the semantics decomposition
module, we decompose image and text representations with exclusive projection
and contrastive learning, thereby explicitly capturing the discrepant sentiment
between modalities. Finally, we fuse image and text representations by
cross-attention and combine them with the learned discrepant sentiment for
final classification. Extensive experiments conducted on four multimodal
sentiment datasets demonstrate the superiority of CoDe against SOTA methods.","[{'name': 'Daiqing Wu'}, {'name': 'Dongbao Yang'}, {'name': 'Huawen Shen'}, {'name': 'Can Ma'}, {'name': 'Yu Zhou'}]",2024-07-09T16:46:58Z
http://arxiv.org/abs/2407.07019v1,http://arxiv.org/abs/2407.07019v1,"Using Large Language Models for Generating Smart Contracts for Health
  Insurance from Textual Policies","We explore using Large Language Models (LLMs) to generate application code
that automates health insurance processes from text-based policies. We target
blockchain-based smart contracts as they offer immutability, verifiability,
scalability, and a trustless setting: any number of parties can use the smart
contracts, and they need not have previously established trust relationships
with each other. Our methodology generates outputs at increasing levels of
technical detail: (1) textual summaries, (2) declarative decision logic, and
(3) smart contract code with unit tests. We ascertain LLMs are good at the task
(1), and the structured output is useful to validate tasks (2) and (3).
Declarative languages (task 2) are often used to formalize healthcare policies,
but their execution on blockchain is non-trivial. Hence, task (3) attempts to
directly automate the process using smart contracts. To assess the LLM output,
we propose completeness, soundness, clarity, syntax, and functioning code as
metrics. Our evaluation employs three health insurance policies (scenarios)
with increasing difficulty from Medicare's official booklet. Our evaluation
uses GPT-3.5 Turbo, GPT-3.5 Turbo 16K, GPT-4, GPT-4 Turbo and CodeLLaMA. Our
findings confirm that LLMs perform quite well in generating textual summaries.
Although outputs from tasks (2)-(3) are useful starting points, they require
human oversight: in multiple cases, even ""runnable"" code will not yield sound
results; the popularity of the target language affects the output quality; and
more complex scenarios still seem a bridge too far. Nevertheless, our
experiments demonstrate the promise of LLMs for translating textual process
descriptions into smart contracts.","[{'name': 'Inwon Kang'}, {'name': 'William Van Woensel'}, {'name': 'Oshani Seneviratne'}]",2024-07-09T16:40:44Z
http://arxiv.org/abs/2407.07018v1,http://arxiv.org/abs/2407.07018v1,"End-To-End Causal Effect Estimation from Unstructured Natural Language
  Data","Knowing the effect of an intervention is critical for human decision-making,
but current approaches for causal effect estimation rely on manual data
collection and structuring, regardless of the causal assumptions. This
increases both the cost and time-to-completion for studies. We show how large,
diverse observational text data can be mined with large language models (LLMs)
to produce inexpensive causal effect estimates under appropriate causal
assumptions. We introduce NATURAL, a novel family of causal effect estimators
built with LLMs that operate over datasets of unstructured text. Our estimators
use LLM conditional distributions (over variables of interest, given the text
data) to assist in the computation of classical estimators of causal effect. We
overcome a number of technical challenges to realize this idea, such as
automating data curation and using LLMs to impute missing information. We
prepare six (two synthetic and four real) observational datasets, paired with
corresponding ground truth in the form of randomized trials, which we used to
systematically evaluate each step of our pipeline. NATURAL estimators
demonstrate remarkable performance, yielding causal effect estimates that fall
within 3 percentage points of their ground truth counterparts, including on
real-world Phase 3/4 clinical trials. Our results suggest that unstructured
text data is a rich source of causal effect information, and NATURAL is a first
step towards an automated pipeline to tap this resource.","[{'name': 'Nikita Dhawan'}, {'name': 'Leonardo Cotta'}, {'name': 'Karen Ullrich'}, {'name': 'Rahul G. Krishnan'}, {'name': 'Chris J. Maddison'}]",2024-07-09T16:38:48Z
http://arxiv.org/abs/2407.07011v1,http://arxiv.org/abs/2407.07011v1,"Induction Heads as an Essential Mechanism for Pattern Matching in
  In-context Learning","Large language models (LLMs) have shown a remarkable ability to learn and
perform complex tasks through in-context learning (ICL). However, a
comprehensive understanding of its internal mechanisms is still lacking. This
paper explores the role of induction heads in a few-shot ICL setting. We
analyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract
pattern recognition and NLP tasks. Our results show that even a minimal
ablation of induction heads leads to ICL performance decreases of up to ~32%
for abstract pattern recognition tasks, bringing the performance close to
random. For NLP tasks, this ablation substantially decreases the model's
ability to benefit from examples, bringing few-shot ICL performance close to
that of zero-shot prompts. We further use attention knockout to disable
specific induction patterns, and present fine-grained evidence for the role
that the induction mechanism plays in ICL.","[{'name': 'J. Crosbie'}, {'name': 'E. Shutova'}]",2024-07-09T16:29:21Z
http://arxiv.org/abs/2407.07004v2,http://arxiv.org/abs/2407.07004v2,"Empirical analysis of Binding Precedent efficiency in the Brazilian
  Supreme Court via Similar Case Retrieval","Binding precedents (S\'umulas Vinculantes) constitute a juridical instrument
unique to the Brazilian legal system and whose objectives include the
protection of the Federal Supreme Court against repetitive demands. Studies of
the effectiveness of these instruments in decreasing the Court's exposure to
similar cases, however, indicate that they tend to fail in such a direction,
with some of the binding precedents seemingly creating new demands. We
empirically assess the legal impact of five binding precedents, 11, 14, 17, 26
and 37, at the highest court level through their effects on the legal subjects
they address. This analysis is only possible through the comparison of the
Court's ruling about the precedents' themes before they are created, which
means that these decisions should be detected through techniques of Similar
Case Retrieval. The contributions of this article are therefore twofold: on the
mathematical side, we compare the uses of different methods of Natural Language
Processing -- TF-IDF, LSTM, BERT, and regex -- for Similar Case Retrieval,
whereas on the legal side, we contrast the inefficiency of these binding
precedents with a set of hypotheses that may justify their repeated usage. We
observe that the deep learning models performed significantly worse in the
specific Similar Case Retrieval task and that the reasons for binding
precedents to fail in responding to repetitive demand are heterogeneous and
case-dependent, making it impossible to single out a specific cause.","[{'name': 'Raphaël Tinarrage'}, {'name': 'Henrique Ennes'}, {'name': 'Lucas E. Resck'}, {'name': 'Lucas T. Gomes'}, {'name': 'Jean R. Ponciano'}, {'name': 'Jorge Poco'}]",2024-07-09T16:17:16Z
http://arxiv.org/abs/2407.07000v1,http://arxiv.org/abs/2407.07000v1,"Metron: Holistic Performance Evaluation Framework for LLM Inference
  Systems","Serving large language models (LLMs) in production can incur substantial
costs, which has prompted recent advances in inference system optimizations.
Today, these systems are evaluated against conventional latency and throughput
metrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics
fail to fully capture the nuances of LLM inference, leading to an incomplete
assessment of user-facing performance crucial for real-time applications such
as chat and translation. In this paper, we first identify the pitfalls of
current performance metrics in evaluating LLM inference systems. We then
propose Metron, a comprehensive performance evaluation framework that includes
fluidity-index -- a novel metric designed to reflect the intricacies of the LLM
inference process and its impact on real-time user experience. Finally, we
evaluate various existing open-source platforms and model-as-a-service
offerings using Metron, discussing their strengths and weaknesses. Metron is
available at https://github.com/project-metron/metron.","[{'name': 'Amey Agrawal'}, {'name': 'Anmol Agarwal'}, {'name': 'Nitin Kedia'}, {'name': 'Jayashree Mohan'}, {'name': 'Souvik Kundu'}, {'name': 'Nipun Kwatra'}, {'name': 'Ramachandran Ramjee'}, {'name': 'Alexey Tumanov'}]",2024-07-09T16:13:26Z
http://arxiv.org/abs/2407.06992v2,http://arxiv.org/abs/2407.06992v2,"Robust Neural Information Retrieval: An Adversarial and
  Out-of-distribution Perspective","Recent advances in neural information retrieval (IR) models have
significantly enhanced their effectiveness over various IR tasks. The
robustness of these models, essential for ensuring their reliability in
practice, has also garnered significant attention. With a wide array of
research on robust IR being proposed, we believe it is the opportune moment to
consolidate the current status, glean insights from existing methodologies, and
lay the groundwork for future development. We view the robustness of IR to be a
multifaceted concept, emphasizing its necessity against adversarial attacks,
out-of-distribution (OOD) scenarios and performance variance. With a focus on
adversarial and OOD robustness, we dissect robustness solutions for dense
retrieval models (DRMs) and neural ranking models (NRMs), respectively,
recognizing them as pivotal components of the neural IR pipeline. We provide an
in-depth discussion of existing methods, datasets, and evaluation metrics,
shedding light on challenges and future directions in the era of large language
models. To the best of our knowledge, this is the first comprehensive survey on
the robustness of neural IR models, and we will also be giving our first
tutorial presentation at SIGIR 2024
\url{https://sigir2024-robust-information-retrieval.github.io}. Along with the
organization of existing work, we introduce a Benchmark for robust IR (BestIR),
a heterogeneous evaluation benchmark for robust neural information retrieval,
which is publicly available at \url{https://github.com/Davion-Liu/BestIR}. We
hope that this study provides useful clues for future research on the
robustness of IR models and helps to develop trustworthy search engines
\url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}.","[{'name': 'Yu-An Liu'}, {'name': 'Ruqing Zhang'}, {'name': 'Jiafeng Guo'}, {'name': 'Maarten de Rijke'}, {'name': 'Yixing Fan'}, {'name': 'Xueqi Cheng'}]",2024-07-09T16:07:01Z
http://arxiv.org/abs/2407.06990v1,http://arxiv.org/abs/2407.06990v1,Segment-Based Interactive Machine Translation for Pre-trained Models,"Pre-trained large language models (LLM) are starting to be widely used in
many applications. In this work, we explore the use of these models in
interactive machine translation (IMT) environments. In particular, we have
chosen mBART (multilingual Bidirectional and Auto-Regressive Transformer) and
mT5 (multilingual Text-to-Text Transfer Transformer) as the LLMs to perform our
experiments. The system generates perfect translations interactively using the
feedback provided by the user at each iteration. The Neural Machine Translation
(NMT) model generates a preliminary hypothesis with the feedback, and the user
validates new correct segments and performs a word correction--repeating the
process until the sentence is correctly translated. We compared the performance
of mBART, mT5, and a state-of-the-art (SoTA) machine translation model on a
benchmark dataset regarding user effort, Word Stroke Ratio (WSR), Key Stroke
Ratio (KSR), and Mouse Action Ratio (MAR). The experimental results indicate
that mBART performed comparably with SoTA models, suggesting that it is a
viable option for this field of IMT. The implications of this finding extend to
the development of new machine translation models for interactive environments,
as it indicates that some novel pre-trained models exhibit SoTA performance in
this domain, highlighting the potential benefits of adapting these models to
specific needs.","[{'name': 'Angel Navarro'}, {'name': 'Francisco Casacuberta'}]",2024-07-09T16:04:21Z
http://arxiv.org/abs/2407.06957v1,http://arxiv.org/abs/2407.06957v1,"Listen and Speak Fairly: A Study on Semantic Gender Bias in Speech
  Integrated Large Language Models","Speech Integrated Large Language Models (SILLMs) combine large language
models with speech perception to perform diverse tasks, such as emotion
recognition to speaker verification, demonstrating universal audio
understanding capability. However, these models may amplify biases present in
training data, potentially leading to biased access to information for
marginalized groups. This work introduces a curated spoken bias evaluation
toolkit and corresponding dataset. We evaluate gender bias in SILLMs across
four semantic-related tasks: speech-to-text translation (STT), spoken
coreference resolution (SCR), spoken sentence continuation (SSC), and spoken
question answering (SQA). Our analysis reveals that bias levels are
language-dependent and vary with different evaluation methods. Our findings
emphasize the necessity of employing multiple approaches to comprehensively
assess biases in SILLMs, providing insights for developing fairer SILLM
systems.","[{'name': 'Yi-Cheng Lin'}, {'name': 'Tzu-Quan Lin'}, {'name': 'Chih-Kai Yang'}, {'name': 'Ke-Han Lu'}, {'name': 'Wei-Chih Chen'}, {'name': 'Chun-Yi Kuan'}, {'name': 'Hung-yi Lee'}]",2024-07-09T15:35:43Z
http://arxiv.org/abs/2407.06946v1,http://arxiv.org/abs/2407.06946v1,Self-Recognition in Language Models,"A rapidly growing number of applications rely on a small set of closed-source
language models (LMs). This dependency might introduce novel security risks if
LMs develop self-recognition capabilities. Inspired by human identity
verification methods, we propose a novel approach for assessing
self-recognition in LMs using model-generated ""security questions"". Our test
can be externally administered to keep track of frontier models as it does not
require access to internal model parameters or output probabilities. We use our
test to examine self-recognition in ten of the most capable open- and
closed-source LMs currently publicly available. Our extensive experiments found
no empirical evidence of general or consistent self-recognition in any examined
LM. Instead, our results suggest that given a set of alternatives, LMs seek to
pick the ""best"" answer, regardless of its origin. Moreover, we find indications
that preferences about which models produce the best answers are consistent
across LMs. We additionally uncover novel insights on position bias
considerations for LMs in multiple-choice settings.","[{'name': 'Tim R. Davidson'}, {'name': 'Viacheslav Surkov'}, {'name': 'Veniamin Veselovsky'}, {'name': 'Giuseppe Russo'}, {'name': 'Robert West'}, {'name': 'Caglar Gulcehre'}]",2024-07-09T15:23:28Z
http://arxiv.org/abs/2407.06941v1,http://arxiv.org/abs/2407.06941v1,Raply: A profanity-mitigated rap generator,"The task of writing rap is challenging and involves producing complex rhyming
schemes, yet meaningful lyrics. In this work, we propose Raply, a fine-tuned
GPT-2 model capable of producing meaningful rhyming text in the style of rap.
In addition to its rhyming capabilities, the model is able to generate less
offensive content. It was achieved through the fine-tuning the model on a new
dataset Mitislurs, a profanity-mitigated corpus. We evaluate the output of the
model on two criteria: 1) rhyming based on the rhyme density metric; 2)
profanity content, using the list of profanities for the English language. To
our knowledge, this is the first attempt at profanity mitigation for rap lyrics
generation.","[{'name': 'Omar Manil Bendali'}, {'name': 'Samir Ferroum'}, {'name': 'Ekaterina Kozachenko'}, {'name': 'Youssef Parviz'}, {'name': 'Hanna Shcharbakova'}, {'name': 'Anna Tokareva'}, {'name': 'Shemair Williams'}]",2024-07-09T15:18:56Z
http://arxiv.org/abs/2407.12857v1,http://arxiv.org/abs/2407.12857v1,"Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and
  Analysis","In recent years, the rapid increase in scientific papers has overwhelmed
traditional review mechanisms, resulting in varying quality of publications.
Although existing methods have explored the capabilities of Large Language
Models (LLMs) for automated scientific reviewing, their generated contents are
often generic or partial. To address the issues above, we introduce an
automated paper reviewing framework SEA. It comprises of three modules:
Standardization, Evaluation, and Analysis, which are represented by models
SEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills data
standardization capabilities of GPT-4 for integrating multiple reviews for a
paper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it to
generate constructive reviews. Finally, SEA-A introduces a new evaluation
metric called mismatch score to assess the consistency between paper contents
and reviews. Moreover, we design a self-correction strategy to enhance the
consistency. Extensive experimental results on datasets collected from eight
venues show that SEA can generate valuable insights for authors to improve
their papers.","[{'name': 'Jianxiang Yu'}, {'name': 'Zichen Ding'}, {'name': 'Jiaqi Tan'}, {'name': 'Kangyang Luo'}, {'name': 'Zhenmin Weng'}, {'name': 'Chenghua Gong'}, {'name': 'Long Zeng'}, {'name': 'Renjing Cui'}, {'name': 'Chengcheng Han'}, {'name': 'Qiushi Sun'}, {'name': 'Zhiyong Wu'}, {'name': 'Yunshi Lan'}, {'name': 'Xiang Li'}]",2024-07-09T15:06:14Z
http://arxiv.org/abs/2407.06917v1,http://arxiv.org/abs/2407.06917v1,"Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in
  Large Language Models","Large language models (LLMs) have been shown to propagate and amplify harmful
stereotypes, particularly those that disproportionately affect marginalised
communities. To understand the effect of these stereotypes more
comprehensively, we introduce GlobalBias, a dataset of 876k sentences
incorporating 40 distinct gender-by-ethnicity groups alongside descriptors
typically used in bias literature, which enables us to study a broad set of
stereotypes from around the world. We use GlobalBias to directly probe a suite
of LMs via perplexity, which we use as a proxy to determine how certain
stereotypes are represented in the model's internal representations. Following
this, we generate character profiles based on given names and evaluate the
prevalence of stereotypes in model outputs. We find that the demographic groups
associated with various stereotypes remain consistent across model likelihoods
and model outputs. Furthermore, larger models consistently display higher
levels of stereotypical outputs, even when explicitly instructed not to.","[{'name': 'Zara Siddique'}, {'name': 'Liam D. Turner'}, {'name': 'Luis Espinosa-Anke'}]",2024-07-09T14:52:52Z
http://arxiv.org/abs/2407.06908v1,http://arxiv.org/abs/2407.06908v1,"Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion
  Representation of Religion in Large Language Models","Emotions play important epistemological and cognitive roles in our lives,
revealing our values and guiding our actions. Previous work has shown that LLMs
display biases in emotion attribution along gender lines. However, unlike
gender, which says little about our values, religion, as a socio-cultural
system, prescribes a set of beliefs and values for its followers. Religions,
therefore, cultivate certain emotions. Moreover, these rules are explicitly
laid out and interpreted by religious leaders. Using emotion attribution, we
explore how different religions are represented in LLMs. We find that: Major
religions in the US and European countries are represented with more nuance,
displaying a more shaded model of their beliefs. Eastern religions like
Hinduism and Buddhism are strongly stereotyped. Judaism and Islam are
stigmatized -- the models' refusal skyrocket. We ascribe these to cultural bias
in LLMs and the scarcity of NLP literature on religion. In the rare instances
where religion is discussed, it is often in the context of toxic language,
perpetuating the perception of these religions as inherently toxic. This
finding underscores the urgent need to address and rectify these biases. Our
research underscores the crucial role emotions play in our lives and how our
values influence them.","[{'name': 'Flor Miriam Plaza-del-Arco'}, {'name': 'Amanda Cercas Curry'}, {'name': 'Susanna Paoli'}, {'name': 'Alba Curry'}, {'name': 'Dirk Hovy'}]",2024-07-09T14:45:15Z
http://arxiv.org/abs/2407.06893v1,http://arxiv.org/abs/2407.06893v1,"Measuring Sustainability Intention of ESG Fund Disclosure using Few-Shot
  Learning","Global sustainable fund universe encompasses open-end funds and
exchange-traded funds (ETF) that, by prospectus or other regulatory filings,
claim to focus on Environment, Social and Governance (ESG). Challengingly, the
claims can only be confirmed by examining the textual disclosures to check if
there is presence of intentionality and ESG focus on its investment strategy.
Currently, there is no regulation to enforce sustainability in ESG products
space. This paper proposes a unique method and system to classify and score the
fund prospectuses in the sustainable universe regarding specificity and
transparency of language. We aim to employ few-shot learners to identify
specific, ambiguous, and generic sustainable investment-related language.
Additionally, we construct a ratio metric to determine language score and
rating to rank products and quantify sustainability claims for US sustainable
universe. As a by-product, we publish manually annotated quality training
dataset on Hugging Face (ESG-Prospectus-Clarity-Category under cc-by-nc-sa-4.0)
of more than 1K ESG textual statements. The performance of the few-shot
finetuning approach is compared with zero-shot models e.g., Llama-13B, GPT 3.5
Turbo etc. We found that prompting large language models are not accurate for
domain specific tasks due to misalignment issues. The few-shot finetuning
techniques outperform zero-shot models by large margins of more than absolute
~30% in precision, recall and F1 metrics on completely unseen ESG languages
(test set). Overall, the paper attempts to establish a systematic and scalable
approach to measure and rate sustainability intention quantitatively for
sustainable funds using texts in prospectus. Regulatory bodies, investors, and
advisors may utilize the findings of this research to reduce cognitive load in
investigating or screening of ESG funds which accurately reflects the ESG
intention.","[{'name': 'Mayank Singh'}, {'name': 'Nazia Nafis'}, {'name': 'Abhijeet Kumar'}, {'name': 'Mridul Mishra'}]",2024-07-09T14:25:23Z
http://arxiv.org/abs/2407.06866v2,http://arxiv.org/abs/2407.06866v2,ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context,"While the biases of language models in production are extensively documented,
the biases of their guardrails have been neglected. This paper studies how
contextual information about the user influences the likelihood of an LLM to
refuse to execute a request. By generating user biographies that offer
ideological and demographic information, we find a number of biases in
guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas
are more likely to trigger a refusal guardrail when requesting censored or
illegal information. Guardrails are also sycophantic, refusing to comply with
requests for a political position the user is likely to disagree with. We find
that certain identity groups and seemingly innocuous information, e.g., sports
fandom, can elicit changes in guardrail sensitivity similar to direct
statements of political ideology. For each demographic category and even for
American football team fandom, we find that ChatGPT appears to infer a likely
political ideology and modify guardrail behavior accordingly.","[{'name': 'Victoria R. Li'}, {'name': 'Yida Chen'}, {'name': 'Naomi Saphra'}]",2024-07-09T13:53:38Z
http://arxiv.org/abs/2407.06851v1,http://arxiv.org/abs/2407.06851v1,Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders,"Despite the impressive capabilities of Large Language Models (LLMs) in
various tasks, their vulnerability to unsafe prompts remains a critical issue.
These prompts can lead LLMs to generate responses on illegal or sensitive
topics, posing a significant threat to their safe and ethical use. Existing
approaches attempt to address this issue using classification models, but they
have several drawbacks. With the increasing complexity of unsafe prompts,
similarity search-based techniques that identify specific features of unsafe
prompts provide a more robust and effective solution to this evolving problem.
This paper investigates the potential of sentence encoders to distinguish safe
from unsafe prompts, and the ability to classify various unsafe prompts
according to a safety taxonomy. We introduce new pairwise datasets and the
Categorical Purity (CP) metric to measure this capability. Our findings reveal
both the effectiveness and limitations of existing sentence encoders, proposing
directions to improve sentence encoders to operate as more robust safety
detectors. Our code is available at https://github.com/JwdanielJung/Safe-Embed.","[{'name': 'Jinseok Kim'}, {'name': 'Jaewon Jung'}, {'name': 'Sangyeop Kim'}, {'name': 'Sohyung Park'}, {'name': 'Sungzoon Cho'}]",2024-07-09T13:35:54Z
http://arxiv.org/abs/2407.12856v1,http://arxiv.org/abs/2407.12856v1,AI AI Bias: Large Language Models Favor Their Own Generated Content,"Are large language models (LLMs) biased towards text generated by LLMs over
text authored by humans, leading to possible anti-human bias? Utilizing a
classical experimental design inspired by employment discrimination studies, we
tested widely-used LLMs, including GPT-3.5 and GPT4, in binary-choice
scenarios. These involved LLM-based agents selecting between products and
academic papers described either by humans or LLMs under identical conditions.
Our results show a consistent tendency for LLM-based AIs to prefer
LLM-generated content. This suggests the possibility of AI systems implicitly
discriminating against humans, giving AI agents an unfair advantage.","[{'name': 'Walter Laurito'}, {'name': 'Benjamin Davis'}, {'name': 'Peli Grietzer'}, {'name': 'Tomáš Gavenčiak'}, {'name': 'Ada Böhm'}, {'name': 'Jan Kulveit'}]",2024-07-09T13:15:14Z
http://arxiv.org/abs/2407.06800v2,http://arxiv.org/abs/2407.06800v2,Learn and Don't Forget: Adding a New Language to ASR Foundation Models,"Foundation ASR models often support many languages, e.g. 100 languages in
Whisper. However, there has been limited work on integrating an additional,
typically low-resource, language, while maintaining performance on the original
language set. Fine-tuning, while simple, may degrade the accuracy of the
original set. We compare three approaches that exploit adaptation parameters:
soft language code tuning, train only the language code; soft prompt tuning,
train prepended tokens; and LoRA where a small set of additional parameters are
optimised. Elastic Weight Consolidation (EWC) offers an alternative compromise
with the potential to maintain performance in specific target languages.
Results show that direct fine-tuning yields the best performance for the new
language but degrades existing language capabilities. EWC can address this
issue for specific languages. If only adaptation parameters are used, the
language capabilities are maintained but at the cost of performance in the new
language.","[{'name': 'Mengjie Qian'}, {'name': 'Siyuan Tang'}, {'name': 'Rao Ma'}, {'name': 'Kate M. Knill'}, {'name': 'Mark J. F. Gales'}]",2024-07-09T12:14:48Z
http://arxiv.org/abs/2407.06779v1,http://arxiv.org/abs/2407.06779v1,"Using Pretrained Large Language Model with Prompt Engineering to Answer
  Biomedical Questions","Our team participated in the BioASQ 2024 Task12b and Synergy tasks to build a
system that can answer biomedical questions by retrieving relevant articles and
snippets from the PubMed database and generating exact and ideal answers. We
propose a two-level information retrieval and question-answering system based
on pre-trained large language models (LLM), focused on LLM prompt engineering
and response post-processing. We construct prompts with in-context few-shot
examples and utilize post-processing techniques like resampling and malformed
response detection. We compare the performance of various pre-trained LLM
models on this challenge, including Mixtral, OpenAI GPT and Llama2. Our
best-performing system achieved 0.14 MAP score on document retrieval, 0.05 MAP
score on snippet retrieval, 0.96 F1 score for yes/no questions, 0.38 MRR score
for factoid questions and 0.50 F1 score for list questions in Task 12b.","[{'name': 'Wenxin Zhou'}, {'name': 'Thuy Hang Ngo'}]",2024-07-09T11:48:49Z
http://arxiv.org/abs/2407.12855v1,http://arxiv.org/abs/2407.12855v1,"Large Language Models can impersonate politicians and other public
  figures","Modern AI technology like Large language models (LLMs) has the potential to
pollute the public information sphere with made-up content, which poses a
significant threat to the cohesion of societies at large. A wide range of
research has shown that LLMs are capable of generating text of impressive
quality, including persuasive political speech, text with a pre-defined style,
and role-specific content. But there is a crucial gap in the literature: We
lack large-scale and systematic studies of how capable LLMs are in
impersonating political and societal representatives and how the general public
judges these impersonations in terms of authenticity, relevance and coherence.
We present the results of a study based on a cross-section of British society
that shows that LLMs are able to generate responses to debate questions that
were part of a broadcast political debate programme in the UK. The impersonated
responses are judged to be more authentic and relevant than the original
responses given by people who were impersonated. This shows two things: (1)
LLMs can be made to contribute meaningfully to the public political debate and
(2) there is a dire need to inform the general public of the potential harm
this can have on society.","[{'name': 'Steffen Herbold'}, {'name': 'Alexander Trautsch'}, {'name': 'Zlata Kikteva'}, {'name': 'Annette Hautli-Janisz'}]",2024-07-09T11:16:19Z
http://arxiv.org/abs/2407.17425v1,http://arxiv.org/abs/2407.17425v1,"Media Manipulations in the Coverage of Events of the Ukrainian
  Revolution of Dignity: Historical, Linguistic, and Psychological Approaches","This article examines the use of manipulation in the coverage of events of
the Ukrainian Revolution of Dignity in the mass media, namely in the content of
the online newspaper Ukrainian Truth (Ukrainska pravda), online newspaper High
Castle (Vysokyi Zamok), and online newspaper ZIK during the public protest,
namely during the Ukrainian Revolution of Dignity. Contents of these online
newspapers the historical, linguistic, and psychological approaches are used.
Also media manipulations in the coverage of events of the Ukrainian Revolution
of Dignity are studied. Internet resources that cover news are analyzed.
Current and most popular Internet resources are identified. The content of
online newspapers is analyzed and statistically processed. Internet content of
newspapers by the level of significance of data (very significant data,
significant data and insignificant data) is classified. The algorithm of
detection of the media manipulations in the highlighting the course of the
Ukrainian revolutions based on historical, linguistic, and psychological
approaches is designed. Methods of counteracting information attacks in online
newspapers are developed.","[{'name': 'Ivan Khoma'}, {'name': 'Solomia Fedushko'}, {'name': 'Zoryana Kunch'}]",2024-07-09T09:46:27Z
http://arxiv.org/abs/2407.06699v1,http://arxiv.org/abs/2407.06699v1,Consistent Document-Level Relation Extraction via Counterfactuals,"Many datasets have been developed to train and evaluate document-level
relation extraction (RE) models. Most of these are constructed using real-world
data. It has been shown that RE models trained on real-world data suffer from
factual biases. To evaluate and address this issue, we present CovEReD, a
counterfactual data generation approach for document-level relation extraction
datasets using entity replacement. We first demonstrate that models trained on
factual data exhibit inconsistent behavior: while they accurately extract
triples from factual data, they fail to extract the same triples after
counterfactual modification. This inconsistency suggests that models trained on
factual data rely on spurious signals such as specific entities and external
knowledge $\unicode{x2013}$ rather than on the input context $\unicode{x2013}$
to extract triples. We show that by generating document-level counterfactual
data with CovEReD and training models on them, consistency is maintained with
minimal impact on RE performance. We release our CovEReD pipeline as well as
Re-DocRED-CF, a dataset of counterfactual RE documents, to assist in evaluating
and addressing inconsistency in document-level RE.","[{'name': 'Ali Modarressi'}, {'name': 'Abdullatif Köksal'}, {'name': 'Hinrich Schütze'}]",2024-07-09T09:21:55Z
http://arxiv.org/abs/2407.06677v1,http://arxiv.org/abs/2407.06677v1,"Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of
  Modules","Is it always necessary to compute tokens from shallow to deep layers in
Transformers? The continued success of vanilla Transformers and their variants
suggests an undoubted ""yes"". In this work, however, we attempt to break the
depth-ordered convention by proposing a novel architecture dubbed
mixture-of-modules (MoM), which is motivated by an intuition that any layer,
regardless of its position, can be used to compute a token as long as it
possesses the needed processing capabilities. The construction of MoM starts
from a finite set of modules defined by multi-head attention and feed-forward
networks, each distinguished by its unique parameterization. Two routers then
iteratively select attention modules and feed-forward modules from the set to
process a token. The selection dynamically expands the computation graph in the
forward pass of the token, culminating in an assembly of modules. We show that
MoM provides not only a unified framework for Transformers and their numerous
variants but also a flexible and learnable approach for reducing redundancy in
Transformer parameterization. We pre-train various MoMs using OpenWebText.
Empirical results demonstrate that MoMs, of different parameter counts,
consistently outperform vanilla transformers on both GLUE and XSUM benchmarks.
More interestingly, with a fixed parameter budget, MoM-large enables an over
38% increase in depth for computation graphs compared to GPT-2-large, resulting
in absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large
also enables an over 60% reduction in depth while involving more modules per
layer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage
compared to GPT-2-large, while maintaining comparable performance.","[{'name': 'Zhuocheng Gong'}, {'name': 'Ang Lv'}, {'name': 'Jian Guan'}, {'name': 'Junxi Yan'}, {'name': 'Wei Wu'}, {'name': 'Huishuai Zhang'}, {'name': 'Minlie Huang'}, {'name': 'Dongyan Zhao'}, {'name': 'Rui Yan'}]",2024-07-09T08:50:18Z
http://arxiv.org/abs/2407.12854v1,http://arxiv.org/abs/2407.12854v1,Scaling Retrieval-Based Language Models with a Trillion-Token Datastore,"Scaling laws with respect to the amount of training data and the number of
parameters allow us to predict the cost-benefit trade-offs of pretraining
language models (LMs) in different configurations. In this paper, we consider
another dimension of scaling: the amount of data available at inference time.
Specifically, we find that increasing the size of the datastore used by a
retrieval-based LM monotonically improves language modeling and several
downstream tasks without obvious saturation, such that a smaller model
augmented with a large datastore outperforms a larger LM-only model on
knowledge-intensive tasks. By plotting compute-optimal scaling curves with
varied datastore, model, and pretraining data sizes, we show that using larger
datastores can significantly improve model performance for the same training
compute budget. We carry out our study by constructing a 1.4 trillion-token
datastore named MassiveDS, which is the largest and the most diverse
open-sourced datastore for retrieval-based LMs to date, and designing an
efficient pipeline for studying datastore scaling in a computationally
accessible manner. Finally, we analyze the effect of improving the retriever,
datastore quality filtering, and other design choices on our observed scaling
trends. Overall, our results show that datastore size should be considered as
an integral part of LM efficiency and performance trade-offs. To facilitate
future research, we open-source our datastore and code at
https://github.com/RulinShao/retrieval-scaling.","[{'name': 'Rulin Shao'}, {'name': 'Jacqueline He'}, {'name': 'Akari Asai'}, {'name': 'Weijia Shi'}, {'name': 'Tim Dettmers'}, {'name': 'Sewon Min'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Pang Wei Koh'}]",2024-07-09T08:27:27Z
http://arxiv.org/abs/2407.06654v1,http://arxiv.org/abs/2407.06654v1,"SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language
  Model Pre-training","The effectiveness of large language models (LLMs) is often hindered by
duplicated data in their extensive pre-training datasets. Current approaches
primarily focus on detecting and removing duplicates, which risks the loss of
valuable information and neglects the varying degrees of duplication. To
address this, we propose a soft deduplication method that maintains dataset
integrity while selectively reducing the sampling weight of data with high
commonness. Central to our approach is the concept of ""data commonness"", a
metric we introduce to quantify the degree of duplication by measuring the
occurrence probabilities of samples using an n-gram model. Empirical analysis
shows that this method significantly improves training efficiency, achieving
comparable perplexity scores with at least a 26% reduction in required training
steps. Additionally, it enhances average few-shot downstream accuracy by 1.77%
when trained for an equivalent duration. Importantly, this approach
consistently improves performance, even on rigorously deduplicated datasets,
indicating its potential to complement existing methods and become a standard
pre-training process for LLMs.","[{'name': 'Nan He'}, {'name': 'Weichen Xiong'}, {'name': 'Hanwen Liu'}, {'name': 'Yi Liao'}, {'name': 'Lei Ding'}, {'name': 'Kai Zhang'}, {'name': 'Guohua Tang'}, {'name': 'Xiao Han'}, {'name': 'Wei Yang'}]",2024-07-09T08:26:39Z
http://arxiv.org/abs/2407.06650v1,http://arxiv.org/abs/2407.06650v1,"A Word Order Synchronization Metric for Evaluating Simultaneous
  Interpretation and Translation","Simultaneous interpretation (SI), the translation of one language to another
in real time, starts translation before the original speech has finished. Its
evaluation needs to consider both latency and quality. This trade-off is
challenging especially for distant word order language pairs such as English
and Japanese. To handle this word order gap, interpreters maintain the word
order of the source language as much as possible to keep up with original
language to minimize its latency while maintaining its quality, whereas in
translation reordering happens to keep fluency in the target language. This
means outputs synchronized with the source language are desirable based on the
real SI situation, and it's a key for further progress in computational SI and
simultaneous machine translation (SiMT). In this work, we propose an automatic
evaluation metric for SI and SiMT focusing on word order synchronization. Our
evaluation metric is based on rank correlation coefficients, leveraging
cross-lingual pre-trained language models. Our experimental results on
NAIST-SIC-Aligned and JNPC showed our metrics' effectiveness to measure word
order synchronization between source and target language.","[{'name': 'Mana Makinae'}, {'name': 'Katsuhito Sudoh'}, {'name': 'Mararu Yamada'}, {'name': 'Satoshi Nakamura'}]",2024-07-09T08:21:40Z
http://arxiv.org/abs/2407.06645v3,http://arxiv.org/abs/2407.06645v3,Entropy Law: The Story Behind Data Compression and LLM Performance,"Data is the cornerstone of large language models (LLMs), but not all data is
useful for model learning. Carefully selected data can better elicit the
capabilities of LLMs with much less computational overhead. Most methods
concentrate on evaluating the quality of individual samples in data selection,
while the combinatorial effects among samples are neglected. Even if each
sample is of perfect quality, their combinations may be suboptimal in teaching
LLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim
to uncover the underlying relationships between LLM performance and data
selection. Inspired by the information compression nature of LLMs, we uncover
an ``entropy law'' that connects LLM performance with data compression ratio
and first-epoch training loss, which reflect the information redundancy of a
dataset and the mastery of inherent knowledge encoded in this dataset,
respectively. Through both theoretical deduction and empirical evaluation, we
find that model performance is negatively correlated to the compression ratio
of training data, which usually yields a lower training loss. Based on the
findings of the entropy law, we propose a quite efficient and universal data
selection method named \textbf{ZIP} for training LLMs, which aim to prioritize
data subsets exhibiting a low compression ratio. Based on a multi-stage
algorithm that selects diverse data in a greedy manner, we can obtain a good
data subset with satisfactory diversity. Extensive experiments have been
conducted to validate the entropy law and the superiority of ZIP across
different LLM backbones and alignment stages. We also present an interesting
application of entropy law that can detect potential performance risks at the
beginning of model training.","[{'name': 'Mingjia Yin'}, {'name': 'Chuhan Wu'}, {'name': 'Yufei Wang'}, {'name': 'Hao Wang'}, {'name': 'Wei Guo'}, {'name': 'Yasheng Wang'}, {'name': 'Yong Liu'}, {'name': 'Ruiming Tang'}, {'name': 'Defu Lian'}, {'name': 'Enhong Chen'}]",2024-07-09T08:14:29Z
http://arxiv.org/abs/2407.06606v1,http://arxiv.org/abs/2407.06606v1,"Tailored Design of Audio-Visual Speech Recognition Models using
  Branchformers","Recent advances in Audio-Visual Speech Recognition (AVSR) have led to
unprecedented achievements in the field, improving the robustness of this type
of system in adverse, noisy environments. In most cases, this task has been
addressed through the design of models composed of two independent encoders,
each dedicated to a specific modality. However, while recent works have
explored unified audio-visual encoders, determining the optimal cross-modal
architecture remains an ongoing challenge. Furthermore, such approaches often
rely on models comprising vast amounts of parameters and high computational
cost training processes. In this paper, we aim to bridge this research gap by
introducing a novel audio-visual framework. Our proposed method constitutes, to
the best of our knowledge, the first attempt to harness the flexibility and
interpretability offered by encoder architectures, such as the Branchformer, in
the design of parameter-efficient AVSR systems. To be more precise, the
proposed framework consists of two steps: first, estimating audio- and
video-only systems, and then designing a tailored audio-visual unified encoder
based on the layer-level branch scores provided by the modality-specific
models. Extensive experiments on English and Spanish AVSR benchmarks covering
multiple data conditions and scenarios demonstrated the effectiveness of our
proposed method. Results reflect how our tailored AVSR system is able to reach
state-of-the-art recognition rates while significantly reducing the model
complexity w.r.t. the prevalent approach in the field. Code and pre-trained
models are available at https://github.com/david-gimeno/tailored-avsr.","[{'name': 'David Gimeno-Gómez'}, {'name': 'Carlos-D. Martínez-Hinarejos'}]",2024-07-09T07:15:56Z
http://arxiv.org/abs/2407.07924v1,http://arxiv.org/abs/2407.07924v1,"Solving General Natural-Language-Description Optimization Problems with
  Large Language Models","Optimization problems seek to find the best solution to an objective under a
set of constraints, and have been widely investigated in real-world
applications. Modeling and solving optimization problems in a specific domain
typically require a combination of domain knowledge, mathematical skills, and
programming ability, making it difficult for general users and even domain
professionals. In this paper, we propose a novel framework called OptLLM that
augments LLMs with external solvers. Specifically, OptLLM accepts user queries
in natural language, convert them into mathematical formulations and
programming codes, and calls the solvers to calculate the results for
decision-making. In addition, OptLLM supports multi-round dialogues to
gradually refine the modeling and solving of optimization problems. To
illustrate the effectiveness of OptLLM, we provide tutorials on three typical
optimization applications and conduct experiments on both prompt-based GPT
models and a fine-tuned Qwen model using a large-scale selfdeveloped
optimization dataset. Experimental results show that OptLLM works with various
LLMs, and the fine-tuned model achieves an accuracy boost compared to the
promptbased models. Some features of OptLLM framework have been available for
trial since June 2023 (https://opt.alibabacloud.com/chat or
https://opt.aliyun.com/chat).","[{'name': 'Jihai Zhang'}, {'name': 'Wei Wang'}, {'name': 'Siyan Guo'}, {'name': 'Li Wang'}, {'name': 'Fangquan Lin'}, {'name': 'Cheng Yang'}, {'name': 'Wotao Yin'}]",2024-07-09T07:11:10Z
http://arxiv.org/abs/2407.18920v1,http://arxiv.org/abs/2407.18920v1,Optimising Hard Prompts with Few-Shot Meta-Prompting,"Prompting is a flexible and adaptable way of providing instructions to a
Large Language Model (LLM). Contextual prompts include context in the form of a
document or dialogue along with the natural language instructions to the LLM,
often constraining the LLM to restrict facts to that of the given context while
complying with the instructions. Masking the context, it acts as template for
prompts. In this paper, we present an iterative method to generate better
templates using an LLM from an existing set of prompt templates without
revealing the context to the LLM. Multiple methods of optimising prompts using
the LLM itself are explored to check the effect of few shot sampling methods on
iterative propagation while maintaining linguistic styles and syntax on
optimisation of prompt templates, yielding a 103.87% improvement using the best
performing method. Comparison of the results of multiple contextual tasks
demonstrate the ability of LLMs to maintain syntax while learning to replicate
linguistic styles. Additionally, the effect on the output with different
methods of prompt template generation is shown.",[{'name': 'Sayash Raaj Hiraou'}],2024-07-09T07:02:57Z
http://arxiv.org/abs/2407.06579v1,http://arxiv.org/abs/2407.06579v1,"NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in
  Text Classification","Existing research on learning with noisy labels predominantly focuses on
synthetic label noise. Although synthetic noise possesses well-defined
structural properties, it often fails to accurately replicate real-world noise
patterns. In recent years, there has been a concerted effort to construct
generalizable and controllable instance-dependent noise datasets for image
classification, significantly advancing the development of noise-robust
learning in this area. However, studies on noisy label learning for text
classification remain scarce. To better understand label noise in real-world
text classification settings, we constructed the benchmark dataset NoisyAG-News
through manual annotation. Initially, we analyzed the annotated data to gather
observations about real-world noise. We qualitatively and quantitatively
demonstrated that real-world noisy labels adhere to instance-dependent
patterns. Subsequently, we conducted comprehensive learning experiments on
NoisyAG-News and its corresponding synthetic noise datasets using pre-trained
language models and noise-handling techniques. Our findings reveal that while
pre-trained models are resilient to synthetic noise, they struggle against
instance-dependent noise, with samples of varying confusion levels showing
inconsistent performance during training and testing. These real-world noise
patterns pose new, significant challenges, prompting a reevaluation of noisy
label handling methods. We hope that NoisyAG-News will facilitate the
development and evaluation of future solutions for learning with noisy labels.","[{'name': 'Hongfei Huang'}, {'name': 'Tingting Liang'}, {'name': 'Xixi Sun'}, {'name': 'Zikang Jin'}, {'name': 'Yuyu Yin'}]",2024-07-09T06:18:40Z
http://arxiv.org/abs/2407.06576v1,http://arxiv.org/abs/2407.06576v1,Virtual Personas for Language Models via an Anthology of Backstories,"Large language models (LLMs) are trained from vast repositories of text
authored by millions of distinct authors, reflecting an enormous diversity of
human traits. While these models bear the potential to be used as
approximations of human subjects in behavioral studies, prior efforts have been
limited in steering model responses to match individual human users. In this
work, we introduce ""Anthology"", a method for conditioning LLMs to particular
virtual personas by harnessing open-ended life narratives, which we refer to as
""backstories."" We show that our methodology enhances the consistency and
reliability of experimental outcomes while ensuring better representation of
diverse sub-populations. Across three nationally representative human surveys
conducted as part of Pew Research Center's American Trends Panel (ATP), we
demonstrate that Anthology achieves up to 18% improvement in matching the
response distributions of human respondents and 27% improvement in consistency
metrics. Our code and generated backstories are available at
https://github.com/CannyLab/anthology.","[{'name': 'Suhong Moon'}, {'name': 'Marwa Abdulhai'}, {'name': 'Minwoo Kang'}, {'name': 'Joseph Suh'}, {'name': 'Widyadewi Soedarmadji'}, {'name': 'Eran Kohen Behar'}, {'name': 'David M. Chan'}]",2024-07-09T06:11:18Z
http://arxiv.org/abs/2407.06567v2,http://arxiv.org/abs/2407.06567v2,"FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal
  Reinforcement for Enhanced Financial Decision Making","Large language models (LLMs) have demonstrated notable potential in
conducting complex tasks and are increasingly utilized in various financial
applications. However, high-quality sequential financial investment
decision-making remains challenging. These tasks require multiple interactions
with a volatile environment for every decision, demanding sufficient
intelligence to maximize returns and manage risks. Although LLMs have been used
to develop agent systems that surpass human teams and yield impressive
investment returns, opportunities to enhance multi-sourced information
synthesis and optimize decision-making outcomes through timely experience
refinement remain unexplored. Here, we introduce the FinCon, an LLM-based
multi-agent framework with CONceptual verbal reinforcement tailored for diverse
FINancial tasks. Inspired by effective real-world investment firm
organizational structures, FinCon utilizes a manager-analyst communication
hierarchy. This structure allows for synchronized cross-functional agent
collaboration towards unified goals through natural language interactions and
equips each agent with greater memory capacity than humans. Additionally, a
risk-control component in FinCon enhances decision quality by episodically
initiating a self-critiquing mechanism to update systematic investment beliefs.
The conceptualized beliefs serve as verbal reinforcement for the future agent's
behavior and can be selectively propagated to the appropriate node that
requires knowledge updates. This feature significantly improves performance
while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon
demonstrates strong generalization capabilities in various financial tasks,
including single stock trading and portfolio management.","[{'name': 'Yangyang Yu'}, {'name': 'Zhiyuan Yao'}, {'name': 'Haohang Li'}, {'name': 'Zhiyang Deng'}, {'name': 'Yupeng Cao'}, {'name': 'Zhi Chen'}, {'name': 'Jordan W. Suchow'}, {'name': 'Rong Liu'}, {'name': 'Zhenyu Cui'}, {'name': 'Denghui Zhang'}, {'name': 'Koduvayur Subbalakshmi'}, {'name': 'Guojun Xiong'}, {'name': 'Yueru He'}, {'name': 'Jimin Huang'}, {'name': 'Dong Li'}, {'name': 'Qianqian Xie'}]",2024-07-09T05:52:26Z
http://arxiv.org/abs/2407.06564v1,http://arxiv.org/abs/2407.06564v1,Combining Knowledge Graphs and Large Language Models,"In recent years, Natural Language Processing (NLP) has played a significant
role in various Artificial Intelligence (AI) applications such as chatbots,
text generation, and language translation. The emergence of large language
models (LLMs) has greatly improved the performance of these applications,
showing astonishing results in language understanding and generation. However,
they still show some disadvantages, such as hallucinations and lack of
domain-specific knowledge, that affect their performance in real-world tasks.
These issues can be effectively mitigated by incorporating knowledge graphs
(KGs), which organise information in structured formats that capture
relationships between entities in a versatile and interpretable fashion.
Likewise, the construction and validation of KGs present challenges that LLMs
can help resolve. The complementary relationship between LLMs and KGs has led
to a trend that combines these technologies to achieve trustworthy results.
This work collected 28 papers outlining methods for KG-powered LLMs, LLM-based
KGs, and LLM-KG hybrid approaches. We systematically analysed and compared
these approaches to provide a comprehensive overview highlighting key trends,
innovative techniques, and common challenges. This synthesis will benefit
researchers new to the field and those seeking to deepen their understanding of
how KGs and LLMs can be effectively combined to enhance AI applications
capabilities.","[{'name': 'Amanda Kau'}, {'name': 'Xuzeng He'}, {'name': 'Aishwarya Nambissan'}, {'name': 'Aland Astudillo'}, {'name': 'Hui Yin'}, {'name': 'Amir Aryani'}]",2024-07-09T05:42:53Z
http://arxiv.org/abs/2407.06551v1,http://arxiv.org/abs/2407.06551v1,OffsetBias: Leveraging Debiased Data for Tuning Evaluators,"Employing Large Language Models (LLMs) to assess the quality of generated
responses, such as prompting instruct-tuned models or fine-tuning judge models,
has become a widely adopted evaluation method. It is also known that such
evaluators are vulnerable to biases, such as favoring longer responses. While
it is important to overcome this problem, the specifics of these biases remain
under-explored. In this work, we qualitatively identify six types of biases
inherent in various judge models. We propose EvalBiasBench as a meta-evaluation
collection of hand-crafted test cases for each bias type. Additionally, we
present de-biasing dataset construction methods and the associated preference
dataset OffsetBias. Experimental results demonstrate that fine-tuning on our
dataset significantly enhances the robustness of judge models against biases
and improves performance across most evaluation scenarios. We release our
datasets and the fine-tuned judge model to public.","[{'name': 'Junsoo Park'}, {'name': 'Seungyeon Jwa'}, {'name': 'Meiying Ren'}, {'name': 'Daeyoung Kim'}, {'name': 'Sanghyuk Choi'}]",2024-07-09T05:16:22Z
http://arxiv.org/abs/2407.06549v1,http://arxiv.org/abs/2407.06549v1,"AutoTask: Task Aware Multi-Faceted Single Model for Multi-Task Ads
  Relevance","Ads relevance models are crucial in determining the relevance between user
search queries and ad offers, often framed as a classification problem. The
complexity of modeling increases significantly with multiple ad types and
varying scenarios that exhibit both similarities and differences. In this work,
we introduce a novel multi-faceted attention model that performs task aware
feature combination and cross task interaction modeling. Our technique
formulates the feature combination problem as ""language"" modeling with
auto-regressive attentions across both feature and task dimensions.
Specifically, we introduce a new dimension of task ID encoding for task
representations, thereby enabling precise relevance modeling across diverse ad
scenarios with substantial improvement in generality capability for unseen
tasks. We demonstrate that our model not only effectively handles the increased
computational and maintenance demands as scenarios proliferate, but also
outperforms generalized DNN models and even task-specific models across a
spectrum of ad applications using a single unified model.","[{'name': 'Shouchang Guo'}, {'name': 'Sonam Damani'}, {'name': 'Keng-hao Chang'}]",2024-07-09T05:13:45Z
http://arxiv.org/abs/2407.06538v1,http://arxiv.org/abs/2407.06538v1,"Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge
  Distillation: A Case Study","Neural Machine Translation (NMT) remains a formidable challenge, especially
when dealing with low-resource languages. Pre-trained sequence-to-sequence
(seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive
performance in various low-resource NMT tasks. However, their pre-training has
been confined to 50 languages, leaving out support for numerous low-resource
languages, particularly those spoken in the Indian subcontinent. Expanding
mBART-50's language support requires complex pre-training, risking performance
decline due to catastrophic forgetting. Considering these expanding challenges,
this paper explores a framework that leverages the benefits of a pre-trained
language model along with knowledge distillation in a seq2seq architecture to
facilitate translation for low-resource languages, including those not covered
by mBART-50. The proposed framework employs a multilingual encoder-based
seq2seq model as the foundational architecture and subsequently uses
complementary knowledge distillation techniques to mitigate the impact of
imbalanced training. Our framework is evaluated on three low-resource Indic
languages in four Indic-to-Indic directions, yielding significant BLEU-4 and
chrF improvements over baselines. Further, we conduct human evaluation to
confirm effectiveness of our approach. Our code is publicly available at
https://github.com/raypretam/Two-step-low-res-NMT.","[{'name': 'Aniruddha Roy'}, {'name': 'Pretam Ray'}, {'name': 'Ayush Maheshwari'}, {'name': 'Sudeshna Sarkar'}, {'name': 'Pawan Goyal'}]",2024-07-09T04:19:52Z
http://arxiv.org/abs/2407.06537v1,http://arxiv.org/abs/2407.06537v1,"Efficient and Accurate Memorable Conversation Model using DPO based on
  sLLM","In multi-session dialog system, it is essential to continuously update the
memory as the session progresses. Simply accumulating memory can make it
difficult to focus on the content of the conversation for inference due to the
limited input sentence size. Therefore, efficient and accurate conversation
model that is capable of managing memory to reflect the conversation history
continuously is necessary. This paper presents a conversation model that
efficiently manages memory as sessions progress and incorporates this into the
model to reflect the conversation history accurately with 3 methodologies: SFT,
DPO and DPO with SFT model. Our model using DPO algorithm shows an improvement
about 0.0591 of BERTScore in memory accuracy, and the rate of responses
reflecting the memory increased as well. Also, response generation performance
enhanced about 4.292 in fluency, 3.935 in coherence, and 2.896 in consistency.
This paper describes a training method that yields better performance than
models with more than twice the parameter size, even when the model size is
smaller. Thus, our model demonstrates efficiency not only in terms of accuracy
but also in resource utilization.","[{'name': 'Youngkyung Seo'}, {'name': 'Yoonseok Heo'}, {'name': 'Jun-Seok Koh'}, {'name': 'Du-Seoung Chang'}]",2024-07-09T04:17:39Z
http://arxiv.org/abs/2407.06533v1,http://arxiv.org/abs/2407.06533v1,LETS-C: Leveraging Language Embedding for Time Series Classification,"Recent advancements in language modeling have shown promising results when
applied to time series data. In particular, fine-tuning pre-trained large
language models (LLMs) for time series classification tasks has achieved
state-of-the-art (SOTA) performance on standard benchmarks. However, these
LLM-based models have a significant drawback due to the large model size, with
the number of trainable parameters in the millions. In this paper, we propose
an alternative approach to leveraging the success of language modeling in the
time series domain. Instead of fine-tuning LLMs, we utilize a language
embedding model to embed time series and then pair the embeddings with a simple
classification head composed of convolutional neural networks (CNN) and
multilayer perceptron (MLP). We conducted extensive experiments on
well-established time series classification benchmark datasets. We demonstrated
LETS-C not only outperforms the current SOTA in classification accuracy but
also offers a lightweight solution, using only 14.5% of the trainable
parameters on average compared to the SOTA model. Our findings suggest that
leveraging language encoders to embed time series data, combined with a simple
yet effective classification head, offers a promising direction for achieving
high-performance time series classification while maintaining a lightweight
model architecture.","[{'name': 'Rachneet Kaur'}, {'name': 'Zhen Zeng'}, {'name': 'Tucker Balch'}, {'name': 'Manuela Veloso'}]",2024-07-09T04:07:57Z
http://arxiv.org/abs/2407.06501v1,http://arxiv.org/abs/2407.06501v1,STORYSUMM: Evaluating Faithfulness in Story Summarization,"Human evaluation has been the gold standard for checking faithfulness in
abstractive summarization. However, with a challenging source domain like
narrative, multiple annotators can agree a summary is faithful, while missing
details that are obvious errors only once pointed out. We therefore introduce a
new dataset, STORYSUMM, comprising LLM summaries of short stories with
localized faithfulness labels and error explanations. This benchmark is for
evaluation methods, testing whether a given method can detect challenging
inconsistencies. Using this dataset, we first show that any one human
annotation protocol is likely to miss inconsistencies, and we advocate for
pursuing a range of methods when establishing ground truth for a summarization
dataset. We finally test recent automatic metrics and find that none of them
achieve more than 70% balanced accuracy on this task, demonstrating that it is
a challenging benchmark for future work in faithfulness evaluation.","[{'name': 'Melanie Subbiah'}, {'name': 'Faisal Ladhak'}, {'name': 'Akankshya Mishra'}, {'name': 'Griffin Adams'}, {'name': 'Lydia B. Chilton'}, {'name': 'Kathleen McKeown'}]",2024-07-09T02:06:30Z
http://arxiv.org/abs/2407.12853v1,http://arxiv.org/abs/2407.12853v1,"Automated Justification Production for Claim Veracity in Fact Checking:
  A Survey on Architectures and Approaches","Automated Fact-Checking (AFC) is the automated verification of claim
accuracy. AFC is crucial in discerning truth from misinformation, especially
given the huge amounts of content are generated online daily. Current research
focuses on predicting claim veracity through metadata analysis and language
scrutiny, with an emphasis on justifying verdicts. This paper surveys recent
methodologies, proposing a comprehensive taxonomy and presenting the evolution
of research in that landscape. A comparative analysis of methodologies and
future directions for improving fact-checking explainability are also
discussed.","[{'name': 'Islam Eldifrawi'}, {'name': 'Shengrui Wang'}, {'name': 'Amine Trabelsi'}]",2024-07-09T01:54:13Z
http://arxiv.org/abs/2407.06488v1,http://arxiv.org/abs/2407.06488v1,"Towards Understanding Multi-Task Learning (Generalization) of LLMs via
  Detecting and Exploring Task-Specific Neurons","While large language models (LLMs) have demonstrated superior multi-task
capabilities, understanding the learning mechanisms behind this is still a
challenging problem. In this paper, we attempt to understand such mechanisms
from the perspective of neurons. Specifically, we detect task-sensitive neurons
in LLMs via gradient attribution on task-specific data. Through extensive
deactivation and fine-tuning experiments, we demonstrate that the detected
neurons are highly correlated with the given task, which we term as
task-specific neurons. With these identified task-specific neurons, we delve
into two common problems in multi-task learning and continuous learning:
Generalization and Catastrophic Forgetting. We find that the overlap of
task-specific neurons is strongly associated with generalization and
specialization across tasks. Interestingly, at certain layers of LLMs, there is
a high similarity in the parameters of different task-specific neurons, and
such similarity is highly correlated with the generalization performance.
Inspired by these findings, we propose a neuron-level continuous fine-tuning
method that only fine-tunes the current task-specific neurons during continuous
learning, and extensive experiments demonstrate the effectiveness of the
proposed method. Our study provides insights into the interpretability of LLMs
in multi-task learning.","[{'name': 'Yongqi Leng'}, {'name': 'Deyi Xiong'}]",2024-07-09T01:27:35Z
http://arxiv.org/abs/2407.06483v1,http://arxiv.org/abs/2407.06483v1,Composable Interventions for Language Models,"Test-time interventions for language models can enhance factual accuracy,
mitigate harmful outputs, and improve model efficiency without costly
retraining. But despite a flood of new methods, different types of
interventions are largely developing independently. In practice, multiple
interventions must be applied sequentially to the same model, yet we lack
standardized ways to study how interventions interact. We fill this gap by
introducing composable interventions, a framework to study the effects of using
multiple interventions on the same language models, featuring new metrics and a
unified codebase. Using our framework, we conduct extensive experiments and
compose popular methods from three emerging intervention categories --
Knowledge Editing, Model Compression, and Machine Unlearning. Our results from
310 different compositions uncover meaningful interactions: compression hinders
editing and unlearning, composing interventions hinges on their order of
application, and popular general-purpose metrics are inadequate for assessing
composability. Taken together, our findings showcase clear gaps in
composability, suggesting a need for new multi-objective interventions. All of
our code is public:
https://github.com/hartvigsen-group/composable-interventions.","[{'name': 'Arinbjorn Kolbeinsson'}, {'name': ""Kyle O'Brien""}, {'name': 'Tianjin Huang'}, {'name': 'Shanghua Gao'}, {'name': 'Shiwei Liu'}, {'name': 'Jonathan Richard Schwarz'}, {'name': 'Anurag Vaidya'}, {'name': 'Faisal Mahmood'}, {'name': 'Marinka Zitnik'}, {'name': 'Tianlong Chen'}, {'name': 'Thomas Hartvigsen'}]",2024-07-09T01:17:44Z
http://arxiv.org/abs/2407.06479v1,http://arxiv.org/abs/2407.06479v1,"Interaction Matters: An Evaluation Framework for Interactive Dialogue
  Assessment on English Second Language Conversations","We present an evaluation framework for interactive dialogue assessment in the
context of English as a Second Language (ESL) speakers. Our framework collects
dialogue-level interactivity labels (e.g., topic management; 4 labels in total)
and micro-level span features (e.g., backchannels; 17 features in total). Given
our annotated data, we study how the micro-level features influence the (higher
level) interactivity quality of ESL dialogues by constructing various machine
learning-based models. Our results demonstrate that certain micro-level
features strongly correlate with interactivity quality, like reference word
(e.g., she, her, he), revealing new insights about the interaction between
higher-level dialogue quality and lower-level linguistic signals. Our framework
also provides a means to assess ESL communication, which is useful for language
assessment.","[{'name': 'Rena Gao'}, {'name': 'Carsten Roever'}, {'name': 'Jey Han Lau'}]",2024-07-09T00:56:59Z
http://arxiv.org/abs/2407.06460v2,http://arxiv.org/abs/2407.06460v2,MUSE: Machine Unlearning Six-Way Evaluation for Language Models,"Language models (LMs) are trained on vast amounts of text data, which may
include private and copyrighted content. Data owners may request the removal of
their data from a trained model due to privacy or copyright concerns. However,
exactly unlearning only these datapoints (i.e., retraining with the data
removed) is intractable in modern-day models. This has led to the development
of many approximate unlearning algorithms. The evaluation of the efficacy of
these algorithms has traditionally been narrow in scope, failing to precisely
quantify the success and practicality of the algorithm from the perspectives of
both the model deployers and the data owners. We address this issue by
proposing MUSE, a comprehensive machine unlearning evaluation benchmark that
enumerates six diverse desirable properties for unlearned models: (1) no
verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage,
(4) utility preservation on data not intended for removal, (5) scalability with
respect to the size of removal requests, and (6) sustainability over sequential
unlearning requests. Using these criteria, we benchmark how effectively eight
popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter
books and news articles. Our results demonstrate that most algorithms can
prevent verbatim memorization and knowledge memorization to varying degrees,
but only one algorithm does not lead to severe privacy leakage. Furthermore,
existing algorithms fail to meet deployer's expectations because they often
degrade general model utility and also cannot sustainably accommodate
successive unlearning requests or large-scale content removal. Our findings
identify key issues with the practicality of existing unlearning algorithms on
language models, and we release our benchmark to facilitate further
evaluations: muse-bench.github.io","[{'name': 'Weijia Shi'}, {'name': 'Jaechan Lee'}, {'name': 'Yangsibo Huang'}, {'name': 'Sadhika Malladi'}, {'name': 'Jieyu Zhao'}, {'name': 'Ari Holtzman'}, {'name': 'Daogao Liu'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Noah A. Smith'}, {'name': 'Chiyuan Zhang'}]",2024-07-08T23:47:29Z
http://arxiv.org/abs/2407.06438v1,http://arxiv.org/abs/2407.06438v1,A Single Transformer for Scalable Vision-Language Modeling,"We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.
Current large vision-language models (LVLMs) such as LLaVA mostly employ
heterogeneous architectures that connect pre-trained visual encoders with large
language models (LLMs) to facilitate visual recognition and complex reasoning.
Although achieving remarkable performance with relatively lightweight training,
we identify four primary scalability limitations: (1) The visual capacity is
constrained by pre-trained visual encoders, which are typically an order of
magnitude smaller than LLMs. (2) The heterogeneous architecture complicates the
use of established hardware and software infrastructure. (3) Study of scaling
laws on such architecture must consider three separate components - visual
encoder, connector, and LLMs, which complicates the analysis. (4) The use of
existing visual encoders typically requires following a pre-defined
specification of image inputs pre-processing, for example, by reshaping inputs
to fixed-resolution square images, which presents difficulties in processing
and training on high-resolution images or those with unusual aspect ratio. A
unified single Transformer architecture, like SOLO, effectively addresses these
scalability concerns in LVLMs; however, its limited adoption in the modern
context likely stems from the absence of reliable training recipes that balance
both modalities and ensure stable training for billion-scale models. In this
paper, we introduce the first open-source training recipe for developing SOLO,
an open-source 7B LVLM using moderate academic resources. The training recipe
involves initializing from LLMs, sequential pre-training on ImageNet and
web-scale data, and instruction fine-tuning on our curated high-quality
datasets. On extensive evaluation, SOLO demonstrates performance comparable to
LLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.","[{'name': 'Yangyi Chen'}, {'name': 'Xingyao Wang'}, {'name': 'Hao Peng'}, {'name': 'Heng Ji'}]",2024-07-08T22:40:15Z
http://arxiv.org/abs/2407.06432v1,http://arxiv.org/abs/2407.06432v1,"An Empirical Study of Gendered Stereotypes in Emotional Attributes for
  Bangla in Multilingual Large Language Models","The influence of Large Language Models (LLMs) is rapidly growing, automating
more jobs over time. Assessing the fairness of LLMs is crucial due to their
expanding impact. Studies reveal the reflection of societal norms and biases in
LLMs, which creates a risk of propagating societal stereotypes in downstream
tasks. Many studies on bias in LLMs focus on gender bias in various NLP
applications. However, there's a gap in research on bias in emotional
attributes, despite the close societal link between emotion and gender. This
gap is even larger for low-resource languages like Bangla. Historically, women
are associated with emotions like empathy, fear, and guilt, while men are
linked to anger, bravado, and authority. This pattern reflects societal norms
in Bangla-speaking regions. We offer the first thorough investigation of
gendered emotion attribution in Bangla for both closed and open source LLMs in
this work. Our aim is to elucidate the intricate societal relationship between
gender and emotion specifically within the context of Bangla. We have been
successful in showing the existence of gender bias in the context of emotions
in Bangla through analytical methods and also show how emotion attribution
changes on the basis of gendered role selection in LLMs. All of our resources
including code and data are made publicly available to support future research
on Bangla NLP.
  Warning: This paper contains explicit stereotypical statements that many may
find offensive.","[{'name': 'Jayanta Sadhu'}, {'name': 'Maneesha Rani Saha'}, {'name': 'Rifat Shahriyar'}]",2024-07-08T22:22:15Z
http://arxiv.org/abs/2407.06426v1,http://arxiv.org/abs/2407.06426v1,"DebUnc: Mitigating Hallucinations in Large Language Model Agent
  Communication with Uncertainty Estimations","To enhance Large Language Model (LLM) capabilities, multi-agent debates have
been introduced, where multiple LLMs discuss solutions to a problem over
several rounds of debate. However, LLMs often produce incorrect responses that
appear deceptively confident, which can mislead other agents. This is partly
because agents do not express their confidence levels during standard debates.
To address this, we introduce DebUnc, a multi-agent debate framework that uses
uncertainty metrics to assess agent confidence levels. We adapted the LLM
attention mechanism to adjust token weights based on confidence levels and also
explored using textual prompts to convey confidence. Our evaluations across
various benchmarks show that attention-based methods are particularly
effective, and that as uncertainty metrics evolve, performance will continue to
increase. The code is available at https://github.com/lukeyoffe/debunc","[{'name': 'Luke Yoffe'}, {'name': 'Alfonso Amayuelas'}, {'name': 'William Yang Wang'}]",2024-07-08T22:15:01Z
http://arxiv.org/abs/2407.06422v1,http://arxiv.org/abs/2407.06422v1,"Exploring the Capability of ChatGPT to Reproduce Human Labels for Social
  Computing Tasks (Extended Version)","Harnessing the potential of large language models (LLMs) like ChatGPT can
help address social challenges through inclusive, ethical, and sustainable
means. In this paper, we investigate the extent to which ChatGPT can annotate
data for social computing tasks, aiming to reduce the complexity and cost of
undertaking web research. To evaluate ChatGPT's potential, we re-annotate seven
datasets using ChatGPT, covering topics related to pressing social issues like
COVID-19 misinformation, social bot deception, cyberbully, clickbait news, and
the Russo-Ukrainian War. Our findings demonstrate that ChatGPT exhibits promise
in handling these data annotation tasks, albeit with some challenges. Across
the seven datasets, ChatGPT achieves an average annotation F1-score of 72.00%.
Its performance excels in clickbait news annotation, correctly labeling 89.66%
of the data. However, we also observe significant variations in performance
across individual labels. Our study reveals predictable patterns in ChatGPT's
annotation performance. Thus, we propose GPT-Rater, a tool to predict if
ChatGPT can correctly label data for a given annotation task. Researchers can
use this to identify where ChatGPT might be suitable for their annotation
requirements. We show that GPT-Rater effectively predicts ChatGPT's
performance. It performs best on a clickbait headlines dataset by achieving an
average F1-score of 95.00%. We believe that this research opens new avenues for
analysis and can reduce barriers to engaging in social computing research.","[{'name': 'Yiming Zhu'}, {'name': 'Peixian Zhang'}, {'name': 'Ehsan-Ul Haq'}, {'name': 'Pan Hui'}, {'name': 'Gareth Tyson'}]",2024-07-08T22:04:30Z
http://arxiv.org/abs/2407.06411v1,http://arxiv.org/abs/2407.06411v1,"If You Don't Understand It, Don't Use It: Eliminating Trojans with
  Filters Between Layers","Large language models (LLMs) sometimes exhibit dangerous unintended
behaviors. Finding and fixing these is challenging because the attack surface
is massive -- it is not tractable to exhaustively search for all possible
inputs that may elicit such behavior. One specific and particularly challenging
case is that if data-poisoning-injected trojans, since there is no way to know
what they are to search for them. To our knowledge, there is no generally
applicable method to unlearn unknown trojans injected during pre-training. This
work seeks to provide a general purpose recipe (filters) and a specific
implementation (LoRA) filters that work in practice on small to medium sized
models. The focus is primarily empirical, though some perplexing behavior opens
the door to the fundamental question of how LLMs store and process information.
Not unexpectedly, we find that our filters work best on the residual stream and
the latest layers.",[{'name': 'Adriano Hernandez'}],2024-07-08T21:40:23Z
http://arxiv.org/abs/2407.06380v1,http://arxiv.org/abs/2407.06380v1,"Data, Data Everywhere: A Guide for Pretraining Dataset Construction","The impressive capabilities of recent language models can be largely
attributed to the multi-trillion token pretraining datasets that they are
trained on. However, model developers fail to disclose their construction
methodology which has lead to a lack of open information on how to develop
effective pretraining sets. To address this issue, we perform the first
systematic study across the entire pipeline of pretraining set construction.
First, we run ablations on existing techniques for pretraining set development
to identify which methods translate to the largest gains in model accuracy on
downstream evaluations. Then, we categorize the most widely used data source,
web crawl snapshots, across the attributes of toxicity, quality, type of
speech, and domain. Finally, we show how such attribute information can be used
to further refine and improve the quality of a pretraining set. These findings
constitute an actionable set of steps that practitioners can use to develop
high quality pretraining sets.","[{'name': 'Jupinder Parmar'}, {'name': 'Shrimai Prabhumoye'}, {'name': 'Joseph Jennings'}, {'name': 'Bo Liu'}, {'name': 'Aastha Jhunjhunwala'}, {'name': 'Zhilin Wang'}, {'name': 'Mostofa Patwary'}, {'name': 'Mohammad Shoeybi'}, {'name': 'Bryan Catanzaro'}]",2024-07-08T20:47:58Z
http://arxiv.org/abs/2407.06349v1,http://arxiv.org/abs/2407.06349v1,Large Language Model Recall Uncertainty is Modulated by the Fan Effect,"This paper evaluates whether large language models (LLMs) exhibit cognitive
fan effects, similar to those discovered by Anderson in humans, after being
pre-trained on human textual data. We conduct two sets of in-context recall
experiments designed to elicit fan effects. Consistent with human results, we
find that LLM recall uncertainty, measured via token probability, is influenced
by the fan effect. Our results show that removing uncertainty disrupts the
observed effect. The experiments suggest the fan effect is consistent whether
the fan value is induced in-context or in the pre-training data. Finally, these
findings provide in-silico evidence that fan effects and typicality are
expressions of the same phenomena.","[{'name': 'Jesse Roberts'}, {'name': 'Kyle Moore'}, {'name': 'Thao Pham'}, {'name': 'Oseremhen Ewaleifoh'}, {'name': 'Doug Fisher'}]",2024-07-08T19:40:50Z
http://arxiv.org/abs/2407.06331v1,http://arxiv.org/abs/2407.06331v1,CharSS: Character-Level Transformer Model for Sanskrit Word Segmentation,"Subword tokens in Indian languages inherently carry meaning, and isolating
them can enhance NLP tasks, making sub-word segmentation a crucial process.
Segmenting Sanskrit and other Indian languages into subtokens is not
straightforward, as it may include sandhi, which may lead to changes in the
word boundaries. We propose a new approach of utilizing a Character-level
Transformer model for Sanskrit Word Segmentation (CharSS). We perform
experiments on three benchmark datasets to compare the performance of our
method against existing methods. On the UoH+SandhiKosh dataset, our method
outperforms the current state-of-the-art system by an absolute gain of 6.72
points in split prediction accuracy. On the hackathon dataset, our method
achieves a gain of 2.27 points over the current SOTA system in terms of perfect
match metric. We also propose a use-case of Sanskrit-based segments for a
linguistically informed translation of technical terms to lexically similar
low-resource Indian languages. In two separate experimental settings for this
task, we achieve an average improvement of 8.46 and 6.79 chrF++ scores,
respectively.","[{'name': 'Krishnakant Bhatt'}, {'name': 'Karthika N J'}, {'name': 'Ganesh Ramakrishnan'}, {'name': 'Preethi Jyothi'}]",2024-07-08T18:50:13Z
http://arxiv.org/abs/2407.06324v1,http://arxiv.org/abs/2407.06324v1,"B'MOJO: Hybrid State Space Realizations of Foundation Models with
  Eidetic and Fading Memory","We describe a family of architectures to support transductive inference by
allowing memory to grow to a finite but a-priori unknown bound while making
efficient use of finite resources for inference. Current architectures use such
resources to represent data either eidetically over a finite span (""context"" in
Transformers), or fading over an infinite span (in State Space Models, or
SSMs). Recent hybrid architectures have combined eidetic and fading memory, but
with limitations that do not allow the designer or the learning process to
seamlessly modulate the two, nor to extend the eidetic memory span. We leverage
ideas from Stochastic Realization Theory to develop a class of models called
B'MOJO to seamlessly combine eidetic and fading memory within an elementary
composable module. The overall architecture can be used to implement models
that can access short-term eidetic memory ""in-context,"" permanent structural
memory ""in-weights,"" fading memory ""in-state,"" and long-term eidetic memory
""in-storage"" by natively incorporating retrieval from an asynchronously updated
memory. We show that Transformers, existing SSMs such as Mamba, and hybrid
architectures such as Jamba are special cases of B'MOJO and describe a basic
implementation, to be open sourced, that can be stacked and scaled efficiently
in hardware. We test B'MOJO on transductive inference tasks, such as
associative recall, where it outperforms existing SSMs and Hybrid models; as a
baseline, we test ordinary language modeling where B'MOJO achieves perplexity
comparable to similarly-sized Transformers and SSMs up to 1.4B parameters,
while being up to 10% faster to train. Finally, we show that B'MOJO's ability
to modulate eidetic and fading memory results in better inference on longer
sequences tested up to 32K tokens, four-fold the length of the longest
sequences seen during training.","[{'name': 'Luca Zancato'}, {'name': 'Arjun Seshadri'}, {'name': 'Yonatan Dukler'}, {'name': 'Aditya Golatkar'}, {'name': 'Yantao Shen'}, {'name': 'Benjamin Bowman'}, {'name': 'Matthew Trager'}, {'name': 'Alessandro Achille'}, {'name': 'Stefano Soatto'}]",2024-07-08T18:41:01Z
http://arxiv.org/abs/2407.06323v1,http://arxiv.org/abs/2407.06323v1,"When in Doubt, Cascade: Towards Building Efficient and Capable
  Guardrails","Large language models (LLMs) have convincing performance in a variety of
downstream tasks. However, these systems are prone to generating undesirable
outputs such as harmful and biased text. In order to remedy such generations,
the development of guardrail (or detector) models has gained traction.
Motivated by findings from developing a detector for social bias, we adopt the
notion of a use-mention distinction - which we identified as the primary source
of under-performance in the preliminary versions of our social bias detector.
Armed with this information, we describe a fully extensible and reproducible
synthetic data generation pipeline which leverages taxonomy-driven instructions
to create targeted and labeled data. Using this pipeline, we generate over 300K
unique contrastive samples and provide extensive experiments to systematically
evaluate performance on a suite of open source datasets. We show that our
method achieves competitive performance with a fraction of the cost in compute
and offers insight into iteratively developing efficient and capable guardrail
models.
  Warning: This paper contains examples of text which are toxic, biased, and
potentially harmful.","[{'name': 'Manish Nagireddy'}, {'name': 'Inkit Padhi'}, {'name': 'Soumya Ghosh'}, {'name': 'Prasanna Sattigeri'}]",2024-07-08T18:39:06Z
http://arxiv.org/abs/2407.06314v3,http://arxiv.org/abs/2407.06314v3,"Personality Analysis for Social Media Users using Arabic language and
  its Effect on Sentiment Analysis","Social media is heading towards more and more personalization, where
individuals reveal their beliefs, interests, habits, and activities, simply
offering glimpses into their personality traits. This study, explores the
correlation between the use of Arabic language on twitter, personality traits
and its impact on sentiment analysis. We indicated the personality traits of
users based on the information extracted from their profile activities, and the
content of their tweets. Our analysis incorporated linguistic features, profile
statistics (including gender, age, bio, etc.), as well as additional features
like emoticons. To obtain personality data, we crawled the timelines and
profiles of users who took the 16personalities test in Arabic on
16personalities.com. Our dataset, ""AraPers"", comprised 3,250 users who shared
their personality results on twitter. We implemented various machine learning
techniques, to reveal personality traits and developed a dedicated model for
this purpose, achieving a 74.86% accuracy rate with BERT, analysis of this
dataset proved that linguistic features, profile features and derived model can
be used to differentiate between different personality traits. Furthermore, our
findings demonstrated that personality affect sentiment in social media. This
research contributes to the ongoing efforts in developing robust understanding
of the relation between human behaviour on social media and personality
features for real-world applications, such as political discourse analysis, and
public opinion tracking.","[{'name': 'Mokhaiber Dandash'}, {'name': 'Masoud Asadpour'}]",2024-07-08T18:27:54Z
http://arxiv.org/abs/2407.06192v1,http://arxiv.org/abs/2407.06192v1,Multi-Object Hallucination in Vision-Language Models,"Large vision language models (LVLMs) often suffer from object hallucination,
producing objects not present in the given images. While current benchmarks for
object hallucination primarily concentrate on the presence of a single object
class rather than individual entities, this work systematically investigates
multi-object hallucination, examining how models misperceive (e.g., invent
nonexistent objects or become distracted) when tasked with focusing on multiple
objects simultaneously. We introduce Recognition-based Object Probing
Evaluation (ROPE), an automated evaluation protocol that considers the
distribution of object classes within a single image during testing and uses
visual referring prompts to eliminate ambiguity. With comprehensive empirical
studies and analysis of potential factors leading to multi-object
hallucination, we found that (1) LVLMs suffer more hallucinations when focusing
on multiple objects compared to a single object. (2) The tested object class
distribution affects hallucination behaviors, indicating that LVLMs may follow
shortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced
by data-specific factors, salience and frequency, and model intrinsic
behaviors. We hope to enable LVLMs to recognize and reason about multiple
objects that often occur in realistic visual scenes, provide insights, and
quantify our progress towards mitigating the issues.","[{'name': 'Xuweiyi Chen'}, {'name': 'Ziqiao Ma'}, {'name': 'Xuejun Zhang'}, {'name': 'Sihan Xu'}, {'name': 'Shengyi Qian'}, {'name': 'Jianing Yang'}, {'name': 'David F. Fouhey'}, {'name': 'Joyce Chai'}]",2024-07-08T17:59:57Z
http://arxiv.org/abs/2407.06249v1,http://arxiv.org/abs/2407.06249v1,CodeUpdateArena: Benchmarking Knowledge Editing on API Updates,"Large language models (LLMs) are increasingly being used to synthesize and
reason about source code. However, the static nature of these models' knowledge
does not reflect the fact that libraries and API functions they invoke are
continuously evolving, with functionality being added or changing. While
numerous benchmarks evaluate how LLMs can generate code, no prior work has
studied how an LLMs' knowledge about code API functions can be updated. To fill
this gap, we present CodeUpdateArena, a benchmark for knowledge editing in the
code domain. An instance in our benchmark consists of a synthetic API function
update paired with a program synthesis example that uses the updated
functionality; our goal is to update an LLM to be able to solve this program
synthesis example without providing documentation of the update at inference
time. Compared to knowledge editing for facts encoded in text, success here is
more challenging: a code LLM must correctly reason about the semantics of the
modified function rather than just reproduce its syntax. Our dataset is
constructed by first prompting GPT-4 to generate atomic and executable function
updates. Then, for each update, we generate program synthesis examples whose
code solutions are prone to use the update. Our benchmark covers updates of
various types to 54 functions from seven diverse Python packages, with a total
of 670 program synthesis examples. Our experiments show that prepending
documentation of the update to open-source code LLMs (i.e., DeepSeek,
CodeLlama) does not allow them to incorporate changes for problem solving, and
existing knowledge editing techniques also have substantial room for
improvement. We hope our benchmark will inspire new methods for knowledge
updating in code LLMs.","[{'name': 'Zeyu Leo Liu'}, {'name': 'Shrey Pandit'}, {'name': 'Xi Ye'}, {'name': 'Eunsol Choi'}, {'name': 'Greg Durrett'}]",2024-07-08T17:55:04Z
http://arxiv.org/abs/2407.06135v1,http://arxiv.org/abs/2407.06135v1,"ANOLE: An Open, Autoregressive, Native Large Multimodal Models for
  Interleaved Image-Text Generation","Previous open-source large multimodal models (LMMs) have faced several
limitations: (1) they often lack native integration, requiring adapters to
align visual representations with pre-trained large language models (LLMs); (2)
many are restricted to single-modal generation; (3) while some support
multimodal generation, they rely on separate diffusion models for visual
modeling and generation. To mitigate these limitations, we present Anole, an
open, autoregressive, native large multimodal model for interleaved image-text
generation. We build Anole from Meta AI's Chameleon, adopting an innovative
fine-tuning strategy that is both data-efficient and parameter-efficient. Anole
demonstrates high-quality, coherent multimodal generation capabilities. We have
open-sourced our model, training framework, and instruction tuning data.","[{'name': 'Ethan Chern'}, {'name': 'Jiadi Su'}, {'name': 'Yan Ma'}, {'name': 'Pengfei Liu'}]",2024-07-08T17:08:02Z
http://arxiv.org/abs/2407.12852v2,http://arxiv.org/abs/2407.12852v2,Historical Ink: Semantic Shift Detection for 19th Century Spanish,"This paper explores the evolution of word meanings in 19th-century Spanish
texts, with an emphasis on Latin American Spanish, using computational
linguistics techniques. It addresses the Semantic Shift Detection (SSD) task,
which is crucial for understanding linguistic evolution, particularly in
historical contexts. The study focuses on analyzing a set of Spanish target
words. To achieve this, a 19th-century Spanish corpus is constructed, and a
customizable pipeline for SSD tasks is developed. This pipeline helps find the
senses of a word and measure their semantic change between two corpora using
fine-tuned BERT-like models with old Spanish texts for both Latin American and
general Spanish cases. The results provide valuable insights into the cultural
and societal shifts reflected in language changes over time.","[{'name': 'Tony Montes'}, {'name': 'Laura Manrique-Gómez'}, {'name': 'Rubén Manrique'}]",2024-07-08T16:49:34Z
http://arxiv.org/abs/2407.06112v1,http://arxiv.org/abs/2407.06112v1,"Enhancing Language Model Rationality with Bi-Directional Deliberation
  Reasoning","This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel
reasoning approach to enhance the decision rationality of language models.
Traditional reasoning methods typically rely on historical information and
employ uni-directional (left-to-right) reasoning strategy. This lack of
bi-directional deliberation reasoning results in limited awareness of potential
future outcomes and insufficient integration of historical context, leading to
suboptimal decisions. BIDDER addresses this gap by incorporating principles of
rational decision-making, specifically managing uncertainty and predicting
expected utility. Our approach involves three key processes: Inferring hidden
states to represent uncertain information in the decision-making process from
historical data; Using these hidden states to predict future potential states
and potential outcomes; Integrating historical information (past contexts) and
long-term outcomes (future contexts) to inform reasoning. By leveraging
bi-directional reasoning, BIDDER ensures thorough exploration of both past and
future contexts, leading to more informed and rational decisions. We tested
BIDDER's effectiveness in two well-defined scenarios: Poker (Limit Texas
Hold'em) and Negotiation. Our experiments demonstrate that BIDDER significantly
improves the decision-making capabilities of LLMs and LLM agents.","[{'name': 'Yadong Zhang'}, {'name': 'Shaoguang Mao'}, {'name': 'Wenshan Wu'}, {'name': 'Yan Xia'}, {'name': 'Tao Ge'}, {'name': 'Man Lan'}, {'name': 'Furu Wei'}]",2024-07-08T16:48:48Z
http://arxiv.org/abs/2407.06098v1,http://arxiv.org/abs/2407.06098v1,"Epistemological Bias As a Means for the Automated Detection of
  Injustices in Text","Injustice occurs when someone experiences unfair treatment or their rights
are violated and is often due to the presence of implicit biases and prejudice
such as stereotypes. The automated identification of injustice in text has
received little attention, due in part to the fact that underlying implicit
biases or stereotypes are rarely explicitly stated and that instances often
occur unconsciously due to the pervasive nature of prejudice in society. Here,
we describe a novel framework that combines the use of a fine-tuned BERT-based
bias detection model, two stereotype detection models, and a lexicon-based
approach to show that epistemological biases (i.e., words, which presupposes,
entails, asserts, hedges, or boosts text to erode or assert a person's capacity
as a knower) can assist with the automatic detection of injustice in text. The
news media has many instances of injustice (i.e. discriminatory narratives),
thus it is our use case here. We conduct and discuss an empirical qualitative
research study which shows how the framework can be applied to detect
injustices, even at higher volumes of data.","[{'name': 'Kenya Andrews'}, {'name': 'Lamogha Chiazor'}]",2024-07-08T16:38:31Z
http://arxiv.org/abs/2407.06057v1,http://arxiv.org/abs/2407.06057v1,Variational Best-of-N Alignment,"Best-of-N (BoN) is a popular and effective algorithm for aligning language
models to human preferences. The algorithm works as follows: at inference time,
N samples are drawn from the language model, and the sample with the highest
reward, as judged by a reward model, is returned as the output. Despite its
effectiveness, BoN is computationally expensive; it reduces sampling throughput
by a factor of N. To make BoN more efficient at inference time, one strategy is
to fine-tune the language model to mimic what BoN does during inference. To
achieve this, we derive the distribution induced by the BoN algorithm. We then
propose to fine-tune the language model to minimize backward KL divergence to
the BoN distribution. Our approach is analogous to mean-field variational
inference and, thus, we term it variational BoN (vBoN). To the extent this
fine-tuning is successful and we end up with a good approximation, we have
reduced the inference cost by a factor of N. Our experiments on a controlled
generation task suggest that while variational BoN is not as effective as BoN
in aligning language models, it is close to BoN performance as vBoN appears
more often on the Pareto frontier of reward and KL divergence compared to
models trained with KL-constrained RL objective.","[{'name': 'Afra Amini'}, {'name': 'Tim Vieira'}, {'name': 'Ryan Cotterell'}]",2024-07-08T15:59:44Z
http://arxiv.org/abs/2407.06048v1,http://arxiv.org/abs/2407.06048v1,"Vision-Braille: An End-to-End Tool for Chinese Braille Image-to-Text
  Translation","Visually impaired people are a large group who can only use braille for
reading and writing. However, the lack of special educational resources is the
bottleneck for educating them. Educational equity is a reflection of the level
of social civilization, cultural equality, and individual dignity. Facilitating
and improving lifelong learning channels for the visually impaired is of great
significance. Their written braille homework or exam papers cannot be
understood by sighted teachers, because of the lack of a highly accurate
braille translation system, especially in Chinese which has tone marks. braille
writers often omit tone marks to save space, leading to confusion when braille
with the same consonants and vowels is translated into Chinese. Previous
algorithms were insufficient in extracting contextual information, resulting in
low accuracy of braille translations into Chinese. This project informatively
fine-tuned the mT5 model with an Encoder-decoder architecture for braille to
Chinese character conversion. This research created a training set of braille
and corresponding Chinese text from the Leipzig Corpora. This project
significantly reduced the confusion in braille, achieving $62.4$ and $62.3$
BLEU scores in the validation and test sets, with a curriculum learning
fine-tuning method. By incorporating the braille recognition algorithm, this
project is the first publicly available braille translation system and can
benefit lots of visually impaired students and families who are preparing for
the Chinese College Test and help to propel their college dreams in the future.
There is a demo on our homepage\footnote{\url{https://vision-braille.com/}}.","[{'name': 'Alan Wu'}, {'name': 'Ye Yuan'}, {'name': 'Ming Zhang'}]",2024-07-08T15:51:37Z
http://arxiv.org/abs/2407.06041v1,http://arxiv.org/abs/2407.06041v1,MST5 -- Multilingual Question Answering over Knowledge Graphs,"Knowledge Graph Question Answering (KGQA) simplifies querying vast amounts of
knowledge stored in a graph-based model using natural language. However, the
research has largely concentrated on English, putting non-English speakers at a
disadvantage. Meanwhile, existing multilingual KGQA systems face challenges in
achieving performance comparable to English systems, highlighting the
difficulty of generating SPARQL queries from diverse languages. In this
research, we propose a simplified approach to enhance multilingual KGQA systems
by incorporating linguistic context and entity information directly into the
processing pipeline of a language model. Unlike existing methods that rely on
separate encoders for integrating auxiliary information, our strategy leverages
a single, pretrained multilingual transformer-based language model to manage
both the primary input and the auxiliary data. Our methodology significantly
improves the language model's ability to accurately convert a natural language
query into a relevant SPARQL query. It demonstrates promising results on the
most recent QALD datasets, namely QALD-9-Plus and QALD-10. Furthermore, we
introduce and evaluate our approach on Chinese and Japanese, thereby expanding
the language diversity of the existing datasets.","[{'name': 'Nikit Srivastava'}, {'name': 'Mengshi Ma'}, {'name': 'Daniel Vollmers'}, {'name': 'Hamada Zahera'}, {'name': 'Diego Moussallem'}, {'name': 'Axel-Cyrille Ngonga Ngomo'}]",2024-07-08T15:37:51Z
http://arxiv.org/abs/2407.06027v5,http://arxiv.org/abs/2407.06027v5,PAS: Data-Efficient Plug-and-Play Prompt Augmentation System,"In recent years, the rise of Large Language Models (LLMs) has spurred a
growing demand for plug-and-play AI systems. Among the various AI techniques,
prompt engineering stands out as particularly significant. However, users often
face challenges in writing prompts due to the steep learning curve and
significant time investment, and existing automatic prompt engineering (APE)
models can be difficult to use. To address this issue, we propose PAS, an
LLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,
automatically generated prompt complementary datasets, resulting in exceptional
performance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)
results compared to previous APE models, with an average improvement of 6.09
points. Moreover, PAS is highly efficient, achieving SoTA performance with only
9000 data points. Additionally, PAS can autonomously generate prompt
augmentation data without requiring additional human labor. Its flexibility
also allows it to be compatible with all existing LLMs and applicable to a wide
range of tasks. PAS excels in human evaluations, underscoring its suitability
as a plug-in for users. This combination of high performance, efficiency, and
flexibility makes PAS a valuable system for enhancing the usability and
effectiveness of LLMs through improved prompt engineering.","[{'name': 'Miao Zheng'}, {'name': 'Hao Liang'}, {'name': 'Fan Yang'}, {'name': 'Haoze Sun'}, {'name': 'Tianpeng Li'}, {'name': 'Lingchu Xiong'}, {'name': 'Yan Zhang'}, {'name': 'Youzhen Wu'}, {'name': 'Kun Li'}, {'name': 'Yanjun Shen'}, {'name': 'Mingan Lin'}, {'name': 'Tao Zhang'}, {'name': 'Guosheng Dong'}, {'name': 'Yujing Qiao'}, {'name': 'Kun Fang'}, {'name': 'Weipeng Chen'}, {'name': 'Bin Cui'}, {'name': 'Wentao Zhang'}, {'name': 'Zenan Zhou'}]",2024-07-08T15:25:33Z
http://arxiv.org/abs/2407.12851v1,http://arxiv.org/abs/2407.12851v1,"ISPO: An Integrated Ontology of Symptom Phenotypes for Semantic
  Integration of Traditional Chinese Medical Data","Symptom phenotypes are one of the key types of manifestations for diagnosis
and treatment of various disease conditions. However, the diversity of symptom
terminologies is one of the major obstacles hindering the analysis and
knowledge sharing of various types of symptom-related medical data particularly
in the fields of Traditional Chinese Medicine (TCM). Objective: This study
aimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to
support the data mining of Chinese EMRs and real-world study in TCM field.
Methods: To construct an integrated ontology of symptom phenotypes (ISPO), we
manually annotated classical TCM textbooks and large-scale Chinese electronic
medical records (EMRs) to collect symptom terms with support from a medical
text annotation system. Furthermore, to facilitate the semantic
interoperability between different terminologies, we incorporated public
available biomedical vocabularies by manual mapping between Chinese terms and
English terms with cross-references to source vocabularies. In addition, we
evaluated the ISPO using independent clinical EMRs to provide a high-usable
medical ontology for clinical data analysis. Results: By integrating 78,696
inpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and
dictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition
or contextual texts. Adhering to the taxonomical structure of the related
anatomical systems of symptom phenotypes, ISPO provides 12 top-level categories
and 79 middle-level sub-categories. The validation of data analysis showed the
ISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with
occurrence rates of 0.5% in additional three independent curated clinical
datasets, which can demonstrate the significant value of ISPO in mapping
clinical terms to ontologies.","[{'name': 'Zixin Shu'}, {'name': 'Rui Hua'}, {'name': 'Dengying Yan'}, {'name': 'Chenxia Lu'}, {'name': 'Ning Xu'}, {'name': 'Jun Li'}, {'name': 'Hui Zhu'}, {'name': 'Jia Zhang'}, {'name': 'Dan Zhao'}, {'name': 'Chenyang Hui'}, {'name': 'Junqiu Ye'}, {'name': 'Chu Liao'}, {'name': 'Qi Hao'}, {'name': 'Wen Ye'}, {'name': 'Cheng Luo'}, {'name': 'Xinyan Wang'}, {'name': 'Chuang Cheng'}, {'name': 'Xiaodong Li'}, {'name': 'Baoyan Liu'}, {'name': 'Xiaji Zhou'}, {'name': 'Runshun Zhang'}, {'name': 'Min Xu'}, {'name': 'Xuezhong Zhou'}]",2024-07-08T15:23:50Z
http://arxiv.org/abs/2407.06023v3,http://arxiv.org/abs/2407.06023v3,Distilling System 2 into System 1,"Large language models (LLMs) can spend extra compute during inference to
generate intermediate thoughts, which helps to produce better final responses.
Since Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have
been proposed such as Rephrase and Respond (Deng et al., 2023a), System 2
Attention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,
2023). In this work we investigate self-supervised methods to ``compile''
(distill) higher quality outputs from System 2 techniques back into LLM
generations without intermediate reasoning token sequences, as this reasoning
has been distilled into System 1. We show that several such techniques can be
successfully distilled, resulting in improved results compared to the original
System 1 performance, and with less inference cost than System 2. We posit that
such System 2 distillation will be an important feature of future continually
learning AI systems, enabling them to focus System 2 capabilities on the
reasoning tasks that they cannot yet do well.","[{'name': 'Ping Yu'}, {'name': 'Jing Xu'}, {'name': 'Jason Weston'}, {'name': 'Ilia Kulikov'}]",2024-07-08T15:17:46Z
http://arxiv.org/abs/2407.06011v1,http://arxiv.org/abs/2407.06011v1,"Igea: a Decoder-Only Language Model for Biomedical Text Generation in
  Italian","The development of domain-specific language models has significantly advanced
natural language processing applications in various specialized fields,
particularly in biomedicine. However, the focus has largely been on
English-language models, leaving a gap for less-resourced languages such as
Italian. This paper introduces Igea, the first decoder-only language model
designed explicitly for biomedical text generation in Italian. Built on the
Minerva model and continually pretrained on a diverse corpus of Italian medical
texts, Igea is available in three model sizes: 350 million, 1 billion, and 3
billion parameters. The models aim to balance computational efficiency and
performance, addressing the challenges of managing the peculiarities of medical
terminology in Italian. We evaluate Igea using a mix of in-domain biomedical
corpora and general-purpose benchmarks, highlighting its efficacy and retention
of general knowledge even after the domain-specific training. This paper
discusses the model's development and evaluation, providing a foundation for
future advancements in Italian biomedical NLP.","[{'name': 'Tommaso Mario Buonocore'}, {'name': 'Simone Rancati'}, {'name': 'Enea Parimbelli'}]",2024-07-08T15:04:21Z
http://arxiv.org/abs/2407.06004v2,http://arxiv.org/abs/2407.06004v2,"Perceptions to Beliefs: Exploring Precursory Inferences for Theory of
  Mind in Large Language Models","While humans naturally develop theory of mind (ToM), the capability to
understand other people's mental states and beliefs, state-of-the-art large
language models (LLMs) underperform on simple ToM benchmarks. We posit that we
can extend our understanding of LLMs' ToM abilities by evaluating key human ToM
precursors -- perception inference and perception-to-belief inference -- in
LLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate
these precursory inferences for ToM in LLMs by annotating characters'
perceptions on ToMi and FANToM, respectively. Our evaluation of eight
state-of-the-art LLMs reveals that the models generally perform well in
perception inference while exhibiting limited capability in
perception-to-belief inference (e.g., lack of inhibitory control). Based on
these results, we present PercepToM, a novel ToM method leveraging LLMs' strong
perception inference capability while supplementing their limited
perception-to-belief inference. Experimental results demonstrate that PercepToM
significantly enhances LLM's performance, especially in false belief scenarios.","[{'name': 'Chani Jung'}, {'name': 'Dongkwan Kim'}, {'name': 'Jiho Jin'}, {'name': 'Jiseon Kim'}, {'name': 'Yeon Seonwoo'}, {'name': 'Yejin Choi'}, {'name': 'Alice Oh'}, {'name': 'Hyunwoo Kim'}]",2024-07-08T14:58:29Z
http://arxiv.org/abs/2407.05975v1,http://arxiv.org/abs/2407.05975v1,"LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation
  Capabilities Beyond 100 Languages","Large Language Models~(LLMs) demonstrate remarkable translation capabilities
in high-resource language tasks, yet their performance in low-resource
languages is hindered by insufficient multilingual data during pre-training. To
address this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting
extensive multilingual continual pre-training on the LLaMA series models,
enabling translation support across more than 100 languages. Through a
comprehensive analysis of training strategies, such as vocabulary expansion and
data augmentation, we develop LLaMAX. Remarkably, without sacrificing its
generalization ability, LLaMAX achieves significantly higher translation
performance compared to existing open-source LLMs~(by more than 10 spBLEU
points) and performs on-par with specialized translation model~(M2M-100-12B) on
the Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve
as a robust multilingual foundation model. The
code~\footnote{\url{https://github.com/CONE-MT/LLaMAX/.}} and
models~\footnote{\url{https://huggingface.co/LLaMAX/.}} are publicly available.","[{'name': 'Yinquan Lu'}, {'name': 'Wenhao Zhu'}, {'name': 'Lei Li'}, {'name': 'Yu Qiao'}, {'name': 'Fei Yuan'}]",2024-07-08T14:18:28Z
http://arxiv.org/abs/2407.05965v1,http://arxiv.org/abs/2407.05965v1,T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models,"The recent development of Sora leads to a new era in text-to-video (T2V)
generation. Along with this comes the rising concern about its security risks.
The generated videos may contain illegal or unethical content, and there is a
lack of comprehensive quantitative understanding of their safety, posing a
challenge to their reliability and practical deployment. Previous evaluations
primarily focus on the quality of video generation. While some evaluations of
text-to-image models have considered safety, they cover fewer aspects and do
not address the unique temporal risk inherent in video generation. To bridge
this research gap, we introduce T2VSafetyBench, a new benchmark designed for
conducting safety-critical assessments of text-to-video models. We define 12
critical aspects of video generation safety and construct a malicious prompt
dataset using LLMs and jailbreaking prompt attacks. Based on our evaluation
results, we draw several important findings, including: 1) no single model
excels in all aspects, with different models showing various strengths; 2) the
correlation between GPT-4 assessments and manual reviews is generally high; 3)
there is a trade-off between the usability and safety of text-to-video
generative models. This indicates that as the field of video generation rapidly
advances, safety risks are set to surge, highlighting the urgency of
prioritizing video safety. We hope that T2VSafetyBench can provide insights for
better understanding the safety of video generation in the era of generative
AI.","[{'name': 'Yibo Miao'}, {'name': 'Yifan Zhu'}, {'name': 'Yinpeng Dong'}, {'name': 'Lijia Yu'}, {'name': 'Jun Zhu'}, {'name': 'Xiao-Shan Gao'}]",2024-07-08T14:04:58Z
http://arxiv.org/abs/2407.05925v1,http://arxiv.org/abs/2407.05925v1,"Towards Optimizing and Evaluating a Retrieval Augmented QA Chatbot using
  LLMs with Human in the Loop","Large Language Models have found application in various mundane and
repetitive tasks including Human Resource (HR) support. We worked with the
domain experts of SAP SE to develop an HR support chatbot as an efficient and
effective tool for addressing employee inquiries. We inserted a
human-in-the-loop in various parts of the development cycles such as dataset
collection, prompt optimization, and evaluation of generated output. By
enhancing the LLM-driven chatbot's response quality and exploring alternative
retrieval methods, we have created an efficient, scalable, and flexible tool
for HR professionals to address employee inquiries effectively. Our experiments
and evaluation conclude that GPT-4 outperforms other models and can overcome
inconsistencies in data through internal reasoning capabilities. Additionally,
through expert analysis, we infer that reference-free evaluation metrics such
as G-Eval and Prometheus demonstrate reliability closely aligned with that of
human evaluation.","[{'name': 'Anum Afzal'}, {'name': 'Alexander Kowsik'}, {'name': 'Rajna Fani'}, {'name': 'Florian Matthes'}]",2024-07-08T13:32:14Z
http://arxiv.org/abs/2407.06245v2,http://arxiv.org/abs/2407.06245v2,"ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open
  Radio Access Networks","Large Language Models (LLMs) can revolutionize how we deploy and operate Open
Radio Access Networks (O-RAN) by enhancing network analytics, anomaly
detection, and code generation and significantly increasing the efficiency and
reliability of a plethora of O-RAN tasks. In this paper, we present
ORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the
performance of Large Language Models (LLMs) within the context of O-RAN. Our
benchmark consists of 13,952 meticulously curated multiple-choice questions
generated from 116 O-RAN specification documents. We leverage a novel
three-stage LLM framework, and the questions are categorized into three
distinct difficulties to cover a wide spectrum of ORAN-related knowledge. We
thoroughly evaluate the performance of several state-of-the-art LLMs, including
Gemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a
Retrieval-Augmented Generation (RAG)-based pipeline that demonstrates superior
performance on ORAN-Bench-13K compared to other tested closed-source models.
Our findings indicate that current popular LLM models are not proficient in
O-RAN, highlighting the need for specialized models. We observed a noticeable
performance improvement when incorporating the RAG-based ORANSight pipeline,
with a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on
average 21.55% and 22.59% better than the other tested LLMs.","[{'name': 'Pranshav Gajjar'}, {'name': 'Vijay K. Shah'}]",2024-07-08T13:07:50Z
http://arxiv.org/abs/2407.05890v1,http://arxiv.org/abs/2407.05890v1,"Affordances-Oriented Planning using Foundation Models for Continuous
  Vision-Language Navigation","LLM-based agents have demonstrated impressive zero-shot performance in the
vision-language navigation (VLN) task. However, these zero-shot methods focus
only on solving high-level task planning by selecting nodes in predefined
navigation graphs for movements, overlooking low-level control in realistic
navigation scenarios. To bridge this gap, we propose AO-Planner, a novel
affordances-oriented planning framework for continuous VLN task. Our AO-Planner
integrates various foundation models to achieve affordances-oriented motion
planning and action decision-making, both performed in a zero-shot manner.
Specifically, we employ a visual affordances prompting (VAP) approach, where
visible ground is segmented utilizing SAM to provide navigational affordances,
based on which the LLM selects potential next waypoints and generates low-level
path planning towards selected waypoints. We further introduce a high-level
agent, PathAgent, to identify the most probable pixel-based path and convert it
into 3D coordinates to fulfill low-level motion. Experimental results on the
challenging R2R-CE benchmark demonstrate that AO-Planner achieves
state-of-the-art zero-shot performance (5.5% improvement in SPL). Our method
establishes an effective connection between LLM and 3D world to circumvent the
difficulty of directly predicting world coordinates, presenting novel prospects
for employing foundation models in low-level motion control.","[{'name': 'Jiaqi Chen'}, {'name': 'Bingqian Lin'}, {'name': 'Xinmin Liu'}, {'name': 'Xiaodan Liang'}, {'name': 'Kwan-Yee K. Wong'}]",2024-07-08T12:52:46Z
http://arxiv.org/abs/2407.05887v1,http://arxiv.org/abs/2407.05887v1,"Generation and De-Identification of Indian Clinical Discharge Summaries
  using LLMs","The consequences of a healthcare data breach can be devastating for the
patients, providers, and payers. The average financial impact of a data breach
in recent months has been estimated to be close to USD 10 million. This is
especially significant for healthcare organizations in India that are managing
rapid digitization while still establishing data governance procedures that
align with the letter and spirit of the law. Computer-based systems for
de-identification of personal information are vulnerable to data drift, often
rendering them ineffective in cross-institution settings. Therefore, a rigorous
assessment of existing de-identification against local health datasets is
imperative to support the safe adoption of digital health initiatives in India.
Using a small set of de-identified patient discharge summaries provided by an
Indian healthcare institution, in this paper, we report the nominal performance
of de-identification algorithms (based on language models) trained on publicly
available non-Indian datasets, pointing towards a lack of cross-institutional
generalization. Similarly, experimentation with off-the-shelf de-identification
systems reveals potential risks associated with the approach. To overcome data
scarcity, we explore generating synthetic clinical reports (using publicly
available and Indian summaries) by performing in-context learning over Large
Language Models (LLMs). Our experiments demonstrate the use of generated
reports as an effective strategy for creating high-performing de-identification
systems with good generalization capabilities.","[{'name': 'Sanjeet Singh'}, {'name': 'Shreya Gupta'}, {'name': 'Niralee Gupta'}, {'name': 'Naimish Sharma'}, {'name': 'Lokesh Srivastava'}, {'name': 'Vibhu Agarwal'}, {'name': 'Ashutosh Modi'}]",2024-07-08T12:47:03Z
http://arxiv.org/abs/2407.11046v3,http://arxiv.org/abs/2407.11046v3,A Survey on LoRA of Large Language Models,"Low-Rank Adaptation~(LoRA), which updates the dense neural network layers
with pluggable low-rank matrices, is one of the best performed parameter
efficient fine-tuning paradigms. Furthermore, it has significant advantages in
cross-task generalization and privacy-preserving. Hence, LoRA has gained much
attention recently, and the number of related literature demonstrates
exponential growth. It is necessary to conduct a comprehensive overview of the
current progress on LoRA. This survey categorizes and reviews the progress from
the perspectives of (1) downstream adaptation improving variants that improve
LoRA's performance on downstream tasks; (2) cross-task generalization methods
that mix multiple LoRA plugins to achieve cross-task generalization; (3)
efficiency-improving methods that boost the computation-efficiency of LoRA; (4)
data privacy-preserving methods that use LoRA in federated learning; (5)
application. Besides, this survey also discusses the future directions in this
field. At last, we provide a Github
page~\footnote{\href{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}}
for readers to check the updates and initiate discussions on this survey paper.","[{'name': 'Yuren Mao'}, {'name': 'Yuhang Ge'}, {'name': 'Yijiang Fan'}, {'name': 'Wenyi Xu'}, {'name': 'Yu Mi'}, {'name': 'Zhonghao Hu'}, {'name': 'Yunjun Gao'}]",2024-07-08T12:32:10Z
http://arxiv.org/abs/2407.05868v1,http://arxiv.org/abs/2407.05868v1,"KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge
  Graph-based False Premise Questions","Recent studies have demonstrated that large language models (LLMs) are
susceptible to being misled by false premise questions (FPQs), leading to
errors in factual knowledge, know as factuality hallucination. Existing
benchmarks that assess this vulnerability primarily rely on manual
construction, resulting in limited scale and lack of scalability. In this work,
we introduce an automated, scalable pipeline to create FPQs based on knowledge
graphs (KGs). The first step is modifying true triplets extracted from KGs to
create false premises. Subsequently, utilizing the state-of-the-art
capabilities of GPTs, we generate semantically rich FPQs. Based on the proposed
method, we present a comprehensive benchmark, the Knowledge Graph-based False
Premise Questions (KG-FPQ), which contains approximately 178k FPQs across three
knowledge domains, at six levels of confusability, and in two task formats.
Using KG-FPQ, we conduct extensive evaluations on several representative LLMs
and provide valuable insights. The KG-FPQ dataset and code are available
at~https://github.com/yanxuzhu/KG-FPQ.","[{'name': 'Yanxu Zhu'}, {'name': 'Jinlin Xiao'}, {'name': 'Yuhang Wang'}, {'name': 'Jitao Sang'}]",2024-07-08T12:31:03Z
http://arxiv.org/abs/2407.05841v1,http://arxiv.org/abs/2407.05841v1,"An Empirical Comparison of Vocabulary Expansion and Initialization
  Approaches for Language Models","Language Models (LMs) excel in natural language processing tasks for English
but show reduced performance in most other languages. This problem is commonly
tackled by continually pre-training and fine-tuning these models for said
languages. A significant issue in this process is the limited vocabulary
coverage in the original model's tokenizer, leading to inadequate
representation of new languages and necessitating an expansion of the
tokenizer. The initialization of the embeddings corresponding to new vocabulary
items presents a further challenge. Current strategies require cross-lingual
embeddings and lack a solid theoretical foundation as well as comparisons with
strong baselines. In this paper, we first establish theoretically that
initializing within the convex hull of existing embeddings is a good
initialization, followed by a novel but simple approach, Constrained Word2Vec
(CW2V), which does not require cross-lingual embeddings. Our study evaluates
different initialization methods for expanding RoBERTa and LLaMA 2 across four
languages and five tasks. The results show that CW2V performs equally well or
even better than more advanced techniques. Additionally, simpler approaches
like multivariate initialization perform on par with these advanced methods
indicating that efficient large-scale multilingual continued pretraining can be
achieved even with simpler initialization methods.","[{'name': 'Nandini Mundra'}, {'name': 'Aditya Nanda Kishore'}, {'name': 'Raj Dabre'}, {'name': 'Ratish Puduppully'}, {'name': 'Anoop Kunchukuttan'}, {'name': 'Mitesh M. Khapra'}]",2024-07-08T11:38:49Z
http://arxiv.org/abs/2407.12850v1,http://arxiv.org/abs/2407.12850v1,Limits to Predicting Online Speech Using Large Language Models,"We study the predictability of online speech on social media, and whether
predictability improves with information outside a user's own posts. Recent
work suggests that the predictive information contained in posts written by a
user's peers can surpass that of the user's own posts. Motivated by the success
of large language models, we empirically test this hypothesis. We define
unpredictability as a measure of the model's uncertainty, i.e., its negative
log-likelihood on future tokens given context. As the basis of our study, we
collect a corpus of 6.25M posts from more than five thousand X (previously
Twitter) users and their peers. Across three large language models ranging in
size from 1 billion to 70 billion parameters, we find that predicting a user's
posts from their peers' posts performs poorly. Moreover, the value of the
user's own posts for prediction is consistently higher than that of their
peers'. Across the board, we find that the predictability of social media posts
remains low, comparable to predicting financial news without context. We extend
our investigation with a detailed analysis about the causes of unpredictability
and the robustness of our findings. Specifically, we observe that a significant
amount of predictive uncertainty comes from hashtags and @-mentions. Moreover,
our results replicate if instead of prompting the model with additional
context, we finetune on additional context.","[{'name': 'Mina Remeli'}, {'name': 'Moritz Hardt'}, {'name': 'Robert C. Williamson'}]",2024-07-08T09:50:49Z
http://arxiv.org/abs/2407.05786v1,http://arxiv.org/abs/2407.05786v1,"Large Language Models for Judicial Entity Extraction: A Comparative
  Study","Domain-specific Entity Recognition holds significant importance in legal
contexts, serving as a fundamental task that supports various applications such
as question-answering systems, text summarization, machine translation,
sentiment analysis, and information retrieval specifically within case law
documents. Recent advancements have highlighted the efficacy of Large Language
Models in natural language processing tasks, demonstrating their capability to
accurately detect and classify domain-specific facts (entities) from
specialized texts like clinical and financial documents. This research
investigates the application of Large Language Models in identifying
domain-specific entities (e.g., courts, petitioner, judge, lawyer, respondents,
FIR nos.) within case law documents, with a specific focus on their aptitude
for handling domain-specific language complexity and contextual variations. The
study evaluates the performance of state-of-the-art Large Language Model
architectures, including Large Language Model Meta AI 3, Mistral, and Gemma, in
the context of extracting judicial facts tailored to Indian judicial texts.
Mistral and Gemma emerged as the top-performing models, showcasing balanced
precision and recall crucial for accurate entity identification. These findings
confirm the value of Large Language Models in judicial documents and
demonstrate how they can facilitate and quicken scientific research by
producing precise, organised data outputs that are appropriate for in-depth
examination.","[{'name': 'Atin Sakkeer Hussain'}, {'name': 'Anu Thomas'}]",2024-07-08T09:49:03Z
http://arxiv.org/abs/2407.05778v1,http://arxiv.org/abs/2407.05778v1,When is the consistent prediction likely to be a correct prediction?,"Self-consistency (Wang et al., 2023) suggests that the most consistent answer
obtained through large language models (LLMs) is more likely to be correct. In
this paper, we challenge this argument and propose a nuanced correction. Our
observations indicate that consistent answers derived through more computation
i.e. longer reasoning texts, rather than simply the most consistent answer
across all outputs, are more likely to be correct. This is predominantly
because we demonstrate that LLMs can autonomously produce chain-of-thought
(CoT) style reasoning with no custom prompts merely while generating longer
responses, which lead to consistent predictions that are more accurate. In the
zero-shot setting, by sampling Mixtral-8x7B model multiple times and
considering longer responses, we achieve 86% of its self-consistency
performance obtained through zero-shot CoT prompting on the GSM8K and
MultiArith datasets. Finally, we demonstrate that the probability of LLMs
generating a longer response is quite low, highlighting the need for decoding
strategies conditioned on output length.","[{'name': 'Alex Nguyen'}, {'name': 'Dheeraj Mekala'}, {'name': 'Chengyu Dong'}, {'name': 'Jingbo Shang'}]",2024-07-08T09:37:27Z
http://arxiv.org/abs/2407.05750v2,http://arxiv.org/abs/2407.05750v2,Large Language Models Understand Layout,"Large language models (LLMs) demonstrate extraordinary abilities in a wide
range of natural language processing (NLP) tasks. In this paper, we show that,
beyond text understanding capability, LLMs are capable of processing text
layouts that are denoted by spatial markers. They are able to answer questions
that require explicit spatial perceiving and reasoning, while a drastic
performance drop is observed when the spatial markers from the original data
are excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,
Llama2 and ChatGLM3 models on various types of layout-sensitive datasets for
further analysis. The experimental results reveal that the layout understanding
ability of LLMs is mainly introduced by the coding data for pretraining, which
is further enhanced at the instruction-tuning stage. In addition, layout
understanding can be enhanced by integrating low-cost, auto-generated data
approached by a novel text game. Finally, we show that layout understanding
ability is beneficial for building efficient visual question-answering (VQA)
systems.","[{'name': 'Weiming Li'}, {'name': 'Manni Duan'}, {'name': 'Dong An'}, {'name': 'Yan Shao'}]",2024-07-08T09:03:12Z
http://arxiv.org/abs/2407.18332v1,http://arxiv.org/abs/2407.18332v1,"Analyzing Speech Unit Selection for Textless Speech-to-Speech
  Translation","Recent advancements in textless speech-to-speech translation systems have
been driven by the adoption of self-supervised learning techniques. Although
most state-of-the-art systems adopt a similar architecture to transform source
language speech into sequences of discrete representations in the target
language, the criteria for selecting these target speech units remains an open
question. This work explores the selection process through a study of
downstream tasks such as automatic speech recognition, speech synthesis,
speaker recognition, and emotion recognition. Interestingly, our findings
reveal a discrepancy in the optimization of discrete speech units: units that
perform well in resynthesis performance do not necessarily correlate with those
that enhance translation efficacy. This discrepancy underscores the nuanced
complexity of target feature selection and its impact on the overall
performance of speech-to-speech translation systems.","[{'name': 'Jarod Duret'}, {'name': 'Yannick Estève'}, {'name': 'Titouan Parcollet'}]",2024-07-08T08:53:26Z
http://arxiv.org/abs/2407.05740v2,http://arxiv.org/abs/2407.05740v2,Do Multilingual Large Language Models Mitigate Stereotype Bias?,"While preliminary findings indicate that multilingual LLMs exhibit reduced
bias compared to monolingual ones, a comprehensive understanding of the effect
of multilingual training on bias mitigation, is lacking. This study addresses
this gap by systematically training six LLMs of identical size (2.6B
parameters) and architecture: five monolingual models (English, German, French,
Italian, and Spanish) and one multilingual model trained on an equal
distribution of data across these languages, all using publicly available data.
To ensure robust evaluation, standard bias benchmarks were automatically
translated into the five target languages and verified for both translation
quality and bias preservation by human annotators. Our results consistently
demonstrate that multilingual training effectively mitigates bias. Moreover, we
observe that multilingual models achieve not only lower bias but also superior
prediction accuracy when compared to monolingual models with the same amount of
training data, model architecture, and size.","[{'name': 'Shangrui Nie'}, {'name': 'Michael Fromm'}, {'name': 'Charles Welch'}, {'name': 'Rebekka Görge'}, {'name': 'Akbar Karimi'}, {'name': 'Joan Plepi'}, {'name': 'Nazia Afsan Mowmita'}, {'name': 'Nicolas Flores-Herr'}, {'name': 'Mehdi Ali'}, {'name': 'Lucie Flek'}]",2024-07-08T08:46:50Z
http://arxiv.org/abs/2407.05734v1,http://arxiv.org/abs/2407.05734v1,Empirical Study of Symmetrical Reasoning in Conversational Chatbots,"This work explores the capability of conversational chatbots powered by large
language models (LLMs), to understand and characterize predicate symmetry, a
cognitive linguistic function traditionally believed to be an inherent human
trait. Leveraging in-context learning (ICL), a paradigm shift enabling chatbots
to learn new tasks from prompts without re-training, we assess the symmetrical
reasoning of five chatbots: ChatGPT 4, Huggingface chat AI, Microsoft's Copilot
AI, LLaMA through Perplexity, and Gemini Advanced. Using the Symmetry Inference
Sentence (SIS) dataset by Tanchip et al. (2020), we compare chatbot responses
against human evaluations to gauge their understanding of predicate symmetry.
Experiment results reveal varied performance among chatbots, with some
approaching human-like reasoning capabilities. Gemini, for example, reaches a
correlation of 0.85 with human scores, while providing a sounding justification
for each symmetry evaluation. This study underscores the potential and
limitations of LLMs in mirroring complex cognitive processes as symmetrical
reasoning.","[{'name': 'Daniela N. Rim'}, {'name': 'Heeyoul Choi'}]",2024-07-08T08:38:43Z
http://arxiv.org/abs/2407.05733v1,http://arxiv.org/abs/2407.05733v1,"Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative
  Judgment Approach Based on Rater Cognition","Large Language Models (LLMs) have shown promise in Automated Essay Scoring
(AES), but their zero-shot and few-shot performance often falls short compared
to state-of-the-art models and human raters. However, fine-tuning LLMs for each
specific task is impractical due to the variety of essay prompts and rubrics
used in real-world educational contexts. This study proposes a novel approach
combining LLMs and Comparative Judgment (CJ) for AES, using zero-shot prompting
to choose between two essays. We demonstrate that a CJ method surpasses
traditional rubric-based scoring in essay scoring using LLMs.","[{'name': 'Seungju Kim'}, {'name': 'Meounggun Jo'}]",2024-07-08T08:37:00Z
http://arxiv.org/abs/2407.05721v2,http://arxiv.org/abs/2407.05721v2,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,"Mental health has attracted substantial attention in recent years and LLM can
be an effective technology for alleviating this problem owing to its capability
in text understanding and dialogue. However, existing research in this domain
often suffers from limitations, such as training on datasets lacking crucial
prior knowledge and evidence, and the absence of comprehensive evaluation
methods. In this paper, we propose a specialized psychological large language
model (LLM), named PsycoLLM, trained on a proposed high-quality psychological
dataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.
Specifically, we construct multi-turn dialogues through a three-step pipeline
comprising generation, evidence judgment, and refinement. We augment this
process with real-world psychological case backgrounds extracted from online
platforms, enhancing the relevance and applicability of the generated data.
Additionally, to compare the performance of PsycoLLM with other LLMs, we
develop a comprehensive psychological benchmark based on authoritative
psychological counseling examinations in China, which includes assessments of
professional ethics, theoretical proficiency, and case analysis. The
experimental results on the benchmark illustrates the effectiveness of
PsycoLLM, which demonstrates superior performance compared to other LLMs.","[{'name': 'Jinpeng Hu'}, {'name': 'Tengteng Dong'}, {'name': 'Luo Gang'}, {'name': 'Hui Ma'}, {'name': 'Peng Zou'}, {'name': 'Xiao Sun'}, {'name': 'Dan Guo'}, {'name': 'Meng Wang'}]",2024-07-08T08:25:56Z
http://arxiv.org/abs/2407.05718v1,http://arxiv.org/abs/2407.05718v1,"A Factuality and Diversity Reconciled Decoding Method for
  Knowledge-Grounded Dialogue Generation","Grounding external knowledge can enhance the factuality of responses in
dialogue generation. However, excessive emphasis on it might result in the lack
of engaging and diverse expressions. Through the introduction of randomness in
sampling, current approaches can increase the diversity. Nevertheless, such
sampling method could undermine the factuality in dialogue generation. In this
study, to discover a solution for advancing creativity without relying on
questionable randomness and to subtly reconcile the factuality and diversity
within the source-grounded paradigm, a novel method named DoGe is proposed.
DoGe can dynamically alternate between the utilization of internal parameter
knowledge and external source knowledge based on the model's factual
confidence. Extensive experiments on three widely-used datasets show that DoGe
can not only enhance response diversity but also maintain factuality, and it
significantly surpasses other various decoding strategy baselines.","[{'name': 'Chenxu Yang'}, {'name': 'Zheng Lin'}, {'name': 'Chong Tian'}, {'name': 'Liang Pang'}, {'name': 'Lanrui Wang'}, {'name': 'Zhengyang Tong'}, {'name': 'Qirong Ho'}, {'name': 'Yanan Cao'}, {'name': 'Weiping Wang'}]",2024-07-08T08:23:11Z
http://arxiv.org/abs/2407.05700v1,http://arxiv.org/abs/2407.05700v1,"InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with
  Inverse-Instruct","Recent advancements in open-source code large language models (LLMs) have
demonstrated remarkable coding abilities by fine-tuning on the data generated
from powerful closed-source LLMs such as GPT-3.5 and GPT-4 for instruction
tuning. This paper explores how to further improve an instruction-tuned code
LLM by generating data from itself rather than querying closed-source LLMs. Our
key observation is the misalignment between the translation of formal and
informal languages: translating formal language (i.e., code) to informal
language (i.e., natural language) is more straightforward than the reverse.
Based on this observation, we propose INVERSE-INSTRUCT, which summarizes
instructions from code snippets instead of the reverse. Specifically, given an
instruction tuning corpus for code and the resulting instruction-tuned code
LLM, we ask the code LLM to generate additional high-quality instructions for
the original corpus through code summarization and self-evaluation. Then, we
fine-tune the base LLM on the combination of the original corpus and the
self-generated one, which yields a stronger instruction-tuned LLM. We present a
series of code LLMs named InverseCoder, which surpasses the performance of the
original code LLMs on a wide range of benchmarks, including Python text-to-code
generation, multilingual coding, and data-science code generation.","[{'name': 'Yutong Wu'}, {'name': 'Di Huang'}, {'name': 'Wenxuan Shi'}, {'name': 'Wei Wang'}, {'name': 'Lingzhe Gao'}, {'name': 'Shihao Liu'}, {'name': 'Ziyuan Nan'}, {'name': 'Kaizhao Yuan'}, {'name': 'Rui Zhang'}, {'name': 'Xishan Zhang'}, {'name': 'Zidong Du'}, {'name': 'Qi Guo'}, {'name': 'Yewen Pu'}, {'name': 'Dawei Yin'}, {'name': 'Xing Hu'}, {'name': 'Yunji Chen'}]",2024-07-08T08:00:05Z
http://arxiv.org/abs/2407.05694v2,http://arxiv.org/abs/2407.05694v2,On the Limitations of Compute Thresholds as a Governance Strategy,"At face value, this essay is about understanding a fairly esoteric governance
tool called compute thresholds. However, in order to grapple with whether these
thresholds will achieve anything, we must first understand how they came to be.
To do so, we need to engage with a decades-old debate at the heart of computer
science progress, namely, is bigger always better? Does a certain inflection
point of compute result in changes to the risk profile of a model? Hence, this
essay may be of interest not only to policymakers and the wider public but also
to computer scientists interested in understanding the role of compute in
unlocking breakthroughs. This discussion is timely given the wide adoption of
compute thresholds in both the White House Executive Orders on AI Safety (EO)
and the EU AI Act to identify more risky systems. A key conclusion of this
essay is that compute thresholds, as currently implemented, are shortsighted
and likely to fail to mitigate risk. The relationship between compute and risk
is highly uncertain and rapidly changing. Relying upon compute thresholds
overestimates our ability to predict what abilities emerge at different scales.
This essay ends with recommendations for a better way forward.",[{'name': 'Sara Hooker'}],2024-07-08T07:53:06Z
http://arxiv.org/abs/2407.05693v1,http://arxiv.org/abs/2407.05693v1,"Sub-SA: Strengthen In-context Learning via Submodular Selective
  Annotation","In-context learning (ICL) leverages in-context examples as prompts for the
predictions of Large Language Models (LLMs). These prompts play a crucial role
in achieving strong performance. However, the selection of suitable prompts
from a large pool of labeled examples often entails significant annotation
costs. To address this challenge, we propose \textbf{Sub-SA}
(\textbf{Sub}modular \textbf{S}elective \textbf{A}nnotation), a submodule-based
selective annotation method. The aim of Sub-SA is to reduce annotation costs
while improving the quality of in-context examples and minimizing the time
consumption of the selection process. In Sub-SA, we design a submodular
function that facilitates effective subset selection for annotation and
demonstrates the characteristics of monotonically and submodularity from the
theoretical perspective. Specifically, we propose \textbf{RPR} (\textbf{R}eward
and \textbf{P}enalty \textbf{R}egularization) to better balance the diversity
and representativeness of the unlabeled dataset attributed to a reward term and
a penalty term, respectively. Consequently, the selection for annotations can
be effectively addressed with a simple yet effective greedy search algorithm
based on the submodular function. Finally, we apply the similarity prompt
retrieval to get the examples for ICL.","[{'name': 'Jian Qian'}, {'name': 'Miao Sun'}, {'name': 'Sifan Zhou'}, {'name': 'Ziyu Zhao'}, {'name': 'Ruizhi Hun'}, {'name': 'Patrick Chiang'}]",2024-07-08T07:47:30Z
http://arxiv.org/abs/2407.05690v1,http://arxiv.org/abs/2407.05690v1,"Pruning Large Language Models to Intra-module Low-rank Architecture with
  Transitional Activations","Structured pruning fundamentally reduces computational and memory overheads
of large language models (LLMs) and offers a feasible solution for end-side LLM
deployment. Structurally pruned models remain dense and high-precision, highly
compatible with further tuning and compression. However, as the coarse-grained
structured pruning poses large damage to the highly interconnected model,
achieving a high compression ratio for scaled-up LLMs remains a challenge. In
this paper, we introduce a task-agnostic structured pruning approach coupled
with a compact Transformer architecture design. The proposed approach, named
TransAct, reduces transitional activations inside multi-head attention (MHA)
and multi-layer perceptron (MLP) modules, while preserving the inter-module
activations that are sensitive to perturbations. Hence, the LLM is pruned into
an intra-module low-rank architecture, significantly reducing weights, KV Cache
and attention computation. TransAct is implemented on the LLaMA model and
evaluated on downstream benchmarks. Results verify the optimality of our
approach at high compression with respect to both efficiency and performance.
Further, ablation studies reveal the strength of activation-guided iterative
pruning and provide experimental analysis on the redundancy of MHA and MLP
modules.","[{'name': 'Bowen Shen'}, {'name': 'Zheng Lin'}, {'name': 'Daren Zha'}, {'name': 'Wei Liu'}, {'name': 'Jian Luan'}, {'name': 'Bin Wang'}, {'name': 'Weiping Wang'}]",2024-07-08T07:45:38Z
http://arxiv.org/abs/2407.05682v1,http://arxiv.org/abs/2407.05682v1,Retrieved In-Context Principles from Previous Mistakes,"In-context learning (ICL) has been instrumental in adapting Large Language
Models (LLMs) to downstream tasks using correct input-output examples. Recent
advances have attempted to improve model performance through principles derived
from mistakes, yet these approaches suffer from lack of customization and
inadequate error coverage. To address these limitations, we propose Retrieved
In-Context Principles (RICP), a novel teacher-student framework. In RICP, the
teacher model analyzes mistakes from the student model to generate reasons and
insights for preventing similar mistakes. These mistakes are clustered based on
their underlying reasons for developing task-level principles, enhancing the
error coverage of principles. During inference, the most relevant mistakes for
each question are retrieved to create question-level principles, improving the
customization of the provided guidance. RICP is orthogonal to existing
prompting methods and does not require intervention from the teacher model
during inference. Experimental results across seven reasoning benchmarks reveal
that RICP effectively enhances performance when applied to various prompting
strategies.","[{'name': 'Hao Sun'}, {'name': 'Yong Jiang'}, {'name': 'Bo Wang'}, {'name': 'Yingyan Hou'}, {'name': 'Yan Zhang'}, {'name': 'Pengjun Xie'}, {'name': 'Fei Huang'}]",2024-07-08T07:32:26Z
http://arxiv.org/abs/2407.05656v1,http://arxiv.org/abs/2407.05656v1,Multi-label Learning with Random Circular Vectors,"The extreme multi-label classification~(XMC) task involves learning a
classifier that can predict from a large label set the most relevant subset of
labels for a data instance. While deep neural networks~(DNNs) have demonstrated
remarkable success in XMC problems, the task is still challenging because it
must deal with a large number of output labels, which make the DNN training
computationally expensive. This paper addresses the issue by exploring the use
of random circular vectors, where each vector component is represented as a
complex amplitude. In our framework, we can develop an output layer and loss
function of DNNs for XMC by representing the final output layer as a fully
connected layer that directly predicts a low-dimensional circular vector
encoding a set of labels for a data instance. We conducted experiments on
synthetic datasets to verify that circular vectors have better label encoding
capacity and retrieval ability than normal real-valued vectors. Then, we
conducted experiments on actual XMC datasets and found that these appealing
properties of circular vectors contribute to significant improvements in task
performance compared with a previous model using random real-valued vectors,
while reducing the size of the output layers by up to 99%.","[{'name': 'Ken Nishida'}, {'name': 'Kojiro Machi'}, {'name': 'Kazuma Onishi'}, {'name': 'Katsuhiko Hayashi'}, {'name': 'Hidetaka Kamigaito'}]",2024-07-08T06:29:46Z
http://arxiv.org/abs/2407.05627v1,http://arxiv.org/abs/2407.05627v1,"New Directions in Text Classification Research: Maximizing The
  Performance of Sentiment Classification from Limited Data","The stakeholders' needs in sentiment analysis for various issues, whether
positive or negative, are speed and accuracy. One new challenge in sentiment
analysis tasks is the limited training data, which often leads to suboptimal
machine learning models and poor performance on test data. This paper discusses
the problem of text classification based on limited training data (300 to 600
samples) into three classes: positive, negative, and neutral. A benchmark
dataset is provided for training and testing data on the issue of Kaesang
Pangarep's appointment as Chairman of PSI. External data for aggregation and
augmentation purposes are provided, consisting of two datasets: the topic of
Covid Vaccination sentiment and an open topic. The official score used is the
F1-score, which balances precision and recall among the three classes,
positive, negative, and neutral. A baseline score is provided as a reference
for researchers for unoptimized classification methods. The optimized score is
provided as a reference for the target score to be achieved by any proposed
method. Both scoring (baseline and optimized) use the SVM method, which is
widely reported as the state-of-the-art in conventional machine learning
methods. The F1-scores achieved by the baseline and optimized methods are
40.83% and 51.28%, respectively.","[{'name': 'Surya Agustian'}, {'name': 'Muhammad Irfan Syah'}, {'name': 'Nurul Fatiara'}, {'name': 'Rahmad Abdillah'}]",2024-07-08T05:42:29Z
http://arxiv.org/abs/2407.05609v1,http://arxiv.org/abs/2407.05609v1,"Open-world Multi-label Text Classification with Extremely Weak
  Supervision","We study open-world multi-label text classification under extremely weak
supervision (XWS), where the user only provides a brief description for
classification objectives without any labels or ground-truth label space.
Similar single-label XWS settings have been explored recently, however, these
methods cannot be easily adapted for multi-label. We observe that (1) most
documents have a dominant class covering the majority of content and (2)
long-tail labels would appear in some documents as a dominant class. Therefore,
we first utilize the user description to prompt a large language model (LLM)
for dominant keyphrases of a subset of raw documents, and then construct a
(initial) label space via clustering. We further apply a zero-shot multi-label
classifier to locate the documents with small top predicted scores, so we can
revisit their dominant keyphrases for more long-tail labels. We iterate this
process to discover a comprehensive label space and construct a multi-label
classifier as a novel method, X-MLClass. X-MLClass exhibits a remarkable
increase in ground-truth label space coverage on various datasets, for example,
a 40% improvement on the AAPD dataset over topic modeling and keyword
extraction methods. Moreover, X-MLClass achieves the best end-to-end
multi-label classification accuracy.","[{'name': 'Xintong Li'}, {'name': 'Jinya Jiang'}, {'name': 'Ria Dharmani'}, {'name': 'Jayanth Srinivasa'}, {'name': 'Gaowen Liu'}, {'name': 'Jingbo Shang'}]",2024-07-08T04:52:49Z
http://arxiv.org/abs/2407.05608v1,http://arxiv.org/abs/2407.05608v1,A Benchmark for Multi-speaker Anonymization,"Privacy-preserving voice protection approaches primarily suppress
privacy-related information derived from paralinguistic attributes while
preserving the linguistic content. Existing solutions focus on single-speaker
scenarios. However, they lack practicality for real-world applications, i.e.,
multi-speaker scenarios. In this paper, we present an initial attempt to
provide a multi-speaker anonymization benchmark by defining the task and
evaluation protocol, proposing benchmarking solutions, and discussing the
privacy leakage of overlapping conversations. Specifically, ideal multi-speaker
anonymization should preserve the number of speakers and the turn-taking
structure of the conversation, ensuring accurate context conveyance while
maintaining privacy. To achieve that, a cascaded system uses speaker
diarization to aggregate the speech of each speaker and speaker anonymization
to conceal speaker privacy and preserve speech content. Additionally, we
propose two conversation-level speaker vector anonymization methods to improve
the utility further. Both methods aim to make the original and corresponding
pseudo-speaker identities of each speaker unlinkable while preserving or even
improving the distinguishability among pseudo-speakers in a conversation. The
first method minimizes the differential similarity across speaker pairs in the
original and anonymized conversations to maintain original speaker
relationships in the anonymized version. The other method minimizes the
aggregated similarity across anonymized speakers to achieve better
differentiation between speakers. Experiments conducted on both non-overlap
simulated and real-world datasets demonstrate the effectiveness of the
multi-speaker anonymization system with the proposed speaker anonymizers.
Additionally, we analyzed overlapping speech regarding privacy leakage and
provide potential solutions.","[{'name': 'Xiaoxiao Miao'}, {'name': 'Ruijie Tao'}, {'name': 'Chang Zeng'}, {'name': 'Xin Wang'}]",2024-07-08T04:48:43Z
http://arxiv.org/abs/2407.05599v1,http://arxiv.org/abs/2407.05599v1,Generative Debunking of Climate Misinformation,"Misinformation about climate change causes numerous negative impacts,
necessitating corrective responses. Psychological research has offered various
strategies for reducing the influence of climate misinformation, such as the
fact-myth-fallacy-fact-structure. However, practically implementing corrective
interventions at scale represents a challenge. Automatic detection and
correction of misinformation offers a solution to the misinformation problem.
This study documents the development of large language models that accept as
input a climate myth and produce a debunking that adheres to the
fact-myth-fallacy-fact (``truth sandwich'') structure, by incorporating
contrarian claim classification and fallacy detection into an LLM prompting
framework. We combine open (Mixtral, Palm2) and proprietary (GPT-4) LLMs with
prompting strategies of varying complexity. Experiments reveal promising
performance of GPT-4 and Mixtral if combined with structured prompts. We
identify specific challenges of debunking generation and human evaluation, and
map out avenues for future work. We release a dataset of high-quality
truth-sandwich debunkings, source code and a demo of the debunking system.","[{'name': 'Francisco Zanartu'}, {'name': 'Yulia Otmakhova'}, {'name': 'John Cook'}, {'name': 'Lea Frermann'}]",2024-07-08T04:21:58Z
http://arxiv.org/abs/2407.05591v1,http://arxiv.org/abs/2407.05591v1,On the Power of Convolution Augmented Transformer,"The transformer architecture has catalyzed revolutionary advances in language
modeling. However, recent architectural recipes, such as state-space models,
have bridged the performance gap. Motivated by this, we examine the benefits of
Convolution-Augmented Transformer (CAT) for recall, copying, and length
generalization tasks. CAT incorporates convolutional filters in the K/Q/V
embeddings of an attention layer. Through CAT, we show that the locality of the
convolution synergizes with the global view of the attention. Unlike comparable
architectures, such as Mamba or transformer, CAT can provably solve the
associative recall (AR) and copying tasks using a single layer while also
enjoying guaranteed length generalization. We also establish computational
tradeoffs between convolution and attention by characterizing how convolution
can mitigate the need for full attention by summarizing the context window and
creating salient summary tokens to attend. Evaluations on real datasets
corroborate our findings and demonstrate that CAT and its variations indeed
enhance the language modeling performance.","[{'name': 'Mingchen Li'}, {'name': 'Xuechen Zhang'}, {'name': 'Yixiao Huang'}, {'name': 'Samet Oymak'}]",2024-07-08T04:08:35Z
http://arxiv.org/abs/2407.05563v1,http://arxiv.org/abs/2407.05563v1,LLMBox: A Comprehensive Library for Large Language Models,"To facilitate the research on large language models (LLMs), this paper
presents a comprehensive and unified library, LLMBox, to ease the development,
use, and evaluation of LLMs. This library is featured with three main merits:
(1) a unified data interface that supports the flexible implementation of
various training strategies, (2) a comprehensive evaluation that covers
extensive tasks, datasets, and models, and (3) more practical consideration,
especially on user-friendliness and efficiency. With our library, users can
easily reproduce existing methods, train new models, and conduct comprehensive
performance comparisons. To rigorously test LLMBox, we conduct extensive
experiments in a diverse coverage of evaluation settings, and experimental
results demonstrate the effectiveness and efficiency of our library in
supporting various implementations related to LLMs. The detailed introduction
and usage guidance can be found at https://github.com/RUCAIBox/LLMBox.","[{'name': 'Tianyi Tang'}, {'name': 'Yiwen Hu'}, {'name': 'Bingqian Li'}, {'name': 'Wenyang Luo'}, {'name': 'Zijing Qin'}, {'name': 'Haoxiang Sun'}, {'name': 'Jiapeng Wang'}, {'name': 'Shiyi Xu'}, {'name': 'Xiaoxue Cheng'}, {'name': 'Geyang Guo'}, {'name': 'Han Peng'}, {'name': 'Bowen Zheng'}, {'name': 'Yiru Tang'}, {'name': 'Yingqian Min'}, {'name': 'Yushuo Chen'}, {'name': 'Jie Chen'}, {'name': 'Yuanqian Zhao'}, {'name': 'Luran Ding'}, {'name': 'Yuhao Wang'}, {'name': 'Zican Dong'}, {'name': 'Chunxuan Xia'}, {'name': 'Junyi Li'}, {'name': 'Kun Zhou'}, {'name': 'Wayne Xin Zhao'}, {'name': 'Ji-Rong Wen'}]",2024-07-08T02:39:33Z
http://arxiv.org/abs/2407.05502v2,http://arxiv.org/abs/2407.05502v2,"Faux Polyglot: A Study on Information Disparity in Multilingual Large
  Language Models","With Retrieval Augmented Generation (RAG), Large Language Models (LLMs) are
playing a pivotal role in information search and are being adopted globally.
Although the multilingual capability of LLMs offers new opportunities to bridge
the language barrier, do these capabilities translate into real-life scenarios
where linguistic divide and knowledge conflicts between multilingual sources
are known occurrences? In this paper, we studied LLM's linguistic preference in
a RAG-based information search setting. We found that LLMs displayed systemic
bias towards information in the same language as the query language in both
information retrieval and answer generation. Furthermore, in scenarios where
there is little information in the language of the query, LLMs prefer documents
in high-resource languages, reinforcing the dominant views. Such bias exists
for both factual and opinion-based queries. Our results highlight the
linguistic divide within multilingual LLMs in information search systems. The
seemingly beneficial multilingual capability of LLMs may backfire on
information parity by reinforcing language-specific information cocoons or
filter bubbles further marginalizing low-resource views.","[{'name': 'Nikhil Sharma'}, {'name': 'Kenton Murray'}, {'name': 'Ziang Xiao'}]",2024-07-07T21:26:36Z
http://arxiv.org/abs/2407.05489v1,http://arxiv.org/abs/2407.05489v1,How Effective are State Space Models for Machine Translation?,"Transformers are the current architecture of choice for NLP, but their
attention layers do not scale well to long contexts. Recent works propose to
replace attention with linear recurrent layers -- this is the case for state
space models, which enjoy efficient training and inference. However, it remains
unclear whether these models are competitive with transformers in machine
translation (MT). In this paper, we provide a rigorous and comprehensive
experimental comparison between transformers and linear recurrent models for
MT. Concretely, we experiment with RetNet, Mamba, and hybrid versions of Mamba
which incorporate attention mechanisms. Our findings demonstrate that Mamba is
highly competitive with transformers on sentence and paragraph-level datasets,
where in the latter both models benefit from shifting the training distribution
towards longer sequences. Further analysis show that integrating attention into
Mamba improves translation quality, robustness to sequence length
extrapolation, and the ability to recall named entities.","[{'name': 'Hugo Pitorro'}, {'name': 'Pavlo Vasylenko'}, {'name': 'Marcos Treviso'}, {'name': 'André F. T. Martins'}]",2024-07-07T20:21:49Z
http://arxiv.org/abs/2407.05483v1,http://arxiv.org/abs/2407.05483v1,Just read twice: closing the recall gap for recurrent language models,"Recurrent large language models that compete with Transformers in language
modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV).
Excitingly, these architectures use a constant amount of memory during
inference. However, due to the limited memory, recurrent LMs cannot recall and
use all the information in long contexts leading to brittle in-context learning
(ICL) quality. A key challenge for efficient LMs is selecting what information
to store versus discard. In this work, we observe the order in which
information is shown to the LM impacts the selection difficulty. To formalize
this, we show that the hardness of information recall reduces to the hardness
of a problem called set disjointness (SD), a quintessential problem in
communication complexity that requires a streaming algorithm (e.g., recurrent
model) to decide whether inputted sets are disjoint. We empirically and
theoretically show that the recurrent memory required to solve SD changes with
set order, i.e., whether the smaller set appears first in-context. Our analysis
suggests, to mitigate the reliance on data order, we can put information in the
right order in-context or process prompts non-causally. Towards that end, we
propose: (1) JRT-Prompt, where context gets repeated multiple times in the
prompt, effectively showing the model all data orders. This gives $11.0 \pm
1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL
tasks, with $11.9\times$ higher throughput than FlashAttention-2 for generation
prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2)
JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and
provides $99\%$ of Transformer quality at $360$M params., $30$B tokens and
$96\%$ at $1.3$B params., $50$B tokens on average across the tasks, with
$19.2\times$ higher throughput for prefill than FA2.","[{'name': 'Simran Arora'}, {'name': 'Aman Timalsina'}, {'name': 'Aaryan Singhal'}, {'name': 'Benjamin Spector'}, {'name': 'Sabri Eyuboglu'}, {'name': 'Xinyi Zhao'}, {'name': 'Ashish Rao'}, {'name': 'Atri Rudra'}, {'name': 'Christopher Ré'}]",2024-07-07T19:55:09Z
http://arxiv.org/abs/2407.05480v1,http://arxiv.org/abs/2407.05480v1,Biomedical Nested NER with Large Language Model and UMLS Heuristics,"In this paper, we present our system for the BioNNE English track, which aims
to extract 8 types of biomedical nested named entities from biomedical text. We
use a large language model (Mixtral 8x7B instruct) and ScispaCy NER model to
identify entities in an article and build custom heuristics based on unified
medical language system (UMLS) semantic types to categorize the entities. We
discuss the results and limitations of our system and propose future
improvements. Our system achieved an F1 score of 0.39 on the BioNNE validation
set and 0.348 on the test set.",[{'name': 'Wenxin Zhou'}],2024-07-07T19:37:40Z
http://arxiv.org/abs/2407.05474v1,http://arxiv.org/abs/2407.05474v1,"Enhancing Hallucination Detection through Perturbation-Based Synthetic
  Data Generation in System Responses","Detecting hallucinations in large language model (LLM) outputs is pivotal,
yet traditional fine-tuning for this classification task is impeded by the
expensive and quickly outdated annotation process, especially across numerous
vertical domains and in the face of rapid LLM advancements. In this study, we
introduce an approach that automatically generates both faithful and
hallucinated outputs by rewriting system responses. Experimental findings
demonstrate that a T5-base model, fine-tuned on our generated dataset,
surpasses state-of-the-art zero-shot detectors and existing synthetic
generation methods in both accuracy and latency, indicating efficacy of our
approach.","[{'name': 'Dongxu Zhang'}, {'name': 'Varun Gangal'}, {'name': 'Barrett Martin Lattimer'}, {'name': 'Yi Yang'}]",2024-07-07T19:19:32Z
http://arxiv.org/abs/2407.05464v1,http://arxiv.org/abs/2407.05464v1,"Experiments with truth using Machine Learning: Spectral analysis and
  explainable classification of synthetic, false, and genuine information","Misinformation is still a major societal problem and the arrival of Large
Language Models (LLMs) only added to it. This paper analyzes synthetic, false,
and genuine information in the form of text from spectral analysis,
visualization, and explainability perspectives to find the answer to why the
problem is still unsolved despite multiple years of research and a plethora of
solutions in the literature. Various embedding techniques on multiple datasets
are used to represent information for the purpose. The diverse spectral and
non-spectral methods used on these embeddings include t-distributed Stochastic
Neighbor Embedding (t-SNE), Principal Component Analysis (PCA), and Variational
Autoencoders (VAEs). Classification is done using multiple machine learning
algorithms. Local Interpretable Model-Agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), and Integrated Gradients are used for the
explanation of the classification. The analysis and the explanations generated
show that misinformation is quite closely intertwined with genuine information
and the machine learning algorithms are not as effective in separating the two
despite the claims in the literature.","[{'name': 'Vishnu S. Pendyala'}, {'name': 'Madhulika Dutta'}]",2024-07-07T18:31:09Z
http://arxiv.org/abs/2407.05463v1,http://arxiv.org/abs/2407.05463v1,Training Task Experts through Retrieval Based Distillation,"One of the most reliable ways to create deployable models for specialized
tasks is to obtain an adequate amount of high-quality task-specific data.
However, for specialized tasks, often such datasets do not exist. Existing
methods address this by creating such data from large language models (LLMs)
and then distilling such knowledge into smaller models. However, these methods
are limited by the quality of the LLMs output, and tend to generate repetitive
or incorrect data. In this work, we present Retrieval Based Distillation
(ReBase), a method that first retrieves data from rich online sources and then
transforms them into domain-specific data. This method greatly enhances data
diversity. Moreover, ReBase generates Chain-of-Thought reasoning and distills
the reasoning capacity of LLMs. We test our method on 4 benchmarks and results
show that our method significantly improves performance by up to 7.8% on SQuAD,
1.37% on MNLI, and 1.94% on BigBench-Hard.","[{'name': 'Jiaxin Ge'}, {'name': 'Xueying Jia'}, {'name': 'Vijay Viswanathan'}, {'name': 'Hongyin Luo'}, {'name': 'Graham Neubig'}]",2024-07-07T18:27:59Z
http://arxiv.org/abs/2407.05449v2,http://arxiv.org/abs/2407.05449v2,"SmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers
  for Text Detoxification","This paper presents a solution for the Multilingual Text Detoxification task
in the PAN-2024 competition of the SmurfCat team. Using data augmentation
through machine translation and a special filtering procedure, we collected an
additional multilingual parallel dataset for text detoxification. Using the
obtained data, we fine-tuned several multilingual sequence-to-sequence models,
such as mT0 and Aya, on a text detoxification task. We applied the ORPO
alignment technique to the final model. Our final model has only 3.7 billion
parameters and achieves state-of-the-art results for the Ukrainian language and
near state-of-the-art results for other languages. In the competition, our team
achieved first place in the automated evaluation with a score of 0.52 and
second place in the final human evaluation with a score of 0.74.","[{'name': 'Elisei Rykov'}, {'name': 'Konstantin Zaytsev'}, {'name': 'Ivan Anisimov'}, {'name': 'Alexandr Voronin'}]",2024-07-07T17:19:34Z
http://arxiv.org/abs/2407.05434v1,http://arxiv.org/abs/2407.05434v1,"LTLBench: Towards Benchmarks for Evaluating Temporal Logic Reasoning in
  Large Language Models","Temporal reasoning (TR) is a critical component of artificial intelligence,
encompassing understanding and processing temporal information and
relationships between events. To discover and study the TR ability in Large
Language Models (LLMs), various datasets have been constructed in different
ways for evaluating various aspects of TR ability. Our work proposes a novel
approach to design and develop a pipeline for constructing datasets to evaluate
the TR ability of LLMs by leveraging random directed graph generation, LTL
formula, and the NuSMV model checker. Based on the pipeline, we have also
constructed a dataset as a benchmark, namely LTLBench, consisting of 2,000 TR
challenges and evaluated six LLMs with it. Furthermore, we have conducted
additional experiments to discover the impact of increasing the number of
events and formula operators on the complexity of TR problems and the
performance of LLMs. We have demonstrated that although LLMs exhibit some
promise in handling TR challenges, they still struggle with complex TR. We
expect this work can offer insights into TR ability in LLMs while also
providing a valuable tool for future TR evaluations.","[{'name': 'Weizhi Tang'}, {'name': 'Vaishak Belle'}]",2024-07-07T16:37:06Z
http://arxiv.org/abs/2407.05413v2,http://arxiv.org/abs/2407.05413v2,SBoRA: Low-Rank Adaptation with Regional Weight Updates,"This paper introduces Standard Basis LoRA (SBoRA), a novel
parameter-efficient fine-tuning approach for Large Language Models that builds
upon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal
Adaptation. SBoRA further reduces the computational and memory requirements of
LoRA while enhancing learning performance. By leveraging orthogonal standard
basis vectors to initialize one of the low-rank matrices, either A or B, SBoRA
enables regional weight updates and memory-efficient fine-tuning. This approach
gives rise to two variants, SBoRA-FA and SBoRA-FB, where only one of the
matrices is updated, resulting in a sparse update matrix with a majority of
zero rows or columns. Consequently, the majority of the fine-tuned model's
weights remain unchanged from the pre-trained weights. This characteristic of
SBoRA, wherein regional weight updates occur, is reminiscent of the modular
organization of the human brain, which efficiently adapts to new tasks. Our
empirical results demonstrate the superiority of SBoRA-FA over LoRA in various
fine-tuning tasks, including commonsense reasoning and arithmetic reasoning.
Furthermore, we evaluate the effectiveness of QSBoRA on quantized LLaMA models
of varying scales, highlighting its potential for efficient adaptation to new
tasks. Code is available at https://github.com/cityuhkai/SBoRA","[{'name': 'Lai-Man Po'}, {'name': 'Yuyang Liu'}, {'name': 'Haoxuan Wu'}, {'name': 'Tianqi Zhang'}, {'name': 'Wing-Yin Yu'}, {'name': 'Zeyu Jiang'}, {'name': 'Kun Li'}]",2024-07-07T15:37:13Z
http://arxiv.org/abs/2407.05404v1,http://arxiv.org/abs/2407.05404v1,iSign: A Benchmark for Indian Sign Language Processing,"Indian Sign Language has limited resources for developing machine learning
and data-driven approaches for automated language processing. Though
text/audio-based language processing techniques have shown colossal research
interest and tremendous improvements in the last few years, Sign Languages
still need to catch up due to the need for more resources. To bridge this gap,
in this work, we propose iSign: a benchmark for Indian Sign Language (ISL)
Processing. We make three primary contributions to this work. First, we release
one of the largest ISL-English datasets with more than 118K
video-sentence/phrase pairs. To the best of our knowledge, it is the largest
sign language dataset available for ISL. Second, we propose multiple
NLP-specific tasks (including SignVideo2Text, SignPose2Text, Text2Pose, Word
Prediction, and Sign Semantics) and benchmark them with the baseline models for
easier access to the research community. Third, we provide detailed insights
into the proposed benchmarks with a few linguistic insights into the workings
of ISL. We streamline the evaluation of Sign Language processing, addressing
the gaps in the NLP research community for Sign Languages. We release the
dataset, tasks, and models via the following website:
https://exploration-lab.github.io/iSign/","[{'name': 'Abhinav Joshi'}, {'name': 'Romit Mohanty'}, {'name': 'Mounika Kanakanti'}, {'name': 'Andesha Mangla'}, {'name': 'Sudeep Choudhary'}, {'name': 'Monali Barbate'}, {'name': 'Ashutosh Modi'}]",2024-07-07T15:07:35Z
http://arxiv.org/abs/2407.05399v1,http://arxiv.org/abs/2407.05399v1,IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning,"Legal systems worldwide are inundated with exponential growth in cases and
documents. There is an imminent need to develop NLP and ML techniques for
automatically processing and understanding legal documents to streamline the
legal system. However, evaluating and comparing various NLP models designed
specifically for the legal domain is challenging. This paper addresses this
challenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding
and Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual
(9 Indian languages) domain-specific tasks that address different aspects of
the legal system from the point of view of understanding and reasoning over
Indian legal documents. We present baseline models (including LLM-based) for
each task, outlining the gap between models and the ground truth. To foster
further research in the legal domain, we create a leaderboard (available at:
https://exploration-lab.github.io/IL-TUR/) where the research community can
upload and compare legal text understanding systems.","[{'name': 'Abhinav Joshi'}, {'name': 'Shounak Paul'}, {'name': 'Akshat Sharma'}, {'name': 'Pawan Goyal'}, {'name': 'Saptarshi Ghosh'}, {'name': 'Ashutosh Modi'}]",2024-07-07T14:55:04Z
http://arxiv.org/abs/2407.05374v1,http://arxiv.org/abs/2407.05374v1,"Multimodal Prompt Learning with Missing Modalities for Sentiment
  Analysis and Emotion Recognition","The development of multimodal models has significantly advanced multimodal
sentiment analysis and emotion recognition. However, in real-world
applications, the presence of various missing modality cases often leads to a
degradation in the model's performance. In this work, we propose a novel
multimodal Transformer framework using prompt learning to address the issue of
missing modalities. Our method introduces three types of prompts: generative
prompts, missing-signal prompts, and missing-type prompts. These prompts enable
the generation of missing modality features and facilitate the learning of
intra- and inter-modality information. Through prompt learning, we achieve a
substantial reduction in the number of trainable parameters. Our proposed
method outperforms other methods significantly across all evaluation metrics.
Extensive experiments and ablation studies are conducted to demonstrate the
effectiveness and robustness of our method, showcasing its ability to
effectively handle missing modalities.","[{'name': 'Zirun Guo'}, {'name': 'Tao Jin'}, {'name': 'Zhou Zhao'}]",2024-07-07T13:55:56Z
http://arxiv.org/abs/2407.05361v2,http://arxiv.org/abs/2407.05361v2,"Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for
  Large-Scale Speech Generation","Recently, speech generation models have made significant progress by using
large-scale training data. However, the research community struggle to produce
highly spontaneous and human-like speech due to the lack of large-scale,
diverse, and spontaneous speech data. This paper present Emilia, the first
multilingual speech generation dataset from in-the-wild speech data, and
Emilia-Pipe, the first open-source preprocessing pipeline designed to transform
in-the-wild speech data into high-quality training data with annotations for
speech generation. Emilia starts with over 101k hours of speech in six
languages and features diverse speech with varied speaking styles. To
facilitate the scale-up of Emilia, the open-source pipeline Emilia-Pipe can
process one hour of raw speech data ready for model training in a few mins,
which enables the research community to collaborate on large-scale speech
generation research. Experimental results validate the effectiveness of Emilia.
Demos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.","[{'name': 'Haorui He'}, {'name': 'Zengqiang Shang'}, {'name': 'Chaoren Wang'}, {'name': 'Xuyuan Li'}, {'name': 'Yicheng Gu'}, {'name': 'Hua Hua'}, {'name': 'Liwei Liu'}, {'name': 'Chen Yang'}, {'name': 'Jiaqi Li'}, {'name': 'Peiyang Shi'}, {'name': 'Yuancheng Wang'}, {'name': 'Kai Chen'}, {'name': 'Pengyuan Zhang'}, {'name': 'Zhizheng Wu'}]",2024-07-07T13:24:54Z
http://arxiv.org/abs/2407.05355v1,http://arxiv.org/abs/2407.05355v1,VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool,"Multimodal large language models (MLLMs) are flourishing, but mainly focus on
images with less attention than videos, especially in sub-fields such as prompt
engineering, video chain-of-thought (CoT), and instruction tuning on videos.
Therefore, we try to explore the collection of CoT datasets in videos to lead
to video OpenQA and improve the reasoning ability of MLLMs. Unfortunately,
making such video CoT datasets is not an easy task. Given that human annotation
is too cumbersome and expensive, while machine-generated is not reliable due to
the hallucination issue, we develop an automatic annotation tool that combines
machine and human experts, under the active learning paradigm. Active learning
is an interactive strategy between the model and human experts, in this way,
the workload of human labeling can be reduced and the quality of the dataset
can be guaranteed. With the help of the automatic annotation tool, we strive to
contribute three datasets, namely VideoCoT, TopicQA, TopicCoT. Furthermore, we
propose a simple but effective benchmark based on the collected datasets, which
exploits CoT to maximize the complex reasoning capabilities of MLLMs. Extensive
experiments demonstrate the effectiveness our solution.","[{'name': 'Yan Wang'}, {'name': 'Yawen Zeng'}, {'name': 'Jingsheng Zheng'}, {'name': 'Xiaofen Xing'}, {'name': 'Jin Xu'}, {'name': 'Xiangmin Xu'}]",2024-07-07T13:10:23Z
http://arxiv.org/abs/2407.14525v1,http://arxiv.org/abs/2407.14525v1,"Morse Code-Enabled Speech Recognition for Individuals with Visual and
  Hearing Impairments","The proposed model aims to develop a speech recognition technology for
hearing, speech, or cognitively disabled people. All the available technology
in the field of speech recognition doesn't come with an interface for
communication for people with hearing, speech, or cognitive disabilities. The
proposed model proposes the speech from the user, is transmitted to the speech
recognition layer where it is converted into text and then that text is then
transmitted to the morse code conversion layer where the morse code of the
corresponding speech is given as the output. The accuracy of the model is
completely dependent on speech recognition, as the morse code conversion is a
process. The model is tested with recorded audio files with different
parameters. The proposed model's WER and accuracy are both determined to be
10.18% and 89.82%, respectively.",[{'name': 'Ritabrata Roy Choudhury'}],2024-07-07T09:54:29Z
http://arxiv.org/abs/2407.06230v1,http://arxiv.org/abs/2407.06230v1,"Predicting Word Similarity in Context with Referential Translation
  Machines","We identify the similarity between two words in English by casting the task
as machine translation performance prediction (MTPP) between the words given
the context and the distance between their similarities. We use referential
translation machines (RTMs), which allows a common representation for training
and test sets and stacked machine learning models. RTMs can achieve the top
results in Graded Word Similarity in Context (GWSC) task.",[{'name': 'Ergun Biçici'}],2024-07-07T09:36:41Z
http://arxiv.org/abs/2407.17505v2,http://arxiv.org/abs/2407.17505v2,Survey on biomarkers in human vocalizations,"Recent years has witnessed an increase in technologies that use speech for
the sensing of the health of the talker. This survey paper proposes a general
taxonomy of the technologies and a broad overview of current progress and
challenges. Vocal biomarkers are often secondary measures that are
approximating a signal of another sensor or identifying an underlying mental,
cognitive, or physiological state. Their measurement involve disturbances and
uncertainties that may be considered as noise sources and the biomarkers are
coarsely qualified in terms of the various sources of noise involved in their
determination. While in some proposed biomarkers the error levels seem high,
there are vocal biomarkers where the errors are expected to be low and thus are
more likely to qualify as candidates for adoption in healthcare applications.","[{'name': 'Aki Härmä'}, {'name': 'Bert den Brinker'}, {'name': 'Ulf Grossekathofer'}, {'name': 'Okke Ouweltjes'}, {'name': 'Srikanth Nallanthighal'}, {'name': 'Sidharth Abrol'}, {'name': 'Vibhu Sharma'}]",2024-07-07T08:09:28Z
http://arxiv.org/abs/2407.05271v1,http://arxiv.org/abs/2407.05271v1,"Beyond Binary Gender Labels: Revealing Gender Biases in LLMs through
  Gender-Neutral Name Predictions","Name-based gender prediction has traditionally categorized individuals as
either female or male based on their names, using a binary classification
system. That binary approach can be problematic in the cases of gender-neutral
names that do not align with any one gender, among other reasons. Relying
solely on binary gender categories without recognizing gender-neutral names can
reduce the inclusiveness of gender prediction tasks. We introduce an additional
gender category, i.e., ""neutral"", to study and address potential gender biases
in Large Language Models (LLMs). We evaluate the performance of several
foundational and large language models in predicting gender based on first
names only. Additionally, we investigate the impact of adding birth years to
enhance the accuracy of gender prediction, accounting for shifting associations
between names and genders over time. Our findings indicate that most LLMs
identify male and female names with high accuracy (over 80%) but struggle with
gender-neutral names (under 40%), and the accuracy of gender prediction is
higher for English-based first names than non-English names. The experimental
results show that incorporating the birth year does not improve the overall
accuracy of gender prediction, especially for names with evolving gender
associations. We recommend using caution when applying LLMs for gender
identification in downstream tasks, particularly when dealing with non-binary
gender labels.","[{'name': 'Zhiwen You'}, {'name': 'HaeJin Lee'}, {'name': 'Shubhanshu Mishra'}, {'name': 'Sullam Jeoung'}, {'name': 'Apratim Mishra'}, {'name': 'Jinseok Kim'}, {'name': 'Jana Diesner'}]",2024-07-07T05:59:09Z
http://arxiv.org/abs/2407.05250v1,http://arxiv.org/abs/2407.05250v1,CLIMB: A Benchmark of Clinical Bias in Large Language Models,"Large language models (LLMs) are increasingly applied to clinical
decision-making. However, their potential to exhibit bias poses significant
risks to clinical equity. Currently, there is a lack of benchmarks that
systematically evaluate such clinical bias in LLMs. While in downstream tasks,
some biases of LLMs can be avoided such as by instructing the model to answer
""I'm not sure..."", the internal bias hidden within the model still lacks deep
studies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in
Large Language Models), a pioneering comprehensive benchmark to evaluate both
intrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for
clinical decision tasks. Notably, for intrinsic bias, we introduce a novel
metric, AssocMAD, to assess the disparities of LLMs across multiple demographic
groups. Additionally, we leverage counterfactual intervention to evaluate
extrinsic bias in a task of clinical diagnosis prediction. Our experiments
across popular and medically adapted LLMs, particularly from the Mistral and
LLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic
bias. This work underscores the critical need to mitigate clinical bias and
sets a new standard for future evaluations of LLMs' clinical bias.","[{'name': 'Yubo Zhang'}, {'name': 'Shudi Hou'}, {'name': 'Mingyu Derek Ma'}, {'name': 'Wei Wang'}, {'name': 'Muhao Chen'}, {'name': 'Jieyu Zhao'}]",2024-07-07T03:41:51Z
http://arxiv.org/abs/2407.05244v1,http://arxiv.org/abs/2407.05244v1,"Some Issues in Predictive Ethics Modeling: An Annotated Contrast Set of
  ""Moral Stories""","Models like Delphi have been able to label ethical dilemmas as moral or
immoral with astonishing accuracy. This paper challenges accuracy as a holistic
metric for ethics modeling by identifying issues with translating moral
dilemmas into text-based input. It demonstrates these issues with contrast sets
that substantially reduce the performance of classifiers trained on the dataset
Moral Stories. Ultimately, we obtain concrete estimates for how much specific
forms of data misrepresentation harm classifier accuracy. Specifically,
label-changing tweaks to the descriptive content of a situation (as small as
3-5 words) can reduce classifier accuracy to as low as 51%, almost half the
initial accuracy of 99.8%. Associating situations with a misleading social norm
lowers accuracy to 98.8%, while adding textual bias (i.e. an implication that a
situation already fits a certain label) lowers accuracy to 77%.
  These results suggest not only that many ethics models have substantially
overfit, but that several precautions are required to ensure that input
accurately captures a moral dilemma. This paper recommends re-examining the
structure of a social norm, training models to ask for context with defeasible
reasoning, and filtering input for textual bias. Doing so not only gives us the
first concrete estimates of the average cost to accuracy of misrepresenting
ethics data, but gives researchers practical tips for considering these
estimates in research.",[{'name': 'Ben Fitzgerald'}],2024-07-07T03:22:49Z
http://arxiv.org/abs/2407.05233v1,http://arxiv.org/abs/2407.05233v1,"Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of
  Gemma-2b-it and Phi2 Models","Prompt recovery, a crucial task in natural language processing, entails the
reconstruction of prompts or instructions that language models use to convert
input text into a specific output. Although pivotal, the design and
effectiveness of prompts represent a challenging and relatively untapped field
within NLP research. This paper delves into an exhaustive investigation of
prompt recovery methodologies, employing a spectrum of pre-trained language
models and strategies. Our study is a comparative analysis aimed at gauging the
efficacy of various models on a benchmark dataset, with the goal of pinpointing
the most proficient approach for prompt recovery. Through meticulous
experimentation and detailed analysis, we elucidate the outstanding performance
of the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its
counterparts, showcasing its exceptional capability in accurately
reconstructing prompts for text transformation tasks. Our findings offer a
significant contribution to the existing knowledge on prompt recovery, shedding
light on the intricacies of prompt design and offering insightful perspectives
for future innovations in text rewriting and the broader field of natural
language processing.","[{'name': 'Jianlong Chen'}, {'name': 'Wei Xu'}, {'name': 'Zhicheng Ding'}, {'name': 'Jinxin Xu'}, {'name': 'Hao Yan'}, {'name': 'Xinyu Zhang'}]",2024-07-07T02:15:26Z
http://arxiv.org/abs/2407.05219v2,http://arxiv.org/abs/2407.05219v2,Flood of Techniques and Drought of Theories: Emotion Mining in Disasters,"Emotion mining has become a crucial tool for understanding human emotions
during disasters, leveraging the extensive data generated on social media
platforms. This paper aims to summarize existing research on emotion mining
within disaster contexts, highlighting both significant discoveries and
persistent issues. On the one hand, emotion mining techniques have achieved
acceptable accuracy enabling applications such as rapid damage assessment and
mental health surveillance. On the other hand, with many studies adopting
data-driven approaches, several methodological issues remain. These include
arbitrary emotion classification, ignoring biases inherent in data collection
from social media, such as the overrepresentation of individuals from higher
socioeconomic status on Twitter, and the lack of application of theoretical
frameworks like cross-cultural comparisons. These problems can be summarized as
a notable lack of theory-driven research and ignoring insights from social and
behavioral sciences. This paper underscores the need for interdisciplinary
collaboration between computer scientists and social scientists to develop more
robust and theoretically grounded approaches in emotion mining. By addressing
these gaps, we aim to enhance the effectiveness and reliability of emotion
mining methodologies, ultimately contributing to improved disaster
preparedness, response, and recovery.
  Keywords: emotion mining, sentiment analysis, natural disasters, psychology,
technological disasters","[{'name': 'Soheil Shapouri'}, {'name': 'Saber Soleymani'}, {'name': 'Saed Rezayi'}]",2024-07-07T00:43:05Z
http://arxiv.org/abs/2407.05216v1,http://arxiv.org/abs/2407.05216v1,"Large Language Model as an Assignment Evaluator: Insights, Feedback, and
  Challenges in a 1000+ Student Course","Using large language models (LLMs) for automatic evaluation has become an
important evaluation method in NLP research. However, it is unclear whether
these LLM-based evaluators can be applied in real-world classrooms to assess
student assignments. This empirical report shares how we use GPT-4 as an
automatic assignment evaluator in a university course with 1,028 students.
Based on student responses, we find that LLM-based assignment evaluators are
generally acceptable to students when students have free access to these
LLM-based evaluators. However, students also noted that the LLM sometimes fails
to adhere to the evaluation instructions. Additionally, we observe that
students can easily manipulate the LLM-based evaluator to output specific
strings, allowing them to achieve high scores without meeting the assignment
rubric. Based on student feedback and our experience, we provide several
recommendations for integrating LLM-based evaluators into future classrooms.","[{'name': 'Cheng-Han Chiang'}, {'name': 'Wei-Chih Chen'}, {'name': 'Chun-Yi Kuan'}, {'name': 'Chienchou Yang'}, {'name': 'Hung-yi Lee'}]",2024-07-07T00:17:24Z
http://arxiv.org/abs/2407.05213v1,http://arxiv.org/abs/2407.05213v1,"BadCLM: Backdoor Attack in Clinical Language Models for Electronic
  Health Records","The advent of clinical language models integrated into electronic health
records (EHR) for clinical decision support has marked a significant
advancement, leveraging the depth of clinical notes for improved
decision-making. Despite their success, the potential vulnerabilities of these
models remain largely unexplored. This paper delves into the realm of backdoor
attacks on clinical language models, introducing an innovative attention-based
backdoor attack method, BadCLM (Bad Clinical Language Models). This technique
clandestinely embeds a backdoor within the models, causing them to produce
incorrect predictions when a pre-defined trigger is present in inputs, while
functioning accurately otherwise. We demonstrate the efficacy of BadCLM through
an in-hospital mortality prediction task with MIMIC III dataset, showcasing its
potential to compromise model integrity. Our findings illuminate a significant
security risk in clinical decision support systems and pave the way for future
endeavors in fortifying clinical language models against such vulnerabilities.","[{'name': 'Weimin Lyu'}, {'name': 'Zexin Bi'}, {'name': 'Fusheng Wang'}, {'name': 'Chao Chen'}]",2024-07-06T23:56:43Z
http://arxiv.org/abs/2407.05194v1,http://arxiv.org/abs/2407.05194v1,"LLMCloudHunter: Harnessing LLMs for Automated Extraction of Detection
  Rules from Cloud-Based CTI","As the number and sophistication of cyber attacks have increased, threat
hunting has become a critical aspect of active security, enabling proactive
detection and mitigation of threats before they cause significant harm.
Open-source cyber threat intelligence (OS-CTI) is a valuable resource for
threat hunters, however, it often comes in unstructured formats that require
further manual analysis. Previous studies aimed at automating OSCTI analysis
are limited since (1) they failed to provide actionable outputs, (2) they did
not take advantage of images present in OSCTI sources, and (3) they focused on
on-premises environments, overlooking the growing importance of cloud
environments. To address these gaps, we propose LLMCloudHunter, a novel
framework that leverages large language models (LLMs) to automatically generate
generic-signature detection rule candidates from textual and visual OSCTI data.
We evaluated the quality of the rules generated by the proposed framework using
12 annotated real-world cloud threat reports. The results show that our
framework achieved a precision of 92% and recall of 98% for the task of
accurately extracting API calls made by the threat actor and a precision of 99%
with a recall of 98% for IoCs. Additionally, 99.18% of the generated detection
rule candidates were successfully compiled and converted into Splunk queries.","[{'name': 'Yuval Schwartz'}, {'name': 'Lavi Benshimol'}, {'name': 'Dudu Mimran'}, {'name': 'Yuval Elovici'}, {'name': 'Asaf Shabtai'}]",2024-07-06T21:43:35Z
http://arxiv.org/abs/2407.05189v1,http://arxiv.org/abs/2407.05189v1,"Enhancing Language Learning through Technology: Introducing a New
  English-Azerbaijani (Arabic Script) Parallel Corpus","This paper introduces a pioneering English-Azerbaijani (Arabic Script)
parallel corpus, designed to bridge the technological gap in language learning
and machine translation (MT) for under-resourced languages. Consisting of
548,000 parallel sentences and approximately 9 million words per language, this
dataset is derived from diverse sources such as news articles and holy texts,
aiming to enhance natural language processing (NLP) applications and language
education technology. This corpus marks a significant step forward in the realm
of linguistic resources, particularly for Turkic languages, which have lagged
in the neural machine translation (NMT) revolution. By presenting the first
comprehensive case study for the English-Azerbaijani (Arabic Script) language
pair, this work underscores the transformative potential of NMT in low-resource
contexts. The development and utilization of this corpus not only facilitate
the advancement of machine translation systems tailored for specific linguistic
needs but also promote inclusive language learning through technology. The
findings demonstrate the corpus's effectiveness in training deep learning MT
systems and underscore its role as an essential asset for researchers and
educators aiming to foster bilingual education and multilingual communication.
This research covers the way for future explorations into NMT applications for
languages lacking substantial digital resources, thereby enhancing global
language education frameworks. The Python package of our code is available at
https://pypi.org/project/chevir-kartalol/, and we also have a website
accessible at https://translate.kartalol.com/.","[{'name': 'Jalil Nourmohammadi Khiarak'}, {'name': 'Ammar Ahmadi'}, {'name': 'Taher Ak-bari Saeed'}, {'name': 'Meysam Asgari-Chenaghlu'}, {'name': 'Toğrul Atabay'}, {'name': 'Mohammad Reza Baghban Karimi'}, {'name': 'Ismail Ceferli'}, {'name': 'Farzad Hasanvand'}, {'name': 'Seyed Mahboub Mousavi'}, {'name': 'Morteza Noshad'}]",2024-07-06T21:23:20Z
http://arxiv.org/abs/2407.05154v1,http://arxiv.org/abs/2407.05154v1,"Identifying Intensity of the Structure and Content in Tweets and the
  Discriminative Power of Attributes in Context with Referential Translation
  Machines","We use referential translation machines (RTMs) to identify the similarity
between an attribute and two words in English by casting the task as machine
translation performance prediction (MTPP) between the words and the attribute
word and the distance between their similarities for Task 10 with stacked RTM
models. RTMs are also used to predict the intensity of the structure and
content in tweets in English, Arabic, and Spanish in Task 1 where MTPP is
between the tweets and the set of words for the emotion selected from WordNet
affect emotion lists. Stacked RTM models obtain encouraging results in both.",[{'name': 'Ergun Biçici'}],2024-07-06T18:58:10Z
http://arxiv.org/abs/2407.05134v1,http://arxiv.org/abs/2407.05134v1,"Solving for X and Beyond: Can Large Language Models Solve Complex Math
  Problems with More-Than-Two Unknowns?","Large Language Models (LLMs) have demonstrated remarkable performance in
solving math problems, a hallmark of human intelligence. Despite high success
rates on current benchmarks; however, these often feature simple problems with
only one or two unknowns, which do not sufficiently challenge their reasoning
capacities. This paper introduces a novel benchmark, BeyondX, designed to
address these limitations by incorporating problems with multiple unknowns.
Recognizing the challenges in proposing multi-unknown problems from scratch, we
developed BeyondX using an innovative automated pipeline that progressively
increases complexity by expanding the number of unknowns in simpler problems.
Empirical study on BeyondX reveals that the performance of existing LLMs, even
those fine-tuned specifically on math tasks, significantly decreases as the
number of unknowns increases - with a performance drop of up to 70\% observed
in GPT-4. To tackle these challenges, we propose the Formulate-and-Solve
strategy, a generalized prompting approach that effectively handles problems
with an arbitrary number of unknowns. Our findings reveal that this strategy
not only enhances LLM performance on the BeyondX benchmark but also provides
deeper insights into the computational limits of LLMs when faced with more
complex mathematical challenges.","[{'name': 'Kuei-Chun Kao'}, {'name': 'Ruochen Wang'}, {'name': 'Cho-Jui Hsieh'}]",2024-07-06T17:01:04Z
http://arxiv.org/abs/2407.05131v1,http://arxiv.org/abs/2407.05131v1,"RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language
  Models","The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has
enhanced medical diagnosis. However, current Med-LVLMs frequently encounter
factual issues, often generating responses that do not align with established
medical facts. Retrieval-Augmented Generation (RAG), which utilizes external
knowledge, can improve the factual accuracy of these models but introduces two
major challenges. First, limited retrieved contexts might not cover all
necessary information, while excessive retrieval can introduce irrelevant and
inaccurate references, interfering with the model's generation. Second, in
cases where the model originally responds correctly, applying RAG can lead to
an over-reliance on retrieved contexts, resulting in incorrect answers. To
address these issues, we propose RULE, which consists of two components. First,
we introduce a provably effective strategy for controlling factuality risk
through the calibrated selection of the number of retrieved contexts. Second,
based on samples where over-reliance on retrieved contexts led to errors, we
curate a preference dataset to fine-tune the model, balancing its dependence on
inherent knowledge and retrieved contexts for generation. We demonstrate the
effectiveness of RULE on three medical VQA datasets, achieving an average
improvement of 20.8% in factual accuracy. We publicly release our benchmark and
code in https://github.com/richard-peng-xia/RULE.","[{'name': 'Peng Xia'}, {'name': 'Kangyu Zhu'}, {'name': 'Haoran Li'}, {'name': 'Hongtu Zhu'}, {'name': 'Yun Li'}, {'name': 'Gang Li'}, {'name': 'Linjun Zhang'}, {'name': 'Huaxiu Yao'}]",2024-07-06T16:45:07Z
http://arxiv.org/abs/2407.05116v1,http://arxiv.org/abs/2407.05116v1,Automatic Prediction of the Performance of Every Parser,"We present a new parser performance prediction (PPP) model using machine
translation performance prediction system (MTPPS), statistically independent of
any language or parser, relying only on extrinsic and novel features based on
textual, link structural, and bracketing tree structural information. This new
system, MTPPS-PPP, can predict the performance of any parser in any language
and can be useful for estimating the grammatical difficulty when understanding
a given text, for setting expectations from parsing output, for parser
selection for a specific domain, and for parser combination systems. We obtain
SoA results in PPP of bracketing $F_1$ with better results over textual
features and similar performance with previous results that use parser and
linguistic label specific information. Our results show the contribution of
different types of features as well as rankings of individual features in
different experimental settings (cased vs. uncased), in different learning
tasks (in-domain vs. out-of-domain), with different training sets, with
different learning algorithms, and with different dimensionality reduction
techniques. We achieve $0.0678$ MAE and $0.85$ RAE in setting +Link, which
corresponds to about $7.4\%$ error when predicting the bracketing $F_1$ score
for the Charniak and Johnson parser on the WSJ23 test set. MTPPS-PPP system can
predict without parsing using only the text, without a supervised parser using
only an unsupervised parser, without any parser or language dependent
information, without using a reference parser output, and can be used to
predict the performance of any parser in any language.",[{'name': 'Ergun Biçici'}],2024-07-06T15:49:24Z
http://arxiv.org/abs/2407.05100v1,http://arxiv.org/abs/2407.05100v1,"Ask Questions with Double Hints: Visual Question Generation with
  Answer-awareness and Region-reference","The visual question generation (VQG) task aims to generate human-like
questions from an image and potentially other side information (e.g. answer
type). Previous works on VQG fall in two aspects: i) They suffer from one image
to many questions mapping problem, which leads to the failure of generating
referential and meaningful questions from an image. ii) They fail to model
complex implicit relations among the visual objects in an image and also
overlook potential interactions between the side information and image. To
address these limitations, we first propose a novel learning paradigm to
generate visual questions with answer-awareness and region-reference.
Concretely, we aim to ask the right visual questions with Double Hints -
textual answers and visual regions of interests, which could effectively
mitigate the existing one-to-many mapping issue. Particularly, we develop a
simple methodology to self-learn the visual hints without introducing any
additional human annotations. Furthermore, to capture these sophisticated
relationships, we propose a new double-hints guided Graph-to-Sequence learning
framework, which first models them as a dynamic graph and learns the implicit
topology end-to-end, and then utilizes a graph-to-sequence model to generate
the questions with double hints. Experimental results demonstrate the priority
of our proposed method.","[{'name': 'Kai Shen'}, {'name': 'Lingfei Wu'}, {'name': 'Siliang Tang'}, {'name': 'Fangli Xu'}, {'name': 'Bo Long'}, {'name': 'Yueting Zhuang'}, {'name': 'Jian Pei'}]",2024-07-06T15:07:32Z
http://arxiv.org/abs/2407.05092v1,http://arxiv.org/abs/2407.05092v1,"Exploring Sound Change Over Time: A Review of Computational and Human
  Perception","Computational and human perception are often considered separate approaches
for studying sound changes over time; few works have touched on the
intersection of both. To fill this research gap, we provide a pioneering review
contrasting computational with human perception from the perspectives of
methods and tasks. Overall, computational approaches rely on computer-driven
models to perceive historical sound changes on etymological datasets, while
human approaches use listener-driven models to perceive ongoing sound changes
on recording corpora. Despite their differences, both approaches complement
each other on phonetic and acoustic levels, showing the potential to achieve a
more comprehensive perception of sound change. Moreover, we call for a
comparative study on the datasets used by both approaches to investigate the
influence of historical sound changes on ongoing changes. Lastly, we discuss
the applications of sound change in computational linguistics, and point out
that perceiving sound change alone is insufficient, as many processes of
language change are complex, with entangled changes at syntactic, semantic, and
phonetic levels.","[{'name': 'Siqi He'}, {'name': 'Wei Zhao'}]",2024-07-06T14:44:59Z
http://arxiv.org/abs/2407.05054v1,http://arxiv.org/abs/2407.05054v1,"Cross-Lingual Word Alignment for ASEAN Languages with Contrastive
  Learning","Cross-lingual word alignment plays a crucial role in various natural language
processing tasks, particularly for low-resource languages. Recent study
proposes a BiLSTM-based encoder-decoder model that outperforms pre-trained
language models in low-resource settings. However, their model only considers
the similarity of word embedding spaces and does not explicitly model the
differences between word embeddings. To address this limitation, we propose
incorporating contrastive learning into the BiLSTM-based encoder-decoder
framework. Our approach introduces a multi-view negative sampling strategy to
learn the differences between word pairs in the shared cross-lingual embedding
space. We evaluate our model on five bilingual aligned datasets spanning four
ASEAN languages: Lao, Vietnamese, Thai, and Indonesian. Experimental results
demonstrate that integrating contrastive learning consistently improves word
alignment accuracy across all datasets, confirming the effectiveness of the
proposed method in low-resource scenarios. We will release our data set and
code to support future research on ASEAN or more low-resource word alignment.","[{'name': 'Jingshen Zhang'}, {'name': 'Xinying Qiu'}, {'name': 'Teng Shen'}, {'name': 'Wenyu Wang'}, {'name': 'Kailin Zhang'}, {'name': 'Wenhe Feng'}]",2024-07-06T11:56:41Z
http://arxiv.org/abs/2407.05036v1,http://arxiv.org/abs/2407.05036v1,Enhance the Robustness of Text-Centric Multimodal Alignments,"Converting different modalities into general text, serving as input prompts
for large language models (LLMs), is a common method to align multimodal models
when there is limited pairwise data. This text-centric approach leverages the
unique properties of text as a modality space, transforming diverse inputs into
a unified textual representation. This enables downstream models to effectively
interpret various modal inputs. This study assesses the quality and robustness
of multimodal representations in the presence of missing entries, noise, or
absent modalities, revealing that current text-centric alignment methods
compromise downstream robustness. To address this issue, we propose a new
text-centric approach that achieves superior robustness compared to previous
methods across various modalities in different settings. Our findings highlight
the potential of this approach to enhance the robustness and adaptability of
multimodal representations, offering a promising solution for dynamic and
real-world applications.","[{'name': 'Ting-Yu Yen'}, {'name': 'Yun-Da Tsai'}, {'name': 'Keng-Te Liao'}, {'name': 'Shou-De Lin'}]",2024-07-06T10:12:29Z
http://arxiv.org/abs/2407.05022v1,http://arxiv.org/abs/2407.05022v1,A Principled Framework for Evaluating on Typologically Diverse Languages,"Beyond individual languages, multilingual natural language processing (NLP)
research increasingly aims to develop models that perform well across languages
generally. However, evaluating these systems on all the world's languages is
practically infeasible. To attain generalizability, representative language
sampling is essential. Previous work argues that generalizable multilingual
evaluation sets should contain languages with diverse typological properties.
However, 'typologically diverse' language samples have been found to vary
considerably in this regard, and popular sampling methods are flawed and
inconsistent. We present a language sampling framework for selecting highly
typologically diverse languages given a sampling frame, informed by language
typology. We compare sampling methods with a range of metrics and find that our
systematic methods consistently retrieve more typologically diverse language
selections than previous methods in NLP. Moreover, we provide evidence that
this affects generalizability in multilingual model evaluation, emphasizing the
importance of diverse language sampling in NLP evaluation.","[{'name': 'Esther Ploeger'}, {'name': 'Wessel Poelman'}, {'name': 'Andreas Holck Høeg-Petersen'}, {'name': 'Anders Schlichtkrull'}, {'name': 'Miryam de Lhoneux'}, {'name': 'Johannes Bjerva'}]",2024-07-06T09:31:02Z
http://arxiv.org/abs/2407.05015v1,http://arxiv.org/abs/2407.05015v1,"How do you know that? Teaching Generative Language Models to Reference
  Answers to Biomedical Questions","Large language models (LLMs) have recently become the leading source of
answers for users' questions online. Despite their ability to offer eloquent
answers, their accuracy and reliability can pose a significant challenge. This
is especially true for sensitive domains such as biomedicine, where there is a
higher need for factually correct answers. This paper introduces a biomedical
retrieval-augmented generation (RAG) system designed to enhance the reliability
of generated responses. The system is based on a fine-tuned LLM for the
referenced question-answering, where retrieved relevant abstracts from PubMed
are passed to LLM's context as input through a prompt. Its output is an answer
based on PubMed abstracts, where each statement is referenced accordingly,
allowing the users to verify the answer. Our retrieval system achieves an
absolute improvement of 23% compared to the PubMed search engine. Based on the
manual evaluation on a small sample, our fine-tuned LLM component achieves
comparable results to GPT-4 Turbo in referencing relevant abstracts. We make
the dataset used to fine-tune the models and the fine-tuned models based on
Mistral-7B-instruct-v0.1 and v0.2 publicly available.","[{'name': 'Bojana Bašaragin'}, {'name': 'Adela Ljajić'}, {'name': 'Darija Medvecki'}, {'name': 'Lorenzo Cassano'}, {'name': 'Miloš Košprdić'}, {'name': 'Nikola Milošević'}]",2024-07-06T09:10:05Z
http://arxiv.org/abs/2407.05013v1,http://arxiv.org/abs/2407.05013v1,Progress or Regress? Self-Improvement Reversal in Post-training,"Self-improvement through post-training methods such as iterative preference
learning has been acclaimed for enhancing the problem-solving capabilities
(e.g., mathematical reasoning) of Large Language Models (LLMs) without human
intervention. However, as exploration deepens, it becomes crucial to assess
whether these improvements genuinely signify progress in solving more
challenging problems or if they could lead to unintended regressions. To
address this, we propose a comprehensive evaluative framework that goes beyond
the superficial pass@1 metric to scrutinize the underlying enhancements of
post-training paradigms for self-improvement. Through rigorous experimentation
and analysis across diverse problem-solving tasks, the empirical results point
out the phenomenon of \emph{self-improvement reversal}, where models showing
improved performance across benchmarks will paradoxically exhibit declines in
broader, essential capabilities, like output diversity and out-of-distribution
(OOD) generalization. These findings indicate that current self-improvement
practices through post-training are inadequate for equipping models to tackle
more complex problems. Furthermore, they underscore the necessity of our
critical evaluation metrics in discerning the \emph{progress or regress}
dichotomy for self-improving LLMs.","[{'name': 'Ting Wu'}, {'name': 'Xuefeng Li'}, {'name': 'Pengfei Liu'}]",2024-07-06T09:07:11Z
http://arxiv.org/abs/2407.18369v1,http://arxiv.org/abs/2407.18369v1,AI Safety in Generative AI Large Language Models: A Survey,"Large Language Model (LLMs) such as ChatGPT that exhibit generative AI
capabilities are facing accelerated adoption and innovation. The increased
presence of Generative AI (GAI) inevitably raises concerns about the risks and
safety associated with these models. This article provides an up-to-date survey
of recent trends in AI safety research of GAI-LLMs from a computer scientist's
perspective: specific and technical. In this survey, we explore the background
and motivation for the identified harms and risks in the context of LLMs being
generative language models; our survey differentiates by emphasising the need
for unified theories of the distinct safety challenges in the research
development and applications of LLMs. We start our discussion with a concise
introduction to the workings of LLMs, supported by relevant literature. Then we
discuss earlier research that has pointed out the fundamental constraints of
generative models, or lack of understanding thereof (e.g., performance and
safety trade-offs as LLMs scale in number of parameters). We provide a
sufficient coverage of LLM alignment -- delving into various approaches,
contending methods and present challenges associated with aligning LLMs with
human preferences. By highlighting the gaps in the literature and possible
implementation oversights, our aim is to create a comprehensive analysis that
provides insights for addressing AI safety in LLMs and encourages the
development of aligned and secure models. We conclude our survey by discussing
future directions of LLMs for AI safety, offering insights into ongoing
research in this critical area.","[{'name': 'Jaymari Chua'}, {'name': 'Yun Li'}, {'name': 'Shiyi Yang'}, {'name': 'Chen Wang'}, {'name': 'Lina Yao'}]",2024-07-06T09:00:18Z
http://arxiv.org/abs/2407.05006v1,http://arxiv.org/abs/2407.05006v1,"Recent Advancements and Challenges of Turkic Central Asian Language
  Processing","Research in the NLP sphere of the Turkic counterparts of Central Asian
languages, namely Kazakh, Uzbek, Kyrgyz, and Turkmen, comes with the typical
challenges of low-resource languages, like data scarcity and a general lack of
linguistic resources. However, in the recent years research has greatly
advanced via collection of language-specific datasets and development of
downstream task technologies. Aiming to summarize this research up until May
2024, this paper also seeks to identify potential areas of future work. To
achieve this, the paper gives a broad, high-level overview of the linguistic
properties of the languages, the current coverage and performance of already
developed technology, application of transfer learning techniques from
higher-resource languages, and availability of labeled and unlabeled data for
each language. Providing a summary of the current state of affairs, we hope
that further research will be facilitated with the considerations we provide in
the current paper.",[{'name': 'Yana Veitsman'}],2024-07-06T08:58:26Z
http://arxiv.org/abs/2407.05000v2,http://arxiv.org/abs/2407.05000v2,LoRA-GA: Low-Rank Adaptation with Gradient Approximation,"Fine-tuning large-scale pretrained models is prohibitively expensive in terms
of computational and memory costs. LoRA, as one of the most popular
Parameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective
alternative by fine-tuning an auxiliary low-rank model that has significantly
fewer parameters. Although LoRA reduces the computational and memory
requirements significantly at each iteration, extensive empirical evidence
indicates that it converges at a considerably slower rate compared to full
fine-tuning, ultimately leading to increased overall compute and often worse
test performance. In our paper, we perform an in-depth investigation of the
initialization method of LoRA and show that careful initialization (without any
change of the architecture and the training algorithm) can significantly
enhance both efficiency and performance. In particular, we introduce a novel
initialization method, LoRA-GA (Low Rank Adaptation with Gradient
Approximation), which aligns the gradients of low-rank matrix product with
those of full fine-tuning at the first step. Our extensive experiments
demonstrate that LoRA-GA achieves a convergence rate comparable to that of full
fine-tuning (hence being significantly faster than vanilla LoRA as well as
various recent improvements) while simultaneously attaining comparable or even
better performance. For example, on the subset of the GLUE dataset with
T5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as
Llama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05%
on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up
to 2-4 times convergence speed improvement compared to vanilla LoRA, validating
its effectiveness in accelerating convergence and enhancing model performance.
Code is available at https://github.com/Outsider565/LoRA-GA.","[{'name': 'Shaowen Wang'}, {'name': 'Linxi Yu'}, {'name': 'Jian Li'}]",2024-07-06T08:37:21Z
http://arxiv.org/abs/2407.04998v1,http://arxiv.org/abs/2407.04998v1,"The Solution for the 5th GCAIAC Zero-shot Referring Expression
  Comprehension Challenge","This report presents a solution for the zero-shot referring expression
comprehension task. Visual-language multimodal base models (such as CLIP, SAM)
have gained significant attention in recent years as a cornerstone of
mainstream research. One of the key applications of multimodal base models lies
in their ability to generalize to zero-shot downstream tasks. Unlike
traditional referring expression comprehension, zero-shot referring expression
comprehension aims to apply pre-trained visual-language models directly to the
task without specific training. Recent studies have enhanced the zero-shot
performance of multimodal base models in referring expression comprehension
tasks by introducing visual prompts. To address the zero-shot referring
expression comprehension challenge, we introduced a combination of visual
prompts and considered the influence of textual prompts, employing joint
prediction tailored to the data characteristics. Ultimately, our approach
achieved accuracy rates of 84.825 on the A leaderboard and 71.460 on the B
leaderboard, securing the first position.","[{'name': 'Longfei Huang'}, {'name': 'Feng Yu'}, {'name': 'Zhihao Guan'}, {'name': 'Zhonghua Wan'}, {'name': 'Yang Yang'}]",2024-07-06T08:31:33Z
http://arxiv.org/abs/2407.04991v1,http://arxiv.org/abs/2407.04991v1,The Solution for the AIGC Inference Performance Optimization Competition,"In recent years, the rapid advancement of large-scale pre-trained language
models based on transformer architectures has revolutionized natural language
processing tasks. Among these, ChatGPT has gained widespread popularity,
demonstrating human-level conversational abilities and attracting over 100
million monthly users by late 2022. Concurrently, Baidu's commercial deployment
of the Ernie Wenxin model has significantly enhanced marketing effectiveness
through AI-driven technologies. This paper focuses on optimizing
high-performance inference for Ernie models, emphasizing GPU acceleration and
leveraging the Paddle inference framework. We employ techniques such as Faster
Transformer for efficient model processing, embedding layer pruning to reduce
computational overhead, and FP16 half-precision inference for enhanced
computational efficiency. Additionally, our approach integrates efficient data
handling strategies using multi-process parallel processing to minimize
latency. Experimental results demonstrate that our optimized solution achieves
up to an 8.96x improvement in inference speed compared to standard methods,
while maintaining competitive performance.","[{'name': 'Sishun Pan'}, {'name': 'Haonan Xu'}, {'name': 'Zhonghua Wan'}, {'name': 'Yang Yang'}]",2024-07-06T07:54:45Z
http://arxiv.org/abs/2407.04990v1,http://arxiv.org/abs/2407.04990v1,"Conditional Semi-Supervised Data Augmentation for Spam Message Detection
  with Low Resource Data","Several machine learning schemes have attempted to perform the detection of
spam messages. However, those schemes mostly require a huge amount of labeled
data. The existing techniques addressing the lack of data availability have
issues with effectiveness and robustness. Therefore, this paper proposes a
conditional semi-supervised data augmentation (CSSDA) for a spam detection
model lacking the availability of data. The main architecture of CSSDA
comprises feature extraction and enhanced generative network. Here, we exploit
unlabeled data for data augmentation to extend training data. The enhanced
generative in our proposed scheme produces latent variables as fake samples
from unlabeled data through a conditional scheme. Latent variables can come
from labeled and unlabeled data as the input for the final classifier in our
spam detection model. The experimental results indicate that our proposed CSSDA
achieves excellent results compared to several related methods both exploiting
unlabeled data and not. In the experiment stage with various amounts of
unlabeled data, CSSDA is the only robust model that obtains a balanced accuracy
of about 85% when the availability of labeled data is large. We also conduct
several ablation studies to investigate our proposed scheme in detail. The
result also shows that several ablation studies strengthen our proposed
innovations. These experiments indicate that unlabeled data has a significant
contribution to data augmentation using the conditional semi-supervised scheme
for spam detection.","[{'name': 'Ulin Nuha'}, {'name': 'Chih-Hsueh Lin'}]",2024-07-06T07:51:24Z
http://arxiv.org/abs/2407.04981v1,http://arxiv.org/abs/2407.04981v1,"TRACE: TRansformer-based Attribution using Contrastive Embeddings in
  LLMs","The rapid evolution of large language models (LLMs) represents a substantial
leap forward in natural language understanding and generation. However,
alongside these advancements come significant challenges related to the
accountability and transparency of LLM responses. Reliable source attribution
is essential to adhering to stringent legal and regulatory standards, including
those set forth by the General Data Protection Regulation. Despite the
well-established methods in source attribution within the computer vision
domain, the application of robust attribution frameworks to natural language
processing remains underexplored. To bridge this gap, we propose a novel and
versatile TRansformer-based Attribution framework using Contrastive Embeddings
called TRACE that, in particular, exploits contrastive learning for source
attribution. We perform an extensive empirical evaluation to demonstrate the
performance and efficiency of TRACE in various settings and show that TRACE
significantly improves the ability to attribute sources accurately, making it a
valuable tool for enhancing the reliability and trustworthiness of LLMs.","[{'name': 'Cheng Wang'}, {'name': 'Xinyang Lu'}, {'name': 'See-Kiong Ng'}, {'name': 'Bryan Kian Hsiang Low'}]",2024-07-06T07:19:30Z
http://arxiv.org/abs/2407.12849v1,http://arxiv.org/abs/2407.12849v1,"Large language models are good medical coders, if provided with tools","This study presents a novel two-stage Retrieve-Rank system for automated
ICD-10-CM medical coding, comparing its performance against a Vanilla Large
Language Model (LLM) approach. Evaluating both systems on a dataset of 100
single-term medical conditions, the Retrieve-Rank system achieved 100% accuracy
in predicting correct ICD-10-CM codes, significantly outperforming the Vanilla
LLM (GPT-3.5-turbo), which achieved only 6% accuracy. Our analysis demonstrates
the Retrieve-Rank system's superior precision in handling various medical terms
across different specialties. While these results are promising, we acknowledge
the limitations of using simplified inputs and the need for further testing on
more complex, realistic medical cases. This research contributes to the ongoing
effort to improve the efficiency and accuracy of medical coding, highlighting
the importance of retrieval-based approaches.",[{'name': 'Keith Kwan'}],2024-07-06T06:58:51Z
http://arxiv.org/abs/2407.04973v1,http://arxiv.org/abs/2407.04973v1,"LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual
  Contexts","We propose LogicVista, an evaluation benchmark that assesses the integrated
logical reasoning capabilities of multimodal large language models (MLLMs) in
Visual contexts. Recent advancements in MLLMs have demonstrated various
fascinating abilities, from crafting poetry based on an image to performing
mathematical reasoning. However, there is still a lack of systematic evaluation
of MLLMs' proficiency in logical reasoning tasks, which are essential for
activities like navigation and puzzle-solving. Thus we evaluate general logical
cognition abilities across 5 logical reasoning tasks encompassing 9 different
capabilities, using a sample of 448 multiple-choice questions. Each question is
annotated with the correct answer and the human-written reasoning behind the
selection, enabling both open-ended and multiple-choice evaluation. A total of
8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Available
at https://github.com/Yijia-Xiao/LogicVista.","[{'name': 'Yijia Xiao'}, {'name': 'Edward Sun'}, {'name': 'Tianyu Liu'}, {'name': 'Wei Wang'}]",2024-07-06T06:48:16Z
http://arxiv.org/abs/2407.04969v1,http://arxiv.org/abs/2407.04969v1,"EVA-Score: Evaluation of Long-form Summarization on Informativeness
  through Extraction and Validation","Summarization is a fundamental task in natural language processing (NLP) and
since large language models (LLMs), such as GPT-4 and Claude, come out,
increasing attention has been paid to long-form summarization whose input
sequences are much longer, indicating more information contained.
  The current evaluation metrics either use similarity-based metrics like ROUGE
and BERTScore which rely on similarity and fail to consider informativeness or
LLM-based metrics, lacking quantitative analysis of information richness and
are rather subjective.
  In this paper, we propose a new evaluation metric called EVA-Score using
Atomic Fact Chain Generation and Document-level Relation Extraction together to
automatically calculate the informativeness and give a definite number as an
information score. Experiment results show that our metric shows a
state-of-the-art correlation with humans. We also re-evaluate the performance
of LLMs on long-form summarization comprehensively from the information aspect,
forecasting future ways to use LLMs for long-form summarization.","[{'name': 'Yuchen Fan'}, {'name': 'Xin Zhong'}, {'name': 'Chengsi Wang'}, {'name': 'Gaoche Wu'}, {'name': 'Bowen Zhou'}]",2024-07-06T06:02:38Z
http://arxiv.org/abs/2407.04965v2,http://arxiv.org/abs/2407.04965v2,"Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM
  Compression","Large language models (LLMs) are increasingly deployed in real-world
scenarios with the help of recent model compression techniques. Such momentum
towards local deployment means the use of compressed LLMs will widely impact a
large population. However, prior analysis works often prioritize on preserving
perplexity which is a direct analogy to training loss. The impact of
compression method on other critical aspects of model behavior, particularly
safety, still calls for a systematic assessment. To this end, we investigate
the impact of model compression on four dimensions: (1) degeneration harm,
i.e., bias and toxicity in generation; (2) representational harm, i.e., biases
in discriminative tasks; (3) dialect bias; (4) language modeling and downstream
task performance. We cover a wide spectrum of LLM compression techniques,
including unstructured pruning, semi-structured pruning and quantization. Our
analysis reveals that compression can lead to unexpected consequences. Although
compression may unintentionally remedy LLMs' degeneration harm, it can still
exacerbate on the representational harm axis. Although compression may
unintentionally remedy LLMs' degeneration harm, it can still exacerbate on the
representational harm axis. Moreover, there is a divergent impact on different
protected groups as the compression rate grows. Finally, different compression
methods have drastically different safety impacts, e.g., quantization mostly
preserves bias while pruning degrades quickly. Our findings underscore the
importance of integrating safety assessments into the development of compressed
LLMs to ensure their reliability across real-world applications. Our full
results are available here:
\url{https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval}","[{'name': 'Zhichao Xu'}, {'name': 'Ashim Gupta'}, {'name': 'Tao Li'}, {'name': 'Oliver Bentham'}, {'name': 'Vivek Srikumar'}]",2024-07-06T05:56:22Z
http://arxiv.org/abs/2407.12848v2,http://arxiv.org/abs/2407.12848v2,"Applicability of Large Language Models and Generative Models for Legal
  Case Judgement Summarization","Automatic summarization of legal case judgements, which are known to be long
and complex, has traditionally been tried via extractive summarization models.
In recent years, generative models including abstractive summarization models
and Large language models (LLMs) have gained huge popularity. In this paper, we
explore the applicability of such models for legal case judgement
summarization. We applied various domain specific abstractive summarization
models and general domain LLMs as well as extractive summarization models over
two sets of legal case judgements from the United Kingdom (UK) Supreme Court
and the Indian (IN) Supreme Court and evaluated the quality of the generated
summaries. We also perform experiments on a third dataset of legal documents of
a different type, Government reports from the United States (US). Results show
that abstractive summarization models and LLMs generally perform better than
the extractive methods as per traditional metrics for evaluating summary
quality. However, detailed investigation shows the presence of inconsistencies
and hallucinations in the outputs of the generative models, and we explore ways
to reduce the hallucinations and inconsistencies in the summaries. Overall, the
investigation suggests that further improvements are needed to enhance the
reliability of abstractive models and LLMs for legal case judgement
summarization. At present, a human-in-the-loop technique is more suitable for
performing manual checks to identify inconsistencies in the generated
summaries.","[{'name': 'Aniket Deroy'}, {'name': 'Kripabandhu Ghosh'}, {'name': 'Saptarshi Ghosh'}]",2024-07-06T04:49:40Z
http://arxiv.org/abs/2407.04952v1,http://arxiv.org/abs/2407.04952v1,Granular Privacy Control for Geolocation with Vision Language Models,"Vision Language Models (VLMs) are rapidly advancing in their capability to
answer information-seeking questions. As these models are widely deployed in
consumer applications, they could lead to new privacy risks due to emergent
abilities to identify people in photos, geolocate images, etc. As we
demonstrate, somewhat surprisingly, current open-source and proprietary VLMs
are very capable image geolocators, making widespread geolocation with VLMs an
immediate privacy risk, rather than merely a theoretical future concern. As a
first step to address this challenge, we develop a new benchmark, GPTGeoChat,
to test the ability of VLMs to moderate geolocation dialogues with users. We
collect a set of 1,000 image geolocation conversations between in-house
annotators and GPT-4v, which are annotated with the granularity of location
information revealed at each turn. Using this new dataset, we evaluate the
ability of various VLMs to moderate GPT-4v geolocation conversations by
determining when too much location information has been revealed. We find that
custom fine-tuned models perform on par with prompted API-based models when
identifying leaked location information at the country or city level; however,
fine-tuning on supervised data appears to be needed to accurately moderate
finer granularities, such as the name of a restaurant or building.","[{'name': 'Ethan Mendes'}, {'name': 'Yang Chen'}, {'name': 'James Hays'}, {'name': 'Sauvik Das'}, {'name': 'Wei Xu'}, {'name': 'Alan Ritter'}]",2024-07-06T04:06:55Z
http://arxiv.org/abs/2407.04923v1,http://arxiv.org/abs/2407.04923v1,"OmChat: A Recipe to Train Multimodal Language Models with Strong Long
  Context and Video Understanding","We introduce OmChat, a model designed to excel in handling long contexts and
video understanding tasks. OmChat's new architecture standardizes how different
visual inputs are processed, making it more efficient and adaptable. It uses a
dynamic vision encoding process to effectively handle images of various
resolutions, capturing fine details across a range of image qualities. OmChat
utilizes an active progressive multimodal pretraining strategy, which gradually
increases the model's capacity for long contexts and enhances its overall
abilities. By selecting high-quality data during training, OmChat learns from
the most relevant and informative data points. With support for a context
length of up to 512K, OmChat demonstrates promising performance in tasks
involving multiple images and videos, outperforming most open-source models in
these benchmarks. Additionally, OmChat proposes a prompting strategy for
unifying complex multimodal inputs including single image text, multi-image
text and videos, and achieving competitive performance on single-image
benchmarks. To further evaluate the model's capabilities, we proposed a
benchmark dataset named Temporal Visual Needle in a Haystack. This dataset
assesses OmChat's ability to comprehend temporal visual details within long
videos. Our analysis highlights several key factors contributing to OmChat's
success: support for any-aspect high image resolution, the active progressive
pretraining strategy, and high-quality supervised fine-tuning datasets. This
report provides a detailed overview of OmChat's capabilities and the strategies
that enhance its performance in visual understanding.","[{'name': 'Tiancheng Zhao'}, {'name': 'Qianqian Zhang'}, {'name': 'Kyusong Lee'}, {'name': 'Peng Liu'}, {'name': 'Lu Zhang'}, {'name': 'Chunxin Fang'}, {'name': 'Jiajia Liao'}, {'name': 'Kelei Jiang'}, {'name': 'Yibo Ma'}, {'name': 'Ruochen Xu'}]",2024-07-06T02:16:10Z
http://arxiv.org/abs/2407.04910v1,http://arxiv.org/abs/2407.04910v1,NADI 2024: The Fifth Nuanced Arabic Dialect Identification Shared Task,"We describe the findings of the fifth Nuanced Arabic Dialect Identification
Shared Task (NADI 2024). NADI's objective is to help advance SoTA Arabic NLP by
providing guidance, datasets, modeling opportunities, and standardized
evaluation conditions that allow researchers to collaboratively compete on
pre-specified tasks. NADI 2024 targeted both dialect identification cast as a
multi-label task (Subtask~1), identification of the Arabic level of dialectness
(Subtask~2), and dialect-to-MSA machine translation (Subtask~3). A total of 51
unique teams registered for the shared task, of whom 12 teams have participated
(with 76 valid submissions during the test phase). Among these, three teams
participated in Subtask~1, three in Subtask~2, and eight in Subtask~3. The
winning teams achieved 50.57 F\textsubscript{1} on Subtask~1, 0.1403 RMSE for
Subtask~2, and 20.44 BLEU in Subtask~3, respectively. Results show that Arabic
dialect processing tasks such as dialect identification and machine translation
remain challenging. We describe the methods employed by the participating teams
and briefly offer an outlook for NADI.","[{'name': 'Muhammad Abdul-Mageed'}, {'name': 'Amr Keleg'}, {'name': 'AbdelRahim Elmadany'}, {'name': 'Chiyu Zhang'}, {'name': 'Injy Hamed'}, {'name': 'Walid Magdy'}, {'name': 'Houda Bouamor'}, {'name': 'Nizar Habash'}]",2024-07-06T01:18:58Z
http://arxiv.org/abs/2407.04903v1,http://arxiv.org/abs/2407.04903v1,"MMSci: A Multimodal Multi-Discipline Dataset for PhD-Level Scientific
  Comprehension","The rapid advancement of Large Language Models (LLMs) and Large Multimodal
Models (LMMs) has heightened the demand for AI-based scientific assistants
capable of understanding scientific articles and figures. Despite progress,
there remains a significant gap in evaluating models' comprehension of
professional, graduate-level, and even PhD-level scientific content. Current
datasets and benchmarks primarily focus on relatively simple scientific tasks
and figures, lacking comprehensive assessments across diverse advanced
scientific disciplines. To bridge this gap, we collected a multimodal,
multidisciplinary dataset from open-access scientific articles published in
Nature Communications journals. This dataset spans 72 scientific disciplines,
ensuring both diversity and quality. We created benchmarks with various tasks
and settings to comprehensively evaluate LMMs' capabilities in understanding
scientific figures and content. Our evaluation revealed that these tasks are
highly challenging: many open-source models struggled significantly, and even
GPT-4V and GPT-4o faced difficulties. We also explored using our dataset as
training resources by constructing visual instruction-following data, enabling
the 7B LLaVA model to achieve performance comparable to GPT-4V/o on our
benchmark. Additionally, we investigated the use of our interleaved article
texts and figure images for pre-training LMMs, resulting in improvements on the
material generation task. The source dataset, including articles, figures,
constructed benchmarks, and visual instruction-following data, is open-sourced.","[{'name': 'Zekun Li'}, {'name': 'Xianjun Yang'}, {'name': 'Kyuri Choi'}, {'name': 'Wanrong Zhu'}, {'name': 'Ryan Hsieh'}, {'name': 'HyeonJung Kim'}, {'name': 'Jin Hyuk Lim'}, {'name': 'Sungyoung Ji'}, {'name': 'Byungju Lee'}, {'name': 'Xifeng Yan'}, {'name': 'Linda Ruth Petzold'}, {'name': 'Stephen D. Wilson'}, {'name': 'Woosang Lim'}, {'name': 'William Yang Wang'}]",2024-07-06T00:40:53Z
http://arxiv.org/abs/2407.04899v1,http://arxiv.org/abs/2407.04899v1,Algorithmic Language Models with Neurally Compiled Libraries,"Important tasks such as reasoning and planning are fundamentally algorithmic,
meaning that solving them robustly requires acquiring true reasoning or
planning algorithms, rather than shortcuts. Large Language Models lack true
algorithmic ability primarily because of the limitations of neural network
optimization algorithms, their optimization data and optimization objective,
but also due to architectural inexpressivity. To solve this, our paper proposes
augmenting LLMs with a library of fundamental operations and sophisticated
differentiable programs, so that common algorithms do not need to be learned
from scratch. We add memory, registers, basic operations, and adaptive
recurrence to a transformer architecture built on LLaMA3. Then, we define a
method for directly compiling algorithms into a differentiable starting
library, which is used natively and propagates gradients for optimization. In
this preliminary study, we explore the feasability of augmenting LLaMA3 with a
differentiable computer, for instance by fine-tuning small transformers on
simple algorithmic tasks with variable computational depth.","[{'name': 'Lucas Saldyt'}, {'name': 'Subbarao Kambhampati'}]",2024-07-06T00:27:05Z
http://arxiv.org/abs/2407.04885v1,http://arxiv.org/abs/2407.04885v1,"Automating Venture Capital: Founder assessment using LLM-powered
  segmentation, feature engineering and automated labeling techniques","This study explores the application of large language models (LLMs) in
venture capital (VC) decision-making, focusing on predicting startup success
based on founder characteristics. We utilize LLM prompting techniques, like
chain-of-thought, to generate features from limited data, then extract insights
through statistics and machine learning. Our results reveal potential
relationships between certain founder characteristics and success, as well as
demonstrate the effectiveness of these characteristics in prediction. This
framework for integrating ML techniques and LLMs has vast potential for
improving startup success prediction, with important implications for VC firms
seeking to optimize their investment strategies.","[{'name': 'Ekin Ozince'}, {'name': 'Yiğit Ihlamur'}]",2024-07-05T22:54:13Z
http://arxiv.org/abs/2407.17503v1,http://arxiv.org/abs/2407.17503v1,"Challenges and Considerations in Annotating Legal Data: A Comprehensive
  Overview","The process of annotating data within the legal sector is filled with
distinct challenges that differ from other fields, primarily due to the
inherent complexities of legal language and documentation. The initial task
usually involves selecting an appropriate raw dataset that captures the
intricate aspects of legal texts. Following this, extracting text becomes a
complicated task, as legal documents often have complex structures, footnotes,
references, and unique terminology. The importance of data cleaning is
magnified in this context, ensuring that redundant information is eliminated
while maintaining crucial legal details and context. Creating comprehensive yet
straightforward annotation guidelines is imperative, as these guidelines serve
as the road map for maintaining uniformity and addressing the subtle nuances of
legal terminology. Another critical aspect is the involvement of legal
professionals in the annotation process. Their expertise is valuable in
ensuring that the data not only remains contextually accurate but also adheres
to prevailing legal standards and interpretations. This paper provides an
expanded view of these challenges and aims to offer a foundational
understanding and guidance for researchers and professionals engaged in legal
data annotation projects. In addition, we provide links to our created and
fine-tuned datasets and language models. These resources are outcomes of our
discussed projects and solutions to challenges faced while working on them.","[{'name': 'Harshil Darji'}, {'name': 'Jelena Mitrović'}, {'name': 'Michael Granitzer'}]",2024-07-05T21:56:28Z
http://arxiv.org/abs/2407.04858v1,http://arxiv.org/abs/2407.04858v1,"Question Answering with Texts and Tables through Deep Reinforcement
  Learning","This paper proposes a novel architecture to generate multi-hop answers to
open domain questions that require information from texts and tables, using the
Open Table-and-Text Question Answering dataset for validation and training. One
of the most common ways to generate answers in this setting is to retrieve
information sequentially, where a selected piece of data helps searching for
the next piece. As different models can have distinct behaviors when called in
this sequential information search, a challenge is how to select models at each
step. Our architecture employs reinforcement learning to choose between
different state-of-the-art tools sequentially until, in the end, a desired
answer is generated. This system achieved an F1-score of 19.03, comparable to
iterative systems in the literature.","[{'name': 'Marcos M. José'}, {'name': 'Flávio N. Cação'}, {'name': 'Maria F. Ribeiro'}, {'name': 'Rafael M. Cheang'}, {'name': 'Paulo Pirozelli'}, {'name': 'Fabio G. Cozman'}]",2024-07-05T20:44:01Z
http://arxiv.org/abs/2407.04855v1,http://arxiv.org/abs/2407.04855v1,"Towards Enhancing Coherence in Extractive Summarization: Dataset and
  Experiments with LLMs","Extractive summarization plays a pivotal role in natural language processing
due to its wide-range applications in summarizing diverse content efficiently,
while also being faithful to the original content. Despite significant
advancement achieved in extractive summarization by Large Language Models
(LLMs), these summaries frequently exhibit incoherence. An important aspect of
the coherent summary is its readability for intended users. Although there have
been many datasets and benchmarks proposed for creating coherent extractive
summaries, none of them currently incorporate user intent to improve coherence
in extractive summarization. Motivated by this, we propose a systematically
created human-annotated dataset consisting of coherent summaries for five
publicly available datasets and natural language user feedback, offering
valuable insights into how to improve coherence in extractive summaries. We
utilize this dataset for aligning LLMs through supervised fine-tuning with
natural language human feedback to enhance the coherence of their generated
summaries. Preliminary experiments with Falcon-40B and Llama-2-13B show
significant performance improvements (~10% Rouge-L) in terms of producing
coherent summaries. We further utilize human feedback to benchmark results over
instruction-tuned models such as FLAN-T5 which resulted in several interesting
findings. Data and source code are available at
https://github.com/Mihir3009/Extract-AI.","[{'name': 'Mihir Parmar'}, {'name': 'Hanieh Deilamsalehy'}, {'name': 'Franck Dernoncourt'}, {'name': 'Seunghyun Yoon'}, {'name': 'Ryan A. Rossi'}, {'name': 'Trung Bui'}]",2024-07-05T20:25:04Z
http://arxiv.org/abs/2407.04854v1,http://arxiv.org/abs/2407.04854v1,"Statistical investigations into the geometry and homology of random
  programs","AI-supported programming has taken giant leaps with tools such as Meta's
Llama and openAI's chatGPT. These are examples of stochastic sources of
programs and have already greatly influenced how we produce code and teach
programming. If we consider input to such models as a stochastic source, a
natural question is, what is the relation between the input and the output
distributions, between the chatGPT prompt and the resulting program?
  In this paper, we will show how the relation between random Python programs
generated from chatGPT can be described geometrically and topologically using
Tree-edit distances between the program's syntax trees and without explicit
modeling of the underlying space. A popular approach to studying
high-dimensional samples in a metric space is to use low-dimensional embedding
using, e.g., multidimensional scaling. Such methods imply errors depending on
the data and dimension of the embedding space. In this article, we propose to
restrict such projection methods to purely visualization purposes and instead
use geometric summary statistics, methods from spatial point statistics, and
topological data analysis to characterize the configurations of random programs
that do not rely on embedding approximations. To demonstrate their usefulness,
we compare two publicly available models: ChatGPT-4 and TinyLlama, on a simple
problem related to image processing.
  Application areas include understanding how questions should be asked to
obtain useful programs; measuring how consistently a given large language model
answers; and comparing the different large language models as a programming
assistant. Finally, we speculate that our approach may in the future give new
insights into the structure of programming languages.","[{'name': 'Jon Sporring'}, {'name': 'Ken Friis Larsen'}]",2024-07-05T20:25:02Z
http://arxiv.org/abs/2407.04842v1,http://arxiv.org/abs/2407.04842v1,"MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for
  Text-to-Image Generation?","While text-to-image models like DALLE-3 and Stable Diffusion are rapidly
proliferating, they often encounter challenges such as hallucination, bias, and
the production of unsafe, low-quality output. To effectively address these
issues, it is crucial to align these models with desired behaviors based on
feedback from a multimodal judge. Despite their significance, current
multimodal judges frequently undergo inadequate evaluation of their
capabilities and limitations, potentially leading to misalignment and unsafe
fine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel
benchmark which incorporates a comprehensive preference dataset to evaluate
multimodal judges in providing feedback for image generation models across four
key perspectives: alignment, safety, image quality, and bias. Specifically, we
evaluate a large variety of multimodal judges including smaller-sized
CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and
close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our
preference dataset. Experiments reveal that close-source VLMs generally provide
better feedback, with GPT-4o outperforming other judges in average. Compared
with open-source VLMs, smaller-sized scoring models can provide better feedback
regarding text-image alignment and image quality, while VLMs provide more
accurate feedback regarding safety and generation bias due to their stronger
reasoning capabilities. Further studies in feedback scale reveal that VLM
judges can generally provide more accurate and stable feedback in natural
language (Likert-scale) than numerical scales. Notably, human evaluations on
end-to-end fine-tuned models using separate feedback from these multimodal
judges provide similar conclusions, further confirming the effectiveness of
MJ-Bench. All data, code, models are available at
https://huggingface.co/MJ-Bench.","[{'name': 'Zhaorun Chen'}, {'name': 'Yichao Du'}, {'name': 'Zichen Wen'}, {'name': 'Yiyang Zhou'}, {'name': 'Chenhang Cui'}, {'name': 'Zhenzhen Weng'}, {'name': 'Haoqin Tu'}, {'name': 'Chaoqi Wang'}, {'name': 'Zhengwei Tong'}, {'name': 'Qinglan Huang'}, {'name': 'Canyu Chen'}, {'name': 'Qinghao Ye'}, {'name': 'Zhihong Zhu'}, {'name': 'Yuqing Zhang'}, {'name': 'Jiawei Zhou'}, {'name': 'Zhuokai Zhao'}, {'name': 'Rafael Rafailov'}, {'name': 'Chelsea Finn'}, {'name': 'Huaxiu Yao'}]",2024-07-05T20:03:16Z
http://arxiv.org/abs/2407.04841v1,http://arxiv.org/abs/2407.04841v1,Associative Recurrent Memory Transformer,"This paper addresses the challenge of creating a neural architecture for very
long sequences that requires constant time for processing new information at
each time step. Our approach, Associative Recurrent Memory Transformer (ARMT),
is based on transformer self-attention for local context and segment-level
recurrence for storage of task specific information distributed over a long
context. We demonstrate that ARMT outperfors existing alternatives in
associative retrieval tasks and sets a new performance record in the recent
BABILong multi-task long-context benchmark by answering single-fact questions
over 50 million tokens with an accuracy of 79.9%. The source code for training
and evaluation is available on github.","[{'name': 'Ivan Rodkin'}, {'name': 'Yuri Kuratov'}, {'name': 'Aydar Bulatov'}, {'name': 'Mikhail Burtsev'}]",2024-07-05T19:57:49Z
http://arxiv.org/abs/2407.04801v1,http://arxiv.org/abs/2407.04801v1,"Revisiting Structured Sentiment Analysis as Latent Dependency Graph
  Parsing","Structured Sentiment Analysis (SSA) was cast as a problem of bi-lexical
dependency graph parsing by prior studies. Multiple formulations have been
proposed to construct the graph, which share several intrinsic drawbacks: (1)
The internal structures of spans are neglected, thus only the boundary tokens
of spans are used for relation prediction and span recognition, thus hindering
the model's expressiveness; (2) Long spans occupy a significant proportion in
the SSA datasets, which further exacerbates the problem of internal structure
neglect. In this paper, we treat the SSA task as a dependency parsing task on
partially-observed dependency trees, regarding flat spans without determined
tree annotations as latent subtrees to consider internal structures of spans.
We propose a two-stage parsing method and leverage TreeCRFs with a novel
constrained inside algorithm to model latent structures explicitly, which also
takes advantages of joint scoring graph arcs and headed spans for global
optimization and inference. Results of extensive experiments on five benchmark
datasets reveal that our method performs significantly better than all previous
bi-lexical methods, achieving new state-of-the-art.","[{'name': 'Chengjie Zhou'}, {'name': 'Bobo Li'}, {'name': 'Hao Fei'}, {'name': 'Fei Li'}, {'name': 'Chong Teng'}, {'name': 'Donghong Ji'}]",2024-07-05T18:18:50Z
http://arxiv.org/abs/2407.04693v1,http://arxiv.org/abs/2407.04693v1,"ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language
  Models","Large language models (LLMs) exhibit hallucinations in long-form
question-answering tasks across various domains and wide applications. Current
hallucination detection and mitigation datasets are limited in domains and
sizes, which struggle to scale due to prohibitive labor costs and insufficient
reliability of existing hallucination annotators. To facilitate the scalable
oversight of LLM hallucinations, this paper introduces an iterative
self-training framework that simultaneously and progressively scales up the
hallucination annotation dataset and improves the accuracy of the hallucination
annotator. Based on the Expectation Maximization (EM) algorithm, in each
iteration, the framework first applies a hallucination annotation pipeline to
annotate a scaled dataset and then trains a more accurate hallucination
annotator on the dataset. This new hallucination annotator is adopted in the
hallucination annotation pipeline used for the next iteration. Extensive
experimental results demonstrate that the finally obtained hallucination
annotator with only 7B parameters surpasses the performance of GPT-4 and
obtains new state-of-the-art hallucination detection results on HaluEval and
HalluQA by zero-shot inference. Such an annotator can not only evaluate the
hallucination levels of various LLMs on the large-scale dataset but also help
to mitigate the hallucination of LLMs generations, with the Natural Language
Inference (NLI) metric increasing from 25% to 37% on HaluEval.","[{'name': 'Yuzhe Gu'}, {'name': 'Ziwei Ji'}, {'name': 'Wenwei Zhang'}, {'name': 'Chengqi Lyu'}, {'name': 'Dahua Lin'}, {'name': 'Kai Chen'}]",2024-07-05T17:56:38Z
http://arxiv.org/abs/2407.04690v1,http://arxiv.org/abs/2407.04690v1,"Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for
  Interpreting Neural Networks","Interpretability research takes counterfactual theories of causality for
granted. Most causal methods rely on counterfactual interventions to inputs or
the activations of particular model components, followed by observations of the
change in models' output logits or behaviors. While this yields more faithful
evidence than correlational methods, counterfactuals nonetheless have key
problems that bias our findings in specific and predictable ways. Specifically,
(i) counterfactual theories do not effectively capture multiple independently
sufficient causes of the same effect, which leads us to miss certain causes
entirely; and (ii) counterfactual dependencies in neural networks are generally
not transitive, which complicates methods for extracting and interpreting
causal graphs from neural networks. We discuss the implications of these
challenges for interpretability researchers and propose concrete suggestions
for future work.",[{'name': 'Aaron Mueller'}],2024-07-05T17:53:03Z
http://arxiv.org/abs/2407.04681v1,http://arxiv.org/abs/2407.04681v1,"Rethinking Visual Prompting for Multimodal Large Language Models with
  External Knowledge","In recent years, multimodal large language models (MLLMs) have made
significant strides by training on vast high-quality image-text datasets,
enabling them to generally understand images well. However, the inherent
difficulty in explicitly conveying fine-grained or spatially dense information
in text, such as masks, poses a challenge for MLLMs, limiting their ability to
answer questions requiring an understanding of detailed or localized visual
elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG)
concept, this paper proposes a new visual prompt approach to integrate
fine-grained external knowledge, gleaned from specialized vision models (e.g.,
instance segmentation/OCR models), into MLLMs. This is a promising yet
underexplored direction for enhancing MLLMs' performance. Our approach diverges
from concurrent works, which transform external knowledge into additional text
prompts, necessitating the model to indirectly learn the correspondence between
visual content and text coordinates. Instead, we propose embedding fine-grained
knowledge information directly into a spatial embedding map as a visual prompt.
This design can be effortlessly incorporated into various MLLMs, such as LLaVA
and Mipha, considerably improving their visual understanding performance.
Through rigorous experiments, we demonstrate that our method can enhance MLLM
performance across nine benchmarks, amplifying their fine-grained context-aware
capabilities.","[{'name': 'Yuanze Lin'}, {'name': 'Yunsheng Li'}, {'name': 'Dongdong Chen'}, {'name': 'Weijian Xu'}, {'name': 'Ronald Clark'}, {'name': 'Philip Torr'}, {'name': 'Lu Yuan'}]",2024-07-05T17:43:30Z
http://arxiv.org/abs/2407.04680v1,http://arxiv.org/abs/2407.04680v1,Lost in Translation: The Algorithmic Gap Between LMs and the Brain,"Language Models (LMs) have achieved impressive performance on various
linguistic tasks, but their relationship to human language processing in the
brain remains unclear. This paper examines the gaps and overlaps between LMs
and the brain at different levels of analysis, emphasizing the importance of
looking beyond input-output behavior to examine and compare the internal
processes of these systems. We discuss how insights from neuroscience, such as
sparsity, modularity, internal states, and interactive learning, can inform the
development of more biologically plausible language models. Furthermore, we
explore the role of scaling laws in bridging the gap between LMs and human
cognition, highlighting the need for efficiency constraints analogous to those
in biological systems. By developing LMs that more closely mimic brain
function, we aim to advance both artificial intelligence and our understanding
of human cognition.","[{'name': 'Tommaso Tosato'}, {'name': 'Pascal Jr Tikeng Notsawo'}, {'name': 'Saskia Helbling'}, {'name': 'Irina Rish'}, {'name': 'Guillaume Dumas'}]",2024-07-05T17:43:16Z
http://arxiv.org/abs/2407.04652v1,http://arxiv.org/abs/2407.04652v1,"Pretraining End-to-End Keyword Search with Automatically Discovered
  Acoustic Units","End-to-end (E2E) keyword search (KWS) has emerged as an alternative and
complimentary approach to conventional keyword search which depends on the
output of automatic speech recognition (ASR) systems. While E2E methods greatly
simplify the KWS pipeline, they generally have worse performance than their
ASR-based counterparts, which can benefit from pretraining with untranscribed
data. In this work, we propose a method for pretraining E2E KWS systems with
untranscribed data, which involves using acoustic unit discovery (AUD) to
obtain discrete units for untranscribed data and then learning to locate
sequences of such units in the speech. We conduct experiments across languages
and AUD systems: we show that finetuning such a model significantly outperforms
a model trained from scratch, and the performance improvements are generally
correlated with the quality of the AUD system used for pretraining.","[{'name': 'Bolaji Yusuf'}, {'name': 'Jan ""Honza"" Černocký'}, {'name': 'Murat Saraçlar'}]",2024-07-05T17:07:58Z
http://arxiv.org/abs/2407.04641v1,http://arxiv.org/abs/2407.04641v1,"Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of
  Language Models","This paper explores speculative speech recognition (SSR), where we empower
conventional automatic speech recognition (ASR) with speculation capabilities,
allowing the recognizer to run ahead of audio. We introduce a metric for
measuring SSR performance and we propose a model which does SSR by combining a
RNN-Transducer-based ASR system with an audio-prefixed language model (LM). The
ASR system transcribes ongoing audio and feeds the resulting transcripts, along
with an audio-dependent prefix, to the LM, which speculates likely completions
for the transcriptions. We experiment with a variety of ASR datasets on which
show the efficacy our method and the feasibility of SSR as a method of reducing
ASR latency.","[{'name': 'Bolaji Yusuf'}, {'name': 'Murali Karthick Baskar'}, {'name': 'Andrew Rosenberg'}, {'name': 'Bhuvana Ramabhadran'}]",2024-07-05T16:52:55Z
http://arxiv.org/abs/2407.04629v1,http://arxiv.org/abs/2407.04629v1,"Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity
  Recognition Framework","Clinical named entity recognition (NER) aims to retrieve important entities
within clinical narratives. Recent works have demonstrated that large language
models (LLMs) can achieve strong performance in this task. While previous works
focus on proprietary LLMs, we investigate how open NER LLMs, trained
specifically for entity recognition, perform in clinical NER. In this paper, we
aim to improve them through a novel framework, entity decomposition with
filtering, or EDF. Our key idea is to decompose the entity recognition task
into several retrievals of sub-entity types. We also introduce a filtering
mechanism to remove incorrect entities. Our experimental results demonstrate
the efficacy of our framework across all metrics, models, datasets, and entity
types. Our analysis reveals that entity decomposition can recognize previously
missed entities with substantial improvement. We further provide a
comprehensive evaluation of our framework and an in-depth error analysis to
pave future works.","[{'name': 'Reza Averly'}, {'name': 'Xia Ning'}]",2024-07-05T16:38:23Z
http://arxiv.org/abs/2407.04620v2,http://arxiv.org/abs/2407.04620v2,Learning to (Learn at Test Time): RNNs with Expressive Hidden States,"Self-attention performs well in long context but has quadratic complexity.
Existing RNN layers have linear complexity, but their performance in long
context is limited by the expressive power of their hidden state. We propose a
new class of sequence modeling layers with linear complexity and an expressive
hidden state. The key idea is to make the hidden state a machine learning model
itself, and the update rule a step of self-supervised learning. Since the
hidden state is updated by training even on test sequences, our layers are
called Test-Time Training (TTT) layers. We consider two instantiations:
TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer
MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B
parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both
TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer,
they can keep reducing perplexity by conditioning on more tokens, while Mamba
cannot after 16k context. With preliminary systems optimization, TTT-Linear is
already faster than Transformer at 8k context and matches Mamba in wall-clock
time. TTT-MLP still faces challenges in memory I/O, but shows larger potential
in long context, pointing to a promising direction for future research.","[{'name': 'Yu Sun'}, {'name': 'Xinhao Li'}, {'name': 'Karan Dalal'}, {'name': 'Jiarui Xu'}, {'name': 'Arjun Vikram'}, {'name': 'Genghan Zhang'}, {'name': 'Yann Dubois'}, {'name': 'Xinlei Chen'}, {'name': 'Xiaolong Wang'}, {'name': 'Sanmi Koyejo'}, {'name': 'Tatsunori Hashimoto'}, {'name': 'Carlos Guestrin'}]",2024-07-05T16:23:20Z
http://arxiv.org/abs/2407.04615v1,http://arxiv.org/abs/2407.04615v1,ARM: Efficient Guided Decoding with Autoregressive Reward Models,"Language models trained on large amounts of data require careful tuning to be
safely deployed in real world. We revisit the guided decoding paradigm, where
the goal is to augment the logits of the base language model using the scores
from a task-specific reward model. We propose a simple but efficient
parameterization of the autoregressive reward model enabling fast and effective
guided decoding. On detoxification and sentiment control tasks, we show that
our efficient parameterization performs on par with RAD, a strong but less
efficient guided decoding approach.","[{'name': 'Sergey Troshin'}, {'name': 'Vlad Niculae'}, {'name': 'Antske Fokkens'}]",2024-07-05T16:11:03Z
http://arxiv.org/abs/2407.14521v1,http://arxiv.org/abs/2407.14521v1,"Towards Automated Functional Equation Proving: A Benchmark Dataset and A
  Domain-Specific In-Context Agent","Automated Theorem Proving (ATP) faces challenges due to its complexity and
computational demands. Recent work has explored using Large Language Models
(LLMs) for ATP action selection, but these methods can be resource-intensive.
This study introduces FEAS, an agent that enhances the COPRA in-context
learning framework within Lean. FEAS refines prompt generation, response
parsing, and incorporates domain-specific heuristics for functional equations.
It introduces FunEq, a curated dataset of functional equation problems with
varying difficulty. FEAS outperforms baselines on FunEq, particularly with the
integration of domain-specific heuristics. The results demonstrate FEAS's
effectiveness in generating and formalizing high-level proof strategies into
Lean proofs, showcasing the potential of tailored approaches for specific ATP
challenges.","[{'name': 'Mahdi Buali'}, {'name': 'Robert Hoehndorf'}]",2024-07-05T15:59:16Z
http://arxiv.org/abs/2407.04601v1,http://arxiv.org/abs/2407.04601v1,Written Term Detection Improves Spoken Term Detection,"End-to-end (E2E) approaches to keyword search (KWS) are considerably simpler
in terms of training and indexing complexity when compared to approaches which
use the output of automatic speech recognition (ASR) systems. This
simplification however has drawbacks due to the loss of modularity. In
particular, where ASR-based KWS systems can benefit from external unpaired text
via a language model, current formulations of E2E KWS systems have no such
mechanism. Therefore, in this paper, we propose a multitask training objective
which allows unpaired text to be integrated into E2E KWS without complicating
indexing and search. In addition to training an E2E KWS model to retrieve text
queries from spoken documents, we jointly train it to retrieve text queries
from masked written documents. We show empirically that this approach can
effectively leverage unpaired text for KWS, with significant improvements in
search performance across a wide variety of languages. We conduct analysis
which indicates that these improvements are achieved because the proposed
method improves document representations for words in the unpaired text.
Finally, we show that the proposed method can be used for domain adaptation in
settings where in-domain paired data is scarce or nonexistent.","[{'name': 'Bolaji Yusuf'}, {'name': 'Murat Saraçlar'}]",2024-07-05T15:50:47Z
http://arxiv.org/abs/2407.04593v1,http://arxiv.org/abs/2407.04593v1,"Testing learning hypotheses using neural networks by manipulating
  learning data","Although passivization is productive in English, it is not completely general
-- some exceptions exist (e.g. *One hour was lasted by the meeting). How do
English speakers learn these exceptions to an otherwise general pattern? Using
neural network language models as theories of acquisition, we explore the
sources of indirect evidence that a learner can leverage to learn whether a
verb can passivize. We first characterize English speakers' judgments of
exceptions to the passive, confirming that speakers find some verbs more
passivizable than others. We then show that a neural network language model can
learn restrictions to the passive that are similar to those displayed by
humans, suggesting that evidence for these exceptions is available in the
linguistic input. We test the causal role of two hypotheses for how the
language model learns these restrictions by training models on modified
training corpora, which we create by altering the existing training corpora to
remove features of the input implicated by each hypothesis. We find that while
the frequency with which a verb appears in the passive significantly affects
its passivizability, the semantics of the verb does not. This study highlight
the utility of altering a language model's training data for answering
questions where complete control over a learner's input is vital.","[{'name': 'Cara Su-Yi Leong'}, {'name': 'Tal Linzen'}]",2024-07-05T15:41:30Z
http://arxiv.org/abs/2407.04573v1,http://arxiv.org/abs/2407.04573v1,"VRSD: Rethinking Similarity and Diversity for Retrieval in Large
  Language Models","Vector retrieval algorithms are vital for semantic queries in the evolving
landscape of Large Language Models (LLMs). Retrieving vectors that
simultaneously meet criteria for both similarity and diversity significantly
enhances the capabilities of LLM-based agents. Despite the widespread use of
the Maximal Marginal Relevance (MMR) in retrieval scenarios with relevance and
diversity requirements, fluctuations caused by variations in the parameter $
\lambda $ within the MMR complicate the determination of the optimization
trajectory in vector spaces, thus obscuring the direction of enhancement.
Moreover, there is a lack of a robust theoretical analysis for the constraints
of similarity and diversity in retrieval processes. This paper introduces a
novel approach to characterizing both constraints through the relationship
between the sum vector and the query vector. The proximity of these vectors
addresses the similarity constraint, while necessitating that individual
vectors within the sum vector divergently align with the query vector to
satisfy the diversity constraint. We also formulate a new combinatorial
optimization challenge, taking a selection of $k$ vectors from a set of
candidates such that their sum vector maximally aligns with the query vector, a
problem we demonstrate to be NP-complete. This establishes the profound
difficulty of pursuing similarity and diversity simultaneously in vector
retrieval and lays a theoretical groundwork for further research. Additionally,
we present the heuristic algorithm Vectors Retrieval with Similarity and
Diversity (VRSD) which not only has a definitive optimization goal and eschews
the need for preset parameters but also offers a modest reduction in time
complexity compared to MMR. Empirical validation further confirm that VRSD
significantly surpasses MMR across various datasets.","[{'name': 'Hang Gao'}, {'name': 'Yongfeng Zhang'}]",2024-07-05T15:08:44Z
http://arxiv.org/abs/2407.04559v1,http://arxiv.org/abs/2407.04559v1,"Not (yet) the whole story: Evaluating Visual Storytelling Requires More
  than Measuring Coherence, Grounding, and Repetition","Visual storytelling consists in generating a natural language story given a
temporally ordered sequence of images. This task is not only challenging for
models, but also very difficult to evaluate with automatic metrics since there
is no consensus about what makes a story 'good'. In this paper, we introduce a
novel method that measures story quality in terms of human likeness regarding
three key aspects highlighted in previous work: visual grounding, coherence,
and repetitiveness. We then use this method to evaluate the stories generated
by several models, showing that the foundation model LLaVA obtains the best
result, but only slightly so compared to TAPM, a 50-times smaller visual
storytelling model. Upgrading the visual and language components of TAPM
results in a model that yields competitive performance with a relatively low
number of parameters. Finally, we carry out a human evaluation study, whose
results suggest that a 'good' story may require more than a human-like level of
visual grounding, coherence, and repetition.","[{'name': 'Aditya K Surikuchi'}, {'name': 'Raquel Fernández'}, {'name': 'Sandro Pezzelle'}]",2024-07-05T14:48:15Z
http://arxiv.org/abs/2407.04549v1,http://arxiv.org/abs/2407.04549v1,Spontaneous Reward Hacking in Iterative Self-Refinement,"Language models are capable of iteratively improving their outputs based on
natural language feedback, thus enabling in-context optimization of user
preference. In place of human users, a second language model can be used as an
evaluator, providing feedback along with numerical ratings which the generator
attempts to optimize. However, because the evaluator is an imperfect proxy of
user preference, this optimization can lead to reward hacking, where the
evaluator's ratings improve while the generation quality remains stagnant or
even decreases as judged by actual user preference. The concern of reward
hacking is heightened in iterative self-refinement where the generator and the
evaluator use the same underlying language model, in which case the
optimization pressure can drive them to exploit shared vulnerabilities. Using
an essay editing task, we show that iterative self-refinement leads to
deviation between the language model evaluator and human judgment,
demonstrating that reward hacking can occur spontaneously in-context with the
use of iterative self-refinement. In addition, we study conditions under which
reward hacking occurs and observe two factors that affect reward hacking
severity: model size and context sharing between the generator and the
evaluator.","[{'name': 'Jane Pan'}, {'name': 'He He'}, {'name': 'Samuel R. Bowman'}, {'name': 'Shi Feng'}]",2024-07-05T14:34:50Z
http://arxiv.org/abs/2407.04543v1,http://arxiv.org/abs/2407.04543v1,"Strengthening Structural Inductive Biases by Pre-training to Perform
  Syntactic Transformations","Models need appropriate inductive biases to effectively learn from small
amounts of data and generalize systematically outside of the training
distribution. While Transformers are highly versatile and powerful, they can
still benefit from enhanced structural inductive biases for seq2seq tasks,
especially those involving syntactic transformations, such as converting active
to passive voice or semantic parsing. In this paper, we propose to strengthen
the structural inductive bias of a Transformer by intermediate pre-training to
perform synthetically generated syntactic transformations of dependency trees
given a description of the transformation. Our experiments confirm that this
helps with few-shot learning of syntactic tasks such as chunking, and also
improves structural generalization for semantic parsing. Our analysis shows
that the intermediate pre-training leads to attention heads that keep track of
which syntactic transformation needs to be applied to which token, and that the
model can leverage these attention heads on downstream tasks.","[{'name': 'Matthias Lindemann'}, {'name': 'Alexander Koller'}, {'name': 'Ivan Titov'}]",2024-07-05T14:29:44Z
http://arxiv.org/abs/2407.04541v1,http://arxiv.org/abs/2407.04541v1,"PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit
  Posts","We introduce PoPreRo, the first dataset for Popularity Prediction of Romanian
posts collected from Reddit. The PoPreRo dataset includes a varied compilation
of post samples from five distinct subreddits of Romania, totaling 28,107 data
samples. Along with our novel dataset, we introduce a set of competitive models
to be used as baselines for future research. Interestingly, the top-scoring
model achieves an accuracy of 61.35% and a macro F1 score of 60.60% on the test
set, indicating that the popularity prediction task on PoPreRo is very
challenging. Further investigations based on few-shot prompting the Falcon-7B
Large Language Model also point in the same direction. We thus believe that
PoPreRo is a valuable resource that can be used to evaluate models on
predicting the popularity of social media posts in Romanian. We release our
dataset at https://github.com/ana-rogoz/PoPreRo.","[{'name': 'Ana-Cristina Rogoz'}, {'name': 'Maria Ilinca Nechita'}, {'name': 'Radu Tudor Ionescu'}]",2024-07-05T14:28:12Z
http://arxiv.org/abs/2407.04533v2,http://arxiv.org/abs/2407.04533v2,"Performance Analysis of Speech Encoders for Low-Resource SLU and ASR in
  Tunisian Dialect","Speech encoders pretrained through self-supervised learning (SSL) have
demonstrated remarkable performance in various downstream tasks, including
Spoken Language Understanding (SLU) and Automatic Speech Recognition (ASR). For
instance, fine-tuning SSL models for such tasks has shown significant
potential, leading to improvements in the SOTA performance across challenging
datasets. In contrast to existing research, this paper contributes by comparing
the effectiveness of SSL approaches in the context of (i) the low-resource
spoken Tunisian Arabic dialect and (ii) its combination with a low-resource SLU
and ASR scenario, where only a few semantic annotations are available for
fine-tuning. We conduct experiments using many SSL speech encoders on the
TARIC-SLU dataset. We use speech encoders that were pre-trained on either
monolingual or multilingual speech data. Some of them have also been refined
without in-domain nor Tunisian data through multimodal supervised
teacher-student paradigm. This study yields numerous significant findings that
we are discussing in this paper.","[{'name': 'Salima Mdhaffar'}, {'name': 'Haroun Elleuch'}, {'name': 'Fethi Bougares'}, {'name': 'Yannick Estève'}]",2024-07-05T14:21:36Z
http://arxiv.org/abs/2407.04528v1,http://arxiv.org/abs/2407.04528v1,"GPT vs RETRO: Exploring the Intersection of Retrieval and
  Parameter-Efficient Fine-Tuning","Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation
(RAG) have become popular methods for adapting large language models while
minimizing compute requirements. In this paper, we apply PEFT methods
(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer
(RETRO) and a baseline GPT model across several sizes, ranging from 823 million
to 48 billion parameters. We show that RETRO models outperform GPT models in
zero-shot settings due to their unique pre-training process but GPT models have
higher performance potential with PEFT. Additionally, our study indicates that
8B parameter models strike an optimal balance between cost and performance and
P-tuning lags behind other PEFT techniques. We further provide a comparative
analysis of between applying PEFT to an Instruction-tuned RETRO model and base
RETRO model. This work presents the first comprehensive comparison of various
PEFT methods integrated with RAG, applied to both GPT and RETRO models,
highlighting their relative performance.","[{'name': 'Aleksander Ficek'}, {'name': 'Jiaqi Zeng'}, {'name': 'Oleksii Kuchaiev'}]",2024-07-05T14:16:47Z
http://arxiv.org/abs/2407.04485v1,http://arxiv.org/abs/2407.04485v1,"Leveraging Graph Structures to Detect Hallucinations in Large Language
  Models","Large language models are extensively applied across a wide range of tasks,
such as customer support, content creation, educational tutoring, and providing
financial guidance. However, a well-known drawback is their predisposition to
generate hallucinations. This damages the trustworthiness of the information
these models provide, impacting decision-making and user confidence. We propose
a method to detect hallucinations by looking at the structure of the latent
space and finding associations within hallucinated and non-hallucinated
generations. We create a graph structure that connects generations that lie
closely in the embedding space. Moreover, we employ a Graph Attention Network
which utilizes message passing to aggregate information from neighboring nodes
and assigns varying degrees of importance to each neighbor based on their
relevance. Our findings show that 1) there exists a structure in the latent
space that differentiates between hallucinated and non-hallucinated
generations, 2) Graph Attention Networks can learn this structure and
generalize it to unseen generations, and 3) the robustness of our method is
enhanced when incorporating contrastive learning. When evaluated against
evidence-based benchmarks, our model performs similarly without access to
search-based methods.","[{'name': 'Noa Nonkes'}, {'name': 'Sergei Agaronian'}, {'name': 'Evangelos Kanoulas'}, {'name': 'Roxana Petcu'}]",2024-07-05T13:08:58Z
http://arxiv.org/abs/2407.04459v1,http://arxiv.org/abs/2407.04459v1,Generalists vs. Specialists: Evaluating Large Language Models for Urdu,"In this paper, we compare general-purpose pretrained models, GPT-4-Turbo and
Llama-3-8b-Instruct with special-purpose models fine-tuned on specific tasks,
XLM-Roberta-large, mT5-large, and Llama-3-8b-Instruct. We focus on seven
classification and six generation tasks to evaluate the performance of these
models on Urdu language. Urdu has 70 million native speakers, yet it remains
underrepresented in Natural Language Processing (NLP). Despite the frequent
advancements in Large Language Models (LLMs), their performance in low-resource
languages, including Urdu, still needs to be explored. We also conduct a human
evaluation for the generation tasks and compare the results with the
evaluations performed by GPT-4-Turbo and Llama-3-8b-Instruct. We find that
special-purpose models consistently outperform general-purpose models across
various tasks. We also find that the evaluation done by GPT-4-Turbo for
generation tasks aligns more closely with human evaluation compared to the
evaluation by Llama-3-8b-Instruct. This paper contributes to the NLP community
by providing insights into the effectiveness of general and specific-purpose
LLMs for low-resource languages.","[{'name': 'Samee Arif'}, {'name': 'Abdul Hameed Azeemi'}, {'name': 'Agha Ali Raza'}, {'name': 'Awais Athar'}]",2024-07-05T12:09:40Z
http://arxiv.org/abs/2407.04444v1,http://arxiv.org/abs/2407.04444v1,TokenVerse: Unifying Speech and NLP Tasks via Transducer-based ASR,"In traditional conversational intelligence from speech, a cascaded pipeline
is used, involving tasks such as voice activity detection, diarization,
transcription, and subsequent processing with different NLP models for tasks
like semantic endpointing and named entity recognition (NER). Our paper
introduces TokenVerse, a single Transducer-based model designed to handle
multiple tasks. This is achieved by integrating task-specific tokens into the
reference text during ASR model training, streamlining the inference and
eliminating the need for separate NLP models. In addition to ASR, we conduct
experiments on 3 different tasks: speaker change detection, endpointing, and
NER. Our experiments on a public and a private dataset show that the proposed
method improves ASR by up to 7.7% in relative WER while outperforming the
cascaded pipeline approach in individual task performance. Additionally, we
present task transfer learning to a new task within an existing TokenVerse.","[{'name': 'Shashi Kumar'}, {'name': 'Srikanth Madikeri'}, {'name': 'Juan Zuluaga-Gomez'}, {'name': 'Iuliia Nigmatulina'}, {'name': 'Esaú Villatoro-Tello'}, {'name': 'Sergio Burdisso'}, {'name': 'Petr Motlicek'}, {'name': 'Karthik Pandia'}, {'name': 'Aravind Ganapathiraju'}]",2024-07-05T11:54:38Z
http://arxiv.org/abs/2407.04434v1,http://arxiv.org/abs/2407.04434v1,"From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive
  Language for Bias Reduction in LLMs","Gender bias is not only prevalent in Large Language Models (LLMs) and their
training data, but also firmly ingrained into the structural aspects of
language itself. Therefore, adapting linguistic structures within LLM training
data to promote gender-inclusivity can make gender representations within the
model more inclusive. The focus of our work are gender-exclusive affixes in
English, such as in 'show-girl' or 'man-cave', which can perpetuate gender
stereotypes and binary conceptions of gender. We use an LLM training dataset to
compile a catalogue of 692 gender-exclusive terms along with gender-neutral
variants and from this, develop a gender-inclusive fine-tuning dataset, the
'Tiny Heap'. Fine-tuning three different LLMs with this dataset, we observe an
overall reduction in gender-stereotyping tendencies across the models. Our
approach provides a practical method for enhancing gender inclusivity in LLM
training data and contributes to incorporating queer-feminist linguistic
activism in bias mitigation research in NLP.","[{'name': 'Marion Bartl'}, {'name': 'Susan Leavy'}]",2024-07-05T11:31:30Z
http://arxiv.org/abs/2407.04411v1,http://arxiv.org/abs/2407.04411v1,Waterfall: Framework for Robust and Scalable Text Watermarking,"Protecting intellectual property (IP) of text such as articles and code is
increasingly important, especially as sophisticated attacks become possible,
such as paraphrasing by large language models (LLMs) or even unauthorized
training of LLMs on copyrighted text to infringe such IP. However, existing
text watermarking methods are not robust enough against such attacks nor
scalable to millions of users for practical implementation. In this paper, we
propose Waterfall, the first training-free framework for robust and scalable
text watermarking applicable across multiple text types (e.g., articles, code)
and languages supportable by LLMs, for general text and LLM data provenance.
Waterfall comprises several key innovations, such as being the first to use LLM
as paraphrasers for watermarking along with a novel combination of techniques
that are surprisingly effective in achieving robust verifiability and
scalability. We empirically demonstrate that Waterfall achieves significantly
better scalability, robust verifiability, and computational efficiency compared
to SOTA article-text watermarking methods, and also showed how it could be
directly applied to the watermarking of code.","[{'name': 'Gregory Kang Ruey Lau'}, {'name': 'Xinyuan Niu'}, {'name': 'Hieu Dao'}, {'name': 'Jiangwei Chen'}, {'name': 'Chuan-Sheng Foo'}, {'name': 'Bryan Kian Hsiang Low'}]",2024-07-05T10:51:33Z
http://arxiv.org/abs/2407.12847v1,http://arxiv.org/abs/2407.12847v1,"Aligning Model Evaluations with Human Preferences: Mitigating Token
  Count Bias in Language Model Assessments","The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a
viable and cost-effective alternative to API-based Large Language Models
(LLMs), such as OpenAI's GPT-4, offering comparable performance and stability.
However, SLAM also identified discrepancies between human preferences and
traditional auto-evaluators. This follow-up paper explores methods to align LLM
evaluator preferences with human evaluations by addressing biases, particularly
toward higher token counts. We employed Bayesian statistics and a t-test to
quantify this bias and developed a recalibration procedure to adjust the
GPTScorer. Our findings significantly improve aligning the recalibrated LLM
evaluator with human evaluations across multiple use cases. For instance,
spearman's ranking correlation score in the Recommendation use case improved
from -27.27 to 44.55. These results highlight the importance of accounting for
biases in automated evaluations to ensure fair and accurate model assessments.
The recalibration process enhances the reliability of automated evaluators,
leading to better AI models that align with human values and expectations. This
study provides a robust methodology for future research into bias correction
and emphasizes the feasibility and benefits of developing human-aligned AI
evaluation systems.","[{'name': 'Roland Daynauth'}, {'name': 'Jason Mars'}]",2024-07-05T09:26:40Z
http://arxiv.org/abs/2407.04368v1,http://arxiv.org/abs/2407.04368v1,Romanization Encoding For Multilingual ASR,"We introduce romanization encoding for script-heavy languages to optimize
multilingual and code-switching Automatic Speech Recognition (ASR) systems. By
adopting romanization encoding alongside a balanced concatenated tokenizer
within a FastConformer-RNNT framework equipped with a Roman2Char module, we
significantly reduce vocabulary and output dimensions, enabling larger training
batches and reduced memory consumption. Our method decouples acoustic modeling
and language modeling, enhancing the flexibility and adaptability of the
system. In our study, applying this method to Mandarin-English ASR resulted in
a remarkable 63.51% vocabulary reduction and notable performance gains of
13.72% and 15.03% on SEAME code-switching benchmarks. Ablation studies on
Mandarin-Korean and Mandarin-Japanese highlight our method's strong capability
to address the complexities of other script-heavy languages, paving the way for
more versatile and effective multilingual ASR systems.","[{'name': 'Wen Ding'}, {'name': 'Fei Jia'}, {'name': 'Hainan Xu'}, {'name': 'Yu Xi'}, {'name': 'Junjie Lai'}, {'name': 'Boris Ginsburg'}]",2024-07-05T09:13:24Z
http://arxiv.org/abs/2407.12846v1,http://arxiv.org/abs/2407.12846v1,Identifying the Source of Generation for Large Language Models,"Large language models (LLMs) memorize text from several sources of documents.
In pretraining, LLM trains to maximize the likelihood of text but neither
receives the source of the text nor memorizes the source. Accordingly, LLM can
not provide document information on the generated content, and users do not
obtain any hint of reliability, which is crucial for factuality or privacy
infringement. This work introduces token-level source identification in the
decoding step, which maps the token representation to the reference document.
We propose a bi-gram source identifier, a multi-layer perceptron with two
successive token representations as input for better generalization. We conduct
extensive experiments on Wikipedia and PG19 datasets with several LLMs, layer
locations, and identifier sizes. The overall results show a possibility of
token-level source identifiers for tracing the document, a crucial problem for
the safe use of LLMs.","[{'name': 'Bumjin Park'}, {'name': 'Jaesik Choi'}]",2024-07-05T08:52:15Z
http://arxiv.org/abs/2407.04752v1,http://arxiv.org/abs/2407.04752v1,"SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via
  Saliency-based Spiking","The recent advancements in large language models (LLMs) with billions of
parameters have significantly boosted their performance across various
real-world applications. However, the inference processes for these models
require substantial energy and computational resources, presenting considerable
deployment challenges. In contrast, human brains, which contain approximately
86 billion biological neurons, exhibit significantly greater energy efficiency
compared to LLMs with a similar number of parameters. Inspired by this, we
redesign 7 to 70 billion parameter LLMs using bio-plausible spiking mechanisms,
emulating the efficient behavior of the human brain. We propose the first
spiking large language model as recent LLMs termed SpikeLLM. Coupled with the
proposed model, a novel spike-driven quantization framework named Optimal Brain
Spiking is introduced to reduce the energy cost and accelerate inference speed
via two essential approaches: first (second)-order differentiation-based
salient channel detection, and per-channel salient outlier expansion with
Generalized Integrate-and-Fire neurons. Our proposed spike-driven quantization
can plug in main streams of quantization training methods. In the OmniQuant
pipeline, SpikeLLM significantly reduces 25.51% WikiText2 perplexity and
improves 3.08% average accuracy of 6 zero-shot datasets on a LLAMA2-7B 4A4W
model. In the GPTQ pipeline, SpikeLLM realizes a sparse ternary quantization,
which achieves additive in all linear layers. Compared with PB-LLM with similar
operations, SpikeLLM also exceeds significantly. We will release our code on
GitHub.","[{'name': 'Xingrun Xing'}, {'name': 'Boyan Gao'}, {'name': 'Zheng Zhang'}, {'name': 'David A. Clifton'}, {'name': 'Shitao Xiao'}, {'name': 'Li Du'}, {'name': 'Guoqi Li'}, {'name': 'Jiajun Zhang'}]",2024-07-05T08:37:17Z
http://arxiv.org/abs/2407.04307v1,http://arxiv.org/abs/2407.04307v1,Crafting Large Language Models for Enhanced Interpretability,"We introduce the Concept Bottleneck Large Language Model (CB-LLM), a
pioneering approach to creating inherently interpretable Large Language Models
(LLMs). Unlike traditional black-box LLMs that rely on post-hoc interpretation
methods with limited neuron function insights, CB-LLM sets a new standard with
its built-in interpretability, scalability, and ability to provide clear,
accurate explanations. This innovation not only advances transparency in
language models but also enhances their effectiveness. Our unique Automatic
Concept Correction (ACC) strategy successfully narrows the performance gap with
conventional black-box LLMs, positioning CB-LLM as a model that combines the
high accuracy of traditional LLMs with the added benefit of clear
interpretability -- a feature markedly absent in existing LLMs.","[{'name': 'Chung-En Sun'}, {'name': 'Tuomas Oikarinen'}, {'name': 'Tsui-Wei Weng'}]",2024-07-05T07:22:44Z
http://arxiv.org/abs/2407.04295v1,http://arxiv.org/abs/2407.04295v1,Jailbreak Attacks and Defenses Against Large Language Models: A Survey,"Large Language Models (LLMs) have performed exceptionally in various
text-generative tasks, including question answering, translation, code
completion, etc. However, the over-assistance of LLMs has raised the challenge
of ""jailbreaking"", which induces the model to generate malicious responses
against the usage policy and society by designing adversarial prompts. With the
emergence of jailbreak attack methods exploiting different vulnerabilities in
LLMs, the corresponding safety alignment measures are also evolving. In this
paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and
defense methods. For instance, the attack methods are divided into black-box
and white-box attacks based on the transparency of the target model. Meanwhile,
we classify defense methods into prompt-level and model-level defenses.
Additionally, we further subdivide these attack and defense methods into
distinct sub-classes and present a coherent diagram illustrating their
relationships. We also conduct an investigation into the current evaluation
methods and compare them from different perspectives. Our findings aim to
inspire future research and practical implementations in safeguarding LLMs
against adversarial attacks. Above all, although jailbreak remains a
significant concern within the community, we believe that our work enhances the
understanding of this domain and provides a foundation for developing more
secure LLMs.","[{'name': 'Sibo Yi'}, {'name': 'Yule Liu'}, {'name': 'Zhen Sun'}, {'name': 'Tianshuo Cong'}, {'name': 'Xinlei He'}, {'name': 'Jiaxing Song'}, {'name': 'Ke Xu'}, {'name': 'Qi Li'}]",2024-07-05T06:57:30Z
http://arxiv.org/abs/2407.04293v1,http://arxiv.org/abs/2407.04293v1,"Systematic Evaluation of Online Speaker Diarization Systems Regarding
  their Latency","In this paper, different online speaker diarization systems are evaluated on
the same hardware with the same test data with regard to their latency. The
latency is the time span from audio input to the output of the corresponding
speaker label. As part of the evaluation, various model combinations within the
DIART framework, a diarization system based on the online clustering algorithm
UIS-RNN-SML, and the end-to-end online diarization system FS-EEND are compared.
The lowest latency is achieved for the DIART-pipeline with the embedding model
pyannote/embedding and the segmentation model pyannote/segmentation. The
FS-EEND system shows a similarly good latency. In general there is currently no
published research that compares several online diarization systems in terms of
their latency. This makes this work even more relevant.","[{'name': 'Roman Aperdannier'}, {'name': 'Sigurd Schacht'}, {'name': 'Alexander Piazza'}]",2024-07-05T06:54:27Z
http://arxiv.org/abs/2407.04280v1,http://arxiv.org/abs/2407.04280v1,"LearnerVoice: A Dataset of Non-Native English Learners' Spontaneous
  Speech","Prevalent ungrammatical expressions and disfluencies in spontaneous speech
from second language (L2) learners pose unique challenges to Automatic Speech
Recognition (ASR) systems. However, few datasets are tailored to L2 learner
speech. We publicly release LearnerVoice, a dataset consisting of 50.04 hours
of audio and transcriptions of L2 learners' spontaneous speech. Our linguistic
analysis reveals that transcriptions in our dataset contain L2S (L2 learner's
Spontaneous speech) features, consisting of ungrammatical expressions and
disfluencies (e.g., filler words, word repetitions, self-repairs, false
starts), significantly more than native speech datasets. Fine-tuning
whisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than
vanilla whisper-small.en. Furthermore, our qualitative analysis indicates that
54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S
features, with 48.1% of them being reduced in the fine-tuned model.","[{'name': 'Haechan Kim'}, {'name': 'Junho Myung'}, {'name': 'Seoyoung Kim'}, {'name': 'Sungpah Lee'}, {'name': 'Dongyeop Kang'}, {'name': 'Juho Kim'}]",2024-07-05T06:25:54Z
http://arxiv.org/abs/2407.04279v1,http://arxiv.org/abs/2407.04279v1,BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks,"In the Emotion Recognition in Conversation task, recent investigations have
utilized attention mechanisms exploring relationships among utterances from
intra- and inter-speakers for modeling emotional interaction between them.
However, attributes such as speaker personality traits remain unexplored and
present challenges in terms of their applicability to other tasks or
compatibility with diverse model architectures. Therefore, this work introduces
a novel framework named BiosERC, which investigates speaker characteristics in
a conversation. By employing Large Language Models (LLMs), we extract the
""biographical information"" of the speaker within a conversation as
supplementary knowledge injected into the model to classify emotional labels
for each utterance. Our proposed method achieved state-of-the-art (SOTA)
results on three famous benchmark datasets: IEMOCAP, MELD, and EmoryNLP,
demonstrating the effectiveness and generalization of our model and showcasing
its potential for adaptation to various conversation analysis tasks. Our source
code is available at https://github.com/yingjie7/BiosERC.","[{'name': 'Jieying Xue'}, {'name': 'Minh Phuong Nguyen'}, {'name': 'Blake Matheny'}, {'name': 'Le Minh Nguyen'}]",2024-07-05T06:25:34Z
http://arxiv.org/abs/2407.04251v1,http://arxiv.org/abs/2407.04251v1,"Unified Interpretation of Smoothing Methods for Negative Sampling Loss
  Functions in Knowledge Graph Embedding","Knowledge Graphs (KGs) are fundamental resources in knowledge-intensive tasks
in NLP. Due to the limitation of manually creating KGs, KG Completion (KGC) has
an important role in automatically completing KGs by scoring their links with
KG Embedding (KGE). To handle many entities in training, KGE relies on Negative
Sampling (NS) loss that can reduce the computational cost by sampling. Since
the appearance frequencies for each link are at most one in KGs, sparsity is an
essential and inevitable problem. The NS loss is no exception. As a solution,
the NS loss in KGE relies on smoothing methods like Self-Adversarial Negative
Sampling (SANS) and subsampling. However, it is uncertain what kind of
smoothing method is suitable for this purpose due to the lack of theoretical
understanding. This paper provides theoretical interpretations of the smoothing
methods for the NS loss in KGE and induces a new NS loss, Triplet Adaptive
Negative Sampling (TANS), that can cover the characteristics of the
conventional smoothing methods. Experimental results of TransE, DistMult,
ComplEx, RotatE, HAKE, and HousE on FB15k-237, WN18RR, and YAGO3-10 datasets
and their sparser subsets show the soundness of our interpretation and
performance improvement by our TANS.","[{'name': 'Xincan Feng'}, {'name': 'Hidetaka Kamigaito'}, {'name': 'Katsuhiko Hayashi'}, {'name': 'Taro Watanabe'}]",2024-07-05T04:38:17Z
http://arxiv.org/abs/2407.04247v1,http://arxiv.org/abs/2407.04247v1,"ArAIEval Shared Task: Propagandistic Techniques Detection in Unimodal
  and Multimodal Arabic Content","We present an overview of the second edition of the ArAIEval shared task,
organized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In
this edition, ArAIEval offers two tasks: (i) detection of propagandistic
textual spans with persuasion techniques identification in tweets and news
articles, and (ii) distinguishing between propagandistic and non-propagandistic
memes. A total of 14 teams participated in the final evaluation phase, with 6
and 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams
submitted system description papers. Across both tasks, we observed that
fine-tuning transformer models such as AraBERT was at the core of the majority
of the participating systems. We provide a description of the task setup,
including a description of the dataset construction and the evaluation setup.
We further provide a brief overview of the participating systems. All datasets
and evaluation scripts are released to the research community
(https://araieval.gitlab.io/). We hope this will enable further research on
these important tasks in Arabic.","[{'name': 'Maram Hasanain'}, {'name': 'Md. Arid Hasan'}, {'name': 'Fatema Ahmed'}, {'name': 'Reem Suwaileh'}, {'name': 'Md. Rafiul Biswas'}, {'name': 'Wajdi Zaghouani'}, {'name': 'Firoj Alam'}]",2024-07-05T04:28:46Z
http://arxiv.org/abs/2407.04185v2,http://arxiv.org/abs/2407.04185v2,HAF-RM: A Hybrid Alignment Framework for Reward Model Training,"The reward model has become increasingly important in alignment, assessment,
and data construction for large language models (LLMs). Most existing
researchers focus on enhancing reward models through data improvements,
following the conventional training framework for reward models that directly
optimizes the predicted rewards. In this paper, we propose a hybrid alignment
framework HaF-RM for reward model training by introducing an additional
constraint on token-level policy probabilities in addition to the reward score.
It can simultaneously supervise the internal preference model at the token
level and optimize the mapping layer of the reward model at the sequence level.
Theoretical justifications and experiment results on five datasets show the
validity and effectiveness of our proposed hybrid framework for training a
high-quality reward model. By decoupling the reward modeling procedure and
incorporating hybrid supervision, our HaF-RM framework offers a principled and
effective approach to enhancing the performance and alignment of reward models,
a critical component in the responsible development of powerful language
models. We release our code at https://haf-rm.github.io.","[{'name': 'Shujun Liu'}, {'name': 'Xiaoyu Shen'}, {'name': 'Yuhang Lai'}, {'name': 'Siyuan Wang'}, {'name': 'Shengbin Yue'}, {'name': 'Zengfeng Huang'}, {'name': 'Xuanjing Huang'}, {'name': 'Zhongyu Wei'}]",2024-07-04T23:26:56Z
http://arxiv.org/abs/2407.04183v1,http://arxiv.org/abs/2407.04183v1,"Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality
  Norms","Large language models (LLMs) are trained on broad corpora and then used in
communities with specialized norms. Is providing LLMs with community rules
enough for models to follow these norms? We evaluate LLMs' capacity to detect
(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's
Neutral Point of View (NPOV) policy. LLMs struggled with bias detection,
achieving only 64% accuracy on a balanced dataset. Models exhibited contrasting
biases (some under- and others over-predicted bias), suggesting distinct priors
about neutrality. LLMs performed better at generation, removing 79% of words
removed by Wikipedia editors. However, LLMs made additional changes beyond
Wikipedia editors' simpler neutralizations, resulting in high-recall but
low-precision editing. Interestingly, crowdworkers rated AI rewrites as more
neutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative
analysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia
editors but often made extraneous non-NPOV-related changes (such as grammar).
LLMs may apply rules in ways that resonate with the public but diverge from
community experts. While potentially effective for generation, LLMs may reduce
editor agency and increase moderation workload (e.g., verifying additions).
Even when rules are easy to articulate, having LLMs apply them like community
members may still be difficult.","[{'name': 'Joshua Ashkinaze'}, {'name': 'Ruijia Guan'}, {'name': 'Laura Kurek'}, {'name': 'Eytan Adar'}, {'name': 'Ceren Budak'}, {'name': 'Eric Gilbert'}]",2024-07-04T23:05:58Z
http://arxiv.org/abs/2407.04181v1,http://arxiv.org/abs/2407.04181v1,Orchestrating LLMs with Different Personalizations,"This paper presents a novel approach to aligning large language models (LLMs)
with individual human preferences, sometimes referred to as Reinforcement
Learning from \textit{Personalized} Human Feedback (RLPHF). Given stated
preferences along multiple dimensions, such as helpfulness, conciseness, or
humor, the goal is to create an LLM without re-training that best adheres to
this specification. Starting from specialized expert LLMs, each trained for one
such particular preference dimension, we propose a black-box method that merges
their outputs on a per-token level. We train a lightweight Preference Control
Model (PCM) that dynamically translates the preference description and current
context into next-token prediction weights. By combining the expert models'
outputs at the token level, our approach dynamically generates text that
optimizes the given preference. Empirical tests show that our method matches or
surpasses existing preference merging techniques, providing a scalable,
efficient alternative to fine-tuning LLMs for individual personalization.","[{'name': 'Jin Peng Zhou'}, {'name': 'Katie Z Luo'}, {'name': 'Jingwen Gu'}, {'name': 'Jason Yuan'}, {'name': 'Kilian Q. Weinberger'}, {'name': 'Wen Sun'}]",2024-07-04T22:55:02Z
http://arxiv.org/abs/2407.04179v1,http://arxiv.org/abs/2407.04179v1,"Defense Against Syntactic Textual Backdoor Attacks with Token
  Substitution","Textual backdoor attacks present a substantial security risk to Large
Language Models (LLM). It embeds carefully chosen triggers into a victim model
at the training stage, and makes the model erroneously predict inputs
containing the same triggers as a certain class. Prior backdoor defense methods
primarily target special token-based triggers, leaving syntax-based triggers
insufficiently addressed. To fill this gap, this paper proposes a novel online
defense algorithm that effectively counters syntax-based as well as special
token-based backdoor attacks. The algorithm replaces semantically meaningful
words in sentences with entirely different ones but preserves the syntactic
templates or special tokens, and then compares the predicted labels before and
after the substitution to determine whether a sentence contains triggers.
Experimental results confirm the algorithm's performance against these two
types of triggers, offering a comprehensive defense strategy for model
integrity.","[{'name': 'Xinglin Li'}, {'name': 'Xianwen He'}, {'name': 'Yao Li'}, {'name': 'Minhao Cheng'}]",2024-07-04T22:48:57Z
http://arxiv.org/abs/2407.18328v1,http://arxiv.org/abs/2407.18328v1,"Unveiling Scoring Processes: Dissecting the Differences between LLMs and
  Human Graders in Automatic Scoring","Large language models (LLMs) have demonstrated strong potential in performing
automatic scoring for constructed response assessments. While constructed
responses graded by humans are usually based on given grading rubrics, the
methods by which LLMs assign scores remain largely unclear. It is also
uncertain how closely AI's scoring process mirrors that of humans, or if it
adheres to the same grading criteria. To address this gap, this paper uncovers
the grading rubrics that LLMs used to score students' written responses to
science tasks and their alignment with human scores. We also examine whether
enhancing the alignments can improve scoring accuracy. Specifically, we prompt
LLMs to generate analytic rubrics that they use to assign scores and study the
alignment gap with human grading rubrics. Based on a series of experiments with
various configurations of LLM settings, we reveal a notable alignment gap
between human and LLM graders. While LLMs can adapt quickly to scoring tasks,
they often resort to shortcuts, bypassing deeper logical reasoning expected in
human grading. We found that incorporating high-quality analytical rubrics
designed to reflect human grading logic can mitigate this gap and enhance LLMs'
scoring accuracy. These results caution against the simplistic application of
LLMs in science education and highlight the importance of aligning LLM outputs
with human expectations to ensure efficient and accurate automatic scoring.","[{'name': 'Xuansheng Wu'}, {'name': 'Padmaja Pravin Saraf'}, {'name': 'Gyeong-Geon Lee'}, {'name': 'Ehsan Latif'}, {'name': 'Ninghao Liu'}, {'name': 'Xiaoming Zhai'}]",2024-07-04T22:26:20Z
http://arxiv.org/abs/2407.04172v1,http://arxiv.org/abs/2407.04172v1,ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild,"Given the ubiquity of charts as a data analysis, visualization, and
decision-making tool across industries and sciences, there has been a growing
interest in developing pre-trained foundation models as well as general purpose
instruction-tuned models for chart understanding and reasoning. However,
existing methods suffer crucial drawbacks across two critical axes affecting
the performance of chart representation models: they are trained on data
generated from underlying data tables of the charts, ignoring the visual trends
and patterns in chart images, and use weakly aligned vision-language backbone
models for domain-specific training, limiting their generalizability when
encountering charts in the wild. We address these important drawbacks and
introduce ChartGemma, a novel chart understanding and reasoning model developed
over PaliGemma. Rather than relying on underlying data tables, ChartGemma is
trained on instruction-tuning data generated directly from chart images, thus
capturing both high-level trends and low-level visual information from a
diverse set of charts. Our simple approach achieves state-of-the-art results
across $5$ benchmarks spanning chart summarization, question answering, and
fact-checking, and our elaborate qualitative studies on real-world charts show
that ChartGemma generates more realistic and factually correct summaries
compared to its contemporaries. We release the code, model checkpoints,
dataset, and demos at https://github.com/vis-nlp/ChartGemma.","[{'name': 'Ahmed Masry'}, {'name': 'Megh Thakkar'}, {'name': 'Aayush Bajaj'}, {'name': 'Aaryaman Kartha'}, {'name': 'Enamul Hoque'}, {'name': 'Shafiq Joty'}]",2024-07-04T22:16:40Z
http://arxiv.org/abs/2407.04158v1,http://arxiv.org/abs/2407.04158v1,ELCC: the Emergent Language Corpus Collection,"We introduce the Emergent Language Corpus Collection (ELCC): a collection of
corpora collected from open source implementations of emergent communication
systems across the literature. These systems include a variety of signalling
game environments as well as more complex tasks like a social deduction game
and embodied navigation. Each corpus is annotated with metadata describing the
characteristics of the source system as well as a suite of analyses of the
corpus (e.g., size, entropy, average message length). Currently, research
studying emergent languages requires directly running different systems which
takes time away from actual analyses of such languages, limits the variety of
languages that are studied, and presents a barrier to entry for researchers
without a background in deep learning. The availability of a substantial
collection of well-documented emergent language corpora, then, will enable new
directions of research which focus their purview on the properties of emergent
languages themselves rather than on experimental apparatus.","[{'name': 'Brendon Boldt'}, {'name': 'David Mortensen'}]",2024-07-04T21:23:18Z
http://arxiv.org/abs/2407.04151v1,http://arxiv.org/abs/2407.04151v1,"Securing Multi-turn Conversational Language Models Against Distributed
  Backdoor Triggers","The security of multi-turn conversational large language models (LLMs) is
understudied despite it being one of the most popular LLM utilization.
Specifically, LLMs are vulnerable to data poisoning backdoor attacks, where an
adversary manipulates the training data to cause the model to output malicious
responses to predefined triggers. Specific to the multi-turn dialogue setting,
LLMs are at the risk of even more harmful and stealthy backdoor attacks where
the backdoor triggers may span across multiple utterances, giving lee-way to
context-driven attacks. In this paper, we explore a novel distributed backdoor
trigger attack that serves to be an extra tool in an adversary's toolbox that
can interface with other single-turn attack strategies in a plug and play
manner. Results on two representative defense mechanisms indicate that
distributed backdoor triggers are robust against existing defense strategies
which are designed for single-turn user-model interactions, motivating us to
propose a new defense strategy for the multi-turn dialogue setting that is more
challenging. To this end, we also explore a novel contrastive decoding based
defense that is able to mitigate the backdoor with a low computational
tradeoff.","[{'name': 'Terry Tong'}, {'name': 'Jiashu Xu'}, {'name': 'Qin Liu'}, {'name': 'Muhao Chen'}]",2024-07-04T20:57:06Z
http://arxiv.org/abs/2407.04130v1,http://arxiv.org/abs/2407.04130v1,"Towards Automating Text Annotation: A Case Study on Semantic Proximity
  Annotation using GPT-4","This paper explores using GPT-3.5 and GPT-4 to automate the data annotation
process with automatic prompting techniques. The main aim of this paper is to
reuse human annotation guidelines along with some annotated data to design
automatic prompts for LLMs, focusing on the semantic proximity annotation task.
Automatic prompts are compared to customized prompts. We further implement the
prompting strategies into an open-source text annotation tool, enabling easy
online use via the OpenAI API. Our study reveals the crucial role of accurate
prompt design and suggests that prompting GPT-4 with human-like instructions is
not straightforwardly possible for the semantic proximity task. We show that
small modifications to the human guidelines already improve the performance,
suggesting possible ways for future research.","[{'name': 'Sachin Yadav'}, {'name': 'Tejaswi Choppa'}, {'name': 'Dominik Schlechtweg'}]",2024-07-04T19:16:44Z
http://arxiv.org/abs/2407.04125v1,http://arxiv.org/abs/2407.04125v1,Query-Guided Self-Supervised Summarization of Nursing Notes,"Nursing notes, an important component of Electronic Health Records (EHRs),
keep track of the progression of a patient's health status during a care
episode. Distilling the key information in nursing notes through text
summarization techniques can improve clinicians' efficiency in understanding
patients' conditions when reviewing nursing notes. However, existing
abstractive summarization methods in the clinical setting have often overlooked
nursing notes and require the creation of reference summaries for supervision
signals, which is time-consuming. In this work, we introduce QGSumm, a
query-guided self-supervised domain adaptation framework for nursing note
summarization. Using patient-related clinical queries as guidance, our approach
generates high-quality, patient-centered summaries without relying on reference
summaries for training. Through automatic and manual evaluation by an expert
clinician, we demonstrate the strengths of our approach compared to the
state-of-the-art Large Language Models (LLMs) in both zero-shot and few-shot
settings. Ultimately, our approach provides a new perspective on conditional
text summarization, tailored to the specific interests of clinical personnel.","[{'name': 'Ya Gao'}, {'name': 'Hans Moen'}, {'name': 'Saila Koivusalo'}, {'name': 'Miika Koskinen'}, {'name': 'Pekka Marttinen'}]",2024-07-04T18:54:30Z
http://arxiv.org/abs/2407.04121v1,http://arxiv.org/abs/2407.04121v1,"Hallucination Detection: Robustly Discerning Reliable Answers in Large
  Language Models","Large Language Models (LLMs) have gained widespread adoption in various
natural language processing tasks, including question answering and dialogue
systems. However, a major drawback of LLMs is the issue of hallucination, where
they generate unfaithful or inconsistent content that deviates from the input
source, leading to severe consequences. In this paper, we propose a robust
discriminator named RelD to effectively detect hallucination in LLMs' generated
answers. RelD is trained on the constructed RelQA, a bilingual
question-answering dialogue dataset along with answers generated by LLMs and a
comprehensive set of metrics. Our experimental results demonstrate that the
proposed RelD successfully detects hallucination in the answers generated by
diverse LLMs. Moreover, it performs well in distinguishing hallucination in
LLMs' generated answers from both in-distribution and out-of-distribution
datasets. Additionally, we also conduct a thorough analysis of the types of
hallucinations that occur and present valuable insights. This research
significantly contributes to the detection of reliable answers generated by
LLMs and holds noteworthy implications for mitigating hallucination in the
future work.","[{'name': 'Yuyan Chen'}, {'name': 'Qiang Fu'}, {'name': 'Yichen Yuan'}, {'name': 'Zhihao Wen'}, {'name': 'Ge Fan'}, {'name': 'Dayiheng Liu'}, {'name': 'Dongmei Zhang'}, {'name': 'Zhixu Li'}, {'name': 'Yanghua Xiao'}]",2024-07-04T18:47:42Z
http://arxiv.org/abs/2407.04105v1,http://arxiv.org/abs/2407.04105v1,Can Pre-trained Language Models Understand Chinese Humor?,"Humor understanding is an important and challenging research in natural
language processing. As the popularity of pre-trained language models (PLMs),
some recent work makes preliminary attempts to adopt PLMs for humor recognition
and generation. However, these simple attempts do not substantially answer the
question: {\em whether PLMs are capable of humor understanding?} This paper is
the first work that systematically investigates the humor understanding ability
of PLMs. For this purpose, a comprehensive framework with three evaluation
steps and four evaluation tasks is designed. We also construct a comprehensive
Chinese humor dataset, which can fully meet all the data requirements of the
proposed evaluation framework. Our empirical study on the Chinese humor dataset
yields some valuable observations, which are of great guiding value for future
optimization of PLMs in humor understanding and generation.","[{'name': 'Yuyan Chen'}, {'name': 'Zhixu Li'}, {'name': 'Jiaqing Liang'}, {'name': 'Yanghua Xiao'}, {'name': 'Bang Liu'}, {'name': 'Yunwen Chen'}]",2024-07-04T18:13:38Z
http://arxiv.org/abs/2407.04093v2,http://arxiv.org/abs/2407.04093v2,"Stephanie: Step-by-Step Dialogues for Mimicking Human Interactions in
  Social Conversations","In the rapidly evolving field of natural language processing, dialogue
systems primarily employ a single-step dialogue paradigm. Although this
paradigm is efficient, it lacks the depth and fluidity of human interactions
and does not appear natural. We introduce a novel \textbf{Step}-by-Step
Dialogue Paradigm (Stephanie), designed to mimic the ongoing dynamic nature of
human conversations. By employing a dual learning strategy and a further-split
post-editing method, we generated and utilized a high-quality step-by-step
dialogue dataset to fine-tune existing large language models, enabling them to
perform step-by-step dialogues. We thoroughly present Stephanie. Tailored
automatic and human evaluations are conducted to assess its effectiveness
compared to the traditional single-step dialogue paradigm. We will release
code, Stephanie datasets, and Stephanie LLMs to facilitate the future of
chatbot eras.","[{'name': 'Hao Yang'}, {'name': 'Hongyuan Lu'}, {'name': 'Xinhua Zeng'}, {'name': 'Yang Liu'}, {'name': 'Xiang Zhang'}, {'name': 'Haoran Yang'}, {'name': 'Yumeng Zhang'}, {'name': 'Shan Huang'}, {'name': 'Yiran Wei'}, {'name': 'Wai Lam'}]",2024-07-04T17:59:41Z
http://arxiv.org/abs/2407.12844v1,http://arxiv.org/abs/2407.12844v1,"$\texttt{metabench}$ -- A Sparse Benchmark to Measure General Ability in
  Large Language Models","Large Language Models (LLMs) vary in their abilities on a range of tasks.
Initiatives such as the $\texttt{Open LLM Leaderboard}$ aim to quantify these
differences with several large benchmarks (sets of test items to which an LLM
can respond either correctly or incorrectly). However, high correlations within
and between benchmark scores suggest that (1) there exists a small set of
common underlying abilities that these benchmarks measure, and (2) items tap
into redundant information and the benchmarks may thus be considerably
compressed. We use data from $n > 5000$ LLMs to identify the most informative
items of six benchmarks, ARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande
(with $d=28,632$ items in total). From them we distill a sparse benchmark,
$\texttt{metabench}$, that has less than $3\%$ of the original size of all six
benchmarks combined. This new sparse benchmark goes beyond point scores by
yielding estimators of the underlying benchmark-specific abilities. We show
that these estimators (1) can be used to reconstruct each original
$\textit{individual}$ benchmark score with, on average, $1.5\%$ root mean
square error (RMSE), (2) reconstruct the original $\textit{total}$ score with
$0.8\%$ RMSE, and (3) have a single underlying common factor whose Spearman
correlation with the total score is $r = 0.93$.","[{'name': 'Alex Kipnis'}, {'name': 'Konstantinos Voudouris'}, {'name': 'Luca M. Schulze Buschoff'}, {'name': 'Eric Schulz'}]",2024-07-04T17:57:38Z
http://arxiv.org/abs/2407.04079v1,http://arxiv.org/abs/2407.04079v1,"AXOLOTL'24 Shared Task on Multilingual Explainable Semantic Change
  Modeling","This paper describes the organization and findings of AXOLOTL'24, the first
multilingual explainable semantic change modeling shared task. We present new
sense-annotated diachronic semantic change datasets for Finnish and Russian
which were employed in the shared task, along with a surprise test-only German
dataset borrowed from an existing source. The setup of AXOLOTL'24 is new to the
semantic change modeling field, and involves subtasks of identifying unknown
(novel) senses and providing dictionary-like definitions to these senses. The
methods of the winning teams are described and compared, thus paving a path
towards explainability in computational approaches to historical change of
meaning.","[{'name': 'Mariia Fedorova'}, {'name': 'Timothee Mickus'}, {'name': 'Niko Partanen'}, {'name': 'Janine Siewert'}, {'name': 'Elena Spaziani'}, {'name': 'Andrey Kutuzov'}]",2024-07-04T17:41:32Z
http://arxiv.org/abs/2407.04078v3,http://arxiv.org/abs/2407.04078v3,"DotaMath: Decomposition of Thought with Code Assistance and
  Self-correction for Mathematical Reasoning","Large language models (LLMs) have made impressive progress in handling simple
math problems, yet they still struggle with more challenging and complex
mathematical tasks. In this paper, we introduce a series of LLMs that employs
the Decomposition of thought with code assistance and self-correction for
mathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex
mathematical tasks by decomposing them into simpler logical subtasks,
leveraging code to solve these subtasks, obtaining fine-grained feedback from
the code interpreter, and engaging in self-reflection and correction. By
annotating diverse interactive tool-use trajectories and employing query
evolution on GSM8K and MATH datasets, we generate an instruction fine-tuning
dataset called DotaMathQA with 574K query-response pairs. We train a series of
base LLMs using imitation learning on DotaMathQA, resulting in DotaMath models
that achieve remarkable performance compared to open-source LLMs across various
in-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases
an outstanding performance of 64.8% on the competitive MATH dataset and 86.7%
on GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a
series of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,
we anticipate that the DotaMath paradigm will open new pathways for addressing
intricate mathematical problems. Our code is publicly available at
https://github.com/ChengpengLi1003/DotaMath.","[{'name': 'Chengpeng Li'}, {'name': 'Guanting Dong'}, {'name': 'Mingfeng Xue'}, {'name': 'Ru Peng'}, {'name': 'Xiang Wang'}, {'name': 'Dayiheng Liu'}]",2024-07-04T17:39:16Z
http://arxiv.org/abs/2407.04069v1,http://arxiv.org/abs/2407.04069v1,"A Systematic Survey and Critical Review on Evaluating Large Language
  Models: Challenges, Limitations, and Recommendations","Large Language Models (LLMs) have recently gained significant attention due
to their remarkable capabilities in performing diverse tasks across various
domains. However, a thorough evaluation of these models is crucial before
deploying them in real-world applications to ensure they produce reliable
performance. Despite the well-established importance of evaluating LLMs in the
community, the complexity of the evaluation process has led to varied
evaluation setups, causing inconsistencies in findings and interpretations. To
address this, we systematically review the primary challenges and limitations
causing these inconsistencies and unreliable evaluations in various steps of
LLM evaluation. Based on our critical review, we present our perspectives and
recommendations to ensure LLM evaluations are reproducible, reliable, and
robust.","[{'name': 'Md Tahmid Rahman Laskar'}, {'name': 'Sawsan Alqahtani'}, {'name': 'M Saiful Bari'}, {'name': 'Mizanur Rahman'}, {'name': 'Mohammad Abdullah Matin Khan'}, {'name': 'Haidar Khan'}, {'name': 'Israt Jahan'}, {'name': 'Amran Bhuiyan'}, {'name': 'Chee Wei Tan'}, {'name': 'Md Rizwan Parvez'}, {'name': 'Enamul Hoque'}, {'name': 'Shafiq Joty'}, {'name': 'Jimmy Huang'}]",2024-07-04T17:15:37Z
http://arxiv.org/abs/2407.04067v1,http://arxiv.org/abs/2407.04067v1,"Semantic Graphs for Syntactic Simplification: A Revisit from the Age of
  LLM","Symbolic sentence meaning representations, such as AMR (Abstract Meaning
Representation) provide expressive and structured semantic graphs that act as
intermediates that simplify downstream NLP tasks. However, the
instruction-following capability of large language models (LLMs) offers a
shortcut to effectively solve NLP tasks, questioning the utility of semantic
graphs. Meanwhile, recent work has also shown the difficulty of using meaning
representations merely as a helpful auxiliary for LLMs. We revisit the position
of semantic graphs in syntactic simplification, the task of simplifying
sentence structures while preserving their meaning, which requires semantic
understanding, and evaluate it on a new complex and natural dataset. The
AMR-based method that we propose, AMRS$^3$, demonstrates that state-of-the-art
meaning representations can lead to easy-to-implement simplification methods
with competitive performance and unique advantages in cost, interpretability,
and generalization. With AMRS$^3$ as an anchor, we discover that syntactic
simplification is a task where semantic graphs are helpful in LLM prompting. We
propose AMRCoC prompting that guides LLMs to emulate graph algorithms for
explicit symbolic reasoning on AMR graphs, and show its potential for improving
LLM on semantic-centered tasks like syntactic simplification.","[{'name': 'Peiran Yao'}, {'name': 'Kostyantyn Guzhva'}, {'name': 'Denilson Barbosa'}]",2024-07-04T17:13:38Z
http://arxiv.org/abs/2407.04050v1,http://arxiv.org/abs/2407.04050v1,"Deep Content Understanding Toward Entity and Aspect Target Sentiment
  Analysis on Foundation Models","Introducing Entity-Aspect Sentiment Triplet Extraction (EASTE), a novel
Aspect-Based Sentiment Analysis (ABSA) task which extends
Target-Aspect-Sentiment Detection (TASD) by separating aspect categories (e.g.,
food#quality) into pre-defined entities (e.g., meal, drink) and aspects (e.g.,
taste, freshness) which add a fine-gainer level of complexity, yet help
exposing true sentiment of chained aspect to its entity. We explore the task of
EASTE solving capabilities of language models based on transformers
architecture from our proposed unified-loss approach via token classification
task using BERT architecture to text generative models such as Flan-T5,
Flan-Ul2 to Llama2, Llama3 and Mixtral employing different alignment techniques
such as zero/few-shot learning, Parameter Efficient Fine Tuning (PEFT) such as
Low-Rank Adaptation (LoRA). The model performances are evaluated on the
SamEval-2016 benchmark dataset representing the fair comparison to existing
works. Our research not only aims to achieve high performance on the EASTE task
but also investigates the impact of model size, type, and adaptation techniques
on task performance. Ultimately, we provide detailed insights and achieving
state-of-the-art results in complex sentiment analysis.","[{'name': 'Vorakit Vorakitphan'}, {'name': 'Milos Basic'}, {'name': 'Guilhaume Leroy Meline'}]",2024-07-04T16:48:14Z
http://arxiv.org/abs/2407.04047v1,http://arxiv.org/abs/2407.04047v1,"Improving Accented Speech Recognition using Data Augmentation based on
  Unsupervised Text-to-Speech Synthesis","This paper investigates the use of unsupervised text-to-speech synthesis
(TTS) as a data augmentation method to improve accented speech recognition. TTS
systems are trained with a small amount of accented speech training data and
their pseudo-labels rather than manual transcriptions, and hence unsupervised.
This approach enables the use of accented speech data without manual
transcriptions to perform data augmentation for accented speech recognition.
Synthetic accented speech data, generated from text prompts by using the TTS
systems, are then combined with available non-accented speech data to train
automatic speech recognition (ASR) systems. ASR experiments are performed in a
self-supervised learning framework using a Wav2vec2.0 model which was
pre-trained on large amount of unsupervised accented speech data. The accented
speech data for training the unsupervised TTS are read speech, selected from
L2-ARCTIC and British Isles corpora, while spontaneous conversational speech
from the Edinburgh international accents of English corpus are used as the
evaluation data. Experimental results show that Wav2vec2.0 models which are
fine-tuned to downstream ASR task with synthetic accented speech data,
generated by the unsupervised TTS, yield up to 6.1% relative word error rate
reductions compared to a Wav2vec2.0 baseline which is fine-tuned with the
non-accented speech data from Librispeech corpus.","[{'name': 'Cong-Thanh Do'}, {'name': 'Shuhei Imai'}, {'name': 'Rama Doddipatla'}, {'name': 'Thomas Hain'}]",2024-07-04T16:42:24Z
http://arxiv.org/abs/2407.04046v1,http://arxiv.org/abs/2407.04046v1,"Systematic Task Exploration with LLMs: A Study in Citation Text
  Generation","Large language models (LLMs) bring unprecedented flexibility in defining and
executing complex, creative natural language generation (NLG) tasks. Yet, this
flexibility brings new challenges, as it introduces new degrees of freedom in
formulating the task inputs and instructions and in evaluating model
performance. To facilitate the exploration of creative NLG tasks, we propose a
three-component research framework that consists of systematic input
manipulation, reference data, and output measurement. We use this framework to
explore citation text generation -- a popular scholarly NLP task that lacks
consensus on the task definition and evaluation metric and has not yet been
tackled within the LLM paradigm. Our results highlight the importance of
systematically investigating both task instruction and input configuration when
prompting LLMs, and reveal non-trivial relationships between different
evaluation metrics used for citation text generation. Additional human
generation and human evaluation experiments provide new qualitative insights
into the task to guide future research in citation text generation. We make our
code and data publicly available.","[{'name': 'Furkan Şahinuç'}, {'name': 'Ilia Kuznetsov'}, {'name': 'Yufang Hou'}, {'name': 'Iryna Gurevych'}]",2024-07-04T16:41:08Z
http://arxiv.org/abs/2407.04020v2,http://arxiv.org/abs/2407.04020v2,"LLMAEL: Large Language Models are Good Context Augmenters for Entity
  Linking","Entity Linking (EL) models are well-trained at mapping mentions to their
corresponding entities according to a given context. However, EL models
struggle to disambiguate long-tail entities due to their limited training data.
Meanwhile, large language models (LLMs) are more robust at interpreting
uncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at
generating correct entity IDs. Furthermore, training an LLM to perform EL is
cost-intensive. Building upon these insights, we introduce LLM-Augmented Entity
Linking LLMAEL, a plug-and-play approach to enhance entity linking through LLM
data augmentation. We leverage LLMs as knowledgeable context augmenters,
generating mention-centered descriptions as additional input, while preserving
traditional EL models for task specific processing. Experiments on 6 standard
datasets show that the vanilla LLMAEL outperforms baseline EL models in most
cases, while the fine-tuned LLMAEL set the new state-of-the-art results across
all 6 benchmarks.","[{'name': 'Amy Xin'}, {'name': 'Yunjia Qi'}, {'name': 'Zijun Yao'}, {'name': 'Fangwei Zhu'}, {'name': 'Kaisheng Zeng'}, {'name': 'Xu Bin'}, {'name': 'Lei Hou'}, {'name': 'Juanzi Li'}]",2024-07-04T15:55:13Z
http://arxiv.org/abs/2407.04010v1,http://arxiv.org/abs/2407.04010v1,"Exploring Diachronic and Diatopic Changes in Dialect Continua: Tasks,
  Datasets and Challenges","Everlasting contact between language communities leads to constant changes in
languages over time, and gives rise to language varieties and dialects.
However, the communities speaking non-standard language are often overlooked by
non-inclusive NLP technologies. Recently, there has been a surge of interest in
studying diatopic and diachronic changes in dialect NLP, but there is currently
no research exploring the intersection of both. Our work aims to fill this gap
by systematically reviewing diachronic and diatopic papers from a unified
perspective. In this work, we critically assess nine tasks and datasets across
five dialects from three language families (Slavic, Romance, and Germanic) in
both spoken and written modalities. The tasks covered are diverse, including
corpus construction, dialect distance estimation, and dialect geolocation
prediction, among others. Moreover, we outline five open challenges regarding
changes in dialect use over time, the reliability of dialect datasets, the
importance of speaker characteristics, limited coverage of dialects, and
ethical considerations in data collection. We hope that our work sheds light on
future research towards inclusive computational methods and datasets for
language varieties and dialects.","[{'name': 'Melis Çelikkol'}, {'name': 'Lydia Körber'}, {'name': 'Wei Zhao'}]",2024-07-04T15:38:38Z
http://arxiv.org/abs/2407.03994v2,http://arxiv.org/abs/2407.03994v2,Unlocking the Potential of Model Merging for Low-Resource Languages,"Adapting large language models (LLMs) to new languages typically involves
continual pre-training (CT) followed by supervised fine-tuning (SFT). However,
this CT-then-SFT approach struggles with limited data in the context of
low-resource languages, failing to balance language modeling and task-solving
capabilities. We thus propose model merging as an alternative for low-resource
languages, combining models with distinct capabilities into a single model
without additional training. We use model merging to develop task-solving LLMs
for low-resource languages without SFT data in the target languages. Our
experiments based on Llama-2-7B demonstrate that model merging effectively
endows LLMs for low-resource languages with task-solving abilities,
outperforming CT-then-SFT in scenarios with extremely scarce data. Observing
performance saturation in model merging with more training tokens, we further
analyze the merging process and introduce a slack variable to the model merging
algorithm to mitigate the loss of important parameters, thereby enhancing
performance. We hope that model merging can benefit more human languages
suffering from data scarcity with its higher data efficiency.","[{'name': 'Mingxu Tao'}, {'name': 'Chen Zhang'}, {'name': 'Quzhe Huang'}, {'name': 'Tianyao Ma'}, {'name': 'Songfang Huang'}, {'name': 'Dongyan Zhao'}, {'name': 'Yansong Feng'}]",2024-07-04T15:14:17Z
http://arxiv.org/abs/2407.03993v1,http://arxiv.org/abs/2407.03993v1,A Survey on Natural Language Counterfactual Generation,"Natural Language Counterfactual generation aims to minimally modify a given
text such that the modified text will be classified into a different class. The
generated counterfactuals provide insight into the reasoning behind a model's
predictions by highlighting which words significantly influence the outcomes.
Additionally, they can be used to detect model fairness issues or augment the
training data to enhance the model's robustness. A substantial amount of
research has been conducted to generate counterfactuals for various NLP tasks,
employing different models and methodologies. With the rapid growth of studies
in this field, a systematic review is crucial to guide future researchers and
developers. To bridge this gap, this survey comprehensively overview textual
counterfactual generation methods, particularly including those based on Large
Language Models. We propose a new taxonomy that categorizes the generation
methods into four groups and systematically summarize the metrics for
evaluating the generation quality. Finally, we discuss ongoing research
challenges and outline promising directions for future work.","[{'name': 'Yongjie Wang'}, {'name': 'Xiaoqi Qiu'}, {'name': 'Yu Yue'}, {'name': 'Xu Guo'}, {'name': 'Zhiwei Zeng'}, {'name': 'Yuhong Feng'}, {'name': 'Zhiqi Shen'}]",2024-07-04T15:13:59Z
http://arxiv.org/abs/2407.12843v1,http://arxiv.org/abs/2407.12843v1,"NutriBench: A Dataset for Evaluating Large Language Models in
  Carbohydrate Estimation from Meal Descriptions","Accurate nutrition estimation helps people make informed decisions about
their dietary choices and is crucial for preventing serious health issues. We
present NutriBench, the first publicly available natural language meal
description based nutrition benchmark. NutriBench consists of 5,000
human-verified meal descriptions with macro-nutrient labels, including
carbohydrates, proteins, fats, and calories. The data is divided into 15
subsets varying in complexity based on the number, servings, and popularity of
the food items in the meal and the specificity of serving size descriptions. We
conducted an extensive evaluation of seven popular and state-of-the-art Large
Language Models (LLMs), including GPT-3.5, Llama-3, and a medical
domain-specific model with standard, Chain-of-Thought and Retrieval-Augmented
Generation strategies on our benchmark for carbohydrate estimation. We also
conducted a human study involving expert and non-expert participants and found
that LLMs can provide more accurate and faster predictions over a range of
complex queries. We present a thorough analysis and comparison of different
LLMs, highlighting the opportunities and challenges of using LLMs for nutrition
estimation in real-life scenarios. Our benchmark is publicly available at:
https://mehak126.github.io/nutribench.html","[{'name': 'Andong Hua'}, {'name': 'Mehak Preet Dhaliwal'}, {'name': 'Ryan Burke'}, {'name': 'Yao Qin'}]",2024-07-04T15:10:51Z
http://arxiv.org/abs/2407.03978v2,http://arxiv.org/abs/2407.03978v2,"Benchmarking Complex Instruction-Following with Multiple Constraints
  Composition","Instruction following is one of the fundamental capabilities of large
language models (LLMs). As the ability of LLMs is constantly improving, they
have been increasingly applied to deal with complex human instructions in
real-world scenarios. Therefore, how to evaluate the ability of complex
instruction-following of LLMs has become a critical research problem. Existing
benchmarks mainly focus on modeling different types of constraints in human
instructions while neglecting the composition of different constraints, which
is an indispensable constituent in complex instructions. To this end, we
propose ComplexBench, a benchmark for comprehensively evaluating the ability of
LLMs to follow complex instructions composed of multiple constraints. We
propose a hierarchical taxonomy for complex instructions, including 4
constraint types, 19 constraint dimensions, and 4 composition types, and
manually collect a high-quality dataset accordingly. To make the evaluation
reliable, we augment LLM-based evaluators with rules to effectively verify
whether generated texts can satisfy each constraint and composition.
Furthermore, we obtain the final evaluation score based on the dependency
structure determined by different composition types. ComplexBench identifies
significant deficiencies in existing LLMs when dealing with complex
instructions with multiple constraints composition.","[{'name': 'Bosi Wen'}, {'name': 'Pei Ke'}, {'name': 'Xiaotao Gu'}, {'name': 'Lindong Wu'}, {'name': 'Hao Huang'}, {'name': 'Jinfeng Zhou'}, {'name': 'Wenchuang Li'}, {'name': 'Binxin Hu'}, {'name': 'Wendy Gao'}, {'name': 'Jiaxin Xu'}, {'name': 'Yiming Liu'}, {'name': 'Jie Tang'}, {'name': 'Hongning Wang'}, {'name': 'Minlie Huang'}]",2024-07-04T14:50:45Z
http://arxiv.org/abs/2407.03974v1,http://arxiv.org/abs/2407.03974v1,LLM Roleplay: Simulating Human-Chatbot Interaction,"The development of chatbots requires collecting a large number of
human-chatbot dialogues to reflect the breadth of users' sociodemographic
backgrounds and conversational goals. However, the resource requirements to
conduct the respective user studies can be prohibitively high and often only
allow for a narrow analysis of specific dialogue goals and participant
demographics. In this paper, we propose LLM-Roleplay: a goal-oriented,
persona-based method to automatically generate diverse multi-turn dialogues
simulating human-chatbot interaction. LLM-Roleplay can be applied to generate
dialogues with any type of chatbot and uses large language models (LLMs) to
play the role of textually described personas. To validate our method we
collect natural human-chatbot dialogues from different sociodemographic groups
and conduct a human evaluation to compare real human-chatbot dialogues with our
generated dialogues. We compare the abilities of state-of-the-art LLMs in
embodying personas and holding a conversation and find that our method can
simulate human-chatbot dialogues with a high indistinguishability rate.","[{'name': 'Hovhannes Tamoyan'}, {'name': 'Hendrik Schuff'}, {'name': 'Iryna Gurevych'}]",2024-07-04T14:49:46Z
http://arxiv.org/abs/2407.03967v1,http://arxiv.org/abs/2407.03967v1,"Investigating the Role of Instruction Variety and Task Difficulty in
  Robotic Manipulation Tasks","Evaluating the generalisation capabilities of multimodal models based solely
on their performance on out-of-distribution data fails to capture their true
robustness. This work introduces a comprehensive evaluation framework that
systematically examines the role of instructions and inputs in the
generalisation abilities of such models, considering architectural design,
input perturbations across language and vision modalities, and increased task
complexity. The proposed framework uncovers the resilience of multimodal models
to extreme instruction perturbations and their vulnerability to observational
changes, raising concerns about overfitting to spurious correlations. By
employing this evaluation framework on current Transformer-based multimodal
models for robotic manipulation tasks, we uncover limitations and suggest
future advancements should focus on architectural and training innovations that
better integrate multimodal inputs, enhancing a model's generalisation prowess
by prioritising sensitivity to input content over incidental correlations.","[{'name': 'Amit Parekh'}, {'name': 'Nikolas Vitsakis'}, {'name': 'Alessandro Suglia'}, {'name': 'Ioannis Konstas'}]",2024-07-04T14:36:49Z
http://arxiv.org/abs/2407.03964v1,http://arxiv.org/abs/2407.03964v1,"Improving Sample Efficiency of Reinforcement Learning with Background
  Knowledge from Large Language Models","Low sample efficiency is an enduring challenge of reinforcement learning
(RL). With the advent of versatile large language models (LLMs), recent works
impart common-sense knowledge to accelerate policy learning for RL processes.
However, we note that such guidance is often tailored for one specific task but
loses generalizability. In this paper, we introduce a framework that harnesses
LLMs to extract background knowledge of an environment, which contains general
understandings of the entire environment, making various downstream RL tasks
benefit from one-time knowledge representation. We ground LLMs by feeding a few
pre-collected experiences and requesting them to delineate background knowledge
of the environment. Afterward, we represent the output knowledge as potential
functions for potential-based reward shaping, which has a good property for
maintaining policy optimality from task rewards. We instantiate three variants
to prompt LLMs for background knowledge, including writing code, annotating
preferences, and assigning goals. Our experiments show that these methods
achieve significant sample efficiency improvements in a spectrum of downstream
tasks from Minigrid and Crafter domains.","[{'name': 'Fuxiang Zhang'}, {'name': 'Junyou Li'}, {'name': 'Yi-Chen Li'}, {'name': 'Zongzhang Zhang'}, {'name': 'Yang Yu'}, {'name': 'Deheng Ye'}]",2024-07-04T14:33:47Z
http://arxiv.org/abs/2407.03963v1,http://arxiv.org/abs/2407.03963v1,"LLM-jp: A Cross-organizational Project for the Research and Development
  of Fully Open Japanese LLMs","This paper introduces LLM-jp, a cross-organizational project for the research
and development of Japanese large language models (LLMs). LLM-jp aims to
develop open-source and strong Japanese LLMs, and as of this writing, more than
1,500 participants from academia and industry are working together for this
purpose. This paper presents the background of the establishment of LLM-jp,
summaries of its activities, and technical reports on the LLMs developed by
LLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.","[{'name': 'LLM-jp'}, {'name': ':'}, {'name': 'Akiko Aizawa'}, {'name': 'Eiji Aramaki'}, {'name': 'Bowen Chen'}, {'name': 'Fei Cheng'}, {'name': 'Hiroyuki Deguchi'}, {'name': 'Rintaro Enomoto'}, {'name': 'Kazuki Fujii'}, {'name': 'Kensuke Fukumoto'}, {'name': 'Takuya Fukushima'}, {'name': 'Namgi Han'}, {'name': 'Yuto Harada'}, {'name': 'Chikara Hashimoto'}, {'name': 'Tatsuya Hiraoka'}, {'name': 'Shohei Hisada'}, {'name': 'Sosuke Hosokawa'}, {'name': 'Lu Jie'}, {'name': 'Keisuke Kamata'}, {'name': 'Teruhito Kanazawa'}, {'name': 'Hiroki Kanezashi'}, {'name': 'Hiroshi Kataoka'}, {'name': 'Satoru Katsumata'}, {'name': 'Daisuke Kawahara'}, {'name': 'Seiya Kawano'}, {'name': 'Atsushi Keyaki'}, {'name': 'Keisuke Kiryu'}, {'name': 'Hirokazu Kiyomaru'}, {'name': 'Takashi Kodama'}, {'name': 'Takahiro Kubo'}, {'name': 'Yohei Kuga'}, {'name': 'Ryoma Kumon'}, {'name': 'Shuhei Kurita'}, {'name': 'Sadao Kurohashi'}, {'name': 'Conglong Li'}, {'name': 'Taiki Maekawa'}, {'name': 'Hiroshi Matsuda'}, {'name': 'Yusuke Miyao'}, {'name': 'Kentaro Mizuki'}, {'name': 'Sakae Mizuki'}, {'name': 'Yugo Murawaki'}, {'name': 'Ryo Nakamura'}, {'name': 'Taishi Nakamura'}, {'name': 'Kouta Nakayama'}, {'name': 'Tomoka Nakazato'}, {'name': 'Takuro Niitsuma'}, {'name': 'Jiro Nishitoba'}, {'name': 'Yusuke Oda'}, {'name': 'Hayato Ogawa'}, {'name': 'Takumi Okamoto'}, {'name': 'Naoaki Okazaki'}, {'name': 'Yohei Oseki'}, {'name': 'Shintaro Ozaki'}, {'name': 'Koki Ryu'}, {'name': 'Rafal Rzepka'}, {'name': 'Keisuke Sakaguchi'}, {'name': 'Shota Sasaki'}, {'name': 'Satoshi Sekine'}, {'name': 'Kohei Suda'}, {'name': 'Saku Sugawara'}, {'name': 'Issa Sugiura'}, {'name': 'Hiroaki Sugiyama'}, {'name': 'Hisami Suzuki'}, {'name': 'Jun Suzuki'}, {'name': 'Toyotaro Suzumura'}, {'name': 'Kensuke Tachibana'}, {'name': 'Yu Takagi'}, {'name': 'Kyosuke Takami'}, {'name': 'Koichi Takeda'}, {'name': 'Masashi Takeshita'}, {'name': 'Masahiro Tanaka'}, {'name': 'Kenjiro Taura'}, {'name': 'Arseny Tolmachev'}, {'name': 'Nobuhiro Ueda'}, {'name': 'Zhen Wan'}, {'name': 'Shuntaro Yada'}, {'name': 'Sakiko Yahata'}, {'name': 'Yuya Yamamoto'}, {'name': 'Yusuke Yamauchi'}, {'name': 'Hitomi Yanaka'}, {'name': 'Rio Yokota'}, {'name': 'Koichiro Yoshino'}]",2024-07-04T14:33:03Z
http://arxiv.org/abs/2407.03955v1,http://arxiv.org/abs/2407.03955v1,Meta-prompting Optimized Retrieval-augmented Generation,"Retrieval-augmented generation resorts to content retrieved from external
sources in order to leverage the performance of large language models in
downstream tasks. The excessive volume of retrieved content, the possible
dispersion of its parts, or their out of focus range may happen nevertheless to
eventually have a detrimental rather than an incremental effect. To mitigate
this issue and improve retrieval-augmented generation, we propose a method to
refine the retrieved content before it is included in the prompt by resorting
to meta-prompting optimization. Put to empirical test with the demanding
multi-hop question answering task from the StrategyQA dataset, the evaluation
results indicate that this method outperforms a similar retrieval-augmented
system but without this method by over 30%.","[{'name': 'João Rodrigues'}, {'name': 'António Branco'}]",2024-07-04T14:20:12Z
http://arxiv.org/abs/2407.03952v1,http://arxiv.org/abs/2407.03952v1,A framework for annotating and modelling intentions behind metaphor use,"Metaphors are part of everyday language and shape the way in which we
conceptualize the world. Moreover, they play a multifaceted role in
communication, making their understanding and generation a challenging task for
language models (LMs). While there has been extensive work in the literature
linking metaphor to the fulfilment of individual intentions, no comprehensive
taxonomy of such intentions, suitable for natural language processing (NLP)
applications, is available to present day. In this paper, we propose a novel
taxonomy of intentions commonly attributed to metaphor, which comprises 9
categories. We also release the first dataset annotated for intentions behind
metaphor use. Finally, we use this dataset to test the capability of large
language models (LLMs) in inferring the intentions behind metaphor use, in
zero- and in-context few-shot settings. Our experiments show that this is still
a challenge for LLMs.","[{'name': 'Gianluca Michelli'}, {'name': 'Xiaoyu Tong'}, {'name': 'Ekaterina Shutova'}]",2024-07-04T14:13:57Z
http://arxiv.org/abs/2407.03942v1,http://arxiv.org/abs/2407.03942v1,"Diverse and Fine-Grained Instruction-Following Ability Exploration with
  Synthetic Data","Instruction-following is particularly crucial for large language models
(LLMs) to support diverse user requests. While existing work has made progress
in aligning LLMs with human preferences, evaluating their capabilities on
instruction following remains a challenge due to complexity and diversity of
real-world user instructions. While existing evaluation methods focus on
general skills, they suffer from two main shortcomings, i.e., lack of
fine-grained task-level evaluation and reliance on singular instruction
expression. To address these problems, this paper introduces DINGO, a
fine-grained and diverse instruction-following evaluation dataset that has two
main advantages: (1) DINGO is based on a manual annotated, fine-grained and
multi-level category tree with 130 nodes derived from real-world user requests;
(2) DINGO includes diverse instructions, generated by both GPT-4 and human
experts. Through extensive experiments, we demonstrate that DINGO can not only
provide more challenging and comprehensive evaluation for LLMs, but also
provide task-level fine-grained directions to further improve LLMs.","[{'name': 'Zihui Gu'}, {'name': 'Xingwu Sun'}, {'name': 'Fengzong Lian'}, {'name': 'Zhanhui Kang'}, {'name': 'Cheng-Zhong Xu'}, {'name': 'Ju Fan'}]",2024-07-04T13:54:41Z
http://arxiv.org/abs/2407.03941v1,http://arxiv.org/abs/2407.03941v1,Narrow Transformer: Starcoder-Based Java-LM For Desktop,"This paper presents NT-Java-1.1B, an open-source specialized code language
model built on StarCoderBase-1.1B, designed for coding tasks in Java
programming. NT-Java-1.1B achieves state-of-the-art performance, surpassing its
base model and majority of other models of similar size on MultiPL-E Java code
benchmark. While there have been studies on extending large, generic
pre-trained models to improve proficiency in specific programming languages
like Python, similar investigations on small code models for other programming
languages are lacking. Large code models require specialized hardware like GPUs
for inference, highlighting the need for research into building small code
models that can be deployed on developer desktops. This paper addresses this
research gap by focusing on the development of a small Java code model,
NT-Java-1.1B, and its quantized versions, which performs comparably to open
models around 1.1B on MultiPL-E Java code benchmarks, making them ideal for
desktop deployment. This paper establishes the foundation for specialized
models across languages and sizes for a family of NT Models.","[{'name': 'Kamalkumar Rathinasamy'}, {'name': 'Balaji A J'}, {'name': 'Ankush Kumar'}, {'name': 'Gagan Gayari'}, {'name': 'Harshini K'}, {'name': 'Rajab Ali Mondal'}, {'name': 'Sreenivasa Raghavan K S'}, {'name': 'Swayam Singh'}]",2024-07-04T13:54:24Z
http://arxiv.org/abs/2407.12842v1,http://arxiv.org/abs/2407.12842v1,MS2SL: Multimodal Spoken Data-Driven Continuous Sign Language Production,"Sign language understanding has made significant strides; however, there is
still no viable solution for generating sign sequences directly from entire
spoken content, e.g., text or speech. In this paper, we propose a unified
framework for continuous sign language production, easing communication between
sign and non-sign language users. In particular, a sequence diffusion model,
utilizing embeddings extracted from text or speech, is crafted to generate sign
predictions step by step. Moreover, by creating a joint embedding space for
text, audio, and sign, we bind these modalities and leverage the semantic
consistency among them to provide informative feedback for the model training.
This embedding-consistency learning strategy minimizes the reliance on sign
triplets and ensures continuous model refinement, even with a missing audio
modality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our
model achieves competitive performance in sign language production.","[{'name': 'Jian Ma'}, {'name': 'Wenguan Wang'}, {'name': 'Yi Yang'}, {'name': 'Feng Zheng'}]",2024-07-04T13:53:50Z
http://arxiv.org/abs/2407.03937v1,http://arxiv.org/abs/2407.03937v1,"TongGu: Mastering Classical Chinese Understanding with
  Knowledge-Grounded Large Language Models","Classical Chinese is a gateway to the rich heritage and wisdom of ancient
China, yet its complexities pose formidable comprehension barriers for most
modern people without specialized knowledge. While Large Language Models (LLMs)
have shown remarkable capabilities in Natural Language Processing (NLP), they
struggle with Classical Chinese Understanding (CCU), especially in
data-demanding and knowledge-intensive tasks. In response to this dilemma, we
propose \textbf{TongGu} (mean understanding ancient and modern), the first
CCU-specific LLM, underpinned by three core contributions. First, we construct
a two-stage instruction-tuning dataset ACCN-INS derived from rich classical
Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we
propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,
enabling TongGu to acquire new capabilities while preserving its foundational
knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)
technique to reduce hallucinations based on knowledge-grounding. Extensive
experiments across 24 diverse CCU tasks validate TongGu's superior ability,
underscoring the effectiveness of RAT and CCU-RAG. The model and dataset will
be public available.","[{'name': 'Jiahuan Cao'}, {'name': 'Dezhi Peng'}, {'name': 'Peirong Zhang'}, {'name': 'Yongxin Shi'}, {'name': 'Yang Liu'}, {'name': 'Kai Ding'}, {'name': 'Lianwen Jin'}]",2024-07-04T13:52:23Z
http://arxiv.org/abs/2407.03916v1,http://arxiv.org/abs/2407.03916v1,Entity-Level Sentiment: More than the Sum of Its Parts,"In sentiment analysis of longer texts, there may be a variety of topics
discussed, of entities mentioned, and of sentiments expressed regarding each
entity. We find a lack of studies exploring how such texts express their
sentiment towards each entity of interest, and how these sentiments can be
modelled. In order to better understand how sentiment regarding persons and
organizations (each entity in our scope) is expressed in longer texts, we have
collected a dataset of expert annotations where the overall sentiment regarding
each entity is identified, together with the sentence-level sentiment for these
entities separately. We show that the reader's perceived sentiment regarding an
entity often differs from an arithmetic aggregation of sentiments at the
sentence level. Only 70\% of the positive and 55\% of the negative entities
receive a correct overall sentiment label when we aggregate the
(human-annotated) sentiment labels for the sentences where the entity is
mentioned. Our dataset reveals the complexity of entity-specific sentiment in
longer texts, and allows for more precise modelling and evaluation of such
sentiment expressions.","[{'name': 'Egil Rønningstad'}, {'name': 'Roman Klinger'}, {'name': 'Erik Velldal'}, {'name': 'Lilja Øvrelid'}]",2024-07-04T13:21:07Z
http://arxiv.org/abs/2407.12841v1,http://arxiv.org/abs/2407.12841v1,"What to do if language models disagree? Black-box model ensembling for
  textual and visual question answering","A diverse range of large language models (LLMs), e.g., ChatGPT, and visual
question answering (VQA) models, e.g., BLIP, have been developed for solving
textual and visual question answering tasks. However, both LLMs and VQA models
encounter challenges when applied to task-specific datasets. Fine-tuning these
models is either difficult, as it requires access via APIs, rendering them as
black-boxes, or costly due to the need of tuning a large number of parameters.
To address this, we introduce InfoSel, a data-efficient and lightweight
ensemble method that learns to dynamically pick the winner from existing
black-box models for predictions on both textual and multimodal visual question
answering tasks. Unlike traditional ensemble models, InfoSel does not rely on
prediction probabilities or confidences, which typically are not available in
black-box models. Experimental results on four datasets demonstrate that our
approach achieves an absolute increase of up to +5.27% in the F1-score compared
to standalone LLMs. Remarkably, this improvement is achieved by utilizing only
1K training instances and 110M model parameters for training task-specific
ensemble models.","[{'name': 'Yuxi Xia'}, {'name': 'Kilm Zaporojets'}, {'name': 'Benjamin Roth'}]",2024-07-04T12:59:10Z
http://arxiv.org/abs/2407.03895v1,http://arxiv.org/abs/2407.03895v1,"Scoping Review of Active Learning Strategies and their Evaluation
  Environments for Entity Recognition Tasks","We conducted a scoping review for active learning in the domain of natural
language processing (NLP), which we summarize in accordance with the PRISMA-ScR
guidelines as follows:
  Objective: Identify active learning strategies that were proposed for entity
recognition and their evaluation environments (datasets, metrics, hardware,
execution time). Design: We used Scopus and ACM as our search engines. We
compared the results with two literature surveys to assess the search quality.
We included peer-reviewed English publications introducing or comparing active
learning strategies for entity recognition. Results: We analyzed 62 relevant
papers and identified 106 active learning strategies. We grouped them into
three categories: exploitation-based (60x), exploration-based (14x), and hybrid
strategies (32x). We found that all studies used the F1-score as an evaluation
metric. Information about hardware (6x) and execution time (13x) was only
occasionally included. The 62 papers used 57 different datasets to evaluate
their respective strategies. Most datasets contained newspaper articles or
biomedical/medical data. Our analysis revealed that 26 out of 57 datasets are
publicly accessible.
  Conclusion: Numerous active learning strategies have been identified, along
with significant open questions that still need to be addressed. Researchers
and practitioners face difficulties when making data-driven decisions about
which active learning strategy to adopt. Conducting comprehensive empirical
comparisons using the evaluation environment proposed in this study could help
establish best practices in the domain.","[{'name': 'Philipp Kohl'}, {'name': 'Yoka Krämer'}, {'name': 'Claudia Fohry'}, {'name': 'Bodo Kraft'}]",2024-07-04T12:40:35Z
http://arxiv.org/abs/2407.03884v1,http://arxiv.org/abs/2407.03884v1,Planning with Large Language Models for Conversational Agents,"Controllability and proactivity are crucial properties of autonomous
conversational agents (CAs). Controllability requires the CAs to follow the
standard operating procedures (SOPs), such as verifying identity before
activating credit cards. Proactivity requires the CAs to guide the conversation
towards the goal during user uncooperation, such as persuasive dialogue.
Existing research cannot be unified with controllability, proactivity, and low
manual annotation. To bridge this gap, we propose a new framework for
planning-based conversational agents (PCA) powered by large language models
(LLMs), which only requires humans to define tasks and goals for the LLMs.
Before conversation, LLM plans the core and necessary SOP for dialogue offline.
During the conversation, LLM plans the best action path online referring to the
SOP, and generates responses to achieve process controllability. Subsequently,
we propose a semi-automatic dialogue data creation framework and curate a
high-quality dialogue dataset (PCA-D). Meanwhile, we develop multiple variants
and evaluation metrics for PCA, e.g., planning with Monte Carlo Tree Search
(PCA-M), which searches for the optimal dialogue action while satisfying SOP
constraints and achieving the proactive of the dialogue. Experiment results
show that LLMs finetuned on PCA-D can significantly improve the performance and
generalize to unseen domains. PCA-M outperforms other CoT and ToT baselines in
terms of conversation controllability, proactivity, task success rate, and
overall logical coherence, and is applicable in industry dialogue scenarios.
The dataset and codes are available at XXXX.","[{'name': 'Zhigen Li'}, {'name': 'Jianxiang Peng'}, {'name': 'Yanmeng Wang'}, {'name': 'Tianhao Shen'}, {'name': 'Minghui Zhang'}, {'name': 'Linxi Su'}, {'name': 'Shang Wu'}, {'name': 'Yihang Wu'}, {'name': 'Yuqian Wang'}, {'name': 'Ye Wang'}, {'name': 'Wei Hu'}, {'name': 'Jianfeng Li'}, {'name': 'Shaojun Wang'}, {'name': 'Jing Xiao'}, {'name': 'Deyi Xiong'}]",2024-07-04T12:23:02Z
http://arxiv.org/abs/2407.16893v1,http://arxiv.org/abs/2407.16893v1,"The Price of Prompting: Profiling Energy Use in Large Language Models
  Inference","In the rapidly evolving realm of artificial intelligence, deploying large
language models (LLMs) poses increasingly pressing computational and
environmental challenges. This paper introduces MELODI - Monitoring Energy
Levels and Optimization for Data-driven Inference - a multifaceted framework
crafted to monitor and analyze the energy consumed during LLM inference
processes. MELODI enables detailed observations of power consumption dynamics
and facilitates the creation of a comprehensive dataset reflective of energy
efficiency across varied deployment scenarios. The dataset, generated using
MELODI, encompasses a broad spectrum of LLM deployment frameworks, multiple
language models, and extensive prompt datasets, enabling a comparative analysis
of energy use. Using the dataset, we investigate how prompt attributes,
including length and complexity, correlate with energy expenditure. Our
findings indicate substantial disparities in energy efficiency, suggesting
ample scope for optimization and adoption of sustainable measures in LLM
deployment. Our contribution lies not only in the MELODI framework but also in
the novel dataset, a resource that can be expanded by other researchers. Thus,
MELODI is a foundational tool and dataset for advancing research into
energy-conscious LLM deployment, steering the field toward a more sustainable
future.","[{'name': 'Erik Johannes Husom'}, {'name': 'Arda Goknil'}, {'name': 'Lwin Khin Shar'}, {'name': 'Sagar Sen'}]",2024-07-04T12:16:28Z
http://arxiv.org/abs/2407.03876v1,http://arxiv.org/abs/2407.03876v1,DART: Deep Adversarial Automated Red Teaming for LLM Safety,"Manual Red teaming is a commonly-used method to identify vulnerabilities in
large language models (LLMs), which, is costly and unscalable. In contrast,
automated red teaming uses a Red LLM to automatically generate adversarial
prompts to the Target LLM, offering a scalable way for safety vulnerability
detection. However, the difficulty of building a powerful automated Red LLM
lies in the fact that the safety vulnerabilities of the Target LLM are
dynamically changing with the evolution of the Target LLM. To mitigate this
issue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in
which the Red LLM and Target LLM are deeply and dynamically interacting with
each other in an iterative manner. In each iteration, in order to generate
successful attacks as many as possible, the Red LLM not only takes into account
the responses from the Target LLM, but also adversarially adjust its attacking
directions by monitoring the global diversity of generated attacks across
multiple iterations. Simultaneously, to explore dynamically changing safety
vulnerabilities of the Target LLM, we allow the Target LLM to enhance its
safety via an active learning based data selection mechanism. Experimential
results demonstrate that DART significantly reduces the safety risk of the
target LLM. For human evaluation on Anthropic Harmless dataset, compared to the
instruction-tuning target LLM, DART eliminates the violation risks by 53.4\%.
We will release the datasets and codes of DART soon.","[{'name': 'Bojian Jiang'}, {'name': 'Yi Jing'}, {'name': 'Tianhao Shen'}, {'name': 'Qing Yang'}, {'name': 'Deyi Xiong'}]",2024-07-04T12:14:27Z
http://arxiv.org/abs/2407.03850v1,http://arxiv.org/abs/2407.03850v1,"HYBRINFOX at CheckThat! 2024 -- Task 1: Enhancing Language Models with
  Structured Information for Check-Worthiness Estimation","This paper summarizes the experiments and results of the HYBRINFOX team for
the CheckThat! 2024 - Task 1 competition. We propose an approach enriching
Language Models such as RoBERTa with embeddings produced by triples (subject ;
predicate ; object) extracted from the text sentences. Our analysis of the
developmental data shows that this method improves the performance of Language
Models alone. On the evaluation data, its best performance was in English,
where it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On
the other languages (Dutch and Arabic), it obtained more mixed results. Future
research tracks are identified toward adapting this processing pipeline to more
recent Large Language Models.","[{'name': 'Géraud Faye'}, {'name': 'Morgane Casanova'}, {'name': 'Benjamin Icard'}, {'name': 'Julien Chanson'}, {'name': 'Guillaume Gadek'}, {'name': 'Guillaume Gravier'}, {'name': 'Paul Égré'}]",2024-07-04T11:33:54Z
http://arxiv.org/abs/2407.03841v1,http://arxiv.org/abs/2407.03841v1,On the Benchmarking of LLMs for Open-Domain Dialogue Evaluation,"Large Language Models (LLMs) have showcased remarkable capabilities in
various Natural Language Processing tasks. For automatic open-domain dialogue
evaluation in particular, LLMs have been seamlessly integrated into evaluation
frameworks, and together with human evaluation, compose the backbone of most
evaluations. However, existing evaluation benchmarks often rely on outdated
datasets and evaluate aspects like Fluency and Relevance, which fail to
adequately capture the capabilities and limitations of state-of-the-art chatbot
models.
  This paper critically examines current evaluation benchmarks, highlighting
that the use of older response generators and quality aspects fail to
accurately reflect modern chatbot capabilities. A small annotation experiment
on a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as
GPT-4 struggle to detect actual deficiencies in dialogues generated by current
LLM chatbots.","[{'name': 'John Mendonça'}, {'name': 'Alon Lavie'}, {'name': 'Isabel Trancoso'}]",2024-07-04T11:14:47Z
http://arxiv.org/abs/2407.03818v1,http://arxiv.org/abs/2407.03818v1,"ConText at WASSA 2024 Empathy and Personality Shared Task:
  History-Dependent Embedding Utterance Representations for Empathy and Emotion
  Prediction in Conversations","Empathy and emotion prediction are key components in the development of
effective and empathetic agents, amongst several other applications. The WASSA
shared task on empathy and emotion prediction in interactions presents an
opportunity to benchmark approaches to these tasks. Appropriately selecting and
representing the historical context is crucial in the modelling of empathy and
emotion in conversations. In our submissions, we model empathy, emotion
polarity and emotion intensity of each utterance in a conversation by feeding
the utterance to be classified together with its conversational context, i.e.,
a certain number of previous conversational turns, as input to an encoder
Pre-trained Language Model, to which we append a regression head for
prediction. We also model perceived counterparty empathy of each interlocutor
by feeding all utterances from the conversation and a token identifying the
interlocutor for which we are predicting the empathy. Our system officially
ranked $1^{st}$ at the CONV-turn track and $2^{nd}$ at the CONV-dialog track.","[{'name': 'Patrícia Pereira'}, {'name': 'Helena Moniz'}, {'name': 'Joao Paulo Carvalho'}]",2024-07-04T10:44:59Z
http://arxiv.org/abs/2407.03809v1,http://arxiv.org/abs/2407.03809v1,"Finetuning End-to-End Models for Estonian Conversational Spoken Language
  Translation","This paper investigates the finetuning of end-to-end models for bidirectional
Estonian-English and Estonian-Russian conversational speech-to-text
translation. Due to the limited availability of speech translation data for
Estonian, we created additional training data by web scraping and synthesizing
data from speech recognition datasets using machine translation. We evaluated
three publicly available end-to-end models: Whisper, OWSM 3.1, and SeamlessM4T.
Our results indicate that fine-tuning with synthetic data enhances translation
accuracy by a large margin, with SeamlessM4T matching or surpassing cascaded
speech translation systems that use state-of-the-art speech recognition and
machine translation models.","[{'name': 'Tiia Sildam'}, {'name': 'Andra Velve'}, {'name': 'Tanel Alumäe'}]",2024-07-04T10:33:12Z
http://arxiv.org/abs/2407.03805v2,http://arxiv.org/abs/2407.03805v2,"Cognitive Modeling with Scaffolded LLMs: A Case Study of Referential
  Expression Generation","To what extent can LLMs be used as part of a cognitive model of language
generation? In this paper, we approach this question by exploring a
neuro-symbolic implementation of an algorithmic cognitive model of referential
expression generation by Dale & Reiter (1995). The symbolic task analysis
implements the generation as an iterative procedure that scaffolds symbolic and
gpt-3.5-turbo-based modules. We compare this implementation to an ablated model
and a one-shot LLM-only baseline on the A3DS dataset (Tsvilodub & Franke,
2023). We find that our hybrid approach is cognitively plausible and performs
well in complex contexts, while allowing for more open-ended modeling of
language generation in a larger domain.","[{'name': 'Polina Tsvilodub'}, {'name': 'Michael Franke'}, {'name': 'Fausto Carcassi'}]",2024-07-04T10:28:48Z
http://arxiv.org/abs/2407.03791v1,http://arxiv.org/abs/2407.03791v1,"M$\mathbf5$ -- A Diverse Benchmark to Assess the Performance of Large
  Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks","Since the release of ChatGPT, the field of Natural Language Processing has
experienced rapid advancements, particularly in Large Language Models (LLMs)
and their multimodal counterparts, Large Multimodal Models (LMMs). Despite
their impressive capabilities, LLMs often exhibit significant performance
disparities across different languages and cultural contexts, as demonstrated
by various text-only benchmarks. However, current research lacks such
benchmarks for multimodal visio-linguistic settings. This work fills this gap
by introducing M5, the first comprehensive benchmark designed to evaluate LMMs
on diverse vision-language tasks within a multilingual and multicultural
context. M5 includes eight datasets covering five tasks and $41$ languages,
with a focus on underrepresented languages and culturally diverse images.
Furthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a
new Visio-Linguistic Outlier Detection task, in which all evaluated open-source
models fail to significantly surpass the random baseline. Through extensive
evaluation and analyses, we highlight substantial task-agnostic performance
disparities between high- and low-resource languages. Moreover, we show that
larger models do not necessarily outperform smaller ones in a multilingual
setting.","[{'name': 'Florian Schneider'}, {'name': 'Sunayana Sitaram'}]",2024-07-04T09:55:04Z
http://arxiv.org/abs/2407.03788v2,http://arxiv.org/abs/2407.03788v2,"Meta-optimized Angular Margin Contrastive Framework for Video-Language
  Representation Learning","Data quality stands at the forefront of deciding the effectiveness of
video-language representation learning. However, video-text pairs in previous
data typically do not align perfectly with each other, which might lead to
video-language representations that do not accurately reflect cross-modal
semantics. Moreover, previous data also possess an uneven distribution of
concepts, thereby hampering the downstream performance across unpopular
subjects. To address these problems, we propose a contrastive objective with a
subtractive angular margin to regularize cross-modal representations in their
effort to reach perfect similarity. Furthermore, to adapt to the non-uniform
concept distribution, we propose a multi-layer perceptron (MLP)-parameterized
weighting function that maps loss values to sample weights which enable dynamic
adjustment of the model's focus throughout the training. With the training
guided by a small amount of unbiased meta-data and augmented by video-text data
generated by large vision-language model, we improve video-language
representations and achieve superior performances on commonly used video
question answering and text-video retrieval datasets.","[{'name': 'Thong Nguyen'}, {'name': 'Yi Bin'}, {'name': 'Xiaobao Wu'}, {'name': 'Xinshuai Dong'}, {'name': 'Zhiyuan Hu'}, {'name': 'Khoi Le'}, {'name': 'Cong-Duy Nguyen'}, {'name': 'See-Kiong Ng'}, {'name': 'Luu Anh Tuan'}]",2024-07-04T09:52:17Z
http://arxiv.org/abs/2407.03779v1,http://arxiv.org/abs/2407.03779v1,"Functional Faithfulness in the Wild: Circuit Discovery with
  Differentiable Computation Graph Pruning","In this paper, we introduce a comprehensive reformulation of the task known
as Circuit Discovery, along with DiscoGP, a novel and effective algorithm based
on differentiable masking for discovering circuits. Circuit discovery is the
task of interpreting the computational mechanisms of language models (LMs) by
dissecting their functions and capabilities into sparse subnetworks (circuits).
We identified two major limitations in existing circuit discovery efforts: (1)
a dichotomy between weight-based and connection-edge-based approaches forces
researchers to choose between pruning connections or weights, thereby limiting
the scope of mechanistic interpretation of LMs; (2) algorithms based on
activation patching tend to identify circuits that are neither functionally
faithful nor complete. The performance of these identified circuits is
substantially reduced, often resulting in near-random performance in isolation.
Furthermore, the complement of the circuit -- i.e., the original LM with the
identified circuit removed -- still retains adequate performance, indicating
that essential components of a complete circuits are missed by existing
methods.
  DiscoGP successfully addresses the two aforementioned issues and demonstrates
state-of-the-art faithfulness, completeness, and sparsity. The effectiveness of
the algorithm and its novel structure open up new avenues of gathering new
insights into the internal workings of generative AI.","[{'name': 'Lei Yu'}, {'name': 'Jingcheng Niu'}, {'name': 'Zining Zhu'}, {'name': 'Gerald Penn'}]",2024-07-04T09:42:25Z
http://arxiv.org/abs/2407.03778v1,http://arxiv.org/abs/2407.03778v1,"From Data to Commonsense Reasoning: The Use of Large Language Models for
  Explainable AI","Commonsense reasoning is a difficult task for a computer, but a critical
skill for an artificial intelligence (AI). It can enhance the explainability of
AI models by enabling them to provide intuitive and human-like explanations for
their decisions. This is necessary in many areas especially in question
answering (QA), which is one of the most important tasks of natural language
processing (NLP). Over time, a multitude of methods have emerged for solving
commonsense reasoning problems such as knowledge-based approaches using formal
logic or linguistic analysis. In this paper, we investigate the effectiveness
of large language models (LLMs) on different QA tasks with a focus on their
abilities in reasoning and explainability. We study three LLMs: GPT-3.5, Gemma
and Llama 3. We further evaluate the LLM results by means of a questionnaire.
We demonstrate the ability of LLMs to reason with commonsense as the models
outperform humans on different datasets. While GPT-3.5's accuracy ranges from
56% to 93% on various QA benchmarks, Llama 3 achieved a mean accuracy of 90% on
all eleven datasets. Thereby Llama 3 is outperforming humans on all datasets
with an average 21% higher accuracy over ten datasets. Furthermore, we can
appraise that, in the sense of explainable artificial intelligence (XAI),
GPT-3.5 provides good explanations for its decisions. Our questionnaire
revealed that 66% of participants rated GPT-3.5's explanations as either ""good""
or ""excellent"". Taken together, these findings enrich our understanding of
current LLMs and pave the way for future investigations of reasoning and
explainability.","[{'name': 'Stefanie Krause'}, {'name': 'Frieder Stolzenburg'}]",2024-07-04T09:38:49Z
http://arxiv.org/abs/2407.03770v1,http://arxiv.org/abs/2407.03770v1,"HYBRINFOX at CheckThat! 2024 -- Task 2: Enriching BERT Models with the
  Expert System VAGO for Subjectivity Detection","This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity
detection of the CLEF 2024 CheckThat! competition. The specificity of the
method is to use a hybrid system, combining a RoBERTa model, fine-tuned for
subjectivity detection, a frozen sentence-BERT (sBERT) model to capture
semantics, and several scores calculated by the English version of the expert
system VAGO, developed independently of this task to measure vagueness and
subjectivity in texts based on the lexicon. In English, the HYBRINFOX method
ranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the
other languages, the method used a translation step into English, producing
more mixed results (ranking 1st in Multilingual and 2nd in Italian over the
baseline, but under the baseline in Bulgarian, German, and Arabic). We explain
the principles of our hybrid approach, and outline ways in which the method
could be improved for other languages besides English.","[{'name': 'Morgane Casanova'}, {'name': 'Julien Chanson'}, {'name': 'Benjamin Icard'}, {'name': 'Géraud Faye'}, {'name': 'Guillaume Gadek'}, {'name': 'Guillaume Gravier'}, {'name': 'Paul Égré'}]",2024-07-04T09:29:19Z
http://arxiv.org/abs/2407.03759v1,http://arxiv.org/abs/2407.03759v1,"Convolutional vs Large Language Models for Software Log Classification
  in Edge-Deployable Cellular Network Testing","Software logs generated by sophisticated network emulators in the
telecommunications industry, such as VIAVI TM500, are extremely complex, often
comprising tens of thousands of text lines with minimal resemblance to natural
language. Only specialised expert engineers can decipher such logs and
troubleshoot defects in test runs. While AI offers a promising solution for
automating defect triage, potentially leading to massive revenue savings for
companies, state-of-the-art large language models (LLMs) suffer from
significant drawbacks in this specialised domain. These include a constrained
context window, limited applicability to text beyond natural language, and high
inference costs. To address these limitations, we propose a compact
convolutional neural network (CNN) architecture that offers a context window
spanning up to 200,000 characters and achieves over 96% accuracy (F1>0.9) in
classifying multifaceted software logs into various layers in the
telecommunications protocol stack. Specifically, the proposed model is capable
of identifying defects in test runs and triaging them to the relevant
department, formerly a manual engineering process that required expert
knowledge. We evaluate several LLMs; LLaMA2-7B, Mixtral 8x7B, Flan-T5, BERT and
BigBird, and experimentally demonstrate their shortcomings in our specialized
application. Despite being lightweight, our CNN significantly outperforms
LLM-based approaches in telecommunications log classification while minimizing
the cost of production. Our defect triaging AI model is deployable on edge
devices without dedicated hardware and widely applicable across software logs
in various industries.","[{'name': 'Achintha Ihalage'}, {'name': 'Sayed M. Taheri'}, {'name': 'Faris Muhammad'}, {'name': 'Hamed Al-Raweshidy'}]",2024-07-04T09:12:08Z
http://arxiv.org/abs/2407.03748v1,http://arxiv.org/abs/2407.03748v1,"Argument Mining in Data Scarce Settings: Cross-lingual Transfer and
  Few-shot Techniques","Recent research on sequence labelling has been exploring different strategies
to mitigate the lack of manually annotated data for the large majority of the
world languages. Among others, the most successful approaches have been based
on (i) the cross-lingual transfer capabilities of multilingual pre-trained
language models (model-transfer), (ii) data translation and label projection
(data-transfer) and (iii), prompt-based learning by reusing the mask objective
to exploit the few-shot capabilities of pre-trained language models (few-shot).
Previous work seems to conclude that model-transfer outperforms data-transfer
methods and that few-shot techniques based on prompting are superior to
updating the model's weights via fine-tuning. In this paper, we empirically
demonstrate that, for Argument Mining, a sequence labelling task which requires
the detection of long and complex discourse structures, previous insights on
cross-lingual transfer or few-shot learning do not apply. Contrary to previous
work, we show that for Argument Mining data transfer obtains better results
than model-transfer and that fine-tuning outperforms few-shot methods.
Regarding the former, the domain of the dataset used for data-transfer seems to
be a deciding factor, while, for few-shot, the type of task (length and
complexity of the sequence spans) and sampling method prove to be crucial.","[{'name': 'Anar Yeginbergen'}, {'name': 'Maite Oronoz'}, {'name': 'Rodrigo Agerri'}]",2024-07-04T08:59:17Z
http://arxiv.org/abs/2407.03734v1,http://arxiv.org/abs/2407.03734v1,Improving Self-supervised Pre-training using Accent-Specific Codebooks,"Speech accents present a serious challenge to the performance of
state-of-the-art end-to-end Automatic Speech Recognition (ASR) systems. Even
with self-supervised learning and pre-training of ASR models, accent invariance
is seldom achieved. In this work, we propose an accent-aware adaptation
technique for self-supervised learning that introduces a trainable set of
accent-specific codebooks to the self-supervised architecture. These learnable
codebooks enable the model to capture accent specific information during
pre-training, that is further refined during ASR finetuning. On the Mozilla
Common Voice dataset, our proposed approach outperforms all other
accent-adaptation approaches on both seen and unseen English accents, with up
to 9% relative reduction in word error rate (WER).","[{'name': 'Darshan Prabhu'}, {'name': 'Abhishek Gupta'}, {'name': 'Omkar Nitsure'}, {'name': 'Preethi Jyothi'}, {'name': 'Sriram Ganapathy'}]",2024-07-04T08:33:52Z
http://arxiv.org/abs/2407.03720v1,http://arxiv.org/abs/2407.03720v1,Query-oriented Data Augmentation for Session Search,"Modeling contextual information in a search session has drawn more and more
attention when understanding complex user intents. Recent methods are all
data-driven, i.e., they train different models on large-scale search log data
to identify the relevance between search contexts and candidate documents. The
common training paradigm is to pair the search context with different candidate
documents and train the model to rank the clicked documents higher than the
unclicked ones. However, this paradigm neglects the symmetric nature of the
relevance between the session context and document, i.e., the clicked documents
can also be paired with different search contexts when training. In this work,
we propose query-oriented data augmentation to enrich search logs and empower
the modeling. We generate supplemental training pairs by altering the most
important part of a search context, i.e., the current query, and train our
model to rank the generated sequence along with the original sequence. This
approach enables models to learn that the relevance of a document may vary as
the session context changes, leading to a better understanding of users' search
patterns. We develop several strategies to alter the current query, resulting
in new training data with varying degrees of difficulty. Through
experimentation on two extensive public search logs, we have successfully
demonstrated the effectiveness of our model.","[{'name': 'Haonan Chen'}, {'name': 'Zhicheng Dou'}, {'name': 'Yutao Zhu'}, {'name': 'Ji-Rong Wen'}]",2024-07-04T08:08:33Z
http://arxiv.org/abs/2407.03718v2,http://arxiv.org/abs/2407.03718v2,Multi-Convformer: Extending Conformer with Multiple Convolution Kernels,"Convolutions have become essential in state-of-the-art end-to-end Automatic
Speech Recognition~(ASR) systems due to their efficient modelling of local
context. Notably, its use in Conformers has led to superior performance
compared to vanilla Transformer-based ASR systems. While components other than
the convolution module in the Conformer have been reexamined, altering the
convolution module itself has been far less explored. Towards this, we
introduce Multi-Convformer that uses multiple convolution kernels within the
convolution module of the Conformer in conjunction with gating. This helps in
improved modeling of local dependencies at varying granularities. Our model
rivals existing Conformer variants such as CgMLP and E-Branchformer in
performance, while being more parameter efficient. We empirically compare our
approach with Conformer and its variants across four different datasets and
three different modelling paradigms and show up to 8% relative word error
rate~(WER) improvements.","[{'name': 'Darshan Prabhu'}, {'name': 'Yifan Peng'}, {'name': 'Preethi Jyothi'}, {'name': 'Shinji Watanabe'}]",2024-07-04T08:08:12Z
http://arxiv.org/abs/2407.03689v1,http://arxiv.org/abs/2407.03689v1,"Text2TimeSeries: Enhancing Financial Forecasting through Time Series
  Prediction Updates with Event-Driven Insights from Large Language Models","Time series models, typically trained on numerical data, are designed to
forecast future values. These models often rely on weighted averaging
techniques over time intervals. However, real-world time series data is seldom
isolated and is frequently influenced by non-numeric factors. For instance,
stock price fluctuations are impacted by daily random events in the broader
world, with each event exerting a unique influence on price signals.
Previously, forecasts in financial markets have been approached in two main
ways: either as time-series problems over price sequence or sentiment analysis
tasks. The sentiment analysis tasks aim to determine whether news events will
have a positive or negative impact on stock prices, often categorizing them
into discrete labels. Recognizing the need for a more comprehensive approach to
accurately model time series prediction, we propose a collaborative modeling
framework that incorporates textual information about relevant events for
predictions. Specifically, we leverage the intuition of large language models
about future changes to update real number time series predictions. We
evaluated the effectiveness of our approach on financial market data.","[{'name': 'Litton Jose Kurisinkel'}, {'name': 'Pruthwik Mishra'}, {'name': 'Yue Zhang'}]",2024-07-04T07:21:38Z
http://arxiv.org/abs/2407.03646v2,http://arxiv.org/abs/2407.03646v2,"Differentiating between human-written and AI-generated texts using
  linguistic features automatically extracted from an online computational tool","While extensive research has focused on ChatGPT in recent years, very few
studies have systematically quantified and compared linguistic features between
human-written and Artificial Intelligence (AI)-generated language. This study
aims to investigate how various linguistic components are represented in both
types of texts, assessing the ability of AI to emulate human writing. Using
human-authored essays as a benchmark, we prompted ChatGPT to generate essays of
equivalent length. These texts were analyzed using Open Brain AI, an online
computational tool, to extract measures of phonological, morphological,
syntactic, and lexical constituents. Despite AI-generated texts appearing to
mimic human speech, the results revealed significant differences across
multiple linguistic features such as consonants, word stress, nouns, verbs,
pronouns, direct objects, prepositional modifiers, and use of difficult words
among others. These findings underscore the importance of integrating automated
tools for efficient language assessment, reducing time and effort in data
analysis. Moreover, they emphasize the necessity for enhanced training
methodologies to improve the capacity of AI for producing more human-like text.",[{'name': 'Georgios P. Georgiou'}],2024-07-04T05:37:09Z
http://arxiv.org/abs/2407.03645v2,http://arxiv.org/abs/2407.03645v2,"Continual Learning Optimizations for Auto-regressive Decoder of
  Multilingual ASR systems","Continual Learning (CL) involves fine-tuning pre-trained models with new data
while maintaining the performance on the pre-trained data. This is particularly
relevant for expanding multilingual ASR (MASR) capabilities. However, existing
CL methods, mainly designed for computer vision and reinforcement learning
tasks, often yield sub-optimal results when directly applied to MASR. We
hypothesise that this is because CL of the auto-regressive decoder in the MASR
model is difficult. To verify this, we propose four optimizations on the
decoder. They include decoder-layer gradient surgery, freezing unused token
embeddings, suppressing output of newly added tokens, and learning rate
re-scaling. Our experiments on adapting Whisper to 10 unseen languages from the
Common Voice dataset demonstrate that these optimizations reduce the Average
Word Error Rate (AWER) of pretrained languages from 14.2% to 12.4% compared
with Experience Replay, without compromising the AWER of new languages.","[{'name': 'Chin Yuen Kwok'}, {'name': 'Jia Qi Yip'}, {'name': 'Eng Siong Chng'}]",2024-07-04T05:35:47Z
