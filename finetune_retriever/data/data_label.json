[
    {
        "query": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
        "pos": [
            "Extracting social determinants of health (SDoH) from unstructured medical\nnotes depends heavily on labor-intensive annotations, which are typically\ntask-specific, hampering reusability and limiting sharing. In this study we\nintroduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)\nmethod leveraging contrastive examples and concise instructions to extract SDoH\nwithout relying on extensive medical annotations or costly human intervention.\nIt achieved tenfold and twentyfold reductions in time and cost respectively,\nand superior consistency with human annotators measured by Cohen's kappa of up\nto 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the\nstrengths of both, ensuring high accuracy and computational efficiency while\nconsistently maintaining 0.90+ AUROC scores. Testing across three distinct\ndatasets has confirmed its robustness and accuracy. This study highlights the\npotential of leveraging LLMs to revolutionize medical note classification,\ndemonstrating their capability to achieve highly accurate classifications with\nsignificantly reduced time and cost."
        ],
        "neg": []
    },
    {
        "query": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
        "pos": [
            "Personality psychologists have analyzed the relationship between personality\nand safety behaviors in human society. Although Large Language Models (LLMs)\ndemonstrate personality traits, the relationship between personality traits and\nsafety abilities in LLMs still remains a mystery. In this paper, we discover\nthat LLMs' personality traits are closely related to their safety abilities,\ni.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.\nMeanwhile, the safety alignment generally increases various LLMs' Extraversion,\nSensing, and Judging traits. According to such findings, we can edit LLMs'\npersonality traits and improve their safety performance, e.g., inducing\npersonality from ISTJ to ISTP resulted in a relative improvement of\napproximately 43% and 10% in privacy and fairness performance, respectively.\nAdditionally, we find that LLMs with different personality traits are\ndifferentially susceptible to jailbreak. This study pioneers the investigation\nof LLM safety from a personality perspective, providing new insights into LLM\nsafety enhancement.",
            "Large language models (LLMs) with in-context learning have significantly\nimproved the performance of text-to-SQL task. Previous works generally focus on\nusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.\nHowever, they are mostly hard to handle large databases with numerous tables\nand columns, and usually ignore the significance of pre-processing database and\nextracting valuable information for more efficient prompt engineering. Based on\nabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework for\nin-context prompt engineering, which consists of three modules that retrieve\nconcise tables and columns as schema, and targeted examples for in-context\nlearning. Experiment results demonstrate that our model achieves better\nperformance than several competitive baselines on public datasets BIRD and\nSpider."
        ],
        "neg": []
    },
    {
        "query": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
        "pos": [
            "Large language models (LLMs) have achieved remarkable performance on various\nNLP tasks, yet their potential in more challenging and domain-specific task,\nsuch as finance, has not been fully explored. In this paper, we present\nCFinBench: a meticulously crafted, the most comprehensive evaluation benchmark\nto date, for assessing the financial knowledge of LLMs under Chinese context.\nIn practice, to better align with the career trajectory of Chinese financial\npractitioners, we build a systematic evaluation from 4 first-level categories:\n(1) Financial Subject: whether LLMs can memorize the necessary basic knowledge\nof financial subjects, such as economics, statistics and auditing. (2)\nFinancial Qualification: whether LLMs can obtain the needed financial qualified\ncertifications, such as certified public accountant, securities qualification\nand banking qualification. (3) Financial Practice: whether LLMs can fulfill the\npractical financial jobs, such as tax consultant, junior accountant and\nsecurities analyst. (4) Financial Law: whether LLMs can meet the requirement of\nfinancial laws and regulations, such as tax law, insurance law and economic\nlaw. CFinBench comprises 99,100 questions spanning 43 second-level categories\nwith 3 question types: single-choice, multiple-choice and judgment. We conduct\nextensive experiments of 50 representative LLMs with various model size on\nCFinBench. The results show that GPT4 and some Chinese-oriented models lead the\nbenchmark, with the highest average accuracy being 60.16%, highlighting the\nchallenge presented by CFinBench. The dataset and evaluation code are available\nat https://cfinbench.github.io/."
        ],
        "neg": []
    },
    {
        "query": "Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies.",
        "pos": [
            "Speech Integrated Large Language Models (SILLMs) combine large language\nmodels with speech perception to perform diverse tasks, such as emotion\nrecognition to speaker verification, demonstrating universal audio\nunderstanding capability. However, these models may amplify biases present in\ntraining data, potentially leading to biased access to information for\nmarginalized groups. This work introduces a curated spoken bias evaluation\ntoolkit and corresponding dataset. We evaluate gender bias in SILLMs across\nfour semantic-related tasks: speech-to-text translation (STT), spoken\ncoreference resolution (SCR), spoken sentence continuation (SSC), and spoken\nquestion answering (SQA). Our analysis reveals that bias levels are\nlanguage-dependent and vary with different evaluation methods. Our findings\nemphasize the necessity of employing multiple approaches to comprehensively\nassess biases in SILLMs, providing insights for developing fairer SILLM\nsystems."
        ],
        "neg": []
    },
    {
        "query": "Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies.",
        "pos": [
            "Speech Integrated Large Language Models (SILLMs) combine large language\nmodels with speech perception to perform diverse tasks, such as emotion\nrecognition to speaker verification, demonstrating universal audio\nunderstanding capability. However, these models may amplify biases present in\ntraining data, potentially leading to biased access to information for\nmarginalized groups. This work introduces a curated spoken bias evaluation\ntoolkit and corresponding dataset. We evaluate gender bias in SILLMs across\nfour semantic-related tasks: speech-to-text translation (STT), spoken\ncoreference resolution (SCR), spoken sentence continuation (SSC), and spoken\nquestion answering (SQA). Our analysis reveals that bias levels are\nlanguage-dependent and vary with different evaluation methods. Our findings\nemphasize the necessity of employing multiple approaches to comprehensively\nassess biases in SILLMs, providing insights for developing fairer SILLM\nsystems.",
            "Using large language models (LLMs) for automatic evaluation has become an\nimportant evaluation method in NLP research. However, it is unclear whether\nthese LLM-based evaluators can be applied in real-world classrooms to assess\nstudent assignments. This empirical report shares how we use GPT-4 as an\nautomatic assignment evaluator in a university course with 1,028 students.\nBased on student responses, we find that LLM-based assignment evaluators are\ngenerally acceptable to students when students have free access to these\nLLM-based evaluators. However, students also noted that the LLM sometimes fails\nto adhere to the evaluation instructions. Additionally, we observe that\nstudents can easily manipulate the LLM-based evaluator to output specific\nstrings, allowing them to achieve high scores without meeting the assignment\nrubric. Based on student feedback and our experience, we provide several\nrecommendations for integrating LLM-based evaluators into future classrooms."
        ],
        "neg": []
    },
    {
        "query": "Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies.",
        "pos": [
            "Large Language Models (LLMs) have excelled at language understanding and\ngenerating human-level text. However, even with supervised training and human\nalignment, these LLMs are susceptible to adversarial attacks where malicious\nusers can prompt the model to generate undesirable text. LLMs also inherently\nencode potential biases that can cause various harmful effects during\ninteractions. Bias evaluation metrics lack standards as well as consensus and\nexisting methods often rely on human-generated templates and annotations which\nare expensive and labor intensive. In this work, we train models to\nautomatically create adversarial prompts to elicit biased responses from target\nLLMs. We present LLM- based bias evaluation metrics and also analyze several\nexisting automatic evaluation methods and metrics. We analyze the various\nnuances of model responses, identify the strengths and weaknesses of model\nfamilies, and assess where evaluation methods fall short. We compare these\nmetrics to human evaluation and validate that the LLM-as-a-Judge metric aligns\nwith human judgement on bias in response generation.",
            "Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs'\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs' performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs' reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks.",
            "In this study, we explore the proactive ability of LLMs to seek user support,\nusing text-to-SQL generation as a case study. We propose metrics to evaluate\nthe trade-off between performance improvements and user burden, and investigate\nwhether LLMs can determine when to request help and examine their performance\nwith varying levels of information availability. Our experiments reveal that\nwithout external feedback, many LLMs struggle to recognize their need for\nadditional support. Our findings highlight the importance of external signals\nand provide insights for future research on improving support-seeking\nstrategies.",
            "Recent advances in automatic speech recognition (ASR) often rely on large\nspeech foundation models for generating high-quality transcriptions. However,\nthese models can be impractical due to limited computing resources. The\nsituation is even more severe in terms of more realistic or difficult\nscenarios, such as code-switching ASR (CS-ASR). To address this, we present a\nframework for developing more efficient models for CS-ASR through knowledge\ndistillation using realistic speech-only data. Our proposed method, Leave No\nKnowledge Behind During Knowledge Distillation (K$^2$D), leverages both the\nteacher model's knowledge and additional insights from a small auxiliary model.\nWe evaluate our approach on two in-domain and two out-domain datasets,\ndemonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled\nrealistic data, we have successfully obtained a 2-time smaller model with\n5-time faster generation speed while outperforming the baseline methods and the\nteacher model on all the testing sets. We have made our model publicly\navailable on Hugging Face\n(https://huggingface.co/andybi7676/k2d-whisper.zh-en).",
            "In this work, we introduce Speech-Copilot, a modular framework for\ninstruction-oriented speech-processing tasks that minimizes human effort in\ntoolset construction. Unlike end-to-end methods using large audio-language\nmodels, Speech-Copilot builds speech processing-specific toolsets by analyzing\npre-collected task instructions and breaking tasks into manageable sub-tasks.\nIt features a flexible agent based on large language models that performs tasks\nthrough program generation. Our approach achieves state-of-the-art performance\non the Dynamic-SUPERB benchmark, demonstrating its effectiveness across diverse\nspeech-processing tasks. Key contributions include: 1) developing an innovative\nframework for speech processing-specific toolset construction, 2) establishing\na high-performing agent based on large language models, and 3) offering a new\nperspective on addressing challenging instruction-oriented speech-processing\ntasks. Without additional training processes required by end-to-end approaches,\nour method provides a flexible and extendable solution for a wide range of\nspeech-processing applications.",
            "Speech Integrated Large Language Models (SILLMs) combine large language\nmodels with speech perception to perform diverse tasks, such as emotion\nrecognition to speaker verification, demonstrating universal audio\nunderstanding capability. However, these models may amplify biases present in\ntraining data, potentially leading to biased access to information for\nmarginalized groups. This work introduces a curated spoken bias evaluation\ntoolkit and corresponding dataset. We evaluate gender bias in SILLMs across\nfour semantic-related tasks: speech-to-text translation (STT), spoken\ncoreference resolution (SCR), spoken sentence continuation (SSC), and spoken\nquestion answering (SQA). Our analysis reveals that bias levels are\nlanguage-dependent and vary with different evaluation methods. Our findings\nemphasize the necessity of employing multiple approaches to comprehensively\nassess biases in SILLMs, providing insights for developing fairer SILLM\nsystems.",
            "Using large language models (LLMs) for automatic evaluation has become an\nimportant evaluation method in NLP research. However, it is unclear whether\nthese LLM-based evaluators can be applied in real-world classrooms to assess\nstudent assignments. This empirical report shares how we use GPT-4 as an\nautomatic assignment evaluator in a university course with 1,028 students.\nBased on student responses, we find that LLM-based assignment evaluators are\ngenerally acceptable to students when students have free access to these\nLLM-based evaluators. However, students also noted that the LLM sometimes fails\nto adhere to the evaluation instructions. Additionally, we observe that\nstudents can easily manipulate the LLM-based evaluator to output specific\nstrings, allowing them to achieve high scores without meeting the assignment\nrubric. Based on student feedback and our experience, we provide several\nrecommendations for integrating LLM-based evaluators into future classrooms."
        ],
        "neg": []
    },
    {
        "query": "Large language models are susceptible to jailbreak attacks, which can result\nin the generation of harmful content. While prior defenses mitigate these risks\nby perturbing or inspecting inputs, they ignore competing objectives, the\nunderlying cause of alignment failures. In this paper, we propose\nAlignment-Enhanced Decoding (AED), a novel defense that employs adaptive\ndecoding to address the root causes of jailbreak issues. We first define the\nCompetitive Index to quantify alignment failures and utilize feedback from\nself-evaluation to compute post-alignment logits. Then, AED adaptively combines\nAED and post-alignment logits with the original logits to obtain harmless and\nhelpful distributions. Consequently, our method enhances safety alignment while\nmaintaining helpfulness. We conduct experiments across five models and four\ncommon jailbreaks, with the results validating the effectiveness of our\napproach. Code is available at https://github.com/GIGABaozi/AED.git.",
        "pos": [
            "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks."
        ],
        "neg": []
    },
    {
        "query": "Large language models are susceptible to jailbreak attacks, which can result\nin the generation of harmful content. While prior defenses mitigate these risks\nby perturbing or inspecting inputs, they ignore competing objectives, the\nunderlying cause of alignment failures. In this paper, we propose\nAlignment-Enhanced Decoding (AED), a novel defense that employs adaptive\ndecoding to address the root causes of jailbreak issues. We first define the\nCompetitive Index to quantify alignment failures and utilize feedback from\nself-evaluation to compute post-alignment logits. Then, AED adaptively combines\nAED and post-alignment logits with the original logits to obtain harmless and\nhelpful distributions. Consequently, our method enhances safety alignment while\nmaintaining helpfulness. We conduct experiments across five models and four\ncommon jailbreaks, with the results validating the effectiveness of our\napproach. Code is available at https://github.com/GIGABaozi/AED.git.",
        "pos": [
            "In this paper, we present MooER, a LLM-based large-scale automatic speech\nrecognition (ASR) / automatic speech translation (AST) model of Moore Threads.\nA 5000h pseudo labeled dataset containing open source and self collected speech\ndata is used for training. We achieve performance comparable to other open\nsource models trained with up to hundreds of thousands of hours of labeled\nspeech data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest\nthat our model outperforms other open source Speech LLMs. A BLEU score of 25.2\ncan be obtained. The main contributions of this paper are summarized as\nfollows. First, this paper presents a training strategy for encoders and LLMs\non speech related tasks (including ASR and AST) using a small size of pseudo\nlabeled data without any extra manual annotation and selection. Second, we\nrelease our ASR and AST models and plan to open-source our training code and\nstrategy in the near future. Moreover, a model trained on 8wh scale training\ndata is planned to be released later on.",
            "Security concerns for large language models (LLMs) have recently escalated,\nfocusing on thwarting jailbreaking attempts in discrete prompts. However, the\nexploration of jailbreak vulnerabilities arising from continuous embeddings has\nbeen limited, as prior approaches primarily involved appending discrete or\ncontinuous suffixes to inputs. Our study presents a novel channel for\nconducting direct attacks on LLM inputs, eliminating the need for suffix\naddition or specific questions provided that the desired output is predefined.\nWe additionally observe that extensive iterations often lead to overfitting,\ncharacterized by repetition in the output. To counteract this, we propose a\nsimple yet effective strategy named CLIP. Our experiments show that for an\ninput length of 40 at iteration 1000, applying CLIP improves the ASR from 62%\nto 83%",
            "Predicting emotions elicited by news headlines can be challenging as the task\nis largely influenced by the varying nature of people's interpretations and\nbackgrounds. Previous works have explored classifying discrete emotions\ndirectly from news headlines. We provide a different approach to tackling this\nproblem by utilizing people's explanations of their emotion, written in\nfree-text, on how they feel after reading a news headline. Using the dataset\nBU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the\nfree-text explanations have a strong correlation with the dominant emotion\nelicited by the headlines. The free-text explanations also contain more\nsentimental context than the news headlines alone and can serve as a better\ninput to emotion classification models. Therefore, in this work we explored\ngenerating emotion explanations from headlines by training a\nsequence-to-sequence transformer model and by using pretrained large language\nmodel, ChatGPT (GPT-4). We then used the generated emotion explanations for\nemotion classification. In addition, we also experimented with training the\npretrained T5 model for the intermediate task of explanation generation before\nfine-tuning it for emotion classification. Using McNemar's significance test,\nmethods that incorporate GPT-generated free-text emotion explanations\ndemonstrated significant improvement (P-value < 0.05) in emotion classification\nfrom headlines, compared to methods that only use headlines. This underscores\nthe value of using intermediate free-text explanations for emotion prediction\ntasks with headlines."
        ],
        "neg": []
    },
    {
        "query": "Large language models are susceptible to jailbreak attacks, which can result\nin the generation of harmful content. While prior defenses mitigate these risks\nby perturbing or inspecting inputs, they ignore competing objectives, the\nunderlying cause of alignment failures. In this paper, we propose\nAlignment-Enhanced Decoding (AED), a novel defense that employs adaptive\ndecoding to address the root causes of jailbreak issues. We first define the\nCompetitive Index to quantify alignment failures and utilize feedback from\nself-evaluation to compute post-alignment logits. Then, AED adaptively combines\nAED and post-alignment logits with the original logits to obtain harmless and\nhelpful distributions. Consequently, our method enhances safety alignment while\nmaintaining helpfulness. We conduct experiments across five models and four\ncommon jailbreaks, with the results validating the effectiveness of our\napproach. Code is available at https://github.com/GIGABaozi/AED.git.",
        "pos": [
            "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon."
        ],
        "neg": []
    },
    {
        "query": "While cross-linguistic model transfer is effective in many settings, there is\nstill limited understanding of the conditions under which it works. In this\npaper, we focus on assessing the role of lexical semantics in cross-lingual\ntransfer, as we compare its impact to that of other language properties.\nExamining each language property individually, we systematically analyze how\ndifferences between English and a target language influence the capacity to\nalign the language with an English pretrained representation space. We do so by\nartificially manipulating the English sentences in ways that mimic specific\ncharacteristics of the target language, and reporting the effect of each\nmanipulation on the quality of alignment with the representation space. We show\nthat while properties such as the script or word order only have a limited\nimpact on alignment quality, the degree of lexical matching between the two\nlanguages, which we define using a measure of translation entropy, greatly\naffects it.",
        "pos": [
            "The task of image captioning has recently been gaining popularity, and with\nit the complex task of evaluating the quality of image captioning models. In\nthis work, we present the first survey and taxonomy of over 70 different image\ncaptioning metrics and their usage in hundreds of papers. We find that despite\nthe diversity of proposed metrics, the vast majority of studies rely on only\nfive popular metrics, which we show to be weakly correlated with human\njudgements. Instead, we propose EnsembEval -- an ensemble of evaluation methods\nachieving the highest reported correlation with human judgements across 5 image\ncaptioning datasets, showing there is a lot of room for improvement by\nleveraging a diverse set of metrics.",
            "This paper introduces CovScore, an automatic reference-less methodology for\nevaluating thematic title sets, extracted from a corpus of documents. While\nsuch extraction methods are widely used, evaluating their effectiveness remains\nan open question. Moreover, some existing practices heavily rely on slow and\nlaborious human annotation procedures. Inspired by recently introduced\nLLM-based judge methods, we propose a novel methodology that decomposes quality\ninto five main metrics along different aspects of evaluation. This framing\nsimplifies and expedites the manual evaluation process and enables automatic\nand independent LLM-based evaluation. As a test case, we apply our approach to\na corpus of Holocaust survivor testimonies, motivated both by its relevance to\ntitle set extraction and by the moral significance of this pursuit. We validate\nthe methodology by experimenting with naturalistic and synthetic title set\ngeneration systems and compare their performance with the methodology.",
            "Human feedback data is a critical component in developing language models.\nHowever, collecting this feedback is costly and ultimately not scalable. We\npropose a scalable method for extracting feedback that users naturally include\nwhen interacting with chat models, and leveraging it for model training. We are\nfurther motivated by previous work that showed there are also qualitative\nadvantages to using naturalistic (rather than auto-generated) feedback, such as\nless hallucinations and biases. We manually annotated conversation data to\nconfirm the presence of naturally occurring feedback in a standard corpus,\nfinding that as much as 30% of the chats include explicit feedback. We apply\nour method to over 1M conversations to obtain hundreds of thousands of feedback\nsamples. Training with the extracted feedback shows significant performance\nimprovements over baseline models, demonstrating the efficacy of our approach\nin enhancing model alignment to human preferences."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "In recent years, multimodal large language models (MLLMs) have shown strong\npotential in real-world applications. They are developing rapidly due to their\nremarkable ability to comprehend multimodal information and their inherent\npowerful cognitive and reasoning capabilities. Among MLLMs, vision language\nmodels (VLM) stand out for their ability to understand vision information.\nHowever, the scaling trend of VLMs under the current mainstream paradigm has\nnot been extensively studied. Whether we can achieve better performance by\ntraining even larger models is still unclear. To address this issue, we\nconducted experiments on the pretraining stage of MLLMs. We conduct our\nexperiment using different encoder sizes and large language model (LLM) sizes.\nOur findings indicate that merely increasing the size of encoders does not\nnecessarily enhance the performance of VLMs. Moreover, we analyzed the effects\nof LLM backbone parameter size and data quality on the pretraining outcomes.\nAdditionally, we explored the differences in scaling laws between LLMs and\nVLMs.",
            "In recent years, with the rapid advancements in large language models (LLMs),\nachieving excellent empathetic response capabilities has become a crucial\nprerequisite. Consequently, managing and understanding empathetic datasets have\ngained increasing significance. However, empathetic data are typically\nhuman-labeled, leading to insufficient datasets and wasted human labor. In this\nwork, we present Synth-Empathy, an LLM-based data generation and quality and\ndiversity selection pipeline that automatically generates high-quality\nempathetic data while discarding low-quality data. With the data generated from\na low empathetic model, we are able to further improve empathetic response\nperformance and achieve state-of-the-art (SoTA) results across multiple\nbenchmarks. Moreover, our model achieves SoTA performance on various human\nevaluation benchmarks, demonstrating its effectiveness and robustness in\nreal-world applications. Furthermore, we show the trade-off between data\nquantity and quality, providing insights into empathetic data generation and\nselection.",
            "Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.",
            "Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "In recent years, with the rapid advancements in large language models (LLMs),\nachieving excellent empathetic response capabilities has become a crucial\nprerequisite. Consequently, managing and understanding empathetic datasets have\ngained increasing significance. However, empathetic data are typically\nhuman-labeled, leading to insufficient datasets and wasted human labor. In this\nwork, we present Synth-Empathy, an LLM-based data generation and quality and\ndiversity selection pipeline that automatically generates high-quality\nempathetic data while discarding low-quality data. With the data generated from\na low empathetic model, we are able to further improve empathetic response\nperformance and achieve state-of-the-art (SoTA) results across multiple\nbenchmarks. Moreover, our model achieves SoTA performance on various human\nevaluation benchmarks, demonstrating its effectiveness and robustness in\nreal-world applications. Furthermore, we show the trade-off between data\nquantity and quality, providing insights into empathetic data generation and\nselection.",
            "Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).",
            "Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "Large Language Models (LLMs) frequently suffer from inefficiencies, largely\nattributable to the discord between the requirements of auto-regressive\ndecoding and the architecture of contemporary GPUs. Recently, regressive\nlightweight speculative decoding has garnered attention for its notable\nefficiency improvements in text generation tasks. This approach utilizes a\nlightweight regressive draft model, like a Recurrent Neural Network (RNN) or a\nsingle transformer decoder layer, leveraging sequential information to\niteratively predict potential tokens. Specifically, RNN draft models are\ncomputationally economical but tend to deliver lower accuracy, while attention\ndecoder layer models exhibit the opposite traits. This paper presents Clover-2,\nan advanced iteration of Clover, an RNN-based draft model designed to achieve\ncomparable accuracy to that of attention decoder layer models while maintaining\nminimal computational overhead. Clover-2 enhances the model architecture and\nincorporates knowledge distillation to increase Clover's accuracy and improve\noverall efficiency. We conducted experiments using the open-source Vicuna 7B\nand LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses\nexisting methods across various model architectures, showcasing its efficacy\nand robustness.",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.",
            "Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "In recent years, multimodal large language models (MLLMs) have shown strong\npotential in real-world applications. They are developing rapidly due to their\nremarkable ability to comprehend multimodal information and their inherent\npowerful cognitive and reasoning capabilities. Among MLLMs, vision language\nmodels (VLM) stand out for their ability to understand vision information.\nHowever, the scaling trend of VLMs under the current mainstream paradigm has\nnot been extensively studied. Whether we can achieve better performance by\ntraining even larger models is still unclear. To address this issue, we\nconducted experiments on the pretraining stage of MLLMs. We conduct our\nexperiment using different encoder sizes and large language model (LLM) sizes.\nOur findings indicate that merely increasing the size of encoders does not\nnecessarily enhance the performance of VLMs. Moreover, we analyzed the effects\nof LLM backbone parameter size and data quality on the pretraining outcomes.\nAdditionally, we explored the differences in scaling laws between LLMs and\nVLMs.",
            "In recent years, with the rapid advancements in large language models (LLMs),\nachieving excellent empathetic response capabilities has become a crucial\nprerequisite. Consequently, managing and understanding empathetic datasets have\ngained increasing significance. However, empathetic data are typically\nhuman-labeled, leading to insufficient datasets and wasted human labor. In this\nwork, we present Synth-Empathy, an LLM-based data generation and quality and\ndiversity selection pipeline that automatically generates high-quality\nempathetic data while discarding low-quality data. With the data generated from\na low empathetic model, we are able to further improve empathetic response\nperformance and achieve state-of-the-art (SoTA) results across multiple\nbenchmarks. Moreover, our model achieves SoTA performance on various human\nevaluation benchmarks, demonstrating its effectiveness and robustness in\nreal-world applications. Furthermore, we show the trade-off between data\nquantity and quality, providing insights into empathetic data generation and\nselection.",
            "Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).",
            "Tibet, ensconced within China's territorial expanse, is distinguished by its\nlabyrinthine and heterogeneous topography, a testament to its profound\nhistorical heritage, and the cradle of a unique religious ethos. The very\nessence of these attributes, however, has impeded the advancement of Tibet's\ntourism service infrastructure, rendering existing smart tourism services\ninadequate for the region's visitors. This study delves into the ramifications\nof informational disparities at tourist sites on Tibetan tourism and addresses\nthe challenge of establishing the Large Language Model (LLM) evaluation\ncriteria. It introduces an innovative approach, the DualGen Bridge AI system,\nemploying supervised fine-tuning techniques to bolster model functionality and\nenhance optimization processes. Furthermore, it pioneers a multi-structured\ngenerative results assessment framework. Empirical validation confirms the\nefficacy of this framework. The study also explores the application of the\nsupervised fine-tuning method within the proprietary DualGen Bridge AI, aimed\nat refining the generation of tourist site information. The study's findings\noffer valuable insights for optimizing system performance and provide support\nand inspiration for the application of LLM technology in Tibet's tourism\nservices and beyond, potentially revolutionizing the smart tourism industry\nwith advanced, tailored information generation capabilities.",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.",
            "Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets."
        ],
        "neg": []
    },
    {
        "query": "With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering."
        ],
        "neg": []
    },
    {
        "query": "Human biases have been shown to influence the performance of models and\nalgorithms in various fields, including Natural Language Processing. While the\nstudy of this phenomenon is garnering focus in recent years, the available\nresources are still relatively scarce, often focusing on different forms or\nmanifestations of biases. The aim of our work is twofold: 1) gather\npublicly-available datasets and determine how to better combine them to\neffectively train models in the task of hate speech detection and\nclassification; 2) analyse the main issues with these datasets, such as\nscarcity, skewed resources, and reliance on non-persistent data. We discuss\nthese issues in tandem with the development of our experiments, in which we\nshow that the combinations of different datasets greatly impact the models'\nperformance.",
        "pos": [
            "Empathy and emotion prediction are key components in the development of\neffective and empathetic agents, amongst several other applications. The WASSA\nshared task on empathy and emotion prediction in interactions presents an\nopportunity to benchmark approaches to these tasks. Appropriately selecting and\nrepresenting the historical context is crucial in the modelling of empathy and\nemotion in conversations. In our submissions, we model empathy, emotion\npolarity and emotion intensity of each utterance in a conversation by feeding\nthe utterance to be classified together with its conversational context, i.e.,\na certain number of previous conversational turns, as input to an encoder\nPre-trained Language Model, to which we append a regression head for\nprediction. We also model perceived counterparty empathy of each interlocutor\nby feeding all utterances from the conversation and a token identifying the\ninterlocutor for which we are predicting the empathy. Our system officially\nranked $1^{st}$ at the CONV-turn track and $2^{nd}$ at the CONV-dialog track."
        ],
        "neg": []
    },
    {
        "query": "Human biases have been shown to influence the performance of models and\nalgorithms in various fields, including Natural Language Processing. While the\nstudy of this phenomenon is garnering focus in recent years, the available\nresources are still relatively scarce, often focusing on different forms or\nmanifestations of biases. The aim of our work is twofold: 1) gather\npublicly-available datasets and determine how to better combine them to\neffectively train models in the task of hate speech detection and\nclassification; 2) analyse the main issues with these datasets, such as\nscarcity, skewed resources, and reliance on non-persistent data. We discuss\nthese issues in tandem with the development of our experiments, in which we\nshow that the combinations of different datasets greatly impact the models'\nperformance.",
        "pos": [
            "Brazilian Portuguese and European Portuguese are two varieties of the same\nlanguage and, despite their close similarities, they exhibit several\ndifferences. However, there is a significant disproportion in the availability\nof resources between the two variants, with Brazilian Portuguese having more\nabundant resources. This inequity can impact the quality of translation\nservices accessible to European Portuguese speakers. To address this issue, we\npropose the development of a Brazilian Portuguese to European Portuguese\ntranslation system, leveraging recent advancements in neural architectures and\nmodels. To evaluate the performance of such systems, we manually curated a gold\ntest set comprising 500 sentences across five different topics. Each sentence\nin the gold test set has two distinct references, facilitating a\nstraightforward evaluation of future translation models. We experimented with\nvarious models by fine-tuning existing Large Language Models using parallel\ndata extracted from movie subtitles and TED Talks transcripts in both Brazilian\nand European Portuguese. Our evaluation involved the use of conventional\nautomatic metrics as well as a human evaluation. In addition, all models were\ncompared against ChatGPT 3.5 Turbo, which currently yields the best results."
        ],
        "neg": []
    },
    {
        "query": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework named\nBMC, for bridging and modeling correlations in pairwise data. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nby targeted modifications, synthesizing a pseudo winning response through\nimproving the losing response based on the winning response. Secondly, we\nidentify that DPO alone is insufficient to model these correlations and capture\nnuanced variations. Therefore, we propose learning token-level correlations by\ndynamically leveraging the policy model's confidence during training.\nComprehensive experiments on QA, math, and instruction-following tasks\ndemonstrate the effectiveness of our approach, significantly surpassing\ncompetitive baselines, including DPO. Additionally, our in-depth quantitative\nanalysis reveals the reasons behind our method's superior performance over DPO\nand showcases its versatility to other DPO variants.",
        "pos": [
            "Despite the substantial success of Information Retrieval (IR) in various NLP\ntasks, most IR systems predominantly handle queries and corpora in natural\nlanguage, neglecting the domain of code retrieval. Code retrieval is critically\nimportant yet remains under-explored, with existing methods and benchmarks\ninadequately representing the diversity of code in various domains and tasks.\nAddressing this gap, we present \\textbf{\\name} (\\textbf{Co}de\n\\textbf{I}nformation \\textbf{R}etrieval Benchmark), a robust and comprehensive\nbenchmark specifically designed to assess code retrieval capabilities. \\name\ncomprises \\textbf{ten} meticulously curated code datasets, spanning\n\\textbf{eight} distinctive retrieval tasks across \\textbf{seven} diverse\ndomains. We first discuss the construction of \\name and its diverse dataset\ncomposition. Further, we evaluate nine widely used retrieval models using\n\\name, uncovering significant difficulties in performing code retrieval tasks\neven with state-of-the-art systems. To facilitate easy adoption and integration\nwithin existing research workflows, \\name has been developed as a user-friendly\nPython framework, readily installable via pip. It shares same data schema as\nother popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark\nevaluations. Through \\name, we aim to invigorate research in the code retrieval\ndomain, providing a versatile benchmarking tool that encourages further\ndevelopment and exploration of code retrieval systems\\footnote{\\url{\nhttps://github.com/CoIR-team/coir}}."
        ],
        "neg": []
    },
    {
        "query": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework named\nBMC, for bridging and modeling correlations in pairwise data. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nby targeted modifications, synthesizing a pseudo winning response through\nimproving the losing response based on the winning response. Secondly, we\nidentify that DPO alone is insufficient to model these correlations and capture\nnuanced variations. Therefore, we propose learning token-level correlations by\ndynamically leveraging the policy model's confidence during training.\nComprehensive experiments on QA, math, and instruction-following tasks\ndemonstrate the effectiveness of our approach, significantly surpassing\ncompetitive baselines, including DPO. Additionally, our in-depth quantitative\nanalysis reveals the reasons behind our method's superior performance over DPO\nand showcases its versatility to other DPO variants.",
        "pos": [
            "Controlling the format of outputs generated by large language models (LLMs)\nis a critical functionality in various applications. Current methods typically\nemploy constrained decoding with rule-based automata or fine-tuning with\nmanually crafted format instructions, both of which struggle with open-domain\nformat requirements. To address this limitation, we introduce a novel framework\nfor controlled generation in LLMs, leveraging user-provided, one-shot QA pairs.\nThis study investigates LLMs' capabilities to follow open-domain, one-shot\nconstraints and replicate the format of the example answers. We observe that\nthis is a non-trivial problem for current LLMs. We also develop a dataset\ncollection methodology for supervised fine-tuning that enhances the open-domain\nformat control of LLMs without degrading output quality, as well as a benchmark\non which we evaluate both the helpfulness and format correctness of LLM\noutputs. The resulting datasets, named OIFC-SFT, along with the related code,\nwill be made publicly available at https://github.com/cofe-ai/OIFC."
        ],
        "neg": []
    },
    {
        "query": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework named\nBMC, for bridging and modeling correlations in pairwise data. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nby targeted modifications, synthesizing a pseudo winning response through\nimproving the losing response based on the winning response. Secondly, we\nidentify that DPO alone is insufficient to model these correlations and capture\nnuanced variations. Therefore, we propose learning token-level correlations by\ndynamically leveraging the policy model's confidence during training.\nComprehensive experiments on QA, math, and instruction-following tasks\ndemonstrate the effectiveness of our approach, significantly surpassing\ncompetitive baselines, including DPO. Additionally, our in-depth quantitative\nanalysis reveals the reasons behind our method's superior performance over DPO\nand showcases its versatility to other DPO variants.",
        "pos": [
            "Despite the substantial success of Information Retrieval (IR) in various NLP\ntasks, most IR systems predominantly handle queries and corpora in natural\nlanguage, neglecting the domain of code retrieval. Code retrieval is critically\nimportant yet remains under-explored, with existing methods and benchmarks\ninadequately representing the diversity of code in various domains and tasks.\nAddressing this gap, we present \\textbf{\\name} (\\textbf{Co}de\n\\textbf{I}nformation \\textbf{R}etrieval Benchmark), a robust and comprehensive\nbenchmark specifically designed to assess code retrieval capabilities. \\name\ncomprises \\textbf{ten} meticulously curated code datasets, spanning\n\\textbf{eight} distinctive retrieval tasks across \\textbf{seven} diverse\ndomains. We first discuss the construction of \\name and its diverse dataset\ncomposition. Further, we evaluate nine widely used retrieval models using\n\\name, uncovering significant difficulties in performing code retrieval tasks\neven with state-of-the-art systems. To facilitate easy adoption and integration\nwithin existing research workflows, \\name has been developed as a user-friendly\nPython framework, readily installable via pip. It shares same data schema as\nother popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark\nevaluations. Through \\name, we aim to invigorate research in the code retrieval\ndomain, providing a versatile benchmarking tool that encourages further\ndevelopment and exploration of code retrieval systems\\footnote{\\url{\nhttps://github.com/CoIR-team/coir}}."
        ],
        "neg": []
    },
    {
        "query": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework named\nBMC, for bridging and modeling correlations in pairwise data. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nby targeted modifications, synthesizing a pseudo winning response through\nimproving the losing response based on the winning response. Secondly, we\nidentify that DPO alone is insufficient to model these correlations and capture\nnuanced variations. Therefore, we propose learning token-level correlations by\ndynamically leveraging the policy model's confidence during training.\nComprehensive experiments on QA, math, and instruction-following tasks\ndemonstrate the effectiveness of our approach, significantly surpassing\ncompetitive baselines, including DPO. Additionally, our in-depth quantitative\nanalysis reveals the reasons behind our method's superior performance over DPO\nand showcases its versatility to other DPO variants.",
        "pos": [
            "Cyber Threat Intelligence (CTI) summarization task requires the system to\ngenerate concise and accurate highlights from raw intelligence data, which\nplays an important role in providing decision-makers with crucial information\nto quickly detect and respond to cyber threats in the cybersecurity domain.\nHowever, efficient techniques for summarizing CTI reports, including facts,\nanalytical insights, attack processes, etc., have largely been unexplored,\nprimarily due to the lack of available dataset. To this end, we present CTISum,\na new benchmark for CTI summarization task. Considering the importance of\nattack process, a novel fine-grained subtask of attack process summarization is\nproposed to enable defenders to assess risk, identify security gaps,\nvulnerabilities, and so on. Specifically, we first design a multi-stage\nannotation pipeline to gather and annotate the CTI data, and then benchmark the\nCTISum with a collection of extractive and abstractive summarization methods.\nExperimental results show that current state-of-the-art models exhibit\nlimitations when applied to CTISum, underscoring the fact that automatically\nproducing concise summaries of CTI reports remains an open research challenge.",
            "Significant advancements has recently been achieved in the field of\nmulti-modal large language models (MLLMs), demonstrating their remarkable\ncapabilities in understanding and reasoning across diverse tasks. However,\nthese models are often trained for specific tasks and rely on task-specific\ninput-output formats, limiting their applicability to a broader range of tasks.\nThis raises a fundamental question: Can we develop a unified approach to\nrepresent and handle different multi-modal tasks to maximize the\ngeneralizability of MLLMs? In this paper, we propose UnifiedMLLM, a\ncomprehensive model designed to represent various tasks using a unified\nrepresentation. Our model exhibits strong capabilities in comprehending the\nimplicit intent of user instructions and preforming reasoning. In addition to\ngenerating textual responses, our model also outputs task tokens and grounding\ntokens, serving as indicators of task types and task granularity. These outputs\nare subsequently routed through the task router and directed to specific expert\nmodels for task completion. To train our model, we construct a task-specific\ndataset and an 100k multi-task dataset encompassing complex scenarios.\nEmploying a three-stage training strategy, we equip our model with robust\nreasoning and task processing capabilities while preserving its generalization\ncapacity and knowledge reservoir. Extensive experiments showcase the impressive\nperformance of our unified representation approach across various tasks,\nsurpassing existing methodologies. Furthermore, our approach exhibits\nexceptional scalability and generality. Our code, model, and dataset will be\navailable at \\url{https://github.com/lzw-lzw/UnifiedMLLM}.",
            "The vast amount of biomedical information available today presents a\nsignificant challenge for investigators seeking to digest, process, and\nunderstand these findings effectively. Large Language Models (LLMs) have\nemerged as powerful tools to navigate this complex and challenging data\nlandscape. However, LLMs may lead to hallucinatory responses, making Retrieval\nAugmented Generation (RAG) crucial for achieving accurate information. In this\nprotocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease\nDistinction), a comprehensive workflow designed to support investigators with\nknowledge integration and hypothesis generation, identifying validated paths\nforward. Relevant biomedical information from publications and knowledge bases\nare reviewed, integrated, and extracted via text-mining association analysis\nand explainable graph prediction models on disease nodes, forecasting potential\nlinks among drugs and diseases. These analyses, along with biomedical texts,\nare integrated into a framework that facilitates user-directed mechanism\nelucidation as well as hypothesis exploration through RAG-enabled LLMs. A\nclinical use-case demonstrates RUGGED's ability to evaluate and recommend\ntherapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy\n(DCM), analyzing prescribed drugs for molecular interactions and unexplored\nuses. The platform minimizes LLM hallucinations, offers actionable insights,\nand improves the investigation of novel therapeutics.",
            "Recent advancements in open-source code large language models (LLMs) have\ndemonstrated remarkable coding abilities by fine-tuning on the data generated\nfrom powerful closed-source LLMs such as GPT-3.5 and GPT-4 for instruction\ntuning. This paper explores how to further improve an instruction-tuned code\nLLM by generating data from itself rather than querying closed-source LLMs. Our\nkey observation is the misalignment between the translation of formal and\ninformal languages: translating formal language (i.e., code) to informal\nlanguage (i.e., natural language) is more straightforward than the reverse.\nBased on this observation, we propose INVERSE-INSTRUCT, which summarizes\ninstructions from code snippets instead of the reverse. Specifically, given an\ninstruction tuning corpus for code and the resulting instruction-tuned code\nLLM, we ask the code LLM to generate additional high-quality instructions for\nthe original corpus through code summarization and self-evaluation. Then, we\nfine-tune the base LLM on the combination of the original corpus and the\nself-generated one, which yields a stronger instruction-tuned LLM. We present a\nseries of code LLMs named InverseCoder, which surpasses the performance of the\noriginal code LLMs on a wide range of benchmarks, including Python text-to-code\ngeneration, multilingual coding, and data-science code generation.",
            "Large language models (LLMs) are increasingly applied to clinical\ndecision-making. However, their potential to exhibit bias poses significant\nrisks to clinical equity. Currently, there is a lack of benchmarks that\nsystematically evaluate such clinical bias in LLMs. While in downstream tasks,\nsome biases of LLMs can be avoided such as by instructing the model to answer\n\"I'm not sure...\", the internal bias hidden within the model still lacks deep\nstudies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in\nLarge Language Models), a pioneering comprehensive benchmark to evaluate both\nintrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for\nclinical decision tasks. Notably, for intrinsic bias, we introduce a novel\nmetric, AssocMAD, to assess the disparities of LLMs across multiple demographic\ngroups. Additionally, we leverage counterfactual intervention to evaluate\nextrinsic bias in a task of clinical diagnosis prediction. Our experiments\nacross popular and medically adapted LLMs, particularly from the Mistral and\nLLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic\nbias. This work underscores the critical need to mitigate clinical bias and\nsets a new standard for future evaluations of LLMs' clinical bias.",
            "We propose LogicVista, an evaluation benchmark that assesses the integrated\nlogical reasoning capabilities of multimodal large language models (MLLMs) in\nVisual contexts. Recent advancements in MLLMs have demonstrated various\nfascinating abilities, from crafting poetry based on an image to performing\nmathematical reasoning. However, there is still a lack of systematic evaluation\nof MLLMs' proficiency in logical reasoning tasks, which are essential for\nactivities like navigation and puzzle-solving. Thus we evaluate general logical\ncognition abilities across 5 logical reasoning tasks encompassing 9 different\ncapabilities, using a sample of 448 multiple-choice questions. Each question is\nannotated with the correct answer and the human-written reasoning behind the\nselection, enabling both open-ended and multiple-choice evaluation. A total of\n8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Available\nat https://github.com/Yijia-Xiao/LogicVista."
        ],
        "neg": []
    },
    {
        "query": "This paper describes CMU's submission to the IWSLT 2024 Simultaneous Speech\nTranslation (SST) task for translating English speech to German text in a\nstreaming manner. Our end-to-end speech-to-text (ST) system integrates the\nWavLM speech encoder, a modality adapter, and the Llama2-7B-Base model as the\ndecoder. We employ a two-stage training approach: initially, we align the\nrepresentations of speech and text, followed by full fine-tuning. Both stages\nare trained on MuST-c v2 data with cross-entropy loss. We adapt our offline ST\nmodel for SST using a simple fixed hold-n policy. Experiments show that our\nmodel obtains an offline BLEU score of 31.1 and a BLEU score of 29.5 under 2\nseconds latency on the MuST-C-v2 tst-COMMON.",
        "pos": [
            "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges.",
            "Large Language Models~(LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting\nextensive multilingual continual pre-training on the LLaMA series models,\nenabling translation support across more than 100 languages. Through a\ncomprehensive analysis of training strategies, such as vocabulary expansion and\ndata augmentation, we develop LLaMAX. Remarkably, without sacrificing its\ngeneralization ability, LLaMAX achieves significantly higher translation\nperformance compared to existing open-source LLMs~(by more than 10 spBLEU\npoints) and performs on-par with specialized translation model~(M2M-100-12B) on\nthe Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve\nas a robust multilingual foundation model. The\ncode~\\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and\nmodels~\\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available."
        ],
        "neg": []
    },
    {
        "query": "This paper describes CMU's submission to the IWSLT 2024 Simultaneous Speech\nTranslation (SST) task for translating English speech to German text in a\nstreaming manner. Our end-to-end speech-to-text (ST) system integrates the\nWavLM speech encoder, a modality adapter, and the Llama2-7B-Base model as the\ndecoder. We employ a two-stage training approach: initially, we align the\nrepresentations of speech and text, followed by full fine-tuning. Both stages\nare trained on MuST-c v2 data with cross-entropy loss. We adapt our offline ST\nmodel for SST using a simple fixed hold-n policy. Experiments show that our\nmodel obtains an offline BLEU score of 31.1 and a BLEU score of 29.5 under 2\nseconds latency on the MuST-C-v2 tst-COMMON.",
        "pos": [
            "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenDevin, a platform for the development of powerful and\nflexible AI agents that interact with the world in similar ways to those of a\nhuman developer: by writing code, interacting with a command line, and browsing\nthe web. We describe how the platform allows for the implementation of new\nagents, safe interaction with sandboxed environments for code execution,\ncoordination between multiple agents, and incorporation of evaluation\nbenchmarks. Based on our currently incorporated benchmarks, we perform an\nevaluation of agents over 15 challenging tasks, including software engineering\n(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released\nunder the permissive MIT license, OpenDevin is a community project spanning\nacademia and industry with more than 1.3K contributions from over 160\ncontributors and will improve going forward.",
            "Existing text-to-video diffusion models rely solely on text-only encoders for\ntheir pretraining. This limitation stems from the absence of large-scale\nmultimodal prompt video datasets, resulting in a lack of visual grounding and\nrestricting their versatility and application in multimodal integration. To\naddress this, we construct a large-scale multimodal prompt dataset by employing\nretrieval methods to pair in-context examples with the given text prompts and\nthen utilize a two-stage training strategy to enable diverse video generation\ntasks within the same model. In the first stage, we propose a multimodal\nconditional video generation framework for pretraining on these augmented\ndatasets, establishing a foundational model for grounded video generation.\nSecondly, we finetune the model from the first stage on three video generation\ntasks, incorporating multi-modal instructions. This process further refines the\nmodel's ability to handle diverse inputs and tasks, ensuring seamless\nintegration of multi-modal information. After this two-stage train-ing process,\nVIMI demonstrates multimodal understanding capabilities, producing contextually\nrich and personalized videos grounded in the provided inputs, as shown in\nFigure 1. Compared to previous visual grounded video generation methods, VIMI\ncan synthesize consistent and temporally coherent videos with large motion\nwhile retaining the semantic control. Lastly, VIMI also achieves\nstate-of-the-art text-to-video generation results on UCF101 benchmark.",
            "One of the most reliable ways to create deployable models for specialized\ntasks is to obtain an adequate amount of high-quality task-specific data.\nHowever, for specialized tasks, often such datasets do not exist. Existing\nmethods address this by creating such data from large language models (LLMs)\nand then distilling such knowledge into smaller models. However, these methods\nare limited by the quality of the LLMs output, and tend to generate repetitive\nor incorrect data. In this work, we present Retrieval Based Distillation\n(ReBase), a method that first retrieves data from rich online sources and then\ntransforms them into domain-specific data. This method greatly enhances data\ndiversity. Moreover, ReBase generates Chain-of-Thought reasoning and distills\nthe reasoning capacity of LLMs. We test our method on 4 benchmarks and results\nshow that our method significantly improves performance by up to 7.8% on SQuAD,\n1.37% on MNLI, and 1.94% on BigBench-Hard."
        ],
        "neg": []
    },
    {
        "query": "This paper describes CMU's submission to the IWSLT 2024 Simultaneous Speech\nTranslation (SST) task for translating English speech to German text in a\nstreaming manner. Our end-to-end speech-to-text (ST) system integrates the\nWavLM speech encoder, a modality adapter, and the Llama2-7B-Base model as the\ndecoder. We employ a two-stage training approach: initially, we align the\nrepresentations of speech and text, followed by full fine-tuning. Both stages\nare trained on MuST-c v2 data with cross-entropy loss. We adapt our offline ST\nmodel for SST using a simple fixed hold-n policy. Experiments show that our\nmodel obtains an offline BLEU score of 31.1 and a BLEU score of 29.5 under 2\nseconds latency on the MuST-C-v2 tst-COMMON.",
        "pos": [
            "In this work, we present SynesLM, an unified model which can perform three\nmultimodal language understanding tasks: audio-visual automatic speech\nrecognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).\nUnlike previous research that focused on lip motion as visual cues for speech\nsignals, our work explores more general visual information within entire\nframes, such as objects and actions. Additionally, we use synthetic image data\nto enhance the correlation between image and speech data. We benchmark SynesLM\nagainst the How2 dataset, demonstrating performance on par with\nstate-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our\nmultitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA\nperformance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the\nVisSpeech Dataset. Furthermore, our results in VST and VMT outperform the\nprevious results, improving the BLEU score to 43.5 from 37.2 for VST, and to\n54.8 from 54.4 for VMT.",
            "Convolutions have become essential in state-of-the-art end-to-end Automatic\nSpeech Recognition~(ASR) systems due to their efficient modelling of local\ncontext. Notably, its use in Conformers has led to superior performance\ncompared to vanilla Transformer-based ASR systems. While components other than\nthe convolution module in the Conformer have been reexamined, altering the\nconvolution module itself has been far less explored. Towards this, we\nintroduce Multi-Convformer that uses multiple convolution kernels within the\nconvolution module of the Conformer in conjunction with gating. This helps in\nimproved modeling of local dependencies at varying granularities. Our model\nrivals existing Conformer variants such as CgMLP and E-Branchformer in\nperformance, while being more parameter efficient. We empirically compare our\napproach with Conformer and its variants across four different datasets and\nthree different modelling paradigms and show up to 8% relative word error\nrate~(WER) improvements."
        ],
        "neg": []
    },
    {
        "query": "The advances in the digital era have led to rapid dissemination of\ninformation. This has also aggravated the spread of misinformation and\ndisinformation. This has potentially serious consequences, such as civil\nunrest. While fact-checking aims to combat this, manual fact-checking is\ncumbersome and not scalable. While automated fact-checking approaches exist,\nthey do not operate in real-time and do not always account for spread of\nmisinformation through different modalities. This is particularly important as\nproactive fact-checking on live streams in real-time can help people be\ninformed of false narratives and prevent catastrophic consequences that may\ncause civil unrest. This is particularly relevant with the rapid dissemination\nof information through video on social media platforms or other streams like\npolitical rallies and debates. Hence, in this work we develop a platform named\n\\name{}, that can aid in fact-checking live audio streams in real-time. \\name{}\nhas a user-friendly interface that displays the claims detected along with\ntheir veracity and evidence for live streams with associated speakers for\nclaims from respective segments. The app can be accessed at\nhttp://livefc.factiverse.ai and a screen recording of the demo can be found at\nhttps://bit.ly/3WVAoIw.",
        "pos": [
            "This paper describes IAI group's participation for automated check-worthiness\nestimation for claims, within the framework of the 2024 CheckThat! Lab \"Task 1:\nCheck-Worthiness Estimation\". The task involves the automated detection of\ncheck-worthy claims in English, Dutch, and Arabic political debates and Twitter\ndata. We utilized various pre-trained generative decoder and encoder\ntransformer models, employing methods such as few-shot chain-of-thought\nreasoning, fine-tuning, data augmentation, and transfer learning from one\nlanguage to another. Despite variable success in terms of performance, our\nmodels achieved notable placements on the organizer's leaderboard: ninth-best\nin English, third-best in Dutch, and the top placement in Arabic, utilizing\nmultilingual datasets for enhancing the generalizability of check-worthiness\ndetection. Despite a significant drop in performance on the unlabeled test\ndataset compared to the development test dataset, our findings contribute to\nthe ongoing efforts in claim detection research, highlighting the challenges\nand potential of language-specific adaptations in claim verification systems.",
            "Verifying fact-checking claims poses a significant challenge, even for\nhumans. Recent approaches have demonstrated that decomposing claims into\nrelevant questions to gather evidence enhances the efficiency of the\nfact-checking process. In this paper, we provide empirical evidence showing\nthat this question decomposition can be effectively automated. We demonstrate\nthat smaller generative models, fine-tuned for the question generation task\nusing data augmentation from various datasets, outperform large language models\nby up to 8%. Surprisingly, in some cases, the evidence retrieved using\nmachine-generated questions proves to be significantly more effective for\nfact-checking than that obtained from human-written questions. We also perform\nmanual evaluation of the decomposed questions to assess the quality of the\nquestions generated."
        ],
        "neg": []
    },
    {
        "query": "Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful\ntechnique in natural language processing, combining the strengths of\nretrieval-based and generation-based models to enhance text generation tasks.\nHowever, the application of RAG in Arabic, a language with unique\ncharacteristics and resource constraints, remains underexplored. This paper\npresents a comprehensive case study on the implementation and evaluation of RAG\nfor Arabic text. The work focuses on exploring various semantic embedding\nmodels in the retrieval stage and several LLMs in the generation stage, in\norder to investigate what works and what doesn't in the context of Arabic. The\nwork also touches upon the issue of variations between document dialect and\nquery dialect in the retrieval stage. Results show that existing semantic\nembedding models and LLMs can be effectively employed to build Arabic RAG\npipelines.",
        "pos": [
            "We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed."
        ],
        "neg": []
    },
    {
        "query": "Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition.",
        "pos": [
            "Enabling Large Language Models (LLMs) to generate citations in\nQuestion-Answering (QA) tasks is an emerging paradigm aimed at enhancing the\nverifiability of their responses when LLMs are utilizing external references to\ngenerate an answer. However, there is currently no unified framework to\nstandardize and fairly compare different citation generation methods, leading\nto difficulties in reproducing different methods and a comprehensive\nassessment. To cope with the problems above, we introduce \\name, an open-source\nand modular toolkit designed to facilitate the implementation and evaluation of\nexisting citation generation methods, while also fostering the development of\nnew approaches to improve citation quality in LLM outputs. This tool is highly\nextensible, allowing users to utilize 4 main modules and 14 components to\nconstruct a pipeline, evaluating an existing method or innovative designs. Our\nexperiments with two state-of-the-art LLMs and 11 citation generation baselines\ndemonstrate varying strengths of different modules in answer accuracy and\ncitation quality improvement, as well as the challenge of enhancing\ngranularity. Based on our analysis of the effectiveness of components, we\npropose a new method, self-RAG \\snippet, obtaining a balanced answer accuracy\nand citation quality. Citekit is released at\nhttps://github.com/SjJ1017/Citekit."
        ],
        "neg": []
    },
    {
        "query": "Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition.",
        "pos": [
            "Enabling Large Language Models (LLMs) to generate citations in\nQuestion-Answering (QA) tasks is an emerging paradigm aimed at enhancing the\nverifiability of their responses when LLMs are utilizing external references to\ngenerate an answer. However, there is currently no unified framework to\nstandardize and fairly compare different citation generation methods, leading\nto difficulties in reproducing different methods and a comprehensive\nassessment. To cope with the problems above, we introduce \\name, an open-source\nand modular toolkit designed to facilitate the implementation and evaluation of\nexisting citation generation methods, while also fostering the development of\nnew approaches to improve citation quality in LLM outputs. This tool is highly\nextensible, allowing users to utilize 4 main modules and 14 components to\nconstruct a pipeline, evaluating an existing method or innovative designs. Our\nexperiments with two state-of-the-art LLMs and 11 citation generation baselines\ndemonstrate varying strengths of different modules in answer accuracy and\ncitation quality improvement, as well as the challenge of enhancing\ngranularity. Based on our analysis of the effectiveness of components, we\npropose a new method, self-RAG \\snippet, obtaining a balanced answer accuracy\nand citation quality. Citekit is released at\nhttps://github.com/SjJ1017/Citekit."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.",
        "pos": [
            "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.",
        "pos": [
            "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.",
        "pos": [
            "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.",
        "pos": [
            "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency.",
            "The many-to-many multilingual neural machine translation can be regarded as\nthe process of integrating semantic features from the source sentences and\nlinguistic features from the target sentences. To enhance zero-shot\ntranslation, models need to share knowledge across languages, which can be\nachieved through auxiliary tasks for learning a universal representation or\ncross-lingual mapping. To this end, we propose to exploit both semantic and\nlinguistic features between multiple languages to enhance multilingual\ntranslation. On the encoder side, we introduce a disentangling learning task\nthat aligns encoder representations by disentangling semantic and linguistic\nfeatures, thus facilitating knowledge transfer while preserving complete\ninformation. On the decoder side, we leverage a linguistic encoder to integrate\nlow-level linguistic features to assist in the target language generation.\nExperimental results on multilingual datasets demonstrate significant\nimprovement in zero-shot translation compared to the baseline system, while\nmaintaining performance in supervised translation. Further analysis validates\nthe effectiveness of our method in leveraging both semantic and linguistic\nfeatures. The code is available at https://github.com/ictnlp/SemLing-MNMT."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.",
        "pos": [
            "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.",
        "pos": [
            "The recent advancements in large language models (LLMs) with billions of\nparameters have significantly boosted their performance across various\nreal-world applications. However, the inference processes for these models\nrequire substantial energy and computational resources, presenting considerable\ndeployment challenges. In contrast, human brains, which contain approximately\n86 billion biological neurons, exhibit significantly greater energy efficiency\ncompared to LLMs with a similar number of parameters. Inspired by this, we\nredesign 7 to 70 billion parameter LLMs using bio-plausible spiking mechanisms,\nemulating the efficient behavior of the human brain. We propose the first\nspiking large language model as recent LLMs termed SpikeLLM. Coupled with the\nproposed model, a novel spike-driven quantization framework named Optimal Brain\nSpiking is introduced to reduce the energy cost and accelerate inference speed\nvia two essential approaches: first (second)-order differentiation-based\nsalient channel detection, and per-channel salient outlier expansion with\nGeneralized Integrate-and-Fire neurons. Our proposed spike-driven quantization\ncan plug in main streams of quantization training methods. In the OmniQuant\npipeline, SpikeLLM significantly reduces 25.51% WikiText2 perplexity and\nimproves 3.08% average accuracy of 6 zero-shot datasets on a LLAMA2-7B 4A4W\nmodel. In the GPTQ pipeline, SpikeLLM realizes a sparse ternary quantization,\nwhich achieves additive in all linear layers. Compared with PB-LLM with similar\noperations, SpikeLLM also exceeds significantly. We will release our code on\nGitHub."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.",
        "pos": [
            "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.",
        "pos": [
            "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency."
        ],
        "neg": []
    },
    {
        "query": "Data visualization (DV) is the fundamental and premise tool to improve the\nefficiency in conveying the insights behind the big data, which has been widely\naccepted in existing data-driven world. Task automation in DV, such as\nconverting natural language queries to visualizations (i.e., text-to-vis),\ngenerating explanations from visualizations (i.e., vis-to-text), answering\nDV-related questions in free form (i.e. FeVisQA), and explicating tabular data\n(i.e., table-to-text), is vital for advancing the field. Despite their\npotential, the application of pre-trained language models (PLMs) like T5 and\nBERT in DV has been limited by high costs and challenges in handling\ncross-modal information, leading to few studies on PLMs for DV. We introduce\n\\textbf{DataVisT5}, a novel PLM tailored for DV that enhances the T5\narchitecture through a hybrid objective pre-training and multi-task fine-tuning\nstrategy, integrating text and DV datasets to effectively interpret cross-modal\nsemantics. Extensive evaluations on public datasets show that DataVisT5\nconsistently outperforms current state-of-the-art models on various DV-related\ntasks. We anticipate that DataVisT5 will not only inspire further research on\nvertical PLMs but also expand the range of applications for PLMs.",
        "pos": [
            "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
        ],
        "neg": []
    },
    {
        "query": "Previous research on emergence in large language models shows these display\napparent human-like abilities and psychological latent traits. However, results\nare partly contradicting in expression and magnitude of these latent traits,\nyet agree on the worrisome tendencies to score high on the Dark Triad of\nnarcissism, psychopathy, and Machiavellianism, which, together with a track\nrecord of derailments, demands more rigorous research on safety of these\nmodels. We provided a state of the art language model with the same personality\nquestionnaire in nine languages, and performed Bayesian analysis of Gaussian\nMixture Model, finding evidence for a deeper-rooted issue. Our results suggest\nboth interlingual and intralingual instabilities, which indicate that current\nlanguage models do not develop a consistent core personality. This can lead to\nunsafe behaviour of artificial intelligence systems that are based on these\nfoundation models, and are increasingly integrated in human life. We\nsubsequently discuss the shortcomings of modern psychometrics, abstract it, and\nprovide a framework for its species-neutral, substrate-free formulation.",
        "pos": [
            "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with upstream model capabilities, potentially\nenabling \"safetywashing\" -- where capability improvements are misrepresented as\nsafety advancements. Based on these findings, we propose an empirical\nfoundation for developing more meaningful safety metrics and define AI safety\nin a machine learning research context as a set of clearly delineated research\ngoals that are empirically separable from generic capabilities advancements. In\ndoing so, we aim to provide a more rigorous framework for AI safety research,\nadvancing the science of safety evaluations and clarifying the path towards\nmeasurable progress."
        ],
        "neg": []
    },
    {
        "query": "Event Temporal Relation Extraction (ETRE) aims to identify the temporal\nrelationship between two events, which plays an important role in natural\nlanguage understanding. Most previous works follow a single-label\nclassification style, classifying an event pair into either a specific temporal\nrelation (e.g., \\textit{Before}, \\textit{After}), or a special label\n\\textit{Vague} when there may be multiple possible temporal relations between\nthe pair. In our work, instead of directly making predictions on\n\\textit{Vague}, we propose a multi-label classification solution for ETRE\n(METRE) to infer the possibility of each temporal relation independently, where\nwe treat \\textit{Vague} as the cases when there is more than one possible\nrelation between two events. We design a speculation mechanism to explore the\npossible relations hidden behind \\textit{Vague}, which enables the latent\ninformation to be used efficiently. Experiments on TB-Dense, MATRES and UDS-T\nshow that our method can effectively utilize the \\textit{Vague} instances to\nimprove the recognition for specific temporal relations and outperforms most\nstate-of-the-art methods.",
        "pos": [
            "Despite remarkable performance in legal consultation exhibited by legal Large\nLanguage Models(LLMs) combined with legal article retrieval components, there\nare still cases when the advice given is incorrect or baseless. To alleviate\nthese problems, we propose {\\bf ELLA}, a tool for {\\bf E}mpowering {\\bf L}LMs\nfor interpretable, accurate, and informative {\\bf L}egal {\\bf A}dvice. ELLA\nvisually presents the correlation between legal articles and LLM's response by\ncalculating their similarities, providing users with an intuitive legal basis\nfor the responses. Besides, based on the users' queries, ELLA retrieves\nrelevant legal articles and displays them to users. Users can interactively\nselect legal articles for LLM to generate more accurate responses. ELLA also\nretrieves relevant legal cases for user reference. Our user study shows that\npresenting the legal basis for the response helps users understand better. The\naccuracy of LLM's responses also improves when users intervene in selecting\nlegal articles for LLM. Providing relevant legal cases also aids individuals in\nobtaining comprehensive information."
        ],
        "neg": []
    },
    {
        "query": "Event Temporal Relation Extraction (ETRE) aims to identify the temporal\nrelationship between two events, which plays an important role in natural\nlanguage understanding. Most previous works follow a single-label\nclassification style, classifying an event pair into either a specific temporal\nrelation (e.g., \\textit{Before}, \\textit{After}), or a special label\n\\textit{Vague} when there may be multiple possible temporal relations between\nthe pair. In our work, instead of directly making predictions on\n\\textit{Vague}, we propose a multi-label classification solution for ETRE\n(METRE) to infer the possibility of each temporal relation independently, where\nwe treat \\textit{Vague} as the cases when there is more than one possible\nrelation between two events. We design a speculation mechanism to explore the\npossible relations hidden behind \\textit{Vague}, which enables the latent\ninformation to be used efficiently. Experiments on TB-Dense, MATRES and UDS-T\nshow that our method can effectively utilize the \\textit{Vague} instances to\nimprove the recognition for specific temporal relations and outperforms most\nstate-of-the-art methods.",
        "pos": [
            "Adapting large language models (LLMs) to new languages typically involves\ncontinual pre-training (CT) followed by supervised fine-tuning (SFT). However,\nthis CT-then-SFT approach struggles with limited data in the context of\nlow-resource languages, failing to balance language modeling and task-solving\ncapabilities. We thus propose model merging as an alternative for low-resource\nlanguages, combining models with distinct capabilities into a single model\nwithout additional training. We use model merging to develop task-solving LLMs\nfor low-resource languages without SFT data in the target languages. Our\nexperiments based on Llama-2-7B demonstrate that model merging effectively\nendows LLMs for low-resource languages with task-solving abilities,\noutperforming CT-then-SFT in scenarios with extremely scarce data. Observing\nperformance saturation in model merging with more training tokens, we further\nanalyze the merging process and introduce a slack variable to the model merging\nalgorithm to mitigate the loss of important parameters, thereby enhancing\nperformance. We hope that model merging can benefit more human languages\nsuffering from data scarcity with its higher data efficiency."
        ],
        "neg": []
    },
    {
        "query": "Event Temporal Relation Extraction (ETRE) aims to identify the temporal\nrelationship between two events, which plays an important role in natural\nlanguage understanding. Most previous works follow a single-label\nclassification style, classifying an event pair into either a specific temporal\nrelation (e.g., \\textit{Before}, \\textit{After}), or a special label\n\\textit{Vague} when there may be multiple possible temporal relations between\nthe pair. In our work, instead of directly making predictions on\n\\textit{Vague}, we propose a multi-label classification solution for ETRE\n(METRE) to infer the possibility of each temporal relation independently, where\nwe treat \\textit{Vague} as the cases when there is more than one possible\nrelation between two events. We design a speculation mechanism to explore the\npossible relations hidden behind \\textit{Vague}, which enables the latent\ninformation to be used efficiently. Experiments on TB-Dense, MATRES and UDS-T\nshow that our method can effectively utilize the \\textit{Vague} instances to\nimprove the recognition for specific temporal relations and outperforms most\nstate-of-the-art methods.",
        "pos": [
            "Despite remarkable performance in legal consultation exhibited by legal Large\nLanguage Models(LLMs) combined with legal article retrieval components, there\nare still cases when the advice given is incorrect or baseless. To alleviate\nthese problems, we propose {\\bf ELLA}, a tool for {\\bf E}mpowering {\\bf L}LMs\nfor interpretable, accurate, and informative {\\bf L}egal {\\bf A}dvice. ELLA\nvisually presents the correlation between legal articles and LLM's response by\ncalculating their similarities, providing users with an intuitive legal basis\nfor the responses. Besides, based on the users' queries, ELLA retrieves\nrelevant legal articles and displays them to users. Users can interactively\nselect legal articles for LLM to generate more accurate responses. ELLA also\nretrieves relevant legal cases for user reference. Our user study shows that\npresenting the legal basis for the response helps users understand better. The\naccuracy of LLM's responses also improves when users intervene in selecting\nlegal articles for LLM. Providing relevant legal cases also aids individuals in\nobtaining comprehensive information.",
            "Conditional question answering (CQA) is an important task that aims to find\nprobable answers and identify conditions that need to be satisfied to support\nthe answer. Existing approaches struggle with CQA due to two main challenges:\n(1) precisely identifying conditions and their logical relationship, and (2)\nverifying and solving the conditions. To address these challenges, we propose\nChain of Condition, a novel prompting approach by firstly identifying all\nconditions and constructing their logical relationships explicitly according to\nthe document, then verifying whether these conditions are satisfied, finally\nsolving the logical expression by tools to indicate any missing conditions and\ngenerating the answer based on the resolved conditions. The experiments on two\nbenchmark conditional question answering datasets shows chain of condition\noutperforms existing prompting baselines, establishing a new state-of-the-art.\nFurthermore, with backbone models like GPT-3.5-Turbo or GPT-4, it surpasses all\nsupervised baselines with only few-shot settings.",
            "Adapting large language models (LLMs) to new languages typically involves\ncontinual pre-training (CT) followed by supervised fine-tuning (SFT). However,\nthis CT-then-SFT approach struggles with limited data in the context of\nlow-resource languages, failing to balance language modeling and task-solving\ncapabilities. We thus propose model merging as an alternative for low-resource\nlanguages, combining models with distinct capabilities into a single model\nwithout additional training. We use model merging to develop task-solving LLMs\nfor low-resource languages without SFT data in the target languages. Our\nexperiments based on Llama-2-7B demonstrate that model merging effectively\nendows LLMs for low-resource languages with task-solving abilities,\noutperforming CT-then-SFT in scenarios with extremely scarce data. Observing\nperformance saturation in model merging with more training tokens, we further\nanalyze the merging process and introduce a slack variable to the model merging\nalgorithm to mitigate the loss of important parameters, thereby enhancing\nperformance. We hope that model merging can benefit more human languages\nsuffering from data scarcity with its higher data efficiency."
        ],
        "neg": []
    },
    {
        "query": "Gene transformer models such as Nucleotide Transformer, DNABert, and LOGO are\ntrained to learn optimal gene sequence representations by using the Masked\nLanguage Modeling (MLM) training objective over the complete Human Reference\nGenome. However, the typical tokenization methods employ a basic sliding window\nof tokens, such as k-mers, that fail to utilize gene-centric semantics. This\ncould result in the (trivial) masking of easily predictable sequences, leading\nto inefficient MLM training. Time-variant training strategies are known to\nimprove pretraining efficiency in both language and vision tasks. In this work,\nwe focus on using curriculum masking where we systematically increase the\ndifficulty of masked token prediction task by using a Pointwise Mutual\nInformation-based difficulty criterion, as gene sequences lack well-defined\nsemantic units similar to words or sentences of NLP domain. Our proposed\nCurriculum Masking-based Gene Masking Strategy (CM-GEMS) demonstrates superior\nrepresentation learning capabilities compared to baseline masking approaches\nwhen evaluated on downstream gene sequence classification tasks. We perform\nextensive evaluation in both few-shot (five datasets) and full dataset settings\n(Genomic Understanding Evaluation benchmark consisting of 27 tasks). Our\nfindings reveal that CM-GEMS outperforms state-of-the-art models (DNABert-2,\nNucleotide transformer, DNABert) trained at 120K steps, achieving similar\nresults in just 10K and 1K steps. We also demonstrate that Curriculum-Learned\nLOGO (a 2-layer DNABert-like model) can achieve nearly 90% of the\nstate-of-the-art model performance of 120K steps. We will make the models and\ncodes publicly available at https://github.com/roysoumya/curriculum-GeneMask.",
        "pos": [
            "The ever-increasing volume of digital information necessitates efficient\nmethods for users to extract key insights from lengthy documents. Aspect-based\nsummarization offers a targeted approach, generating summaries focused on\nspecific aspects within a document. Despite advancements in aspect-based\nsummarization research, there is a continuous quest for improved model\nperformance. Given that large language models (LLMs) have demonstrated the\npotential to revolutionize diverse tasks within natural language processing,\nparticularly in the problem of summarization, this paper explores the potential\nof fine-tuning LLMs for the aspect-based summarization task. We evaluate the\nimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,\nGemma and Aya, on a publicly available domain-specific aspect based summary\ndataset. We hypothesize that this approach will enable these models to\neffectively identify and extract aspect-related information, leading to\nsuperior quality aspect-based summaries compared to the state-of-the-art. We\nestablish a comprehensive evaluation framework to compare the performance of\nfine-tuned LLMs against competing aspect-based summarization methods and\nvanilla counterparts of the fine-tuned LLMs. Our work contributes to the field\nof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs\nfor generating high-quality aspect-based summaries. Furthermore, it opens doors\nfor further exploration of using LLMs for targeted information extraction tasks\nacross various NLP domains."
        ],
        "neg": []
    },
    {
        "query": "Inspired by protein folding, we explored the construction of\nthree-dimensional structures and machines from one-dimensional chains of simple\nbuilding blocks. This approach not only allows us to recreate the\nself-replication mechanism introduced earlier, but also significantly\nsimplifies the process. We introduced a new set of folding blocks that\nfacilitate the formation of secondary structures such as {\\alpha}-helices and\n\\b{eta}-sheets, as well as more advanced tertiary and quaternary structures,\nincluding self-replicating machines. The introduction of rotational degrees of\nfreedom leads to a reduced variety of blocks and, most importantly, reduces the\noverall size of the machines by a factor of five. In addition, we present a\nuniversal copier-constructor, a highly efficient self-replicating mechanism\ncomposed of approximately 40 blocks, including the restictions posed on it. The\npaper also addresses evolutionary considerations, outlining several steps on\nthe evolutionary ladder towards more sophisticated self-replicating systems.\nFinally, this study offers a clear rationale for nature's preference for\none-dimensional chains in constructing three-dimensional structures.",
        "pos": [
            "This study presents a theoretical model for a self-replicating mechanical\nsystem inspired by biological processes within living cells and supported by\ncomputer simulations. The model decomposes self-replication into core\ncomponents, each of which is executed by a single machine constructed from a\nset of basic block types. Key functionalities such as sorting, copying, and\nbuilding, are demonstrated. The model provides valuable insights into the\nconstraints of self-replicating systems. The discussion also addresses the\nspatial and timing behavior of the system, as well as its efficiency and\ncomplexity. This work provides a foundational framework for future studies on\nself-replicating mechanisms and their information-processing applications."
        ],
        "neg": []
    },
    {
        "query": "This chapter critically examines the potential contributions of modern\nlanguage models to theoretical linguistics. Despite their focus on engineering\ngoals, these models' ability to acquire sophisticated linguistic knowledge from\nmere exposure to data warrants a careful reassessment of their relevance to\nlinguistic theory. I review a growing body of empirical evidence suggesting\nthat language models can learn hierarchical syntactic structure and exhibit\nsensitivity to various linguistic phenomena, even when trained on\ndevelopmentally plausible amounts of data. While the competence/performance\ndistinction has been invoked to dismiss the relevance of such models to\nlinguistic theory, I argue that this assessment may be premature. By carefully\ncontrolling learning conditions and making use of causal intervention methods,\nexperiments with language models can potentially constrain hypotheses about\nlanguage acquisition and competence. I conclude that closer collaboration\nbetween theoretical linguists and computational researchers could yield\nvaluable insights, particularly in advancing debates about linguistic nativism.",
        "pos": [
            "Evaluating the cognitive capacities of large language models (LLMs) requires\novercoming not only anthropomorphic but also anthropocentric biases. This\narticle identifies two types of anthropocentric bias that have been neglected:\noverlooking how auxiliary factors can impede LLM performance despite competence\n(Type-I), and dismissing LLM mechanistic strategies that differ from those of\nhumans as not genuinely competent (Type-II). Mitigating these biases\nnecessitates an empirically-driven, iterative approach to mapping cognitive\ntasks to LLM-specific capacities and mechanisms, which can be done by\nsupplementing carefully designed behavioral experiments with mechanistic\nstudies."
        ],
        "neg": []
    },
    {
        "query": "Fingerspelling poses challenges for sign language processing due to its\nhigh-frequency motion and use for open-vocabulary terms. While prior work has\nstudied fingerspelling recognition, there has been little attention to\nevaluating how well sign language translation models understand fingerspelling\nin the context of entire sentences -- and improving this capability. We\nmanually annotate instances of fingerspelling within FLEURS-ASL and use them to\nevaluate the effect of two simple measures to improve fingerspelling\nrecognition within American Sign Language to English translation: 1) use a\nmodel family (ByT5) with character- rather than subword-level tokenization, and\n2) mix fingerspelling recognition data into the translation training mixture.\nWe find that 1) substantially improves understanding of fingerspelling (and\ntherefore translation quality overall), but the effect of 2) is mixed.",
        "pos": [
            "Sign language translation (SLT) addresses the problem of translating\ninformation from a sign language in video to a spoken language in text.\nExisting studies, while showing progress, are often limited to narrow domains\nand/or few sign languages and struggle with open-domain tasks. In this paper,\nwe push forward the frontier of SLT by scaling pretraining data, model size,\nand number of translation directions. We perform large-scale SLT pretraining on\ndifferent data including 1) noisy multilingual YouTube SLT data, 2) parallel\ntext corpora, and 3) SLT data augmented by translating video captions to other\nlanguages with off-the-shelf machine translation models. We unify different\npretraining tasks with task-specific prompts under the encoder-decoder\narchitecture, and initialize the SLT model with pretrained (m/By)T5 models\nacross model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL\nto 42 spoken languages) demonstrate the significance of data/model scaling and\ncross-lingual cross-modal transfer, as well as the feasibility of zero-shot\nSLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT\nbenchmarks covering 5 sign languages. Experiments show substantial quality\nimprovements over the vanilla baselines, surpassing the previous\nstate-of-the-art (SOTA) by wide margins.",
            "Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25."
        ],
        "neg": []
    },
    {
        "query": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges.",
        "pos": [
            "Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models."
        ],
        "neg": []
    },
    {
        "query": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges.",
        "pos": [
            "Despite the recent proliferation of large language models (LLMs), their\ntraining recipes -- model architecture, pre-training data and optimization\nalgorithm -- are often very similar. This naturally raises the question of the\nsimilarity among the resulting models. In this paper, we propose a novel\nsetting, imaginary question answering (IQA), to better understand model\nsimilarity. In IQA, we ask one model to generate purely imaginary questions\n(e.g., on completely made-up concepts in physics) and prompt another model to\nanswer. Surprisingly, despite the total fictionality of these questions, all\nmodels can answer each other's questions with remarkable success, suggesting a\n\"shared imagination space\" in which these models operate during such\nhallucinations. We conduct a series of investigations into this phenomenon and\ndiscuss implications on model homogeneity, hallucination, and computational\ncreativity."
        ],
        "neg": []
    },
    {
        "query": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges.",
        "pos": [
            "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
            "Despite the recent proliferation of large language models (LLMs), their\ntraining recipes -- model architecture, pre-training data and optimization\nalgorithm -- are often very similar. This naturally raises the question of the\nsimilarity among the resulting models. In this paper, we propose a novel\nsetting, imaginary question answering (IQA), to better understand model\nsimilarity. In IQA, we ask one model to generate purely imaginary questions\n(e.g., on completely made-up concepts in physics) and prompt another model to\nanswer. Surprisingly, despite the total fictionality of these questions, all\nmodels can answer each other's questions with remarkable success, suggesting a\n\"shared imagination space\" in which these models operate during such\nhallucinations. We conduct a series of investigations into this phenomenon and\ndiscuss implications on model homogeneity, hallucination, and computational\ncreativity.",
            "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research.",
            "Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io."
        ],
        "neg": []
    },
    {
        "query": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to a particular domain or\ntask. Model MoErging methods aim to recycle expert models to create an\naggregate system with improved performance or generalization. A key component\nof MoErging methods is the creation of a router that decides which expert\nmodel(s) to use for a particular input or application. The promise,\neffectiveness, and large design space of MoErging has spurred the development\nof many new methods over the past few years. This rapid pace of development has\nmade it challenging to compare different MoErging methods, which are rarely\ncompared to one another and are often validated in different experimental\nsetups. To remedy such gaps, we present a comprehensive survey of MoErging\nmethods that includes a novel taxonomy for cataloging key design choices and\nclarifying suitable applications for each method. Apart from surveying MoErging\nresearch, we inventory software tools and applications that make use of\nMoErging. We additionally discuss related fields of study such as model\nmerging, multitask learning, and mixture-of-experts models. Taken as a whole,\nour survey provides a unified overview of existing MoErging methods and creates\na solid foundation for future work in this burgeoning field.",
        "pos": [
            "Extracting social determinants of health (SDoH) from unstructured medical\nnotes depends heavily on labor-intensive annotations, which are typically\ntask-specific, hampering reusability and limiting sharing. In this study we\nintroduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)\nmethod leveraging contrastive examples and concise instructions to extract SDoH\nwithout relying on extensive medical annotations or costly human intervention.\nIt achieved tenfold and twentyfold reductions in time and cost respectively,\nand superior consistency with human annotators measured by Cohen's kappa of up\nto 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the\nstrengths of both, ensuring high accuracy and computational efficiency while\nconsistently maintaining 0.90+ AUROC scores. Testing across three distinct\ndatasets has confirmed its robustness and accuracy. This study highlights the\npotential of leveraging LLMs to revolutionize medical note classification,\ndemonstrating their capability to achieve highly accurate classifications with\nsignificantly reduced time and cost.",
            "Large language models are often expected to constantly adapt to new sources\nof knowledge and knowledge editing techniques aim to efficiently patch the\noutdated model knowledge, with minimal modification. Most prior works focus on\nmonolingual knowledge editing in English, even though new information can\nemerge in any language from any part of the world. We propose the Cross-Lingual\nMulti-Hop Knowledge Editing paradigm, for measuring and analyzing the\nperformance of various SoTA knowledge editing techniques in a cross-lingual\nsetup. Specifically, we create a parallel cross-lingual benchmark,\nCROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive\nanalysis over various knowledge editing techniques uncover significant gaps in\nperformance between the cross-lingual and English-centric setting. Following\nthis, we propose a significantly improved system for cross-lingual multi-hop\nknowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and\ngenerate knowledge editing framework, where a retriever is formulated to recall\nedited facts and support an LLM to adhere to knowledge edits. We develop\nlanguage-aware and hard-negative based contrastive objectives for improving the\ncross-lingual and fine-grained fact retrieval and verification process used in\nthis framework. Extensive experiments on three LLMs, eight languages, and two\ndatasets show CLEVER-CKE's significant gains of up to 30% over prior methods.",
            "Test-time interventions for language models can enhance factual accuracy,\nmitigate harmful outputs, and improve model efficiency without costly\nretraining. But despite a flood of new methods, different types of\ninterventions are largely developing independently. In practice, multiple\ninterventions must be applied sequentially to the same model, yet we lack\nstandardized ways to study how interventions interact. We fill this gap by\nintroducing composable interventions, a framework to study the effects of using\nmultiple interventions on the same language models, featuring new metrics and a\nunified codebase. Using our framework, we conduct extensive experiments and\ncompose popular methods from three emerging intervention categories --\nKnowledge Editing, Model Compression, and Machine Unlearning. Our results from\n310 different compositions uncover meaningful interactions: compression hinders\nediting and unlearning, composing interventions hinges on their order of\napplication, and popular general-purpose metrics are inadequate for assessing\ncomposability. Taken together, our findings showcase clear gaps in\ncomposability, suggesting a need for new multi-objective interventions. All of\nour code is public:\nhttps://github.com/hartvigsen-group/composable-interventions.",
            "In this paper, we introduce Dynamic Layer Operations (DLO), a novel approach\nfor vertically scaling transformer-based Large Language Models (LLMs) by\ndynamically expanding, activating, or skipping layers using a sophisticated\nrouting policy based on layerwise feature similarity. Unlike traditional\nMixture-of-Experts (MoE) methods that focus on extending the model width, our\napproach targets model depth, addressing the redundancy observed across layer\nrepresentations for various input samples. Our framework is integrated with the\nSupervised Fine-Tuning (SFT) stage, eliminating the need for resource-intensive\nContinual Pre-Training (CPT). Experimental results demonstrate that DLO not\nonly outperforms the original unscaled models but also achieves comparable\nresults to densely expanded models with significantly improved efficiency. Our\nwork offers a promising direction for building efficient yet powerful LLMs. We\nwill release our implementation and model weights upon acceptance."
        ],
        "neg": []
    },
    {
        "query": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to a particular domain or\ntask. Model MoErging methods aim to recycle expert models to create an\naggregate system with improved performance or generalization. A key component\nof MoErging methods is the creation of a router that decides which expert\nmodel(s) to use for a particular input or application. The promise,\neffectiveness, and large design space of MoErging has spurred the development\nof many new methods over the past few years. This rapid pace of development has\nmade it challenging to compare different MoErging methods, which are rarely\ncompared to one another and are often validated in different experimental\nsetups. To remedy such gaps, we present a comprehensive survey of MoErging\nmethods that includes a novel taxonomy for cataloging key design choices and\nclarifying suitable applications for each method. Apart from surveying MoErging\nresearch, we inventory software tools and applications that make use of\nMoErging. We additionally discuss related fields of study such as model\nmerging, multitask learning, and mixture-of-experts models. Taken as a whole,\nour survey provides a unified overview of existing MoErging methods and creates\na solid foundation for future work in this burgeoning field.",
        "pos": [
            "Language models can be used to solve long-horizon planning problems in two\ndistinct modes: a fast 'System-1' mode, directly generating plans without any\nexplicit search or backtracking, and a slow 'System-2' mode, planning\nstep-by-step by explicitly searching over possible actions. While System-2 is\ntypically more effective, it is also more computationally expensive, making it\ninfeasible for long plans or large action spaces. Moreover, isolated System-1\nor 2 ignores the user's end goals, failing to provide ways to control the\nmodel's behavior. To this end, we propose the System-1.x Planner, a\ncontrollable planning framework with LLMs that is capable of generating hybrid\nplans and balancing between the two planning modes based on the difficulty of\nthe problem at hand. System-1.x consists of (i) a controller, (ii) a System-1\nPlanner, and (iii) a System-2 Planner. Based on a user-specified hybridization\nfactor (x) governing the mixture between System-1 and 2, the controller\ndecomposes a problem into sub-goals, and classifies them as easy or hard to be\nsolved by either System-1 or 2, respectively. We fine-tune all three components\non top of a single base LLM, requiring only search traces as supervision.\nExperiments with two diverse planning tasks -- Maze Navigation and Blocksworld\n-- show that our System-1.x Planner outperforms a System-1 Planner, a System-2\nPlanner trained to approximate A* search, and also a symbolic planner (A*). We\ndemonstrate the following key properties of our planner: (1) controllability:\nincreasing the hybridization factor (e.g., System-1.75 vs 1.5) performs more\nsearch, improving performance, (2) flexibility: by building a neuro-symbolic\nvariant with a neural System-1 and a symbolic System-2, we can use existing\nsymbolic methods, and (3) generalizability: by being able to learn from\ndifferent search algorithms, our method is robust to the choice of search\nalgorithm."
        ],
        "neg": []
    },
    {
        "query": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to a particular domain or\ntask. Model MoErging methods aim to recycle expert models to create an\naggregate system with improved performance or generalization. A key component\nof MoErging methods is the creation of a router that decides which expert\nmodel(s) to use for a particular input or application. The promise,\neffectiveness, and large design space of MoErging has spurred the development\nof many new methods over the past few years. This rapid pace of development has\nmade it challenging to compare different MoErging methods, which are rarely\ncompared to one another and are often validated in different experimental\nsetups. To remedy such gaps, we present a comprehensive survey of MoErging\nmethods that includes a novel taxonomy for cataloging key design choices and\nclarifying suitable applications for each method. Apart from surveying MoErging\nresearch, we inventory software tools and applications that make use of\nMoErging. We additionally discuss related fields of study such as model\nmerging, multitask learning, and mixture-of-experts models. Taken as a whole,\nour survey provides a unified overview of existing MoErging methods and creates\na solid foundation for future work in this burgeoning field.",
        "pos": [
            "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.",
            "Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: https://github.com/IBM/BenchBench\n  Leaderboard: https://huggingface.co/spaces/per/BenchBench",
            "Human feedback data is a critical component in developing language models.\nHowever, collecting this feedback is costly and ultimately not scalable. We\npropose a scalable method for extracting feedback that users naturally include\nwhen interacting with chat models, and leveraging it for model training. We are\nfurther motivated by previous work that showed there are also qualitative\nadvantages to using naturalistic (rather than auto-generated) feedback, such as\nless hallucinations and biases. We manually annotated conversation data to\nconfirm the presence of naturally occurring feedback in a standard corpus,\nfinding that as much as 30% of the chats include explicit feedback. We apply\nour method to over 1M conversations to obtain hundreds of thousands of feedback\nsamples. Training with the extracted feedback shows significant performance\nimprovements over baseline models, demonstrating the efficacy of our approach\nin enhancing model alignment to human preferences."
        ],
        "neg": []
    },
    {
        "query": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to a particular domain or\ntask. Model MoErging methods aim to recycle expert models to create an\naggregate system with improved performance or generalization. A key component\nof MoErging methods is the creation of a router that decides which expert\nmodel(s) to use for a particular input or application. The promise,\neffectiveness, and large design space of MoErging has spurred the development\nof many new methods over the past few years. This rapid pace of development has\nmade it challenging to compare different MoErging methods, which are rarely\ncompared to one another and are often validated in different experimental\nsetups. To remedy such gaps, we present a comprehensive survey of MoErging\nmethods that includes a novel taxonomy for cataloging key design choices and\nclarifying suitable applications for each method. Apart from surveying MoErging\nresearch, we inventory software tools and applications that make use of\nMoErging. We additionally discuss related fields of study such as model\nmerging, multitask learning, and mixture-of-experts models. Taken as a whole,\nour survey provides a unified overview of existing MoErging methods and creates\na solid foundation for future work in this burgeoning field.",
        "pos": [
            "While finetuning language models from pairwise preferences has proven\nremarkably effective, the underspecified nature of natural language presents\ncritical challenges. Direct preference feedback is uninterpretable, difficult\nto provide where multidimensional criteria may apply, and often inconsistent,\neither because it is based on incomplete instructions or provided by diverse\nprincipals. To address these challenges, we consider the two-step preference\nmodeling procedure that first resolves the under-specification by selecting a\ncontext, and then evaluates preference with respect to the chosen context. We\ndecompose reward modeling error according to these two steps, which suggests\nthat supervising context in addition to context-specific preference may be a\nviable approach to aligning models with diverse human preferences. For this to\nwork, the ability of models to evaluate context-specific preference is\ncritical. To this end, we contribute context-conditioned preference datasets\nand accompanying experiments that investigate the ability of language models to\nevaluate context-specific preference. We use our datasets to (1) show that\nexisting preference models benefit from, but fail to fully consider, added\ncontext, (2) finetune a context-aware reward model with context-specific\nperformance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)\ninvestigate the value of context-aware preference modeling."
        ],
        "neg": []
    },
    {
        "query": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.",
        "pos": [
            "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs.",
            "Event Factuality Detection (EFD) task determines the factuality of textual\nevents, i.e., classifying whether an event is a fact, possibility, or\nimpossibility, which is essential for faithfully understanding and utilizing\nevent knowledge. However, due to the lack of high-quality large-scale data,\nevent factuality detection is under-explored in event understanding research,\nwhich limits the development of EFD community. To address these issues and\nprovide faithful event understanding, we introduce MAVEN-Fact, a large-scale\nand high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes\nfactuality annotations of 112,276 events, making it the largest EFD dataset.\nExtensive experiments demonstrate that MAVEN-Fact is challenging for both\nconventional fine-tuned models and large language models (LLMs). Thanks to the\ncomprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact\nalso supports some further analyses and we find that adopting event arguments\nand relations helps in event factuality detection for fine-tuned models but\ndoes not benefit LLMs. Furthermore, we preliminarily study an application case\nof event factuality detection and find it helps in mitigating event-related\nhallucination in LLMs. Our dataset and codes can be obtained from\n\\url{https://github.com/lcy2723/MAVEN-FACT}",
            "Entity Linking (EL) models are well-trained at mapping mentions to their\ncorresponding entities according to a given context. However, EL models\nstruggle to disambiguate long-tail entities due to their limited training data.\nMeanwhile, large language models (LLMs) are more robust at interpreting\nuncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at\ngenerating correct entity IDs. Furthermore, training an LLM to perform EL is\ncost-intensive. Building upon these insights, we introduce LLM-Augmented Entity\nLinking LLMAEL, a plug-and-play approach to enhance entity linking through LLM\ndata augmentation. We leverage LLMs as knowledgeable context augmenters,\ngenerating mention-centered descriptions as additional input, while preserving\ntraditional EL models for task specific processing. Experiments on 6 standard\ndatasets show that the vanilla LLMAEL outperforms baseline EL models in most\ncases, while the fine-tuned LLMAEL set the new state-of-the-art results across\nall 6 benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.",
        "pos": [
            "Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at \\url{https://github.com/THUDM/VisualAgentBench}.",
            "As a branch of advanced artificial intelligence, dialogue systems are\nprospering. Multi-turn response selection is a general research problem in\ndialogue systems. With the assistance of background information and pre-trained\nlanguage models, the performance of state-of-the-art methods on this problem\ngains impressive improvement. However, existing studies neglect the importance\nof external commonsense knowledge. Hence, we design a Siamese network where a\npre-trained Language model merges with a Graph neural network (SinLG). SinLG\ntakes advantage of Pre-trained Language Models (PLMs) to catch the word\ncorrelations in the context and response candidates and utilizes a Graph Neural\nNetwork (GNN) to reason helpful common sense from an external knowledge graph.\nThe GNN aims to assist the PLM in fine-tuning, and arousing its related\nmemories to attain better performance. Specifically, we first extract related\nconcepts as nodes from an external knowledge graph to construct a subgraph with\nthe context response pair as a super node for each sample. Next, we learn two\nrepresentations for the context response pair via both the PLM and GNN. A\nsimilarity loss between the two representations is utilized to transfer the\ncommonsense knowledge from the GNN to the PLM. Then only the PLM is used to\ninfer online so that efficiency can be guaranteed. Finally, we conduct\nextensive experiments on two variants of the PERSONA-CHAT dataset, which proves\nthat our solution can not only improve the performance of the PLM but also\nachieve an efficient inference."
        ],
        "neg": []
    },
    {
        "query": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.",
        "pos": [
            "Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at \\url{https://github.com/THUDM/VisualAgentBench}.",
            "As a branch of advanced artificial intelligence, dialogue systems are\nprospering. Multi-turn response selection is a general research problem in\ndialogue systems. With the assistance of background information and pre-trained\nlanguage models, the performance of state-of-the-art methods on this problem\ngains impressive improvement. However, existing studies neglect the importance\nof external commonsense knowledge. Hence, we design a Siamese network where a\npre-trained Language model merges with a Graph neural network (SinLG). SinLG\ntakes advantage of Pre-trained Language Models (PLMs) to catch the word\ncorrelations in the context and response candidates and utilizes a Graph Neural\nNetwork (GNN) to reason helpful common sense from an external knowledge graph.\nThe GNN aims to assist the PLM in fine-tuning, and arousing its related\nmemories to attain better performance. Specifically, we first extract related\nconcepts as nodes from an external knowledge graph to construct a subgraph with\nthe context response pair as a super node for each sample. Next, we learn two\nrepresentations for the context response pair via both the PLM and GNN. A\nsimilarity loss between the two representations is utilized to transfer the\ncommonsense knowledge from the GNN to the PLM. Then only the PLM is used to\ninfer online so that efficiency can be guaranteed. Finally, we conduct\nextensive experiments on two variants of the PERSONA-CHAT dataset, which proves\nthat our solution can not only improve the performance of the PLM but also\nachieve an efficient inference."
        ],
        "neg": []
    },
    {
        "query": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.",
        "pos": [
            "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs.",
            "Event Factuality Detection (EFD) task determines the factuality of textual\nevents, i.e., classifying whether an event is a fact, possibility, or\nimpossibility, which is essential for faithfully understanding and utilizing\nevent knowledge. However, due to the lack of high-quality large-scale data,\nevent factuality detection is under-explored in event understanding research,\nwhich limits the development of EFD community. To address these issues and\nprovide faithful event understanding, we introduce MAVEN-Fact, a large-scale\nand high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes\nfactuality annotations of 112,276 events, making it the largest EFD dataset.\nExtensive experiments demonstrate that MAVEN-Fact is challenging for both\nconventional fine-tuned models and large language models (LLMs). Thanks to the\ncomprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact\nalso supports some further analyses and we find that adopting event arguments\nand relations helps in event factuality detection for fine-tuned models but\ndoes not benefit LLMs. Furthermore, we preliminarily study an application case\nof event factuality detection and find it helps in mitigating event-related\nhallucination in LLMs. Our dataset and codes can be obtained from\n\\url{https://github.com/lcy2723/MAVEN-FACT}",
            "Entity Linking (EL) models are well-trained at mapping mentions to their\ncorresponding entities according to a given context. However, EL models\nstruggle to disambiguate long-tail entities due to their limited training data.\nMeanwhile, large language models (LLMs) are more robust at interpreting\nuncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at\ngenerating correct entity IDs. Furthermore, training an LLM to perform EL is\ncost-intensive. Building upon these insights, we introduce LLM-Augmented Entity\nLinking LLMAEL, a plug-and-play approach to enhance entity linking through LLM\ndata augmentation. We leverage LLMs as knowledgeable context augmenters,\ngenerating mention-centered descriptions as additional input, while preserving\ntraditional EL models for task specific processing. Experiments on 6 standard\ndatasets show that the vanilla LLMAEL outperforms baseline EL models in most\ncases, while the fine-tuned LLMAEL set the new state-of-the-art results across\nall 6 benchmarks."
        ],
        "neg": []
    },
    {
        "query": "As large language models (LLMs) continue to scale, their enhanced performance\noften proves insufficient for solving domain-specific tasks. Systematically\nanalyzing their failures and effectively enhancing their performance remain\nsignificant challenges. This paper introduces the Re-TASK framework, a novel\ntheoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge\nperspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space\nTheory. The Re-TASK framework provides a systematic methodology to deepen our\nunderstanding, evaluation, and enhancement of LLMs for domain-specific tasks.\nIt explores the interplay among an LLM's capabilities, the knowledge it\nprocesses, and the skills it applies, elucidating how these elements are\ninterconnected and impact task performance. Our application of the Re-TASK\nframework reveals that many failures in domain-specific tasks can be attributed\nto insufficient knowledge or inadequate skill adaptation. With this insight, we\npropose structured strategies for enhancing LLMs through targeted knowledge\ninjection and skill adaptation. Specifically, we identify key capability items\nassociated with tasks and employ a deliberately designed prompting strategy to\nenhance task performance, thereby reducing the need for extensive fine-tuning.\nAlternatively, we fine-tune the LLM using capability-specific instructions,\nfurther validating the efficacy of our framework. Experimental results confirm\nthe framework's effectiveness, demonstrating substantial improvements in both\nthe performance and applicability of LLMs.",
        "pos": [
            "Dysarthric speech recognition (DSR) presents a formidable challenge due to\ninherent inter-speaker variability, leading to severe performance degradation\nwhen applying DSR models to new dysarthric speakers. Traditional speaker\nadaptation methodologies typically involve fine-tuning models for each speaker,\nbut this strategy is cost-prohibitive and inconvenient for disabled users,\nrequiring substantial data collection. To address this issue, we introduce a\nprototype-based approach that markedly improves DSR performance for unseen\ndysarthric speakers without additional fine-tuning. Our method employs a\nfeature extractor trained with HuBERT to produce per-word prototypes that\nencapsulate the characteristics of previously unseen speakers. These prototypes\nserve as the basis for classification. Additionally, we incorporate supervised\ncontrastive learning to refine feature extraction. By enhancing representation\nquality, we further improve DSR performance, enabling effective personalized\nDSR. We release our code at https://github.com/NKU-HLT/PB-DSR.",
            "Recent advancements in LLMs have showcased their remarkable role-playing\ncapabilities, able to accurately simulate the dialogue styles and cognitive\nprocesses of various roles based on different instructions and contexts.\nStudies indicate that assigning LLMs the roles of experts, a strategy known as\nrole-play prompting, can enhance their performance in the corresponding\ndomains. However, the prompt needs to be manually designed for the given\nproblem, requiring certain expertise and iterative modifications. To this end,\nwe propose self-prompt tuning, making LLMs themselves generate role-play\nprompts through fine-tuning. Leveraging the LIMA dataset as our foundational\ncorpus, we employ GPT-4 to annotate role-play prompts for each data points,\nresulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like\nLlama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned\nLLMs can automatically generate expert role prompts for any given question. We\nextensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and\nopen-ended question test. Our empirical results illustrate that self-prompt\ntuned LLMs outperform standard instruction tuned baselines across most\ndatasets. This highlights the great potential of utilizing fine-tuning to\nenable LLMs to self-prompt, thereby automating complex prompting strategies. We\nrelease the dataset, models, and code at this\n\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}."
        ],
        "neg": []
    },
    {
        "query": "As large language models (LLMs) continue to scale, their enhanced performance\noften proves insufficient for solving domain-specific tasks. Systematically\nanalyzing their failures and effectively enhancing their performance remain\nsignificant challenges. This paper introduces the Re-TASK framework, a novel\ntheoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge\nperspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space\nTheory. The Re-TASK framework provides a systematic methodology to deepen our\nunderstanding, evaluation, and enhancement of LLMs for domain-specific tasks.\nIt explores the interplay among an LLM's capabilities, the knowledge it\nprocesses, and the skills it applies, elucidating how these elements are\ninterconnected and impact task performance. Our application of the Re-TASK\nframework reveals that many failures in domain-specific tasks can be attributed\nto insufficient knowledge or inadequate skill adaptation. With this insight, we\npropose structured strategies for enhancing LLMs through targeted knowledge\ninjection and skill adaptation. Specifically, we identify key capability items\nassociated with tasks and employ a deliberately designed prompting strategy to\nenhance task performance, thereby reducing the need for extensive fine-tuning.\nAlternatively, we fine-tune the LLM using capability-specific instructions,\nfurther validating the efficacy of our framework. Experimental results confirm\nthe framework's effectiveness, demonstrating substantial improvements in both\nthe performance and applicability of LLMs.",
        "pos": [
            "Language, as an information medium created by advanced organisms, has always\nbeen a concern of neuroscience regarding how it is represented in the brain.\nDecoding linguistic representations in the evoked brain has shown\ngroundbreaking achievements, thanks to the rapid improvement of neuroimaging,\nmedical technology, life sciences and artificial intelligence. In this work, we\npresent a taxonomy of brain-to-language decoding of both textual and speech\nformats. This work integrates two types of research: neuroscience focusing on\nlanguage understanding and deep learning-based brain decoding. Generating\ndiscernible language information from brain activity could not only help those\nwith limited articulation, especially amyotrophic lateral sclerosis (ALS)\npatients but also open up a new way for the next generation's brain-computer\ninterface (BCI). This article will help brain scientists and deep-learning\nresearchers to gain a bird's eye view of fine-grained language perception, and\nthus facilitate their further investigation and research of neural process and\nlanguage decoding.",
            "Instruction tuning in multimodal large language models (MLLMs) aims to\nsmoothly integrate a backbone LLM with a pre-trained feature encoder for\ndownstream tasks. The major challenge is how to efficiently find the synergy\nthrough cooperative learning where LLMs adapt their reasoning abilities in\ndownstream tasks while feature encoders adjust their encoding to provide more\nrelevant modal information. In this paper, we analyze the MLLM instruction\ntuning from both theoretical and empirical perspectives, where we find\nunbalanced learning between the two components, i.e., the feature encoder and\nthe LLM, can cause diminishing learning gradients that slow the model\nconvergence and often lead to sub-optimal results due to insufficient learning.\nInspired by our findings, we propose a measurement to quantitatively evaluate\nthe learning balance, based on which we further design a dynamic learning\nscheduler that better coordinates the learning. In addition, we introduce an\nauxiliary loss regularization method to promote updating of the generation\ndistribution of MLLMs considering the learning state of each model component,\nwhich potentially prevents each component from gradient diminishing and enables\na more accurate estimation of the learning balance coefficient. We conduct\nexperiments with multiple LLM backbones and feature encoders, where our\ntechniques are model-agnostic and can be generically integrated with various\nMLLM backbones. Experiment results on multiple downstream tasks and modalities\nin vision and audio, demonstrate the proposed method's better efficiency and\neffectiveness in MLLM instruction tuning."
        ],
        "neg": []
    },
    {
        "query": "As large language models (LLMs) continue to scale, their enhanced performance\noften proves insufficient for solving domain-specific tasks. Systematically\nanalyzing their failures and effectively enhancing their performance remain\nsignificant challenges. This paper introduces the Re-TASK framework, a novel\ntheoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge\nperspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space\nTheory. The Re-TASK framework provides a systematic methodology to deepen our\nunderstanding, evaluation, and enhancement of LLMs for domain-specific tasks.\nIt explores the interplay among an LLM's capabilities, the knowledge it\nprocesses, and the skills it applies, elucidating how these elements are\ninterconnected and impact task performance. Our application of the Re-TASK\nframework reveals that many failures in domain-specific tasks can be attributed\nto insufficient knowledge or inadequate skill adaptation. With this insight, we\npropose structured strategies for enhancing LLMs through targeted knowledge\ninjection and skill adaptation. Specifically, we identify key capability items\nassociated with tasks and employ a deliberately designed prompting strategy to\nenhance task performance, thereby reducing the need for extensive fine-tuning.\nAlternatively, we fine-tune the LLM using capability-specific instructions,\nfurther validating the efficacy of our framework. Experimental results confirm\nthe framework's effectiveness, demonstrating substantial improvements in both\nthe performance and applicability of LLMs.",
        "pos": [
            "Large language models (LLMs) are increasingly applied to clinical\ndecision-making. However, their potential to exhibit bias poses significant\nrisks to clinical equity. Currently, there is a lack of benchmarks that\nsystematically evaluate such clinical bias in LLMs. While in downstream tasks,\nsome biases of LLMs can be avoided such as by instructing the model to answer\n\"I'm not sure...\", the internal bias hidden within the model still lacks deep\nstudies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in\nLarge Language Models), a pioneering comprehensive benchmark to evaluate both\nintrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for\nclinical decision tasks. Notably, for intrinsic bias, we introduce a novel\nmetric, AssocMAD, to assess the disparities of LLMs across multiple demographic\ngroups. Additionally, we leverage counterfactual intervention to evaluate\nextrinsic bias in a task of clinical diagnosis prediction. Our experiments\nacross popular and medically adapted LLMs, particularly from the Mistral and\nLLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic\nbias. This work underscores the critical need to mitigate clinical bias and\nsets a new standard for future evaluations of LLMs' clinical bias."
        ],
        "neg": []
    },
    {
        "query": "The scaling of large language models (LLMs) has revolutionized their\ncapabilities in various tasks, yet this growth must be matched with efficient\ncomputational strategies. The Mixture-of-Experts (MoE) architecture stands out\nfor its ability to scale model size without significantly increasing training\ncosts. Despite their advantages, current MoE models often display parameter\ninefficiency. For instance, a pre-trained MoE-based LLM with 52 billion\nparameters might perform comparably to a standard model with 6.7 billion\nparameters. Being a crucial part of MoE, current routers in different layers\nindependently assign tokens without leveraging historical routing information,\npotentially leading to suboptimal token-expert combinations and the parameter\ninefficiency problem. To alleviate this issue, we introduce the Layerwise\nRecurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated\nRecurrent Unit (GRU) to establish dependencies between routing decisions across\nconsecutive layers. Such layerwise recurrence can be efficiently parallelly\ncomputed for input tokens and introduces negotiable costs. Our extensive\nempirical evaluations demonstrate that RMoE-based language models consistently\noutperform a spectrum of baseline models. Furthermore, RMoE integrates a novel\ncomputation stage orthogonal to existing methods, allowing seamless\ncompatibility with other MoE architectures. Our analyses attribute RMoE's gains\nto its effective cross-layer information sharing, which also improves expert\nselection and diversity. Our code is at https://github.com/qiuzh20/RMoE",
        "pos": [
            "Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.",
            "Explanation regularisation (ER) has been introduced as a way to guide models\nto make their predictions in a manner more akin to humans, i.e., making their\nattributions \"plausible\". This is achieved by introducing an auxiliary\nexplanation loss, that measures how well the output of an input attribution\ntechnique for the model agrees with relevant human-annotated rationales. One\npositive outcome of using ER appears to be improved performance in\nout-of-domain (OOD) settings, presumably due to an increased reliance on\n\"plausible\" tokens. However, previous work has under-explored the impact of the\nER objective on model attributions, in particular when obtained with techniques\nother than the one used to train ER. In this work, we contribute a study of\nER's effectiveness at informing classification decisions on plausible tokens,\nand the relationship between increased plausibility and robustness to OOD\nconditions. Through a series of analyses, we find that the connection between\nER and the ability of a classifier to rely on plausible features has been\noverstated and that a stronger reliance on plausible tokens does not seem to be\nthe cause for any perceived OOD improvements.",
            "Models need appropriate inductive biases to effectively learn from small\namounts of data and generalize systematically outside of the training\ndistribution. While Transformers are highly versatile and powerful, they can\nstill benefit from enhanced structural inductive biases for seq2seq tasks,\nespecially those involving syntactic transformations, such as converting active\nto passive voice or semantic parsing. In this paper, we propose to strengthen\nthe structural inductive bias of a Transformer by intermediate pre-training to\nperform synthetically generated syntactic transformations of dependency trees\ngiven a description of the transformation. Our experiments confirm that this\nhelps with few-shot learning of syntactic tasks such as chunking, and also\nimproves structural generalization for semantic parsing. Our analysis shows\nthat the intermediate pre-training leads to attention heads that keep track of\nwhich syntactic transformation needs to be applied to which token, and that the\nmodel can leverage these attention heads on downstream tasks."
        ],
        "neg": []
    },
    {
        "query": "Classical knowledge graph completion (KGC) methods rely solely on structural\ninformation, struggling with the inherent sparsity of knowledge graphs (KGs).\nLarge Language Models (LLMs) learn extensive knowledge from large corpora with\npowerful context modeling, which is ideal for mitigating the limitations of\nprevious methods. Directly fine-tuning LLMs offers great capability but comes\nat the cost of huge time and memory consumption, while utilizing frozen LLMs\nyields suboptimal results. In this work, we aim to leverage LLMs for KGC\neffectively and efficiently. We capture the context-aware hidden states of\nknowledge triples by employing prompts to stimulate the intermediate layers of\nLLMs. We then train a data-efficient classifier on these hidden states to\nharness the inherent capabilities of frozen LLMs in KGC. We also generate\nentity descriptions with subgraph sampling on KGs, reducing the ambiguity of\ntriplets and enriching the knowledge representation. Extensive experiments on\nstandard benchmarks showcase the efficiency and effectiveness of our approach.\nWe outperform classical KGC methods on most datasets and match the performance\nof fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU\nmemory efficiency by \\textbf{$188\\times$} and speed up training+inference by\n\\textbf{$13.48\\times$}.",
        "pos": [
            "The explosive growth of data fuels data-driven research, facilitating\nprogress across diverse domains. The FAIR principles emerge as a guiding\nstandard, aiming to enhance the findability, accessibility, interoperability,\nand reusability of data. However, current efforts primarily focus on manual\ndata FAIRification, which can only handle targeted data and lack efficiency. To\naddress this issue, we propose AutoFAIR, an architecture designed to enhance\ndata FAIRness automately. Firstly, We align each data and metadata operation\nwith specific FAIR indicators to guide machine-executable actions. Then, We\nutilize Web Reader to automatically extract metadata based on language models,\neven in the absence of structured data webpage schemas. Subsequently, FAIR\nAlignment is employed to make metadata comply with FAIR principles by ontology\nguidance and semantic matching. Finally, by applying AutoFAIR to various data,\nespecially in the field of mountain hazards, we observe significant\nimprovements in findability, accessibility, interoperability, and reusability\nof data. The FAIRness scores before and after applying AutoFAIR indicate\nenhanced data value."
        ],
        "neg": []
    },
    {
        "query": "Link prediction models can benefit from incorporating textual descriptions of\nentities and relations, enabling fully inductive learning and flexibility in\ndynamic graphs. We address the challenge of also capturing rich structured\ninformation about the local neighbourhood of entities and their relations, by\nintroducing a Transformer-based approach that effectively integrates textual\ndescriptions with graph structure, reducing the reliance on resource-intensive\ntext encoders. Our experiments on three challenging datasets show that our\nFast-and-Frugal Text-Graph (FnF-TG) Transformers achieve superior performance\ncompared to the previous state-of-the-art methods, while maintaining efficiency\nand scalability.",
        "pos": [
            "The ever-growing volume of biomedical publications creates a critical need\nfor efficient knowledge discovery. In this context, we introduce an open-source\nend-to-end framework designed to construct knowledge around specific diseases\ndirectly from raw text. To facilitate research in disease-related knowledge\ndiscovery, we create two annotated datasets focused on Rett syndrome and\nAlzheimer's disease, enabling the identification of semantic relations between\nbiomedical entities. Extensive benchmarking explores various ways to represent\nrelations and entity representations, offering insights into optimal modeling\nstrategies for semantic relation detection and highlighting language models'\ncompetence in knowledge discovery. We also conduct probing experiments using\ndifferent layer representations and attention scores to explore transformers'\nability to capture semantic relations."
        ],
        "neg": []
    },
    {
        "query": "Link prediction models can benefit from incorporating textual descriptions of\nentities and relations, enabling fully inductive learning and flexibility in\ndynamic graphs. We address the challenge of also capturing rich structured\ninformation about the local neighbourhood of entities and their relations, by\nintroducing a Transformer-based approach that effectively integrates textual\ndescriptions with graph structure, reducing the reliance on resource-intensive\ntext encoders. Our experiments on three challenging datasets show that our\nFast-and-Frugal Text-Graph (FnF-TG) Transformers achieve superior performance\ncompared to the previous state-of-the-art methods, while maintaining efficiency\nand scalability.",
        "pos": [
            "The ever-growing volume of biomedical publications creates a critical need\nfor efficient knowledge discovery. In this context, we introduce an open-source\nend-to-end framework designed to construct knowledge around specific diseases\ndirectly from raw text. To facilitate research in disease-related knowledge\ndiscovery, we create two annotated datasets focused on Rett syndrome and\nAlzheimer's disease, enabling the identification of semantic relations between\nbiomedical entities. Extensive benchmarking explores various ways to represent\nrelations and entity representations, offering insights into optimal modeling\nstrategies for semantic relation detection and highlighting language models'\ncompetence in knowledge discovery. We also conduct probing experiments using\ndifferent layer representations and attention scores to explore transformers'\nability to capture semantic relations."
        ],
        "neg": []
    },
    {
        "query": "Link prediction models can benefit from incorporating textual descriptions of\nentities and relations, enabling fully inductive learning and flexibility in\ndynamic graphs. We address the challenge of also capturing rich structured\ninformation about the local neighbourhood of entities and their relations, by\nintroducing a Transformer-based approach that effectively integrates textual\ndescriptions with graph structure, reducing the reliance on resource-intensive\ntext encoders. Our experiments on three challenging datasets show that our\nFast-and-Frugal Text-Graph (FnF-TG) Transformers achieve superior performance\ncompared to the previous state-of-the-art methods, while maintaining efficiency\nand scalability.",
        "pos": [
            "The ever-growing volume of biomedical publications creates a critical need\nfor efficient knowledge discovery. In this context, we introduce an open-source\nend-to-end framework designed to construct knowledge around specific diseases\ndirectly from raw text. To facilitate research in disease-related knowledge\ndiscovery, we create two annotated datasets focused on Rett syndrome and\nAlzheimer's disease, enabling the identification of semantic relations between\nbiomedical entities. Extensive benchmarking explores various ways to represent\nrelations and entity representations, offering insights into optimal modeling\nstrategies for semantic relation detection and highlighting language models'\ncompetence in knowledge discovery. We also conduct probing experiments using\ndifferent layer representations and attention scores to explore transformers'\nability to capture semantic relations."
        ],
        "neg": []
    },
    {
        "query": "Speech rate has been shown to vary across social categories such as gender,\nage, and dialect, while also being conditioned by properties of speech\nplanning. The effect of utterance length, where speech rate is faster and less\nvariable for longer utterances, has also been shown to reduce the role of\nsocial factors once it has been accounted for, leaving unclear the relationship\nbetween social factors and speech production in conditioning speech rate.\nThrough modelling of speech rate across 13 English speech corpora, it is found\nthat utterance length has the largest effect on speech rate, though this effect\nitself varies little across corpora and speakers. While age and gender also\nmodulate speech rate, their effects are much smaller in magnitude. These\nfindings suggest utterance length effects may be conditioned by articulatory\nand perceptual constraints, and that social influences on speech rate should be\ninterpreted in the broader context of how speech rate variation is structured.",
        "pos": [
            "Phonetic production bias is the external force most commonly invoked in\ncomputational models of sound change, despite the fact that it is not\nresponsible for all, or even most, sound changes. Furthermore, the existence of\nproduction bias alone cannot account for how changes do or do not propagate\nthroughout a speech community. While many other factors have been invoked by\n(socio)phoneticians, including but not limited to contact (between\nsubpopulations) and differences in social evaluation (of variants, groups, or\nindividuals), these are not typically modeled in computational simulations of\nsound change. In this paper, we consider whether production biases have a\nunique dynamics in terms of how they impact the population-level spread of\nchange in a setting where agents learn from multiple teachers. We show that,\nwhile the dynamics conditioned by production bias are not unique, it is not the\ncase that all perturbing forces have the same dynamics: in particular, if\nsocial weight is a function of individual teachers and the correlation between\na teacher's social weight and the extent to which they realize a production\nbias is weak, change is unlikely to propagate. Nevertheless, it remains the\ncase that changes initiated from different sources may display a similar\ndynamics. A more nuanced understanding of how population structure interacts\nwith individual biases can thus provide a (partial) solution to the\n`non-phonologization problem'."
        ],
        "neg": []
    },
    {
        "query": "Visual Dialog (VD) is a task where an agent answers a series of image-related\nquestions based on a multi-round dialog history. However, previous VD methods\noften treat the entire dialog history as a simple text input, disregarding the\ninherent conversational information flows at the round level. In this paper, we\nintroduce Multi-round Dialogue State Tracking model (MDST), a framework that\naddresses this limitation by leveraging the dialogue state learned from dialog\nhistory to answer questions. MDST captures each round of dialog history,\nconstructing internal dialogue state representations defined as 2-tuples of\nvision-language representations. These representations effectively ground the\ncurrent question, enabling the generation of accurate answers. Experimental\nresults on the VisDial v1.0 dataset demonstrate that MDST achieves a new\nstate-of-the-art performance in generative setting. Furthermore, through a\nseries of human studies, we validate the effectiveness of MDST in generating\nlong, consistent, and human-like answers while consistently answering a series\nof questions correctly.",
        "pos": [
            "This study explores the potential of Large Language Models (LLMs),\nspecifically GPT-4, to enhance objectivity in organizational task performance\nevaluations. Through comparative analyses across two studies, including various\ntask performance outputs, we demonstrate that LLMs can serve as a reliable and\neven superior alternative to human raters in evaluating knowledge-based\nperformance outputs, which are a key contribution of knowledge workers. Our\nresults suggest that GPT ratings are comparable to human ratings but exhibit\nhigher consistency and reliability. Additionally, combined multiple GPT ratings\non the same performance output show strong correlations with aggregated human\nperformance ratings, akin to the consensus principle observed in performance\nevaluation literature. However, we also find that LLMs are prone to contextual\nbiases, such as the halo effect, mirroring human evaluative biases. Our\nresearch suggests that while LLMs are capable of extracting meaningful\nconstructs from text-based data, their scope is currently limited to specific\nforms of performance evaluation. By highlighting both the potential and\nlimitations of LLMs, our study contributes to the discourse on AI role in\nmanagement studies and sets a foundation for future research to refine AI\ntheoretical and practical applications in management."
        ],
        "neg": []
    },
    {
        "query": "Image editing is an iterative process that requires precise visual evaluation\nand manipulation for the output to match the editing intent. However, current\nimage editing tools do not provide accessible interaction nor sufficient\nfeedback for blind and low vision individuals to achieve this level of control.\nTo address this, we developed EditScribe, a prototype system that makes image\nediting accessible using natural language verification loops powered by large\nmultimodal models. Using EditScribe, the user first comprehends the image\ncontent through initial general and object descriptions, then specifies edit\nactions using open-ended natural language prompts. EditScribe performs the\nimage edit, and provides four types of verification feedback for the user to\nverify the performed edit, including a summary of visual changes, AI judgement,\nand updated general and object descriptions. The user can ask follow-up\nquestions to clarify and probe into the edits or verification feedback, before\nperforming another edit. In a study with ten blind or low-vision users, we\nfound that EditScribe supported participants to perform and verify image edit\nactions non-visually. We observed different prompting strategies from\nparticipants, and their perceptions on the various types of verification\nfeedback. Finally, we discuss the implications of leveraging natural language\nverification loops to make visual authoring non-visually accessible.",
        "pos": [
            "Automated live visual descriptions can aid blind people in understanding\ntheir surroundings with autonomy and independence. However, providing\ndescriptions that are rich, contextual, and just-in-time has been a\nlong-standing challenge in accessibility. In this work, we develop WorldScribe,\na system that generates automated live real-world visual descriptions that are\ncustomizable and adaptive to users' contexts: (i) WorldScribe's descriptions\nare tailored to users' intents and prioritized based on semantic relevance.\n(ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively\nsuccinct descriptions for dynamic scenes, while presenting longer and detailed\nones for stable settings. (iii) WorldScribe is adaptive to sound contexts,\ne.g., increasing volume in noisy environments, or pausing when conversations\nstart. Powered by a suite of vision, language, and sound recognition models,\nWorldScribe introduces a description generation pipeline that balances the\ntradeoffs between their richness and latency to support real-time use. The\ndesign of WorldScribe is informed by prior work on providing visual\ndescriptions and a formative study with blind participants. Our user study and\nsubsequent pipeline evaluation show that WorldScribe can provide real-time and\nfairly accurate visual descriptions to facilitate environment understanding\nthat is adaptive and customized to users' contexts. Finally, we discuss the\nimplications and further steps toward making live visual descriptions more\ncontext-aware and humanized."
        ],
        "neg": []
    },
    {
        "query": "Image editing is an iterative process that requires precise visual evaluation\nand manipulation for the output to match the editing intent. However, current\nimage editing tools do not provide accessible interaction nor sufficient\nfeedback for blind and low vision individuals to achieve this level of control.\nTo address this, we developed EditScribe, a prototype system that makes image\nediting accessible using natural language verification loops powered by large\nmultimodal models. Using EditScribe, the user first comprehends the image\ncontent through initial general and object descriptions, then specifies edit\nactions using open-ended natural language prompts. EditScribe performs the\nimage edit, and provides four types of verification feedback for the user to\nverify the performed edit, including a summary of visual changes, AI judgement,\nand updated general and object descriptions. The user can ask follow-up\nquestions to clarify and probe into the edits or verification feedback, before\nperforming another edit. In a study with ten blind or low-vision users, we\nfound that EditScribe supported participants to perform and verify image edit\nactions non-visually. We observed different prompting strategies from\nparticipants, and their perceptions on the various types of verification\nfeedback. Finally, we discuss the implications of leveraging natural language\nverification loops to make visual authoring non-visually accessible.",
        "pos": [
            "Automated live visual descriptions can aid blind people in understanding\ntheir surroundings with autonomy and independence. However, providing\ndescriptions that are rich, contextual, and just-in-time has been a\nlong-standing challenge in accessibility. In this work, we develop WorldScribe,\na system that generates automated live real-world visual descriptions that are\ncustomizable and adaptive to users' contexts: (i) WorldScribe's descriptions\nare tailored to users' intents and prioritized based on semantic relevance.\n(ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively\nsuccinct descriptions for dynamic scenes, while presenting longer and detailed\nones for stable settings. (iii) WorldScribe is adaptive to sound contexts,\ne.g., increasing volume in noisy environments, or pausing when conversations\nstart. Powered by a suite of vision, language, and sound recognition models,\nWorldScribe introduces a description generation pipeline that balances the\ntradeoffs between their richness and latency to support real-time use. The\ndesign of WorldScribe is informed by prior work on providing visual\ndescriptions and a formative study with blind participants. Our user study and\nsubsequent pipeline evaluation show that WorldScribe can provide real-time and\nfairly accurate visual descriptions to facilitate environment understanding\nthat is adaptive and customized to users' contexts. Finally, we discuss the\nimplications and further steps toward making live visual descriptions more\ncontext-aware and humanized."
        ],
        "neg": []
    },
    {
        "query": "Image editing is an iterative process that requires precise visual evaluation\nand manipulation for the output to match the editing intent. However, current\nimage editing tools do not provide accessible interaction nor sufficient\nfeedback for blind and low vision individuals to achieve this level of control.\nTo address this, we developed EditScribe, a prototype system that makes image\nediting accessible using natural language verification loops powered by large\nmultimodal models. Using EditScribe, the user first comprehends the image\ncontent through initial general and object descriptions, then specifies edit\nactions using open-ended natural language prompts. EditScribe performs the\nimage edit, and provides four types of verification feedback for the user to\nverify the performed edit, including a summary of visual changes, AI judgement,\nand updated general and object descriptions. The user can ask follow-up\nquestions to clarify and probe into the edits or verification feedback, before\nperforming another edit. In a study with ten blind or low-vision users, we\nfound that EditScribe supported participants to perform and verify image edit\nactions non-visually. We observed different prompting strategies from\nparticipants, and their perceptions on the various types of verification\nfeedback. Finally, we discuss the implications of leveraging natural language\nverification loops to make visual authoring non-visually accessible.",
        "pos": [
            "Automated live visual descriptions can aid blind people in understanding\ntheir surroundings with autonomy and independence. However, providing\ndescriptions that are rich, contextual, and just-in-time has been a\nlong-standing challenge in accessibility. In this work, we develop WorldScribe,\na system that generates automated live real-world visual descriptions that are\ncustomizable and adaptive to users' contexts: (i) WorldScribe's descriptions\nare tailored to users' intents and prioritized based on semantic relevance.\n(ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively\nsuccinct descriptions for dynamic scenes, while presenting longer and detailed\nones for stable settings. (iii) WorldScribe is adaptive to sound contexts,\ne.g., increasing volume in noisy environments, or pausing when conversations\nstart. Powered by a suite of vision, language, and sound recognition models,\nWorldScribe introduces a description generation pipeline that balances the\ntradeoffs between their richness and latency to support real-time use. The\ndesign of WorldScribe is informed by prior work on providing visual\ndescriptions and a formative study with blind participants. Our user study and\nsubsequent pipeline evaluation show that WorldScribe can provide real-time and\nfairly accurate visual descriptions to facilitate environment understanding\nthat is adaptive and customized to users' contexts. Finally, we discuss the\nimplications and further steps toward making live visual descriptions more\ncontext-aware and humanized."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\ntraining LLMs on human-written text entails significant risk of privacy and\ncopyright violations, which demands an efficient machine unlearning framework\nto remove knowledge of sensitive data without retraining the model from\nscratch. While Gradient Ascent (GA) is widely used for unlearning by reducing\nthe likelihood of generating unwanted information, the unboundedness of\nincreasing the cross-entropy loss causes not only unstable optimization, but\nalso catastrophic forgetting of knowledge that needs to be retained. We also\ndiscover its joint application under low-rank adaptation results in\nsignificantly suboptimal computational cost vs. generative performance\ntrade-offs. In light of this limitation, we propose two novel techniques for\nrobust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge\nloss that suppresses unwanted tokens by increasing the probability of the next\nmost likely token, thereby retaining fluency and structure in language\ngeneration. We also propose to initialize low-rank adapter weights based on\nFisher-weighted low-rank approximation, which induces faster unlearning and\nbetter knowledge retention by allowing model updates to be focused on\nparameters that are important in generating textual data we wish to remove.",
        "pos": [
            "We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
        ],
        "neg": []
    },
    {
        "query": "In recent years, there has been an increasing number of frameworks developed\nfor biomedical entity and relation extraction. This research effort aims to\naddress the accelerating growth in biomedical publications and the intricate\nnature of biomedical texts, which are written for mainly domain experts. To\nhandle these challenges, we develop a novel framework that utilizes external\nknowledge to construct a task-independent and reusable background knowledge\ngraph for biomedical entity and relation extraction. The design of our model is\ninspired by how humans learn domain-specific topics. In particular, humans\noften first acquire the most basic and common knowledge regarding a field to\nbuild the foundational knowledge and then use that as a basis for extending to\nvarious specialized topics. Our framework employs such common-knowledge-sharing\nmechanism to build a general neural-network knowledge graph that is learning\ntransferable to different domain-specific biomedical texts effectively.\nExperimental evaluations demonstrate that our model, equipped with this\ngeneralized and cross-transferable knowledge base, achieves competitive\nperformance benchmarks, including BioRelEx for binding interaction detection\nand ADE for Adverse Drug Effect identification.",
        "pos": [
            "We introduce an approach to identifying speaker names in dialogue\ntranscripts, a crucial task for enhancing content accessibility and\nsearchability in digital media archives. Despite the advancements in speech\nrecognition, the task of text-based speaker identification (SpeakerID) has\nreceived limited attention, lacking large-scale, diverse datasets for effective\nmodel training. Addressing these gaps, we present a novel, large-scale dataset\nderived from the MediaSum corpus, encompassing transcripts from a wide range of\nmedia sources. We propose novel transformer-based models tailored for\nSpeakerID, leveraging contextual cues within dialogues to accurately attribute\nspeaker names. Through extensive experiments, our best model achieves a great\nprecision of 80.3\\%, setting a new benchmark for SpeakerID. The data and code\nare publicly available here:\n\\url{https://github.com/adobe-research/speaker-identification}"
        ],
        "neg": []
    },
    {
        "query": "Multimodal Large Language Models (MLLMs) demonstrate remarkable\nimage-language capabilities, but their widespread use faces challenges in\ncost-effective training and adaptation. Existing approaches often necessitate\nexpensive language model retraining and limited adaptability. Additionally, the\ncurrent focus on zero-shot performance improvements offers insufficient\nguidance for task-specific tuning. We propose CROME, an efficient\nvision-language instruction tuning framework. It features a novel gated\ncross-modal adapter that effectively combines visual and textual\nrepresentations prior to input into a frozen LLM. This lightweight adapter,\ntrained with minimal parameters, enables efficient cross-modal understanding.\nNotably, CROME demonstrates superior zero-shot performance on standard visual\nquestion answering and instruction-following benchmarks. Moreover, it yields\nfine-tuning with exceptional parameter efficiency, competing with task-specific\nspecialist state-of-the-art methods. CROME demonstrates the potential of pre-LM\nalignment for building scalable, adaptable, and parameter-efficient multimodal\nmodels.",
        "pos": [
            "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io."
        ],
        "neg": []
    },
    {
        "query": "Multimodal Large Language Models (MLLMs) demonstrate remarkable\nimage-language capabilities, but their widespread use faces challenges in\ncost-effective training and adaptation. Existing approaches often necessitate\nexpensive language model retraining and limited adaptability. Additionally, the\ncurrent focus on zero-shot performance improvements offers insufficient\nguidance for task-specific tuning. We propose CROME, an efficient\nvision-language instruction tuning framework. It features a novel gated\ncross-modal adapter that effectively combines visual and textual\nrepresentations prior to input into a frozen LLM. This lightweight adapter,\ntrained with minimal parameters, enables efficient cross-modal understanding.\nNotably, CROME demonstrates superior zero-shot performance on standard visual\nquestion answering and instruction-following benchmarks. Moreover, it yields\nfine-tuning with exceptional parameter efficiency, competing with task-specific\nspecialist state-of-the-art methods. CROME demonstrates the potential of pre-LM\nalignment for building scalable, adaptable, and parameter-efficient multimodal\nmodels.",
        "pos": [
            "Recent advances in large language models (LLMs) have enabled autonomous\nagents with complex reasoning and task-fulfillment capabilities using a wide\nrange of tools. However, effectively identifying the most relevant tools for a\ngiven task becomes a key bottleneck as the toolset size grows, hindering\nreliable tool utilization. To address this, we introduce Re-Invoke, an\nunsupervised tool retrieval method designed to scale effectively to large\ntoolsets without training. Specifically, we first generate a diverse set of\nsynthetic queries that comprehensively cover different aspects of the query\nspace associated with each tool document during the tool indexing phase.\nSecond, we leverage LLM's query understanding capabilities to extract key\ntool-related context and underlying intents from user queries during the\ninference phase. Finally, we employ a novel multi-view similarity ranking\nstrategy based on intents to pinpoint the most relevant tools for each query.\nOur evaluation demonstrates that Re-Invoke significantly outperforms\nstate-of-the-art alternatives in both single-tool and multi-tool scenarios, all\nwithin a fully unsupervised setting. Notably, on the ToolE datasets, we achieve\na 20% relative improvement in nDCG@5 for single-tool retrieval and a 39%\nimprovement for multi-tool retrieval.",
            "Embeddings from Large Language Models (LLMs) have emerged as critical\ncomponents in various applications, particularly for information retrieval.\nWhile high-dimensional embeddings generally demonstrate superior performance as\nthey contain more salient information, their practical application is\nfrequently hindered by elevated computational latency and the associated higher\ncost. To address these challenges, we propose Matryoshka-Adaptor, a novel\ntuning framework designed for the customization of LLM embeddings.\nMatryoshka-Adaptor facilitates substantial dimensionality reduction while\nmaintaining comparable performance levels, thereby achieving a significant\nenhancement in computational efficiency and cost-effectiveness. Our framework\ndirectly modifies the embeddings from pre-trained LLMs which is designed to be\nseamlessly integrated with any LLM architecture, encompassing those accessible\nexclusively through black-box APIs. Also, it exhibits efficacy in both\nunsupervised and supervised learning settings. A rigorous evaluation conducted\nacross a diverse corpus of English, multilingual, and multimodal datasets\nconsistently reveals substantial gains with Matryoshka-Adaptor. Notably, with\nGoogle and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in\ndimensionality ranging from two- to twelve-fold without compromising\nperformance across multiple BEIR datasets.",
            "Retrieval augmented generation (RAG) combines the generative abilities of\nlarge language models (LLMs) with external knowledge sources to provide more\naccurate and up-to-date responses. Recent RAG advancements focus on improving\nretrieval outcomes through iterative LLM refinement or self-critique\ncapabilities acquired through additional instruction tuning of LLMs. In this\nwork, we introduce Speculative RAG - a framework that leverages a larger\ngeneralist LM to efficiently verify multiple RAG drafts produced in parallel by\na smaller, distilled specialist LM. Each draft is generated from a distinct\nsubset of retrieved documents, offering diverse perspectives on the evidence\nwhile reducing input token counts per draft. This approach enhances\ncomprehension of each subset and mitigates potential position bias over long\ncontext. Our method accelerates RAG by delegating drafting to the smaller\nspecialist LM, with the larger generalist LM performing a single verification\npass over the drafts. Extensive experiments demonstrate that Speculative RAG\nachieves state-of-the-art performance with reduced latency on TriviaQA,\nMuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy\nby up to 12.97% while reducing latency by 51% compared to conventional RAG\nsystems on PubHealth."
        ],
        "neg": []
    },
    {
        "query": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs.",
        "pos": [
            "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenDevin, a platform for the development of powerful and\nflexible AI agents that interact with the world in similar ways to those of a\nhuman developer: by writing code, interacting with a command line, and browsing\nthe web. We describe how the platform allows for the implementation of new\nagents, safe interaction with sandboxed environments for code execution,\ncoordination between multiple agents, and incorporation of evaluation\nbenchmarks. Based on our currently incorporated benchmarks, we perform an\nevaluation of agents over 15 challenging tasks, including software engineering\n(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released\nunder the permissive MIT license, OpenDevin is a community project spanning\nacademia and industry with more than 1.3K contributions from over 160\ncontributors and will improve going forward.",
            "Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract\nfine-grained sentiment elements from target domain sentences by leveraging the\nknowledge acquired from the source domain. Due to the absence of labeled data\nin the target domain, recent studies tend to rely on pre-trained language\nmodels to generate large amounts of synthetic data for training purposes.\nHowever, these approaches entail additional computational costs associated with\nthe generation process. Different from them, we discover a striking resemblance\nbetween table-filling methods in ASTE and two-stage Object Detection (OD) in\ncomputer vision, which inspires us to revisit the cross-domain ASTE task and\napproach it from an OD standpoint. This allows the model to benefit from the OD\nextraction paradigm and region-level alignment. Building upon this premise, we\npropose a novel method named \\textbf{T}able-\\textbf{F}illing via \\textbf{M}ean\n\\textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the\nsentence into a 2D table to detect word relations, while TFMT treats the table\nas a feature map and utilizes a region consistency to enhance the quality of\nthose generated pseudo labels. Additionally, considering the existence of the\ndomain gap, a cross-domain consistency based on Maximum Mean Discrepancy is\ndesigned to alleviate domain shift problems. Our method achieves\nstate-of-the-art performance with minimal parameters and computational costs,\nmaking it a strong baseline for cross-domain ASTE.",
            "Event Factuality Detection (EFD) task determines the factuality of textual\nevents, i.e., classifying whether an event is a fact, possibility, or\nimpossibility, which is essential for faithfully understanding and utilizing\nevent knowledge. However, due to the lack of high-quality large-scale data,\nevent factuality detection is under-explored in event understanding research,\nwhich limits the development of EFD community. To address these issues and\nprovide faithful event understanding, we introduce MAVEN-Fact, a large-scale\nand high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes\nfactuality annotations of 112,276 events, making it the largest EFD dataset.\nExtensive experiments demonstrate that MAVEN-Fact is challenging for both\nconventional fine-tuned models and large language models (LLMs). Thanks to the\ncomprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact\nalso supports some further analyses and we find that adopting event arguments\nand relations helps in event factuality detection for fine-tuned models but\ndoes not benefit LLMs. Furthermore, we preliminarily study an application case\nof event factuality detection and find it helps in mitigating event-related\nhallucination in LLMs. Our dataset and codes can be obtained from\n\\url{https://github.com/lcy2723/MAVEN-FACT}",
            "Since language models (LMs) now outperform average humans on many challenging\ntasks, it has become increasingly difficult to develop challenging,\nhigh-quality, and realistic evaluations. We address this issue by examining\nLMs' capabilities to generate code for solving real scientific research\nproblems. Incorporating input from scientists and AI researchers in 16 diverse\nnatural science sub-fields, including mathematics, physics, chemistry, biology,\nand materials science, we created a scientist-curated coding benchmark,\nSciCode. The problems in SciCode naturally factorize into multiple subproblems,\neach involving knowledge recall, reasoning, and code synthesis. In total,\nSciCode contains 338 subproblems decomposed from 80 challenging main problems.\nIt offers optional descriptions specifying useful scientific background\ninformation and scientist-annotated gold-standard solutions and test cases for\nevaluation. Claude3.5-Sonnet, the best-performing model among those tested, can\nsolve only 4.6% of the problems in the most realistic setting. We believe that\nSciCode demonstrates both contemporary LMs' progress towards becoming helpful\nscientific assistants and sheds light on the development and evaluation of\nscientific AI in the future."
        ],
        "neg": []
    },
    {
        "query": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs.",
        "pos": [
            "Event Factuality Detection (EFD) task determines the factuality of textual\nevents, i.e., classifying whether an event is a fact, possibility, or\nimpossibility, which is essential for faithfully understanding and utilizing\nevent knowledge. However, due to the lack of high-quality large-scale data,\nevent factuality detection is under-explored in event understanding research,\nwhich limits the development of EFD community. To address these issues and\nprovide faithful event understanding, we introduce MAVEN-Fact, a large-scale\nand high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes\nfactuality annotations of 112,276 events, making it the largest EFD dataset.\nExtensive experiments demonstrate that MAVEN-Fact is challenging for both\nconventional fine-tuned models and large language models (LLMs). Thanks to the\ncomprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact\nalso supports some further analyses and we find that adopting event arguments\nand relations helps in event factuality detection for fine-tuned models but\ndoes not benefit LLMs. Furthermore, we preliminarily study an application case\nof event factuality detection and find it helps in mitigating event-related\nhallucination in LLMs. Our dataset and codes can be obtained from\n\\url{https://github.com/lcy2723/MAVEN-FACT}"
        ],
        "neg": []
    },
    {
        "query": "Cyber Threat Intelligence (CTI) summarization task requires the system to\ngenerate concise and accurate highlights from raw intelligence data, which\nplays an important role in providing decision-makers with crucial information\nto quickly detect and respond to cyber threats in the cybersecurity domain.\nHowever, efficient techniques for summarizing CTI reports, including facts,\nanalytical insights, attack processes, etc., have largely been unexplored,\nprimarily due to the lack of available dataset. To this end, we present CTISum,\na new benchmark for CTI summarization task. Considering the importance of\nattack process, a novel fine-grained subtask of attack process summarization is\nproposed to enable defenders to assess risk, identify security gaps,\nvulnerabilities, and so on. Specifically, we first design a multi-stage\nannotation pipeline to gather and annotate the CTI data, and then benchmark the\nCTISum with a collection of extractive and abstractive summarization methods.\nExperimental results show that current state-of-the-art models exhibit\nlimitations when applied to CTISum, underscoring the fact that automatically\nproducing concise summaries of CTI reports remains an open research challenge.",
        "pos": [
            "Encrypted traffic classification is the task of identifying the application\nor service associated with encrypted network traffic. One effective approach\nfor this task is to use deep learning methods to encode the raw traffic bytes\ndirectly and automatically extract features for classification (byte-based\nmodels). However, current byte-based models input raw traffic bytes, whether\nplaintext or encrypted text, for automated feature extraction, neglecting the\ndistinct impacts of plaintext and encrypted text on downstream tasks.\nAdditionally, these models primarily focus on improving classification\naccuracy, with little emphasis on the efficiency of models. In this paper, for\nthe first time, we analyze the impact of plaintext and encrypted text on the\nmodel's effectiveness and efficiency. Based on our observations and findings,\nwe propose a two-phase approach to balance the trade-off between plaintext and\nencrypted text in traffic classification. Specifically, Stage one is to\nDetermine whether the Plain text is enough to be accurately Classified (DPC)\nusing the proposed DPC Selector. This stage quickly identifies samples that can\nbe classified using plaintext, leveraging explicit byte features in plaintext\nto enhance model's efficiency. Stage two aims to adaptively make a\nclassification with the result from stage one. This stage incorporates\nencrypted text information for samples that cannot be classified using\nplaintext alone, ensuring the model's effectiveness on traffic classification\ntasks. Experiments on two datasets demonstrate that our proposed model achieves\nstate-of-the-art results in both effectiveness and efficiency.",
            "The transformers have achieved significant accomplishments in the natural\nlanguage processing as its outstanding parallel processing capabilities and\nhighly flexible attention mechanism. In addition, increasing studies based on\ntransformers have been proposed to model single-cell data. In this review, we\nattempt to systematically summarize the single-cell language models and\napplications based on transformers. First, we provide a detailed introduction\nabout the structure and principles of transformers. Then, we review the\nsingle-cell language models and large language models for single-cell data\nanalysis. Moreover, we explore the datasets and applications of single-cell\nlanguage models in downstream tasks such as batch correction, cell clustering,\ncell type annotation, gene regulatory network inference and perturbation\nresponse. Further, we discuss the challenges of single-cell language models and\nprovide promising research directions. We hope this review will serve as an\nup-to-date reference for researchers interested in the direction of single-cell\nlanguage models."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown remarkable achievements across\nvarious language tasks.To enhance the performance of LLMs in scientific\nliterature services, we developed the scientific literature LLM (SciLit-LLM)\nthrough pre-training and supervised fine-tuning on scientific literature,\nbuilding upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge\nservice system Spark Research Assistant (SparkRA) based on our SciLit-LLM.\nSparkRA is accessible online and provides three primary functions: literature\ninvestigation, paper reading, and academic writing. As of July 30, 2024,\nSparkRA has garnered over 50,000 registered users, with a total usage count\nexceeding 1.3 million.",
        "pos": [
            "Recently, speech generation models have made significant progress by using\nlarge-scale training data. However, the research community struggle to produce\nhighly spontaneous and human-like speech due to the lack of large-scale,\ndiverse, and spontaneous speech data. This paper present Emilia, the first\nmultilingual speech generation dataset from in-the-wild speech data, and\nEmilia-Pipe, the first open-source preprocessing pipeline designed to transform\nin-the-wild speech data into high-quality training data with annotations for\nspeech generation. Emilia starts with over 101k hours of speech in six\nlanguages and features diverse speech with varied speaking styles. To\nfacilitate the scale-up of Emilia, the open-source pipeline Emilia-Pipe can\nprocess one hour of raw speech data ready for model training in a few mins,\nwhich enables the research community to collaborate on large-scale speech\ngeneration research. Experimental results validate the effectiveness of Emilia.\nDemos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown remarkable achievements across\nvarious language tasks.To enhance the performance of LLMs in scientific\nliterature services, we developed the scientific literature LLM (SciLit-LLM)\nthrough pre-training and supervised fine-tuning on scientific literature,\nbuilding upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge\nservice system Spark Research Assistant (SparkRA) based on our SciLit-LLM.\nSparkRA is accessible online and provides three primary functions: literature\ninvestigation, paper reading, and academic writing. As of July 30, 2024,\nSparkRA has garnered over 50,000 registered users, with a total usage count\nexceeding 1.3 million.",
        "pos": [
            "Methodological advancements to automate the generation of differential\ndiagnosis (DDx) to predict a list of potential diseases as differentials given\npatients' symptom descriptions are critical to clinical reasoning and\napplications such as decision support. However, providing reasoning or\ninterpretation for these differential diagnoses is more meaningful.\nFortunately, large language models (LLMs) possess powerful language processing\nabilities and have been proven effective in various related tasks. Motivated by\nthis potential, we investigate the use of LLMs for interpretable DDx. First, we\ndevelop a new DDx dataset with expert-derived interpretation on 570 public\nclinical notes. Second, we propose a novel framework, named Dual-Inf, that\nenables LLMs to conduct bidirectional inference for interpretation. Both human\nand automated evaluation demonstrate the effectiveness of Dual-Inf in\npredicting differentials and diagnosis explanations. Specifically, the\nperformance improvement of Dual-Inf over the baseline methods exceeds 32%\nw.r.t. BERTScore in DDx interpretation. Furthermore, experiments verify that\nDual-Inf (1) makes fewer errors in interpretation, (2) has great\ngeneralizability, (3) is promising for rare disease diagnosis and explanation.",
            "Recent advancements in open-source code large language models (LLMs) have\ndemonstrated remarkable coding abilities by fine-tuning on the data generated\nfrom powerful closed-source LLMs such as GPT-3.5 and GPT-4 for instruction\ntuning. This paper explores how to further improve an instruction-tuned code\nLLM by generating data from itself rather than querying closed-source LLMs. Our\nkey observation is the misalignment between the translation of formal and\ninformal languages: translating formal language (i.e., code) to informal\nlanguage (i.e., natural language) is more straightforward than the reverse.\nBased on this observation, we propose INVERSE-INSTRUCT, which summarizes\ninstructions from code snippets instead of the reverse. Specifically, given an\ninstruction tuning corpus for code and the resulting instruction-tuned code\nLLM, we ask the code LLM to generate additional high-quality instructions for\nthe original corpus through code summarization and self-evaluation. Then, we\nfine-tune the base LLM on the combination of the original corpus and the\nself-generated one, which yields a stronger instruction-tuned LLM. We present a\nseries of code LLMs named InverseCoder, which surpasses the performance of the\noriginal code LLMs on a wide range of benchmarks, including Python text-to-code\ngeneration, multilingual coding, and data-science code generation."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown remarkable achievements across\nvarious language tasks.To enhance the performance of LLMs in scientific\nliterature services, we developed the scientific literature LLM (SciLit-LLM)\nthrough pre-training and supervised fine-tuning on scientific literature,\nbuilding upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge\nservice system Spark Research Assistant (SparkRA) based on our SciLit-LLM.\nSparkRA is accessible online and provides three primary functions: literature\ninvestigation, paper reading, and academic writing. As of July 30, 2024,\nSparkRA has garnered over 50,000 registered users, with a total usage count\nexceeding 1.3 million.",
        "pos": [
            "This paper presents a novel and comprehensive solution to enhance both the\nrobustness and efficiency of question answering (QA) systems through supervised\ncontrastive learning (SCL). Training a high-performance QA system has become\nstraightforward with pre-trained language models, requiring only a small amount\nof data and simple fine-tuning. However, despite recent advances, existing QA\nsystems still exhibit significant deficiencies in functionality and training\nefficiency. We address the functionality issue by defining four key tasks: user\ninput intent classification, out-of-domain input detection, new intent\ndiscovery, and continual learning. We then leverage a unified SCL-based\nrepresentation learning method to efficiently build an intra-class compact and\ninter-class scattered feature space, facilitating both known intent\nclassification and unknown intent detection and discovery. Consequently, with\nminimal additional tuning on downstream tasks, our approach significantly\nimproves model efficiency and achieves new state-of-the-art performance across\nall tasks.",
            "Personalized Dialogue Generation (PDG) aims to create coherent responses\naccording to roles or personas. Traditional PDG relies on external role data,\nwhich can be scarce and raise privacy concerns. Approaches address these issues\nby extracting role information from dialogue history, which often fail to\ngenerically model roles in continuous space. To overcome these limitations, we\nintroduce a novel framework \\textbf{MO}dels \\textbf{R}oles from\n\\textbf{P}ersonalized Dialogue \\textbf{H}istory by \\textbf{E}xploring and\n\\textbf{U}tilizing Latent \\textbf{S}pace (MORPHEUS) through a three-stage\ntraining process. Specifically, we create a persona codebook to represent roles\nin latent space compactly, and this codebook is used to construct a posterior\ndistribution of role information. This method enables the model to generalize\nacross roles, allowing the generation of personalized dialogues even for unseen\nroles. Experiments on both Chinese and English datasets demonstrate that\nMORPHEUS enhances the extraction of role information, and improves response\ngeneration without external role data. Additionally, MORPHEUS can be considered\nan efficient fine-tuning for large language models."
        ],
        "neg": []
    },
    {
        "query": "Multi-modal Large Language Models (MLLMs) have advanced significantly,\noffering powerful vision-language understanding capabilities. However, these\nmodels often inherit severe social biases from their training datasets, leading\nto unfair predictions based on attributes like race and gender. This paper\naddresses the issue of social biases in MLLMs by i) Introducing a comprehensive\nCounterfactual dataset with Multiple Social Concepts (CMSC), which provides a\nmore diverse and extensive training set compared to existing datasets. ii)\nProposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by\nrevisiting the MLLM training process, rescaling the autoregressive loss\nfunction, and improving data sampling methods to counteract biases. Through\nextensive experiments on various MLLMs, our CMSC dataset and ASD method\ndemonstrate a significant reduction in social biases while maintaining the\nmodels' original performance.",
        "pos": [
            "Building a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks."
        ],
        "neg": []
    },
    {
        "query": "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency.",
        "pos": [
            "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",
            "Large language models (LLMs) have demonstrated great success in various\nfields, benefiting from their huge amount of parameters that store knowledge.\nHowever, LLMs still suffer from several key issues, such as hallucination\nproblems, knowledge update issues, and lacking domain-specific expertise. The\nappearance of retrieval-augmented generation (RAG), which leverages an external\nknowledge database to augment LLMs, makes up those drawbacks of LLMs. This\npaper reviews all significant techniques of RAG, especially in the retriever\nand the retrieval fusions. Besides, tutorial codes are provided for\nimplementing the representative techniques in RAG. This paper further discusses\nthe RAG training, including RAG with/without datastore update. Then, we\nintroduce the application of RAG in representative natural language processing\ntasks and industrial scenarios. Finally, this paper discusses the future\ndirections and challenges of RAG for promoting its development.",
            "Visually impaired people are a large group who can only use braille for\nreading and writing. However, the lack of special educational resources is the\nbottleneck for educating them. Educational equity is a reflection of the level\nof social civilization, cultural equality, and individual dignity. Facilitating\nand improving lifelong learning channels for the visually impaired is of great\nsignificance. Their written braille homework or exam papers cannot be\nunderstood by sighted teachers, because of the lack of a highly accurate\nbraille translation system, especially in Chinese which has tone marks. braille\nwriters often omit tone marks to save space, leading to confusion when braille\nwith the same consonants and vowels is translated into Chinese. Previous\nalgorithms were insufficient in extracting contextual information, resulting in\nlow accuracy of braille translations into Chinese. This project informatively\nfine-tuned the mT5 model with an Encoder-decoder architecture for braille to\nChinese character conversion. This research created a training set of braille\nand corresponding Chinese text from the Leipzig Corpora. This project\nsignificantly reduced the confusion in braille, achieving $62.4$ and $62.3$\nBLEU scores in the validation and test sets, with a curriculum learning\nfine-tuning method. By incorporating the braille recognition algorithm, this\nproject is the first publicly available braille translation system and can\nbenefit lots of visually impaired students and families who are preparing for\nthe Chinese College Test and help to propel their college dreams in the future.\nThere is a demo on our homepage\\footnote{\\url{https://vision-braille.com/}}."
        ],
        "neg": []
    },
    {
        "query": "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency.",
        "pos": [
            "The recent advancements in large language models (LLMs) with billions of\nparameters have significantly boosted their performance across various\nreal-world applications. However, the inference processes for these models\nrequire substantial energy and computational resources, presenting considerable\ndeployment challenges. In contrast, human brains, which contain approximately\n86 billion biological neurons, exhibit significantly greater energy efficiency\ncompared to LLMs with a similar number of parameters. Inspired by this, we\nredesign 7 to 70 billion parameter LLMs using bio-plausible spiking mechanisms,\nemulating the efficient behavior of the human brain. We propose the first\nspiking large language model as recent LLMs termed SpikeLLM. Coupled with the\nproposed model, a novel spike-driven quantization framework named Optimal Brain\nSpiking is introduced to reduce the energy cost and accelerate inference speed\nvia two essential approaches: first (second)-order differentiation-based\nsalient channel detection, and per-channel salient outlier expansion with\nGeneralized Integrate-and-Fire neurons. Our proposed spike-driven quantization\ncan plug in main streams of quantization training methods. In the OmniQuant\npipeline, SpikeLLM significantly reduces 25.51% WikiText2 perplexity and\nimproves 3.08% average accuracy of 6 zero-shot datasets on a LLAMA2-7B 4A4W\nmodel. In the GPTQ pipeline, SpikeLLM realizes a sparse ternary quantization,\nwhich achieves additive in all linear layers. Compared with PB-LLM with similar\noperations, SpikeLLM also exceeds significantly. We will release our code on\nGitHub."
        ],
        "neg": []
    },
    {
        "query": "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency.",
        "pos": [
            "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",
            "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
            "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems.",
            "Visually impaired people are a large group who can only use braille for\nreading and writing. However, the lack of special educational resources is the\nbottleneck for educating them. Educational equity is a reflection of the level\nof social civilization, cultural equality, and individual dignity. Facilitating\nand improving lifelong learning channels for the visually impaired is of great\nsignificance. Their written braille homework or exam papers cannot be\nunderstood by sighted teachers, because of the lack of a highly accurate\nbraille translation system, especially in Chinese which has tone marks. braille\nwriters often omit tone marks to save space, leading to confusion when braille\nwith the same consonants and vowels is translated into Chinese. Previous\nalgorithms were insufficient in extracting contextual information, resulting in\nlow accuracy of braille translations into Chinese. This project informatively\nfine-tuned the mT5 model with an Encoder-decoder architecture for braille to\nChinese character conversion. This research created a training set of braille\nand corresponding Chinese text from the Leipzig Corpora. This project\nsignificantly reduced the confusion in braille, achieving $62.4$ and $62.3$\nBLEU scores in the validation and test sets, with a curriculum learning\nfine-tuning method. By incorporating the braille recognition algorithm, this\nproject is the first publicly available braille translation system and can\nbenefit lots of visually impaired students and families who are preparing for\nthe Chinese College Test and help to propel their college dreams in the future.\nThere is a demo on our homepage\\footnote{\\url{https://vision-braille.com/}}."
        ],
        "neg": []
    },
    {
        "query": "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency.",
        "pos": [
            "As the academic landscape expands, the challenge of efficiently identifying\npotentially high-impact articles among the vast number of newly published works\nbecomes critical. This paper introduces a promising approach, leveraging the\ncapabilities of fine-tuned LLMs to predict the future impact of newborn\narticles solely based on titles and abstracts. Moving beyond traditional\nmethods heavily reliant on external information, the proposed method discerns\nthe shared semantic features of highly impactful papers from a large collection\nof title-abstract and potential impact pairs. These semantic features are\nfurther utilized to regress an improved metric, TNCSI_SP, which has been\nendowed with value, field, and time normalization properties. Additionally, a\ncomprehensive dataset has been constructed and released for fine-tuning the\nLLM, containing over 12,000 entries with corresponding titles, abstracts, and\nTNCSI_SP. The quantitative results, with an NDCG@20 of 0.901, demonstrate that\nthe proposed approach achieves state-of-the-art performance in predicting the\nimpact of newborn articles when compared to competitive counterparts. Finally,\nwe demonstrate a real-world application for predicting the impact of newborn\njournal articles to demonstrate its noteworthy practical value. Overall, our\nfindings challenge existing paradigms and propose a shift towards a more\ncontent-focused prediction of academic impact, offering new insights for\nassessing newborn article impact.",
            "The capability gap between open-source and closed-source large language\nmodels (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we\nintroduce a synthetic data approach that combines data produced by larger, more\npowerful models (strong models) with error information data generated by\nsmaller, not well-aligned models (weak models). The method not only enhances\nthe domain generalization of text-to-SQL models but also explores the potential\nof error data supervision through preference learning. Furthermore, we employ\nthe synthetic data approach for instruction tuning on open-source LLMs,\nresulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is\ndemonstrated through state-of-the-art results on the SPIDER and BIRD\nbenchmarks, bridging the performance gap between open-source models and methods\nprompted by closed-source models.",
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
            "Instruction tuning as an effective technique aligns the outputs of large\nlanguage models (LLMs) with human preference. But how to generate the seasonal\nmulti-turn dialogues from raw documents for instruction tuning still requires\nfurther exploration. In this paper, we present a novel framework named R2S that\nleverages the CoD-Chain of Dialogue logic to guide large language models (LLMs)\nin generating knowledge-intensive multi-turn dialogues for instruction tuning.\nBy integrating raw documents from both open-source datasets and domain-specific\nweb-crawled documents into a benchmark K-BENCH, we cover diverse areas such as\nWikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach\nfirst decides the logic flow of the current dialogue and then prompts LLMs to\nproduce key phrases for sourcing relevant response content. This methodology\nenables the creation of the G I NSTRUCT instruction dataset, retaining raw\ndocument knowledge within dialoguestyle interactions. Utilizing this dataset,\nwe fine-tune GLLM, a model designed to transform raw documents into structured\nmulti-turn dialogues, thereby injecting comprehensive domain knowledge into the\nSFT model for enhanced instruction tuning. This work signifies a stride towards\nrefining the adaptability and effectiveness of LLMs in processing and\ngenerating more accurate, contextually nuanced responses across various fields."
        ],
        "neg": []
    },
    {
        "query": "Recent research in neural machine translation (NMT) has shown that training\non high-quality machine-generated data can outperform training on\nhuman-generated data. This work accompanies the first-ever release of a\nLLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and\nmulti-sentence examples. We perform extensive experiments to demonstrate the\nquality of our dataset in terms of its downstream impact on NMT model\nperformance. We find that training from scratch on our (machine-generated)\ndataset outperforms training on the (web-crawled) WMT'23 training dataset\n(which is 300 times larger), and also outperforms training on the top-quality\nsubset of the WMT'23 training dataset. We also find that performing\nself-distillation by finetuning the LLM which generated this dataset\noutperforms the LLM's strong few-shot baseline. These findings corroborate the\nquality of our dataset, and demonstrate the value of high-quality\nmachine-generated data in improving performance of NMT models.",
        "pos": [
            "This is the preliminary ranking of WMT24 General MT systems based on\nautomatic metrics. The official ranking will be a human evaluation, which is\nsuperior to the automatic ranking and supersedes it. The purpose of this report\nis not to interpret any findings but only provide preliminary results to the\nparticipants of the General MT task that may be useful during the writing of\nthe system submission."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, especially in Motivational Interviewing\n(MI). However, how to employ strategies, a set of motivational interviewing\n(MI) skills, to generate therapeutic-adherent conversations with explainability\nis underexplored. We propose an approach called strategy-aware dialogue\ngeneration with Chain-of-Strategy (CoS) planning, which first predicts MI\nstrategies as reasoning and utilizes these strategies to guide the subsequent\ndialogue generation. It brings the potential for controllable and explainable\ngeneration in psychotherapy by aligning the generated MI dialogues with\ntherapeutic strategies. Extensive experiments including automatic and human\nevaluations are conducted to validate the effectiveness of the MI strategy. Our\nfindings demonstrate the potential of LLMs in producing strategically aligned\ndialogues and suggest directions for practical applications in\npsychotherapeutic settings.",
        "pos": [
            "Large language models (LLMs) exhibit remarkable in-context learning (ICL)\ncapabilities. However, the underlying working mechanism of ICL remains poorly\nunderstood. Recent research presents two conflicting views on ICL: One\nattributes it to LLMs' inherent ability of task recognition, deeming label\ncorrectness and shot numbers of demonstrations as not crucial; the other\nemphasizes the impact of similar examples in the demonstrations, stressing the\nneed for label correctness and more shots. In this work, we provide a\nTwo-Dimensional Coordinate System that unifies both views into a systematic\nframework. The framework explains the behavior of ICL through two orthogonal\nvariables: whether LLMs can recognize the task and whether similar examples are\npresented in the demonstrations. We propose the peak inverse rank metric to\ndetect the task recognition ability of LLMs and study LLMs' reactions to\ndifferent definitions of similarity. Based on these, we conduct extensive\nexperiments to elucidate how ICL functions across each quadrant on multiple\nrepresentative classification tasks. Finally, we extend our analyses to\ngeneration tasks, showing that our coordinate system can also be used to\ninterpret ICL for generation tasks effectively.",
            "The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nTheoretical justifications and experiment results on five datasets show the\nvalidity and effectiveness of our proposed hybrid framework for training a\nhigh-quality reward model. By decoupling the reward modeling procedure and\nincorporating hybrid supervision, our HaF-RM framework offers a principled and\neffective approach to enhancing the performance and alignment of reward models,\na critical component in the responsible development of powerful language\nmodels. We release our code at https://haf-rm.github.io."
        ],
        "neg": []
    },
    {
        "query": "Despite their wide adoption, the biases and unintended behaviors of language\nmodels remain poorly understood. In this paper, we identify and characterize a\nphenomenon never discussed before, which we call semantic leakage, where models\nleak irrelevant information from the prompt into the generation in unexpected\nways. We propose an evaluation setting to detect semantic leakage both by\nhumans and automatically, curate a diverse test suite for diagnosing this\nbehavior, and measure significant semantic leakage in 13 flagship models. We\nalso show that models exhibit semantic leakage in languages besides English and\nacross different settings and generation scenarios. This discovery highlights\nyet another type of bias in language models that affects their generation\npatterns and behavior.",
        "pos": [
            "In multilingual settings, non-Latin scripts and low-resource languages are\nusually disadvantaged in terms of language models' utility, efficiency, and\ncost. Specifically, previous studies have reported multiple modeling biases\nthat the current tokenization algorithms introduce to non-Latin script\nlanguages, the main one being over-segmentation. In this work, we propose\nMAGNET; multilingual adaptive gradient-based tokenization to reduce\nover-segmentation via adaptive gradient-based subword tokenization. MAGNET\nlearns to predict segment boundaries between byte tokens in a sequence via\nsub-modules within the model, which act as internal boundary predictors\n(tokenizers). Previous gradient-based tokenization methods aimed for uniform\ncompression across sequences by integrating a single boundary predictor during\ntraining and optimizing it end-to-end through stochastic reparameterization\nalongside the next token prediction objective. However, this approach still\nresults in over-segmentation for non-Latin script languages in multilingual\nsettings. In contrast, MAGNET offers a customizable architecture where\nbyte-level sequences are routed through language-script-specific predictors,\neach optimized for its respective language script. This modularity enforces\nequitable segmentation granularity across different language scripts compared\nto previous methods. Through extensive experiments, we demonstrate that in\naddition to reducing segmentation disparities, MAGNET also enables faster\nlanguage modelling and improves downstream utility."
        ],
        "neg": []
    },
    {
        "query": "Despite their wide adoption, the biases and unintended behaviors of language\nmodels remain poorly understood. In this paper, we identify and characterize a\nphenomenon never discussed before, which we call semantic leakage, where models\nleak irrelevant information from the prompt into the generation in unexpected\nways. We propose an evaluation setting to detect semantic leakage both by\nhumans and automatically, curate a diverse test suite for diagnosing this\nbehavior, and measure significant semantic leakage in 13 flagship models. We\nalso show that models exhibit semantic leakage in languages besides English and\nacross different settings and generation scenarios. This discovery highlights\nyet another type of bias in language models that affects their generation\npatterns and behavior.",
        "pos": [
            "The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs."
        ],
        "neg": []
    },
    {
        "query": "Despite their wide adoption, the biases and unintended behaviors of language\nmodels remain poorly understood. In this paper, we identify and characterize a\nphenomenon never discussed before, which we call semantic leakage, where models\nleak irrelevant information from the prompt into the generation in unexpected\nways. We propose an evaluation setting to detect semantic leakage both by\nhumans and automatically, curate a diverse test suite for diagnosing this\nbehavior, and measure significant semantic leakage in 13 flagship models. We\nalso show that models exhibit semantic leakage in languages besides English and\nacross different settings and generation scenarios. This discovery highlights\nyet another type of bias in language models that affects their generation\npatterns and behavior.",
        "pos": [
            "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.",
            "Scaling laws with respect to the amount of training data and the number of\nparameters allow us to predict the cost-benefit trade-offs of pretraining\nlanguage models (LMs) in different configurations. In this paper, we consider\nanother dimension of scaling: the amount of data available at inference time.\nSpecifically, we find that increasing the size of the datastore used by a\nretrieval-based LM monotonically improves language modeling and several\ndownstream tasks without obvious saturation, such that a smaller model\naugmented with a large datastore outperforms a larger LM-only model on\nknowledge-intensive tasks. By plotting compute-optimal scaling curves with\nvaried datastore, model, and pretraining data sizes, we show that using larger\ndatastores can significantly improve model performance for the same training\ncompute budget. We carry out our study by constructing a 1.4 trillion-token\ndatastore named MassiveDS, which is the largest and the most diverse\nopen-sourced datastore for retrieval-based LMs to date, and designing an\nefficient pipeline for studying datastore scaling in a computationally\naccessible manner. Finally, we analyze the effect of improving the retriever,\ndatastore quality filtering, and other design choices on our observed scaling\ntrends. Overall, our results show that datastore size should be considered as\nan integral part of LM efficiency and performance trade-offs. To facilitate\nfuture research, we open-source our datastore and code at\nhttps://github.com/RulinShao/retrieval-scaling."
        ],
        "neg": []
    },
    {
        "query": "Despite their wide adoption, the biases and unintended behaviors of language\nmodels remain poorly understood. In this paper, we identify and characterize a\nphenomenon never discussed before, which we call semantic leakage, where models\nleak irrelevant information from the prompt into the generation in unexpected\nways. We propose an evaluation setting to detect semantic leakage both by\nhumans and automatically, curate a diverse test suite for diagnosing this\nbehavior, and measure significant semantic leakage in 13 flagship models. We\nalso show that models exhibit semantic leakage in languages besides English and\nacross different settings and generation scenarios. This discovery highlights\nyet another type of bias in language models that affects their generation\npatterns and behavior.",
        "pos": [
            "The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.",
            "In multilingual settings, non-Latin scripts and low-resource languages are\nusually disadvantaged in terms of language models' utility, efficiency, and\ncost. Specifically, previous studies have reported multiple modeling biases\nthat the current tokenization algorithms introduce to non-Latin script\nlanguages, the main one being over-segmentation. In this work, we propose\nMAGNET; multilingual adaptive gradient-based tokenization to reduce\nover-segmentation via adaptive gradient-based subword tokenization. MAGNET\nlearns to predict segment boundaries between byte tokens in a sequence via\nsub-modules within the model, which act as internal boundary predictors\n(tokenizers). Previous gradient-based tokenization methods aimed for uniform\ncompression across sequences by integrating a single boundary predictor during\ntraining and optimizing it end-to-end through stochastic reparameterization\nalongside the next token prediction objective. However, this approach still\nresults in over-segmentation for non-Latin script languages in multilingual\nsettings. In contrast, MAGNET offers a customizable architecture where\nbyte-level sequences are routed through language-script-specific predictors,\neach optimized for its respective language script. This modularity enforces\nequitable segmentation granularity across different language scripts compared\nto previous methods. Through extensive experiments, we demonstrate that in\naddition to reducing segmentation disparities, MAGNET also enables faster\nlanguage modelling and improves downstream utility.",
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "Cross-lingual conversational speech summarization is an important problem,\nbut suffers from a dearth of resources. While transcriptions exist for a number\nof languages, translated conversational speech is rare and datasets containing\nsummaries are non-existent. We build upon the existing Fisher and Callhome\nSpanish-English Speech Translation corpus by supplementing the translations\nwith summaries. The summaries are generated using GPT-4 from the reference\ntranslations and are treated as ground truth. The task is to generate similar\nsummaries in the presence of transcription and translation errors. We build a\nbaseline cascade-based system using open-source speech recognition and machine\ntranslation models. We test a range of LLMs for summarization and analyze the\nimpact of transcription and translation errors. Adapting the Mistral-7B model\nfor this task performs significantly better than off-the-shelf models and\nmatches the performance of GPT-4.",
        "pos": [
            "Transcribing the speech of multiple overlapping speakers typically requires\nseparating the audio into multiple streams and recognizing each one\nindependently. More recent work jointly separates and transcribes, but requires\na separate decoding component for each speaker. We propose the TOGGL model to\nsimultaneously transcribe the speech of multiple speakers. The TOGGL model uses\nspecial output tokens to attribute the speech to each speaker with only a\nsingle decoder. Our approach generalizes beyond two speakers, even when trained\nonly on two-speaker data. We demonstrate superior performance compared to\ncompeting approaches on a conversational speech dataset. Our approach also\nimproves performance on single-speaker audio."
        ],
        "neg": []
    },
    {
        "query": "Cross-lingual conversational speech summarization is an important problem,\nbut suffers from a dearth of resources. While transcriptions exist for a number\nof languages, translated conversational speech is rare and datasets containing\nsummaries are non-existent. We build upon the existing Fisher and Callhome\nSpanish-English Speech Translation corpus by supplementing the translations\nwith summaries. The summaries are generated using GPT-4 from the reference\ntranslations and are treated as ground truth. The task is to generate similar\nsummaries in the presence of transcription and translation errors. We build a\nbaseline cascade-based system using open-source speech recognition and machine\ntranslation models. We test a range of LLMs for summarization and analyze the\nimpact of transcription and translation errors. Adapting the Mistral-7B model\nfor this task performs significantly better than off-the-shelf models and\nmatches the performance of GPT-4.",
        "pos": [
            "Transcribing the speech of multiple overlapping speakers typically requires\nseparating the audio into multiple streams and recognizing each one\nindependently. More recent work jointly separates and transcribes, but requires\na separate decoding component for each speaker. We propose the TOGGL model to\nsimultaneously transcribe the speech of multiple speakers. The TOGGL model uses\nspecial output tokens to attribute the speech to each speaker with only a\nsingle decoder. Our approach generalizes beyond two speakers, even when trained\nonly on two-speaker data. We demonstrate superior performance compared to\ncompeting approaches on a conversational speech dataset. Our approach also\nimproves performance on single-speaker audio."
        ],
        "neg": []
    },
    {
        "query": "We introduce Differential Performance Evaluation (DPE), a framework designed\nto reliably evaluate Large Language Models (LLMs) for efficient code\ngeneration. Traditional coding benchmarks often fail to provide reliable\ninsights into code efficiency, due to their reliance on simplistic test inputs\nand the absence of effective compound metrics. DPE addresses these issues by\nfocusing on efficiency-demanding programming tasks and establishing an\ninsightful compound metric for performance evaluation. DPE operates in two\nphases: To curate efficiency datasets, it selects efficiency-demanding tasks\nfrom existing coding benchmarks and generates computationally expensive inputs\nto stress the efficiency of LLM solutions. To assess the code efficiency, DPE\nprofiles the new solution and compares it globally against a set of reference\nsolutions that exhibit distinct efficiency levels, where the matched level\ndefines its efficiency score. As a proof of concept, we use DPE to create\nEvalPerf, a benchmark with 121 performance-challenging coding tasks. Our\ncomprehensive evaluation draws interesting findings on the efficiency impact of\nmodel sizes, instruction tuning, and prompting. For example, while the scaling\nlaw fails to account for code efficiency, general instruction tuning benefits\nboth code correctness and efficiency. We also evaluate the evaluation by\nexamining the effectiveness of DPE, showing that EvalPerf is reliable and\nconvenient to use even across platforms.",
        "pos": [
            "This paper tackles a key issue in the interpretation of scientific figures:\nthe fine-grained alignment of text and figures. It advances beyond prior\nresearch that primarily dealt with straightforward, data-driven visualizations\nsuch as bar and pie charts and only offered a basic understanding of diagrams\nthrough captioning and classification. We introduce a novel task, Figure\nIntegrity Verification, designed to evaluate the precision of technologies in\naligning textual knowledge with visual elements in scientific figures. To\nsupport this, we develop a semi-automated method for constructing a large-scale\ndataset, Figure-seg, specifically designed for this task. Additionally, we\npropose an innovative framework, Every Part Matters (EPM), which leverages\nMultimodal Large Language Models (MLLMs) to not only incrementally improve the\nalignment and verification of text-figure integrity but also enhance integrity\nthrough analogical reasoning. Our comprehensive experiments show that these\ninnovations substantially improve upon existing methods, allowing for more\nprecise and thorough analysis of complex scientific figures. This progress not\nonly enhances our understanding of multimodal technologies but also stimulates\nfurther research and practical applications across fields requiring the\naccurate interpretation of complex visual data.",
            "Retrieval-Augmented Generation (RAG) is applied to solve hallucination\nproblems and real-time constraints of large language models, but it also\ninduces vulnerabilities against retrieval corruption attacks. Existing research\nmainly explores the unreliability of RAG in white-box and closed-domain QA\ntasks. In this paper, we aim to reveal the vulnerabilities of\nRetrieval-Enhanced Generative (RAG) models when faced with black-box attacks\nfor opinion manipulation. We explore the impact of such attacks on user\ncognition and decision-making, providing new insight to enhance the reliability\nand security of RAG models. We manipulate the ranking results of the retrieval\nmodel in RAG with instruction and use these results as data to train a\nsurrogate model. By employing adversarial retrieval attack methods to the\nsurrogate model, black-box transfer attacks on RAG are further realized.\nExperiments conducted on opinion datasets across multiple topics show that the\nproposed attack strategy can significantly alter the opinion polarity of the\ncontent generated by RAG. This demonstrates the model's vulnerability and, more\nimportantly, reveals the potential negative impact on user cognition and\ndecision-making, making it easier to mislead users into accepting incorrect or\nbiased information."
        ],
        "neg": []
    },
    {
        "query": "We introduce Differential Performance Evaluation (DPE), a framework designed\nto reliably evaluate Large Language Models (LLMs) for efficient code\ngeneration. Traditional coding benchmarks often fail to provide reliable\ninsights into code efficiency, due to their reliance on simplistic test inputs\nand the absence of effective compound metrics. DPE addresses these issues by\nfocusing on efficiency-demanding programming tasks and establishing an\ninsightful compound metric for performance evaluation. DPE operates in two\nphases: To curate efficiency datasets, it selects efficiency-demanding tasks\nfrom existing coding benchmarks and generates computationally expensive inputs\nto stress the efficiency of LLM solutions. To assess the code efficiency, DPE\nprofiles the new solution and compares it globally against a set of reference\nsolutions that exhibit distinct efficiency levels, where the matched level\ndefines its efficiency score. As a proof of concept, we use DPE to create\nEvalPerf, a benchmark with 121 performance-challenging coding tasks. Our\ncomprehensive evaluation draws interesting findings on the efficiency impact of\nmodel sizes, instruction tuning, and prompting. For example, while the scaling\nlaw fails to account for code efficiency, general instruction tuning benefits\nboth code correctness and efficiency. We also evaluate the evaluation by\nexamining the effectiveness of DPE, showing that EvalPerf is reliable and\nconvenient to use even across platforms.",
        "pos": [
            "Trading is a highly competitive task that requires a combination of strategy,\nknowledge, and psychological fortitude. With the recent success of large\nlanguage models(LLMs), it is appealing to apply the emerging intelligence of\nLLM agents in this competitive arena and understanding if they can outperform\nprofessional traders. In this survey, we provide a comprehensive review of the\ncurrent research on using LLMs as agents in financial trading. We summarize the\ncommon architecture used in the agent, the data inputs, and the performance of\nLLM trading agents in backtesting as well as the challenges presented in these\nresearch. This survey aims to provide insights into the current state of\nLLM-based financial trading agents and outline future research directions in\nthis field."
        ],
        "neg": []
    },
    {
        "query": "Open Domain Question Answering (ODQA) has been advancing rapidly in recent\ntimes, driven by significant developments in dense passage retrieval and\npretrained language models. Current models typically incorporate the FiD\nframework, which is composed by a neural retriever alongside an encoder-decoder\nneural reader. In the answer generation process, the retriever will retrieve\nnumerous passages (around 100 for instance), each of which is then individually\nencoded by the encoder. Subsequently, the decoder makes predictions based on\nthese encoded passages. Nevertheless, this framework can be relatively\ntime-consuming, particularly due to the extensive length of the gathered\npassages. To address this, we introduce FastFiD in this paper, a novel approach\nthat executes sentence selection on the encoded passages. This aids in\nretaining valuable sentences while reducing the context length required for\ngenerating answers. Experiments on three commonly used datasets (Natural\nQuestions, TriviaQA and ASQA) demonstrate that our method can enhance the\ninference speed by 2.3X-5.7X, while simultaneously maintaining the model's\nperformance. Moreover, an in-depth analysis of the model's attention reveals\nthat the selected sentences indeed hold a substantial contribution towards the\nfinal answer. The codes are publicly available at\nhttps://github.com/thunlp/FastFiD.",
        "pos": [
            "Large language models exhibit aspects of human-level intelligence that\ncatalyze their application as human-like agents in domains such as social\nsimulations, human-machine interactions, and collaborative multi-agent systems.\nHowever, the absence of distinct personalities, such as displaying ingratiating\nbehaviors, inconsistent opinions, and uniform response patterns, diminish LLMs\nutility in practical applications. Addressing this, the development of\npersonality traits in LLMs emerges as a crucial area of research to unlock\ntheir latent potential. Existing methods to personify LLMs generally involve\nstrategies like employing stylized training data for instruction tuning or\nusing prompt engineering to simulate different personalities. These methods\nonly capture superficial linguistic styles instead of the core of personalities\nand are therefore not stable. In this study, we propose PersLLM, integrating\npsychology-grounded principles of personality: social practice, consistency,\nand dynamic development, into a comprehensive training methodology. We\nincorporate personality traits directly into the model parameters, enhancing\nthe model's resistance to induction, promoting consistency, and supporting the\ndynamic evolution of personality. Single-agent evaluation validates our\nmethod's superiority, as it produces responses more aligned with reference\npersonalities compared to other approaches. Case studies for multi-agent\ncommunication highlight its benefits in enhancing opinion consistency within\nindividual agents and fostering collaborative creativity among multiple agents\nin dialogue contexts, potentially benefiting human simulation and multi-agent\ncooperation. Additionally, human-agent interaction evaluations indicate that\nour personified models significantly enhance interactive experiences,\nunderscoring the practical implications of our research.",
            "Large Language Models (LLMs) exhibit various emergent abilities. Among these\nabilities, some might reveal the internal working mechanisms of models. In this\npaper, we uncover a novel emergent capability in models: the intrinsic ability\nto perform extended sequences of calculations without relying on\nchain-of-thought step-by-step solutions. Remarkably, the most advanced models\ncan directly output the results of two-digit number additions with lengths\nextending up to 15 addends. We hypothesize that the model emerges Implicit\nDiscrete State Representations (IDSRs) within its hidden states and performs\nsymbolic calculations internally. To test this hypothesis, we design a sequence\nof experiments that look into the hidden states. Specifically, we first confirm\nthat IDSRs exist. Then, we provide interesting observations about the formation\nof IDSRs from layer, digit, and sequence perspectives. Finally, we confirm that\nmodels indeed use IDSRs to produce the final answers. However, we also discover\nthat these state representations are far from lossless in current open-sourced\nmodels, leading to inaccuracies in their final performance. Our work presents a\nnovel exploration of LLMs' symbolic calculation abilities and the underlying\nmechanisms."
        ],
        "neg": []
    },
    {
        "query": "The cognitive essence of humans is deeply intertwined with the concept of\nanimacy, which plays an essential role in shaping their memory, vision, and\nmulti-layered language understanding. Although animacy appears in language via\nnuanced constraints on verbs and adjectives, it is also learned and refined\nthrough extralinguistic information. Similarly, we assume that the LLMs'\nlimited abilities to understand natural language when processing animacy are\nmotivated by the fact that these models are trained exclusively on text.\n  Hence, the question this paper aims to answer arises: can LLMs, in their\ndigital wisdom, process animacy in a similar way to what humans would do? We\nthen propose a systematic analysis via prompting approaches. In particular, we\nprobe different LLMs by prompting them using animate, inanimate, usual, and\nstranger contexts. Results reveal that, although LLMs have been trained\npredominantly on textual data, they exhibit human-like behavior when faced with\ntypical animate and inanimate entities in alignment with earlier studies.\nHence, LLMs can adapt to understand unconventional situations by recognizing\noddities as animated without needing to interface with unspoken cognitive\ntriggers humans rely on to break down animations.",
        "pos": [
            "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks."
        ],
        "neg": []
    },
    {
        "query": "Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at \\url{https://github.com/THUDM/VisualAgentBench}.",
        "pos": [
            "Question Under Discussion (QUD) is a discourse framework that uses implicit\nquestions to reveal discourse relationships between sentences. In QUD parsing,\neach sentence is viewed as an answer to a question triggered by an anchor\nsentence in prior context. The resulting QUD structure is required to conform\nto several theoretical criteria like answer compatibility (how well the\nquestion is answered), making QUD parsing a challenging task. Previous works\nconstruct QUD parsers in a pipelined manner (i.e. detect the trigger sentence\nin context and then generate the question). However, these parsers lack a\nholistic view of the task and can hardly satisfy all the criteria. In this\nwork, we introduce QUDSELECT, a joint-training framework that selectively\ndecodes the QUD dependency structures considering the QUD criteria. Using\ninstruction-tuning, we train models to simultaneously predict the anchor\nsentence and generate the associated question. To explicitly incorporate the\ncriteria, we adopt a selective decoding strategy of sampling multiple QUD\ncandidates during inference, followed by selecting the best one with criteria\nscorers. Our method outperforms the state-of-the-art baseline models by 9% in\nhuman evaluation and 4% in automatic evaluation, demonstrating the\neffectiveness of our framework.",
            "With the development of large language models (LLMs) like ChatGPT, both their\nvast applications and potential vulnerabilities have come to the forefront.\nWhile developers have integrated multiple safety mechanisms to mitigate their\nmisuse, a risk remains, particularly when models encounter adversarial inputs.\nThis study unveils an attack mechanism that capitalizes on human conversation\nstrategies to extract harmful information from LLMs. We delineate three pivotal\nstrategies: (i) decomposing malicious questions into seemingly innocent\nsub-questions; (ii) rewriting overtly malicious questions into more covert,\nbenign-sounding ones; (iii) enhancing the harmfulness of responses by prompting\nmodels for illustrative examples. Unlike conventional methods that target\nexplicit malicious responses, our approach delves deeper into the nature of the\ninformation provided in responses. Through our experiments conducted on\nGPT-3.5-turbo, GPT-4, and Llama2, our method has demonstrated a marked efficacy\ncompared to conventional attack methods. In summary, this work introduces a\nnovel attack method that outperforms previous approaches, raising an important\nquestion: How to discern whether the ultimate intent in a dialogue is\nmalicious?",
            "LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiment with different approaches to attribution on 4 LLMs\nof different sizes, both prompted and fine-tuned. We find that citation, i.e.\nresponse generation and evidence extraction in one step, mostly performs best.\nWe investigate whether the ``Lost in the Middle'' phenomenon exists for\nattribution, but do not find this. We also find that evidence quality can\npredict response quality on datasets with simple responses, but not so for\ncomplex responses, as models struggle with providing evidence for complex\nclaims. We release code and data for further investigation."
        ],
        "neg": []
    },
    {
        "query": "Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at \\url{https://github.com/THUDM/VisualAgentBench}.",
        "pos": [
            "Decision Transformer (DT) has emerged as a promising class of algorithms in\noffline reinforcement learning (RL) tasks, leveraging pre-collected datasets\nand Transformer's capability to model long sequences. Recent works have\ndemonstrated that using parts of trajectories from training tasks as prompts in\nDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.\nHowever, collecting data from specific environments can be both costly and\nunsafe in many scenarios, leading to suboptimal performance and limited\nfew-shot prompt abilities due to the data-hungry nature of Transformer-based\nmodels. Additionally, the limited datasets used in pre-training make it\nchallenging for Prompt-DT type of methods to distinguish between various RL\ntasks through prompts alone. To address these challenges, we introduce the\nLanguage model-initialized Prompt Decision Transformer (LPDT), which leverages\npre-trained language models for meta-RL tasks and fine-tunes the model using\nLow-rank Adaptation (LoRA). We further incorporate prompt regularization to\neffectively differentiate between tasks based on prompt feature\nrepresentations. Our approach integrates pre-trained language model and RL\ntasks seamlessly. Extensive empirical studies demonstrate that initializing\nwith a pre-trained language model significantly enhances the performance of\nPrompt-DT on unseen tasks compared to baseline methods.",
            "Training with larger mini-batches improves the performance and convergence\nrate of training machine learning models. However, training with large\nmini-batches becomes prohibitive for Large Language Models (LLMs) with billions\nof parameters, due to the large GPU memory requirement. To address this\nproblem, we propose finding small mini-batches that simulate the dynamics of\ntraining with larger mini-batches. Specifically, we formulate selecting smaller\nmini-batches of examples that closely capture gradients of large mini-batches\nas a submodular maximization problem. Nevertheless, the very large\ndimensionality of the gradients makes the problem very challenging to solve. To\naddress this, we leverage ideas from zeroth-order optimization and neural\nnetwork pruning to find lower-dimensional gradient estimates that allow finding\nhigh-quality subsets effectively with a limited amount of memory. We prove the\nsuperior convergence rate of training on the small mini-batches found by our\nmethod and empirically show its effectiveness. Our method can effectively\nreduce the memory requirement by 2x and speed up training by 1.3x, as we\nconfirm for fine-tuning Phi-2 on MathInstruct. Our method can be easily stacked\nwith LoRA and other memory-efficient methods to further reduce the memory\nrequirements of training LLMs."
        ],
        "neg": []
    },
    {
        "query": "Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at \\url{https://github.com/THUDM/VisualAgentBench}.",
        "pos": [
            "The rise of large language models (LLMs) has enabled us to seek answers to\ninherently debatable questions on LLM chatbots, necessitating a reliable way to\nevaluate their ability. However, traditional QA benchmarks assume fixed answers\nare inadequate for this purpose. To address this, we introduce DebateQA, a\ndataset of 2,941 debatable questions, each accompanied by multiple\nhuman-annotated partial answers that capture a variety of perspectives. We\ndevelop two metrics: Perspective Diversity, which evaluates the\ncomprehensiveness of perspectives, and Dispute Awareness, which assesses if the\nLLM acknowledges the question's debatable nature. Experiments demonstrate that\nboth metrics align with human preferences and are stable across different\nunderlying models. Using DebateQA with two metrics, we assess 12 popular LLMs\nand retrieval-augmented generation methods. Our findings reveal that while LLMs\ngenerally excel at recognizing debatable issues, their ability to provide\ncomprehensive answers encompassing diverse perspectives varies considerably.",
            "The common toxicity and societal bias in contents generated by large language\nmodels (LLMs) necessitate strategies to reduce harm. Present solutions often\ndemand white-box access to the model or substantial training, which is\nimpractical for cutting-edge commercial LLMs. Moreover, prevailing prompting\nmethods depend on external tool feedback and fail to simultaneously lessen\ntoxicity and bias. Motivated by social psychology principles, we propose a\nnovel strategy named \\textbf{perspective-taking prompting (\\textsc{PeT})} that\ninspires LLMs to integrate diverse human perspectives and self-regulate their\nresponses. This self-correction mechanism can significantly diminish toxicity\n(up to $89\\%$) and bias (up to $73\\%$) in LLMs' responses. Rigorous evaluations\nand ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and\nthree open-source LLMs, revealing \\textsc{PeT}'s superiority in producing less\nharmful responses, outperforming five strong baselines."
        ],
        "neg": []
    },
    {
        "query": "Vision language models can now generate long-form answers to questions about\nimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a\ndataset of long-form answers to visual questions posed by blind and low vision\n(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,\ncollected from human expert describers and six VQA models. We develop and\nannotate functional roles of sentences of LFVQA and demonstrate that long-form\nanswers contain information beyond the question answer such as explanations and\nsuggestions. We further conduct automatic and human evaluations with BLV and\nsighted people to evaluate long-form answers. BLV people perceive both\nhuman-written and generated long-form answers to be plausible, but generated\nanswers often hallucinate incorrect visual details, especially for unanswerable\nvisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,\nwe evaluate the ability of VQA models to abstain from answering unanswerable\nquestions across multiple prompting strategies.",
        "pos": [
            "Large language models (LLMs) are increasingly being used to synthesize and\nreason about source code. However, the static nature of these models' knowledge\ndoes not reflect the fact that libraries and API functions they invoke are\ncontinuously evolving, with functionality being added or changing. While\nnumerous benchmarks evaluate how LLMs can generate code, no prior work has\nstudied how an LLMs' knowledge about code API functions can be updated. To fill\nthis gap, we present CodeUpdateArena, a benchmark for knowledge editing in the\ncode domain. An instance in our benchmark consists of a synthetic API function\nupdate paired with a program synthesis example that uses the updated\nfunctionality; our goal is to update an LLM to be able to solve this program\nsynthesis example without providing documentation of the update at inference\ntime. Compared to knowledge editing for facts encoded in text, success here is\nmore challenging: a code LLM must correctly reason about the semantics of the\nmodified function rather than just reproduce its syntax. Our dataset is\nconstructed by first prompting GPT-4 to generate atomic and executable function\nupdates. Then, for each update, we generate program synthesis examples whose\ncode solutions are prone to use the update. Our benchmark covers updates of\nvarious types to 54 functions from seven diverse Python packages, with a total\nof 670 program synthesis examples. Our experiments show that prepending\ndocumentation of the update to open-source code LLMs (i.e., DeepSeek,\nCodeLlama) does not allow them to incorporate changes for problem solving, and\nexisting knowledge editing techniques also have substantial room for\nimprovement. We hope our benchmark will inspire new methods for knowledge\nupdating in code LLMs."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
        "pos": [
            "Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge.Existing methods, mostly\nsupervised, face challenges in obtaining labeled entity pairs. To address this,\nrecent studies have shifted towards a self-supervised and unsupervised\nframeworks. Despite their effectiveness, these approaches have limitations: (1)\nthey mainly focus on entity features, neglecting the semantic information of\nrelations, (2) they assume isomorphism between source and target graphs,\nleading to noise and reduced alignment accuracy, and (3) they are susceptible\nto noise in the textual features, especially when encountering inconsistent\ntranslations or Out-Of-Vocabulary (OOV) problems.\n  In this paper, we propose ERAlign, an unsupervised and robust cross-lingual\nEA framework that jointly performs Entity-level and Relation-level Alignment\nusing semantic textual features of relations and entities. Its refinement\nprocess iteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification\nprocess examines the entities' neighbor triples as the linearized text. This\n\\textit{Align-and-Verify} pipeline that rigorously assesses alignment results,\nachieving near-perfect alignment even in the presence of noisy textual features\nof entities. Our extensive experiments demonstrate that robustness and general\napplicability of \\proposed improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
        "pos": [
            "Previous studies on continual knowledge learning (CKL) in large language\nmodels (LLMs) have predominantly focused on approaches such as regularization,\narchitectural modifications, and rehearsal techniques to mitigate catastrophic\nforgetting. However, these methods naively inherit the inefficiencies of\nstandard training procedures, indiscriminately applying uniform weight across\nall tokens, which can lead to unnecessary parameter updates and increased\nforgetting. To address these shortcomings, we propose a novel CKL approach\ntermed Train-Attention-Augmented Language Model (TAALM), which enhances\nlearning efficiency by dynamically predicting and applying weights to tokens\nbased on their usefulness. This method employs a meta-learning framework that\noptimizes token importance predictions, facilitating targeted knowledge updates\nand minimizing forgetting. Also, we observe that existing benchmarks do not\nclearly exhibit the trade-off between learning and retaining, therefore we\npropose a new benchmark, \\textsc{LAMA-ckl}, to address this issue. Through\nexperiments conducted on both newly introduced and established CKL benchmarks,\nTAALM proves the state-of-the-art performance upon the baselines, and also\nshows synergistic compatibility when integrated with previous CKL approaches.",
            "Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge.Existing methods, mostly\nsupervised, face challenges in obtaining labeled entity pairs. To address this,\nrecent studies have shifted towards a self-supervised and unsupervised\nframeworks. Despite their effectiveness, these approaches have limitations: (1)\nthey mainly focus on entity features, neglecting the semantic information of\nrelations, (2) they assume isomorphism between source and target graphs,\nleading to noise and reduced alignment accuracy, and (3) they are susceptible\nto noise in the textual features, especially when encountering inconsistent\ntranslations or Out-Of-Vocabulary (OOV) problems.\n  In this paper, we propose ERAlign, an unsupervised and robust cross-lingual\nEA framework that jointly performs Entity-level and Relation-level Alignment\nusing semantic textual features of relations and entities. Its refinement\nprocess iteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification\nprocess examines the entities' neighbor triples as the linearized text. This\n\\textit{Align-and-Verify} pipeline that rigorously assesses alignment results,\nachieving near-perfect alignment even in the presence of noisy textual features\nof entities. Our extensive experiments demonstrate that robustness and general\napplicability of \\proposed improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications.",
            "Recently, the demand for psychological counseling has significantly increased\nas more individuals express concerns about their mental health. This surge has\naccelerated efforts to improve the accessibility of counseling by using large\nlanguage models (LLMs) as counselors. To ensure client privacy, training\nopen-source LLMs faces a key challenge: the absence of realistic counseling\ndatasets. To address this, we introduce Cactus, a multi-turn dialogue dataset\nthat emulates real-life interactions using the goal-oriented and structured\napproach of Cognitive Behavioral Therapy (CBT). We create a diverse and\nrealistic dataset by designing clients with varied, specific personas, and\nhaving counselors systematically apply CBT techniques in their interactions. To\nassess the quality of our data, we benchmark against established psychological\ncriteria used to evaluate real counseling sessions, ensuring alignment with\nexpert evaluations. Experimental results demonstrate that Camel, a model\ntrained with Cactus, outperforms other models in counseling skills,\nhighlighting its effectiveness and potential as a counseling agent. We make our\ndata, model, and code publicly available."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
        "pos": [
            "Previous studies on continual knowledge learning (CKL) in large language\nmodels (LLMs) have predominantly focused on approaches such as regularization,\narchitectural modifications, and rehearsal techniques to mitigate catastrophic\nforgetting. However, these methods naively inherit the inefficiencies of\nstandard training procedures, indiscriminately applying uniform weight across\nall tokens, which can lead to unnecessary parameter updates and increased\nforgetting. To address these shortcomings, we propose a novel CKL approach\ntermed Train-Attention-Augmented Language Model (TAALM), which enhances\nlearning efficiency by dynamically predicting and applying weights to tokens\nbased on their usefulness. This method employs a meta-learning framework that\noptimizes token importance predictions, facilitating targeted knowledge updates\nand minimizing forgetting. Also, we observe that existing benchmarks do not\nclearly exhibit the trade-off between learning and retaining, therefore we\npropose a new benchmark, \\textsc{LAMA-ckl}, to address this issue. Through\nexperiments conducted on both newly introduced and established CKL benchmarks,\nTAALM proves the state-of-the-art performance upon the baselines, and also\nshows synergistic compatibility when integrated with previous CKL approaches.",
            "Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge.Existing methods, mostly\nsupervised, face challenges in obtaining labeled entity pairs. To address this,\nrecent studies have shifted towards a self-supervised and unsupervised\nframeworks. Despite their effectiveness, these approaches have limitations: (1)\nthey mainly focus on entity features, neglecting the semantic information of\nrelations, (2) they assume isomorphism between source and target graphs,\nleading to noise and reduced alignment accuracy, and (3) they are susceptible\nto noise in the textual features, especially when encountering inconsistent\ntranslations or Out-Of-Vocabulary (OOV) problems.\n  In this paper, we propose ERAlign, an unsupervised and robust cross-lingual\nEA framework that jointly performs Entity-level and Relation-level Alignment\nusing semantic textual features of relations and entities. Its refinement\nprocess iteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification\nprocess examines the entities' neighbor triples as the linearized text. This\n\\textit{Align-and-Verify} pipeline that rigorously assesses alignment results,\nachieving near-perfect alignment even in the presence of noisy textual features\nof entities. Our extensive experiments demonstrate that robustness and general\napplicability of \\proposed improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications.",
            "Recently, the demand for psychological counseling has significantly increased\nas more individuals express concerns about their mental health. This surge has\naccelerated efforts to improve the accessibility of counseling by using large\nlanguage models (LLMs) as counselors. To ensure client privacy, training\nopen-source LLMs faces a key challenge: the absence of realistic counseling\ndatasets. To address this, we introduce Cactus, a multi-turn dialogue dataset\nthat emulates real-life interactions using the goal-oriented and structured\napproach of Cognitive Behavioral Therapy (CBT). We create a diverse and\nrealistic dataset by designing clients with varied, specific personas, and\nhaving counselors systematically apply CBT techniques in their interactions. To\nassess the quality of our data, we benchmark against established psychological\ncriteria used to evaluate real counseling sessions, ensuring alignment with\nexpert evaluations. Experimental results demonstrate that Camel, a model\ntrained with Cactus, outperforms other models in counseling skills,\nhighlighting its effectiveness and potential as a counseling agent. We make our\ndata, model, and code publicly available."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github.",
        "pos": [
            "Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model's vocabulary space and named them \"glitch tokens\". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.",
            "Security concerns for large language models (LLMs) have recently escalated,\nfocusing on thwarting jailbreaking attempts in discrete prompts. However, the\nexploration of jailbreak vulnerabilities arising from continuous embeddings has\nbeen limited, as prior approaches primarily involved appending discrete or\ncontinuous suffixes to inputs. Our study presents a novel channel for\nconducting direct attacks on LLM inputs, eliminating the need for suffix\naddition or specific questions provided that the desired output is predefined.\nWe additionally observe that extensive iterations often lead to overfitting,\ncharacterized by repetition in the output. To counteract this, we propose a\nsimple yet effective strategy named CLIP. Our experiments show that for an\ninput length of 40 at iteration 1000, applying CLIP improves the ASR from 62%\nto 83%"
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github.",
        "pos": [
            "While large language models (LLMs) have demonstrated superior multi-task\ncapabilities, understanding the learning mechanisms behind this is still a\nchallenging problem. In this paper, we attempt to understand such mechanisms\nfrom the perspective of neurons. Specifically, we detect task-sensitive neurons\nin LLMs via gradient attribution on task-specific data. Through extensive\ndeactivation and fine-tuning experiments, we demonstrate that the detected\nneurons are highly correlated with the given task, which we term as\ntask-specific neurons. With these identified task-specific neurons, we delve\ninto two common problems in multi-task learning and continuous learning:\nGeneralization and Catastrophic Forgetting. We find that the overlap of\ntask-specific neurons is strongly associated with generalization and\nspecialization across tasks. Interestingly, at certain layers of LLMs, there is\na high similarity in the parameters of different task-specific neurons, and\nsuch similarity is highly correlated with the generalization performance.\nInspired by these findings, we propose a neuron-level continuous fine-tuning\nmethod that only fine-tunes the current task-specific neurons during continuous\nlearning, and extensive experiments demonstrate the effectiveness of the\nproposed method. Our study provides insights into the interpretability of LLMs\nin multi-task learning.",
            "Controllability and proactivity are crucial properties of autonomous\nconversational agents (CAs). Controllability requires the CAs to follow the\nstandard operating procedures (SOPs), such as verifying identity before\nactivating credit cards. Proactivity requires the CAs to guide the conversation\ntowards the goal during user uncooperation, such as persuasive dialogue.\nExisting research cannot be unified with controllability, proactivity, and low\nmanual annotation. To bridge this gap, we propose a new framework for\nplanning-based conversational agents (PCA) powered by large language models\n(LLMs), which only requires humans to define tasks and goals for the LLMs.\nBefore conversation, LLM plans the core and necessary SOP for dialogue offline.\nDuring the conversation, LLM plans the best action path online referring to the\nSOP, and generates responses to achieve process controllability. Subsequently,\nwe propose a semi-automatic dialogue data creation framework and curate a\nhigh-quality dialogue dataset (PCA-D). Meanwhile, we develop multiple variants\nand evaluation metrics for PCA, e.g., planning with Monte Carlo Tree Search\n(PCA-M), which searches for the optimal dialogue action while satisfying SOP\nconstraints and achieving the proactive of the dialogue. Experiment results\nshow that LLMs finetuned on PCA-D can significantly improve the performance and\ngeneralize to unseen domains. PCA-M outperforms other CoT and ToT baselines in\nterms of conversation controllability, proactivity, task success rate, and\noverall logical coherence, and is applicable in industry dialogue scenarios.\nThe dataset and codes are available at XXXX.",
            "Manual Red teaming is a commonly-used method to identify vulnerabilities in\nlarge language models (LLMs), which, is costly and unscalable. In contrast,\nautomated red teaming uses a Red LLM to automatically generate adversarial\nprompts to the Target LLM, offering a scalable way for safety vulnerability\ndetection. However, the difficulty of building a powerful automated Red LLM\nlies in the fact that the safety vulnerabilities of the Target LLM are\ndynamically changing with the evolution of the Target LLM. To mitigate this\nissue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in\nwhich the Red LLM and Target LLM are deeply and dynamically interacting with\neach other in an iterative manner. In each iteration, in order to generate\nsuccessful attacks as many as possible, the Red LLM not only takes into account\nthe responses from the Target LLM, but also adversarially adjust its attacking\ndirections by monitoring the global diversity of generated attacks across\nmultiple iterations. Simultaneously, to explore dynamically changing safety\nvulnerabilities of the Target LLM, we allow the Target LLM to enhance its\nsafety via an active learning based data selection mechanism. Experimential\nresults demonstrate that DART significantly reduces the safety risk of the\ntarget LLM. For human evaluation on Anthropic Harmless dataset, compared to the\ninstruction-tuning target LLM, DART eliminates the violation risks by 53.4\\%.\nWe will release the datasets and codes of DART soon."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.",
        "pos": [
            "The development of monolingual language models for low and mid-resource\nlanguages continues to be hindered by the difficulty in sourcing high-quality\ntraining data. In this study, we present a novel cross-lingual vocabulary\ntransfer strategy, trans-tokenization, designed to tackle this challenge and\nenable more efficient language adaptation. Our approach focuses on adapting a\nhigh-resource monolingual LLM to an unseen target language by initializing the\ntoken embeddings of the target language using a weighted average of\nsemantically similar token embeddings from the source language. For this, we\nleverage a translation resource covering both the source and target languages.\nWe validate our method with the Tweeties, a series of trans-tokenized LLMs, and\ndemonstrate their competitive performance on various downstream tasks across a\nsmall but diverse set of languages. Additionally, we introduce Hydra LLMs,\nmodels with multiple swappable language modeling heads and embedding tables,\nwhich further extend the capabilities of our trans-tokenization strategy. By\ndesigning a Hydra LLM based on the multilingual model TowerInstruct, we\ndeveloped a state-of-the-art machine translation model for Tatar, in a\nzero-shot manner, completely bypassing the need for high-quality parallel data.\nThis breakthrough is particularly significant for low-resource languages like\nTatar, where high-quality parallel data is hard to come by. By lowering the\ndata and time requirements for training high-quality models, our\ntrans-tokenization strategy allows for the development of LLMs for a wider\nrange of languages, especially those with limited resources. We hope that our\nwork will inspire further research and collaboration in the field of\ncross-lingual vocabulary transfer and contribute to the empowerment of\nlanguages on a global scale."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.",
        "pos": [
            "Natural Language Processing (NLP) systems are increasingly taking the form of\nmulti-stage pipelines involving multiple distinct language models (LMs) and\nprompting strategies. Here we address the question of how to fine-tune such\nsystems to improve their performance. We cast this as a problem of optimizing\nthe underlying LM weights and the prompting strategies together, and consider a\nchallenging but highly realistic scenario in which we have no gold labels for\nany intermediate stages in the pipeline. To address this challenge, we evaluate\napproximate optimization strategies in which we bootstrap training labels for\nall pipeline stages and use these to optimize the pipeline's prompts and\nfine-tune its weights alternatingly. In experiments with multi-hop QA,\nmathematical reasoning, and feature-based classification, we find that simple\napproaches for optimizing the prompts and weights together outperform directly\noptimizing weights alone and prompts alone by up to 65% and 5%, respectively,\non average across LMs and tasks. We will release our new optimizers in DSPy at\nhttp://dspy.ai"
        ],
        "neg": []
    },
    {
        "query": "Visual storytelling systems generate multi-sentence stories from image\nsequences. In this task, capturing contextual information and bridging visual\nvariation bring additional challenges. We propose a simple yet effective\nframework that leverages the generalization capabilities of pretrained\nfoundation models, only training a lightweight vision-language mapping network\nto connect modalities, while incorporating context to enhance coherence. We\nintroduce a multimodal contrastive objective that also improves visual\nrelevance and story informativeness. Extensive experimental results, across\nboth automatic metrics and human evaluations, demonstrate that the stories\ngenerated by our framework are diverse, coherent, informative, and interesting.",
        "pos": [
            "Various benchmarks have been proposed to test linguistic understanding in\npre-trained vision \\& language (VL) models. Here we build on the existence task\nfrom the VALSE benchmark (Parcalabescu et al, 2022) which we use to test\nmodels' understanding of negation, a particularly interesting issue for\nmultimodal models. However, while such VL benchmarks are useful for measuring\nmodel performance, they do not reveal anything about the internal processes\nthrough which these models arrive at their outputs in such visio-linguistic\ntasks. We take inspiration from the growing literature on model\ninterpretability to explain the behaviour of VL models on the understanding of\nnegation. Specifically, we approach these questions through an in-depth\nanalysis of the text encoder in CLIP (Radford et al, 2021), a highly\ninfluential VL model. We localise parts of the encoder that process negation\nand analyse the role of attention heads in this task. Our contributions are\nthreefold. We demonstrate how methods from the language model interpretability\nliterature (such as causal tracing) can be translated to multimodal models and\ntasks; we provide concrete insights into how CLIP processes negation on the\nVALSE existence task; and we highlight inherent limitations in the VALSE\ndataset as a benchmark for linguistic understanding."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces rStar, a self-play mutual reasoning approach that\nsignificantly improves reasoning capabilities of small language models (SLMs)\nwithout fine-tuning or superior models. rStar decouples reasoning into a\nself-play mutual generation-discrimination process. First, a target SLM\naugments the Monte Carlo Tree Search (MCTS) with a rich set of human-like\nreasoning actions to construct higher quality reasoning trajectories. Next,\nanother SLM, with capabilities similar to the target SLM, acts as a\ndiscriminator to verify each trajectory generated by the target SLM. The\nmutually agreed reasoning trajectories are considered mutual consistent, thus\nare more likely to be correct. Extensive experiments across five SLMs\ndemonstrate rStar can effectively solve diverse reasoning problems, including\nGSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K\naccuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for\nMistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be\navailable at https://github.com/zhentingqi/rStar.",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering."
        ],
        "neg": []
    },
    {
        "query": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.",
        "pos": [
            "As large language models (LLMs) continue to evolve, the need for robust and\nstandardized evaluation benchmarks becomes paramount. Evaluating the\nperformance of these models is a complex challenge that requires careful\nconsideration of various linguistic tasks, model architectures, and\nbenchmarking methodologies. In recent years, various frameworks have emerged as\nnoteworthy contributions to the field, offering comprehensive evaluation tests\nand benchmarks for assessing the capabilities of LLMs across diverse domains.\nThis paper provides an exploration and critical analysis of some of these\nevaluation methodologies, shedding light on their strengths, limitations, and\nimpact on advancing the state-of-the-art in natural language processing."
        ],
        "neg": []
    },
    {
        "query": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.",
        "pos": [
            "As large language models (LLMs) continue to evolve, the need for robust and\nstandardized evaluation benchmarks becomes paramount. Evaluating the\nperformance of these models is a complex challenge that requires careful\nconsideration of various linguistic tasks, model architectures, and\nbenchmarking methodologies. In recent years, various frameworks have emerged as\nnoteworthy contributions to the field, offering comprehensive evaluation tests\nand benchmarks for assessing the capabilities of LLMs across diverse domains.\nThis paper provides an exploration and critical analysis of some of these\nevaluation methodologies, shedding light on their strengths, limitations, and\nimpact on advancing the state-of-the-art in natural language processing."
        ],
        "neg": []
    },
    {
        "query": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.",
        "pos": [
            "As large language models (LLMs) continue to evolve, the need for robust and\nstandardized evaluation benchmarks becomes paramount. Evaluating the\nperformance of these models is a complex challenge that requires careful\nconsideration of various linguistic tasks, model architectures, and\nbenchmarking methodologies. In recent years, various frameworks have emerged as\nnoteworthy contributions to the field, offering comprehensive evaluation tests\nand benchmarks for assessing the capabilities of LLMs across diverse domains.\nThis paper provides an exploration and critical analysis of some of these\nevaluation methodologies, shedding light on their strengths, limitations, and\nimpact on advancing the state-of-the-art in natural language processing."
        ],
        "neg": []
    },
    {
        "query": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.",
        "pos": [
            "As large language models (LLMs) continue to evolve, the need for robust and\nstandardized evaluation benchmarks becomes paramount. Evaluating the\nperformance of these models is a complex challenge that requires careful\nconsideration of various linguistic tasks, model architectures, and\nbenchmarking methodologies. In recent years, various frameworks have emerged as\nnoteworthy contributions to the field, offering comprehensive evaluation tests\nand benchmarks for assessing the capabilities of LLMs across diverse domains.\nThis paper provides an exploration and critical analysis of some of these\nevaluation methodologies, shedding light on their strengths, limitations, and\nimpact on advancing the state-of-the-art in natural language processing."
        ],
        "neg": []
    },
    {
        "query": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.",
        "pos": [
            "As large language models (LLMs) continue to evolve, the need for robust and\nstandardized evaluation benchmarks becomes paramount. Evaluating the\nperformance of these models is a complex challenge that requires careful\nconsideration of various linguistic tasks, model architectures, and\nbenchmarking methodologies. In recent years, various frameworks have emerged as\nnoteworthy contributions to the field, offering comprehensive evaluation tests\nand benchmarks for assessing the capabilities of LLMs across diverse domains.\nThis paper provides an exploration and critical analysis of some of these\nevaluation methodologies, shedding light on their strengths, limitations, and\nimpact on advancing the state-of-the-art in natural language processing."
        ],
        "neg": []
    },
    {
        "query": "Research on hate speech has predominantly revolved around detection and\ninterpretation from textual inputs, leaving verbal content largely unexplored.\nWhile there has been limited exploration into hate speech detection within\nverbal acoustic speech inputs, the aspect of interpretability has been\noverlooked. Therefore, we introduce a new task of explainable audio hate speech\ndetection. Specifically, we aim to identify the precise time intervals,\nreferred to as audio frame-level rationales, which serve as evidence for hate\nspeech classification. Towards this end, we propose two different approaches:\ncascading and End-to-End (E2E). The cascading approach initially converts audio\nto transcripts, identifies hate speech within these transcripts, and\nsubsequently locates the corresponding audio time frames. Conversely, the E2E\napproach processes audio utterances directly, which allows it to pinpoint hate\nspeech within specific time frames. Additionally, due to the lack of\nexplainable audio hate speech datasets that include audio frame-level\nrationales, we curated a synthetic audio dataset to train our models. We\nfurther validated these models on actual human speech utterances and found that\nthe E2E approach outperforms the cascading method in terms of the audio frame\nIntersection over Union (IoU) metric. Furthermore, we observed that including\nframe-level rationales significantly enhances hate speech detection accuracy\nfor the E2E approach.\n  \\textbf{Disclaimer} The reader may encounter content of an offensive or\nhateful nature. However, given the nature of the work, this cannot be avoided.",
        "pos": [
            "Recent dialogue systems rely on turn-based spoken interactions, requiring\naccurate Automatic Speech Recognition (ASR). Errors in ASR can significantly\nimpact downstream dialogue tasks. To address this, using dialogue context from\nuser and agent interactions for transcribing subsequent utterances has been\nproposed. This method incorporates the transcription of the user's speech and\nthe agent's response as model input, using the accumulated context generated by\neach turn. However, this context is susceptible to ASR errors because it is\ngenerated by the ASR model in an auto-regressive fashion. Such noisy context\ncan further degrade the benefits of context input, resulting in suboptimal ASR\nperformance. In this paper, we introduce Context Noise Representation Learning\n(CNRL) to enhance robustness against noisy context, ultimately improving\ndialogue speech recognition accuracy. To maximize the advantage of context\nawareness, our approach includes decoder pre-training using text-based dialogue\ndata and noise representation learning for a context encoder. Based on the\nevaluation of speech dialogues, our method shows superior results compared to\nbaselines. Furthermore, the strength of our approach is highlighted in noisy\nenvironments where user speech is barely audible due to real-world noise,\nrelying on contextual information to transcribe the input accurately."
        ],
        "neg": []
    },
    {
        "query": "Research on hate speech has predominantly revolved around detection and\ninterpretation from textual inputs, leaving verbal content largely unexplored.\nWhile there has been limited exploration into hate speech detection within\nverbal acoustic speech inputs, the aspect of interpretability has been\noverlooked. Therefore, we introduce a new task of explainable audio hate speech\ndetection. Specifically, we aim to identify the precise time intervals,\nreferred to as audio frame-level rationales, which serve as evidence for hate\nspeech classification. Towards this end, we propose two different approaches:\ncascading and End-to-End (E2E). The cascading approach initially converts audio\nto transcripts, identifies hate speech within these transcripts, and\nsubsequently locates the corresponding audio time frames. Conversely, the E2E\napproach processes audio utterances directly, which allows it to pinpoint hate\nspeech within specific time frames. Additionally, due to the lack of\nexplainable audio hate speech datasets that include audio frame-level\nrationales, we curated a synthetic audio dataset to train our models. We\nfurther validated these models on actual human speech utterances and found that\nthe E2E approach outperforms the cascading method in terms of the audio frame\nIntersection over Union (IoU) metric. Furthermore, we observed that including\nframe-level rationales significantly enhances hate speech detection accuracy\nfor the E2E approach.\n  \\textbf{Disclaimer} The reader may encounter content of an offensive or\nhateful nature. However, given the nature of the work, this cannot be avoided.",
        "pos": [
            "While machines learn from existing corpora, humans have the unique capability\nto establish and accept new language systems. This makes human form unique\nlanguage systems within social groups. Aligning with this, we focus on a gap\nremaining in addressing translation challenges within social groups, where\nin-group members utilize unique terminologies. We propose KpopMT dataset, which\naims to fill this gap by enabling precise terminology translation, choosing\nKpop fandom as an initiative for social groups given its global popularity.\nExpert translators provide 1k English translations for Korean posts and\ncomments, each annotated with specific terminology within social groups'\nlanguage systems. We evaluate existing translation systems including GPT models\non KpopMT to identify their failure cases. Results show overall low scores,\nunderscoring the challenges of reflecting group-specific terminologies and\nstyles in translation. We make KpopMT publicly available."
        ],
        "neg": []
    },
    {
        "query": "Research on hate speech has predominantly revolved around detection and\ninterpretation from textual inputs, leaving verbal content largely unexplored.\nWhile there has been limited exploration into hate speech detection within\nverbal acoustic speech inputs, the aspect of interpretability has been\noverlooked. Therefore, we introduce a new task of explainable audio hate speech\ndetection. Specifically, we aim to identify the precise time intervals,\nreferred to as audio frame-level rationales, which serve as evidence for hate\nspeech classification. Towards this end, we propose two different approaches:\ncascading and End-to-End (E2E). The cascading approach initially converts audio\nto transcripts, identifies hate speech within these transcripts, and\nsubsequently locates the corresponding audio time frames. Conversely, the E2E\napproach processes audio utterances directly, which allows it to pinpoint hate\nspeech within specific time frames. Additionally, due to the lack of\nexplainable audio hate speech datasets that include audio frame-level\nrationales, we curated a synthetic audio dataset to train our models. We\nfurther validated these models on actual human speech utterances and found that\nthe E2E approach outperforms the cascading method in terms of the audio frame\nIntersection over Union (IoU) metric. Furthermore, we observed that including\nframe-level rationales significantly enhances hate speech detection accuracy\nfor the E2E approach.\n  \\textbf{Disclaimer} The reader may encounter content of an offensive or\nhateful nature. However, given the nature of the work, this cannot be avoided.",
        "pos": [
            "Dialogue systems for mental health care aim to provide appropriate support to\nindividuals experiencing mental distress. While extensive research has been\nconducted to deliver adequate emotional support, existing studies cannot\nidentify individuals who require professional medical intervention and cannot\noffer suitable guidance. We introduce the Diagnostic Emotional Support\nConversation task for an advanced mental health management system. We develop\nthe DESC dataset to assess depression symptoms while maintaining user\nexperience by utilizing task-specific utterance generation prompts and a strict\nfiltering algorithm. Evaluations by professional psychological counselors\nindicate that DESC has a superior ability to diagnose depression than existing\ndata. Additionally, conversational quality evaluation reveals that DESC\nmaintains fluent, consistent, and coherent dialogues.",
            "Recent dialogue systems rely on turn-based spoken interactions, requiring\naccurate Automatic Speech Recognition (ASR). Errors in ASR can significantly\nimpact downstream dialogue tasks. To address this, using dialogue context from\nuser and agent interactions for transcribing subsequent utterances has been\nproposed. This method incorporates the transcription of the user's speech and\nthe agent's response as model input, using the accumulated context generated by\neach turn. However, this context is susceptible to ASR errors because it is\ngenerated by the ASR model in an auto-regressive fashion. Such noisy context\ncan further degrade the benefits of context input, resulting in suboptimal ASR\nperformance. In this paper, we introduce Context Noise Representation Learning\n(CNRL) to enhance robustness against noisy context, ultimately improving\ndialogue speech recognition accuracy. To maximize the advantage of context\nawareness, our approach includes decoder pre-training using text-based dialogue\ndata and noise representation learning for a context encoder. Based on the\nevaluation of speech dialogues, our method shows superior results compared to\nbaselines. Furthermore, the strength of our approach is highlighted in noisy\nenvironments where user speech is barely audible due to real-world noise,\nrelying on contextual information to transcribe the input accurately."
        ],
        "neg": []
    },
    {
        "query": "In the rapidly evolving fields of natural language processing and computer\nvision, Visual Word Sense Disambiguation (VWSD) stands as a critical, yet\nchallenging task. The quest for models that can seamlessly integrate and\ninterpret multimodal data is more pressing than ever. Imagine a system that can\nunderstand language with the depth and nuance of human cognition, while\nsimultaneously interpreting the rich visual context of the world around it.\n  We present ARPA, an architecture that fuses the unparalleled contextual\nunderstanding of large language models with the advanced feature extraction\ncapabilities of transformers, which then pass through a custom Graph Neural\nNetwork (GNN) layer to learn intricate relationships and subtle nuances within\nthe data. This innovative architecture not only sets a new benchmark in visual\nword disambiguation but also introduces a versatile framework poised to\ntransform how linguistic and visual data interact by harnessing the synergistic\nstrengths of its components, ensuring robust performance even in the most\ncomplex disambiguation scenarios. Through a series of experiments and\ncomparative analysis, we reveal the substantial advantages of our model,\nunderscoring its potential to redefine standards in the field. Beyond its\narchitectural prowess, our architecture excels through experimental\nenrichments, including sophisticated data augmentation and multi-modal training\ntechniques.\n  ARPA's introduction marks a significant milestone in visual word\ndisambiguation, offering a compelling solution that bridges the gap between\nlinguistic and visual modalities. We invite researchers and practitioners to\nexplore the capabilities of our model, envisioning a future where such hybrid\nmodels drive unprecedented advancements in artificial intelligence.",
        "pos": [
            "As NLP models become increasingly integral to decision-making processes, the\nneed for explainability and interpretability has become paramount. In this\nwork, we propose a framework that achieves the aforementioned by generating\nsemantically edited inputs, known as counterfactual interventions, which change\nthe model prediction, thus providing a form of counterfactual explanations for\nthe model. We test our framework on two NLP tasks - binary sentiment\nclassification and topic classification - and show that the generated edits are\ncontrastive, fluent and minimal, while the whole process remains significantly\nfaster that other state-of-the-art counterfactual editors."
        ],
        "neg": []
    },
    {
        "query": "In the rapidly evolving fields of natural language processing and computer\nvision, Visual Word Sense Disambiguation (VWSD) stands as a critical, yet\nchallenging task. The quest for models that can seamlessly integrate and\ninterpret multimodal data is more pressing than ever. Imagine a system that can\nunderstand language with the depth and nuance of human cognition, while\nsimultaneously interpreting the rich visual context of the world around it.\n  We present ARPA, an architecture that fuses the unparalleled contextual\nunderstanding of large language models with the advanced feature extraction\ncapabilities of transformers, which then pass through a custom Graph Neural\nNetwork (GNN) layer to learn intricate relationships and subtle nuances within\nthe data. This innovative architecture not only sets a new benchmark in visual\nword disambiguation but also introduces a versatile framework poised to\ntransform how linguistic and visual data interact by harnessing the synergistic\nstrengths of its components, ensuring robust performance even in the most\ncomplex disambiguation scenarios. Through a series of experiments and\ncomparative analysis, we reveal the substantial advantages of our model,\nunderscoring its potential to redefine standards in the field. Beyond its\narchitectural prowess, our architecture excels through experimental\nenrichments, including sophisticated data augmentation and multi-modal training\ntechniques.\n  ARPA's introduction marks a significant milestone in visual word\ndisambiguation, offering a compelling solution that bridges the gap between\nlinguistic and visual modalities. We invite researchers and practitioners to\nexplore the capabilities of our model, envisioning a future where such hybrid\nmodels drive unprecedented advancements in artificial intelligence.",
        "pos": [
            "As NLP models become increasingly integral to decision-making processes, the\nneed for explainability and interpretability has become paramount. In this\nwork, we propose a framework that achieves the aforementioned by generating\nsemantically edited inputs, known as counterfactual interventions, which change\nthe model prediction, thus providing a form of counterfactual explanations for\nthe model. We test our framework on two NLP tasks - binary sentiment\nclassification and topic classification - and show that the generated edits are\ncontrastive, fluent and minimal, while the whole process remains significantly\nfaster that other state-of-the-art counterfactual editors."
        ],
        "neg": []
    },
    {
        "query": "Existing sample-based methods, like influence functions and representer\npoints, measure the importance of a training point by approximating the effect\nof its removal from training. As such, they are skewed towards outliers and\npoints that are very close to the decision boundaries. The explanations\nprovided by these methods are often static and not specific enough for\ndifferent test points. In this paper, we propose a method to generate an\nexplanation in the form of support spectrums which are based on two main ideas:\nthe support sets and a global-to-local importance measure. The support set is\nthe set of training points, in the predicted class, that ``lie in between'' the\ntest point and training points in the other classes. They indicate how well the\ntest point can be distinguished from the points not in the predicted class. The\nglobal-to-local importance measure is obtained by decoupling existing methods\ninto the global and local components which are then used to select the points\nin the support set. Using this method, we are able to generate explanations\nthat are tailored to specific test points. In the experiments, we show the\neffectiveness of the method in image classification and text generation tasks.",
        "pos": [
            "The rapid evolution of large language models (LLMs) represents a substantial\nleap forward in natural language understanding and generation. However,\nalongside these advancements come significant challenges related to the\naccountability and transparency of LLM responses. Reliable source attribution\nis essential to adhering to stringent legal and regulatory standards, including\nthose set forth by the General Data Protection Regulation. Despite the\nwell-established methods in source attribution within the computer vision\ndomain, the application of robust attribution frameworks to natural language\nprocessing remains underexplored. To bridge this gap, we propose a novel and\nversatile TRansformer-based Attribution framework using Contrastive Embeddings\ncalled TRACE that, in particular, exploits contrastive learning for source\nattribution. We perform an extensive empirical evaluation to demonstrate the\nperformance and efficiency of TRACE in various settings and show that TRACE\nsignificantly improves the ability to attribute sources accurately, making it a\nvaluable tool for enhancing the reliability and trustworthiness of LLMs."
        ],
        "neg": []
    },
    {
        "query": "Existing sample-based methods, like influence functions and representer\npoints, measure the importance of a training point by approximating the effect\nof its removal from training. As such, they are skewed towards outliers and\npoints that are very close to the decision boundaries. The explanations\nprovided by these methods are often static and not specific enough for\ndifferent test points. In this paper, we propose a method to generate an\nexplanation in the form of support spectrums which are based on two main ideas:\nthe support sets and a global-to-local importance measure. The support set is\nthe set of training points, in the predicted class, that ``lie in between'' the\ntest point and training points in the other classes. They indicate how well the\ntest point can be distinguished from the points not in the predicted class. The\nglobal-to-local importance measure is obtained by decoupling existing methods\ninto the global and local components which are then used to select the points\nin the support set. Using this method, we are able to generate explanations\nthat are tailored to specific test points. In the experiments, we show the\neffectiveness of the method in image classification and text generation tasks.",
        "pos": [
            "Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real datasets validate our\nproposed model.",
            "The rapid evolution of large language models (LLMs) represents a substantial\nleap forward in natural language understanding and generation. However,\nalongside these advancements come significant challenges related to the\naccountability and transparency of LLM responses. Reliable source attribution\nis essential to adhering to stringent legal and regulatory standards, including\nthose set forth by the General Data Protection Regulation. Despite the\nwell-established methods in source attribution within the computer vision\ndomain, the application of robust attribution frameworks to natural language\nprocessing remains underexplored. To bridge this gap, we propose a novel and\nversatile TRansformer-based Attribution framework using Contrastive Embeddings\ncalled TRACE that, in particular, exploits contrastive learning for source\nattribution. We perform an extensive empirical evaluation to demonstrate the\nperformance and efficiency of TRACE in various settings and show that TRACE\nsignificantly improves the ability to attribute sources accurately, making it a\nvaluable tool for enhancing the reliability and trustworthiness of LLMs.",
            "Protecting intellectual property (IP) of text such as articles and code is\nincreasingly important, especially as sophisticated attacks become possible,\nsuch as paraphrasing by large language models (LLMs) or even unauthorized\ntraining of LLMs on copyrighted text to infringe such IP. However, existing\ntext watermarking methods are not robust enough against such attacks nor\nscalable to millions of users for practical implementation. In this paper, we\npropose Waterfall, the first training-free framework for robust and scalable\ntext watermarking applicable across multiple text types (e.g., articles, code)\nand languages supportable by LLMs, for general text and LLM data provenance.\nWaterfall comprises several key innovations, such as being the first to use LLM\nas paraphrasers for watermarking along with a novel combination of techniques\nthat are surprisingly effective in achieving robust verifiability and\nscalability. We empirically demonstrate that Waterfall achieves significantly\nbetter scalability, robust verifiability, and computational efficiency compared\nto SOTA article-text watermarking methods, and also showed how it could be\ndirectly applied to the watermarking of code."
        ],
        "neg": []
    },
    {
        "query": "Psychological trauma can manifest following various distressing events and is\ncaptured in diverse online contexts. However, studies traditionally focus on a\nsingle aspect of trauma, often neglecting the transferability of findings\nacross different scenarios. We address this gap by training language models\nwith progressing complexity on trauma-related datasets, including\ngenocide-related court data, a Reddit dataset on post-traumatic stress disorder\n(PTSD), counseling conversations, and Incel forum posts. Our results show that\nthe fine-tuned RoBERTa model excels in predicting traumatic events across\ndomains, slightly outperforming large language models like GPT-4. Additionally,\nSLALOM-feature scores and conceptual explanations effectively differentiate and\ncluster trauma-related language, highlighting different trauma aspects and\nidentifying sexual abuse and experiences related to death as a common traumatic\nevent across all datasets. This transferability is crucial as it allows for the\ndevelopment of tools to enhance trauma detection and intervention in diverse\npopulations and settings.",
        "pos": [
            "In the dynamic field of artificial intelligence (AI), the development and\napplication of Large Language Models (LLMs) for text analysis are of\nsignificant academic interest. Despite the promising capabilities of various\nLLMs in conducting qualitative analysis, their use in the humanities and social\nsciences has not been thoroughly examined. This article contributes to the\nemerging literature on LLMs in qualitative analysis by documenting an\nexperimental study involving GPT-4. The study focuses on performing thematic\nanalysis (TA) using a YouTube dataset derived from an EU-funded project, which\nwas previously analyzed by other researchers. This dataset is about the\nrepresentation of Roma migrants in Sweden during 2016, a period marked by the\naftermath of the 2015 refugee crisis and preceding the Swedish national\nelections in 2017. Our study seeks to understand the potential of combining\nhuman intelligence with AI's scalability and efficiency, examining the\nadvantages and limitations of employing LLMs in qualitative research within the\nhumanities and social sciences. Additionally, we discuss future directions for\napplying LLMs in these fields."
        ],
        "neg": []
    },
    {
        "query": "Psychological trauma can manifest following various distressing events and is\ncaptured in diverse online contexts. However, studies traditionally focus on a\nsingle aspect of trauma, often neglecting the transferability of findings\nacross different scenarios. We address this gap by training language models\nwith progressing complexity on trauma-related datasets, including\ngenocide-related court data, a Reddit dataset on post-traumatic stress disorder\n(PTSD), counseling conversations, and Incel forum posts. Our results show that\nthe fine-tuned RoBERTa model excels in predicting traumatic events across\ndomains, slightly outperforming large language models like GPT-4. Additionally,\nSLALOM-feature scores and conceptual explanations effectively differentiate and\ncluster trauma-related language, highlighting different trauma aspects and\nidentifying sexual abuse and experiences related to death as a common traumatic\nevent across all datasets. This transferability is crucial as it allows for the\ndevelopment of tools to enhance trauma detection and intervention in diverse\npopulations and settings.",
        "pos": [
            "LLMs are changing the way humans create and interact with content,\npotentially affecting citizens' political opinions and voting decisions. As\nLLMs increasingly shape our digital information ecosystems, auditing to\nevaluate biases, sycophancy, or steerability has emerged as an active field of\nresearch. In this paper, we evaluate and compare the alignment of six LLMs by\nOpenAI, Anthropic, and Cohere with German party positions and evaluate\nsycophancy based on a prompt experiment. We contribute to evaluating political\nbias and sycophancy in multi-party systems across major commercial LLMs. First,\nwe develop the benchmark dataset GermanPartiesQA based on the Voting Advice\nApplication Wahl-o-Mat covering 10 state and 1 national elections between 2021\nand 2023. In our study, we find a left-green tendency across all examined LLMs.\nWe then conduct our prompt experiment for which we use the benchmark and\nsociodemographic data of leading German parliamentarians to evaluate changes in\nLLMs responses. To differentiate between sycophancy and steerabilty, we use 'I\nam [politician X], ...' and 'You are [politician X], ...' prompts. Against our\nexpectations, we do not observe notable differences between prompting 'I am'\nand 'You are'. While our findings underscore that LLM responses can be\nideologically steered with political personas, they suggest that observed\nchanges in LLM outputs could be better described as personalization to the\ngiven context rather than sycophancy."
        ],
        "neg": []
    },
    {
        "query": "Psychological trauma can manifest following various distressing events and is\ncaptured in diverse online contexts. However, studies traditionally focus on a\nsingle aspect of trauma, often neglecting the transferability of findings\nacross different scenarios. We address this gap by training language models\nwith progressing complexity on trauma-related datasets, including\ngenocide-related court data, a Reddit dataset on post-traumatic stress disorder\n(PTSD), counseling conversations, and Incel forum posts. Our results show that\nthe fine-tuned RoBERTa model excels in predicting traumatic events across\ndomains, slightly outperforming large language models like GPT-4. Additionally,\nSLALOM-feature scores and conceptual explanations effectively differentiate and\ncluster trauma-related language, highlighting different trauma aspects and\nidentifying sexual abuse and experiences related to death as a common traumatic\nevent across all datasets. This transferability is crucial as it allows for the\ndevelopment of tools to enhance trauma detection and intervention in diverse\npopulations and settings.",
        "pos": [
            "Although the spread of behaviors is influenced by many social factors,\nexisting literature tends to study the effects of single factors -- most often,\nproperties of the social network -- on the final cascade. In order to move\ntowards a more integrated view of cascades, this paper offers the first\ncomprehensive investigation into the role of two social factors in the\ndiffusion of 1,337 popular hashtags representing the production of novel\nculture on Twitter: 1) the topology of the Twitter social network and 2)\nperformance of each user's probable demographic identity. Here, we show that\ncascades are best modeled using a combination of network and identity, rather\nthan either factor alone. This combined model best reproduces a composite index\nof ten cascade properties across all 1,337 hashtags. However, there is\nimportant heterogeneity in what social factors are required to reproduce\ndifferent properties of hashtag cascades. For instance, while a combined\nnetwork+identity model best predicts the popularity of cascades, a network-only\nmodel has better performance in predicting cascade growth and an identity-only\nmodel in adopter composition. We are able to predict what type of hashtag is\nbest modeled by each combination of features and use this to further improve\nperformance. Additionally, consistent with prior literature on the combined\nnetwork+identity model most outperforms the single-factor counterfactuals among\nhashtags used for expressing racial or regional identity, stance-taking,\ntalking about sports, or variants of existing cultural trends with very slow-\nor fast-growing communicative need. In sum, our results imply the utility of\nmulti-factor models in predicting cascades, in order to account for the varied\nways in which network, identity, and other social factors play a role in the\ndiffusion of hashtags on Twitter.",
            "This study introduces ValueScope, a framework leveraging language models to\nquantify social norms and values within online communities, grounded in social\nscience perspectives on normative structures. We employ ValueScope to dissect\nand analyze linguistic and stylistic expressions across 13 Reddit communities\ncategorized under gender, politics, science, and finance. Our analysis provides\na quantitative foundation showing that even closely related communities exhibit\nremarkably diverse norms. This diversity supports existing theories and adds a\nnew dimension--community preference--to understanding community interactions.\nValueScope not only delineates differing social norms among communities but\nalso effectively traces their evolution and the influence of significant\nexternal events like the U.S. presidential elections and the emergence of new\nsub-communities. The framework thus highlights the pivotal role of social norms\nin shaping online interactions, presenting a substantial advance in both the\ntheory and application of social norm studies in digital spaces."
        ],
        "neg": []
    },
    {
        "query": "The rapid advancement of Large Language Models (LLMs) and conversational\nassistants necessitates dynamic, scalable, and configurable conversational\ndatasets for training and evaluation. These datasets must accommodate diverse\nuser interaction modes, including text and voice, each presenting unique\nmodeling challenges. Knowledge Graphs (KGs), with their structured and evolving\nnature, offer an ideal foundation for current and precise knowledge. Although\nhuman-curated KG-based conversational datasets exist, they struggle to keep\npace with the rapidly changing user information needs. We present ConvKGYarn, a\nscalable method for generating up-to-date and configurable conversational KGQA\ndatasets. Qualitative psychometric analyses confirm our method can generate\nhigh-quality datasets rivaling a popular conversational KGQA dataset while\noffering it at scale and covering a wide range of human-interaction\nconfigurations. We showcase its utility by testing LLMs on diverse\nconversations - exploring model behavior on conversational KGQA sets with\ndifferent configurations grounded in the same KG fact set. Our results\nhighlight the ability of ConvKGYarn to improve KGQA foundations and evaluate\nparametric knowledge of LLMs, thus offering a robust solution to the constantly\nevolving landscape of conversational assistants.",
        "pos": [
            "Vision--Language Models (VLMs) have demonstrated success across diverse\napplications, yet their potential to assist in relevance judgments remains\nuncertain. This paper assesses the relevance estimation capabilities of VLMs,\nincluding CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc}\nretrieval task tailored for multimedia content creation in a zero-shot fashion.\nPreliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,\nencompassing open-source and closed-source visual-instruction-tuned Large\nLanguage Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared\nto human relevance judgments, surpassing the CLIPScore metric. (2) While\nCLIPScore is strongly preferred, LLMs are less biased towards CLIP-based\nretrieval systems. (3) GPT-4V's score distribution aligns more closely with\nhuman judgments than other models, achieving a Cohen's $\\kappa$ value of around\n0.08, which outperforms CLIPScore at approximately -0.096. These findings\nunderscore the potential of LLM-powered VLMs in enhancing relevance judgments."
        ],
        "neg": []
    },
    {
        "query": "With the increase in the more fluent ad texts automatically created by\nnatural language generation technology, it is in the high demand to verify the\nquality of these creatives in a real-world setting. We propose AdTEC, the first\npublic benchmark to evaluate ad texts in multiple aspects from the perspective\nof practical advertising operations. Our contributions are: (i) Defining five\ntasks for evaluating the quality of ad texts and building a dataset based on\nthe actual operational experience of advertising agencies, which is typically\nkept in-house. (ii) Validating the performance of existing pre-trained language\nmodels (PLMs) and human evaluators on the dataset. (iii) Analyzing the\ncharacteristics and providing challenges of the benchmark. The results show\nthat while PLMs have already reached the practical usage level in several\ntasks, human still outperforms in certain domains, implying that there is\nsignificant room for improvement in such area.",
        "pos": [
            "Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks\nthat outperforms conventional maximum a posterior (MAP) decoding using beam\nsearch by selecting high-quality outputs based on a utility function rather\nthan those with high-probability. Typically, it finds the most suitable\nhypothesis from the set of hypotheses under the sampled pseudo-references. mbrs\nis a library of MBR decoding, which can flexibly combine various metrics,\nalternative expectation estimations, and algorithmic variants. It is designed\nwith a focus on speed measurement and calling count of code blocks,\ntransparency, reproducibility, and extensibility, which are essential for\nresearchers and developers. We published our mbrs as an MIT-licensed\nopen-source project, and the code is available on GitHub.\n  GitHub: https://github.com/naist-nlp/mbrs"
        ],
        "neg": []
    },
    {
        "query": "With the increase in the more fluent ad texts automatically created by\nnatural language generation technology, it is in the high demand to verify the\nquality of these creatives in a real-world setting. We propose AdTEC, the first\npublic benchmark to evaluate ad texts in multiple aspects from the perspective\nof practical advertising operations. Our contributions are: (i) Defining five\ntasks for evaluating the quality of ad texts and building a dataset based on\nthe actual operational experience of advertising agencies, which is typically\nkept in-house. (ii) Validating the performance of existing pre-trained language\nmodels (PLMs) and human evaluators on the dataset. (iii) Analyzing the\ncharacteristics and providing challenges of the benchmark. The results show\nthat while PLMs have already reached the practical usage level in several\ntasks, human still outperforms in certain domains, implying that there is\nsignificant room for improvement in such area.",
        "pos": [
            "Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks\nthat outperforms conventional maximum a posterior (MAP) decoding using beam\nsearch by selecting high-quality outputs based on a utility function rather\nthan those with high-probability. Typically, it finds the most suitable\nhypothesis from the set of hypotheses under the sampled pseudo-references. mbrs\nis a library of MBR decoding, which can flexibly combine various metrics,\nalternative expectation estimations, and algorithmic variants. It is designed\nwith a focus on speed measurement and calling count of code blocks,\ntransparency, reproducibility, and extensibility, which are essential for\nresearchers and developers. We published our mbrs as an MIT-licensed\nopen-source project, and the code is available on GitHub.\n  GitHub: https://github.com/naist-nlp/mbrs",
            "Knowledge Graphs (KGs) are fundamental resources in knowledge-intensive tasks\nin NLP. Due to the limitation of manually creating KGs, KG Completion (KGC) has\nan important role in automatically completing KGs by scoring their links with\nKG Embedding (KGE). To handle many entities in training, KGE relies on Negative\nSampling (NS) loss that can reduce the computational cost by sampling. Since\nthe appearance frequencies for each link are at most one in KGs, sparsity is an\nessential and inevitable problem. The NS loss is no exception. As a solution,\nthe NS loss in KGE relies on smoothing methods like Self-Adversarial Negative\nSampling (SANS) and subsampling. However, it is uncertain what kind of\nsmoothing method is suitable for this purpose due to the lack of theoretical\nunderstanding. This paper provides theoretical interpretations of the smoothing\nmethods for the NS loss in KGE and induces a new NS loss, Triplet Adaptive\nNegative Sampling (TANS), that can cover the characteristics of the\nconventional smoothing methods. Experimental results of TransE, DistMult,\nComplEx, RotatE, HAKE, and HousE on FB15k-237, WN18RR, and YAGO3-10 datasets\nand their sparser subsets show the soundness of our interpretation and\nperformance improvement by our TANS.",
            "Recent work in reframing, within the scope of text style transfer, has so far\nmade use of out-of-context, task-prompted utterances in order to produce\nneutralizing or optimistic reframes. Our work aims to generalize reframing\nbased on the subreddit r/ChangeMyView (CMV). We build a dataset that leverages\nCMV's community's interactions and conventions to identify high-value,\ncommunity-recognized utterances that produce changes of perspective. With this\ndata, we widen the scope of the direction of reframing since the changes in\nperspective do not only occur in neutral or positive directions. We fine tune\ntransformer-based models, make use of a modern LLM to refine our dataset, and\nexplore challenges in the dataset creation and evaluation around this type of\nreframing."
        ],
        "neg": []
    },
    {
        "query": "Hierarchical text classification (HTC) is a special sub-task of multi-label\nclassification (MLC) whose taxonomy is constructed as a tree and each sample is\nassigned with at least one path in the tree. Latest HTC models contain three\nmodules: a text encoder, a structure encoder and a multi-label classification\nhead. Specially, the structure encoder is designed to encode the hierarchy of\ntaxonomy. However, the structure encoder has scale problem. As the taxonomy\nsize increases, the learnable parameters of recent HTC works grow rapidly.\nRecursive regularization is another widely-used method to introduce\nhierarchical information but it has collapse problem and generally relaxed by\nassigning with a small weight (ie. 1e-6). In this paper, we propose a\nHierarchy-aware Light Global model with Hierarchical local conTrastive learning\n(HiLight), a lightweight and efficient global model only consisting of a text\nencoder and a multi-label classification head. We propose a new learning task\nto introduce the hierarchical information, called Hierarchical Local\nContrastive Learning (HiLCL). Extensive experiments are conducted on two\nbenchmark datasets to demonstrate the effectiveness of our model.",
        "pos": [
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "Hierarchical text classification (HTC) is a special sub-task of multi-label\nclassification (MLC) whose taxonomy is constructed as a tree and each sample is\nassigned with at least one path in the tree. Latest HTC models contain three\nmodules: a text encoder, a structure encoder and a multi-label classification\nhead. Specially, the structure encoder is designed to encode the hierarchy of\ntaxonomy. However, the structure encoder has scale problem. As the taxonomy\nsize increases, the learnable parameters of recent HTC works grow rapidly.\nRecursive regularization is another widely-used method to introduce\nhierarchical information but it has collapse problem and generally relaxed by\nassigning with a small weight (ie. 1e-6). In this paper, we propose a\nHierarchy-aware Light Global model with Hierarchical local conTrastive learning\n(HiLight), a lightweight and efficient global model only consisting of a text\nencoder and a multi-label classification head. We propose a new learning task\nto introduce the hierarchical information, called Hierarchical Local\nContrastive Learning (HiLCL). Extensive experiments are conducted on two\nbenchmark datasets to demonstrate the effectiveness of our model.",
        "pos": [
            "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment."
        ],
        "neg": []
    },
    {
        "query": "Test-Time Adaptation (TTA) has emerged as a crucial solution to the domain\nshift challenge, wherein the target environment diverges from the original\ntraining environment. A prime exemplification is TTA for Automatic Speech\nRecognition (ASR), which enhances model performance by leveraging output\nprediction entropy minimization as a self-supervision signal. However, a key\nlimitation of this self-supervision lies in its primary focus on acoustic\nfeatures, with minimal attention to the linguistic properties of the input. To\naddress this gap, we propose Language Informed Test-Time Adaptation (LI-TTA),\nwhich incorporates linguistic insights during TTA for ASR. LI-TTA integrates\ncorrections from an external language model to merge linguistic with acoustic\ninformation by minimizing the CTC loss from the correction alongside the\nstandard TTA loss. With extensive experiments, we show that LI-TTA effectively\nimproves the performance of TTA for ASR in various distribution shift\nsituations.",
        "pos": [
            "Reinforcement Learning from Human Feedback (RLHF) leverages human preference\ndata to train language models to align more closely with human essence. These\nhuman preference data, however, are labeled at the sequence level, creating a\nmismatch between sequence-level preference labels and tokens, which are\nautoregressively generated from the language model. Although several recent\napproaches have tried to provide token-level (i.e., dense) rewards for each\nindividual token, these typically rely on predefined discrete reward values\n(e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying\ndegrees of preference inherent to each token. To address this limitation, we\nintroduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a\ndiscriminator trained to distinguish positive and negative tokens, and the\nconfidence of the discriminator is used to assign continuous rewards to each\ntoken considering the context. Extensive experiments show that our proposed\nTLCR leads to consistent performance improvements over previous sequence-level\nor token-level discrete rewards on open-ended generation benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Test-Time Adaptation (TTA) has emerged as a crucial solution to the domain\nshift challenge, wherein the target environment diverges from the original\ntraining environment. A prime exemplification is TTA for Automatic Speech\nRecognition (ASR), which enhances model performance by leveraging output\nprediction entropy minimization as a self-supervision signal. However, a key\nlimitation of this self-supervision lies in its primary focus on acoustic\nfeatures, with minimal attention to the linguistic properties of the input. To\naddress this gap, we propose Language Informed Test-Time Adaptation (LI-TTA),\nwhich incorporates linguistic insights during TTA for ASR. LI-TTA integrates\ncorrections from an external language model to merge linguistic with acoustic\ninformation by minimizing the CTC loss from the correction alongside the\nstandard TTA loss. With extensive experiments, we show that LI-TTA effectively\nimproves the performance of TTA for ASR in various distribution shift\nsituations.",
        "pos": [
            "Reinforcement Learning from Human Feedback (RLHF) leverages human preference\ndata to train language models to align more closely with human essence. These\nhuman preference data, however, are labeled at the sequence level, creating a\nmismatch between sequence-level preference labels and tokens, which are\nautoregressively generated from the language model. Although several recent\napproaches have tried to provide token-level (i.e., dense) rewards for each\nindividual token, these typically rely on predefined discrete reward values\n(e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying\ndegrees of preference inherent to each token. To address this limitation, we\nintroduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a\ndiscriminator trained to distinguish positive and negative tokens, and the\nconfidence of the discriminator is used to assign continuous rewards to each\ntoken considering the context. Extensive experiments show that our proposed\nTLCR leads to consistent performance improvements over previous sequence-level\nor token-level discrete rewards on open-ended generation benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Test-Time Adaptation (TTA) has emerged as a crucial solution to the domain\nshift challenge, wherein the target environment diverges from the original\ntraining environment. A prime exemplification is TTA for Automatic Speech\nRecognition (ASR), which enhances model performance by leveraging output\nprediction entropy minimization as a self-supervision signal. However, a key\nlimitation of this self-supervision lies in its primary focus on acoustic\nfeatures, with minimal attention to the linguistic properties of the input. To\naddress this gap, we propose Language Informed Test-Time Adaptation (LI-TTA),\nwhich incorporates linguistic insights during TTA for ASR. LI-TTA integrates\ncorrections from an external language model to merge linguistic with acoustic\ninformation by minimizing the CTC loss from the correction alongside the\nstandard TTA loss. With extensive experiments, we show that LI-TTA effectively\nimproves the performance of TTA for ASR in various distribution shift\nsituations.",
        "pos": [
            "Reinforcement Learning from Human Feedback (RLHF) leverages human preference\ndata to train language models to align more closely with human essence. These\nhuman preference data, however, are labeled at the sequence level, creating a\nmismatch between sequence-level preference labels and tokens, which are\nautoregressively generated from the language model. Although several recent\napproaches have tried to provide token-level (i.e., dense) rewards for each\nindividual token, these typically rely on predefined discrete reward values\n(e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying\ndegrees of preference inherent to each token. To address this limitation, we\nintroduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a\ndiscriminator trained to distinguish positive and negative tokens, and the\nconfidence of the discriminator is used to assign continuous rewards to each\ntoken considering the context. Extensive experiments show that our proposed\nTLCR leads to consistent performance improvements over previous sequence-level\nor token-level discrete rewards on open-ended generation benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Large vision-language models (LVLMs) have made significant progress in recent\nyears. While LVLMs exhibit excellent ability in language understanding,\nquestion answering, and conversations of visual inputs, they are prone to\nproducing hallucinations. While several methods are proposed to evaluate the\nhallucinations in LVLMs, most are reference-based and depend on external tools,\nwhich complicates their practical application. To assess the viability of\nalternative methods, it is critical to understand whether the reference-free\napproaches, which do not rely on any external tools, can efficiently detect\nhallucinations. Therefore, we initiate an exploratory study to demonstrate the\neffectiveness of different reference-free solutions in detecting hallucinations\nin LVLMs. In particular, we conduct an extensive study on three kinds of\ntechniques: uncertainty-based, consistency-based, and supervised uncertainty\nquantification methods on four representative LVLMs across two different tasks.\nThe empirical results show that the reference-free approaches are capable of\neffectively detecting non-factual responses in LVLMs, with the supervised\nuncertainty quantification method outperforming the others, achieving the best\nperformance across different settings.",
        "pos": [
            "This paper addresses the need for improved precision in existing\nRetrieval-Augmented Generation (RAG) methods that primarily focus on enhancing\nrecall. We propose a multi-layer knowledge pyramid approach within the RAG\nframework to achieve a better balance between precision and recall. The\nknowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),\nand chunk-based raw text. We employ cross-layer augmentation techniques for\ncomprehensive knowledge coverage and dynamic updates of the Ontology schema and\ninstances. To ensure compactness, we utilize cross-layer filtering methods for\nknowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall\nmodel for retrieval, starting from the top of the pyramid and progressing down\nuntil a confident answer is obtained. We introduce two benchmarks for\ndomain-specific knowledge retrieval, one in the academic domain and the other\nin the financial domain. The effectiveness of the methods has been validated\nthrough comprehensive experiments by outperforming 19 SOTA methods. An\nencouraging observation is that the proposed method has augmented the GPT-4,\nproviding 395\\% F1 gain by improving its performance from 0.1636 to 0.8109.",
            "With an increasing social demand for fine-grained sentiment analysis (SA),\nimplicit sentiment analysis (ISA) poses a significant challenge with the\nabsence of salient cue words in expressions. It necessitates reliable reasoning\nto understand how the sentiment is aroused and thus determine implicit\nsentiments. In the era of Large Language Models (LLMs), Encoder-Decoder (ED)\nLLMs have gained popularity to serve as backbone models for SA applications,\nconsidering impressive text comprehension and reasoning ability among diverse\ntasks. On the other hand, Decoder-only (DO) LLMs exhibit superior natural\nlanguage generation and in-context learning capabilities. However, their\nresponses may contain misleading or inaccurate information. To identify\nimplicit sentiment with reliable reasoning, this study proposes RVISA, a\ntwo-stage reasoning framework that harnesses the generation ability of DO LLMs\nand the reasoning ability of ED LLMs to train an enhanced reasoner.\nSpecifically, we adopt three-hop reasoning prompting to explicitly furnish\nsentiment elements as cues. The generated rationales are utilized to fine-tune\nan ED LLM into a skilled reasoner. Additionally, we develop a straightforward\nyet effective verification mechanism to ensure the reliability of the reasoning\nlearning. We evaluated the proposed method on two benchmark datasets and\nachieved state-of-the-art results in ISA performance."
        ],
        "neg": []
    },
    {
        "query": "Large vision-language models (LVLMs) have made significant progress in recent\nyears. While LVLMs exhibit excellent ability in language understanding,\nquestion answering, and conversations of visual inputs, they are prone to\nproducing hallucinations. While several methods are proposed to evaluate the\nhallucinations in LVLMs, most are reference-based and depend on external tools,\nwhich complicates their practical application. To assess the viability of\nalternative methods, it is critical to understand whether the reference-free\napproaches, which do not rely on any external tools, can efficiently detect\nhallucinations. Therefore, we initiate an exploratory study to demonstrate the\neffectiveness of different reference-free solutions in detecting hallucinations\nin LVLMs. In particular, we conduct an extensive study on three kinds of\ntechniques: uncertainty-based, consistency-based, and supervised uncertainty\nquantification methods on four representative LVLMs across two different tasks.\nThe empirical results show that the reference-free approaches are capable of\neffectively detecting non-factual responses in LVLMs, with the supervised\nuncertainty quantification method outperforming the others, achieving the best\nperformance across different settings.",
        "pos": [
            "The widespread accessibility of large language models (LLMs) to the general\npublic has significantly amplified the dissemination of machine-generated texts\n(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in\ndiscerning the origin of a text (human-authored vs machinegenerated). This\nraises concerns regarding the potential misuse of MGTs, particularly within\neducational and academic domains. In this paper, we present\n$\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.\nIt is able to classify texts into four categories: human-written,\nmachine-generated, machine-written machine-humanized, and human-written\nmachine-polished. Contrary to previous MGT detectors that perform binary\nclassification, introducing two additional categories in LLM-DetectiAIve offers\ninsights into the varying degrees of LLM intervention during the text creation.\nThis might be useful in some domains like education, where any LLM intervention\nis usually prohibited. Experiments show that LLM-DetectAIve can effectively\nidentify the authorship of textual content, proving its usefulness in enhancing\nintegrity in education, academia, and other domains. LLM-DetectAIve is publicly\naccessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video\ndescribing our system is available at https://youtu.be/E8eT_bE7k8c."
        ],
        "neg": []
    },
    {
        "query": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/",
        "pos": [
            "Knowledge Graph Alignment (KGA) aims to integrate knowledge from multiple\nsources to address the limitations of individual Knowledge Graphs (KGs) in\nterms of coverage and depth. However, current KGA models fall short in\nachieving a ``complete'' knowledge graph alignment. Existing models primarily\nemphasize the linkage of cross-graph entities but overlook aligning relations\nacross KGs, thereby providing only a partial solution to KGA. The semantic\ncorrelations embedded in relations are largely overlooked, potentially\nrestricting a comprehensive understanding of cross-KG signals. In this paper,\nwe propose to conceptualize relation alignment as an independent task and\nconduct KGA by decomposing it into two distinct but highly correlated\nsub-tasks: entity alignment and relation alignment. To capture the mutually\nreinforcing correlations between these objectives, we propose a novel\nExpectation-Maximization-based model, EREM, which iteratively optimizes both\nsub-tasks. Experimental results on real-world datasets demonstrate that EREM\nconsistently outperforms state-of-the-art models in both entity alignment and\nrelation alignment tasks."
        ],
        "neg": []
    },
    {
        "query": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/",
        "pos": [
            "Fake news becomes a growing threat to information security and public opinion\nwith the rapid sprawl of media manipulation. Therefore, fake news detection\nattracts widespread attention from academic community. Traditional fake news\ndetection models demonstrate remarkable performance on authenticity binary\nclassification but their ability to reason detailed faked traces based on the\nnews content remains under-explored. Furthermore, due to the lack of external\nknowledge, the performance of existing methods on fact-related news is\nquestionable, leaving their practical implementation unclear. In this paper, we\npropose a new multi-media research topic, namely manipulation reasoning.\nManipulation reasoning aims to reason manipulations based on news content. To\nsupport the research, we introduce a benchmark for fake news detection and\nmanipulation reasoning, referred to as Human-centric and Fact-related Fake News\n(HFFN). The benchmark highlights the centrality of human and the high factual\nrelevance, with detailed manual annotations. HFFN encompasses four realistic\ndomains with fake news samples generated through three manipulation approaches.\nMoreover, a Multi-modal news Detection and Reasoning langUage Model (M-DRUM) is\npresented not only to judge on the authenticity of multi-modal news, but also\nraise analytical reasoning about potential manipulations. On the feature\nextraction level, a cross-attention mechanism is employed to extract\nfine-grained fusion features from multi-modal inputs. On the reasoning level, a\nlarge vision-language model (LVLM) serves as the backbone to facilitate\nfact-related reasoning. A two-stage training framework is deployed to better\nactivate the capacity of identification and reasoning. Comprehensive\nexperiments demonstrate that our model outperforms state-of-the-art (SOTA) fake\nnews detection models and powerful LVLMs like GPT-4 and LLaVA."
        ],
        "neg": []
    },
    {
        "query": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/",
        "pos": [
            "Significant advancements has recently been achieved in the field of\nmulti-modal large language models (MLLMs), demonstrating their remarkable\ncapabilities in understanding and reasoning across diverse tasks. However,\nthese models are often trained for specific tasks and rely on task-specific\ninput-output formats, limiting their applicability to a broader range of tasks.\nThis raises a fundamental question: Can we develop a unified approach to\nrepresent and handle different multi-modal tasks to maximize the\ngeneralizability of MLLMs? In this paper, we propose UnifiedMLLM, a\ncomprehensive model designed to represent various tasks using a unified\nrepresentation. Our model exhibits strong capabilities in comprehending the\nimplicit intent of user instructions and preforming reasoning. In addition to\ngenerating textual responses, our model also outputs task tokens and grounding\ntokens, serving as indicators of task types and task granularity. These outputs\nare subsequently routed through the task router and directed to specific expert\nmodels for task completion. To train our model, we construct a task-specific\ndataset and an 100k multi-task dataset encompassing complex scenarios.\nEmploying a three-stage training strategy, we equip our model with robust\nreasoning and task processing capabilities while preserving its generalization\ncapacity and knowledge reservoir. Extensive experiments showcase the impressive\nperformance of our unified representation approach across various tasks,\nsurpassing existing methodologies. Furthermore, our approach exhibits\nexceptional scalability and generality. Our code, model, and dataset will be\navailable at \\url{https://github.com/lzw-lzw/UnifiedMLLM}."
        ],
        "neg": []
    },
    {
        "query": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/",
        "pos": [
            "Fake news becomes a growing threat to information security and public opinion\nwith the rapid sprawl of media manipulation. Therefore, fake news detection\nattracts widespread attention from academic community. Traditional fake news\ndetection models demonstrate remarkable performance on authenticity binary\nclassification but their ability to reason detailed faked traces based on the\nnews content remains under-explored. Furthermore, due to the lack of external\nknowledge, the performance of existing methods on fact-related news is\nquestionable, leaving their practical implementation unclear. In this paper, we\npropose a new multi-media research topic, namely manipulation reasoning.\nManipulation reasoning aims to reason manipulations based on news content. To\nsupport the research, we introduce a benchmark for fake news detection and\nmanipulation reasoning, referred to as Human-centric and Fact-related Fake News\n(HFFN). The benchmark highlights the centrality of human and the high factual\nrelevance, with detailed manual annotations. HFFN encompasses four realistic\ndomains with fake news samples generated through three manipulation approaches.\nMoreover, a Multi-modal news Detection and Reasoning langUage Model (M-DRUM) is\npresented not only to judge on the authenticity of multi-modal news, but also\nraise analytical reasoning about potential manipulations. On the feature\nextraction level, a cross-attention mechanism is employed to extract\nfine-grained fusion features from multi-modal inputs. On the reasoning level, a\nlarge vision-language model (LVLM) serves as the backbone to facilitate\nfact-related reasoning. A two-stage training framework is deployed to better\nactivate the capacity of identification and reasoning. Comprehensive\nexperiments demonstrate that our model outperforms state-of-the-art (SOTA) fake\nnews detection models and powerful LVLMs like GPT-4 and LLaVA."
        ],
        "neg": []
    },
    {
        "query": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/",
        "pos": [
            "Adapting large language models (LLMs) to new languages typically involves\ncontinual pre-training (CT) followed by supervised fine-tuning (SFT). However,\nthis CT-then-SFT approach struggles with limited data in the context of\nlow-resource languages, failing to balance language modeling and task-solving\ncapabilities. We thus propose model merging as an alternative for low-resource\nlanguages, combining models with distinct capabilities into a single model\nwithout additional training. We use model merging to develop task-solving LLMs\nfor low-resource languages without SFT data in the target languages. Our\nexperiments based on Llama-2-7B demonstrate that model merging effectively\nendows LLMs for low-resource languages with task-solving abilities,\noutperforming CT-then-SFT in scenarios with extremely scarce data. Observing\nperformance saturation in model merging with more training tokens, we further\nanalyze the merging process and introduce a slack variable to the model merging\nalgorithm to mitigate the loss of important parameters, thereby enhancing\nperformance. We hope that model merging can benefit more human languages\nsuffering from data scarcity with its higher data efficiency."
        ],
        "neg": []
    },
    {
        "query": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/",
        "pos": [
            "Fake news becomes a growing threat to information security and public opinion\nwith the rapid sprawl of media manipulation. Therefore, fake news detection\nattracts widespread attention from academic community. Traditional fake news\ndetection models demonstrate remarkable performance on authenticity binary\nclassification but their ability to reason detailed faked traces based on the\nnews content remains under-explored. Furthermore, due to the lack of external\nknowledge, the performance of existing methods on fact-related news is\nquestionable, leaving their practical implementation unclear. In this paper, we\npropose a new multi-media research topic, namely manipulation reasoning.\nManipulation reasoning aims to reason manipulations based on news content. To\nsupport the research, we introduce a benchmark for fake news detection and\nmanipulation reasoning, referred to as Human-centric and Fact-related Fake News\n(HFFN). The benchmark highlights the centrality of human and the high factual\nrelevance, with detailed manual annotations. HFFN encompasses four realistic\ndomains with fake news samples generated through three manipulation approaches.\nMoreover, a Multi-modal news Detection and Reasoning langUage Model (M-DRUM) is\npresented not only to judge on the authenticity of multi-modal news, but also\nraise analytical reasoning about potential manipulations. On the feature\nextraction level, a cross-attention mechanism is employed to extract\nfine-grained fusion features from multi-modal inputs. On the reasoning level, a\nlarge vision-language model (LVLM) serves as the backbone to facilitate\nfact-related reasoning. A two-stage training framework is deployed to better\nactivate the capacity of identification and reasoning. Comprehensive\nexperiments demonstrate that our model outperforms state-of-the-art (SOTA) fake\nnews detection models and powerful LVLMs like GPT-4 and LLaVA."
        ],
        "neg": []
    },
    {
        "query": "Previous studies reveal that Electronic Health Records (EHR), which have been\nwidely adopted in the U.S. to allow patients to access their personal medical\ninformation, do not have high readability to patients due to the prevalence of\nmedical jargon. Tailoring medical notes to individual comprehension by\nidentifying jargon that is difficult for each person will enhance the utility\nof generative models. We present the first quantitative analysis to measure the\nimpact of role-playing in LLM in medical term extraction. By comparing the\nresults of Mechanical Turk workers over 20 sentences, our study demonstrates\nthat LLM role-playing improves F1 scores in 95% of cases across 14 different\nsocio-demographic backgrounds. Furthermore, applying role-playing with\nin-context learning outperformed the previous state-of-the-art models. Our\nresearch showed that ChatGPT can improve traditional medical term extraction\nsystems by utilizing role-play to deliver personalized patient education, a\npotential that previous models had not achieved.",
        "pos": [
            "Directly learning from examples of random difficulty levels is often\nchallenging for both humans and machine learning models. A more effective\nstrategy involves exposing learners to examples in a progressive order, from\neasy to difficult. Curriculum Learning (CL) has been proposed to implement this\nstrategy in machine learning model training. However, two key challenges\npersist in CL framework design: defining the difficulty of training data and\ndetermining the appropriate amount of data to input at each training step. This\npaper presents a Psychology-based Unified Dynamic Framework for Curriculum\nLearning (PUDF), drawing inspiration from psychometrics. We quantify the\ndifficulty of training data by applying Item Response Theory (IRT) to responses\nfrom Artificial Crowds (AC). This theory-driven IRT-AC approach leads to global\n(i.e., model-independent) and interpretable difficulty values. Leveraging IRT,\nwe propose a Dynamic Data Selection via Model Ability Estimation (DDS-MAE)\nstrategy to schedule the appropriate amount of data during model training.\nSince our difficulty labeling and model ability estimation are based on a\nconsistent theory, namely IRT, their values are comparable within the same\nscope, potentially leading to a faster convergence compared to the other CL\nmethods. Experimental results demonstrate that fine-tuning pre-trained language\nmodels with PUDF enhances their performance on the GLUE benchmark. Moreover,\nPUDF surpasses other state-of-the-art (SOTA) CL methods on the GLUE benchmark.\nWe further explore the components of PUDF, namely the difficulty measurer\n(IRT-AC) and the training scheduler (DDS-MAE) qualitatively and quantitatively.\nLastly, we conduct an ablation study to clarify which components of PUDF\ncontribute to faster convergence and higher accuracy."
        ],
        "neg": []
    },
    {
        "query": "Previous studies reveal that Electronic Health Records (EHR), which have been\nwidely adopted in the U.S. to allow patients to access their personal medical\ninformation, do not have high readability to patients due to the prevalence of\nmedical jargon. Tailoring medical notes to individual comprehension by\nidentifying jargon that is difficult for each person will enhance the utility\nof generative models. We present the first quantitative analysis to measure the\nimpact of role-playing in LLM in medical term extraction. By comparing the\nresults of Mechanical Turk workers over 20 sentences, our study demonstrates\nthat LLM role-playing improves F1 scores in 95% of cases across 14 different\nsocio-demographic backgrounds. Furthermore, applying role-playing with\nin-context learning outperformed the previous state-of-the-art models. Our\nresearch showed that ChatGPT can improve traditional medical term extraction\nsystems by utilizing role-play to deliver personalized patient education, a\npotential that previous models had not achieved.",
        "pos": [
            "Directly learning from examples of random difficulty levels is often\nchallenging for both humans and machine learning models. A more effective\nstrategy involves exposing learners to examples in a progressive order, from\neasy to difficult. Curriculum Learning (CL) has been proposed to implement this\nstrategy in machine learning model training. However, two key challenges\npersist in CL framework design: defining the difficulty of training data and\ndetermining the appropriate amount of data to input at each training step. This\npaper presents a Psychology-based Unified Dynamic Framework for Curriculum\nLearning (PUDF), drawing inspiration from psychometrics. We quantify the\ndifficulty of training data by applying Item Response Theory (IRT) to responses\nfrom Artificial Crowds (AC). This theory-driven IRT-AC approach leads to global\n(i.e., model-independent) and interpretable difficulty values. Leveraging IRT,\nwe propose a Dynamic Data Selection via Model Ability Estimation (DDS-MAE)\nstrategy to schedule the appropriate amount of data during model training.\nSince our difficulty labeling and model ability estimation are based on a\nconsistent theory, namely IRT, their values are comparable within the same\nscope, potentially leading to a faster convergence compared to the other CL\nmethods. Experimental results demonstrate that fine-tuning pre-trained language\nmodels with PUDF enhances their performance on the GLUE benchmark. Moreover,\nPUDF surpasses other state-of-the-art (SOTA) CL methods on the GLUE benchmark.\nWe further explore the components of PUDF, namely the difficulty measurer\n(IRT-AC) and the training scheduler (DDS-MAE) qualitatively and quantitatively.\nLastly, we conduct an ablation study to clarify which components of PUDF\ncontribute to faster convergence and higher accuracy."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-Augmented Large Language Models (RALMs) have made significant\nstrides in enhancing the accuracy of generated responses.However, existing\nresearch often overlooks the data quality issues within retrieval results,\noften caused by inaccurate existing vector-distance-based retrieval methods.We\npropose to boost the precision of RALMs' answers from a data quality\nperspective through the Context-Driven Index Trimming (CDIT) framework, where\nContext Matching Dependencies (CMDs) are employed as logical data quality rules\nto capture and regulate the consistency between retrieved contexts.Based on the\nsemantic comprehension capabilities of Large Language Models (LLMs), CDIT can\neffectively identify and discard retrieval results that are inconsistent with\nthe query context and further modify indexes in the database, thereby improving\nanswer quality.Experiments demonstrate on challenging question-answering\ntasks.Also, the flexibility of CDIT is verified through its compatibility with\nvarious language models and indexing methods, which offers a promising approach\nto bolster RALMs' data quality and retrieval precision jointly.",
        "pos": [
            "Despite the success of integrating large language models into the development\nof conversational systems, many studies have shown the effectiveness of\nretrieving and augmenting external knowledge for informative responses. Hence,\nmany existing studies commonly assume the always need for Retrieval Augmented\nGeneration (RAG) in a conversational system without explicit control. This\nraises a research question about such a necessity. In this study, we propose to\ninvestigate the need for each turn of system response to be augmented with\nexternal knowledge. In particular, by leveraging human judgements on the binary\nchoice of adaptive augmentation, we develop RAGate, a gating model, which\nmodels conversation context and relevant inputs to predict if a conversational\nsystem requires RAG for improved responses. We conduct extensive experiments on\ndevising and applying RAGate to conversational models and well-rounded analyses\nof different conversational scenarios. Our experimental results and analysis\nindicate the effective application of RAGate in RAG-based conversational\nsystems in identifying system responses for appropriate RAG with high-quality\nresponses and a high generation confidence. This study also identifies the\ncorrelation between the generation's confidence level and the relevance of the\naugmented knowledge."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-Augmented Large Language Models (RALMs) have made significant\nstrides in enhancing the accuracy of generated responses.However, existing\nresearch often overlooks the data quality issues within retrieval results,\noften caused by inaccurate existing vector-distance-based retrieval methods.We\npropose to boost the precision of RALMs' answers from a data quality\nperspective through the Context-Driven Index Trimming (CDIT) framework, where\nContext Matching Dependencies (CMDs) are employed as logical data quality rules\nto capture and regulate the consistency between retrieved contexts.Based on the\nsemantic comprehension capabilities of Large Language Models (LLMs), CDIT can\neffectively identify and discard retrieval results that are inconsistent with\nthe query context and further modify indexes in the database, thereby improving\nanswer quality.Experiments demonstrate on challenging question-answering\ntasks.Also, the flexibility of CDIT is verified through its compatibility with\nvarious language models and indexing methods, which offers a promising approach\nto bolster RALMs' data quality and retrieval precision jointly.",
        "pos": [
            "The recognition of named entities in visually-rich documents (VrD-NER) plays\na critical role in various real-world scenarios and applications. However, the\nresearch in VrD-NER faces three major challenges: complex document layouts,\nincorrect reading orders, and unsuitable task formulations. To address these\nchallenges, we propose a query-aware entity extraction head, namely UNER, to\ncollaborate with existing multi-modal document transformers to develop more\nrobust VrD-NER models. The UNER head considers the VrD-NER task as a\ncombination of sequence labeling and reading order prediction, effectively\naddressing the issues of discontinuous entities in documents. Experimental\nevaluations on diverse datasets demonstrate the effectiveness of UNER in\nimproving entity extraction performance. Moreover, the UNER head enables a\nsupervised pre-training stage on various VrD-NER datasets to enhance the\ndocument transformer backbones and exhibits substantial knowledge transfer from\nthe pre-training stage to the fine-tuning stage. By incorporating universal\nlayout understanding, a pre-trained UNER-based model demonstrates significant\nadvantages in few-shot and cross-linguistic scenarios and exhibits zero-shot\nentity extraction abilities."
        ],
        "neg": []
    },
    {
        "query": "Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the \\textit{most\ncomprehensive support} for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.",
        "pos": [
            "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce CodexGraph, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, CodexGraph enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess CodexGraph using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, CodexGraph demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent."
        ],
        "neg": []
    },
    {
        "query": "Gaze estimation is pivotal in human scene comprehension tasks, particularly\nin medical diagnostic analysis. Eye-tracking technology facilitates the\nrecording of physicians' ocular movements during image interpretation, thereby\nelucidating their visual attention patterns and information-processing\nstrategies. In this paper, we initially define the context-aware gaze\nestimation problem in medical radiology report settings. To understand the\nattention allocation and cognitive behavior of radiologists during the medical\nimage interpretation process, we propose a context-aware Gaze EstiMation (GEM)\nnetwork that utilizes eye gaze data collected from radiologists to simulate\ntheir visual search behavior patterns throughout the image interpretation\nprocess. It consists of a context-awareness module, visual behavior graph\nconstruction, and visual behavior matching. Within the context-awareness\nmodule, we achieve intricate multimodal registration by establishing\nconnections between medical reports and images. Subsequently, for a more\naccurate simulation of genuine visual search behavior patterns, we introduce a\nvisual behavior graph structure, capturing such behavior through high-order\nrelationships (edges) between gaze points (nodes). To maintain the authenticity\nof visual behavior, we devise a visual behavior-matching approach, adjusting\nthe high-order relationships between them by matching the graph constructed\nfrom real and estimated gaze points. Extensive experiments on four publicly\navailable datasets demonstrate the superiority of GEM over existing methods and\nits strong generalizability, which also provides a new direction for the\neffective utilization of diverse modalities in medical image interpretation and\nenhances the interpretability of models in the field of medical imaging.\nhttps://github.com/Tiger-SN/GEM",
        "pos": [
            "Despite the advanced intelligence abilities of large language models (LLMs)\nin various applications, they still face significant computational and storage\ndemands. Knowledge Distillation (KD) has emerged as an effective strategy to\nimprove the performance of a smaller LLM (i.e., the student model) by\ntransferring knowledge from a high-performing LLM (i.e., the teacher model).\nPrevailing techniques in LLM distillation typically use a black-box model API\nto generate high-quality pretrained and aligned datasets, or utilize white-box\ndistillation by altering the loss function to better transfer knowledge from\nthe teacher LLM. However, these methods ignore the knowledge differences\nbetween the student and teacher LLMs across domains. This results in excessive\nfocus on domains with minimal performance gaps and insufficient attention to\ndomains with large gaps, reducing overall performance. In this paper, we\nintroduce a new LLM distillation framework called DDK, which dynamically\nadjusts the composition of the distillation dataset in a smooth manner\naccording to the domain performance differences between the teacher and student\nmodels, making the distillation process more stable and effective. Extensive\nevaluations show that DDK significantly improves the performance of student\nmodels, outperforming both continuously pretrained baselines and existing\nknowledge distillation methods by a large margin."
        ],
        "neg": []
    },
    {
        "query": "Inspired by the recent advancements of Large Language Models (LLMs) in NLP\ntasks, there's growing interest in applying LLMs to graph-related tasks. This\nstudy delves into the capabilities of instruction-following LLMs for engaging\nwith real-world graphs, aiming to offer empirical insights into how LLMs can\neffectively interact with graphs and generalize across graph tasks. We begin by\nconstructing a dataset designed for instruction tuning, which comprises a\ndiverse collection of 79 graph-related tasks from academic and e-commerce\ndomains, featuring 44,240 training instances and 18,960 test samples. Utilizing\nthis benchmark, our initial investigation focuses on identifying the optimal\ngraph representation that serves as a conduit for LLMs to understand complex\ngraph structures. Our findings indicate that JSON format for graph\nrepresentation consistently outperforms natural language and code formats\nacross various LLMs and graph types. Furthermore, we examine the key factors\nthat influence the generalization abilities of instruction-tuned LLMs by\nevaluating their performance on both in-domain and out-of-domain graph tasks.",
        "pos": [
            "Episodic structures are inherently interpretable and adaptable to evolving\nlarge-scale key events. However, state-of-the-art automatic event detection\nmethods overlook event episodes and, therefore, struggle with these crucial\ncharacteristics. This paper introduces a novel task, episode detection, aimed\nat identifying episodes from a news corpus containing key event articles. An\nepisode describes a cohesive cluster of core entities (e.g., \"protesters\",\n\"police\") performing actions at a specific time and location. Furthermore, an\nepisode is a significant part of a larger group of episodes under a particular\nkey event. Automatically detecting episodes is challenging because, unlike key\nevents and atomic actions, we cannot rely on explicit mentions of times and\nlocations to distinguish between episodes or use semantic similarity to merge\ninconsistent episode co-references. To address these challenges, we introduce\nEpiMine, an unsupervised episode detection framework that (1) automatically\nidentifies the most salient, key-event-relevant terms and segments, (2)\ndetermines candidate episodes in an article based on natural episodic\npartitions estimated through shifts in discriminative term combinations, and\n(3) refines and forms final episode clusters using large language model-based\nreasoning on the candidate episodes. We construct three diverse, real-world\nevent datasets annotated at the episode level. EpiMine outperforms all\nbaselines on these datasets by an average 59.2% increase across all metrics.",
            "Language models are known to encode a great amount of factual knowledge\nthrough pretraining. However, such knowledge might be insufficient to cater to\nuser requests, requiring the model to integrate external knowledge sources and\nadhere to user-provided specifications. When answering questions about ongoing\nevents, the model should use recent news articles to update its response; when\nasked to provide recommendations, the model should prioritize user\nspecifications over retrieved product reviews; when some facts are edited in\nthe model, the updated facts should override all prior knowledge learned by the\nmodel even if they are conflicting. In all of the cases above, the model faces\na decision between its own parametric knowledge, (retrieved) contextual\nknowledge, and user instruction knowledge. In this paper, we (1) unify such\nsettings into the problem of knowledge preference and define a three-level\npreference hierarchy over these knowledge sources; (2) compile a collection of\nexisting datasets IfQA, MQuAKE, and MRQA covering a combination of settings\n(with/without user specifications, with/without context documents) to\nsystematically evaluate how well models obey the intended knowledge preference;\nand (3) propose a dataset synthesis method that composes diverse\nquestion-answer pairs with user assumptions and related context to directly\nfine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a\n7B model, fine-tuned on only a few thousand examples automatically generated by\nour proposed method, effectively achieves superior performance (more than 18%\nimprovement across all evaluation benchmarks) in adhering to the desired\nknowledge preference hierarchy."
        ],
        "neg": []
    },
    {
        "query": "Inspired by the recent advancements of Large Language Models (LLMs) in NLP\ntasks, there's growing interest in applying LLMs to graph-related tasks. This\nstudy delves into the capabilities of instruction-following LLMs for engaging\nwith real-world graphs, aiming to offer empirical insights into how LLMs can\neffectively interact with graphs and generalize across graph tasks. We begin by\nconstructing a dataset designed for instruction tuning, which comprises a\ndiverse collection of 79 graph-related tasks from academic and e-commerce\ndomains, featuring 44,240 training instances and 18,960 test samples. Utilizing\nthis benchmark, our initial investigation focuses on identifying the optimal\ngraph representation that serves as a conduit for LLMs to understand complex\ngraph structures. Our findings indicate that JSON format for graph\nrepresentation consistently outperforms natural language and code formats\nacross various LLMs and graph types. Furthermore, we examine the key factors\nthat influence the generalization abilities of instruction-tuned LLMs by\nevaluating their performance on both in-domain and out-of-domain graph tasks.",
        "pos": [
            "Converting different modalities into general text, serving as input prompts\nfor large language models (LLMs), is a common method to align multimodal models\nwhen there is limited pairwise data. This text-centric approach leverages the\nunique properties of text as a modality space, transforming diverse inputs into\na unified textual representation. This enables downstream models to effectively\ninterpret various modal inputs. This study assesses the quality and robustness\nof multimodal representations in the presence of missing entries, noise, or\nabsent modalities, revealing that current text-centric alignment methods\ncompromise downstream robustness. To address this issue, we propose a new\ntext-centric approach that achieves superior robustness compared to previous\nmethods across various modalities in different settings. Our findings highlight\nthe potential of this approach to enhance the robustness and adaptability of\nmultimodal representations, offering a promising solution for dynamic and\nreal-world applications."
        ],
        "neg": []
    },
    {
        "query": "Inspired by the recent advancements of Large Language Models (LLMs) in NLP\ntasks, there's growing interest in applying LLMs to graph-related tasks. This\nstudy delves into the capabilities of instruction-following LLMs for engaging\nwith real-world graphs, aiming to offer empirical insights into how LLMs can\neffectively interact with graphs and generalize across graph tasks. We begin by\nconstructing a dataset designed for instruction tuning, which comprises a\ndiverse collection of 79 graph-related tasks from academic and e-commerce\ndomains, featuring 44,240 training instances and 18,960 test samples. Utilizing\nthis benchmark, our initial investigation focuses on identifying the optimal\ngraph representation that serves as a conduit for LLMs to understand complex\ngraph structures. Our findings indicate that JSON format for graph\nrepresentation consistently outperforms natural language and code formats\nacross various LLMs and graph types. Furthermore, we examine the key factors\nthat influence the generalization abilities of instruction-tuned LLMs by\nevaluating their performance on both in-domain and out-of-domain graph tasks.",
        "pos": [
            "Episodic structures are inherently interpretable and adaptable to evolving\nlarge-scale key events. However, state-of-the-art automatic event detection\nmethods overlook event episodes and, therefore, struggle with these crucial\ncharacteristics. This paper introduces a novel task, episode detection, aimed\nat identifying episodes from a news corpus containing key event articles. An\nepisode describes a cohesive cluster of core entities (e.g., \"protesters\",\n\"police\") performing actions at a specific time and location. Furthermore, an\nepisode is a significant part of a larger group of episodes under a particular\nkey event. Automatically detecting episodes is challenging because, unlike key\nevents and atomic actions, we cannot rely on explicit mentions of times and\nlocations to distinguish between episodes or use semantic similarity to merge\ninconsistent episode co-references. To address these challenges, we introduce\nEpiMine, an unsupervised episode detection framework that (1) automatically\nidentifies the most salient, key-event-relevant terms and segments, (2)\ndetermines candidate episodes in an article based on natural episodic\npartitions estimated through shifts in discriminative term combinations, and\n(3) refines and forms final episode clusters using large language model-based\nreasoning on the candidate episodes. We construct three diverse, real-world\nevent datasets annotated at the episode level. EpiMine outperforms all\nbaselines on these datasets by an average 59.2% increase across all metrics.",
            "Language models are known to encode a great amount of factual knowledge\nthrough pretraining. However, such knowledge might be insufficient to cater to\nuser requests, requiring the model to integrate external knowledge sources and\nadhere to user-provided specifications. When answering questions about ongoing\nevents, the model should use recent news articles to update its response; when\nasked to provide recommendations, the model should prioritize user\nspecifications over retrieved product reviews; when some facts are edited in\nthe model, the updated facts should override all prior knowledge learned by the\nmodel even if they are conflicting. In all of the cases above, the model faces\na decision between its own parametric knowledge, (retrieved) contextual\nknowledge, and user instruction knowledge. In this paper, we (1) unify such\nsettings into the problem of knowledge preference and define a three-level\npreference hierarchy over these knowledge sources; (2) compile a collection of\nexisting datasets IfQA, MQuAKE, and MRQA covering a combination of settings\n(with/without user specifications, with/without context documents) to\nsystematically evaluate how well models obey the intended knowledge preference;\nand (3) propose a dataset synthesis method that composes diverse\nquestion-answer pairs with user assumptions and related context to directly\nfine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a\n7B model, fine-tuned on only a few thousand examples automatically generated by\nour proposed method, effectively achieves superior performance (more than 18%\nimprovement across all evaluation benchmarks) in adhering to the desired\nknowledge preference hierarchy."
        ],
        "neg": []
    },
    {
        "query": "Metaphor Components Identification (MCI) contributes to enhancing machine\nunderstanding of metaphors, thereby advancing downstream natural language\nprocessing tasks. However, the complexity, diversity, and dependency on context\nand background knowledge pose significant challenges for MCI. Large language\nmodels (LLMs) offer new avenues for accurate comprehension of complex natural\nlanguage texts due to their strong semantic analysis and extensive commonsense\nknowledge. In this research, a new LLM-based framework is proposed, named\nLinguistics-aware In-context Learning with Data Augmentation (LaiDA).\nSpecifically, ChatGPT and supervised fine-tuning are utilized to tailor a\nhigh-quality dataset. LaiDA incorporates a simile dataset for pre-training. A\ngraph attention network encoder generates linguistically rich feature\nrepresentations to retrieve similar examples. Subsequently, LLM is fine-tuned\nwith prompts that integrate linguistically similar examples. LaiDA ranked 2nd\nin Subtask 2 of NLPCC2024 Shared Task 9, demonstrating its effectiveness. Code\nand data are available at https://github.com/WXLJZ/LaiDA.",
        "pos": [
            "The DimABSA task requires fine-grained sentiment intensity prediction for\nrestaurant reviews, including scores for Valence and Arousal dimensions for\neach Aspect Term. In this study, we propose a Coarse-to-Fine In-context\nLearning(CFICL) method based on the Baichuan2-7B model for the DimABSA task in\nthe SIGHAN 2024 workshop. Our method improves prediction accuracy through a\ntwo-stage optimization process. In the first stage, we use fixed in-context\nexamples and prompt templates to enhance the model's sentiment recognition\ncapability and provide initial predictions for the test data. In the second\nstage, we encode the Opinion field using BERT and select the most similar\ntraining data as new in-context examples based on similarity. These examples\ninclude the Opinion field and its scores, as well as related opinion words and\ntheir average scores. By filtering for sentiment polarity, we ensure that the\nexamples are consistent with the test data. Our method significantly improves\nprediction accuracy and consistency by effectively utilizing training data and\noptimizing in-context examples, as validated by experimental results."
        ],
        "neg": []
    },
    {
        "query": "Data-driven storytelling is a powerful method for conveying insights by\ncombining narrative techniques with visualizations and text. These stories\nintegrate visual aids, such as highlighted bars and lines in charts, along with\ntextual annotations explaining insights. However, creating such stories\nrequires a deep understanding of the data and meticulous narrative planning,\noften necessitating human intervention, which can be time-consuming and\nmentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,\ntheir ability to generate coherent and comprehensive data stories remains\nunderexplored. In this work, we introduce a novel task for data story\ngeneration and a benchmark containing 1,449 stories from diverse sources. To\naddress the challenges of crafting coherent data stories, we propose a\nmultiagent framework employing two LLM agents designed to replicate the human\nstorytelling process: one for understanding and describing the data\n(Reflection), generating the outline, and narration, and another for\nverification at each intermediary step. While our agentic framework generally\noutperforms non-agentic counterparts in both model-based and human evaluations,\nthe results also reveal unique challenges in data story generation.",
        "pos": [
            "Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust."
        ],
        "neg": []
    },
    {
        "query": "Data-driven storytelling is a powerful method for conveying insights by\ncombining narrative techniques with visualizations and text. These stories\nintegrate visual aids, such as highlighted bars and lines in charts, along with\ntextual annotations explaining insights. However, creating such stories\nrequires a deep understanding of the data and meticulous narrative planning,\noften necessitating human intervention, which can be time-consuming and\nmentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,\ntheir ability to generate coherent and comprehensive data stories remains\nunderexplored. In this work, we introduce a novel task for data story\ngeneration and a benchmark containing 1,449 stories from diverse sources. To\naddress the challenges of crafting coherent data stories, we propose a\nmultiagent framework employing two LLM agents designed to replicate the human\nstorytelling process: one for understanding and describing the data\n(Reflection), generating the outline, and narration, and another for\nverification at each intermediary step. While our agentic framework generally\noutperforms non-agentic counterparts in both model-based and human evaluations,\nthe results also reveal unique challenges in data story generation.",
        "pos": [
            "Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust."
        ],
        "neg": []
    },
    {
        "query": "Data-driven storytelling is a powerful method for conveying insights by\ncombining narrative techniques with visualizations and text. These stories\nintegrate visual aids, such as highlighted bars and lines in charts, along with\ntextual annotations explaining insights. However, creating such stories\nrequires a deep understanding of the data and meticulous narrative planning,\noften necessitating human intervention, which can be time-consuming and\nmentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,\ntheir ability to generate coherent and comprehensive data stories remains\nunderexplored. In this work, we introduce a novel task for data story\ngeneration and a benchmark containing 1,449 stories from diverse sources. To\naddress the challenges of crafting coherent data stories, we propose a\nmultiagent framework employing two LLM agents designed to replicate the human\nstorytelling process: one for understanding and describing the data\n(Reflection), generating the outline, and narration, and another for\nverification at each intermediary step. While our agentic framework generally\noutperforms non-agentic counterparts in both model-based and human evaluations,\nthe results also reveal unique challenges in data story generation.",
        "pos": [
            "Given the ubiquity of charts as a data analysis, visualization, and\ndecision-making tool across industries and sciences, there has been a growing\ninterest in developing pre-trained foundation models as well as general purpose\ninstruction-tuned models for chart understanding and reasoning. However,\nexisting methods suffer crucial drawbacks across two critical axes affecting\nthe performance of chart representation models: they are trained on data\ngenerated from underlying data tables of the charts, ignoring the visual trends\nand patterns in chart images, and use weakly aligned vision-language backbone\nmodels for domain-specific training, limiting their generalizability when\nencountering charts in the wild. We address these important drawbacks and\nintroduce ChartGemma, a novel chart understanding and reasoning model developed\nover PaliGemma. Rather than relying on underlying data tables, ChartGemma is\ntrained on instruction-tuning data generated directly from chart images, thus\ncapturing both high-level trends and low-level visual information from a\ndiverse set of charts. Our simple approach achieves state-of-the-art results\nacross $5$ benchmarks spanning chart summarization, question answering, and\nfact-checking, and our elaborate qualitative studies on real-world charts show\nthat ChartGemma generates more realistic and factually correct summaries\ncompared to its contemporaries. We release the code, model checkpoints,\ndataset, and demos at https://github.com/vis-nlp/ChartGemma.",
            "Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust."
        ],
        "neg": []
    },
    {
        "query": "Data-driven storytelling is a powerful method for conveying insights by\ncombining narrative techniques with visualizations and text. These stories\nintegrate visual aids, such as highlighted bars and lines in charts, along with\ntextual annotations explaining insights. However, creating such stories\nrequires a deep understanding of the data and meticulous narrative planning,\noften necessitating human intervention, which can be time-consuming and\nmentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,\ntheir ability to generate coherent and comprehensive data stories remains\nunderexplored. In this work, we introduce a novel task for data story\ngeneration and a benchmark containing 1,449 stories from diverse sources. To\naddress the challenges of crafting coherent data stories, we propose a\nmultiagent framework employing two LLM agents designed to replicate the human\nstorytelling process: one for understanding and describing the data\n(Reflection), generating the outline, and narration, and another for\nverification at each intermediary step. While our agentic framework generally\noutperforms non-agentic counterparts in both model-based and human evaluations,\nthe results also reveal unique challenges in data story generation.",
        "pos": [
            "Given the ubiquity of charts as a data analysis, visualization, and\ndecision-making tool across industries and sciences, there has been a growing\ninterest in developing pre-trained foundation models as well as general purpose\ninstruction-tuned models for chart understanding and reasoning. However,\nexisting methods suffer crucial drawbacks across two critical axes affecting\nthe performance of chart representation models: they are trained on data\ngenerated from underlying data tables of the charts, ignoring the visual trends\nand patterns in chart images, and use weakly aligned vision-language backbone\nmodels for domain-specific training, limiting their generalizability when\nencountering charts in the wild. We address these important drawbacks and\nintroduce ChartGemma, a novel chart understanding and reasoning model developed\nover PaliGemma. Rather than relying on underlying data tables, ChartGemma is\ntrained on instruction-tuning data generated directly from chart images, thus\ncapturing both high-level trends and low-level visual information from a\ndiverse set of charts. Our simple approach achieves state-of-the-art results\nacross $5$ benchmarks spanning chart summarization, question answering, and\nfact-checking, and our elaborate qualitative studies on real-world charts show\nthat ChartGemma generates more realistic and factually correct summaries\ncompared to its contemporaries. We release the code, model checkpoints,\ndataset, and demos at https://github.com/vis-nlp/ChartGemma.",
            "Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust."
        ],
        "neg": []
    },
    {
        "query": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. To the best of our knowledge, we are the first to\nexploit non-awakening interaction and audio interrupt in MLLM. VITA is the\nfirst step for the open-source community to explore the seamless integration of\nmultimodal understanding and interaction. While there is still lots of work to\nbe done on VITA to get close to close-source counterparts, we hope that its\nrole as a pioneer can serve as a cornerstone for subsequent research. Project\nPage: https://vita-home.github.io.",
        "pos": [
            "Large language models (LLMs) have become powerful tools for advancing natural\nlanguage processing applications in the financial industry. However, existing\nfinancial LLMs often face challenges such as hallucinations or superficial\nparameter training, resulting in suboptimal performance, particularly in\nfinancial computing and machine reading comprehension (MRC). To address these\nissues, we propose a novel large language model specifically designed for the\nChinese financial domain, named SNFinLLM. SNFinLLM excels in domain-specific\ntasks such as answering questions, summarizing financial research reports,\nanalyzing sentiment, and executing financial calculations. We then perform the\nsupervised fine-tuning (SFT) to enhance the model's proficiency across various\nfinancial domains. Specifically, we gather extensive financial data and create\na high-quality instruction dataset composed of news articles, professional\npapers, and research reports of finance domain. Utilizing both domain-specific\nand general datasets, we proceed with continuous pre-training on an established\nopen-source base model, resulting in SNFinLLM-base. Following this, we engage\nin supervised fine-tuning (SFT) to bolster the model's capability across\nmultiple financial tasks. Crucially, we employ a straightforward Direct\nPreference Optimization (DPO) method to better align the model with human\npreferences. Extensive experiments conducted on finance benchmarks and our\nevaluation dataset demonstrate that SNFinLLM markedly outperforms other\nstate-of-the-art financial language models. For more details, check out our\ndemo video here: https://www.youtube.com/watch?v=GYT-65HZwus."
        ],
        "neg": []
    },
    {
        "query": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. To the best of our knowledge, we are the first to\nexploit non-awakening interaction and audio interrupt in MLLM. VITA is the\nfirst step for the open-source community to explore the seamless integration of\nmultimodal understanding and interaction. While there is still lots of work to\nbe done on VITA to get close to close-source counterparts, we hope that its\nrole as a pioneer can serve as a cornerstone for subsequent research. Project\nPage: https://vita-home.github.io.",
        "pos": [
            "Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering."
        ],
        "neg": []
    },
    {
        "query": "Language model continual learning (CL) has recently garnered significant\ninterest due to its potential to adapt large language models (LLMs) to dynamic\nreal-world environments without re-training. A key challenge in this field is\ncatastrophic forgetting, where models lose previously acquired knowledge when\nlearning new tasks. Existing methods commonly employ multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge for each task, but these approaches lack efficiency and overlook the\npotential for knowledge transfer through task interaction. In this paper, we\npresent a novel CL framework for language models called Task Skill Localization\nand Consolidation (TaSL), which enhances knowledge transfer without relying on\nmemory replay. TaSL first divides the model into `skill units' based on\nparameter dependencies, enabling more granular control. It then employs a novel\ngroup-wise skill localization technique to identify the importance distribution\nof skill units for a new task. By comparing this importance distribution with\nthose from previous tasks, we implement a fine-grained skill consolidation\nstrategy that retains task-specific knowledge, thereby preventing forgetting,\nand updates task-shared knowledge, which facilitates bi-directional knowledge\ntransfer. As a result, TaSL achieves a superior balance between retaining\nprevious knowledge and excelling in new tasks. TaSL also shows strong\ngeneralizability, suitable for general models and customizable for PEFT methods\nlike LoRA. Additionally, it demonstrates notable extensibility, allowing\nintegration with memory replay to further enhance performance. Extensive\nexperiments on two CL benchmarks, with varying model sizes (from 220M to 7B),\ndemonstrate the effectiveness of TaSL and its variants across different\nsettings.",
        "pos": [
            "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors in the\nintricate and realistic retrieval scenarios. To this end, we propose a\nKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving\ncontrollable knowledge selection in real retrieval scenarios. Concretely, we\nexplore and simulate error types across diverse context combinations and learn\nhow to avoid these negative signals through preference optimization methods.\nSimultaneously, by adjusting the balance between response length and the\nproportion of preference data representing different behavior patterns, we\nenhance the adherence capabilities and noise robustness of LLMs in a balanced\nmanner. Experimental results show that KaPO outperforms previous methods for\nhandling knowledge conflicts by over 37%, while also exhibiting robust\ngeneralization across various out-of-distribution datasets."
        ],
        "neg": []
    },
    {
        "query": "Language model continual learning (CL) has recently garnered significant\ninterest due to its potential to adapt large language models (LLMs) to dynamic\nreal-world environments without re-training. A key challenge in this field is\ncatastrophic forgetting, where models lose previously acquired knowledge when\nlearning new tasks. Existing methods commonly employ multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge for each task, but these approaches lack efficiency and overlook the\npotential for knowledge transfer through task interaction. In this paper, we\npresent a novel CL framework for language models called Task Skill Localization\nand Consolidation (TaSL), which enhances knowledge transfer without relying on\nmemory replay. TaSL first divides the model into `skill units' based on\nparameter dependencies, enabling more granular control. It then employs a novel\ngroup-wise skill localization technique to identify the importance distribution\nof skill units for a new task. By comparing this importance distribution with\nthose from previous tasks, we implement a fine-grained skill consolidation\nstrategy that retains task-specific knowledge, thereby preventing forgetting,\nand updates task-shared knowledge, which facilitates bi-directional knowledge\ntransfer. As a result, TaSL achieves a superior balance between retaining\nprevious knowledge and excelling in new tasks. TaSL also shows strong\ngeneralizability, suitable for general models and customizable for PEFT methods\nlike LoRA. Additionally, it demonstrates notable extensibility, allowing\nintegration with memory replay to further enhance performance. Extensive\nexperiments on two CL benchmarks, with varying model sizes (from 220M to 7B),\ndemonstrate the effectiveness of TaSL and its variants across different\nsettings.",
        "pos": [
            "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors in the\nintricate and realistic retrieval scenarios. To this end, we propose a\nKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving\ncontrollable knowledge selection in real retrieval scenarios. Concretely, we\nexplore and simulate error types across diverse context combinations and learn\nhow to avoid these negative signals through preference optimization methods.\nSimultaneously, by adjusting the balance between response length and the\nproportion of preference data representing different behavior patterns, we\nenhance the adherence capabilities and noise robustness of LLMs in a balanced\nmanner. Experimental results show that KaPO outperforms previous methods for\nhandling knowledge conflicts by over 37%, while also exhibiting robust\ngeneralization across various out-of-distribution datasets."
        ],
        "neg": []
    },
    {
        "query": "Language model continual learning (CL) has recently garnered significant\ninterest due to its potential to adapt large language models (LLMs) to dynamic\nreal-world environments without re-training. A key challenge in this field is\ncatastrophic forgetting, where models lose previously acquired knowledge when\nlearning new tasks. Existing methods commonly employ multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge for each task, but these approaches lack efficiency and overlook the\npotential for knowledge transfer through task interaction. In this paper, we\npresent a novel CL framework for language models called Task Skill Localization\nand Consolidation (TaSL), which enhances knowledge transfer without relying on\nmemory replay. TaSL first divides the model into `skill units' based on\nparameter dependencies, enabling more granular control. It then employs a novel\ngroup-wise skill localization technique to identify the importance distribution\nof skill units for a new task. By comparing this importance distribution with\nthose from previous tasks, we implement a fine-grained skill consolidation\nstrategy that retains task-specific knowledge, thereby preventing forgetting,\nand updates task-shared knowledge, which facilitates bi-directional knowledge\ntransfer. As a result, TaSL achieves a superior balance between retaining\nprevious knowledge and excelling in new tasks. TaSL also shows strong\ngeneralizability, suitable for general models and customizable for PEFT methods\nlike LoRA. Additionally, it demonstrates notable extensibility, allowing\nintegration with memory replay to further enhance performance. Extensive\nexperiments on two CL benchmarks, with varying model sizes (from 220M to 7B),\ndemonstrate the effectiveness of TaSL and its variants across different\nsettings.",
        "pos": [
            "Differing from sentiment transfer, positive reframing seeks to substitute\nnegative perspectives with positive expressions while preserving the original\nmeaning. With the emergence of pre-trained language models (PLMs), it is\npossible to achieve acceptable results by fine-tuning PLMs. Nevertheless,\ngenerating fluent, diverse and task-constrained reframing text remains a\nsignificant challenge. To tackle this issue, a \\textbf{m}ulti-\\textbf{s}trategy\n\\textbf{o}ptimization \\textbf{f}ramework (MSOF) is proposed in this paper.\nStarting from the objective of positive reframing, we first design positive\nsentiment reward and content preservation reward to encourage the model to\ntransform the negative expressions of the original text while ensuring the\nintegrity and consistency of the semantics. Then, different decoding\noptimization approaches are introduced to improve the quality of text\ngeneration. Finally, based on the modeling formula of positive reframing, we\npropose a multi-dimensional re-ranking method that further selects candidate\nsentences from three dimensions: strategy consistency, text similarity and\nfluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate\nour framework achieves significant improvements on unconstrained and controlled\npositive reframing tasks.",
            "The impressive capabilities of recent language models can be largely\nattributed to the multi-trillion token pretraining datasets that they are\ntrained on. However, model developers fail to disclose their construction\nmethodology which has lead to a lack of open information on how to develop\neffective pretraining sets. To address this issue, we perform the first\nsystematic study across the entire pipeline of pretraining set construction.\nFirst, we run ablations on existing techniques for pretraining set development\nto identify which methods translate to the largest gains in model accuracy on\ndownstream evaluations. Then, we categorize the most widely used data source,\nweb crawl snapshots, across the attributes of toxicity, quality, type of\nspeech, and domain. Finally, we show how such attribute information can be used\nto further refine and improve the quality of a pretraining set. These findings\nconstitute an actionable set of steps that practitioners can use to develop\nhigh quality pretraining sets."
        ],
        "neg": []
    },
    {
        "query": "Language model continual learning (CL) has recently garnered significant\ninterest due to its potential to adapt large language models (LLMs) to dynamic\nreal-world environments without re-training. A key challenge in this field is\ncatastrophic forgetting, where models lose previously acquired knowledge when\nlearning new tasks. Existing methods commonly employ multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge for each task, but these approaches lack efficiency and overlook the\npotential for knowledge transfer through task interaction. In this paper, we\npresent a novel CL framework for language models called Task Skill Localization\nand Consolidation (TaSL), which enhances knowledge transfer without relying on\nmemory replay. TaSL first divides the model into `skill units' based on\nparameter dependencies, enabling more granular control. It then employs a novel\ngroup-wise skill localization technique to identify the importance distribution\nof skill units for a new task. By comparing this importance distribution with\nthose from previous tasks, we implement a fine-grained skill consolidation\nstrategy that retains task-specific knowledge, thereby preventing forgetting,\nand updates task-shared knowledge, which facilitates bi-directional knowledge\ntransfer. As a result, TaSL achieves a superior balance between retaining\nprevious knowledge and excelling in new tasks. TaSL also shows strong\ngeneralizability, suitable for general models and customizable for PEFT methods\nlike LoRA. Additionally, it demonstrates notable extensibility, allowing\nintegration with memory replay to further enhance performance. Extensive\nexperiments on two CL benchmarks, with varying model sizes (from 220M to 7B),\ndemonstrate the effectiveness of TaSL and its variants across different\nsettings.",
        "pos": [
            "Recent research has demonstrated the feasibility of training efficient intent\ndetectors based on pre-trained language model~(PLM) with limited labeled data.\nHowever, deploying these detectors in resource-constrained environments such as\nmobile devices poses challenges due to their large sizes. In this work, we aim\nto address this issue by exploring techniques to minimize the size of PLM-based\nintent detectors trained with few-shot data. Specifically, we utilize large\nlanguage models (LLMs) for data augmentation, employ a cutting-edge model\ncompression method for knowledge distillation, and devise a vocabulary pruning\nmechanism called V-Prune. Through these approaches, we successfully achieve a\ncompression ratio of 21 in model memory usage, including both Transformer and\nthe vocabulary, while maintaining almost identical performance levels on four\nreal-world benchmarks."
        ],
        "neg": []
    },
    {
        "query": "The task of deciding whether two documents are written by the same author is\nchallenging for both machines and humans. This task is even more challenging\nwhen the two documents are written about different topics (e.g. baseball vs.\npolitics) or in different genres (e.g. a blog post vs. an academic article).\nFor machines, the problem is complicated by the relative lack of real-world\ntraining examples that cross the topic boundary and the vanishing scarcity of\ncross-genre data. We propose targeted methods for training data selection and a\nnovel learning curriculum that are designed to discourage a model's reliance on\ntopic information for authorship attribution and correspondingly force it to\nincorporate information more robustly indicative of style no matter the topic.\nThese refinements yield a 62.7% relative improvement in average cross-genre\nauthorship attribution, as well as 16.6% in the per-genre condition.",
        "pos": [
            "Large Language Models (LLMs) such as GPT-4 have shown enough promise in the\nfew-shot learning context to suggest use in the generation of \"silver\" data and\nrefinement of new ontologies through iterative application and review. Such\nworkflows become more effective with reliable confidence estimation.\nUnfortunately, confidence estimation is a documented weakness of models such as\nGPT-4, and established methods to compensate require significant additional\ncomplexity and computation. The present effort explores methods for effective\nconfidence estimation with GPT-4 with few-shot learning for event detection in\nthe BETTER ontology as a vehicle. The key innovation is expanding the prompt\nand task presented to GPT-4 to provide License to speculate when unsure and\nOpportunity to quantify and explain its uncertainty (L&O). This approach\nimproves accuracy and provides usable confidence measures (0.759 AUC) with no\nadditional machinery."
        ],
        "neg": []
    },
    {
        "query": "The task of deciding whether two documents are written by the same author is\nchallenging for both machines and humans. This task is even more challenging\nwhen the two documents are written about different topics (e.g. baseball vs.\npolitics) or in different genres (e.g. a blog post vs. an academic article).\nFor machines, the problem is complicated by the relative lack of real-world\ntraining examples that cross the topic boundary and the vanishing scarcity of\ncross-genre data. We propose targeted methods for training data selection and a\nnovel learning curriculum that are designed to discourage a model's reliance on\ntopic information for authorship attribution and correspondingly force it to\nincorporate information more robustly indicative of style no matter the topic.\nThese refinements yield a 62.7% relative improvement in average cross-genre\nauthorship attribution, as well as 16.6% in the per-genre condition.",
        "pos": [
            "Large Language Models (LLMs) such as GPT-4 have shown enough promise in the\nfew-shot learning context to suggest use in the generation of \"silver\" data and\nrefinement of new ontologies through iterative application and review. Such\nworkflows become more effective with reliable confidence estimation.\nUnfortunately, confidence estimation is a documented weakness of models such as\nGPT-4, and established methods to compensate require significant additional\ncomplexity and computation. The present effort explores methods for effective\nconfidence estimation with GPT-4 with few-shot learning for event detection in\nthe BETTER ontology as a vehicle. The key innovation is expanding the prompt\nand task presented to GPT-4 to provide License to speculate when unsure and\nOpportunity to quantify and explain its uncertainty (L&O). This approach\nimproves accuracy and provides usable confidence measures (0.759 AUC) with no\nadditional machinery."
        ],
        "neg": []
    },
    {
        "query": "Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse\ndecomposition of a neural network's latent representations into seemingly\ninterpretable features. Despite recent excitement about their potential,\nresearch applications outside of industry are limited by the high cost of\ntraining a comprehensive suite of SAEs. In this work, we introduce Gemma Scope,\nan open suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2\n2B and 9B and select layers of Gemma 2 27B base models. We primarily train SAEs\non the Gemma 2 pre-trained models, but additionally release SAEs trained on\ninstruction-tuned Gemma 2 9B for comparison. We evaluate the quality of each\nSAE on standard metrics and release these results. We hope that by releasing\nthese SAE weights, we can help make more ambitious safety and interpretability\nresearch easier for the community. Weights and a tutorial can be found at\nhttps://huggingface.co/google/gemma-scope and an interactive demo can be found\nat https://www.neuronpedia.org/gemma-scope",
        "pos": [
            "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community."
        ],
        "neg": []
    },
    {
        "query": "Textual graphs are ubiquitous in real-world applications, featuring rich text\ninformation with complex relationships, which enables advanced research across\nvarious fields. Textual graph representation learning aims to generate\nlow-dimensional feature embeddings from textual graphs that can improve the\nperformance of downstream tasks. A high-quality feature embedding should\neffectively capture both the structural and the textual information in a\ntextual graph. However, most textual graph dataset benchmarks rely on word2vec\ntechniques to generate feature embeddings, which inherently limits their\ncapabilities. Recent works on textual graph representation learning can be\ncategorized into two folds: supervised and unsupervised methods. Supervised\nmethods finetune a language model on labeled nodes, which have limited\ncapabilities when labeled data is scarce. Unsupervised methods, on the other\nhand, extract feature embeddings by developing complex training pipelines. To\naddress these limitations, we propose a novel unified unsupervised learning\nautoencoder framework, named Node Level Graph AutoEncoder (NodeGAE). We employ\nlanguage models as the backbone of the autoencoder, with pretraining on text\nreconstruction. Additionally, we add an auxiliary loss term to make the feature\nembeddings aware of the local graph structure. Our method maintains simplicity\nin the training process and demonstrates generalizability across diverse\ntextual graphs and downstream tasks. We evaluate our method on two core graph\nrepresentation learning downstream tasks: node classification and link\nprediction. Comprehensive experiments demonstrate that our approach\nsubstantially enhances the performance of diverse graph neural networks (GNNs)\nacross multiple textual graph datasets.",
        "pos": [
            "Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract\nfine-grained sentiment elements from target domain sentences by leveraging the\nknowledge acquired from the source domain. Due to the absence of labeled data\nin the target domain, recent studies tend to rely on pre-trained language\nmodels to generate large amounts of synthetic data for training purposes.\nHowever, these approaches entail additional computational costs associated with\nthe generation process. Different from them, we discover a striking resemblance\nbetween table-filling methods in ASTE and two-stage Object Detection (OD) in\ncomputer vision, which inspires us to revisit the cross-domain ASTE task and\napproach it from an OD standpoint. This allows the model to benefit from the OD\nextraction paradigm and region-level alignment. Building upon this premise, we\npropose a novel method named \\textbf{T}able-\\textbf{F}illing via \\textbf{M}ean\n\\textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the\nsentence into a 2D table to detect word relations, while TFMT treats the table\nas a feature map and utilizes a region consistency to enhance the quality of\nthose generated pseudo labels. Additionally, considering the existence of the\ndomain gap, a cross-domain consistency based on Maximum Mean Discrepancy is\ndesigned to alleviate domain shift problems. Our method achieves\nstate-of-the-art performance with minimal parameters and computational costs,\nmaking it a strong baseline for cross-domain ASTE.",
            "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has\nenhanced medical diagnosis. However, current Med-LVLMs frequently encounter\nfactual issues, often generating responses that do not align with established\nmedical facts. Retrieval-Augmented Generation (RAG), which utilizes external\nknowledge, can improve the factual accuracy of these models but introduces two\nmajor challenges. First, limited retrieved contexts might not cover all\nnecessary information, while excessive retrieval can introduce irrelevant and\ninaccurate references, interfering with the model's generation. Second, in\ncases where the model originally responds correctly, applying RAG can lead to\nan over-reliance on retrieved contexts, resulting in incorrect answers. To\naddress these issues, we propose RULE, which consists of two components. First,\nwe introduce a provably effective strategy for controlling factuality risk\nthrough the calibrated selection of the number of retrieved contexts. Second,\nbased on samples where over-reliance on retrieved contexts led to errors, we\ncurate a preference dataset to fine-tune the model, balancing its dependence on\ninherent knowledge and retrieved contexts for generation. We demonstrate the\neffectiveness of RULE on three medical VQA datasets, achieving an average\nimprovement of 20.8% in factual accuracy. We publicly release our benchmark and\ncode in https://github.com/richard-peng-xia/RULE."
        ],
        "neg": []
    },
    {
        "query": "Textual graphs are ubiquitous in real-world applications, featuring rich text\ninformation with complex relationships, which enables advanced research across\nvarious fields. Textual graph representation learning aims to generate\nlow-dimensional feature embeddings from textual graphs that can improve the\nperformance of downstream tasks. A high-quality feature embedding should\neffectively capture both the structural and the textual information in a\ntextual graph. However, most textual graph dataset benchmarks rely on word2vec\ntechniques to generate feature embeddings, which inherently limits their\ncapabilities. Recent works on textual graph representation learning can be\ncategorized into two folds: supervised and unsupervised methods. Supervised\nmethods finetune a language model on labeled nodes, which have limited\ncapabilities when labeled data is scarce. Unsupervised methods, on the other\nhand, extract feature embeddings by developing complex training pipelines. To\naddress these limitations, we propose a novel unified unsupervised learning\nautoencoder framework, named Node Level Graph AutoEncoder (NodeGAE). We employ\nlanguage models as the backbone of the autoencoder, with pretraining on text\nreconstruction. Additionally, we add an auxiliary loss term to make the feature\nembeddings aware of the local graph structure. Our method maintains simplicity\nin the training process and demonstrates generalizability across diverse\ntextual graphs and downstream tasks. We evaluate our method on two core graph\nrepresentation learning downstream tasks: node classification and link\nprediction. Comprehensive experiments demonstrate that our approach\nsubstantially enhances the performance of diverse graph neural networks (GNNs)\nacross multiple textual graph datasets.",
        "pos": [
            "Large language models (LLMs) have shown success in handling simple games with\nimperfect information and enabling multi-agent coordination, but their ability\nto facilitate practical collaboration against other agents in complex,\nimperfect information environments, especially in a non-English environment,\nstill needs to be explored. This study investigates the applicability of\nknowledge acquired by open-source and API-based LLMs to sophisticated\ntext-based games requiring agent collaboration under imperfect information,\ncomparing their performance to established baselines using other types of\nagents. We propose a Theory of Mind (ToM) planning technique that allows LLM\nagents to adapt their strategy against various adversaries using only game\nrules, current state, and historical context as input. An external tool was\nincorporated to mitigate the challenge of dynamic and extensive action spaces\nin this card game. Our results show that although a performance gap exists\nbetween current LLMs and state-of-the-art reinforcement learning (RL) models,\nLLMs demonstrate ToM capabilities in this game setting. It consistently\nimproves their performance against opposing agents, suggesting their ability to\nunderstand the actions of allies and adversaries and establish collaboration\nwith allies. To encourage further research and understanding, we have made our\ncodebase openly accessible.",
            "While large language models (LLMs) have demonstrated impressive capabilities\nacross various natural language processing tasks by acquiring rich factual\nknowledge from their broad training data, their ability to synthesize and\nlogically reason with this knowledge in complex ways remains underexplored. In\nthis work, we present a systematic evaluation of state-of-the-art LLMs' complex\nlogical reasoning abilities through a novel benchmark of automatically\ngenerated complex reasoning questions over general domain and biomedical\nknowledge graphs. Our extensive experiments, employing diverse in-context\nlearning techniques, reveal that LLMs excel at reasoning over general world\nknowledge but face significant challenges with specialized domain-specific\nknowledge. We find that prompting with explicit Chain-of-Thought demonstrations\ncan substantially improve LLM performance on complex logical reasoning tasks\nwith diverse logical operations. Interestingly, our controlled evaluations\nuncover an asymmetry where LLMs display proficiency at set union operations,\nbut struggle considerably with set intersections - a key building block of\nlogical reasoning. To foster further work, we will publicly release our\nevaluation benchmark and code.",
            "Dialogical Argument Mining(DialAM) is an important branch of Argument\nMining(AM). DialAM-2024 is a shared task focusing on dialogical argument\nmining, which requires us to identify argumentative relations and illocutionary\nrelations among proposition nodes and locution nodes. To accomplish this, we\npropose a two-stage pipeline, which includes the Two-Step S-Node Prediction\nModel in Stage 1 and the YA-Node Prediction Model in Stage 2. We also augment\nthe training data in both stages and introduce context in Stage 2. We\nsuccessfully completed the task and achieved good results. Our team Pokemon\nranked 1st in the ARI Focused score and 4th in the Global Focused score."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we present MooER, a LLM-based large-scale automatic speech\nrecognition (ASR) / automatic speech translation (AST) model of Moore Threads.\nA 5000h pseudo labeled dataset containing open source and self collected speech\ndata is used for training. We achieve performance comparable to other open\nsource models trained with up to hundreds of thousands of hours of labeled\nspeech data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest\nthat our model outperforms other open source Speech LLMs. A BLEU score of 25.2\ncan be obtained. The main contributions of this paper are summarized as\nfollows. First, this paper presents a training strategy for encoders and LLMs\non speech related tasks (including ASR and AST) using a small size of pseudo\nlabeled data without any extra manual annotation and selection. Second, we\nrelease our ASR and AST models and plan to open-source our training code and\nstrategy in the near future. Moreover, a model trained on 8wh scale training\ndata is planned to be released later on.",
        "pos": [
            "Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs."
        ],
        "neg": []
    },
    {
        "query": "The task of multi-objective alignment aims at balancing and controlling the\ndifferent alignment objectives (e.g., helpfulness, harmlessness and honesty) of\nlarge language models to meet the personalized requirements of different users.\nHowever, previous methods tend to train multiple models to deal with various\nuser preferences, with the number of trained models growing linearly with the\nnumber of alignment objectives and the number of different preferences.\nMeanwhile, existing methods are generally poor in extensibility and require\nsignificant re-training for each new alignment objective considered.\nConsidering the limitation of previous approaches, we propose MCA\n(Multi-objective Contrastive Alignemnt), which constructs an expert prompt and\nan adversarial prompt for each objective to contrast at the decoding time and\nbalances the objectives through combining the contrast. Our approach is\nverified to be superior to previous methods in obtaining a well-distributed\nPareto front among different alignment objectives.",
        "pos": [
            "Customers reach out to online live chat agents with various intents, such as\nasking about product details or requesting a return. In this paper, we propose\nthe problem of predicting user intent from browsing history and address it\nthrough a two-stage approach. The first stage classifies a user's browsing\nhistory into high-level intent categories. Here, we represent each browsing\nhistory as a text sequence of page attributes and use the ground-truth class\nlabels to fine-tune pretrained Transformers. The second stage provides a large\nlanguage model (LLM) with the browsing history and predicted intent class to\ngenerate fine-grained intents. For automatic evaluation, we use a separate LLM\nto judge the similarity between generated and ground-truth intents, which\nclosely aligns with human judgments. Our two-stage approach yields significant\nperformance gains compared to generating intents without the classification\nstage.",
            "Instruction tuning in multimodal large language models (MLLMs) aims to\nsmoothly integrate a backbone LLM with a pre-trained feature encoder for\ndownstream tasks. The major challenge is how to efficiently find the synergy\nthrough cooperative learning where LLMs adapt their reasoning abilities in\ndownstream tasks while feature encoders adjust their encoding to provide more\nrelevant modal information. In this paper, we analyze the MLLM instruction\ntuning from both theoretical and empirical perspectives, where we find\nunbalanced learning between the two components, i.e., the feature encoder and\nthe LLM, can cause diminishing learning gradients that slow the model\nconvergence and often lead to sub-optimal results due to insufficient learning.\nInspired by our findings, we propose a measurement to quantitatively evaluate\nthe learning balance, based on which we further design a dynamic learning\nscheduler that better coordinates the learning. In addition, we introduce an\nauxiliary loss regularization method to promote updating of the generation\ndistribution of MLLMs considering the learning state of each model component,\nwhich potentially prevents each component from gradient diminishing and enables\na more accurate estimation of the learning balance coefficient. We conduct\nexperiments with multiple LLM backbones and feature encoders, where our\ntechniques are model-agnostic and can be generically integrated with various\nMLLM backbones. Experiment results on multiple downstream tasks and modalities\nin vision and audio, demonstrate the proposed method's better efficiency and\neffectiveness in MLLM instruction tuning."
        ],
        "neg": []
    },
    {
        "query": "The task of multi-objective alignment aims at balancing and controlling the\ndifferent alignment objectives (e.g., helpfulness, harmlessness and honesty) of\nlarge language models to meet the personalized requirements of different users.\nHowever, previous methods tend to train multiple models to deal with various\nuser preferences, with the number of trained models growing linearly with the\nnumber of alignment objectives and the number of different preferences.\nMeanwhile, existing methods are generally poor in extensibility and require\nsignificant re-training for each new alignment objective considered.\nConsidering the limitation of previous approaches, we propose MCA\n(Multi-objective Contrastive Alignemnt), which constructs an expert prompt and\nan adversarial prompt for each objective to contrast at the decoding time and\nbalances the objectives through combining the contrast. Our approach is\nverified to be superior to previous methods in obtaining a well-distributed\nPareto front among different alignment objectives.",
        "pos": [
            "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.",
            "Speculative decoding has emerged as a promising technique to accelerate the\ninference of Large Language Models (LLMs) by employing a small language model\nto draft a hypothesis sequence, which is then validated by the LLM. The\neffectiveness of this approach heavily relies on the balance between\nperformance and efficiency of the draft model. In our research, we focus on\nenhancing the proportion of draft tokens that are accepted to the final output\nby generating multiple hypotheses instead of just one. This allows the LLM more\noptions to choose from and select the longest sequence that meets its\nstandards. Our analysis reveals that hypotheses produced by the draft model\nshare many common token sequences, suggesting a potential for optimizing\ncomputation. Leveraging this observation, we introduce an innovative approach\nutilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This\nstructure enables us to efficiently predict and merge recurring token\nsequences, vastly reducing the computational demands of the draft model. We\nterm this approach Graph-structured Speculative Decoding (GSD). We apply GSD\nacross a range of LLMs, including a 70-billion parameter LLaMA-2 model, and\nobserve a remarkable speedup of 1.73$\\times$ to 1.96$\\times$, significantly\nsurpassing standard speculative decoding.",
            "Is it always necessary to compute tokens from shallow to deep layers in\nTransformers? The continued success of vanilla Transformers and their variants\nsuggests an undoubted \"yes\". In this work, however, we attempt to break the\ndepth-ordered convention by proposing a novel architecture dubbed\nmixture-of-modules (MoM), which is motivated by an intuition that any layer,\nregardless of its position, can be used to compute a token as long as it\npossesses the needed processing capabilities. The construction of MoM starts\nfrom a finite set of modules defined by multi-head attention and feed-forward\nnetworks, each distinguished by its unique parameterization. Two routers then\niteratively select attention modules and feed-forward modules from the set to\nprocess a token. The selection dynamically expands the computation graph in the\nforward pass of the token, culminating in an assembly of modules. We show that\nMoM provides not only a unified framework for Transformers and their numerous\nvariants but also a flexible and learnable approach for reducing redundancy in\nTransformer parameterization. We pre-train various MoMs using OpenWebText.\nEmpirical results demonstrate that MoMs, of different parameter counts,\nconsistently outperform vanilla transformers on both GLUE and XSUM benchmarks.\nMore interestingly, with a fixed parameter budget, MoM-large enables an over\n38% increase in depth for computation graphs compared to GPT-2-large, resulting\nin absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large\nalso enables an over 60% reduction in depth while involving more modules per\nlayer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage\ncompared to GPT-2-large, while maintaining comparable performance."
        ],
        "neg": []
    },
    {
        "query": "We propose \\textit{re}CSE, a self supervised contrastive learning sentence\nrepresentation framework based on feature reshaping. This framework is\ndifferent from the current advanced models that use discrete data augmentation\nmethods, but instead reshapes the input features of the original sentence,\naggregates the global information of each token in the sentence, and alleviates\nthe common problems of representation polarity and GPU memory consumption\nlinear increase in current advanced models. In addition, our \\textit{re}CSE has\nachieved competitive performance in semantic similarity tasks. And the\nexperiment proves that our proposed feature reshaping method has strong\nuniversality, which can be transplanted to other self supervised contrastive\nlearning frameworks and enhance their representation ability, even achieving\nstate-of-the-art performance. Our code is available at\nhttps://github.com/heavenhellchen/reCSE.",
        "pos": [
            "In this work, we report our efforts to advance the standard operation\nprocedure of developing Large Language Models (LLMs) or LLMs-based systems or\nservices in industry. We introduce the concept of Large Language Model\nDevelopment Lifecycle (LDLC) and then highlight the importance of consistency\ntest in ensuring the delivery quality. The principled solution of consistency\ntest, however, is usually overlooked by industrial practitioners and not urgent\nin academia, and current practical solutions are insufficiently rigours and\nlabor-intensive. We thus propose a simple yet effective consistency test\nprotocol, named SimCT. SimCT is mainly to proactively check the consistency\nacross different development stages of \"bare metal\" LLMs or associated services\nwithout accessing the model artifacts, in an attempt to expedite the delivery\nby reducing the back-and-forth alignment communications among multiple teams\ninvolved in different development stages.\n  Specifically, SimCT encompasses response-wise and model-wise tests. We\nimplement the protocol with LightGBM and Student's t-test for two components\nrespectively, and perform extensive experiments to substantiate the\neffectiveness of SimCT and the involved components."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model's vocabulary space and named them \"glitch tokens\". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.",
        "pos": [
            "Security concerns for large language models (LLMs) have recently escalated,\nfocusing on thwarting jailbreaking attempts in discrete prompts. However, the\nexploration of jailbreak vulnerabilities arising from continuous embeddings has\nbeen limited, as prior approaches primarily involved appending discrete or\ncontinuous suffixes to inputs. Our study presents a novel channel for\nconducting direct attacks on LLM inputs, eliminating the need for suffix\naddition or specific questions provided that the desired output is predefined.\nWe additionally observe that extensive iterations often lead to overfitting,\ncharacterized by repetition in the output. To counteract this, we propose a\nsimple yet effective strategy named CLIP. Our experiments show that for an\ninput length of 40 at iteration 1000, applying CLIP improves the ASR from 62%\nto 83%"
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model's vocabulary space and named them \"glitch tokens\". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.",
        "pos": [
            "Large Language Models (LLMs) are increasingly integrated into diverse\nindustries, posing substantial security risks due to unauthorized replication\nand misuse. To mitigate these concerns, robust identification mechanisms are\nwidely acknowledged as an effective strategy. Identification systems for LLMs\nnow rely heavily on watermarking technology to manage and protect intellectual\nproperty and ensure data security. However, previous studies have primarily\nconcentrated on the basic principles of algorithms and lacked a comprehensive\nanalysis of watermarking theory and practice from the perspective of\nintelligent identification. To bridge this gap, firstly, we explore how a\nrobust identity recognition system can be effectively implemented and managed\nwithin LLMs by various participants using watermarking technology. Secondly, we\npropose a mathematical framework based on mutual information theory, which\nsystematizes the identification process to achieve more precise and customized\nwatermarking. Additionally, we present a comprehensive evaluation of\nperformance metrics for LLM watermarking, reflecting participant preferences\nand advancing discussions on its identification applications. Lastly, we\noutline the existing challenges in current watermarking technologies and\ntheoretical frameworks, and provide directional guidance to address these\nchallenges. Our systematic classification and detailed exposition aim to\nenhance the comparison and evaluation of various methods, fostering further\nresearch and development toward a transparent, secure, and equitable LLM\necosystem."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model's vocabulary space and named them \"glitch tokens\". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.",
        "pos": [
            "Text-to-SQL conversion is a critical innovation, simplifying the transition\nfrom complex SQL to intuitive natural language queries, especially significant\ngiven SQL's prevalence in the job market across various roles. The rise of\nLarge Language Models (LLMs) like GPT-3.5 and GPT-4 has greatly advanced this\nfield, offering improved natural language understanding and the ability to\ngenerate nuanced SQL statements. However, the potential of open-source LLMs in\nText-to-SQL applications remains underexplored, with many frameworks failing to\nleverage their full capabilities, particularly in handling complex database\nqueries and incorporating feedback for iterative refinement. Addressing these\nlimitations, this paper introduces SQLfuse, a robust system integrating\nopen-source LLMs with a suite of tools to enhance Text-to-SQL translation's\naccuracy and usability. SQLfuse features four modules: schema mining, schema\nlinking, SQL generation, and a SQL critic module, to not only generate but also\ncontinuously enhance SQL query quality. Demonstrated by its leading performance\non the Spider Leaderboard and deployment by Ant Group, SQLfuse showcases the\npractical merits of open-source LLMs in diverse business contexts.",
            "A critical component in knowledge distillation is the means of coupling the\nteacher and student. The predominant sequence knowledge distillation method\ninvolves supervised learning of the student against teacher-decoded outputs,\nand is exemplified by the current state of the art, which incorporates minimum\nBayes risk (MBR) decoding. In this paper we seek to integrate MBR more tightly\nin distillation training, specifically by using several high scoring MBR\ntranslations, rather than a single selected sequence, thus capturing a rich\ndiversity of teacher outputs. Our experiments on English to German and English\nto Japanese translation show consistent improvements over strong baseline\nmethods for both tasks and with varying model sizes. Additionally, we conduct a\ndetailed analysis focusing on data efficiency and capacity curse aspects to\nelucidate MBR-n and explore its further potential.",
            "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model's vocabulary space and named them \"glitch tokens\". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.",
        "pos": [
            "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.",
            "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext.",
            "Large language models (LLMs) have achieved remarkable performance on various\nNLP tasks, yet their potential in more challenging and domain-specific task,\nsuch as finance, has not been fully explored. In this paper, we present\nCFinBench: a meticulously crafted, the most comprehensive evaluation benchmark\nto date, for assessing the financial knowledge of LLMs under Chinese context.\nIn practice, to better align with the career trajectory of Chinese financial\npractitioners, we build a systematic evaluation from 4 first-level categories:\n(1) Financial Subject: whether LLMs can memorize the necessary basic knowledge\nof financial subjects, such as economics, statistics and auditing. (2)\nFinancial Qualification: whether LLMs can obtain the needed financial qualified\ncertifications, such as certified public accountant, securities qualification\nand banking qualification. (3) Financial Practice: whether LLMs can fulfill the\npractical financial jobs, such as tax consultant, junior accountant and\nsecurities analyst. (4) Financial Law: whether LLMs can meet the requirement of\nfinancial laws and regulations, such as tax law, insurance law and economic\nlaw. CFinBench comprises 99,100 questions spanning 43 second-level categories\nwith 3 question types: single-choice, multiple-choice and judgment. We conduct\nextensive experiments of 50 representative LLMs with various model size on\nCFinBench. The results show that GPT4 and some Chinese-oriented models lead the\nbenchmark, with the highest average accuracy being 60.16%, highlighting the\nchallenge presented by CFinBench. The dataset and evaluation code are available\nat https://cfinbench.github.io/."
        ],
        "neg": []
    },
    {
        "query": "Automatic Chart Question Answering (ChartQA) is challenging due to the\ncomplex distribution of chart elements with patterns of the underlying data not\nexplicitly displayed in charts. To address this challenge, we design a joint\nmultimodal scene graph for charts to explicitly represent the relationships\nbetween chart elements and their patterns. Our proposed multimodal scene graph\nincludes a visual graph and a textual graph to jointly capture the structural\nand semantical knowledge from the chart. This graph module can be easily\nintegrated with different vision transformers as inductive bias. Our\nexperiments demonstrate that incorporating the proposed graph module enhances\nthe understanding of charts' elements' structure and semantics, thereby\nimproving performance on publicly available benchmarks, ChartQA and OpenCQA.",
        "pos": [
            "Visually Rich Documents (VRDs) are essential in academia, finance, medical\nfields, and marketing due to their multimodal information content. Traditional\nmethods for extracting information from VRDs depend on expert knowledge and\nmanual labor, making them costly and inefficient. The advent of deep learning\nhas revolutionized this process, introducing models that leverage multimodal\ninformation vision, text, and layout along with pretraining tasks to develop\ncomprehensive document representations. These models have achieved\nstate-of-the-art performance across various downstream tasks, significantly\nenhancing the efficiency and accuracy of information extraction from VRDs. In\nresponse to the growing demands and rapid developments in Visually Rich\nDocument Understanding (VRDU), this paper provides a comprehensive review of\ndeep learning-based VRDU frameworks. We systematically survey and analyze\nexisting methods and benchmark datasets, categorizing them based on adopted\nstrategies and downstream tasks. Furthermore, we compare different techniques\nused in VRDU models, focusing on feature representation and fusion, model\narchitecture, and pretraining methods, while highlighting their strengths,\nlimitations, and appropriate scenarios. Finally, we identify emerging trends\nand challenges in VRDU, offering insights into future research directions and\npractical applications. This survey aims to provide a thorough understanding of\nVRDU advancements, benefiting both academic and industrial sectors.",
            "The significance of mental health classification is paramount in contemporary\nsociety, where digital platforms serve as crucial sources for monitoring\nindividuals' well-being. However, existing social media mental health datasets\nprimarily consist of text-only samples, potentially limiting the efficacy of\nmodels trained on such data. Recognising that humans utilise cross-modal\ninformation to comprehend complex situations or issues, we present a novel\napproach to address the limitations of current methodologies. In this work, we\nintroduce a Multimodal and Multi-Teacher Knowledge Distillation model for\nMental Health Classification, leveraging insights from cross-modal human\nunderstanding. Unlike conventional approaches that often rely on simple\nconcatenation to integrate diverse features, our model addresses the challenge\nof appropriately representing inputs of varying natures (e.g., texts and\nsounds). To mitigate the computational complexity associated with integrating\nall features into a single model, we employ a multimodal and multi-teacher\narchitecture. By distributing the learning process across multiple teachers,\neach specialising in a particular feature extraction aspect, we enhance the\noverall mental health classification performance. Through experimental\nvalidation, we demonstrate the efficacy of our model in achieving improved\nperformance."
        ],
        "neg": []
    },
    {
        "query": "Automatic Chart Question Answering (ChartQA) is challenging due to the\ncomplex distribution of chart elements with patterns of the underlying data not\nexplicitly displayed in charts. To address this challenge, we design a joint\nmultimodal scene graph for charts to explicitly represent the relationships\nbetween chart elements and their patterns. Our proposed multimodal scene graph\nincludes a visual graph and a textual graph to jointly capture the structural\nand semantical knowledge from the chart. This graph module can be easily\nintegrated with different vision transformers as inductive bias. Our\nexperiments demonstrate that incorporating the proposed graph module enhances\nthe understanding of charts' elements' structure and semantics, thereby\nimproving performance on publicly available benchmarks, ChartQA and OpenCQA.",
        "pos": [
            "The explosive growth of data fuels data-driven research, facilitating\nprogress across diverse domains. The FAIR principles emerge as a guiding\nstandard, aiming to enhance the findability, accessibility, interoperability,\nand reusability of data. However, current efforts primarily focus on manual\ndata FAIRification, which can only handle targeted data and lack efficiency. To\naddress this issue, we propose AutoFAIR, an architecture designed to enhance\ndata FAIRness automately. Firstly, We align each data and metadata operation\nwith specific FAIR indicators to guide machine-executable actions. Then, We\nutilize Web Reader to automatically extract metadata based on language models,\neven in the absence of structured data webpage schemas. Subsequently, FAIR\nAlignment is employed to make metadata comply with FAIR principles by ontology\nguidance and semantic matching. Finally, by applying AutoFAIR to various data,\nespecially in the field of mountain hazards, we observe significant\nimprovements in findability, accessibility, interoperability, and reusability\nof data. The FAIRness scores before and after applying AutoFAIR indicate\nenhanced data value.",
            "In recent years, with the rapid development of deep learning technology,\nlarge language models (LLMs) such as BERT and GPT have achieved breakthrough\nresults in natural language processing tasks. Machine translation (MT), as one\nof the core tasks of natural language processing, has also benefited from the\ndevelopment of large language models and achieved a qualitative leap. Despite\nthe significant progress in translation performance achieved by large language\nmodels, machine translation still faces many challenges. Therefore, in this\npaper, we construct the dataset Euas-20 to evaluate the performance of large\nlanguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability of\nLLMs for researchers and developers.",
            "Multimodal conversational agents are highly desirable because they offer\nnatural and human-like interaction. However, there is a lack of comprehensive\nend-to-end solutions to support collaborative development and benchmarking.\nWhile proprietary systems like GPT-4o and Gemini demonstrating impressive\nintegration of audio, video, and text with response times of 200-250ms,\nchallenges remain in balancing latency, accuracy, cost, and data privacy. To\nbetter understand and quantify these issues, we developed OpenOmni, an\nopen-source, end-to-end pipeline benchmarking tool that integrates advanced\ntechnologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented\nGeneration, Large Language Models, along with the ability to integrate\ncustomized models. OpenOmni supports local and cloud deployment, ensuring data\nprivacy and supporting latency and accuracy benchmarking. This flexible\nframework allows researchers to customize the pipeline, focusing on real\nbottlenecks and facilitating rapid proof-of-concept development. OpenOmni can\nsignificantly enhance applications like indoor assistance for visually impaired\nindividuals, advancing human-computer interaction. Our demonstration video is\navailable https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via\nhttps://openomni.ai4wa.com, code is available via\nhttps://github.com/AI4WA/OpenOmniFramework.",
            "Exceptional mathematical reasoning ability is one of the key features that\ndemonstrate the power of large language models (LLMs). How to comprehensively\ndefine and evaluate the mathematical abilities of LLMs, and even reflect the\nuser experience in real-world scenarios, has emerged as a critical issue.\nCurrent benchmarks predominantly concentrate on problem-solving capabilities,\nwhich presents a substantial risk of model overfitting and fails to accurately\nrepresent genuine mathematical reasoning abilities. In this paper, we argue\nthat if a model really understands a problem, it should be robustly and readily\napplied across a diverse array of tasks. Motivated by this, we introduce\nMATHCHECK, a well-designed checklist for testing task generalization and\nreasoning robustness, as well as an automatic tool to generate checklists\nefficiently. MATHCHECK includes multiple mathematical reasoning tasks and\nrobustness test types to facilitate a comprehensive evaluation of both\nmathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we\ndevelop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual\nreasoning and multi-modal reasoning capabilities, respectively, serving as\nupgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K.\nWe adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs,\nassessing their comprehensive mathematical reasoning abilities. Our results\ndemonstrate that while frontier LLMs like GPT-4o continue to excel in various\nabilities on the checklist, many other model families exhibit a significant\ndecline. Further experiments indicate that, compared to traditional math\nbenchmarks, MATHCHECK better reflects true mathematical abilities and\nrepresents mathematical intelligence more linearly, thereby supporting our\ndesign. On our MATHCHECK, we can easily conduct detailed behavior analysis to\ndeeply investigate models."
        ],
        "neg": []
    },
    {
        "query": "The mental health assessment of middle school students has always been one of\nthe focuses in the field of education. This paper introduces a new ensemble\nlearning network based on BERT, employing the concept of enhancing model\nperformance by integrating multiple classifiers. We trained a range of\nBERT-based learners, which combined using the majority voting method. We\ncollect social network text data of middle school students through China's\nWeibo and apply the method to the task of classifying emotional tendencies in\nmiddle school students' social network texts. Experimental results suggest that\nthe ensemble learning network has a better performance than the base model and\nthe performance of the ensemble learning model, consisting of three\nsingle-layer BERT models, is barely the same as a three-layer BERT model but\nrequires 11.58% more training time. Therefore, in terms of balancing prediction\neffect and efficiency, the deeper BERT network should be preferred for\ntraining. However, for interpretability, network ensembles can provide\nacceptable solutions.",
        "pos": [
            "Large language models (LLMs) hold great promise in summarizing medical\nevidence. Most recent studies focus on the application of proprietary LLMs.\nUsing proprietary LLMs introduces multiple risk factors, including a lack of\ntransparency and vendor dependency. While open-source LLMs allow better\ntransparency and customization, their performance falls short compared to\nproprietary ones. In this study, we investigated to what extent fine-tuning\nopen-source LLMs can further improve their performance in summarizing medical\nevidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs\nof systematic reviews and summaries, we fine-tuned three broadly-used,\nopen-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned\nLLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:\n8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and\n15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of\nfine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,\nsmaller fine-tuned models sometimes even demonstrated superior performance\ncompared to larger zero-shot models. The above trends of improvement were also\nmanifested in both human and GPT4-simulated evaluations. Our results can be\napplied to guide model selection for tasks demanding particular domain\nknowledge, such as medical evidence summarization."
        ],
        "neg": []
    },
    {
        "query": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in executing instructions for a variety of single-image tasks.\nDespite this progress, significant challenges remain in modeling long image\nsequences. In this work, we introduce the versatile multi-modal large language\nmodel, mPLUG-Owl3, which enhances the capability for long image-sequence\nunderstanding in scenarios that incorporate retrieved image-text knowledge,\ninterleaved image-text, and lengthy videos. Specifically, we propose novel\nhyper attention blocks to efficiently integrate vision and language into a\ncommon language-guided semantic space, thereby facilitating the processing of\nextended multi-image scenarios. Extensive experimental results suggest that\nmPLUG-Owl3 achieves state-of-the-art performance among models with a similar\nsize on single-image, multi-image, and video benchmarks. Moreover, we propose a\nchallenging long visual sequence evaluation named Distractor Resistance to\nassess the ability of models to maintain focus amidst distractions. Finally,\nwith the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance\non ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to\nthe development of more efficient and powerful multimodal large language\nmodels.",
        "pos": [
            "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous\nLLMs into one with all the capabilities. Ideally, any LLMs sharing the same\nbackbone should be mergeable, irrespective of whether they are Fine-Tuned (FT)\nwith minor parameter changes or Pre-Trained (PT) with substantial parameter\nshifts. However, existing methods often manually assign the model importance,\nrendering them feasible only for LLMs with similar parameter alterations, such\nas multiple FT LLMs. The diverse parameter changed ranges between FT and PT\nLLMs pose challenges for current solutions in empirically determining the\noptimal combination. In this paper, we make a pioneering effort to broaden the\napplicability of merging techniques from FT to PT LLMs. We initially examine\nthe efficacy of current methods in merging FT and PT LLMs, discovering that\nthey struggle to deal with PT LLMs. Subsequently, we introduce an approach\nbased on WeIght DisENtanglement (WIDEN) to effectively extend the merging\nscope, which first disentangles model weights into magnitude and direction\ncomponents, and then performs adaptive fusion by considering their respective\ncontributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with\ninstruction-following skills) with Sailor (a PT LLM with multilingual\nabilities) across 7B and 14B model scales. Results reveal that: (1) existing\nsolutions usually fail when merging Sailor, either losing both abilities or\nonly retaining instruction-following skills; (2) WIDEN successfully injects the\nmultilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in\nSoutheast Asian languages, achieving enhancements in the fundamental\ncapabilities. In light of previous research, we also merge multiple 13B FT LLMs\nand observe that WIDEN achieves a balanced amalgamation of instruction\nfollowing, mathematical reasoning, and code generation skills.",
            "We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.",
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in executing instructions for a variety of single-image tasks.\nDespite this progress, significant challenges remain in modeling long image\nsequences. In this work, we introduce the versatile multi-modal large language\nmodel, mPLUG-Owl3, which enhances the capability for long image-sequence\nunderstanding in scenarios that incorporate retrieved image-text knowledge,\ninterleaved image-text, and lengthy videos. Specifically, we propose novel\nhyper attention blocks to efficiently integrate vision and language into a\ncommon language-guided semantic space, thereby facilitating the processing of\nextended multi-image scenarios. Extensive experimental results suggest that\nmPLUG-Owl3 achieves state-of-the-art performance among models with a similar\nsize on single-image, multi-image, and video benchmarks. Moreover, we propose a\nchallenging long visual sequence evaluation named Distractor Resistance to\nassess the ability of models to maintain focus amidst distractions. Finally,\nwith the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance\non ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to\nthe development of more efficient and powerful multimodal large language\nmodels.",
        "pos": [
            "We initiate a formal investigation into the design and analysis of LLM-based\nalgorithms, i.e. algorithms that contain one or multiple calls of large\nlanguage models (LLMs) as sub-routines and critically rely on the capabilities\nof LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt\nengineering to complicated LLM-powered agent systems and compound AI systems,\nhave achieved remarkable empirical success, the design and optimization of them\nhave mostly relied on heuristics and trial-and-errors, which is largely due to\na lack of formal and analytical study for these algorithms. To fill this gap,\nwe start by identifying the computational-graph representation of LLM-based\nalgorithms, the design principle of task decomposition, and some key\nabstractions, which then facilitate our formal analysis for the accuracy and\nefficiency of LLM-based algorithms, despite the black-box nature of LLMs. We\nfurther consider parallel decomposition for a case study, providing extensive\nanalytical and empirical study for four concrete examples of this pattern. Our\nproposed framework holds promise for advancing LLM-based algorithms, by\nrevealing the reasons behind curious empirical phenomena, guiding the choices\nof hyperparameters, predicting the empirical performance of algorithms, and\ninspiring new algorithm design. To promote further study of LLM-based\nalgorithms, we release our source code at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.",
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.",
        "pos": [
            "The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs."
        ],
        "neg": []
    },
    {
        "query": "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.",
        "pos": [
            "Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.",
            "Large Language Models (LLMs) are rapidly surpassing human knowledge in many\ndomains. While improving these models traditionally relies on costly human\ndata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs\ncan improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation\nduring iterative training. To address this issue, we introduce a novel\nMeta-Rewarding step to the self-improvement process, where the model judges its\nown judgements and uses that feedback to refine its judgment skills.\nSurprisingly, this unsupervised approach improves the model's ability to judge\n{\\em and} follow instructions, as demonstrated by a win rate improvement of\nLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on\nArena-Hard. These results strongly suggest the potential for self-improving\nmodels without human supervision.",
            "Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well."
        ],
        "neg": []
    },
    {
        "query": "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.",
        "pos": [
            "Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples."
        ],
        "neg": []
    },
    {
        "query": "In this work, we use language modeling to investigate the factors that\ninfluence code-switching. Code-switching occurs when a speaker alternates\nbetween one language variety (the primary language) and another (the secondary\nlanguage), and is widely observed in multilingual contexts. Recent work has\nshown that code-switching is often correlated with areas of high information\nload in the primary language, but it is unclear whether high primary language\nload only makes the secondary language relatively easier to produce at\ncode-switching points (speaker-driven code-switching), or whether\ncode-switching is additionally used by speakers to signal the need for greater\nattention on the part of listeners (audience-driven code-switching). In this\npaper, we use bilingual Chinese-English online forum posts and transcripts of\nspontaneous Chinese-English speech to replicate prior findings that high\nprimary language (Chinese) information load is correlated with switches to the\nsecondary language (English). We then demonstrate that the information load of\nthe English productions is even higher than that of meaning equivalent Chinese\nalternatives, and these are therefore not easier to produce, providing evidence\nof audience-driven influences in code-switching at the level of the\ncommunication channel, not just at the sociolinguistic level, in both writing\nand speech.",
        "pos": [
            "Pretrained language model (PLM) hidden states are frequently employed as\ncontextual word embeddings (CWE): high-dimensional representations that encode\nsemantic information given linguistic context. Across many areas of\ncomputational linguistics research, similarity between CWEs is interpreted as\nsemantic similarity. However, it remains unclear exactly what information is\nencoded in PLM hidden states. We investigate this practice by probing PLM\nrepresentations using minimal orthographic noise. We expect that if CWEs\nprimarily encode semantic information, a single character swap in the input\nword will not drastically affect the resulting representation,given sufficient\nlinguistic context. Surprisingly, we find that CWEs generated by popular PLMs\nare highly sensitive to noise in input data, and that this sensitivity is\nrelated to subword tokenization: the fewer tokens used to represent a word at\ninput, the more sensitive its corresponding CWE. This suggests that CWEs\ncapture information unrelated to word-level meaning and can be manipulated\nthrough trivial modifications of input data. We conclude that these PLM-derived\nCWEs may not be reliable semantic proxies, and that caution is warranted when\ninterpreting representational similarity"
        ],
        "neg": []
    },
    {
        "query": "Despite the impressive performance on information-seeking tasks, large\nlanguage models (LLMs) still struggle with hallucinations. Attributed LLMs,\nwhich augment generated text with in-line citations, have shown potential in\nmitigating hallucinations and improving verifiability. However, current\napproaches suffer from suboptimal citation quality due to their reliance on\nin-context learning. Furthermore, the practice of citing only coarse document\nidentifiers makes it challenging for users to perform fine-grained\nverification. In this work, we introduce FRONT, a training framework designed\nto teach LLMs to generate Fine-Grained Grounded Citations. By grounding model\noutputs in fine-grained supporting quotes, these quotes guide the generation of\ngrounded and consistent responses, not only improving citation quality but also\nfacilitating fine-grained verification. Experiments on the ALCE benchmark\ndemonstrate the efficacy of FRONT in generating superior grounded responses and\nhighly supportive citations. With LLaMA-2-7B, the framework significantly\noutperforms all the baselines, achieving an average of 14.21% improvement in\ncitation quality across all datasets, even surpassing ChatGPT.",
        "pos": [
            "Through reading the documentation in the context, tool-using language models\ncan dynamically extend their capability using external tools. The cost is that\nwe have to input lengthy documentation every time the model needs to use the\ntool, occupying the input window as well as slowing down the decoding process.\n  Given the progress in general-purpose compression, soft context compression\nis a suitable approach to alleviate the problem. However, when compressing tool\ndocumentation, existing methods suffer from the weaknesses of key information\nloss (specifically, tool/parameter name errors) and difficulty in adjusting the\nlength of compressed sequences based on documentation lengths.\n  To address these problems, we propose two strategies for compressing tool\ndocumentation into concise and precise summary sequences for tool-using\nlanguage models. 1) Selective compression strategy mitigates key information\nloss by deliberately retaining key information as raw text tokens. 2) Block\ncompression strategy involves dividing tool documentation into short chunks and\nthen employing a fixed-length compression model to achieve variable-length\ncompression. This strategy facilitates the flexible adjustment of the\ncompression ratio.\n  Results on API-Bank and APIBench show that our approach reaches a performance\ncomparable to the upper-bound baseline under up to 16x compression ratio."
        ],
        "neg": []
    },
    {
        "query": "Despite the impressive performance on information-seeking tasks, large\nlanguage models (LLMs) still struggle with hallucinations. Attributed LLMs,\nwhich augment generated text with in-line citations, have shown potential in\nmitigating hallucinations and improving verifiability. However, current\napproaches suffer from suboptimal citation quality due to their reliance on\nin-context learning. Furthermore, the practice of citing only coarse document\nidentifiers makes it challenging for users to perform fine-grained\nverification. In this work, we introduce FRONT, a training framework designed\nto teach LLMs to generate Fine-Grained Grounded Citations. By grounding model\noutputs in fine-grained supporting quotes, these quotes guide the generation of\ngrounded and consistent responses, not only improving citation quality but also\nfacilitating fine-grained verification. Experiments on the ALCE benchmark\ndemonstrate the efficacy of FRONT in generating superior grounded responses and\nhighly supportive citations. With LLaMA-2-7B, the framework significantly\noutperforms all the baselines, achieving an average of 14.21% improvement in\ncitation quality across all datasets, even surpassing ChatGPT.",
        "pos": [
            "Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.",
            "To improve the performance of large language models (LLMs), researchers have\nexplored providing LLMs with textual task-solving experience via prompts.\nHowever, they rely on manual efforts to acquire and apply such experience for\neach task, which is not feasible for the growing demand for LLMs and the\nvariety of user questions. To address this issue, we design a lifelong\nautonomous experiential learning framework based on LLMs to explore whether\nLLMs can imitate human ability for learning and utilizing experience. It\nautonomously learns and accumulates experience through experience transfer and\ninduction, categorizing the types of input questions to select which\naccumulated experience to employ for them. Experimental results on six widely\nused NLP datasets show that our framework performs reliably in each\nintermediate step and effectively improves the performance of GPT-3.5 and\nGPT-4. This validates the feasibility of using LLMs to mimic human experiential\nlearning and application capabilities. Additionally, we provide a detailed\nanalysis of the behavior of our framework at each step."
        ],
        "neg": []
    },
    {
        "query": "Due to the cost-prohibitive nature of training Large Language Models (LLMs),\nfine-tuning has emerged as an attractive alternative for specializing LLMs for\nspecific tasks using limited compute resources in a cost-effective manner. In\nthis paper, we characterize sparse Mixture of Experts (MoE) based LLM\nfine-tuning to understand their accuracy and runtime performance on a single\nGPU. Our evaluation provides unique insights into the training efficacy of\nsparse and dense versions of MoE models, as well as their runtime\ncharacteristics, including maximum batch size, execution time breakdown,\nend-to-end throughput, GPU hardware utilization, and load distribution. Our\nstudy identifies the optimization of the MoE layer as crucial for further\nimproving the performance of LLM fine-tuning. Using our profiling results, we\nalso develop and validate an analytical model to estimate the cost of LLM\nfine-tuning on the cloud. This model, based on parameters of the model and GPU\narchitecture, estimates LLM throughput and the cost of training, aiding\npractitioners in industry and academia to budget the cost of fine-tuning a\nspecific model.",
        "pos": [
            "In this paper, we present STBLLM, the first structural binarization framework\nfor compressing Large Language Models (LLMs) to less than 1-bit precision. LLMs\nhave achieved remarkable performance, but their heavy memory requirements have\nhindered widespread adoption, particularly on resource-constrained devices.\nBinarization, which quantifies weights to a mere 1-bit, achieves a milestone in\nincreasing computational efficiency. However, we observe that some weights in\nbinarized LLMs can be randomly flipped without significant performance\ndegradation, indicating the potential for further compression. To exploit this,\nour STBLLM employs an N:M sparsity to perform structural binarization of the\nweights. First, we introduce a new Standardized Importance (SI) metric that\nconsiders weight magnitude and input feature norm to better evaluate weight\nsignificance. Then, we propose a layer-wise approach where different layers of\nthe LLM can be sparsified with varying N:M ratios, balancing compression and\naccuracy. Finally, we use residual approximation with double binarization to\npreserve information for salient weights. In addition, we utilize a\nfine-grained grouping strategy for less important weights that applies\ndifferent quantization schemes to sparse, intermediate, and dense regions. We\nconduct extensive experiments on various language models, including the\nLLaMA-1/2/3, OPT family, and Mistral, to evaluate the effectiveness of STBLLM.\nThe results demonstrate that our approach performs better than other compressed\nbinarization LLM methods while significantly reducing memory requirements."
        ],
        "neg": []
    },
    {
        "query": "Due to the cost-prohibitive nature of training Large Language Models (LLMs),\nfine-tuning has emerged as an attractive alternative for specializing LLMs for\nspecific tasks using limited compute resources in a cost-effective manner. In\nthis paper, we characterize sparse Mixture of Experts (MoE) based LLM\nfine-tuning to understand their accuracy and runtime performance on a single\nGPU. Our evaluation provides unique insights into the training efficacy of\nsparse and dense versions of MoE models, as well as their runtime\ncharacteristics, including maximum batch size, execution time breakdown,\nend-to-end throughput, GPU hardware utilization, and load distribution. Our\nstudy identifies the optimization of the MoE layer as crucial for further\nimproving the performance of LLM fine-tuning. Using our profiling results, we\nalso develop and validate an analytical model to estimate the cost of LLM\nfine-tuning on the cloud. This model, based on parameters of the model and GPU\narchitecture, estimates LLM throughput and the cost of training, aiding\npractitioners in industry and academia to budget the cost of fine-tuning a\nspecific model.",
        "pos": [
            "Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Metron, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Metron, discussing their strengths and weaknesses. Metron is\navailable at https://github.com/project-metron/metron."
        ],
        "neg": []
    },
    {
        "query": "Prompts are how humans communicate with LLMs. Informative prompts are\nessential for guiding LLMs to produce the desired output. However, prompt\nengineering is often tedious and time-consuming, requiring significant\nexpertise, limiting its widespread use. We propose Conversational Prompt\nEngineering (CPE), a user-friendly tool that helps users create personalized\nprompts for their specific tasks. CPE uses a chat model to briefly interact\nwith users, helping them articulate their output preferences and integrating\nthese into the prompt. The process includes two main stages: first, the model\nuses user-provided unlabeled data to generate data-driven questions and utilize\nuser responses to shape the initial instruction. Then, the model shares the\noutputs generated by the instruction and uses user feedback to further refine\nthe instruction and the outputs. The final result is a few-shot prompt, where\nthe outputs approved by the user serve as few-shot examples. A user study on\nsummarization tasks demonstrates the value of CPE in creating personalized,\nhigh-performing prompts. The results suggest that the zero-shot prompt obtained\nis comparable to its - much longer - few-shot counterpart, indicating\nsignificant savings in scenarios involving repetitive tasks with large text\nvolumes.",
        "pos": [
            "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners."
        ],
        "neg": []
    },
    {
        "query": "Prompts are how humans communicate with LLMs. Informative prompts are\nessential for guiding LLMs to produce the desired output. However, prompt\nengineering is often tedious and time-consuming, requiring significant\nexpertise, limiting its widespread use. We propose Conversational Prompt\nEngineering (CPE), a user-friendly tool that helps users create personalized\nprompts for their specific tasks. CPE uses a chat model to briefly interact\nwith users, helping them articulate their output preferences and integrating\nthese into the prompt. The process includes two main stages: first, the model\nuses user-provided unlabeled data to generate data-driven questions and utilize\nuser responses to shape the initial instruction. Then, the model shares the\noutputs generated by the instruction and uses user feedback to further refine\nthe instruction and the outputs. The final result is a few-shot prompt, where\nthe outputs approved by the user serve as few-shot examples. A user study on\nsummarization tasks demonstrates the value of CPE in creating personalized,\nhigh-performing prompts. The results suggest that the zero-shot prompt obtained\nis comparable to its - much longer - few-shot counterpart, indicating\nsignificant savings in scenarios involving repetitive tasks with large text\nvolumes.",
        "pos": [
            "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners."
        ],
        "neg": []
    },
    {
        "query": "Prompts are how humans communicate with LLMs. Informative prompts are\nessential for guiding LLMs to produce the desired output. However, prompt\nengineering is often tedious and time-consuming, requiring significant\nexpertise, limiting its widespread use. We propose Conversational Prompt\nEngineering (CPE), a user-friendly tool that helps users create personalized\nprompts for their specific tasks. CPE uses a chat model to briefly interact\nwith users, helping them articulate their output preferences and integrating\nthese into the prompt. The process includes two main stages: first, the model\nuses user-provided unlabeled data to generate data-driven questions and utilize\nuser responses to shape the initial instruction. Then, the model shares the\noutputs generated by the instruction and uses user feedback to further refine\nthe instruction and the outputs. The final result is a few-shot prompt, where\nthe outputs approved by the user serve as few-shot examples. A user study on\nsummarization tasks demonstrates the value of CPE in creating personalized,\nhigh-performing prompts. The results suggest that the zero-shot prompt obtained\nis comparable to its - much longer - few-shot counterpart, indicating\nsignificant savings in scenarios involving repetitive tasks with large text\nvolumes.",
        "pos": [
            "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners."
        ],
        "neg": []
    },
    {
        "query": "Prompts are how humans communicate with LLMs. Informative prompts are\nessential for guiding LLMs to produce the desired output. However, prompt\nengineering is often tedious and time-consuming, requiring significant\nexpertise, limiting its widespread use. We propose Conversational Prompt\nEngineering (CPE), a user-friendly tool that helps users create personalized\nprompts for their specific tasks. CPE uses a chat model to briefly interact\nwith users, helping them articulate their output preferences and integrating\nthese into the prompt. The process includes two main stages: first, the model\nuses user-provided unlabeled data to generate data-driven questions and utilize\nuser responses to shape the initial instruction. Then, the model shares the\noutputs generated by the instruction and uses user feedback to further refine\nthe instruction and the outputs. The final result is a few-shot prompt, where\nthe outputs approved by the user serve as few-shot examples. A user study on\nsummarization tasks demonstrates the value of CPE in creating personalized,\nhigh-performing prompts. The results suggest that the zero-shot prompt obtained\nis comparable to its - much longer - few-shot counterpart, indicating\nsignificant savings in scenarios involving repetitive tasks with large text\nvolumes.",
        "pos": [
            "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners."
        ],
        "neg": []
    },
    {
        "query": "Prompts are how humans communicate with LLMs. Informative prompts are\nessential for guiding LLMs to produce the desired output. However, prompt\nengineering is often tedious and time-consuming, requiring significant\nexpertise, limiting its widespread use. We propose Conversational Prompt\nEngineering (CPE), a user-friendly tool that helps users create personalized\nprompts for their specific tasks. CPE uses a chat model to briefly interact\nwith users, helping them articulate their output preferences and integrating\nthese into the prompt. The process includes two main stages: first, the model\nuses user-provided unlabeled data to generate data-driven questions and utilize\nuser responses to shape the initial instruction. Then, the model shares the\noutputs generated by the instruction and uses user feedback to further refine\nthe instruction and the outputs. The final result is a few-shot prompt, where\nthe outputs approved by the user serve as few-shot examples. A user study on\nsummarization tasks demonstrates the value of CPE in creating personalized,\nhigh-performing prompts. The results suggest that the zero-shot prompt obtained\nis comparable to its - much longer - few-shot counterpart, indicating\nsignificant savings in scenarios involving repetitive tasks with large text\nvolumes.",
        "pos": [
            "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners."
        ],
        "neg": []
    },
    {
        "query": "Prompts are how humans communicate with LLMs. Informative prompts are\nessential for guiding LLMs to produce the desired output. However, prompt\nengineering is often tedious and time-consuming, requiring significant\nexpertise, limiting its widespread use. We propose Conversational Prompt\nEngineering (CPE), a user-friendly tool that helps users create personalized\nprompts for their specific tasks. CPE uses a chat model to briefly interact\nwith users, helping them articulate their output preferences and integrating\nthese into the prompt. The process includes two main stages: first, the model\nuses user-provided unlabeled data to generate data-driven questions and utilize\nuser responses to shape the initial instruction. Then, the model shares the\noutputs generated by the instruction and uses user feedback to further refine\nthe instruction and the outputs. The final result is a few-shot prompt, where\nthe outputs approved by the user serve as few-shot examples. A user study on\nsummarization tasks demonstrates the value of CPE in creating personalized,\nhigh-performing prompts. The results suggest that the zero-shot prompt obtained\nis comparable to its - much longer - few-shot counterpart, indicating\nsignificant savings in scenarios involving repetitive tasks with large text\nvolumes.",
        "pos": [
            "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners."
        ],
        "neg": []
    },
    {
        "query": "Prompts are how humans communicate with LLMs. Informative prompts are\nessential for guiding LLMs to produce the desired output. However, prompt\nengineering is often tedious and time-consuming, requiring significant\nexpertise, limiting its widespread use. We propose Conversational Prompt\nEngineering (CPE), a user-friendly tool that helps users create personalized\nprompts for their specific tasks. CPE uses a chat model to briefly interact\nwith users, helping them articulate their output preferences and integrating\nthese into the prompt. The process includes two main stages: first, the model\nuses user-provided unlabeled data to generate data-driven questions and utilize\nuser responses to shape the initial instruction. Then, the model shares the\noutputs generated by the instruction and uses user feedback to further refine\nthe instruction and the outputs. The final result is a few-shot prompt, where\nthe outputs approved by the user serve as few-shot examples. A user study on\nsummarization tasks demonstrates the value of CPE in creating personalized,\nhigh-performing prompts. The results suggest that the zero-shot prompt obtained\nis comparable to its - much longer - few-shot counterpart, indicating\nsignificant savings in scenarios involving repetitive tasks with large text\nvolumes.",
        "pos": [
            "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners."
        ],
        "neg": []
    },
    {
        "query": "Whether or not several Creole languages which developed during the early\nmodern period can be considered genetic descendants of European languages has\nbeen the subject of intense debate. This is in large part due to the absence of\nevidence of intermediate forms. This work introduces a new open corpus, the\nMoly\\'e corpus, which combines stereotypical representations of three kinds of\nlanguage variation in Europe with early attestations of French-based Creole\nlanguages across a period of 400 years. It is intended to facilitate future\nresearch on the continuity between contact situations in Europe and Creolophone\n(former) colonies.",
        "pos": [
            "The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. In this paper, we focus\non machine translation (MT), a task that has been shown to benefit from\nin-context translation examples. However no systematic studies have been\npublished on how best to select examples, and mixed results have been reported\non the usefulness of similarity-based selection over random selection. We\nprovide a study covering multiple LLMs and multiple in-context example\nretrieval strategies, comparing multilingual sentence embeddings. We cover\nseveral language directions, representing different levels of language\nresourcedness (English into French, German, Swahili and Wolof). Contrarily to\npreviously published results, we find that sentence embedding similarity can\nimprove MT, especially for low-resource language directions, and discuss the\nbalance between selection pool diversity and quality. We also highlight\npotential problems with the evaluation of LLM-based MT and suggest a more\nappropriate evaluation protocol, adapting the COMET metric to the evaluation of\nLLMs. Code and outputs are freely available at\nhttps://github.com/ArmelRandy/ICL-MT.",
            "Current multimodal machine translation (MMT) systems rely on fully supervised\ndata (i.e models are trained on sentences with their translations and\naccompanying images). However, this type of data is costly to collect, limiting\nthe extension of MMT to other language pairs for which such data does not\nexist. In this work, we propose a method to bypass the need for fully\nsupervised data to train MMT systems, using multimodal English data only. Our\nmethod, called ZeroMMT, consists in adapting a strong text-only machine\ntranslation (MT) model by training it on a mixture of two objectives: visually\nconditioned masked language modelling and the Kullback-Leibler divergence\nbetween the original and new MMT outputs. We evaluate on standard MMT\nbenchmarks and the recently released CoMMuTE, a contrastive benchmark aiming to\nevaluate how well models use images to disambiguate English sentences. We\nobtain disambiguation performance close to state-of-the-art MMT models trained\nadditionally on fully supervised examples. To prove that our method generalizes\nto languages with no fully supervised training data available, we extend the\nCoMMuTE evaluation dataset to three new languages: Arabic, Russian and Chinese.\nWe further show that we can control the trade-off between disambiguation\ncapabilities and translation fidelity at inference time using classifier-free\nguidance and without any additional data. Our code, data and trained models are\npublicly accessible."
        ],
        "neg": []
    },
    {
        "query": "This paper focuses on detecting propagandistic spans and persuasion\ntechniques in Arabic text from tweets and news paragraphs. Each entry in the\ndataset contains a text sample and corresponding labels that indicate the start\nand end positions of propaganda techniques within the text. Tokens falling\nwithin a labeled span were assigned \"B\" (Begin) or \"I\" (Inside), \"O\",\ncorresponding to the specific propaganda technique. Using attention masks, we\ncreated uniform lengths for each span and assigned BIO tags to each token based\non the provided labels. Then, we used AraBERT-base pre-trained model for Arabic\ntext tokenization and embeddings with a token classification layer to identify\npropaganda techniques. Our training process involves a two-phase fine-tuning\napproach. First, we train only the classification layer for a few epochs,\nfollowed by full model fine-tuning, updating all parameters. This methodology\nallows the model to adapt to the specific characteristics of the propaganda\ndetection task while leveraging the knowledge captured by the pre-trained\nAraBERT model. Our approach achieved an F1 score of 0.2774, securing the 3rd\nposition in the leaderboard of Task 1.",
        "pos": [
            "We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed.",
            "This study addresses a binary classification task to determine whether a text\nsequence, either a sentence or paragraph, is subjective or objective. The task\nspans five languages: Arabic, Bulgarian, English, German, and Italian, along\nwith a multilingual category. Our approach involved several key techniques.\nInitially, we preprocessed the data through parts of speech (POS) tagging,\nidentification of question marks, and application of attention masks. We\nfine-tuned the sentiment-based Transformer model\n'MarieAngeA13/Sentiment-Analysis-BERT' on our dataset. Given the imbalance with\nmore objective data, we implemented a custom classifier that assigned greater\nweight to objective data. Additionally, we translated non-English data into\nEnglish to maintain consistency across the dataset. Our model achieved notable\nresults, scoring top marks for the multilingual dataset (Macro F1=0.7121) and\nGerman (Macro F1=0.7908). It ranked second for Arabic (Macro F1=0.4908) and\nBulgarian (Macro F1=0.7169), third for Italian (Macro F1=0.7430), and ninth for\nEnglish (Macro F1=0.6893).",
            "We present an overview of the second edition of the ArAIEval shared task,\norganized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In\nthis edition, ArAIEval offers two tasks: (i) detection of propagandistic\ntextual spans with persuasion techniques identification in tweets and news\narticles, and (ii) distinguishing between propagandistic and non-propagandistic\nmemes. A total of 14 teams participated in the final evaluation phase, with 6\nand 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams\nsubmitted system description papers. Across both tasks, we observed that\nfine-tuning transformer models such as AraBERT was at the core of the majority\nof the participating systems. We provide a description of the task setup,\nincluding a description of the dataset construction and the evaluation setup.\nWe further provide a brief overview of the participating systems. All datasets\nand evaluation scripts are released to the research community\n(https://araieval.gitlab.io/). We hope this will enable further research on\nthese important tasks in Arabic."
        ],
        "neg": []
    },
    {
        "query": "As diverse linguistic communities and users adopt large language models\n(LLMs), assessing their safety across languages becomes critical. Despite\nongoing efforts to make LLMs safe, they can still be made to behave unsafely\nwith jailbreaking, a technique in which models are prompted to act outside\ntheir operational guidelines. Research on LLM safety and jailbreaking, however,\nhas so far mostly focused on English, limiting our understanding of LLM safety\nin other languages. We contribute towards closing this gap by investigating the\neffectiveness of many-shot jailbreaking, where models are prompted with unsafe\ndemonstrations to induce unsafe behaviour, in Italian. To enable our analysis,\nwe create a new dataset of unsafe Italian question-answer pairs. With this\ndataset, we identify clear safety vulnerabilities in four families of\nopen-weight LLMs. We find that the models exhibit unsafe behaviors even when\nprompted with few unsafe demonstrations, and -- more alarmingly -- that this\ntendency rapidly escalates with more demonstrations.",
        "pos": [
            "Emotions play important epistemological and cognitive roles in our lives,\nrevealing our values and guiding our actions. Previous work has shown that LLMs\ndisplay biases in emotion attribution along gender lines. However, unlike\ngender, which says little about our values, religion, as a socio-cultural\nsystem, prescribes a set of beliefs and values for its followers. Religions,\ntherefore, cultivate certain emotions. Moreover, these rules are explicitly\nlaid out and interpreted by religious leaders. Using emotion attribution, we\nexplore how different religions are represented in LLMs. We find that: Major\nreligions in the US and European countries are represented with more nuance,\ndisplaying a more shaded model of their beliefs. Eastern religions like\nHinduism and Buddhism are strongly stereotyped. Judaism and Islam are\nstigmatized -- the models' refusal skyrocket. We ascribe these to cultural bias\nin LLMs and the scarcity of NLP literature on religion. In the rare instances\nwhere religion is discussed, it is often in the context of toxic language,\nperpetuating the perception of these religions as inherently toxic. This\nfinding underscores the urgent need to address and rectify these biases. Our\nresearch underscores the crucial role emotions play in our lives and how our\nvalues influence them."
        ],
        "neg": []
    },
    {
        "query": "Competitive debate is a comprehensive and complex computational argumentation\ntask. Large Language Models (LLMs) encounter hallucinations and lack\ncompetitiveness in this task. To address these challenges, we introduce Agent\nfor Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs\ndesigned to enhance their capabilities in competitive debate. Drawing\ninspiration from human behavior in debate preparation and execution,\nAgent4Debate employs a collaborative architecture where four specialized agents\n(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.\nThese agents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Chinese Debate\nArena, comprising 66 carefully selected Chinese debate motions. We recruite ten\nexperienced human debaters and collect records of 200 debates involving\nAgent4Debate, baseline models, and humans. The evaluation employs the Debatrix\nautomatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.",
        "pos": [
            "Affective Computing (AC), integrating computer science, psychology, and\ncognitive science knowledge, aims to enable machines to recognize, interpret,\nand simulate human emotions.To create more value, AC can be applied to diverse\nscenarios, including social media, finance, healthcare, education, etc.\nAffective Computing (AC) includes two mainstream tasks, i.e., Affective\nUnderstanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained\nLanguage Models (PLMs) for AU tasks has succeeded considerably. However, these\nmodels lack generalization ability, requiring specialized models for specific\ntasks. Additionally, traditional PLMs face challenges in AG, particularly in\ngenerating diverse and emotionally rich responses. The emergence of Large\nLanguage Models (LLMs), such as the ChatGPT series and LLaMA models, brings new\nopportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess\ncapabilities of in-context learning, common sense reasoning, and advanced\nsequence generation, which present unprecedented opportunities for AU. To\nprovide a comprehensive overview of AC in the LLMs era from an NLP perspective,\nwe summarize the development of LLMs research in this field, aiming to offer\nnew insights. Specifically, we first summarize the traditional tasks related to\nAC and introduce the preliminary study based on LLMs. Subsequently, we outline\nthe relevant techniques of popular LLMs to improve AC tasks, including\nInstruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss\nfull parameter fine-tuning and parameter-efficient methods such as LoRA,\nP-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,\nFew-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To\nclearly understand the performance of LLMs on different Affective Computing\ntasks, we further summarize the existing benchmarks and evaluation methods."
        ],
        "neg": []
    },
    {
        "query": "Competitive debate is a comprehensive and complex computational argumentation\ntask. Large Language Models (LLMs) encounter hallucinations and lack\ncompetitiveness in this task. To address these challenges, we introduce Agent\nfor Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs\ndesigned to enhance their capabilities in competitive debate. Drawing\ninspiration from human behavior in debate preparation and execution,\nAgent4Debate employs a collaborative architecture where four specialized agents\n(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.\nThese agents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Chinese Debate\nArena, comprising 66 carefully selected Chinese debate motions. We recruite ten\nexperienced human debaters and collect records of 200 debates involving\nAgent4Debate, baseline models, and humans. The evaluation employs the Debatrix\nautomatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.",
        "pos": [
            "Affective Computing (AC), integrating computer science, psychology, and\ncognitive science knowledge, aims to enable machines to recognize, interpret,\nand simulate human emotions.To create more value, AC can be applied to diverse\nscenarios, including social media, finance, healthcare, education, etc.\nAffective Computing (AC) includes two mainstream tasks, i.e., Affective\nUnderstanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained\nLanguage Models (PLMs) for AU tasks has succeeded considerably. However, these\nmodels lack generalization ability, requiring specialized models for specific\ntasks. Additionally, traditional PLMs face challenges in AG, particularly in\ngenerating diverse and emotionally rich responses. The emergence of Large\nLanguage Models (LLMs), such as the ChatGPT series and LLaMA models, brings new\nopportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess\ncapabilities of in-context learning, common sense reasoning, and advanced\nsequence generation, which present unprecedented opportunities for AU. To\nprovide a comprehensive overview of AC in the LLMs era from an NLP perspective,\nwe summarize the development of LLMs research in this field, aiming to offer\nnew insights. Specifically, we first summarize the traditional tasks related to\nAC and introduce the preliminary study based on LLMs. Subsequently, we outline\nthe relevant techniques of popular LLMs to improve AC tasks, including\nInstruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss\nfull parameter fine-tuning and parameter-efficient methods such as LoRA,\nP-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,\nFew-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To\nclearly understand the performance of LLMs on different Affective Computing\ntasks, we further summarize the existing benchmarks and evaluation methods."
        ],
        "neg": []
    },
    {
        "query": "Competitive debate is a comprehensive and complex computational argumentation\ntask. Large Language Models (LLMs) encounter hallucinations and lack\ncompetitiveness in this task. To address these challenges, we introduce Agent\nfor Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs\ndesigned to enhance their capabilities in competitive debate. Drawing\ninspiration from human behavior in debate preparation and execution,\nAgent4Debate employs a collaborative architecture where four specialized agents\n(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.\nThese agents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Chinese Debate\nArena, comprising 66 carefully selected Chinese debate motions. We recruite ten\nexperienced human debaters and collect records of 200 debates involving\nAgent4Debate, baseline models, and humans. The evaluation employs the Debatrix\nautomatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.",
        "pos": [
            "Affective Computing (AC), integrating computer science, psychology, and\ncognitive science knowledge, aims to enable machines to recognize, interpret,\nand simulate human emotions.To create more value, AC can be applied to diverse\nscenarios, including social media, finance, healthcare, education, etc.\nAffective Computing (AC) includes two mainstream tasks, i.e., Affective\nUnderstanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained\nLanguage Models (PLMs) for AU tasks has succeeded considerably. However, these\nmodels lack generalization ability, requiring specialized models for specific\ntasks. Additionally, traditional PLMs face challenges in AG, particularly in\ngenerating diverse and emotionally rich responses. The emergence of Large\nLanguage Models (LLMs), such as the ChatGPT series and LLaMA models, brings new\nopportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess\ncapabilities of in-context learning, common sense reasoning, and advanced\nsequence generation, which present unprecedented opportunities for AU. To\nprovide a comprehensive overview of AC in the LLMs era from an NLP perspective,\nwe summarize the development of LLMs research in this field, aiming to offer\nnew insights. Specifically, we first summarize the traditional tasks related to\nAC and introduce the preliminary study based on LLMs. Subsequently, we outline\nthe relevant techniques of popular LLMs to improve AC tasks, including\nInstruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss\nfull parameter fine-tuning and parameter-efficient methods such as LoRA,\nP-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,\nFew-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To\nclearly understand the performance of LLMs on different Affective Computing\ntasks, we further summarize the existing benchmarks and evaluation methods.",
            "Language models are capable of iteratively improving their outputs based on\nnatural language feedback, thus enabling in-context optimization of user\npreference. In place of human users, a second language model can be used as an\nevaluator, providing feedback along with numerical ratings which the generator\nattempts to optimize. However, because the evaluator is an imperfect proxy of\nuser preference, this optimization can lead to reward hacking, where the\nevaluator's ratings improve while the generation quality remains stagnant or\neven decreases as judged by actual user preference. The concern of reward\nhacking is heightened in iterative self-refinement where the generator and the\nevaluator use the same underlying language model, in which case the\noptimization pressure can drive them to exploit shared vulnerabilities. Using\nan essay editing task, we show that iterative self-refinement leads to\ndeviation between the language model evaluator and human judgment,\ndemonstrating that reward hacking can occur spontaneously in-context with the\nuse of iterative self-refinement. In addition, we study conditions under which\nreward hacking occurs and observe two factors that affect reward hacking\nseverity: model size and context sharing between the generator and the\nevaluator.",
            "Mechanistic interpretability (MI) is an emerging sub-field of\ninterpretability that seeks to understand a neural network model by\nreverse-engineering its internal computations. Recently, MI has garnered\nsignificant attention for interpreting transformer-based language models (LMs),\nresulting in many novel insights yet introducing new challenges. However, there\nhas not been work that comprehensively reviews these insights and challenges,\nparticularly as a guide for newcomers to this field. To fill this gap, we\npresent a comprehensive survey outlining fundamental objects of study in MI,\ntechniques that have been used for its investigation, approaches for evaluating\nMI results, and significant findings and applications stemming from the use of\nMI to understand LMs. In particular, we present a roadmap for beginners to\nnavigate the field and leverage MI for their benefit. Finally, we also identify\ncurrent gaps in the field and discuss potential future directions."
        ],
        "neg": []
    },
    {
        "query": "Competitive debate is a comprehensive and complex computational argumentation\ntask. Large Language Models (LLMs) encounter hallucinations and lack\ncompetitiveness in this task. To address these challenges, we introduce Agent\nfor Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs\ndesigned to enhance their capabilities in competitive debate. Drawing\ninspiration from human behavior in debate preparation and execution,\nAgent4Debate employs a collaborative architecture where four specialized agents\n(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.\nThese agents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Chinese Debate\nArena, comprising 66 carefully selected Chinese debate motions. We recruite ten\nexperienced human debaters and collect records of 200 debates involving\nAgent4Debate, baseline models, and humans. The evaluation employs the Debatrix\nautomatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.",
        "pos": [
            "Affective Computing (AC), integrating computer science, psychology, and\ncognitive science knowledge, aims to enable machines to recognize, interpret,\nand simulate human emotions.To create more value, AC can be applied to diverse\nscenarios, including social media, finance, healthcare, education, etc.\nAffective Computing (AC) includes two mainstream tasks, i.e., Affective\nUnderstanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained\nLanguage Models (PLMs) for AU tasks has succeeded considerably. However, these\nmodels lack generalization ability, requiring specialized models for specific\ntasks. Additionally, traditional PLMs face challenges in AG, particularly in\ngenerating diverse and emotionally rich responses. The emergence of Large\nLanguage Models (LLMs), such as the ChatGPT series and LLaMA models, brings new\nopportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess\ncapabilities of in-context learning, common sense reasoning, and advanced\nsequence generation, which present unprecedented opportunities for AU. To\nprovide a comprehensive overview of AC in the LLMs era from an NLP perspective,\nwe summarize the development of LLMs research in this field, aiming to offer\nnew insights. Specifically, we first summarize the traditional tasks related to\nAC and introduce the preliminary study based on LLMs. Subsequently, we outline\nthe relevant techniques of popular LLMs to improve AC tasks, including\nInstruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss\nfull parameter fine-tuning and parameter-efficient methods such as LoRA,\nP-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,\nFew-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To\nclearly understand the performance of LLMs on different Affective Computing\ntasks, we further summarize the existing benchmarks and evaluation methods."
        ],
        "neg": []
    },
    {
        "query": "Competitive debate is a comprehensive and complex computational argumentation\ntask. Large Language Models (LLMs) encounter hallucinations and lack\ncompetitiveness in this task. To address these challenges, we introduce Agent\nfor Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs\ndesigned to enhance their capabilities in competitive debate. Drawing\ninspiration from human behavior in debate preparation and execution,\nAgent4Debate employs a collaborative architecture where four specialized agents\n(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.\nThese agents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Chinese Debate\nArena, comprising 66 carefully selected Chinese debate motions. We recruite ten\nexperienced human debaters and collect records of 200 debates involving\nAgent4Debate, baseline models, and humans. The evaluation employs the Debatrix\nautomatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.",
        "pos": [
            "Affective Computing (AC), integrating computer science, psychology, and\ncognitive science knowledge, aims to enable machines to recognize, interpret,\nand simulate human emotions.To create more value, AC can be applied to diverse\nscenarios, including social media, finance, healthcare, education, etc.\nAffective Computing (AC) includes two mainstream tasks, i.e., Affective\nUnderstanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained\nLanguage Models (PLMs) for AU tasks has succeeded considerably. However, these\nmodels lack generalization ability, requiring specialized models for specific\ntasks. Additionally, traditional PLMs face challenges in AG, particularly in\ngenerating diverse and emotionally rich responses. The emergence of Large\nLanguage Models (LLMs), such as the ChatGPT series and LLaMA models, brings new\nopportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess\ncapabilities of in-context learning, common sense reasoning, and advanced\nsequence generation, which present unprecedented opportunities for AU. To\nprovide a comprehensive overview of AC in the LLMs era from an NLP perspective,\nwe summarize the development of LLMs research in this field, aiming to offer\nnew insights. Specifically, we first summarize the traditional tasks related to\nAC and introduce the preliminary study based on LLMs. Subsequently, we outline\nthe relevant techniques of popular LLMs to improve AC tasks, including\nInstruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss\nfull parameter fine-tuning and parameter-efficient methods such as LoRA,\nP-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,\nFew-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To\nclearly understand the performance of LLMs on different Affective Computing\ntasks, we further summarize the existing benchmarks and evaluation methods."
        ],
        "neg": []
    },
    {
        "query": "Competitive debate is a comprehensive and complex computational argumentation\ntask. Large Language Models (LLMs) encounter hallucinations and lack\ncompetitiveness in this task. To address these challenges, we introduce Agent\nfor Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs\ndesigned to enhance their capabilities in competitive debate. Drawing\ninspiration from human behavior in debate preparation and execution,\nAgent4Debate employs a collaborative architecture where four specialized agents\n(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.\nThese agents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Chinese Debate\nArena, comprising 66 carefully selected Chinese debate motions. We recruite ten\nexperienced human debaters and collect records of 200 debates involving\nAgent4Debate, baseline models, and humans. The evaluation employs the Debatrix\nautomatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.",
        "pos": [
            "Affective Computing (AC), integrating computer science, psychology, and\ncognitive science knowledge, aims to enable machines to recognize, interpret,\nand simulate human emotions.To create more value, AC can be applied to diverse\nscenarios, including social media, finance, healthcare, education, etc.\nAffective Computing (AC) includes two mainstream tasks, i.e., Affective\nUnderstanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained\nLanguage Models (PLMs) for AU tasks has succeeded considerably. However, these\nmodels lack generalization ability, requiring specialized models for specific\ntasks. Additionally, traditional PLMs face challenges in AG, particularly in\ngenerating diverse and emotionally rich responses. The emergence of Large\nLanguage Models (LLMs), such as the ChatGPT series and LLaMA models, brings new\nopportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess\ncapabilities of in-context learning, common sense reasoning, and advanced\nsequence generation, which present unprecedented opportunities for AU. To\nprovide a comprehensive overview of AC in the LLMs era from an NLP perspective,\nwe summarize the development of LLMs research in this field, aiming to offer\nnew insights. Specifically, we first summarize the traditional tasks related to\nAC and introduce the preliminary study based on LLMs. Subsequently, we outline\nthe relevant techniques of popular LLMs to improve AC tasks, including\nInstruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss\nfull parameter fine-tuning and parameter-efficient methods such as LoRA,\nP-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,\nFew-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To\nclearly understand the performance of LLMs on different Affective Computing\ntasks, we further summarize the existing benchmarks and evaluation methods."
        ],
        "neg": []
    },
    {
        "query": "Controlling the format of outputs generated by large language models (LLMs)\nis a critical functionality in various applications. Current methods typically\nemploy constrained decoding with rule-based automata or fine-tuning with\nmanually crafted format instructions, both of which struggle with open-domain\nformat requirements. To address this limitation, we introduce a novel framework\nfor controlled generation in LLMs, leveraging user-provided, one-shot QA pairs.\nThis study investigates LLMs' capabilities to follow open-domain, one-shot\nconstraints and replicate the format of the example answers. We observe that\nthis is a non-trivial problem for current LLMs. We also develop a dataset\ncollection methodology for supervised fine-tuning that enhances the open-domain\nformat control of LLMs without degrading output quality, as well as a benchmark\non which we evaluate both the helpfulness and format correctness of LLM\noutputs. The resulting datasets, named OIFC-SFT, along with the related code,\nwill be made publicly available at https://github.com/cofe-ai/OIFC.",
        "pos": [
            "As the academic landscape expands, the challenge of efficiently identifying\npotentially high-impact articles among the vast number of newly published works\nbecomes critical. This paper introduces a promising approach, leveraging the\ncapabilities of fine-tuned LLMs to predict the future impact of newborn\narticles solely based on titles and abstracts. Moving beyond traditional\nmethods heavily reliant on external information, the proposed method discerns\nthe shared semantic features of highly impactful papers from a large collection\nof title-abstract and potential impact pairs. These semantic features are\nfurther utilized to regress an improved metric, TNCSI_SP, which has been\nendowed with value, field, and time normalization properties. Additionally, a\ncomprehensive dataset has been constructed and released for fine-tuning the\nLLM, containing over 12,000 entries with corresponding titles, abstracts, and\nTNCSI_SP. The quantitative results, with an NDCG@20 of 0.901, demonstrate that\nthe proposed approach achieves state-of-the-art performance in predicting the\nimpact of newborn articles when compared to competitive counterparts. Finally,\nwe demonstrate a real-world application for predicting the impact of newborn\njournal articles to demonstrate its noteworthy practical value. Overall, our\nfindings challenge existing paradigms and propose a shift towards a more\ncontent-focused prediction of academic impact, offering new insights for\nassessing newborn article impact.",
            "GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity.",
            "We measure the performance of in-context learning as a function of task\nnovelty and difficulty for open and closed questions. For that purpose, we\ncreated a novel benchmark consisting of hard scientific questions, each paired\nwith a context of various relevancy. We show that counter-intuitively, a\ncontext that is more aligned with the topic does not always help more than a\nless relevant context. This effect is especially visible for open questions and\nquestions of high difficulty or novelty. This result reveals a fundamental\ndifference between the treatment of close-form and open-form questions by\nlarge-language models and shows a need for a more robust evaluation of\nin-context learning on the variety of different types of questions. It also\nposes a new question of how to optimally select a context for large language\nmodels, especially in the context of Retrieval Augmented Generation (RAG)\nsystems. Our results suggest that the answer to this question can be highly\napplication-dependent and might be contingent on factors including the format\nof the question, the perceived difficulty level of the questions, and the\nnovelty or popularity of the information we seek."
        ],
        "neg": []
    },
    {
        "query": "Controlling the format of outputs generated by large language models (LLMs)\nis a critical functionality in various applications. Current methods typically\nemploy constrained decoding with rule-based automata or fine-tuning with\nmanually crafted format instructions, both of which struggle with open-domain\nformat requirements. To address this limitation, we introduce a novel framework\nfor controlled generation in LLMs, leveraging user-provided, one-shot QA pairs.\nThis study investigates LLMs' capabilities to follow open-domain, one-shot\nconstraints and replicate the format of the example answers. We observe that\nthis is a non-trivial problem for current LLMs. We also develop a dataset\ncollection methodology for supervised fine-tuning that enhances the open-domain\nformat control of LLMs without degrading output quality, as well as a benchmark\non which we evaluate both the helpfulness and format correctness of LLM\noutputs. The resulting datasets, named OIFC-SFT, along with the related code,\nwill be made publicly available at https://github.com/cofe-ai/OIFC.",
        "pos": [
            "Hallucination is often regarded as a major impediment for using large\nlanguage models (LLMs), especially for knowledge-intensive tasks. Even when the\ntraining corpus consists solely of true statements, language models still\ngenerate hallucinations in the form of amalgamations of multiple facts. We coin\nthis phenomenon as ``knowledge overshadowing'': when we query knowledge from a\nlanguage model with multiple conditions, some conditions overshadow others,\nleading to hallucinated outputs. This phenomenon partially stems from training\ndata imbalance, which we verify on both pretrained models and fine-tuned\nmodels, over a wide range of LM model families and sizes.From a theoretical\npoint of view, knowledge overshadowing can be interpreted as\nover-generalization of the dominant conditions (patterns). We show that the\nhallucination rate grows with both the imbalance ratio (between the popular and\nunpopular condition) and the length of dominant condition description,\nconsistent with our derived generalization bound. Finally, we propose to\nutilize overshadowing conditions as a signal to catch hallucination before it\nis produced, along with a training-free self-contrastive decoding method to\nalleviate hallucination during inference. Our proposed approach showcases up to\n82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,\nwith different models and datasets."
        ],
        "neg": []
    },
    {
        "query": "This paper presents the results of the shared task on Chinese metaphor\ngeneration, hosted at the 13th CCF Conference on Natural Language Processing\nand Chinese Computing (NLPCC 2024). The goal of this shared task is to generate\nChinese metaphors using machine learning techniques and effectively identifying\nbasic components of metaphorical sentences. It is divided into two subtasks: 1)\nMetaphor Generation, which involves creating a metaphor from a provided tuple\nconsisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a\nmetaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),\nguided by the concept of the GROUND. 2) Metaphor Components Identification,\nwhich extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a\nmetaphorical sentence. This component requires the identification of the most\nfitting metaphor elements that correspond to the specified grounds. In addition\nto overall results, we report on the setup and insights from the metaphor\ngeneration shared task, which attracted a total of 4 participating teams across\nboth subtasks.",
        "pos": [
            "Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process."
        ],
        "neg": []
    },
    {
        "query": "This paper presents the results of the shared task on Chinese metaphor\ngeneration, hosted at the 13th CCF Conference on Natural Language Processing\nand Chinese Computing (NLPCC 2024). The goal of this shared task is to generate\nChinese metaphors using machine learning techniques and effectively identifying\nbasic components of metaphorical sentences. It is divided into two subtasks: 1)\nMetaphor Generation, which involves creating a metaphor from a provided tuple\nconsisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a\nmetaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),\nguided by the concept of the GROUND. 2) Metaphor Components Identification,\nwhich extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a\nmetaphorical sentence. This component requires the identification of the most\nfitting metaphor elements that correspond to the specified grounds. In addition\nto overall results, we report on the setup and insights from the metaphor\ngeneration shared task, which attracted a total of 4 participating teams across\nboth subtasks.",
        "pos": [
            "Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process.",
            "Despite the advanced intelligence abilities of large language models (LLMs)\nin various applications, they still face significant computational and storage\ndemands. Knowledge Distillation (KD) has emerged as an effective strategy to\nimprove the performance of a smaller LLM (i.e., the student model) by\ntransferring knowledge from a high-performing LLM (i.e., the teacher model).\nPrevailing techniques in LLM distillation typically use a black-box model API\nto generate high-quality pretrained and aligned datasets, or utilize white-box\ndistillation by altering the loss function to better transfer knowledge from\nthe teacher LLM. However, these methods ignore the knowledge differences\nbetween the student and teacher LLMs across domains. This results in excessive\nfocus on domains with minimal performance gaps and insufficient attention to\ndomains with large gaps, reducing overall performance. In this paper, we\nintroduce a new LLM distillation framework called DDK, which dynamically\nadjusts the composition of the distillation dataset in a smooth manner\naccording to the domain performance differences between the teacher and student\nmodels, making the distillation process more stable and effective. Extensive\nevaluations show that DDK significantly improves the performance of student\nmodels, outperforming both continuously pretrained baselines and existing\nknowledge distillation methods by a large margin."
        ],
        "neg": []
    },
    {
        "query": "This paper presents the results of the shared task on Chinese metaphor\ngeneration, hosted at the 13th CCF Conference on Natural Language Processing\nand Chinese Computing (NLPCC 2024). The goal of this shared task is to generate\nChinese metaphors using machine learning techniques and effectively identifying\nbasic components of metaphorical sentences. It is divided into two subtasks: 1)\nMetaphor Generation, which involves creating a metaphor from a provided tuple\nconsisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a\nmetaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),\nguided by the concept of the GROUND. 2) Metaphor Components Identification,\nwhich extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a\nmetaphorical sentence. This component requires the identification of the most\nfitting metaphor elements that correspond to the specified grounds. In addition\nto overall results, we report on the setup and insights from the metaphor\ngeneration shared task, which attracted a total of 4 participating teams across\nboth subtasks.",
        "pos": [
            "Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process."
        ],
        "neg": []
    },
    {
        "query": "This paper presents the results of the shared task on Chinese metaphor\ngeneration, hosted at the 13th CCF Conference on Natural Language Processing\nand Chinese Computing (NLPCC 2024). The goal of this shared task is to generate\nChinese metaphors using machine learning techniques and effectively identifying\nbasic components of metaphorical sentences. It is divided into two subtasks: 1)\nMetaphor Generation, which involves creating a metaphor from a provided tuple\nconsisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a\nmetaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),\nguided by the concept of the GROUND. 2) Metaphor Components Identification,\nwhich extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a\nmetaphorical sentence. This component requires the identification of the most\nfitting metaphor elements that correspond to the specified grounds. In addition\nto overall results, we report on the setup and insights from the metaphor\ngeneration shared task, which attracted a total of 4 participating teams across\nboth subtasks.",
        "pos": [
            "Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process.",
            "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research."
        ],
        "neg": []
    },
    {
        "query": "This paper presents the results of the shared task on Chinese metaphor\ngeneration, hosted at the 13th CCF Conference on Natural Language Processing\nand Chinese Computing (NLPCC 2024). The goal of this shared task is to generate\nChinese metaphors using machine learning techniques and effectively identifying\nbasic components of metaphorical sentences. It is divided into two subtasks: 1)\nMetaphor Generation, which involves creating a metaphor from a provided tuple\nconsisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a\nmetaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),\nguided by the concept of the GROUND. 2) Metaphor Components Identification,\nwhich extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a\nmetaphorical sentence. This component requires the identification of the most\nfitting metaphor elements that correspond to the specified grounds. In addition\nto overall results, we report on the setup and insights from the metaphor\ngeneration shared task, which attracted a total of 4 participating teams across\nboth subtasks.",
        "pos": [
            "Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process."
        ],
        "neg": []
    },
    {
        "query": "In the internet era, almost every business entity is trying to have its\ndigital footprint in digital media and other social media platforms. For these\nentities, word of mouse is also very important. Particularly, this is quite\ncrucial for the hospitality sector dealing with hotels, restaurants etc.\nConsumers do read other consumers reviews before making final decisions. This\nis where it becomes very important to understand which aspects are affecting\nmost in the minds of the consumers while giving their ratings. The current\nstudy focuses on the consumer reviews of Indian hotels to extract aspects\nimportant for final ratings. The study involves gathering data using web\nscraping methods, analyzing the texts using Latent Dirichlet Allocation for\ntopic extraction and sentiment analysis for aspect-specific sentiment mapping.\nFinally, it incorporates Random Forest to understand the importance of the\naspects in predicting the final rating of a user.",
        "pos": [
            "Adversarial attacks against deep learning models represent a major threat to\nthe security and reliability of natural language processing (NLP) systems. In\nthis paper, we propose a modification to the BERT-Attack framework, integrating\nProjected Gradient Descent (PGD) to enhance its effectiveness and robustness.\nThe original BERT-Attack, designed for generating adversarial examples against\nBERT-based models, suffers from limitations such as a fixed perturbation budget\nand a lack of consideration for semantic similarity. The proposed approach in\nthis work, PGD-BERT-Attack, addresses these limitations by leveraging PGD to\niteratively generate adversarial examples while ensuring both imperceptibility\nand semantic similarity to the original input. Extensive experiments are\nconducted to evaluate the performance of PGD-BERT-Attack compared to the\noriginal BERT-Attack and other baseline methods. The results demonstrate that\nPGD-BERT-Attack achieves higher success rates in causing misclassification\nwhile maintaining low perceptual changes. Furthermore, PGD-BERT-Attack produces\nadversarial instances that exhibit greater semantic resemblance to the initial\ninput, enhancing their applicability in real-world scenarios. Overall, the\nproposed modification offers a more effective and robust approach to\nadversarial attacks on BERT-based models, thus contributing to the advancement\nof defense against attacks on NLP systems."
        ],
        "neg": []
    },
    {
        "query": "In automatic speech recognition, subsampling is essential for tackling\ndiverse scenarios. However, the inadequacy of a single subsampling rate to\naddress various real-world situations often necessitates training and deploying\nmultiple models, consequently increasing associated costs. To address this\nissue, we propose HydraFormer, comprising HydraSub, a Conformer-based encoder,\nand a BiTransformer-based decoder. HydraSub encompasses multiple branches, each\nrepresenting a distinct subsampling rate, allowing for the flexible selection\nof any branch during inference based on the specific use case. HydraFormer can\nefficiently manage different subsampling rates, significantly reducing training\nand deployment expenses. Experiments on AISHELL-1 and LibriSpeech datasets\nreveal that HydraFormer effectively adapts to various subsampling rates and\nlanguages while maintaining high recognition performance. Additionally,\nHydraFormer showcases exceptional stability, sustaining consistent performance\nunder various initialization conditions, and exhibits robust transferability by\nlearning from pretrained single subsampling rate automatic speech recognition\nmodels\\footnote{Model code and scripts:\nhttps://github.com/HydraFormer/hydraformer}.",
        "pos": [
            "Spontaneous style speech synthesis, which aims to generate human-like speech,\noften encounters challenges due to the scarcity of high-quality data and\nlimitations in model capabilities. Recent language model-based TTS systems can\nbe trained on large, diverse, and low-quality speech datasets, resulting in\nhighly natural synthesized speech. However, they are limited by the difficulty\nof simulating various spontaneous behaviors and capturing prosody variations in\nspontaneous speech. In this paper, we propose a novel spontaneous speech\nsynthesis system based on language models. We systematically categorize and\nuniformly model diverse spontaneous behaviors. Moreover, fine-grained prosody\nmodeling is introduced to enhance the model's ability to capture subtle prosody\nvariations in spontaneous speech.Experimental results show that our proposed\nmethod significantly outperforms the baseline methods in terms of prosody\nnaturalness and spontaneous behavior naturalness."
        ],
        "neg": []
    },
    {
        "query": "The development of monolingual language models for low and mid-resource\nlanguages continues to be hindered by the difficulty in sourcing high-quality\ntraining data. In this study, we present a novel cross-lingual vocabulary\ntransfer strategy, trans-tokenization, designed to tackle this challenge and\nenable more efficient language adaptation. Our approach focuses on adapting a\nhigh-resource monolingual LLM to an unseen target language by initializing the\ntoken embeddings of the target language using a weighted average of\nsemantically similar token embeddings from the source language. For this, we\nleverage a translation resource covering both the source and target languages.\nWe validate our method with the Tweeties, a series of trans-tokenized LLMs, and\ndemonstrate their competitive performance on various downstream tasks across a\nsmall but diverse set of languages. Additionally, we introduce Hydra LLMs,\nmodels with multiple swappable language modeling heads and embedding tables,\nwhich further extend the capabilities of our trans-tokenization strategy. By\ndesigning a Hydra LLM based on the multilingual model TowerInstruct, we\ndeveloped a state-of-the-art machine translation model for Tatar, in a\nzero-shot manner, completely bypassing the need for high-quality parallel data.\nThis breakthrough is particularly significant for low-resource languages like\nTatar, where high-quality parallel data is hard to come by. By lowering the\ndata and time requirements for training high-quality models, our\ntrans-tokenization strategy allows for the development of LLMs for a wider\nrange of languages, especially those with limited resources. We hope that our\nwork will inspire further research and collaboration in the field of\ncross-lingual vocabulary transfer and contribute to the empowerment of\nlanguages on a global scale.",
        "pos": [
            "The FIFA World Cup in Qatar was discussed extensively in the news and on\nsocial media. Due to news reports with allegations of human rights violations,\nthere were calls to boycott it. Wearing a OneLove armband was part of a planned\nprotest activity. Controversy around the armband arose when FIFA threatened to\nsanction captains who wear it. To understand what topics Twitter users Tweeted\nabout and what the opinion of German Twitter users was towards the OneLove\narmband, we performed an analysis of German Tweets published during the World\nCup using in-context learning with LLMs. We validated the labels on human\nannotations. We found that Twitter users initially discussed the armband's\nimpact, LGBT rights, and politics; after the ban, the conversation shifted\ntowards politics in sports in general, accompanied by a subtle shift in\nsentiment towards neutrality. Our evaluation serves as a framework for future\nresearch to explore the impact of sports activism and evolving public\nsentiment. This is especially useful in settings where labeling datasets for\nspecific opinions is unfeasible, such as when events are unfolding.",
            "An important issue with Large Language Models (LLMs) is their undesired\nability to generate toxic language. In this work, we show that the neurons\nresponsible for toxicity can be determined by their power to discriminate toxic\nsentences, and that toxic language can be mitigated by reducing their\nactivation levels proportionally to this power. We propose AUROC adaptation\n(AurA), an intervention that can be applied to any pre-trained LLM to mitigate\ntoxicity. As the intervention is proportional to the ability of each neuron to\ndiscriminate toxic content, it is free of any model-dependent hyperparameters.\nWe show that AurA can achieve up to $2.2 \\times$ reduction in toxicity with\nonly a $0.72$ perplexity increase. We also show that AurA is effective with\nmodels of different scale (from 1.5B to 40B parameters), and its effectiveness\nin mitigating toxic language, while preserving common-sense zero-shot\nabilities, holds across all scales. AurA can be combined with pre-prompting\nstrategies, boosting its average mitigation potential from $1.28\\times$ to\n$2.35\\times$. Moreover, AurA can counteract adversarial pre-prompts that\nmaliciously elicit toxic content, making it an effective method for deploying\nsafer and less toxic models."
        ],
        "neg": []
    },
    {
        "query": "The development of monolingual language models for low and mid-resource\nlanguages continues to be hindered by the difficulty in sourcing high-quality\ntraining data. In this study, we present a novel cross-lingual vocabulary\ntransfer strategy, trans-tokenization, designed to tackle this challenge and\nenable more efficient language adaptation. Our approach focuses on adapting a\nhigh-resource monolingual LLM to an unseen target language by initializing the\ntoken embeddings of the target language using a weighted average of\nsemantically similar token embeddings from the source language. For this, we\nleverage a translation resource covering both the source and target languages.\nWe validate our method with the Tweeties, a series of trans-tokenized LLMs, and\ndemonstrate their competitive performance on various downstream tasks across a\nsmall but diverse set of languages. Additionally, we introduce Hydra LLMs,\nmodels with multiple swappable language modeling heads and embedding tables,\nwhich further extend the capabilities of our trans-tokenization strategy. By\ndesigning a Hydra LLM based on the multilingual model TowerInstruct, we\ndeveloped a state-of-the-art machine translation model for Tatar, in a\nzero-shot manner, completely bypassing the need for high-quality parallel data.\nThis breakthrough is particularly significant for low-resource languages like\nTatar, where high-quality parallel data is hard to come by. By lowering the\ndata and time requirements for training high-quality models, our\ntrans-tokenization strategy allows for the development of LLMs for a wider\nrange of languages, especially those with limited resources. We hope that our\nwork will inspire further research and collaboration in the field of\ncross-lingual vocabulary transfer and contribute to the empowerment of\nlanguages on a global scale.",
        "pos": [
            "Beyond individual languages, multilingual natural language processing (NLP)\nresearch increasingly aims to develop models that perform well across languages\ngenerally. However, evaluating these systems on all the world's languages is\npractically infeasible. To attain generalizability, representative language\nsampling is essential. Previous work argues that generalizable multilingual\nevaluation sets should contain languages with diverse typological properties.\nHowever, 'typologically diverse' language samples have been found to vary\nconsiderably in this regard, and popular sampling methods are flawed and\ninconsistent. We present a language sampling framework for selecting highly\ntypologically diverse languages given a sampling frame, informed by language\ntypology. We compare sampling methods with a range of metrics and find that our\nsystematic methods consistently retrieve more typologically diverse language\nselections than previous methods in NLP. Moreover, we provide evidence that\nthis affects generalizability in multilingual model evaluation, emphasizing the\nimportance of diverse language sampling in NLP evaluation."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) are supposed to acquire unconscious human\nknowledge and feelings, such as social common sense and biases, by training\nmodels from large amounts of text. However, it is not clear how much the\nsentiments of specific social groups can be captured in various LLMs. In this\nstudy, we focus on social groups defined in terms of nationality, religion, and\nrace/ethnicity, and validate the extent to which sentiments between social\ngroups can be captured in and extracted from LLMs. Specifically, we input\nquestions regarding sentiments from one group to another into LLMs, apply\nsentiment analysis to the responses, and compare the results with social\nsurveys. The validation results using five representative LLMs showed higher\ncorrelations with relatively small p-values for nationalities and religions,\nwhose number of data points were relatively large. This result indicates that\nthe LLM responses including the inter-group sentiments align well with actual\nsocial survey results.",
        "pos": [
            "In recent years, neural machine translation (NMT) has been widely used in\neveryday life. However, the current NMT lacks a mechanism to adjust the\ndifficulty level of translations to match the user's language level.\nAdditionally, due to the bias in the training data for NMT, translations of\nsimple source sentences are often produced with complex words. In particular,\nthis could pose a problem for children, who may not be able to understand the\nmeaning of the translations correctly. In this study, we propose a method that\nreplaces words with high Age of Acquisitions (AoA) in translations with simpler\nwords to match the translations to the user's level. We achieve this by using\nlarge language models (LLMs), providing a triple of a source sentence, a\ntranslation, and a target word to be replaced. We create a benchmark dataset\nusing back-translation on Simple English Wikipedia. The experimental results\nobtained from the dataset show that our method effectively replaces high-AoA\nwords with lower-AoA words and, moreover, can iteratively replace most of the\nhigh-AoA words while still maintaining high BLEU and COMET scores."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) are supposed to acquire unconscious human\nknowledge and feelings, such as social common sense and biases, by training\nmodels from large amounts of text. However, it is not clear how much the\nsentiments of specific social groups can be captured in various LLMs. In this\nstudy, we focus on social groups defined in terms of nationality, religion, and\nrace/ethnicity, and validate the extent to which sentiments between social\ngroups can be captured in and extracted from LLMs. Specifically, we input\nquestions regarding sentiments from one group to another into LLMs, apply\nsentiment analysis to the responses, and compare the results with social\nsurveys. The validation results using five representative LLMs showed higher\ncorrelations with relatively small p-values for nationalities and religions,\nwhose number of data points were relatively large. This result indicates that\nthe LLM responses including the inter-group sentiments align well with actual\nsocial survey results.",
        "pos": [
            "In recent years, neural machine translation (NMT) has been widely used in\neveryday life. However, the current NMT lacks a mechanism to adjust the\ndifficulty level of translations to match the user's language level.\nAdditionally, due to the bias in the training data for NMT, translations of\nsimple source sentences are often produced with complex words. In particular,\nthis could pose a problem for children, who may not be able to understand the\nmeaning of the translations correctly. In this study, we propose a method that\nreplaces words with high Age of Acquisitions (AoA) in translations with simpler\nwords to match the translations to the user's level. We achieve this by using\nlarge language models (LLMs), providing a triple of a source sentence, a\ntranslation, and a target word to be replaced. We create a benchmark dataset\nusing back-translation on Simple English Wikipedia. The experimental results\nobtained from the dataset show that our method effectively replaces high-AoA\nwords with lower-AoA words and, moreover, can iteratively replace most of the\nhigh-AoA words while still maintaining high BLEU and COMET scores.",
            "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
        ],
        "neg": []
    },
    {
        "query": "The widespread accessibility of large language models (LLMs) to the general\npublic has significantly amplified the dissemination of machine-generated texts\n(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in\ndiscerning the origin of a text (human-authored vs machinegenerated). This\nraises concerns regarding the potential misuse of MGTs, particularly within\neducational and academic domains. In this paper, we present\n$\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.\nIt is able to classify texts into four categories: human-written,\nmachine-generated, machine-written machine-humanized, and human-written\nmachine-polished. Contrary to previous MGT detectors that perform binary\nclassification, introducing two additional categories in LLM-DetectiAIve offers\ninsights into the varying degrees of LLM intervention during the text creation.\nThis might be useful in some domains like education, where any LLM intervention\nis usually prohibited. Experiments show that LLM-DetectAIve can effectively\nidentify the authorship of textual content, proving its usefulness in enhancing\nintegrity in education, academia, and other domains. LLM-DetectAIve is publicly\naccessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video\ndescribing our system is available at https://youtu.be/E8eT_bE7k8c.",
        "pos": [
            "This paper presents Papilusion, an AI-generated scientific text detector\ndeveloped within the DAGPap24 shared task on detecting automatically generated\nscientific papers. We propose an ensemble-based approach and conduct ablation\nstudies to analyze the effect of the detector configurations on the\nperformance. Papilusion is ranked 6th on the leaderboard, and we improve our\nperformance after the competition ended, achieving 99.46 (+9.63) of the\nF1-score on the official test set."
        ],
        "neg": []
    },
    {
        "query": "The widespread accessibility of large language models (LLMs) to the general\npublic has significantly amplified the dissemination of machine-generated texts\n(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in\ndiscerning the origin of a text (human-authored vs machinegenerated). This\nraises concerns regarding the potential misuse of MGTs, particularly within\neducational and academic domains. In this paper, we present\n$\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.\nIt is able to classify texts into four categories: human-written,\nmachine-generated, machine-written machine-humanized, and human-written\nmachine-polished. Contrary to previous MGT detectors that perform binary\nclassification, introducing two additional categories in LLM-DetectiAIve offers\ninsights into the varying degrees of LLM intervention during the text creation.\nThis might be useful in some domains like education, where any LLM intervention\nis usually prohibited. Experiments show that LLM-DetectAIve can effectively\nidentify the authorship of textual content, proving its usefulness in enhancing\nintegrity in education, academia, and other domains. LLM-DetectAIve is publicly\naccessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video\ndescribing our system is available at https://youtu.be/E8eT_bE7k8c.",
        "pos": [
            "This paper presents Papilusion, an AI-generated scientific text detector\ndeveloped within the DAGPap24 shared task on detecting automatically generated\nscientific papers. We propose an ensemble-based approach and conduct ablation\nstudies to analyze the effect of the detector configurations on the\nperformance. Papilusion is ranked 6th on the leaderboard, and we improve our\nperformance after the competition ended, achieving 99.46 (+9.63) of the\nF1-score on the official test set."
        ],
        "neg": []
    },
    {
        "query": "The widespread accessibility of large language models (LLMs) to the general\npublic has significantly amplified the dissemination of machine-generated texts\n(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in\ndiscerning the origin of a text (human-authored vs machinegenerated). This\nraises concerns regarding the potential misuse of MGTs, particularly within\neducational and academic domains. In this paper, we present\n$\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.\nIt is able to classify texts into four categories: human-written,\nmachine-generated, machine-written machine-humanized, and human-written\nmachine-polished. Contrary to previous MGT detectors that perform binary\nclassification, introducing two additional categories in LLM-DetectiAIve offers\ninsights into the varying degrees of LLM intervention during the text creation.\nThis might be useful in some domains like education, where any LLM intervention\nis usually prohibited. Experiments show that LLM-DetectAIve can effectively\nidentify the authorship of textual content, proving its usefulness in enhancing\nintegrity in education, academia, and other domains. LLM-DetectAIve is publicly\naccessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video\ndescribing our system is available at https://youtu.be/E8eT_bE7k8c.",
        "pos": [
            "We propose selective debiasing -- an inference-time safety mechanism that\naims to increase the overall quality of models in terms of prediction\nperformance and fairness in the situation when re-training a model is\nprohibitive. The method is inspired by selective prediction, where some\npredictions that are considered low quality are discarded at inference time. In\nour approach, we identify the potentially biased model predictions and, instead\nof discarding them, we debias them using LEACE -- a post-processing debiasing\nmethod. To select problematic predictions, we propose a bias quantification\napproach based on KL divergence, which achieves better results than standard UQ\nmethods. Experiments with text classification datasets demonstrate that\nselective debiasing helps to close the performance gap between post-processing\nmethods and at-training and pre-processing debiasing techniques."
        ],
        "neg": []
    },
    {
        "query": "The widespread accessibility of large language models (LLMs) to the general\npublic has significantly amplified the dissemination of machine-generated texts\n(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in\ndiscerning the origin of a text (human-authored vs machinegenerated). This\nraises concerns regarding the potential misuse of MGTs, particularly within\neducational and academic domains. In this paper, we present\n$\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.\nIt is able to classify texts into four categories: human-written,\nmachine-generated, machine-written machine-humanized, and human-written\nmachine-polished. Contrary to previous MGT detectors that perform binary\nclassification, introducing two additional categories in LLM-DetectiAIve offers\ninsights into the varying degrees of LLM intervention during the text creation.\nThis might be useful in some domains like education, where any LLM intervention\nis usually prohibited. Experiments show that LLM-DetectAIve can effectively\nidentify the authorship of textual content, proving its usefulness in enhancing\nintegrity in education, academia, and other domains. LLM-DetectAIve is publicly\naccessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video\ndescribing our system is available at https://youtu.be/E8eT_bE7k8c.",
        "pos": [
            "We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed.",
            "Dialectal Arabic is the primary spoken language used by native Arabic\nspeakers in daily communication. The rise of social media platforms has notably\nexpanded its use as a written language. However, Arabic dialects do not have\nstandard orthographies. This, combined with the inherent noise in\nuser-generated content on social media, presents a major challenge to NLP\napplications dealing with Dialectal Arabic. In this paper, we explore and\nreport on the task of CODAfication, which aims to normalize Dialectal Arabic\ninto the Conventional Orthography for Dialectal Arabic (CODA). We work with a\nunique parallel corpus of multiple Arabic dialects focusing on five major city\ndialects. We benchmark newly developed pretrained sequence-to-sequence models\non the task of CODAfication. We further show that using dialect identification\ninformation improves the performance across all dialects. We make our code,\ndata, and pretrained models publicly available."
        ],
        "neg": []
    },
    {
        "query": "The widespread accessibility of large language models (LLMs) to the general\npublic has significantly amplified the dissemination of machine-generated texts\n(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in\ndiscerning the origin of a text (human-authored vs machinegenerated). This\nraises concerns regarding the potential misuse of MGTs, particularly within\neducational and academic domains. In this paper, we present\n$\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.\nIt is able to classify texts into four categories: human-written,\nmachine-generated, machine-written machine-humanized, and human-written\nmachine-polished. Contrary to previous MGT detectors that perform binary\nclassification, introducing two additional categories in LLM-DetectiAIve offers\ninsights into the varying degrees of LLM intervention during the text creation.\nThis might be useful in some domains like education, where any LLM intervention\nis usually prohibited. Experiments show that LLM-DetectAIve can effectively\nidentify the authorship of textual content, proving its usefulness in enhancing\nintegrity in education, academia, and other domains. LLM-DetectAIve is publicly\naccessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video\ndescribing our system is available at https://youtu.be/E8eT_bE7k8c.",
        "pos": [
            "This paper presents a shared task that we organized at the Foundations of\nLanguage Technology (FoLT) course in 2023/2024 at the Technical University of\nDarmstadt, which focuses on evaluating the output of Large Language Models\n(LLMs) in generating harmful answers to health-related clinical questions. We\ndescribe the task design considerations and report the feedback we received\nfrom the students. We expect the task and the findings reported in this paper\nto be relevant for instructors teaching natural language processing (NLP) and\ndesigning course assignments.",
            "Argument retrieval is the task of finding relevant arguments for a given\nquery. While existing approaches rely solely on the semantic alignment of\nqueries and arguments, this first shared task on perspective argument retrieval\nincorporates perspectives during retrieval, accounting for latent influences in\nargumentation. We present a novel multilingual dataset covering demographic and\nsocio-cultural (socio) variables, such as age, gender, and political attitude,\nrepresenting minority and majority groups in society. We distinguish between\nthree scenarios to explore how retrieval systems consider explicitly (in both\nquery and corpus) and implicitly (only in query) formulated perspectives. This\npaper provides an overview of this shared task and summarizes the results of\nthe six submitted systems. We find substantial challenges in incorporating\nperspectivism, especially when aiming for personalization based solely on the\ntext of arguments without explicitly providing socio profiles. Moreover,\nretrieval systems tend to be biased towards the majority group but partially\nmitigate bias for the female gender. While we bootstrap perspective argument\nretrieval, further research is essential to optimize retrieval systems to\nfacilitate personalization and reduce polarization.",
            "Multilingual sentence encoders are commonly obtained by training multilingual\nlanguage models to map sentences from different languages into a shared\nsemantic space. As such, they are subject to curse of multilinguality, a loss\nof monolingual representational accuracy due to parameter sharing. Another\nlimitation of multilingual sentence encoders is the trade-off between\nmonolingual and cross-lingual performance. Training for cross-lingual alignment\nof sentence embeddings distorts the optimal monolingual structure of semantic\nspaces of individual languages, harming the utility of sentence embeddings in\nmonolingual tasks. In this work, we address both issues by modular training of\nsentence encoders, i.e., by separating monolingual specialization from\ncross-lingual alignment. We first efficiently train language-specific sentence\nencoders to avoid negative interference between languages (i.e., the curse). We\nthen align all non-English monolingual encoders to the English encoder by\ntraining a cross-lingual alignment adapter on top of each, preventing\ninterference with monolingual specialization from the first step. In both\nsteps, we resort to contrastive learning on machine-translated paraphrase data.\nMonolingual and cross-lingual evaluations on semantic text\nsimilarity/relatedness and multiple-choice QA render our modular solution more\neffective than multilingual sentence encoders, especially benefiting\nlow-resource languages.",
            "Recent advances in measuring hardness-wise properties of data guide language\nmodels in sample selection within low-resource scenarios. However,\nclass-specific properties are overlooked for task setup and learning. How will\nthese properties influence model learning and is it generalizable across\ndatasets? To answer this question, this work formally initiates the concept of\n$\\textit{class-wise hardness}$. Experiments across eight natural language\nunderstanding (NLU) datasets demonstrate a consistent hardness distribution\nacross learning paradigms, models, and human judgment. Subsequent experiments\nunveil a notable challenge in measuring such class-wise hardness with\ninstance-level metrics in previous works. To address this, we propose\n$\\textit{GeoHard}$ for class-wise hardness measurement by modeling class\ngeometry in the semantic embedding space. $\\textit{GeoHard}$ surpasses\ninstance-level metrics by over 59 percent on $\\textit{Pearson}$'s correlation\non measuring class-wise hardness. Our analysis theoretically and empirically\nunderscores the generality of $\\textit{GeoHard}$ as a fresh perspective on data\ndiagnosis. Additionally, we showcase how understanding class-wise hardness can\npractically aid in improving task learning.",
            "Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nhallucination and improves answer quality. Furthermore, humans find answers\ngenerated by our approach comprehensive and highly prefer them (84%) over the\nbaseline answers.",
            "Text anonymization is crucial for sharing sensitive data while maintaining\nprivacy. Existing techniques face the emerging challenges of re-identification\nattack ability of Large Language Models (LLMs), which have shown advanced\ncapability in memorizing detailed information and patterns as well as\nconnecting disparate pieces of information. In defending against LLM-based\nre-identification attacks, anonymization could jeopardize the utility of the\nresulting anonymized data in downstream tasks -- the trade-off between privacy\nand data utility requires deeper understanding within the context of LLMs. This\npaper proposes a framework composed of three LLM-based components -- a privacy\nevaluator, a utility evaluator, and an optimization component, which work\ncollaboratively to perform anonymization. To provide a practical model for\nlarge-scale and real-time environments, we distill the anonymization\ncapabilities into a lightweight model using Direct Preference Optimization\n(DPO). Extensive experiments demonstrate that the proposed models outperform\nbaseline models, showing robustness in reducing the risk of re-identification\nwhile preserving greater data utility in downstream tasks. Our code and dataset\nare available at https://github.com/UKPLab/arxiv2024-rupta.",
            "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain.",
            "Large language models (LLMs) present an opportunity to scale high-quality\npersonalized education to all. A promising approach towards this means is to\nbuild dialog tutoring models that scaffold students' problem-solving. However,\neven though existing LLMs perform well in solving reasoning questions, they\nstruggle to precisely detect student's errors and tailor their feedback to\nthese errors. Inspired by real-world teaching practice where teachers identify\nstudent errors and customize their response based on them, we focus on\nverifying student solutions and show how grounding to such verification\nimproves the overall quality of tutor response generation. We collect a dataset\nof 1K stepwise math reasoning chains with the first error step annotated by\nteachers. We show empirically that finding the mistake in a student solution is\nchallenging for current models. We propose and evaluate several verifiers for\ndetecting these errors. Using both automatic and human evaluation we show that\nthe student solution verifiers steer the generation model towards highly\ntargeted responses to student errors which are more often correct with less\nhallucinations compared to existing baselines.",
            "LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiment with different approaches to attribution on 4 LLMs\nof different sizes, both prompted and fine-tuned. We find that citation, i.e.\nresponse generation and evidence extraction in one step, mostly performs best.\nWe investigate whether the ``Lost in the Middle'' phenomenon exists for\nattribution, but do not find this. We also find that evidence quality can\npredict response quality on datasets with simple responses, but not so for\ncomplex responses, as models struggle with providing evidence for complex\nclaims. We release code and data for further investigation.",
            "Large language models (LLMs) bring unprecedented flexibility in defining and\nexecuting complex, creative natural language generation (NLG) tasks. Yet, this\nflexibility brings new challenges, as it introduces new degrees of freedom in\nformulating the task inputs and instructions and in evaluating model\nperformance. To facilitate the exploration of creative NLG tasks, we propose a\nthree-component research framework that consists of systematic input\nmanipulation, reference data, and output measurement. We use this framework to\nexplore citation text generation -- a popular scholarly NLP task that lacks\nconsensus on the task definition and evaluation metric and has not yet been\ntackled within the LLM paradigm. Our results highlight the importance of\nsystematically investigating both task instruction and input configuration when\nprompting LLMs, and reveal non-trivial relationships between different\nevaluation metrics used for citation text generation. Additional human\ngeneration and human evaluation experiments provide new qualitative insights\ninto the task to guide future research in citation text generation. We make our\ncode and data publicly available."
        ],
        "neg": []
    },
    {
        "query": "The widespread accessibility of large language models (LLMs) to the general\npublic has significantly amplified the dissemination of machine-generated texts\n(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in\ndiscerning the origin of a text (human-authored vs machinegenerated). This\nraises concerns regarding the potential misuse of MGTs, particularly within\neducational and academic domains. In this paper, we present\n$\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.\nIt is able to classify texts into four categories: human-written,\nmachine-generated, machine-written machine-humanized, and human-written\nmachine-polished. Contrary to previous MGT detectors that perform binary\nclassification, introducing two additional categories in LLM-DetectiAIve offers\ninsights into the varying degrees of LLM intervention during the text creation.\nThis might be useful in some domains like education, where any LLM intervention\nis usually prohibited. Experiments show that LLM-DetectAIve can effectively\nidentify the authorship of textual content, proving its usefulness in enhancing\nintegrity in education, academia, and other domains. LLM-DetectAIve is publicly\naccessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video\ndescribing our system is available at https://youtu.be/E8eT_bE7k8c.",
        "pos": [
            "We present an efficient method for adapting a monolingual Large Language\nModel (LLM) to another language, addressing challenges of catastrophic\nforgetting and tokenizer limitations. We focus this study on adapting Llama 2\nto Arabic. Our two-stage approach begins with expanding the vocabulary and\ntraining only the embeddings matrix, followed by full model continual\npre-training on a bilingual corpus. By continually pre-training on a mix of\nArabic and English corpora, the model retains its proficiency in English while\nacquiring capabilities in Arabic. Our approach results in significant\nimprovements in Arabic and slight enhancements in English, demonstrating\ncost-effective cross-lingual transfer. We perform ablations on embedding\ninitialization techniques, data mix ratios, and learning rates and release a\ndetailed training recipe. To demonstrate generalizability of this approach we\nalso adapted Llama 3 8B to Arabic and Llama 2 13B to Hindi."
        ],
        "neg": []
    },
    {
        "query": "This study investigates how BERT processes and represents Argument Structure\nConstructions (ASCs), extending previous LSTM analyses. Using a dataset of 2000\nsentences across four ASC types (transitive, ditransitive, caused-motion,\nresultative), we analyzed BERT's token embeddings across 12 layers.\nVisualizations with MDS and t-SNE and clustering quantified by Generalized\nDiscrimination Value (GDV) were used. Feedforward classifiers (probes)\npredicted construction categories from embeddings. CLS token embeddings\nclustered best in layers 2-4, decreased in intermediate layers, and slightly\nincreased in final layers. DET and SUBJ embeddings showed consistent clustering\nin intermediate layers, VERB embeddings increased in clustering from layer 1 to\n12, and OBJ embeddings peaked in layer 10. Probe accuracies indicated low\nconstruction information in layer 1, with over 90 percent accuracy from layer 2\nonward, revealing latent construction information beyond GDV clustering. Fisher\nDiscriminant Ratio (FDR) analysis of attention weights showed OBJ tokens were\ncrucial for differentiating ASCs, followed by VERB and DET tokens. SUBJ, CLS,\nand SEP tokens had insignificant FDR scores. This study highlights BERT's\nlayered processing of linguistic constructions and its differences from LSTMs.\nFuture research will compare these findings with neuroimaging data to\nunderstand the neural correlates of ASC processing. This research underscores\nneural language models' potential to mirror linguistic processing in the human\nbrain, offering insights into the computational and neural mechanisms\nunderlying language understanding.",
        "pos": [
            "Understanding how language and linguistic constructions are processed in the\nbrain is a fundamental question in cognitive computational neuroscience. In\nthis study, we explore the representation and processing of Argument Structure\nConstructions (ASCs) in a recurrent neural language model. We trained a Long\nShort-Term Memory (LSTM) network on a custom-made dataset consisting of 2000\nsentences, generated using GPT-4, representing four distinct ASCs: transitive,\nditransitive, caused-motion, and resultative constructions.\n  We analyzed the internal activations of the LSTM model's hidden layers using\nMultidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding\n(t-SNE) to visualize the sentence representations. The Generalized\nDiscrimination Value (GDV) was calculated to quantify the degree of clustering\nwithin these representations. Our results show that sentence representations\nform distinct clusters corresponding to the four ASCs across all hidden layers,\nwith the most pronounced clustering observed in the last hidden layer before\nthe output layer. This indicates that even a relatively simple,\nbrain-constrained recurrent neural network can effectively differentiate\nbetween various construction types.\n  These findings are consistent with previous studies demonstrating the\nemergence of word class and syntax rule representations in recurrent language\nmodels trained on next word prediction tasks. In future work, we aim to\nvalidate these results using larger language models and compare them with\nneuroimaging data obtained during continuous speech perception. This study\nhighlights the potential of recurrent neural language models to mirror\nlinguistic processing in the human brain, providing valuable insights into the\ncomputational and neural mechanisms underlying language understanding."
        ],
        "neg": []
    },
    {
        "query": "This study investigates how BERT processes and represents Argument Structure\nConstructions (ASCs), extending previous LSTM analyses. Using a dataset of 2000\nsentences across four ASC types (transitive, ditransitive, caused-motion,\nresultative), we analyzed BERT's token embeddings across 12 layers.\nVisualizations with MDS and t-SNE and clustering quantified by Generalized\nDiscrimination Value (GDV) were used. Feedforward classifiers (probes)\npredicted construction categories from embeddings. CLS token embeddings\nclustered best in layers 2-4, decreased in intermediate layers, and slightly\nincreased in final layers. DET and SUBJ embeddings showed consistent clustering\nin intermediate layers, VERB embeddings increased in clustering from layer 1 to\n12, and OBJ embeddings peaked in layer 10. Probe accuracies indicated low\nconstruction information in layer 1, with over 90 percent accuracy from layer 2\nonward, revealing latent construction information beyond GDV clustering. Fisher\nDiscriminant Ratio (FDR) analysis of attention weights showed OBJ tokens were\ncrucial for differentiating ASCs, followed by VERB and DET tokens. SUBJ, CLS,\nand SEP tokens had insignificant FDR scores. This study highlights BERT's\nlayered processing of linguistic constructions and its differences from LSTMs.\nFuture research will compare these findings with neuroimaging data to\nunderstand the neural correlates of ASC processing. This research underscores\nneural language models' potential to mirror linguistic processing in the human\nbrain, offering insights into the computational and neural mechanisms\nunderlying language understanding.",
        "pos": [
            "Understanding how language and linguistic constructions are processed in the\nbrain is a fundamental question in cognitive computational neuroscience. In\nthis study, we explore the representation and processing of Argument Structure\nConstructions (ASCs) in a recurrent neural language model. We trained a Long\nShort-Term Memory (LSTM) network on a custom-made dataset consisting of 2000\nsentences, generated using GPT-4, representing four distinct ASCs: transitive,\nditransitive, caused-motion, and resultative constructions.\n  We analyzed the internal activations of the LSTM model's hidden layers using\nMultidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding\n(t-SNE) to visualize the sentence representations. The Generalized\nDiscrimination Value (GDV) was calculated to quantify the degree of clustering\nwithin these representations. Our results show that sentence representations\nform distinct clusters corresponding to the four ASCs across all hidden layers,\nwith the most pronounced clustering observed in the last hidden layer before\nthe output layer. This indicates that even a relatively simple,\nbrain-constrained recurrent neural network can effectively differentiate\nbetween various construction types.\n  These findings are consistent with previous studies demonstrating the\nemergence of word class and syntax rule representations in recurrent language\nmodels trained on next word prediction tasks. In future work, we aim to\nvalidate these results using larger language models and compare them with\nneuroimaging data obtained during continuous speech perception. This study\nhighlights the potential of recurrent neural language models to mirror\nlinguistic processing in the human brain, providing valuable insights into the\ncomputational and neural mechanisms underlying language understanding."
        ],
        "neg": []
    },
    {
        "query": "This study investigates how BERT processes and represents Argument Structure\nConstructions (ASCs), extending previous LSTM analyses. Using a dataset of 2000\nsentences across four ASC types (transitive, ditransitive, caused-motion,\nresultative), we analyzed BERT's token embeddings across 12 layers.\nVisualizations with MDS and t-SNE and clustering quantified by Generalized\nDiscrimination Value (GDV) were used. Feedforward classifiers (probes)\npredicted construction categories from embeddings. CLS token embeddings\nclustered best in layers 2-4, decreased in intermediate layers, and slightly\nincreased in final layers. DET and SUBJ embeddings showed consistent clustering\nin intermediate layers, VERB embeddings increased in clustering from layer 1 to\n12, and OBJ embeddings peaked in layer 10. Probe accuracies indicated low\nconstruction information in layer 1, with over 90 percent accuracy from layer 2\nonward, revealing latent construction information beyond GDV clustering. Fisher\nDiscriminant Ratio (FDR) analysis of attention weights showed OBJ tokens were\ncrucial for differentiating ASCs, followed by VERB and DET tokens. SUBJ, CLS,\nand SEP tokens had insignificant FDR scores. This study highlights BERT's\nlayered processing of linguistic constructions and its differences from LSTMs.\nFuture research will compare these findings with neuroimaging data to\nunderstand the neural correlates of ASC processing. This research underscores\nneural language models' potential to mirror linguistic processing in the human\nbrain, offering insights into the computational and neural mechanisms\nunderlying language understanding.",
        "pos": [
            "Understanding how language and linguistic constructions are processed in the\nbrain is a fundamental question in cognitive computational neuroscience. In\nthis study, we explore the representation and processing of Argument Structure\nConstructions (ASCs) in a recurrent neural language model. We trained a Long\nShort-Term Memory (LSTM) network on a custom-made dataset consisting of 2000\nsentences, generated using GPT-4, representing four distinct ASCs: transitive,\nditransitive, caused-motion, and resultative constructions.\n  We analyzed the internal activations of the LSTM model's hidden layers using\nMultidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding\n(t-SNE) to visualize the sentence representations. The Generalized\nDiscrimination Value (GDV) was calculated to quantify the degree of clustering\nwithin these representations. Our results show that sentence representations\nform distinct clusters corresponding to the four ASCs across all hidden layers,\nwith the most pronounced clustering observed in the last hidden layer before\nthe output layer. This indicates that even a relatively simple,\nbrain-constrained recurrent neural network can effectively differentiate\nbetween various construction types.\n  These findings are consistent with previous studies demonstrating the\nemergence of word class and syntax rule representations in recurrent language\nmodels trained on next word prediction tasks. In future work, we aim to\nvalidate these results using larger language models and compare them with\nneuroimaging data obtained during continuous speech perception. This study\nhighlights the potential of recurrent neural language models to mirror\nlinguistic processing in the human brain, providing valuable insights into the\ncomputational and neural mechanisms underlying language understanding."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
        "pos": [
            "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
        "pos": [
            "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
        "pos": [
            "It is time-saving to build a reading assistant for customer service\nrepresentations (CSRs) when reading user manuals, especially information-rich\nones. Current solutions don't fit the online custom service scenarios well due\nto the lack of attention to user questions and possible responses. Hence, we\npropose to develop a time-saving and careful reading assistant for CSRs, named\nCARE. It can help the CSRs quickly find proper responses from the user manuals\nvia explicit clue chains. Specifically, each of the clue chains is formed by\ninferring over the user manuals, starting from the question clue aligned with\nthe user question and ending at a possible response. To overcome the shortage\nof supervised data, we adopt the self-supervised strategy for model learning.\nThe offline experiment shows that CARE is efficient in automatically inferring\naccurate responses from the user manual. The online experiment further\ndemonstrates the superiority of CARE to reduce CSRs' reading burden and keep\nhigh service quality, in particular with >35% decrease in time spent and\nkeeping a >0.75 ICC score."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
        "pos": [
            "Large language models demonstrate reasonable multilingual abilities, despite\npredominantly English-centric pretraining. However, the spontaneous\nmultilingual alignment in these models is shown to be weak, leading to\nunsatisfactory cross-lingual transfer and knowledge sharing. Previous works\nattempt to address this issue by explicitly injecting multilingual alignment\ninformation during or after pretraining. Thus for the early stage in\npretraining, the alignment is weak for sharing information or knowledge across\nlanguages. In this paper, we propose PreAlign, a framework that establishes\nmultilingual alignment prior to language model pretraining. PreAlign injects\nmultilingual alignment by initializing the model to generate similar\nrepresentations of aligned words and preserves this alignment using a\ncode-switching strategy during pretraining. Extensive experiments in a\nsynthetic English to English-Clone setting demonstrate that PreAlign\nsignificantly outperforms standard multilingual joint training in language\nmodeling, zero-shot cross-lingual transfer, and cross-lingual knowledge\napplication. Further experiments in real-world scenarios further validate\nPreAlign's effectiveness across various model sizes.",
            "Decoding by contrasting layers (DoLa), is designed to improve the generation\nquality of large language models (LLMs) by contrasting the prediction\nprobabilities between an early exit output (amateur logits) and the final\noutput (expert logits). However, we find that this approach does not work well\non non-English tasks. Inspired by previous interpretability work on language\ntransition during the model's forward pass, we discover that this issue arises\nfrom a language mismatch between early exit output and final output. In this\nwork, we propose an improved contrastive decoding algorithm that is effective\nfor diverse languages beyond English. To obtain more helpful amateur logits, we\ndevise two strategies to skip a set of bottom, language-agnostic layers based\non our preliminary analysis. Experimental results on multilingual reasoning\nbenchmarks demonstrate that our proposed method outperforms previous\ncontrastive decoding baselines and substantially improves LLM's\nchain-of-thought reasoning accuracy across 11 languages. The project will be\navailable at: https://github.com/NJUNLP/SkipLayerCD."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
        "pos": [
            "Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.",
            "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
        "pos": [
            "Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.",
            "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
        "pos": [
            "Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.",
            "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM.",
            "Large Language Models (LLMs) have gained widespread adoption in various\nnatural language processing tasks, including question answering and dialogue\nsystems. However, a major drawback of LLMs is the issue of hallucination, where\nthey generate unfaithful or inconsistent content that deviates from the input\nsource, leading to severe consequences. In this paper, we propose a robust\ndiscriminator named RelD to effectively detect hallucination in LLMs' generated\nanswers. RelD is trained on the constructed RelQA, a bilingual\nquestion-answering dialogue dataset along with answers generated by LLMs and a\ncomprehensive set of metrics. Our experimental results demonstrate that the\nproposed RelD successfully detects hallucination in the answers generated by\ndiverse LLMs. Moreover, it performs well in distinguishing hallucination in\nLLMs' generated answers from both in-distribution and out-of-distribution\ndatasets. Additionally, we also conduct a thorough analysis of the types of\nhallucinations that occur and present valuable insights. This research\nsignificantly contributes to the detection of reliable answers generated by\nLLMs and holds noteworthy implications for mitigating hallucination in the\nfuture work.",
            "Recent years, Pre-trained Language models (PLMs) have swept into various\nfields of artificial intelligence and achieved great success. However, most\nPLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them\nis often expensive and time consuming, and storing them takes up a lot of\nspace. Therefore, it is necessary to adopt a parameter-efficient approach to\nreduce parameters of PLMs in fine-tuning without compromising their performance\nin downstream tasks. In this paper, we design a novel adapter which only acts\non self-attention outputs in PLMs. This adapter adopts element-wise linear\ntransformation using Hadamard product, hence named as Hadamard adapter,\nrequires the fewest parameters compared to previous parameter-efficient\nadapters. In addition, we also summarize some tuning patterns for Hadamard\nadapter shared by various downstream tasks, expecting to provide some guidance\nfor further parameter reduction with shared adapters in future studies. The\nexperiments conducted on the widely-used GLUE benchmark with several SOTA PLMs\nprove that the Hadamard adapter achieves competitive performance with only\n0.033\\% parameters compared with full fine-tuning, and it has the fewest\nparameters compared with other adapters. Moreover, we further find that there\nis also some redundant layers in the Hadamard adapter which can be removed to\nachieve more parameter efficiency with only 0.022\\% parameters.",
            "Large language models (LLMs) have played a fundamental role in various\nnatural language processing tasks with powerful prompt techniques. However, in\nreal-world applications, there are often similar prompt components for repeated\nqueries, which causes significant computational burdens during inference.\nExisting prompt compression and direct fine-tuning methods aim to tackle these\nchallenges, yet they frequently struggle to strike an optimal balance between\ncost-efficiency and performance effectiveness, especially in complex tasks such\nas NL2Code. In this paper, we propose a novel method namely PromptIntern to\ninternalize the prompt knowledge into model parameters via progressive\nfine-tuning. Our method enables LLMs to emulate the human learning process for\na new task, where detailed templates and examples in a prompt are gradually\ninternalized and phased out progressively as the model grows accustomed to the\ntask. Extensive experiments demonstrate that our method reduces inference\ntokens over 90%, speedups inference by 4.2 times, and saves 88.3% monetary\ncost."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
        "pos": [
            "The recognition of named entities in visually-rich documents (VrD-NER) plays\na critical role in various real-world scenarios and applications. However, the\nresearch in VrD-NER faces three major challenges: complex document layouts,\nincorrect reading orders, and unsuitable task formulations. To address these\nchallenges, we propose a query-aware entity extraction head, namely UNER, to\ncollaborate with existing multi-modal document transformers to develop more\nrobust VrD-NER models. The UNER head considers the VrD-NER task as a\ncombination of sequence labeling and reading order prediction, effectively\naddressing the issues of discontinuous entities in documents. Experimental\nevaluations on diverse datasets demonstrate the effectiveness of UNER in\nimproving entity extraction performance. Moreover, the UNER head enables a\nsupervised pre-training stage on various VrD-NER datasets to enhance the\ndocument transformer backbones and exhibits substantial knowledge transfer from\nthe pre-training stage to the fine-tuning stage. By incorporating universal\nlayout understanding, a pre-trained UNER-based model demonstrates significant\nadvantages in few-shot and cross-linguistic scenarios and exhibits zero-shot\nentity extraction abilities.",
            "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM.",
            "Multimodal large language models (MLLMs) have shown promising advancements in\ngeneral visual and language understanding. However, the representation of\nmultimodal information using MLLMs remains largely unexplored. In this work, we\nintroduce a new framework, E5-V, designed to adapt MLLMs for achieving\nuniversal multimodal embeddings. Our findings highlight the significant\npotential of MLLMs in representing multimodal inputs compared to previous\napproaches. By leveraging MLLMs with prompts, E5-V effectively bridges the\nmodality gap between different types of inputs, demonstrating strong\nperformance in multimodal embeddings even without fine-tuning. We propose a\nsingle modality training approach for E5-V, where the model is trained\nexclusively on text pairs. This method demonstrates significant improvements\nover traditional multimodal training on image-text pairs, while reducing\ntraining costs by approximately 95%. Additionally, this approach eliminates the\nneed for costly multimodal training data collection. Extensive experiments\nacross four types of tasks demonstrate the effectiveness of E5-V. As a\nuniversal multimodal model, E5-V not only achieves but often surpasses\nstate-of-the-art performance in each task, despite being trained on a single\nmodality.",
            "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems."
        ],
        "neg": []
    },
    {
        "query": "Detecting semantic arguments of a predicate word has been conventionally\nmodeled as a sentence-level task. The typical reader, however, perfectly\ninterprets predicate-argument relations in a much wider context than just the\nsentence where the predicate was evoked. In this work, we reformulate the\nproblem of argument detection through textual entailment to capture semantic\nrelations across sentence boundaries. We propose a method that tests whether\nsome semantic relation can be inferred from a full passage by first encoding it\ninto a simple and standalone proposition and then testing for entailment\nagainst the passage. Our method does not require direct supervision, which is\ngenerally absent due to dataset scarcity, but instead builds on existing NLI\nand sentence-level SRL resources. Such a method can potentially explicate\npragmatically understood relations into a set of explicit sentences. We\ndemonstrate it on a recent document-level benchmark, outperforming some\nsupervised methods and contemporary language models.",
        "pos": [
            "Imagine observing someone scratching their arm; to understand why, additional\ncontext would be necessary. However, spotting a mosquito nearby would\nimmediately offer a likely explanation for the person's discomfort, thereby\nalleviating the need for further information. This example illustrates how\nsubtle visual cues can challenge our cognitive skills and demonstrates the\ncomplexity of interpreting visual scenarios. To study these skills, we present\nVisual Riddles, a benchmark aimed to test vision and language models on visual\nriddles requiring commonsense and world knowledge. The benchmark comprises 400\nvisual riddles, each featuring a unique image created by a variety of\ntext-to-image models, question, ground-truth answer, textual hint, and\nattribution. Human evaluation reveals that existing models lag significantly\nbehind human performance, which is at 82\\% accuracy, with Gemini-Pro-1.5\nleading with 40\\% accuracy. Our benchmark comes with automatic evaluation tasks\nto make assessment scalable. These findings underscore the potential of Visual\nRiddles as a valuable resource for enhancing vision and language models'\ncapabilities in interpreting complex visual scenarios."
        ],
        "neg": []
    },
    {
        "query": "Detecting semantic arguments of a predicate word has been conventionally\nmodeled as a sentence-level task. The typical reader, however, perfectly\ninterprets predicate-argument relations in a much wider context than just the\nsentence where the predicate was evoked. In this work, we reformulate the\nproblem of argument detection through textual entailment to capture semantic\nrelations across sentence boundaries. We propose a method that tests whether\nsome semantic relation can be inferred from a full passage by first encoding it\ninto a simple and standalone proposition and then testing for entailment\nagainst the passage. Our method does not require direct supervision, which is\ngenerally absent due to dataset scarcity, but instead builds on existing NLI\nand sentence-level SRL resources. Such a method can potentially explicate\npragmatically understood relations into a set of explicit sentences. We\ndemonstrate it on a recent document-level benchmark, outperforming some\nsupervised methods and contemporary language models.",
        "pos": [
            "Synthetic data has become an important tool in the fine-tuning of language\nmodels to follow instructions and solve complex problems. Nevertheless, the\nmajority of open data to date is often lacking multi-turn data and collected on\nclosed models, limiting progress on advancing open fine-tuning methods. We\nintroduce Self Directed Synthetic Dialogues (SDSD), an experimental dataset\nconsisting of guided conversations of language models talking to themselves.\nThe dataset consists of multi-turn conversations generated with DBRX, Llama 2\n70B, and Mistral Large, all instructed to follow a conversation plan generated\nprior to the conversation. We also explore including principles from\nConstitutional AI and other related works to create synthetic preference data\nvia revisions to the final conversation turn. We hope this work encourages\nfurther exploration in multi-turn data and the use of open models for expanding\nthe impact of synthetic data.",
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox",
        "pos": [
            "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development."
        ],
        "neg": []
    },
    {
        "query": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox",
        "pos": [
            "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenDevin, a platform for the development of powerful and\nflexible AI agents that interact with the world in similar ways to those of a\nhuman developer: by writing code, interacting with a command line, and browsing\nthe web. We describe how the platform allows for the implementation of new\nagents, safe interaction with sandboxed environments for code execution,\ncoordination between multiple agents, and incorporation of evaluation\nbenchmarks. Based on our currently incorporated benchmarks, we perform an\nevaluation of agents over 15 challenging tasks, including software engineering\n(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released\nunder the permissive MIT license, OpenDevin is a community project spanning\nacademia and industry with more than 1.3K contributions from over 160\ncontributors and will improve going forward."
        ],
        "neg": []
    },
    {
        "query": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox",
        "pos": [
            "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development."
        ],
        "neg": []
    },
    {
        "query": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox",
        "pos": [
            "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development."
        ],
        "neg": []
    },
    {
        "query": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox",
        "pos": [
            "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development."
        ],
        "neg": []
    },
    {
        "query": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox",
        "pos": [
            "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development."
        ],
        "neg": []
    },
    {
        "query": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox",
        "pos": [
            "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development."
        ],
        "neg": []
    },
    {
        "query": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox",
        "pos": [
            "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
            "Preference alignment has become a crucial component in enhancing the\nperformance of Large Language Models (LLMs), yet its impact in Multimodal Large\nLanguage Models (MLLMs) remains comparatively underexplored. Similar to\nlanguage models, MLLMs for image understanding tasks encounter challenges like\nhallucination. In MLLMs, hallucination can occur not only by stating incorrect\nfacts but also by producing responses that are inconsistent with the image\ncontent. A primary objective of alignment for MLLMs is to encourage these\nmodels to align responses more closely with image information. Recently,\nmultiple works have introduced preference datasets for MLLMs and examined\ndifferent alignment methods, including Direct Preference Optimization (DPO) and\nProximal Policy Optimization (PPO). However, due to variations in datasets,\nbase model types, and alignment methods, it remains unclear which specific\nelements contribute most significantly to the reported improvements in these\nworks. In this paper, we independently analyze each aspect of preference\nalignment in MLLMs. We start by categorizing the alignment algorithms into two\ngroups, offline (such as DPO), and online (such as online-DPO), and show that\ncombining offline and online methods can improve the performance of the model\nin certain scenarios. We review a variety of published multimodal preference\ndatasets and discuss how the details of their construction impact model\nperformance. Based on these insights, we introduce a novel way of creating\nmultimodal preference data called Bias-Driven Hallucination Sampling (BDHS)\nthat needs neither additional annotation nor external models, and show that it\ncan achieve competitive performance to previously published alignment work for\nmultimodal models across a range of benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox",
        "pos": [
            "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development."
        ],
        "neg": []
    },
    {
        "query": "Our work presents a novel angle for evaluating language models' (LMs)\nmathematical abilities, by investigating whether they can discern skills and\nconcepts enabled by math content. We contribute two datasets: one consisting of\n385 fine-grained descriptions of K-12 math skills and concepts, or standards,\nfrom Achieve the Core (ATC), and another of 9.9K problems labeled with these\nstandards (MathFish). Working with experienced teachers, we find that LMs\nstruggle to tag and verify standards linked to problems, and instead predict\nlabels that are close to ground truth, but differ in subtle ways. We also show\nthat LMs often generate problems that do not fully align with standards\ndescribed in prompts. Finally, we categorize problems in GSM8k using math\nstandards, allowing us to better understand why some problems are more\ndifficult to solve for models than others.",
        "pos": [
            "Synthetic data has become an important tool in the fine-tuning of language\nmodels to follow instructions and solve complex problems. Nevertheless, the\nmajority of open data to date is often lacking multi-turn data and collected on\nclosed models, limiting progress on advancing open fine-tuning methods. We\nintroduce Self Directed Synthetic Dialogues (SDSD), an experimental dataset\nconsisting of guided conversations of language models talking to themselves.\nThe dataset consists of multi-turn conversations generated with DBRX, Llama 2\n70B, and Mistral Large, all instructed to follow a conversation plan generated\nprior to the conversation. We also explore including principles from\nConstitutional AI and other related works to create synthetic preference data\nvia revisions to the final conversation turn. We hope this work encourages\nfurther exploration in multi-turn data and the use of open models for expanding\nthe impact of synthetic data."
        ],
        "neg": []
    },
    {
        "query": "Current language models demonstrate remarkable proficiency in text\ngeneration. However, for many applications it is desirable to control\nattributes, such as sentiment, or toxicity, of the generated language --\nideally tailored towards each specific use case and target audience. For\nauto-regressive language models, existing guidance methods are prone to\ndecoding errors that cascade during generation and degrade performance. In\ncontrast, text diffusion models can easily be guided with, for example, a\nsimple linear sentiment classifier -- however they do suffer from significantly\nhigher perplexity than auto-regressive alternatives. In this paper we use a\nguided diffusion model to produce a latent proposal that steers an\nauto-regressive language model to generate text with desired properties. Our\nmodel inherits the unmatched fluency of the auto-regressive approach and the\nplug-and-play flexibility of diffusion. We show that it outperforms previous\nplug-and-play guidance methods across a wide range of benchmark data sets.\nFurther, controlling a new attribute in our framework is reduced to training a\nsingle logistic regression classifier.",
        "pos": [
            "Large language models (LLMs) currently dominate the field of natural language\nprocessing (NLP), representing the state-of-the-art across a diverse array of\ntasks. Developing a model of this nature, from training to inference, requires\nmaking numerous decisions which define a combinatorial search problem. For\nexample, selecting the optimal pre-trained LLM, prompt, or hyperparameters to\nattain the best performance for a task often requires evaluating multiple\ncandidates on an entire test set. This exhaustive evaluation can be\ntime-consuming and costly, as both inference and metric computation with LLMs\nare resource-intensive. In this paper, we address the challenge of identifying\nthe best method within a limited budget for evaluating methods on test\nexamples. By leveraging the well-studied multi-armed bandit framework, which\nsequentially selects the next method-example pair to evaluate, our approach,\ncombining multi-armed bandit algorithms with low-rank factorization,\nsignificantly reduces the required resources. Experiments show that our\nalgorithms can identify the top-performing method using only 5-15\\% of the\ntypically needed resources, resulting in an 85-95\\% reduction in cost.",
            "This paper presents a novel approach to aligning large language models (LLMs)\nwith individual human preferences, sometimes referred to as Reinforcement\nLearning from \\textit{Personalized} Human Feedback (RLPHF). Given stated\npreferences along multiple dimensions, such as helpfulness, conciseness, or\nhumor, the goal is to create an LLM without re-training that best adheres to\nthis specification. Starting from specialized expert LLMs, each trained for one\nsuch particular preference dimension, we propose a black-box method that merges\ntheir outputs on a per-token level. We train a lightweight Preference Control\nModel (PCM) that dynamically translates the preference description and current\ncontext into next-token prediction weights. By combining the expert models'\noutputs at the token level, our approach dynamically generates text that\noptimizes the given preference. Empirical tests show that our method matches or\nsurpasses existing preference merging techniques, providing a scalable,\nefficient alternative to fine-tuning LLMs for individual personalization."
        ],
        "neg": []
    },
    {
        "query": "This paper advances a novel architectural schema anchored upon the\nTransformer paradigm and innovatively amalgamates the K-means categorization\nalgorithm to augment the contextual apprehension capabilities of the schema.\nThe transformer model performs well in machine translation tasks due to its\nparallel computing power and multi-head attention mechanism. However, it may\nencounter contextual ambiguity or ignore local features when dealing with\nhighly complex language structures. To circumvent this constraint, this\nexposition incorporates the K-Means algorithm, which is used to stratify the\nlexis and idioms of the input textual matter, thereby facilitating superior\nidentification and preservation of the local structure and contextual\nintelligence of the language. The advantage of this combination is that K-Means\ncan automatically discover the topic or concept regions in the text, which may\nbe directly related to translation quality. Consequently, the schema contrived\nherein enlists K-Means as a preparatory phase antecedent to the Transformer and\nrecalibrates the multi-head attention weights to assist in the discrimination\nof lexis and idioms bearing analogous semantics or functionalities. This\nensures the schema accords heightened regard to the contextual intelligence\nembodied by these clusters during the training phase, rather than merely\nfocusing on locational intelligence.",
        "pos": [
            "Retrieval augmented generation (RAG) combines the generative abilities of\nlarge language models (LLMs) with external knowledge sources to provide more\naccurate and up-to-date responses. Recent RAG advancements focus on improving\nretrieval outcomes through iterative LLM refinement or self-critique\ncapabilities acquired through additional instruction tuning of LLMs. In this\nwork, we introduce Speculative RAG - a framework that leverages a larger\ngeneralist LM to efficiently verify multiple RAG drafts produced in parallel by\na smaller, distilled specialist LM. Each draft is generated from a distinct\nsubset of retrieved documents, offering diverse perspectives on the evidence\nwhile reducing input token counts per draft. This approach enhances\ncomprehension of each subset and mitigates potential position bias over long\ncontext. Our method accelerates RAG by delegating drafting to the smaller\nspecialist LM, with the larger generalist LM performing a single verification\npass over the drafts. Extensive experiments demonstrate that Speculative RAG\nachieves state-of-the-art performance with reduced latency on TriviaQA,\nMuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy\nby up to 12.97% while reducing latency by 51% compared to conventional RAG\nsystems on PubHealth."
        ],
        "neg": []
    },
    {
        "query": "The importance of recommender systems is growing rapidly due to the\nexponential increase in the volume of content generated daily. This surge in\ncontent presents unique challenges for designing effective recommender systems.\nKey among these challenges is the need to effectively leverage the vast amounts\nof natural language data and images that represent user preferences. This paper\npresents a novel approach to enhancing recommender systems by leveraging Large\nLanguage Models (LLMs) and deep learning techniques. The proposed framework\naims to improve the accuracy and relevance of recommendations by incorporating\nmulti-modal information processing and by the use of unified latent space\nrepresentation. The study explores the potential of LLMs to better understand\nand utilize natural language data in recommendation contexts, addressing the\nlimitations of previous methods. The framework efficiently extracts and\nintegrates text and image information through LLMs, unifying diverse modalities\nin a latent space to simplify the learning process for the ranking model.\nExperimental results demonstrate the enhanced discriminative power of the model\nwhen utilizing multi-modal information. This research contributes to the\nevolving field of recommender systems by showcasing the potential of LLMs and\nmulti-modal data integration to create more personalized and contextually\nrelevant recommendations.",
        "pos": [
            "Prompt recovery, a crucial task in natural language processing, entails the\nreconstruction of prompts or instructions that language models use to convert\ninput text into a specific output. Although pivotal, the design and\neffectiveness of prompts represent a challenging and relatively untapped field\nwithin NLP research. This paper delves into an exhaustive investigation of\nprompt recovery methodologies, employing a spectrum of pre-trained language\nmodels and strategies. Our study is a comparative analysis aimed at gauging the\nefficacy of various models on a benchmark dataset, with the goal of pinpointing\nthe most proficient approach for prompt recovery. Through meticulous\nexperimentation and detailed analysis, we elucidate the outstanding performance\nof the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its\ncounterparts, showcasing its exceptional capability in accurately\nreconstructing prompts for text transformation tasks. Our findings offer a\nsignificant contribution to the existing knowledge on prompt recovery, shedding\nlight on the intricacies of prompt design and offering insightful perspectives\nfor future innovations in text rewriting and the broader field of natural\nlanguage processing."
        ],
        "neg": []
    },
    {
        "query": "Knowledge graphs (KGs) enhance the performance of large language models\n(LLMs) and search engines by providing structured, interconnected data that\nimproves reasoning and context-awareness. However, KGs only focus on text data,\nthereby neglecting other modalities such as speech. In this work, we introduce\nwav2graph, the first framework for supervised learning knowledge graph from\nspeech data. Our pipeline are straightforward: (1) constructing a KG based on\ntranscribed spoken utterances and a named entity database, (2) converting KG\ninto embedding vectors, and (3) training graph neural networks (GNNs) for node\nclassification and link prediction tasks. Through extensive experiments\nconducted in inductive and transductive learning contexts using\nstate-of-the-art GNN models, we provide baseline results and error analysis for\nnode classification and link prediction tasks on human transcripts and\nautomatic speech recognition (ASR) transcripts, including evaluations using\nboth encoder-based and decoder-based node embeddings, as well as monolingual\nand multilingual acoustic pre-trained models. All related code, data, and\nmodels are published online.",
        "pos": [
            "Transparency in AI decision-making is crucial in healthcare due to the severe\nconsequences of errors, and this is important for building trust among AI and\nusers in sentiment analysis task. Incorporating reasoning capabilities helps\nLarge Language Models (LLMs) understand human emotions within broader contexts,\nhandle nuanced and ambiguous language, and infer underlying sentiments that may\nnot be explicitly stated. In this work, we introduce a new task - Sentiment\nReasoning - for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Our study showed that\nrationale-augmented training enhances model performance in sentiment\nclassification across both human transcript and ASR settings. Also, we found\nthat the generated rationales typically exhibit different vocabularies compared\nto human-generated rationales, but maintain similar semantics. All code, data\n(English-translated and Vietnamese) and models are published online:\nhttps://github.com/leduckhai/MultiMed",
            "Vision-language models have been extensively explored across a wide range of\ntasks, achieving satisfactory performance; however, their application in\nmedical imaging remains underexplored. In this work, we propose a unified\nframework - LiteGPT - for the medical imaging. We leverage multiple pre-trained\nvisual encoders to enrich information and enhance the performance of\nvision-language models. To the best of our knowledge, this is the first study\nto utilize vision-language models for the novel task of joint localization and\nclassification in medical images. Besides, we are pioneers in providing\nbaselines for disease localization in chest X-rays. Finally, we set new\nstate-of-the-art performance in the image classification task on the\nwell-benchmarked VinDr-CXR dataset. All code and models are publicly available\nonline: https://github.com/leduckhai/LiteGPT"
        ],
        "neg": []
    },
    {
        "query": "Knowledge graphs (KGs) enhance the performance of large language models\n(LLMs) and search engines by providing structured, interconnected data that\nimproves reasoning and context-awareness. However, KGs only focus on text data,\nthereby neglecting other modalities such as speech. In this work, we introduce\nwav2graph, the first framework for supervised learning knowledge graph from\nspeech data. Our pipeline are straightforward: (1) constructing a KG based on\ntranscribed spoken utterances and a named entity database, (2) converting KG\ninto embedding vectors, and (3) training graph neural networks (GNNs) for node\nclassification and link prediction tasks. Through extensive experiments\nconducted in inductive and transductive learning contexts using\nstate-of-the-art GNN models, we provide baseline results and error analysis for\nnode classification and link prediction tasks on human transcripts and\nautomatic speech recognition (ASR) transcripts, including evaluations using\nboth encoder-based and decoder-based node embeddings, as well as monolingual\nand multilingual acoustic pre-trained models. All related code, data, and\nmodels are published online.",
        "pos": [
            "Vision-language models have been extensively explored across a wide range of\ntasks, achieving satisfactory performance; however, their application in\nmedical imaging remains underexplored. In this work, we propose a unified\nframework - LiteGPT - for the medical imaging. We leverage multiple pre-trained\nvisual encoders to enrich information and enhance the performance of\nvision-language models. To the best of our knowledge, this is the first study\nto utilize vision-language models for the novel task of joint localization and\nclassification in medical images. Besides, we are pioneers in providing\nbaselines for disease localization in chest X-rays. Finally, we set new\nstate-of-the-art performance in the image classification task on the\nwell-benchmarked VinDr-CXR dataset. All code and models are publicly available\nonline: https://github.com/leduckhai/LiteGPT"
        ],
        "neg": []
    },
    {
        "query": "Knowledge graphs (KGs) enhance the performance of large language models\n(LLMs) and search engines by providing structured, interconnected data that\nimproves reasoning and context-awareness. However, KGs only focus on text data,\nthereby neglecting other modalities such as speech. In this work, we introduce\nwav2graph, the first framework for supervised learning knowledge graph from\nspeech data. Our pipeline are straightforward: (1) constructing a KG based on\ntranscribed spoken utterances and a named entity database, (2) converting KG\ninto embedding vectors, and (3) training graph neural networks (GNNs) for node\nclassification and link prediction tasks. Through extensive experiments\nconducted in inductive and transductive learning contexts using\nstate-of-the-art GNN models, we provide baseline results and error analysis for\nnode classification and link prediction tasks on human transcripts and\nautomatic speech recognition (ASR) transcripts, including evaluations using\nboth encoder-based and decoder-based node embeddings, as well as monolingual\nand multilingual acoustic pre-trained models. All related code, data, and\nmodels are published online.",
        "pos": [
            "Transparency in AI decision-making is crucial in healthcare due to the severe\nconsequences of errors, and this is important for building trust among AI and\nusers in sentiment analysis task. Incorporating reasoning capabilities helps\nLarge Language Models (LLMs) understand human emotions within broader contexts,\nhandle nuanced and ambiguous language, and infer underlying sentiments that may\nnot be explicitly stated. In this work, we introduce a new task - Sentiment\nReasoning - for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Our study showed that\nrationale-augmented training enhances model performance in sentiment\nclassification across both human transcript and ASR settings. Also, we found\nthat the generated rationales typically exhibit different vocabularies compared\nto human-generated rationales, but maintain similar semantics. All code, data\n(English-translated and Vietnamese) and models are published online:\nhttps://github.com/leduckhai/MultiMed",
            "Vision-language models have been extensively explored across a wide range of\ntasks, achieving satisfactory performance; however, their application in\nmedical imaging remains underexplored. In this work, we propose a unified\nframework - LiteGPT - for the medical imaging. We leverage multiple pre-trained\nvisual encoders to enrich information and enhance the performance of\nvision-language models. To the best of our knowledge, this is the first study\nto utilize vision-language models for the novel task of joint localization and\nclassification in medical images. Besides, we are pioneers in providing\nbaselines for disease localization in chest X-rays. Finally, we set new\nstate-of-the-art performance in the image classification task on the\nwell-benchmarked VinDr-CXR dataset. All code and models are publicly available\nonline: https://github.com/leduckhai/LiteGPT"
        ],
        "neg": []
    },
    {
        "query": "Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks\nthat outperforms conventional maximum a posterior (MAP) decoding using beam\nsearch by selecting high-quality outputs based on a utility function rather\nthan those with high-probability. Typically, it finds the most suitable\nhypothesis from the set of hypotheses under the sampled pseudo-references. mbrs\nis a library of MBR decoding, which can flexibly combine various metrics,\nalternative expectation estimations, and algorithmic variants. It is designed\nwith a focus on speed measurement and calling count of code blocks,\ntransparency, reproducibility, and extensibility, which are essential for\nresearchers and developers. We published our mbrs as an MIT-licensed\nopen-source project, and the code is available on GitHub.\n  GitHub: https://github.com/naist-nlp/mbrs",
        "pos": [
            "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
        ],
        "neg": []
    },
    {
        "query": "Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks\nthat outperforms conventional maximum a posterior (MAP) decoding using beam\nsearch by selecting high-quality outputs based on a utility function rather\nthan those with high-probability. Typically, it finds the most suitable\nhypothesis from the set of hypotheses under the sampled pseudo-references. mbrs\nis a library of MBR decoding, which can flexibly combine various metrics,\nalternative expectation estimations, and algorithmic variants. It is designed\nwith a focus on speed measurement and calling count of code blocks,\ntransparency, reproducibility, and extensibility, which are essential for\nresearchers and developers. We published our mbrs as an MIT-licensed\nopen-source project, and the code is available on GitHub.\n  GitHub: https://github.com/naist-nlp/mbrs",
        "pos": [
            "The extreme multi-label classification~(XMC) task involves learning a\nclassifier that can predict from a large label set the most relevant subset of\nlabels for a data instance. While deep neural networks~(DNNs) have demonstrated\nremarkable success in XMC problems, the task is still challenging because it\nmust deal with a large number of output labels, which make the DNN training\ncomputationally expensive. This paper addresses the issue by exploring the use\nof random circular vectors, where each vector component is represented as a\ncomplex amplitude. In our framework, we can develop an output layer and loss\nfunction of DNNs for XMC by representing the final output layer as a fully\nconnected layer that directly predicts a low-dimensional circular vector\nencoding a set of labels for a data instance. We conducted experiments on\nsynthetic datasets to verify that circular vectors have better label encoding\ncapacity and retrieval ability than normal real-valued vectors. Then, we\nconducted experiments on actual XMC datasets and found that these appealing\nproperties of circular vectors contribute to significant improvements in task\nperformance compared with a previous model using random real-valued vectors,\nwhile reducing the size of the output layers by up to 99%.",
            "Knowledge Graphs (KGs) are fundamental resources in knowledge-intensive tasks\nin NLP. Due to the limitation of manually creating KGs, KG Completion (KGC) has\nan important role in automatically completing KGs by scoring their links with\nKG Embedding (KGE). To handle many entities in training, KGE relies on Negative\nSampling (NS) loss that can reduce the computational cost by sampling. Since\nthe appearance frequencies for each link are at most one in KGs, sparsity is an\nessential and inevitable problem. The NS loss is no exception. As a solution,\nthe NS loss in KGE relies on smoothing methods like Self-Adversarial Negative\nSampling (SANS) and subsampling. However, it is uncertain what kind of\nsmoothing method is suitable for this purpose due to the lack of theoretical\nunderstanding. This paper provides theoretical interpretations of the smoothing\nmethods for the NS loss in KGE and induces a new NS loss, Triplet Adaptive\nNegative Sampling (TANS), that can cover the characteristics of the\nconventional smoothing methods. Experimental results of TransE, DistMult,\nComplEx, RotatE, HAKE, and HousE on FB15k-237, WN18RR, and YAGO3-10 datasets\nand their sparser subsets show the soundness of our interpretation and\nperformance improvement by our TANS."
        ],
        "neg": []
    },
    {
        "query": "Developing imaging models capable of detecting pathologies from chest X-rays\ncan be cost and time-prohibitive for large datasets as it requires supervision\nto attain state-of-the-art performance. Instead, labels extracted from\nradiology reports may serve as distant supervision since these are routinely\ngenerated as part of clinical practice. Despite their widespread use, current\nrule-based methods for label extraction rely on extensive rule sets that are\nlimited in their robustness to syntactic variability. To alleviate these\nlimitations, we introduce RadPert, a rule-based system that integrates an\nuncertainty-aware information schema with a streamlined set of rules, enhancing\nperformance. Additionally, we have developed RadPrompt, a multi-turn prompting\nstrategy that leverages RadPert to bolster the zero-shot predictive\ncapabilities of large language models, achieving a statistically significant\nimprovement in weighted average F1 score over GPT-4 Turbo. Most notably,\nRadPrompt surpasses both its underlying models, showcasing the synergistic\npotential of LLMs with rule-based models. We have evaluated our methods on two\nEnglish Corpora: the MIMIC-CXR gold-standard test set and a gold-standard\ndataset collected from the Cambridge University Hospitals.",
        "pos": [
            "Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) can generate text by transferring style\nattributes like formality resulting in formal or informal text. However,\ninstructing LLMs to generate text that when spoken, is more intelligible in an\nacoustically difficult environment, is an under-explored topic. We conduct the\nfirst study to evaluate LLMs on a novel task of generating acoustically\nintelligible paraphrases for better human speech perception in noise. Our\nexperiments in English demonstrated that with standard prompting, LLMs struggle\nto control the non-textual attribute, i.e., acoustic intelligibility, while\nefficiently capturing the desired textual attributes like semantic equivalence.\nTo remedy this issue, we propose a simple prompting approach,\nprompt-and-select, which generates paraphrases by decoupling the desired\ntextual and non-textual attributes in the text generation pipeline. Our\napproach resulted in a 40% relative improvement in human speech perception, by\nparaphrasing utterances that are highly distorted in a listening condition with\nbabble noise at a signal-to-noise ratio (SNR) -5 dB. This study reveals the\nlimitation of LLMs in capturing non-textual attributes, and our proposed method\nshowcases the potential of using LLMs for better human speech perception in\nnoise.",
        "pos": [
            "Identifying beneficial tasks to transfer from is a critical step toward\nsuccessful intermediate-task transfer learning. In this work, we experiment\nwith 130 source-target task combinations and demonstrate that the transfer\nperformance exhibits severe variance across different source tasks and training\nseeds, highlighting the crucial role of intermediate-task selection in a\nbroader context. We compare four representative task selection methods in a\nunified setup, focusing on their effectiveness and consistency. Compared to\nembedding-free methods and text embeddings, task embeddings constructed from\nfine-tuned weights can better estimate task transferability by improving task\nprediction scores from 2.59% to 3.96%. Despite their strong performance, we\nobserve that the task embeddings do not consistently demonstrate superiority\nfor tasks requiring reasoning abilities. Furthermore, we introduce a novel\nmethod that measures pairwise token similarity using maximum inner product\nsearch, leading to the highest performance in task prediction. Our findings\nsuggest that token-wise similarity is better predictive for predicting\ntransferability compared to averaging weights."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) can generate text by transferring style\nattributes like formality resulting in formal or informal text. However,\ninstructing LLMs to generate text that when spoken, is more intelligible in an\nacoustically difficult environment, is an under-explored topic. We conduct the\nfirst study to evaluate LLMs on a novel task of generating acoustically\nintelligible paraphrases for better human speech perception in noise. Our\nexperiments in English demonstrated that with standard prompting, LLMs struggle\nto control the non-textual attribute, i.e., acoustic intelligibility, while\nefficiently capturing the desired textual attributes like semantic equivalence.\nTo remedy this issue, we propose a simple prompting approach,\nprompt-and-select, which generates paraphrases by decoupling the desired\ntextual and non-textual attributes in the text generation pipeline. Our\napproach resulted in a 40% relative improvement in human speech perception, by\nparaphrasing utterances that are highly distorted in a listening condition with\nbabble noise at a signal-to-noise ratio (SNR) -5 dB. This study reveals the\nlimitation of LLMs in capturing non-textual attributes, and our proposed method\nshowcases the potential of using LLMs for better human speech perception in\nnoise.",
        "pos": [
            "Identifying beneficial tasks to transfer from is a critical step toward\nsuccessful intermediate-task transfer learning. In this work, we experiment\nwith 130 source-target task combinations and demonstrate that the transfer\nperformance exhibits severe variance across different source tasks and training\nseeds, highlighting the crucial role of intermediate-task selection in a\nbroader context. We compare four representative task selection methods in a\nunified setup, focusing on their effectiveness and consistency. Compared to\nembedding-free methods and text embeddings, task embeddings constructed from\nfine-tuned weights can better estimate task transferability by improving task\nprediction scores from 2.59% to 3.96%. Despite their strong performance, we\nobserve that the task embeddings do not consistently demonstrate superiority\nfor tasks requiring reasoning abilities. Furthermore, we introduce a novel\nmethod that measures pairwise token similarity using maximum inner product\nsearch, leading to the highest performance in task prediction. Our findings\nsuggest that token-wise similarity is better predictive for predicting\ntransferability compared to averaging weights."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce CodexGraph, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, CodexGraph enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess CodexGraph using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, CodexGraph demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.",
        "pos": [
            "Data quality stands at the forefront of deciding the effectiveness of\nvideo-language representation learning. However, video-text pairs in previous\ndata typically do not align perfectly with each other, which might lead to\nvideo-language representations that do not accurately reflect cross-modal\nsemantics. Moreover, previous data also possess an uneven distribution of\nconcepts, thereby hampering the downstream performance across unpopular\nsubjects. To address these problems, we propose a contrastive objective with a\nsubtractive angular margin to regularize cross-modal representations in their\neffort to reach perfect similarity. Furthermore, to adapt to the non-uniform\nconcept distribution, we propose a multi-layer perceptron (MLP)-parameterized\nweighting function that maps loss values to sample weights which enable dynamic\nadjustment of the model's focus throughout the training. With the training\nguided by a small amount of unbiased meta-data and augmented by video-text data\ngenerated by large vision-language model, we improve video-language\nrepresentations and achieve superior performances on commonly used video\nquestion answering and text-video retrieval datasets."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce CodexGraph, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, CodexGraph enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess CodexGraph using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, CodexGraph demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.",
        "pos": [
            "Traditional knowledge graph (KG) completion models learn embeddings to\npredict missing facts. Recent works attempt to complete KGs in a\ntext-generation manner with large language models (LLMs). However, they need to\nground the output of LLMs to KG entities, which inevitably brings errors. In\nthis paper, we present a finetuning framework, DIFT, aiming to unleash the KG\ncompletion ability of LLMs and avoid grounding errors. Given an incomplete\nfact, DIFT employs a lightweight model to obtain candidate entities and\nfinetunes an LLM with discrimination instructions to select the correct one\nfrom the given candidates. To improve performance while reducing instruction\ndata, DIFT uses a truncated sampling method to select useful facts for\nfinetuning and injects KG embeddings into the LLM. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of our proposed framework.",
            "In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.",
            "In the rapidly evolving field of natural language processing, dialogue\nsystems primarily employ a single-step dialogue paradigm. Although this\nparadigm is efficient, it lacks the depth and fluidity of human interactions\nand does not appear natural. We introduce a novel \\textbf{Step}-by-Step\nDialogue Paradigm (Stephanie), designed to mimic the ongoing dynamic nature of\nhuman conversations. By employing a dual learning strategy and a further-split\npost-editing method, we generated and utilized a high-quality step-by-step\ndialogue dataset to fine-tune existing large language models, enabling them to\nperform step-by-step dialogues. We thoroughly present Stephanie. Tailored\nautomatic and human evaluations are conducted to assess its effectiveness\ncompared to the traditional single-step dialogue paradigm. We will release\ncode, Stephanie datasets, and Stephanie LLMs to facilitate the future of\nchatbot eras.",
            "Classical Chinese is a gateway to the rich heritage and wisdom of ancient\nChina, yet its complexities pose formidable comprehension barriers for most\nmodern people without specialized knowledge. While Large Language Models (LLMs)\nhave shown remarkable capabilities in Natural Language Processing (NLP), they\nstruggle with Classical Chinese Understanding (CCU), especially in\ndata-demanding and knowledge-intensive tasks. In response to this dilemma, we\npropose \\textbf{TongGu} (mean understanding ancient and modern), the first\nCCU-specific LLM, underpinned by three core contributions. First, we construct\na two-stage instruction-tuning dataset ACCN-INS derived from rich classical\nChinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we\npropose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,\nenabling TongGu to acquire new capabilities while preserving its foundational\nknowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)\ntechnique to reduce hallucinations based on knowledge-grounding. Extensive\nexperiments across 24 diverse CCU tasks validate TongGu's superior ability,\nunderscoring the effectiveness of RAT and CCU-RAG. The model and dataset will\nbe public available."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce CodexGraph, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, CodexGraph enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess CodexGraph using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, CodexGraph demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.",
        "pos": [
            "Objective: This study introduces ChatSchema, an effective method for\nextracting and structuring information from unstructured data in medical paper\nreports using a combination of Large Multimodal Models (LMMs) and Optical\nCharacter Recognition (OCR) based on the schema. By integrating predefined\nschema, we intend to enable LMMs to directly extract and standardize\ninformation according to the schema specifications, facilitating further data\nentry. Method: Our approach involves a two-stage process, including\nclassification and extraction for categorizing report scenarios and structuring\ninformation. We established and annotated a dataset to verify the effectiveness\nof ChatSchema, and evaluated key extraction using precision, recall, F1-score,\nand accuracy metrics. Based on key extraction, we further assessed value\nextraction. We conducted ablation studies on two LMMs to illustrate the\nimprovement of structured information extraction with different input modals\nand methods. Result: We analyzed 100 medical reports from Peking University\nFirst Hospital and established a ground truth dataset with 2,945 key-value\npairs. We evaluated ChatSchema using GPT-4o and Gemini 1.5 Pro and found a\nhigher overall performance of GPT-4o. The results are as follows: For the\nresult of key extraction, key-precision was 98.6%, key-recall was 98.5%,\nkey-F1-score was 98.6%. For the result of value extraction based on correct key\nextraction, the overall accuracy was 97.2%, precision was 95.8%, recall was\n95.8%, and F1-score was 95.8%. An ablation study demonstrated that ChatSchema\nachieved significantly higher overall accuracy and overall F1-score of\nkey-value extraction, compared to the Baseline, with increases of 26.9% overall\naccuracy and 27.4% overall F1-score, respectively."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce CodexGraph, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, CodexGraph enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess CodexGraph using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, CodexGraph demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.",
        "pos": [
            "When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\navailable at https://github.com/hannah-aught/space_attack."
        ],
        "neg": []
    },
    {
        "query": "In this system report, we describe the models and methods we used for our\nparticipation in the PLABA2023 task on biomedical abstract simplification, part\nof the TAC 2023 tracks. The system outputs we submitted come from the following\nthree categories: 1) domain fine-tuned T5-like models including Biomedical-T5\nand Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes\n(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work we\ncarried out for this task on BioGPT finetuning. In the official automatic\nevaluation using SARI scores, BeeManc ranks 2nd among all teams and our model\nLaySciFive ranks 3rd among all 13 evaluated systems. In the official human\nevaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score\n92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It\nalso produced a high score 91.57 on Fluency in comparison to the highest score\n93.53. In the second round of submissions, our team using ChatGPT-prompting\nranks the 2nd in several categories including simplified term accuracy score\n92.26 and completeness score 96.58, and a very similar score on faithfulness\nscore 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Our\ncodes, fine-tuned models, prompts, and data splits from the system development\nstage will be available at https://github.com/ HECTA-UoM/PLABA-MU",
        "pos": [
            "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir potential applications across various fields. This paper embarked on a\npivotal inquiry: Can existing LLMs effectively serve as \"water expert models\"\nfor water engineering and research tasks? This study was the first to evaluate\nLLMs' contributions across various water engineering and research tasks by\nestablishing a domain-specific benchmark suite, namely, WaterER. Herein, we\nprepared 983 tasks related to water engineering and research, categorized into\n\"wastewater treatment\", \"environmental restoration\", \"drinking water treatment\nand distribution\", \"sanitation\", \"anaerobic digestion\" and \"contaminants\nassessment\". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5,\nGemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the\nstrengths of GPT-4 in handling diverse and complex tasks of water engineering\nand water research, the specialized capabilities of Gemini in academic\ncontexts, Llama3's strongest capacity to answer Chinese water engineering\nquestions and the competitive performance of Chinese-oriented models like\nGLM-4, ERNIE and QWEN in some water engineering tasks. More specifically,\ncurrent LLMs excelled particularly in generating precise research gaps for\npapers on \"contaminants and related water quality monitoring and assessment\".\nAdditionally, they were more adept at creating appropriate titles for research\npapers on \"treatment processes for wastewaters\", \"environmental restoration\",\nand \"drinking water treatment\". Overall, this study pioneered evaluating LLMs\nin water engineering and research by introducing the WaterER benchmark to\nassess the trustworthiness of their predictions. This standardized evaluation\nframework would also drive future advancements in LLM technology by using\ntargeting datasets, propelling these models towards becoming true \"water\nexpert\".",
            "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across a wide array of text-centric tasks. However, their `large'\nscale introduces significant computational and storage challenges, particularly\nin managing the key-value states of the transformer, which limits their wider\napplicability. Therefore, we propose to adaptively release resources from\ncaches and rebuild the necessary key-value states. Particularly, we accomplish\nthis by a lightweight controller module to approximate an ideal top-$K$ sparse\nattention. This module retains the tokens with the highest top-$K$ attention\nweights and simultaneously rebuilds the discarded but necessary tokens, which\nmay become essential for future decoding. Comprehensive experiments in natural\nlanguage generation and modeling reveal that our method is not only competitive\nwith full attention in terms of performance but also achieves a significant\nthroughput improvement of up to 221.8%. The code for replication is available\non the https://github.com/WHUIR/ADORE."
        ],
        "neg": []
    },
    {
        "query": "In this system report, we describe the models and methods we used for our\nparticipation in the PLABA2023 task on biomedical abstract simplification, part\nof the TAC 2023 tracks. The system outputs we submitted come from the following\nthree categories: 1) domain fine-tuned T5-like models including Biomedical-T5\nand Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes\n(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work we\ncarried out for this task on BioGPT finetuning. In the official automatic\nevaluation using SARI scores, BeeManc ranks 2nd among all teams and our model\nLaySciFive ranks 3rd among all 13 evaluated systems. In the official human\nevaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score\n92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It\nalso produced a high score 91.57 on Fluency in comparison to the highest score\n93.53. In the second round of submissions, our team using ChatGPT-prompting\nranks the 2nd in several categories including simplified term accuracy score\n92.26 and completeness score 96.58, and a very similar score on faithfulness\nscore 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Our\ncodes, fine-tuned models, prompts, and data splits from the system development\nstage will be available at https://github.com/ HECTA-UoM/PLABA-MU",
        "pos": [
            "This study aims to explore the implementation of Natural Language Processing\n(NLP) and machine learning (ML) techniques to automate the coding of medical\nletters with visualised explainability and light-weighted local computer\nsettings. Currently in clinical settings, coding is a manual process that\ninvolves assigning codes to each condition, procedure, and medication in a\npatient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There\nare preliminary research on automatic coding in this field using\nstate-of-the-art ML models; however, due to the complexity and size of the\nmodels, the real-world deployment is not achieved. To further facilitate the\npossibility of automatic coding practice, we explore some solutions in a local\ncomputer setting; in addition, we explore the function of explainability for\ntransparency of AI models. We used the publicly available MIMIC-III database\nand the HAN/HLAN network models for ICD code prediction purposes. We also\nexperimented with the mapping between ICD and SNOMED CT knowledge bases. In our\nexperiments, the models provided useful information for 97.98\\% of codes. The\nresult of this investigation can shed some light on implementing automatic\nclinical coding in practice, such as in hospital settings, on the local\ncomputers used by clinicians , project page\n\\url{https://github.com/Glenj01/Medical-Coding}."
        ],
        "neg": []
    },
    {
        "query": "In this system report, we describe the models and methods we used for our\nparticipation in the PLABA2023 task on biomedical abstract simplification, part\nof the TAC 2023 tracks. The system outputs we submitted come from the following\nthree categories: 1) domain fine-tuned T5-like models including Biomedical-T5\nand Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes\n(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work we\ncarried out for this task on BioGPT finetuning. In the official automatic\nevaluation using SARI scores, BeeManc ranks 2nd among all teams and our model\nLaySciFive ranks 3rd among all 13 evaluated systems. In the official human\nevaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score\n92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It\nalso produced a high score 91.57 on Fluency in comparison to the highest score\n93.53. In the second round of submissions, our team using ChatGPT-prompting\nranks the 2nd in several categories including simplified term accuracy score\n92.26 and completeness score 96.58, and a very similar score on faithfulness\nscore 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Our\ncodes, fine-tuned models, prompts, and data splits from the system development\nstage will be available at https://github.com/ HECTA-UoM/PLABA-MU",
        "pos": [
            "This study aims to explore the implementation of Natural Language Processing\n(NLP) and machine learning (ML) techniques to automate the coding of medical\nletters with visualised explainability and light-weighted local computer\nsettings. Currently in clinical settings, coding is a manual process that\ninvolves assigning codes to each condition, procedure, and medication in a\npatient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There\nare preliminary research on automatic coding in this field using\nstate-of-the-art ML models; however, due to the complexity and size of the\nmodels, the real-world deployment is not achieved. To further facilitate the\npossibility of automatic coding practice, we explore some solutions in a local\ncomputer setting; in addition, we explore the function of explainability for\ntransparency of AI models. We used the publicly available MIMIC-III database\nand the HAN/HLAN network models for ICD code prediction purposes. We also\nexperimented with the mapping between ICD and SNOMED CT knowledge bases. In our\nexperiments, the models provided useful information for 97.98\\% of codes. The\nresult of this investigation can shed some light on implementing automatic\nclinical coding in practice, such as in hospital settings, on the local\ncomputers used by clinicians , project page\n\\url{https://github.com/Glenj01/Medical-Coding}."
        ],
        "neg": []
    },
    {
        "query": "Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL.",
        "pos": [
            "Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's\ngroundbreaking work. Although CLIP performs well, the typical direct latent\nfeature alignment lacks clarity in its representation and similarity scores. On\nthe other hand, lexical representation, a vector whose element represents the\nsimilarity between the sample and a word from the vocabulary, is a natural\nsparse representation and interpretable, providing exact matches for individual\nwords. However, lexical representations is difficult to learn due to no\nground-truth supervision and false-discovery issues, and thus requires complex\ndesign to train effectively. In this paper, we introduce LexVLA, a more\ninterpretable VLA framework by learning a unified lexical representation for\nboth modalities without complex design. We use DINOv2 as our visual model for\nits local-inclined features and Llama 2, a generative language model, to\nleverage its in-context lexical prediction ability. To avoid the false\ndiscovery, we propose an overuse penalty to refrain the lexical representation\nfrom falsely frequently activating meaningless words. We demonstrate that these\ntwo pre-trained uni-modal models can be well-aligned by fine-tuning on modest\nmulti-modal dataset and avoid intricate training configurations. On cross-modal\nretrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,\noutperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those\ntrained from scratch on even bigger datasets (e.g., 1.1B data, including\nCC-12M). We conduct extensive experiments to analyze LexVLA.",
            "The recent advancements in large language models (LLMs) with billions of\nparameters have significantly boosted their performance across various\nreal-world applications. However, the inference processes for these models\nrequire substantial energy and computational resources, presenting considerable\ndeployment challenges. In contrast, human brains, which contain approximately\n86 billion biological neurons, exhibit significantly greater energy efficiency\ncompared to LLMs with a similar number of parameters. Inspired by this, we\nredesign 7 to 70 billion parameter LLMs using bio-plausible spiking mechanisms,\nemulating the efficient behavior of the human brain. We propose the first\nspiking large language model as recent LLMs termed SpikeLLM. Coupled with the\nproposed model, a novel spike-driven quantization framework named Optimal Brain\nSpiking is introduced to reduce the energy cost and accelerate inference speed\nvia two essential approaches: first (second)-order differentiation-based\nsalient channel detection, and per-channel salient outlier expansion with\nGeneralized Integrate-and-Fire neurons. Our proposed spike-driven quantization\ncan plug in main streams of quantization training methods. In the OmniQuant\npipeline, SpikeLLM significantly reduces 25.51% WikiText2 perplexity and\nimproves 3.08% average accuracy of 6 zero-shot datasets on a LLAMA2-7B 4A4W\nmodel. In the GPTQ pipeline, SpikeLLM realizes a sparse ternary quantization,\nwhich achieves additive in all linear layers. Compared with PB-LLM with similar\noperations, SpikeLLM also exceeds significantly. We will release our code on\nGitHub."
        ],
        "neg": []
    },
    {
        "query": "Uncertainty quantification enables users to assess the reliability of\nresponses generated by large language models (LLMs). We present a novel\nQuestion Rephrasing technique to evaluate the input uncertainty of LLMs, which\nrefers to the uncertainty arising from equivalent variations of the inputs\nprovided to LLMs. This technique is integrated with sampling methods that\nmeasure the output uncertainty of LLMs, thereby offering a more comprehensive\nuncertainty assessment. We validated our approach on property prediction and\nreaction prediction for molecular chemistry tasks.",
        "pos": [
            "We present a comprehensive evaluation of proprietary and open-weights large\nlanguage models using the first astronomy-specific benchmarking dataset. This\ndataset comprises 4,425 multiple-choice questions curated from the Annual\nReview of Astronomy and Astrophysics, covering a broad range of astrophysical\ntopics. Our analysis examines model performance across various astronomical\nsubfields and assesses response calibration, crucial for potential deployment\nin research environments. Claude-3.5-Sonnet outperforms competitors by up to\n4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we\nobserved a universal reduction in cost every 3-to-12 months to achieve similar\nscore in this particular astronomy benchmark. Open-source models have rapidly\nimproved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with\nsome of the best proprietary models. We identify performance variations across\ntopics, with non-English-focused models generally struggling more in\nexoplanet-related fields, stellar astrophysics, and instrumentation related\nquestions. These challenges likely stem from less abundant training data,\nlimited historical context, and rapid recent developments in these areas. This\npattern is observed across both open-weights and proprietary models, with\nregional dependencies evident, highlighting the impact of training data\ndiversity on model performance in specialized scientific domains.\nTop-performing models demonstrate well-calibrated confidence, with correlations\nabove 0.9 between confidence and correctness, though they tend to be slightly\nunderconfident. The development for fast, low-cost inference of open-weights\nmodels presents new opportunities for affordable deployment in astronomy. The\nrapid progress observed suggests that LLM-driven research in astronomy may\nbecome feasible in the near future."
        ],
        "neg": []
    },
    {
        "query": "A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.",
        "pos": [
            "State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models."
        ],
        "neg": []
    },
    {
        "query": "A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.",
        "pos": [
            "Emotions are indispensable in human communication, but are often overlooked\nin task-oriented dialogue (ToD) modelling, where the task success is the\nprimary focus. While existing works have explored user emotions or similar\nconcepts in some ToD tasks, none has so far included emotion modelling into a\nfully-fledged ToD system nor conducted interaction with human or simulated\nusers. In this work, we incorporate emotion into the complete ToD processing\nloop, involving understanding, management, and generation. To this end, we\nextend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour\nlabels. Through interactive experimentation involving both simulated and human\nusers, we demonstrate that our proposed framework significantly enhances the\nuser's emotional experience as well as the task success.",
            "State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models."
        ],
        "neg": []
    },
    {
        "query": "A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.",
        "pos": [
            "Emotions are indispensable in human communication, but are often overlooked\nin task-oriented dialogue (ToD) modelling, where the task success is the\nprimary focus. While existing works have explored user emotions or similar\nconcepts in some ToD tasks, none has so far included emotion modelling into a\nfully-fledged ToD system nor conducted interaction with human or simulated\nusers. In this work, we incorporate emotion into the complete ToD processing\nloop, involving understanding, management, and generation. To this end, we\nextend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour\nlabels. Through interactive experimentation involving both simulated and human\nusers, we demonstrate that our proposed framework significantly enhances the\nuser's emotional experience as well as the task success.",
            "State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models."
        ],
        "neg": []
    },
    {
        "query": "A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.",
        "pos": [
            "Emotions are indispensable in human communication, but are often overlooked\nin task-oriented dialogue (ToD) modelling, where the task success is the\nprimary focus. While existing works have explored user emotions or similar\nconcepts in some ToD tasks, none has so far included emotion modelling into a\nfully-fledged ToD system nor conducted interaction with human or simulated\nusers. In this work, we incorporate emotion into the complete ToD processing\nloop, involving understanding, management, and generation. To this end, we\nextend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour\nlabels. Through interactive experimentation involving both simulated and human\nusers, we demonstrate that our proposed framework significantly enhances the\nuser's emotional experience as well as the task success.",
            "State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models."
        ],
        "neg": []
    },
    {
        "query": "A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.",
        "pos": [
            "Emotions are indispensable in human communication, but are often overlooked\nin task-oriented dialogue (ToD) modelling, where the task success is the\nprimary focus. While existing works have explored user emotions or similar\nconcepts in some ToD tasks, none has so far included emotion modelling into a\nfully-fledged ToD system nor conducted interaction with human or simulated\nusers. In this work, we incorporate emotion into the complete ToD processing\nloop, involving understanding, management, and generation. To this end, we\nextend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour\nlabels. Through interactive experimentation involving both simulated and human\nusers, we demonstrate that our proposed framework significantly enhances the\nuser's emotional experience as well as the task success."
        ],
        "neg": []
    },
    {
        "query": "A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.",
        "pos": [
            "Emotions are indispensable in human communication, but are often overlooked\nin task-oriented dialogue (ToD) modelling, where the task success is the\nprimary focus. While existing works have explored user emotions or similar\nconcepts in some ToD tasks, none has so far included emotion modelling into a\nfully-fledged ToD system nor conducted interaction with human or simulated\nusers. In this work, we incorporate emotion into the complete ToD processing\nloop, involving understanding, management, and generation. To this end, we\nextend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour\nlabels. Through interactive experimentation involving both simulated and human\nusers, we demonstrate that our proposed framework significantly enhances the\nuser's emotional experience as well as the task success."
        ],
        "neg": []
    },
    {
        "query": "A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.",
        "pos": [
            "Emotions are indispensable in human communication, but are often overlooked\nin task-oriented dialogue (ToD) modelling, where the task success is the\nprimary focus. While existing works have explored user emotions or similar\nconcepts in some ToD tasks, none has so far included emotion modelling into a\nfully-fledged ToD system nor conducted interaction with human or simulated\nusers. In this work, we incorporate emotion into the complete ToD processing\nloop, involving understanding, management, and generation. To this end, we\nextend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour\nlabels. Through interactive experimentation involving both simulated and human\nusers, we demonstrate that our proposed framework significantly enhances the\nuser's emotional experience as well as the task success.",
            "State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
        "pos": [
            "LLMs are known to be vulnerable to jailbreak attacks, even after safety\nalignment. An important observation is that, while different types of jailbreak\nattacks can generate significantly different queries, they mostly result in\nsimilar responses that are rooted in the same harmful knowledge (e.g., detailed\nsteps to make a bomb). Therefore, we conjecture that directly unlearn the\nharmful knowledge in the LLM can be a more effective way to defend against\njailbreak attacks than the mainstream supervised fine-tuning (SFT) based\napproaches. Our extensive experiments confirmed our insight and suggested\nsurprising generalizability of our unlearning-based approach: using only 20 raw\nharmful questions \\emph{without} any jailbreak prompt during training, our\nsolution reduced the Attack Success Rate (ASR) in Vicuna-7B on\n\\emph{out-of-distribution} (OOD) harmful questions wrapped with various complex\njailbreak prompts from 82.6\\% to 7.7\\%. This significantly outperforms\nLlama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment samples but\nstill has an ASR of 21.9\\% even under the help of an additional safety system\nprompt. Further analysis reveals that the generalization ability of our\nsolution stems from the intrinsic relatedness among harmful responses across\nharmful questions (e.g., response patterns, shared steps and actions, and\nsimilarity among their learned representations in the LLM). Our code is\navailable at \\url{https://github.com/thu-coai/SafeUnlearning}."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
        "pos": [
            "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween two multi-modal knowledge graphs (MMKGs), whose entities can be\nassociated with relational triples and related images. Most previous studies\ntreat the graph structure as a special modality, and fuse different modality\ninformation with separate uni-modal encoders, neglecting valuable relational\nassociations in modalities. Other studies refine each uni-modal information\nwith graph structures, but may introduce unnecessary relations in specific\nmodalities. To this end, we propose a novel local-to-global interaction network\nfor MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal\ninteractions to generate holistic entity semantics and then refine them with\nglobal relational interactions of entity neighbors. In this design, the\nuni-modal information is fused adaptively, and can be refined with relations\naccordingly. To enrich local interactions of multi-modal entity information, we\ndevice modality weights and low-rank interactive fusion, allowing diverse\nimpacts and element-level interactions among modalities. To capture global\ninteractions of graph structures, we adopt relation reflection graph attention\nnetworks, which fully capture relational associations between entities.\nExtensive experiments demonstrate superior results of our method over 5\ncross-KG or bilingual benchmark datasets, indicating the effectiveness of\ncapturing local and global interactions."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
        "pos": [
            "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween two multi-modal knowledge graphs (MMKGs), whose entities can be\nassociated with relational triples and related images. Most previous studies\ntreat the graph structure as a special modality, and fuse different modality\ninformation with separate uni-modal encoders, neglecting valuable relational\nassociations in modalities. Other studies refine each uni-modal information\nwith graph structures, but may introduce unnecessary relations in specific\nmodalities. To this end, we propose a novel local-to-global interaction network\nfor MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal\ninteractions to generate holistic entity semantics and then refine them with\nglobal relational interactions of entity neighbors. In this design, the\nuni-modal information is fused adaptively, and can be refined with relations\naccordingly. To enrich local interactions of multi-modal entity information, we\ndevice modality weights and low-rank interactive fusion, allowing diverse\nimpacts and element-level interactions among modalities. To capture global\ninteractions of graph structures, we adopt relation reflection graph attention\nnetworks, which fully capture relational associations between entities.\nExtensive experiments demonstrate superior results of our method over 5\ncross-KG or bilingual benchmark datasets, indicating the effectiveness of\ncapturing local and global interactions.",
            "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween multi-modal knowledge graphs (MMKGs), where the entities can be\nassociated with related images. Most existing studies integrate multi-modal\ninformation heavily relying on the automatically-learned fusion module, rarely\nsuppressing the redundant information for MMEA explicitly. To this end, we\nexplore variational information bottleneck for multi-modal entity alignment\n(IBMEA), which emphasizes the alignment-relevant information and suppresses the\nalignment-irrelevant information in generating entity representations.\nSpecifically, we devise multi-modal variational encoders to generate\nmodal-specific entity representations as probability distributions. Then, we\npropose four modal-specific information bottleneck regularizers, limiting the\nmisleading clues in refining modal-specific entity representations. Finally, we\npropose a modal-hybrid information contrastive regularizer to integrate all the\nrefined modal-specific representations, enhancing the entity similarity between\nMMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and\nthree bilingual MMEA datasets. Experimental results demonstrate that our model\nconsistently outperforms previous state-of-the-art methods, and also shows\npromising and robust performance in low-resource and high-noise data scenarios."
        ],
        "neg": []
    },
    {
        "query": "It is time-saving to build a reading assistant for customer service\nrepresentations (CSRs) when reading user manuals, especially information-rich\nones. Current solutions don't fit the online custom service scenarios well due\nto the lack of attention to user questions and possible responses. Hence, we\npropose to develop a time-saving and careful reading assistant for CSRs, named\nCARE. It can help the CSRs quickly find proper responses from the user manuals\nvia explicit clue chains. Specifically, each of the clue chains is formed by\ninferring over the user manuals, starting from the question clue aligned with\nthe user question and ending at a possible response. To overcome the shortage\nof supervised data, we adopt the self-supervised strategy for model learning.\nThe offline experiment shows that CARE is efficient in automatically inferring\naccurate responses from the user manual. The online experiment further\ndemonstrates the superiority of CARE to reduce CSRs' reading burden and keep\nhigh service quality, in particular with >35% decrease in time spent and\nkeeping a >0.75 ICC score.",
        "pos": [
            "Automatic extraction of procedural graphs from documents creates a low-cost\nway for users to easily understand a complex procedure by skimming visual\ngraphs. Despite the progress in recent studies, it remains unanswered: whether\nthe existing studies have well solved this task (Q1) and whether the emerging\nlarge language models (LLMs) can bring new opportunities to this task (Q2). To\nthis end, we propose a new benchmark PAGED, equipped with a large high-quality\ndataset and standard evaluations. It investigates five state-of-the-art\nbaselines, revealing that they fail to extract optimal procedural graphs well\nbecause of their heavy reliance on hand-written rules and limited available\ndata. We further involve three advanced LLMs in PAGED and enhance them with a\nnovel self-refine strategy. The results point out the advantages of LLMs in\nidentifying textual elements and their gaps in building logical structures. We\nhope PAGED can serve as a major landmark for automatic procedural graph\nextraction and the investigations in PAGED can offer insights into the research\non logic reasoning among non-sequential elements."
        ],
        "neg": []
    },
    {
        "query": "It is time-saving to build a reading assistant for customer service\nrepresentations (CSRs) when reading user manuals, especially information-rich\nones. Current solutions don't fit the online custom service scenarios well due\nto the lack of attention to user questions and possible responses. Hence, we\npropose to develop a time-saving and careful reading assistant for CSRs, named\nCARE. It can help the CSRs quickly find proper responses from the user manuals\nvia explicit clue chains. Specifically, each of the clue chains is formed by\ninferring over the user manuals, starting from the question clue aligned with\nthe user question and ending at a possible response. To overcome the shortage\nof supervised data, we adopt the self-supervised strategy for model learning.\nThe offline experiment shows that CARE is efficient in automatically inferring\naccurate responses from the user manual. The online experiment further\ndemonstrates the superiority of CARE to reduce CSRs' reading burden and keep\nhigh service quality, in particular with >35% decrease in time spent and\nkeeping a >0.75 ICC score.",
        "pos": [
            "Automatic extraction of procedural graphs from documents creates a low-cost\nway for users to easily understand a complex procedure by skimming visual\ngraphs. Despite the progress in recent studies, it remains unanswered: whether\nthe existing studies have well solved this task (Q1) and whether the emerging\nlarge language models (LLMs) can bring new opportunities to this task (Q2). To\nthis end, we propose a new benchmark PAGED, equipped with a large high-quality\ndataset and standard evaluations. It investigates five state-of-the-art\nbaselines, revealing that they fail to extract optimal procedural graphs well\nbecause of their heavy reliance on hand-written rules and limited available\ndata. We further involve three advanced LLMs in PAGED and enhance them with a\nnovel self-refine strategy. The results point out the advantages of LLMs in\nidentifying textual elements and their gaps in building logical structures. We\nhope PAGED can serve as a major landmark for automatic procedural graph\nextraction and the investigations in PAGED can offer insights into the research\non logic reasoning among non-sequential elements."
        ],
        "neg": []
    },
    {
        "query": "It is time-saving to build a reading assistant for customer service\nrepresentations (CSRs) when reading user manuals, especially information-rich\nones. Current solutions don't fit the online custom service scenarios well due\nto the lack of attention to user questions and possible responses. Hence, we\npropose to develop a time-saving and careful reading assistant for CSRs, named\nCARE. It can help the CSRs quickly find proper responses from the user manuals\nvia explicit clue chains. Specifically, each of the clue chains is formed by\ninferring over the user manuals, starting from the question clue aligned with\nthe user question and ending at a possible response. To overcome the shortage\nof supervised data, we adopt the self-supervised strategy for model learning.\nThe offline experiment shows that CARE is efficient in automatically inferring\naccurate responses from the user manual. The online experiment further\ndemonstrates the superiority of CARE to reduce CSRs' reading burden and keep\nhigh service quality, in particular with >35% decrease in time spent and\nkeeping a >0.75 ICC score.",
        "pos": [
            "Automatic extraction of procedural graphs from documents creates a low-cost\nway for users to easily understand a complex procedure by skimming visual\ngraphs. Despite the progress in recent studies, it remains unanswered: whether\nthe existing studies have well solved this task (Q1) and whether the emerging\nlarge language models (LLMs) can bring new opportunities to this task (Q2). To\nthis end, we propose a new benchmark PAGED, equipped with a large high-quality\ndataset and standard evaluations. It investigates five state-of-the-art\nbaselines, revealing that they fail to extract optimal procedural graphs well\nbecause of their heavy reliance on hand-written rules and limited available\ndata. We further involve three advanced LLMs in PAGED and enhance them with a\nnovel self-refine strategy. The results point out the advantages of LLMs in\nidentifying textual elements and their gaps in building logical structures. We\nhope PAGED can serve as a major landmark for automatic procedural graph\nextraction and the investigations in PAGED can offer insights into the research\non logic reasoning among non-sequential elements."
        ],
        "neg": []
    },
    {
        "query": "Traditional base station siting (BSS) methods rely heavily on drive testing\nand user feedback, which are laborious and require extensive expertise in\ncommunication, networking, and optimization. As large language models (LLMs)\nand their associated technologies advance, particularly in the realms of prompt\nengineering and agent engineering, network optimization will witness a\nrevolutionary approach. This approach entails the strategic use of well-crafted\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\nand the deployment of autonomous agents as a communication bridge to seamlessly\nconnect the machine language based LLMs with human users using natural\nlanguage. This integration represents the future paradigm of artificial\nintelligence (AI) as a service and AI for more ease. As a preliminary\nexploration, this research first develops a novel LLM-empowered BSS\noptimization framework, and heuristically proposes four different potential\nimplementations: the strategies based on Prompt-optimized LLM (PoL),\nhuman-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and\nCooperative multiple LLM-based autonomous BSS agents (CLaBa). Through\nevaluation on real-world data, the experiments demonstrate that prompt-assisted\nLLMs and LLM-based agents can generate more efficient, cost-effective, and\nreliable network deployments, noticeably enhancing the efficiency of BSS\noptimization and reducing trivial manual participation.",
        "pos": [
            "This paper introduces the FinSen dataset that revolutionizes financial market\nanalysis by integrating economic and financial news articles from 197 countries\nwith stock market data. The dataset's extensive coverage spans 15 years from\n2007 to 2023 with temporal information, offering a rich, global perspective\nwith 160,000 records on financial market news. Our study leverages causally\nvalidated sentiment scores and LSTM models to enhance market forecast accuracy\nand reliability. Utilizing the FinSen dataset, we introduce an innovative Focal\nCalibration Loss, reducing Expected Calibration Error (ECE) to 3.34 percent\nwith the DAN 3 model. This not only improves prediction accuracy but also\naligns probabilistic forecasts closely with real outcomes, crucial for the\nfinancial sector where predicted probability is paramount. Our approach\ndemonstrates the effectiveness of combining sentiment analysis with precise\ncalibration techniques for trustworthy financial forecasting where the cost of\nmisinterpretation can be high. Finsen Data can be found at [this github\nURL](https://github.com/EagleAdelaide/FinSen_Dataset.git)."
        ],
        "neg": []
    },
    {
        "query": "Traditional base station siting (BSS) methods rely heavily on drive testing\nand user feedback, which are laborious and require extensive expertise in\ncommunication, networking, and optimization. As large language models (LLMs)\nand their associated technologies advance, particularly in the realms of prompt\nengineering and agent engineering, network optimization will witness a\nrevolutionary approach. This approach entails the strategic use of well-crafted\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\nand the deployment of autonomous agents as a communication bridge to seamlessly\nconnect the machine language based LLMs with human users using natural\nlanguage. This integration represents the future paradigm of artificial\nintelligence (AI) as a service and AI for more ease. As a preliminary\nexploration, this research first develops a novel LLM-empowered BSS\noptimization framework, and heuristically proposes four different potential\nimplementations: the strategies based on Prompt-optimized LLM (PoL),\nhuman-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and\nCooperative multiple LLM-based autonomous BSS agents (CLaBa). Through\nevaluation on real-world data, the experiments demonstrate that prompt-assisted\nLLMs and LLM-based agents can generate more efficient, cost-effective, and\nreliable network deployments, noticeably enhancing the efficiency of BSS\noptimization and reducing trivial manual participation.",
        "pos": [
            "With the introduction of large language models (LLMs), automatic math\nreasoning has seen tremendous success. However, current methods primarily focus\non providing solutions or using techniques like Chain-of-Thought to enhance\nproblem-solving accuracy. In this paper, we focus on improving the capability\nof mathematics teaching via a Socratic teaching-based LLM\n(\\texttt{SocraticLLM}), which guides learners toward profound thinking with\nclarity and self-discovery via conversation. We collect and release a\nhigh-quality mathematical teaching dataset, named \\texttt{SocraticMATH}, which\nprovides Socratic-style conversations of problems with extra knowledge. Also,\nwe propose a knowledge-enhanced LLM as a strong baseline to generate reliable\nresponses with review, guidance/heuristic, rectification, and summarization.\nExperimental results show the great advantages of \\texttt{SocraticLLM} by\ncomparing it with several strong generative models. The codes and datasets are\navailable on \\url{https://github.com/ECNU-ICALK/SocraticMath}.",
            "Gender bias has been a focal point in the study of bias in machine\ntranslation and language models. Existing machine translation gender bias\nevaluations are primarily focused on male and female genders, limiting the\nscope of the evaluation. To assess gender bias accurately, these studies often\nrely on calculating the accuracy of gender pronouns or the masculine and\nfeminine attributes of grammatical gender via the stereotypes triggered by\noccupations or sentiment words ({\\em i.e.}, clear positive or negative\nattitude), which cannot extend to non-binary groups. This study presents a\nbenchmark AmbGIMT (Gender-Inclusive Machine Translation with Ambiguous attitude\nwords), which assesses gender bias beyond binary gender. Meanwhile, we propose\na novel process to evaluate gender bias based on the Emotional Attitude Score\n(EAS), which is used to quantify ambiguous attitude words. In evaluating three\nrecent and effective open-source LLMs and one powerful multilingual\ntranslation-specific model, our main observations are: (1) The translation\nperformance within non-binary gender contexts is markedly inferior in terms of\ntranslation quality and exhibits more negative attitudes than binary-gender\ncontexts. (2) The analysis experiments indicate that incorporating constraint\ncontext in prompts for gender identity terms can substantially reduce\ntranslation bias, while the bias remains evident despite the presence of the\nconstraints. The code is publicly available at\n\\url{https://github.com/pppa2019/ambGIMT}.",
            "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}.",
            "In-image machine translation (IIMT) aims to translate an image containing\ntexts in source language into an image containing translations in target\nlanguage. In this regard, conventional cascaded methods suffer from issues such\nas error propagation, massive parameters, and difficulties in deployment and\nretaining visual characteristics of the input image. Thus, constructing\nend-to-end models has become an option, which, however, faces two main\nchallenges: 1) the huge modeling burden, as it is required to simultaneously\nlearn alignment across languages and preserve the visual characteristics of the\ninput image; 2) the difficulties of directly predicting excessively lengthy\npixel sequences. In this paper, we propose \\textit{Translatotron-V(ision)}, an\nend-to-end IIMT model consisting of four modules. In addition to an image\nencoder, and an image decoder, our model contains a target text decoder and an\nimage tokenizer. Among them, the target text decoder is used to alleviate the\nlanguage alignment burden, and the image tokenizer converts long sequences of\npixels into shorter sequences of visual tokens, preventing the model from\nfocusing on low-level visual features. Besides, we present a two-stage training\nframework for our model to assist the model in learning alignment across\nmodalities and languages. Finally, we propose a location-aware evaluation\nmetric called Structure-BLEU to assess the translation quality of the generated\nimages. Experimental results demonstrate that our model achieves competitive\nperformance compared to cascaded models with only 70.9\\% of parameters, and\nsignificantly outperforms the pixel-level end-to-end IIMT model."
        ],
        "neg": []
    },
    {
        "query": "Background: The accuracy of spelling in Electronic Health Records (EHRs) is a\ncritical factor for efficient clinical care, research, and ensuring patient\nsafety. The Persian language, with its abundant vocabulary and complex\ncharacteristics, poses unique challenges for real-word error correction. This\nresearch aimed to develop an innovative approach for detecting and correcting\nspelling errors in Persian clinical text.\n  Methods: Our strategy employs a state-of-the-art pre-trained model that has\nbeen meticulously fine-tuned specifically for the task of spelling correction\nin the Persian clinical domain. This model is complemented by an innovative\northographic similarity matching algorithm, PERTO, which uses visual similarity\nof characters for ranking correction candidates.\n  Results: The evaluation of our approach demonstrated its robustness and\nprecision in detecting and rectifying word errors in Persian clinical text. In\nterms of non-word error correction, our model achieved an F1-Score of 90.0%\nwhen the PERTO algorithm was employed. For real-word error detection, our model\ndemonstrated its highest performance, achieving an F1-Score of 90.6%.\nFurthermore, the model reached its highest F1-Score of 91.5% for real-word\nerror correction when the PERTO algorithm was employed.\n  Conclusions: Despite certain limitations, our method represents a substantial\nadvancement in the field of spelling error detection and correction for Persian\nclinical text. By effectively addressing the unique challenges posed by the\nPersian language, our approach paves the way for more accurate and efficient\nclinical documentation, contributing to improved patient care and safety.\nFuture research could explore its use in other areas of the Persian medical\ndomain, enhancing its impact and utility.",
        "pos": [
            "Automatic spelling correction stands as a pivotal challenge within the ambit\nof natural language processing (NLP), demanding nuanced solutions. Traditional\nspelling correction techniques are typically only capable of detecting and\ncorrecting non-word errors, such as typos and misspellings. However,\ncontext-sensitive errors, also known as real-word errors, are more challenging\nto detect because they are valid words that are used incorrectly in a given\ncontext. The Persian language, characterized by its rich morphology and complex\nsyntax, presents formidable challenges to automatic spelling correction\nsystems. Furthermore, the limited availability of Persian language resources\nmakes it difficult to train effective spelling correction models. This paper\nintroduces a cutting-edge approach for precise and efficient real-word error\ncorrection in Persian text. Our methodology adopts a structured, multi-tiered\napproach, employing semantic analysis, feature selection, and advanced\nclassifiers to enhance error detection and correction efficacy. The innovative\narchitecture discovers and stores semantic similarities between words and\nphrases in Persian text. The classifiers accurately identify real-word errors,\nwhile the semantic ranking algorithm determines the most probable corrections\nfor real-word errors, taking into account specific spelling correction and\ncontext properties such as context, semantic similarity, and edit-distance\nmeasures. Evaluations have demonstrated that our proposed method surpasses\nprevious Persian real-word error correction models. Our method achieves an\nimpressive F-measure of 96.6% in the detection phase and an accuracy of 99.1%\nin the correction phase. These results clearly indicate that our approach is a\nhighly promising solution for automatic real-word error correction in Persian\ntext.",
            "This research introduces a state-of-the-art Persian spelling correction\nsystem that seamlessly integrates deep learning techniques with phonetic\nanalysis, significantly enhancing the accuracy and efficiency of natural\nlanguage processing (NLP) for Persian. Utilizing a fine-tuned language\nrepresentation model, our methodology effectively combines deep contextual\nanalysis with phonetic insights, adeptly correcting both non-word and real-word\nspelling errors. This strategy proves particularly effective in tackling the\nunique complexities of Persian spelling, including its elaborate morphology and\nthe challenge of homophony. A thorough evaluation on a wide-ranging dataset\nconfirms our system's superior performance compared to existing methods, with\nimpressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for\ncorrecting them. Additionally, the system demonstrates a strong capability in\nnon-word error correction, achieving an F1-Score of 0.891. These results\nillustrate the significant benefits of incorporating phonetic insights into\ndeep learning models for spelling correction. Our contributions not only\nadvance Persian language processing by providing a versatile solution for a\nvariety of NLP applications but also pave the way for future research in the\nfield, emphasizing the critical role of phonetic analysis in developing\neffective spelling correction system."
        ],
        "neg": []
    },
    {
        "query": "Despite the remarkable performance of Large Language Models (LLMs), they\nstill struggle with generating logically sound arguments, resulting in\npotential risks such as spreading misinformation. An important factor\ncontributing to LLMs' suboptimal performance in generating coherent arguments\nis their oversight of logical fallacies. To address this issue, we introduce\nFIPO, a fallacy-informed framework that leverages preference optimization\nmethods to steer LLMs toward logically sound arguments. FIPO includes a\nclassification loss, to capture the fine-grained information on fallacy\ncategories. Our results on argumentation datasets show that our method reduces\nthe fallacy errors by up to 17.5%. Furthermore, our human evaluation results\nindicate that the quality of the generated arguments by our method\nsignificantly outperforms the fine-tuned baselines, as well as prior preference\noptimization methods, such as DPO. These findings highlight the importance of\nensuring models are aware of logical fallacies for effective argument\ngeneration.",
        "pos": [
            "A rapidly growing number of applications rely on a small set of closed-source\nlanguage models (LMs). This dependency might introduce novel security risks if\nLMs develop self-recognition capabilities. Inspired by human identity\nverification methods, we propose a novel approach for assessing\nself-recognition in LMs using model-generated \"security questions\". Our test\ncan be externally administered to keep track of frontier models as it does not\nrequire access to internal model parameters or output probabilities. We use our\ntest to examine self-recognition in ten of the most capable open- and\nclosed-source LMs currently publicly available. Our extensive experiments found\nno empirical evidence of general or consistent self-recognition in any examined\nLM. Instead, our results suggest that given a set of alternatives, LMs seek to\npick the \"best\" answer, regardless of its origin. Moreover, we find indications\nthat preferences about which models produce the best answers are consistent\nacross LMs. We additionally uncover novel insights on position bias\nconsiderations for LMs in multiple-choice settings."
        ],
        "neg": []
    },
    {
        "query": "While high-performing language models are typically trained on hundreds of\nbillions of words, human children become fluent language users with a much\nsmaller amount of data. What are the features of the data they receive, and how\ndo these features support language modeling objectives? To investigate this\nquestion, we train GPT-2 models on 29M words of English-language child-directed\nspeech and a new matched, synthetic dataset (TinyDialogues), comparing to a\nheterogeneous blend of datasets from the BabyLM challenge. We evaluate both the\nsyntactic and semantic knowledge of these models using developmentally-inspired\nevaluations. Through pretraining experiments, we test whether the global\ndevelopmental ordering or the local discourse ordering of children's training\ndata support high performance relative to other datasets. The local properties\nof the data affect model results, but somewhat surprisingly, global properties\ndo not. Further, child language input is not uniquely valuable for training\nlanguage models. These findings support the hypothesis that, rather than\nproceeding from better data, children's learning is instead substantially more\nefficient than current language modeling techniques.",
        "pos": [
            "Language models (LMs) are increasingly used to simulate human-like responses\nin scenarios where accurately mimicking a population's behavior can guide\ndecision-making, such as in developing educational materials and designing\npublic policies. The objective of these simulations is for LMs to capture the\nvariations in human responses, rather than merely providing the expected\ncorrect answers. Prior work has shown that LMs often generate unrealistically\naccurate responses, but there are no established metrics to quantify how\nclosely the knowledge distribution of LMs aligns with that of humans. To\naddress this, we introduce \"psychometric alignment,\" a metric that measures the\nextent to which LMs reflect human knowledge distribution. Assessing this\nalignment involves collecting responses from both LMs and humans to the same\nset of test items and using Item Response Theory to analyze the differences in\nitem functioning between the groups. We demonstrate that our metric can capture\nimportant variations in populations that traditional metrics, like differences\nin accuracy, fail to capture. We apply this metric to assess existing LMs for\ntheir alignment with human knowledge distributions across three real-world\ndomains. We find significant misalignment between LMs and human populations,\nthough using persona-based prompts can improve alignment. Interestingly,\nsmaller LMs tend to achieve greater psychometric alignment than larger LMs.\nFurther, training LMs on human response data from the target distribution\nenhances their psychometric alignment on unseen test items, but the\neffectiveness of such training varies across domains."
        ],
        "neg": []
    },
    {
        "query": "As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.",
        "pos": [
            "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios."
        ],
        "neg": []
    },
    {
        "query": "As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.",
        "pos": [
            "Office automation significantly enhances human productivity by automatically\nfinishing routine tasks in the workflow. Beyond the basic information\nextraction studied in much of the prior document AI literature, the office\nautomation research should be extended to more realistic office tasks which\nrequire to integrate various information sources in the office system and\nproduce outputs through a series of decision-making processes. We introduce\nOfficeBench, one of the first office automation benchmarks for evaluating\ncurrent LLM agents' capability to address office tasks in realistic office\nworkflows. OfficeBench requires LLM agents to perform feasible long-horizon\nplanning, proficiently switch between applications in a timely manner, and\naccurately ground their actions within a large combined action space, based on\nthe contextual demands of the workflow. Applying our customized evaluation\nmethods on each task, we find that GPT-4 Omni achieves the highest pass rate of\n47.00%, demonstrating a decent performance in handling office tasks. However,\nthis is still far below the human performance and accuracy standards required\nby real-world office workflows. We further observe that most issues are related\nto operation redundancy and hallucinations, as well as limitations in switching\nbetween multiple applications, which may provide valuable insights for\ndeveloping effective agent frameworks for office automation.",
            "Retrieval augmented generation (RAG) combines the generative abilities of\nlarge language models (LLMs) with external knowledge sources to provide more\naccurate and up-to-date responses. Recent RAG advancements focus on improving\nretrieval outcomes through iterative LLM refinement or self-critique\ncapabilities acquired through additional instruction tuning of LLMs. In this\nwork, we introduce Speculative RAG - a framework that leverages a larger\ngeneralist LM to efficiently verify multiple RAG drafts produced in parallel by\na smaller, distilled specialist LM. Each draft is generated from a distinct\nsubset of retrieved documents, offering diverse perspectives on the evidence\nwhile reducing input token counts per draft. This approach enhances\ncomprehension of each subset and mitigates potential position bias over long\ncontext. Our method accelerates RAG by delegating drafting to the smaller\nspecialist LM, with the larger generalist LM performing a single verification\npass over the drafts. Extensive experiments demonstrate that Speculative RAG\nachieves state-of-the-art performance with reduced latency on TriviaQA,\nMuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy\nby up to 12.97% while reducing latency by 51% compared to conventional RAG\nsystems on PubHealth."
        ],
        "neg": []
    },
    {
        "query": "We explore visual prompt injection (VPI) that maliciously exploits the\nability of large vision-language models (LVLMs) to follow instructions drawn\nonto the input image. We propose a new VPI method, \"goal hijacking via visual\nprompt injection\" (GHVPI), that swaps the execution task of LVLMs from an\noriginal task to an alternative task designated by an attacker. The\nquantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and\ndemonstrates a notable attack success rate of 15.8%, which is an unignorable\nsecurity risk. Our analysis also shows that successful GHVPI requires high\ncharacter recognition capability and instruction-following ability in LVLMs.",
        "pos": [
            "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
        ],
        "neg": []
    },
    {
        "query": "We explore visual prompt injection (VPI) that maliciously exploits the\nability of large vision-language models (LVLMs) to follow instructions drawn\nonto the input image. We propose a new VPI method, \"goal hijacking via visual\nprompt injection\" (GHVPI), that swaps the execution task of LVLMs from an\noriginal task to an alternative task designated by an attacker. The\nquantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and\ndemonstrates a notable attack success rate of 15.8%, which is an unignorable\nsecurity risk. Our analysis also shows that successful GHVPI requires high\ncharacter recognition capability and instruction-following ability in LVLMs.",
        "pos": [
            "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
        ],
        "neg": []
    },
    {
        "query": "We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
        "pos": [
            "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) excel in various tasks but are primarily trained\non text data, limiting their application scope. Expanding LLM capabilities to\ninclude vision-language understanding is vital, yet training them on multimodal\ndata from scratch is challenging and costly. Existing instruction tuning\nmethods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs\nvia fully fine-tuning LLMs to bridge the modality gap. However, full\nfine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous\nknowledge, and high training costs particularly in the era of increasing tasks\nand modalities. To solve this issue, we introduce MoExtend, an effective\nframework designed to streamline the modality adaptation and extension of\nMixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts\ninto pre-trained MoE models, endowing them with novel knowledge without the\nneed to tune pretrained models such as MoE and vision encoders. This approach\nenables rapid adaptation and extension to new modal data or tasks, effectively\naddressing the challenge of accommodating new modalities within LLMs.\nFurthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk\nof catastrophic forgetting. Experimental results demonstrate the efficacy and\nefficiency of MoExtend in enhancing the multimodal capabilities of LLMs,\ncontributing to advancements in multimodal AI research. Code:\nhttps://github.com/zhongshsh/MoExtend.",
        "pos": [
            "Test-time interventions for language models can enhance factual accuracy,\nmitigate harmful outputs, and improve model efficiency without costly\nretraining. But despite a flood of new methods, different types of\ninterventions are largely developing independently. In practice, multiple\ninterventions must be applied sequentially to the same model, yet we lack\nstandardized ways to study how interventions interact. We fill this gap by\nintroducing composable interventions, a framework to study the effects of using\nmultiple interventions on the same language models, featuring new metrics and a\nunified codebase. Using our framework, we conduct extensive experiments and\ncompose popular methods from three emerging intervention categories --\nKnowledge Editing, Model Compression, and Machine Unlearning. Our results from\n310 different compositions uncover meaningful interactions: compression hinders\nediting and unlearning, composing interventions hinges on their order of\napplication, and popular general-purpose metrics are inadequate for assessing\ncomposability. Taken together, our findings showcase clear gaps in\ncomposability, suggesting a need for new multi-objective interventions. All of\nour code is public:\nhttps://github.com/hartvigsen-group/composable-interventions."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) excel in various tasks but are primarily trained\non text data, limiting their application scope. Expanding LLM capabilities to\ninclude vision-language understanding is vital, yet training them on multimodal\ndata from scratch is challenging and costly. Existing instruction tuning\nmethods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs\nvia fully fine-tuning LLMs to bridge the modality gap. However, full\nfine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous\nknowledge, and high training costs particularly in the era of increasing tasks\nand modalities. To solve this issue, we introduce MoExtend, an effective\nframework designed to streamline the modality adaptation and extension of\nMixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts\ninto pre-trained MoE models, endowing them with novel knowledge without the\nneed to tune pretrained models such as MoE and vision encoders. This approach\nenables rapid adaptation and extension to new modal data or tasks, effectively\naddressing the challenge of accommodating new modalities within LLMs.\nFurthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk\nof catastrophic forgetting. Experimental results demonstrate the efficacy and\nefficiency of MoExtend in enhancing the multimodal capabilities of LLMs,\ncontributing to advancements in multimodal AI research. Code:\nhttps://github.com/zhongshsh/MoExtend.",
        "pos": [
            "Test-time interventions for language models can enhance factual accuracy,\nmitigate harmful outputs, and improve model efficiency without costly\nretraining. But despite a flood of new methods, different types of\ninterventions are largely developing independently. In practice, multiple\ninterventions must be applied sequentially to the same model, yet we lack\nstandardized ways to study how interventions interact. We fill this gap by\nintroducing composable interventions, a framework to study the effects of using\nmultiple interventions on the same language models, featuring new metrics and a\nunified codebase. Using our framework, we conduct extensive experiments and\ncompose popular methods from three emerging intervention categories --\nKnowledge Editing, Model Compression, and Machine Unlearning. Our results from\n310 different compositions uncover meaningful interactions: compression hinders\nediting and unlearning, composing interventions hinges on their order of\napplication, and popular general-purpose metrics are inadequate for assessing\ncomposability. Taken together, our findings showcase clear gaps in\ncomposability, suggesting a need for new multi-objective interventions. All of\nour code is public:\nhttps://github.com/hartvigsen-group/composable-interventions."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) excel in various tasks but are primarily trained\non text data, limiting their application scope. Expanding LLM capabilities to\ninclude vision-language understanding is vital, yet training them on multimodal\ndata from scratch is challenging and costly. Existing instruction tuning\nmethods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs\nvia fully fine-tuning LLMs to bridge the modality gap. However, full\nfine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous\nknowledge, and high training costs particularly in the era of increasing tasks\nand modalities. To solve this issue, we introduce MoExtend, an effective\nframework designed to streamline the modality adaptation and extension of\nMixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts\ninto pre-trained MoE models, endowing them with novel knowledge without the\nneed to tune pretrained models such as MoE and vision encoders. This approach\nenables rapid adaptation and extension to new modal data or tasks, effectively\naddressing the challenge of accommodating new modalities within LLMs.\nFurthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk\nof catastrophic forgetting. Experimental results demonstrate the efficacy and\nefficiency of MoExtend in enhancing the multimodal capabilities of LLMs,\ncontributing to advancements in multimodal AI research. Code:\nhttps://github.com/zhongshsh/MoExtend.",
        "pos": [
            "GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity."
        ],
        "neg": []
    },
    {
        "query": "In this paper we demonstrate how logic programming systems and Automated\nfirst-order logic Theorem Provers (ATPs) can improve the accuracy of Large\nLanguage Models (LLMs) for logical reasoning tasks where the baseline\nperformance is given by direct LLM solutions. We first evaluate LLM reasoning\non steamroller problems using the PRONTOQA benchmark. We show how accuracy can\nbe improved with a neuro-symbolic architecture where the LLM acts solely as a\nfront-end for translating a given problem into a formal logic language and an\nautomated reasoning engine is called for solving it. However, this approach\ncritically hinges on the correctness of the LLM translation. To assess this\ntranslation correctness, we secondly define a framework of syntactic and\nsemantic error categories. We implemented the framework and used it to identify\nerrors that LLMs make in the benchmark domain. Based on these findings, we\nthirdly extended our method with capabilities for automatically correcting\nsyntactic and semantic errors. For semantic error correction we integrate\nfirst-order logic ATPs, which is our main and novel contribution. We\ndemonstrate that this approach reduces semantic errors significantly and\nfurther increases the accurracy of LLM logical reasoning.",
        "pos": [
            "This study presents the first examination of the ability of Large Language\nModels (LLMs) to follow reasoning strategies that are used to guide Automated\nTheorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and\nGoogle's recent Gemini model on problems from a steamroller domain. In addition\nto determining accuracy we make use of the Natural Language Processing library\nspaCy to explore new methods of investigating LLM's reasoning capabilities.\nThis led to one alarming result, the low correlation between correct reasoning\nand correct answers for any of the tested models. We found that the models'\nperformance when using the ATP reasoning strategies was comparable to one-shot\nchain of thought and observe that attention to uncertainty in the accuracy\nresults is critical when drawing conclusions about model performance.\nConsistent with previous speculation we confirm that LLMs have a preference\nfor, and are best able to follow, bottom up reasoning processes. However, the\nreasoning strategies can still be beneficial for deriving small and relevant\nsets of formulas for external processing by a trusted inference engine."
        ],
        "neg": []
    },
    {
        "query": "In this paper we demonstrate how logic programming systems and Automated\nfirst-order logic Theorem Provers (ATPs) can improve the accuracy of Large\nLanguage Models (LLMs) for logical reasoning tasks where the baseline\nperformance is given by direct LLM solutions. We first evaluate LLM reasoning\non steamroller problems using the PRONTOQA benchmark. We show how accuracy can\nbe improved with a neuro-symbolic architecture where the LLM acts solely as a\nfront-end for translating a given problem into a formal logic language and an\nautomated reasoning engine is called for solving it. However, this approach\ncritically hinges on the correctness of the LLM translation. To assess this\ntranslation correctness, we secondly define a framework of syntactic and\nsemantic error categories. We implemented the framework and used it to identify\nerrors that LLMs make in the benchmark domain. Based on these findings, we\nthirdly extended our method with capabilities for automatically correcting\nsyntactic and semantic errors. For semantic error correction we integrate\nfirst-order logic ATPs, which is our main and novel contribution. We\ndemonstrate that this approach reduces semantic errors significantly and\nfurther increases the accurracy of LLM logical reasoning.",
        "pos": [
            "This study presents the first examination of the ability of Large Language\nModels (LLMs) to follow reasoning strategies that are used to guide Automated\nTheorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and\nGoogle's recent Gemini model on problems from a steamroller domain. In addition\nto determining accuracy we make use of the Natural Language Processing library\nspaCy to explore new methods of investigating LLM's reasoning capabilities.\nThis led to one alarming result, the low correlation between correct reasoning\nand correct answers for any of the tested models. We found that the models'\nperformance when using the ATP reasoning strategies was comparable to one-shot\nchain of thought and observe that attention to uncertainty in the accuracy\nresults is critical when drawing conclusions about model performance.\nConsistent with previous speculation we confirm that LLMs have a preference\nfor, and are best able to follow, bottom up reasoning processes. However, the\nreasoning strategies can still be beneficial for deriving small and relevant\nsets of formulas for external processing by a trusted inference engine."
        ],
        "neg": []
    },
    {
        "query": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "pos": [
            "With the rise of large language models (LLMs), researchers are increasingly\nexploring their applications in var ious vertical domains, such as software\nengineering. LLMs have achieved remarkable success in areas including code\ngeneration and vulnerability detection. However, they also exhibit numerous\nlimitations and shortcomings. LLM-based agents, a novel tech nology with the\npotential for Artificial General Intelligence (AGI), combine LLMs as the core\nfor decision-making and action-taking, addressing some of the inherent\nlimitations of LLMs such as lack of autonomy and self-improvement. Despite\nnumerous studies and surveys exploring the possibility of using LLMs in\nsoftware engineering, it lacks a clear distinction between LLMs and LLM based\nagents. It is still in its early stage for a unified standard and benchmarking\nto qualify an LLM solution as an LLM-based agent in its domain. In this survey,\nwe broadly investigate the current practice and solutions for LLMs and\nLLM-based agents for software engineering. In particular we summarise six key\ntopics: requirement engineering, code generation, autonomous decision-making,\nsoftware design, test generation, and software maintenance. We review and\ndifferentiate the work of LLMs and LLM-based agents from these six topics,\nexamining their differences and similarities in tasks, benchmarks, and\nevaluation metrics. Finally, we discuss the models and benchmarks used,\nproviding a comprehensive analysis of their applications and effectiveness in\nsoftware engineering. We anticipate this work will shed some lights on pushing\nthe boundaries of LLM-based agents in software engineering for future research.",
            "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.",
            "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.",
            "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT"
        ],
        "neg": []
    },
    {
        "query": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "pos": [
            "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.",
            "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT"
        ],
        "neg": []
    },
    {
        "query": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "pos": [
            "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT"
        ],
        "neg": []
    },
    {
        "query": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "pos": [
            "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT"
        ],
        "neg": []
    },
    {
        "query": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "pos": [
            "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT",
            "Despite the substantial success of Information Retrieval (IR) in various NLP\ntasks, most IR systems predominantly handle queries and corpora in natural\nlanguage, neglecting the domain of code retrieval. Code retrieval is critically\nimportant yet remains under-explored, with existing methods and benchmarks\ninadequately representing the diversity of code in various domains and tasks.\nAddressing this gap, we present \\textbf{\\name} (\\textbf{Co}de\n\\textbf{I}nformation \\textbf{R}etrieval Benchmark), a robust and comprehensive\nbenchmark specifically designed to assess code retrieval capabilities. \\name\ncomprises \\textbf{ten} meticulously curated code datasets, spanning\n\\textbf{eight} distinctive retrieval tasks across \\textbf{seven} diverse\ndomains. We first discuss the construction of \\name and its diverse dataset\ncomposition. Further, we evaluate nine widely used retrieval models using\n\\name, uncovering significant difficulties in performing code retrieval tasks\neven with state-of-the-art systems. To facilitate easy adoption and integration\nwithin existing research workflows, \\name has been developed as a user-friendly\nPython framework, readily installable via pip. It shares same data schema as\nother popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark\nevaluations. Through \\name, we aim to invigorate research in the code retrieval\ndomain, providing a versatile benchmarking tool that encourages further\ndevelopment and exploration of code retrieval systems\\footnote{\\url{\nhttps://github.com/CoIR-team/coir}}."
        ],
        "neg": []
    },
    {
        "query": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "pos": [
            "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench."
        ],
        "neg": []
    },
    {
        "query": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "pos": [
            "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench."
        ],
        "neg": []
    },
    {
        "query": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "pos": [
            "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.",
            "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT"
        ],
        "neg": []
    },
    {
        "query": "There is a growing line of research on verifying the correctness of language\nmodels' outputs. At the same time, LMs are being used to tackle complex queries\nthat require reasoning. We introduce CoverBench, a challenging benchmark\nfocused on verifying LM outputs in complex reasoning settings. Datasets that\ncan be used for this purpose are often designed for other complex reasoning\ntasks (e.g., QA) targeting specific use-cases (e.g., financial tables),\nrequiring transformations, negative sampling and selection of hard examples to\ncollect such a benchmark. CoverBench provides a diversified evaluation for\ncomplex claim verification in a variety of domains, types of reasoning,\nrelatively long inputs, and a variety of standardizations, such as multiple\nrepresentations for tables where available, and a consistent schema. We\nmanually vet the data for quality to ensure low levels of label noise. Finally,\nwe report a variety of competitive baseline results to show CoverBench is\nchallenging and has very significant headroom. The data is available at\nhttps://huggingface.co/datasets/google/coverbench .",
        "pos": [
            "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community."
        ],
        "neg": []
    },
    {
        "query": "There is a growing line of research on verifying the correctness of language\nmodels' outputs. At the same time, LMs are being used to tackle complex queries\nthat require reasoning. We introduce CoverBench, a challenging benchmark\nfocused on verifying LM outputs in complex reasoning settings. Datasets that\ncan be used for this purpose are often designed for other complex reasoning\ntasks (e.g., QA) targeting specific use-cases (e.g., financial tables),\nrequiring transformations, negative sampling and selection of hard examples to\ncollect such a benchmark. CoverBench provides a diversified evaluation for\ncomplex claim verification in a variety of domains, types of reasoning,\nrelatively long inputs, and a variety of standardizations, such as multiple\nrepresentations for tables where available, and a consistent schema. We\nmanually vet the data for quality to ensure low levels of label noise. Finally,\nwe report a variety of competitive baseline results to show CoverBench is\nchallenging and has very significant headroom. The data is available at\nhttps://huggingface.co/datasets/google/coverbench .",
        "pos": [
            "Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.",
            "Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under uncertainty, and investigate the connection\nbetween them. We categorize fallback behaviors -- sequence repetitions,\ndegenerate text, and hallucinations -- and extensively analyze them in models\nfrom the same family that differ by the amount of pretraining tokens, parameter\ncount, or the inclusion of instruction-following training. Our experiments\nreveal a clear and consistent ordering of fallback behaviors, across all these\naxes: the more advanced an LLM is (i.e., trained on more tokens, has more\nparameters, or instruction-tuned), its fallback behavior shifts from sequence\nrepetitions, to degenerate text, and then to hallucinations. Moreover, the same\nordering is observed throughout a single generation, even for the\nbest-performing models; as uncertainty increases, models shift from generating\nhallucinations to producing degenerate text and then sequence repetitions.\nLastly, we demonstrate that while common decoding techniques, such as random\nsampling, might alleviate some unwanted behaviors like sequence repetitions,\nthey increase harder-to-detect hallucinations."
        ],
        "neg": []
    },
    {
        "query": "Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.",
        "pos": [
            "We propose a framework for analyzing discourse by combining two\ninterdependent concepts from sociolinguistic theory: face acts and politeness.\nWhile politeness has robust existing tools and data, face acts are less\nresourced. We introduce a new corpus created by annotating Wikipedia talk pages\nwith face acts and we use this to train a face act tagger. We then employ our\nframework to study how face and politeness interact with gender and power in\ndiscussions between Wikipedia editors. Among other findings, we observe that\nfemale Wikipedians are not only more polite, which is consistent with prior\nstudies, but that this difference corresponds with significantly more language\ndirected at humbling aspects of their own face. Interestingly, the distinction\nnearly vanishes once limiting to editors with administrative power."
        ],
        "neg": []
    },
    {
        "query": "Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.",
        "pos": [
            "We propose a framework for analyzing discourse by combining two\ninterdependent concepts from sociolinguistic theory: face acts and politeness.\nWhile politeness has robust existing tools and data, face acts are less\nresourced. We introduce a new corpus created by annotating Wikipedia talk pages\nwith face acts and we use this to train a face act tagger. We then employ our\nframework to study how face and politeness interact with gender and power in\ndiscussions between Wikipedia editors. Among other findings, we observe that\nfemale Wikipedians are not only more polite, which is consistent with prior\nstudies, but that this difference corresponds with significantly more language\ndirected at humbling aspects of their own face. Interestingly, the distinction\nnearly vanishes once limiting to editors with administrative power."
        ],
        "neg": []
    },
    {
        "query": "Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.",
        "pos": [
            "A central piece in enabling intelligent agentic behavior in foundation models\nis to make them capable of introspecting upon their behavior, reasoning, and\ncorrecting their mistakes as more computation or interaction is available. Even\nthe strongest proprietary large language models (LLMs) do not quite exhibit the\nability of continually improving their responses sequentially, even in\nscenarios where they are explicitly told that they are making a mistake. In\nthis paper, we develop RISE: Recursive IntroSpEction, an approach for\nfine-tuning LLMs to introduce this capability, despite prior work hypothesizing\nthat this capability may not be possible to attain. Our approach prescribes an\niterative fine-tuning procedure, which attempts to teach the model how to alter\nits response after having executed previously unsuccessful attempts to solve a\nhard test-time problem, with optionally additional environment feedback. RISE\nposes fine-tuning for a single-turn prompt as solving a multi-turn Markov\ndecision process (MDP), where the initial state is the prompt. Inspired by\nprinciples in online imitation learning and reinforcement learning, we propose\nstrategies for multi-turn data collection and training so as to imbue an LLM\nwith the capability to recursively detect and correct its previous mistakes in\nsubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,\nand Mistral models to improve themselves with more turns on math reasoning\ntasks, outperforming several single-turn strategies given an equal amount of\ninference-time computation. We also find that RISE scales well, often attaining\nlarger benefits with more capable models. Our analysis shows that RISE makes\nmeaningful improvements to responses to arrive at the correct solution for\nchallenging prompts, without disrupting one-turn abilities as a result of\nexpressing more complex distributions."
        ],
        "neg": []
    },
    {
        "query": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors in the\nintricate and realistic retrieval scenarios. To this end, we propose a\nKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving\ncontrollable knowledge selection in real retrieval scenarios. Concretely, we\nexplore and simulate error types across diverse context combinations and learn\nhow to avoid these negative signals through preference optimization methods.\nSimultaneously, by adjusting the balance between response length and the\nproportion of preference data representing different behavior patterns, we\nenhance the adherence capabilities and noise robustness of LLMs in a balanced\nmanner. Experimental results show that KaPO outperforms previous methods for\nhandling knowledge conflicts by over 37%, while also exhibiting robust\ngeneralization across various out-of-distribution datasets.",
        "pos": [
            "The use of Large Language Models (LLMs) in medicine is growing, but their\nability to handle both structured Electronic Health Record (EHR) data and\nunstructured clinical notes is not well-studied. This study benchmarks various\nmodels, including GPT-based LLMs, BERT-based models, and traditional clinical\npredictive models, for non-generative medical tasks utilizing renowned\ndatasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7\ntraditional predictive models using the MIMIC dataset (ICU patient records) and\nthe TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality\nand readmission prediction, disease hierarchy reconstruction, and biomedical\nsentence matching, comparing both zero-shot and finetuned performance. Results\nindicated that LLMs exhibited robust zero-shot predictive capabilities on\nstructured EHR data when using well-designed prompting strategies, frequently\nsurpassing traditional models. However, for unstructured medical texts, LLMs\ndid not outperform finetuned BERT models, which excelled in both supervised and\nunsupervised tasks. Consequently, while LLMs are effective for zero-shot\nlearning on structured data, finetuned BERT models are more suitable for\nunstructured texts, underscoring the importance of selecting models based on\nspecific task requirements and data characteristics to optimize the application\nof NLP technology in healthcare."
        ],
        "neg": []
    },
    {
        "query": "With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.",
        "pos": [
            "Tibet, ensconced within China's territorial expanse, is distinguished by its\nlabyrinthine and heterogeneous topography, a testament to its profound\nhistorical heritage, and the cradle of a unique religious ethos. The very\nessence of these attributes, however, has impeded the advancement of Tibet's\ntourism service infrastructure, rendering existing smart tourism services\ninadequate for the region's visitors. This study delves into the ramifications\nof informational disparities at tourist sites on Tibetan tourism and addresses\nthe challenge of establishing the Large Language Model (LLM) evaluation\ncriteria. It introduces an innovative approach, the DualGen Bridge AI system,\nemploying supervised fine-tuning techniques to bolster model functionality and\nenhance optimization processes. Furthermore, it pioneers a multi-structured\ngenerative results assessment framework. Empirical validation confirms the\nefficacy of this framework. The study also explores the application of the\nsupervised fine-tuning method within the proprietary DualGen Bridge AI, aimed\nat refining the generation of tourist site information. The study's findings\noffer valuable insights for optimizing system performance and provide support\nand inspiration for the application of LLM technology in Tibet's tourism\nservices and beyond, potentially revolutionizing the smart tourism industry\nwith advanced, tailored information generation capabilities."
        ],
        "neg": []
    },
    {
        "query": "We introduce LAMPO, a novel paradigm that leverages Large Language Models\n(LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike\nconventional methods, which concatenate all demonstration examples with the\ntest instance and prompt LLMs to produce the pointwise prediction, our\nframework uses the LLM as a preference machine that makes a relative\ncomparative decision between the test instance and each demonstration. A\nself-supervised method is then introduced to aggregate these binary comparisons\ninto the final ordinal decision. LAMPO addresses several limitations inherent\nin previous methods, including context length constraints, ordering biases, and\nchallenges associated with absolute point-wise estimation. Extensive\nexperiments on seven public datasets demonstrate LAMPO's remarkably competitive\nperformance across a diverse spectrum of applications (e.g., movie review\nanalysis and hate speech detection). Notably, in certain applications, the\nimprovement can be substantial, exceeding 20% in an absolute term. Moreover, we\nbelieve LAMPO represents an interesting addition to the non-parametric\napplication layered on top of LLMs, as it supports black-box LLMs without\nnecessitating the outputting of LLM's internal states (e.g., embeddings), as\nseen in previous approaches.",
        "pos": [
            "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. They are trained using preference datasets where each\nexample consists of one input prompt, two responses, and a preference label. As\ncurating a high-quality human labeled preference dataset is both time-consuming\nand expensive, people often rely on existing powerful LLMs for preference label\ngeneration. This can potentially introduce noise and impede RM training. In\nthis work, we present RMBoost, a novel synthetic preference data generation\nparadigm to boost reward model quality. Unlike traditional methods, which\ngenerate two responses before obtaining the preference label, RMBoost first\ngenerates one response and selects a preference label, followed by generating\nthe second more (or less) preferred response conditioned on the pre-selected\npreference label and the first response. This approach offers two main\nadvantages. First, RMBoost reduces labeling noise since preference pairs are\nconstructed intentionally. Second, RMBoost facilitates the creation of more\ndiverse responses by incorporating various quality aspects (e.g., helpfulness,\nrelevance, completeness) into the prompts. We conduct extensive experiments\nacross three diverse datasets and demonstrate that RMBoost outperforms other\nsynthetic preference data generation techniques and significantly boosts the\nperformance of four distinct reward models.",
            "The popularity of automated news headline generation has surged with\nadvancements in pre-trained language models. However, these models often suffer\nfrom the ``hallucination'' problem, where the generated headline is not fully\nsupported by its source article. Efforts to address this issue have\npredominantly focused on English, using over-simplistic classification schemes\nthat overlook nuanced hallucination types. In this study, we introduce the\nfirst multilingual, fine-grained news headline hallucination detection dataset\nthat contains over 11 thousand pairs in 5 languages, each annotated with\ndetailed hallucination types by experts. We conduct extensive experiments on\nthis dataset under two settings. First, we implement several supervised\nfine-tuning approaches as preparatory solutions and demonstrate this dataset's\nchallenges and utilities. Second, we test various large language models'\nin-context learning abilities and propose two novel techniques,\nlanguage-dependent demonstration selection and coarse-to-fine prompting, to\nboost the few-shot hallucination detection performance in terms of the\nexample-F1 metric. We release this dataset to foster further research in\nmultilingual, fine-grained headline hallucination detection."
        ],
        "neg": []
    },
    {
        "query": "We introduce LAMPO, a novel paradigm that leverages Large Language Models\n(LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike\nconventional methods, which concatenate all demonstration examples with the\ntest instance and prompt LLMs to produce the pointwise prediction, our\nframework uses the LLM as a preference machine that makes a relative\ncomparative decision between the test instance and each demonstration. A\nself-supervised method is then introduced to aggregate these binary comparisons\ninto the final ordinal decision. LAMPO addresses several limitations inherent\nin previous methods, including context length constraints, ordering biases, and\nchallenges associated with absolute point-wise estimation. Extensive\nexperiments on seven public datasets demonstrate LAMPO's remarkably competitive\nperformance across a diverse spectrum of applications (e.g., movie review\nanalysis and hate speech detection). Notably, in certain applications, the\nimprovement can be substantial, exceeding 20% in an absolute term. Moreover, we\nbelieve LAMPO represents an interesting addition to the non-parametric\napplication layered on top of LLMs, as it supports black-box LLMs without\nnecessitating the outputting of LLM's internal states (e.g., embeddings), as\nseen in previous approaches.",
        "pos": [
            "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. They are trained using preference datasets where each\nexample consists of one input prompt, two responses, and a preference label. As\ncurating a high-quality human labeled preference dataset is both time-consuming\nand expensive, people often rely on existing powerful LLMs for preference label\ngeneration. This can potentially introduce noise and impede RM training. In\nthis work, we present RMBoost, a novel synthetic preference data generation\nparadigm to boost reward model quality. Unlike traditional methods, which\ngenerate two responses before obtaining the preference label, RMBoost first\ngenerates one response and selects a preference label, followed by generating\nthe second more (or less) preferred response conditioned on the pre-selected\npreference label and the first response. This approach offers two main\nadvantages. First, RMBoost reduces labeling noise since preference pairs are\nconstructed intentionally. Second, RMBoost facilitates the creation of more\ndiverse responses by incorporating various quality aspects (e.g., helpfulness,\nrelevance, completeness) into the prompts. We conduct extensive experiments\nacross three diverse datasets and demonstrate that RMBoost outperforms other\nsynthetic preference data generation techniques and significantly boosts the\nperformance of four distinct reward models.",
            "The popularity of automated news headline generation has surged with\nadvancements in pre-trained language models. However, these models often suffer\nfrom the ``hallucination'' problem, where the generated headline is not fully\nsupported by its source article. Efforts to address this issue have\npredominantly focused on English, using over-simplistic classification schemes\nthat overlook nuanced hallucination types. In this study, we introduce the\nfirst multilingual, fine-grained news headline hallucination detection dataset\nthat contains over 11 thousand pairs in 5 languages, each annotated with\ndetailed hallucination types by experts. We conduct extensive experiments on\nthis dataset under two settings. First, we implement several supervised\nfine-tuning approaches as preparatory solutions and demonstrate this dataset's\nchallenges and utilities. Second, we test various large language models'\nin-context learning abilities and propose two novel techniques,\nlanguage-dependent demonstration selection and coarse-to-fine prompting, to\nboost the few-shot hallucination detection performance in terms of the\nexample-F1 metric. We release this dataset to foster further research in\nmultilingual, fine-grained headline hallucination detection."
        ],
        "neg": []
    },
    {
        "query": "We introduce LAMPO, a novel paradigm that leverages Large Language Models\n(LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike\nconventional methods, which concatenate all demonstration examples with the\ntest instance and prompt LLMs to produce the pointwise prediction, our\nframework uses the LLM as a preference machine that makes a relative\ncomparative decision between the test instance and each demonstration. A\nself-supervised method is then introduced to aggregate these binary comparisons\ninto the final ordinal decision. LAMPO addresses several limitations inherent\nin previous methods, including context length constraints, ordering biases, and\nchallenges associated with absolute point-wise estimation. Extensive\nexperiments on seven public datasets demonstrate LAMPO's remarkably competitive\nperformance across a diverse spectrum of applications (e.g., movie review\nanalysis and hate speech detection). Notably, in certain applications, the\nimprovement can be substantial, exceeding 20% in an absolute term. Moreover, we\nbelieve LAMPO represents an interesting addition to the non-parametric\napplication layered on top of LLMs, as it supports black-box LLMs without\nnecessitating the outputting of LLM's internal states (e.g., embeddings), as\nseen in previous approaches.",
        "pos": [
            "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. They are trained using preference datasets where each\nexample consists of one input prompt, two responses, and a preference label. As\ncurating a high-quality human labeled preference dataset is both time-consuming\nand expensive, people often rely on existing powerful LLMs for preference label\ngeneration. This can potentially introduce noise and impede RM training. In\nthis work, we present RMBoost, a novel synthetic preference data generation\nparadigm to boost reward model quality. Unlike traditional methods, which\ngenerate two responses before obtaining the preference label, RMBoost first\ngenerates one response and selects a preference label, followed by generating\nthe second more (or less) preferred response conditioned on the pre-selected\npreference label and the first response. This approach offers two main\nadvantages. First, RMBoost reduces labeling noise since preference pairs are\nconstructed intentionally. Second, RMBoost facilitates the creation of more\ndiverse responses by incorporating various quality aspects (e.g., helpfulness,\nrelevance, completeness) into the prompts. We conduct extensive experiments\nacross three diverse datasets and demonstrate that RMBoost outperforms other\nsynthetic preference data generation techniques and significantly boosts the\nperformance of four distinct reward models.",
            "The popularity of automated news headline generation has surged with\nadvancements in pre-trained language models. However, these models often suffer\nfrom the ``hallucination'' problem, where the generated headline is not fully\nsupported by its source article. Efforts to address this issue have\npredominantly focused on English, using over-simplistic classification schemes\nthat overlook nuanced hallucination types. In this study, we introduce the\nfirst multilingual, fine-grained news headline hallucination detection dataset\nthat contains over 11 thousand pairs in 5 languages, each annotated with\ndetailed hallucination types by experts. We conduct extensive experiments on\nthis dataset under two settings. First, we implement several supervised\nfine-tuning approaches as preparatory solutions and demonstrate this dataset's\nchallenges and utilities. Second, we test various large language models'\nin-context learning abilities and propose two novel techniques,\nlanguage-dependent demonstration selection and coarse-to-fine prompting, to\nboost the few-shot hallucination detection performance in terms of the\nexample-F1 metric. We release this dataset to foster further research in\nmultilingual, fine-grained headline hallucination detection."
        ],
        "neg": []
    },
    {
        "query": "The capability gap between open-source and closed-source large language\nmodels (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we\nintroduce a synthetic data approach that combines data produced by larger, more\npowerful models (strong models) with error information data generated by\nsmaller, not well-aligned models (weak models). The method not only enhances\nthe domain generalization of text-to-SQL models but also explores the potential\nof error data supervision through preference learning. Furthermore, we employ\nthe synthetic data approach for instruction tuning on open-source LLMs,\nresulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is\ndemonstrated through state-of-the-art results on the SPIDER and BIRD\nbenchmarks, bridging the performance gap between open-source models and methods\nprompted by closed-source models.",
        "pos": [
            "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenDevin, a platform for the development of powerful and\nflexible AI agents that interact with the world in similar ways to those of a\nhuman developer: by writing code, interacting with a command line, and browsing\nthe web. We describe how the platform allows for the implementation of new\nagents, safe interaction with sandboxed environments for code execution,\ncoordination between multiple agents, and incorporation of evaluation\nbenchmarks. Based on our currently incorporated benchmarks, we perform an\nevaluation of agents over 15 challenging tasks, including software engineering\n(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released\nunder the permissive MIT license, OpenDevin is a community project spanning\nacademia and industry with more than 1.3K contributions from over 160\ncontributors and will improve going forward.",
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "The capability gap between open-source and closed-source large language\nmodels (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we\nintroduce a synthetic data approach that combines data produced by larger, more\npowerful models (strong models) with error information data generated by\nsmaller, not well-aligned models (weak models). The method not only enhances\nthe domain generalization of text-to-SQL models but also explores the potential\nof error data supervision through preference learning. Furthermore, we employ\nthe synthetic data approach for instruction tuning on open-source LLMs,\nresulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is\ndemonstrated through state-of-the-art results on the SPIDER and BIRD\nbenchmarks, bridging the performance gap between open-source models and methods\nprompted by closed-source models.",
        "pos": [
            "Traditional legal retrieval systems designed to retrieve legal documents,\nstatutes, precedents, and other legal information are unable to give\nsatisfactory answers due to lack of semantic understanding of specific\nquestions. Large Language Models (LLMs) have achieved excellent results in a\nvariety of natural language processing tasks, which inspired us that we train a\nLLM in the legal domain to help legal retrieval. However, in the Chinese legal\ndomain, due to the complexity of legal questions and the rigour of legal\narticles, there is no legal large model with satisfactory practical application\nyet. In this paper, we present DeliLaw, a Chinese legal counselling system\nbased on a large language model. DeliLaw integrates a legal retrieval module\nand a case retrieval module to overcome the model hallucination. Users can\nconsult professional legal questions, search for legal articles and relevant\njudgement cases, etc. on the DeliLaw system in a dialogue mode. In addition,\nDeliLaw supports the use of English for counseling. we provide the address of\nthe system: https://data.delilegal.com/lawQuestion.",
            "The rapid progress in Large Language Models (LLMs) has prompted the creation\nof numerous benchmarks to evaluate their capabilities.This study focuses on the\nComprehensive Medical Benchmark in Chinese (CMB), showcasing how dataset\ndiversity and distribution in supervised fine-tuning (SFT) may enhance LLM\nperformance.Remarkably, We successfully trained a smaller base model to achieve\nscores comparable to larger models, indicating that a diverse and\nwell-distributed dataset can optimize performance regardless of model size.This\nstudy suggests that even smaller models may reach high performance levels with\ncarefully curated and varied datasets. By integrating a wide range of\ninstructional content, our approach addresses potential issues such as data\nquality inconsistencies. Our results imply that a broader spectrum of training\ndata may enhance a model's ability to generalize and perform effectively across\ndifferent medical scenarios, highlighting the importance of dataset quality and\ndiversity in fine-tuning processes. We open-source the model for future\nresearch at https://github.com/CAS-SIAT-XinHai/CollectiveSFT",
            "Empathetic response generation is designed to comprehend the emotions of\nothers and select the most appropriate strategies to assist them in resolving\nemotional challenges. Empathy can be categorized into cognitive empathy and\naffective empathy. The former pertains to the ability to understand and discern\nthe emotional issues and situations of others, while the latter involves the\ncapacity to provide comfort. To enhance one's empathetic abilities, it is\nessential to develop both these aspects. Therefore, we develop an innovative\nframework that combines retrieval augmentation and emotional support strategy\nintegration. Our framework starts with the introduction of a comprehensive\nemotional palette for empathy. We then apply appraisal theory to decompose this\npalette and create a database of empathetic responses. This database serves as\nan external resource and enhances the LLM's empathy by integrating semantic\nretrieval mechanisms. Moreover, our framework places a strong emphasis on the\nproper articulation of response strategies. By incorporating emotional support\nstrategies, we aim to enrich the model's capabilities in both cognitive and\naffective empathy, leading to a more nuanced and comprehensive empathetic\nresponse. Finally, we extract datasets ED and ET from the empathetic dialogue\ndataset \\textsc{EmpatheticDialogues} and ExTES based on dialogue length.\nExperiments demonstrate that our framework can enhance the empathy ability of\nLLMs from both cognitive and affective empathy perspectives. Our code is\nreleased at https://github.com/CAS-SIAT-XinHai/APTNESS."
        ],
        "neg": []
    },
    {
        "query": "The capability gap between open-source and closed-source large language\nmodels (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we\nintroduce a synthetic data approach that combines data produced by larger, more\npowerful models (strong models) with error information data generated by\nsmaller, not well-aligned models (weak models). The method not only enhances\nthe domain generalization of text-to-SQL models but also explores the potential\nof error data supervision through preference learning. Furthermore, we employ\nthe synthetic data approach for instruction tuning on open-source LLMs,\nresulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is\ndemonstrated through state-of-the-art results on the SPIDER and BIRD\nbenchmarks, bridging the performance gap between open-source models and methods\nprompted by closed-source models.",
        "pos": [
            "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenDevin, a platform for the development of powerful and\nflexible AI agents that interact with the world in similar ways to those of a\nhuman developer: by writing code, interacting with a command line, and browsing\nthe web. We describe how the platform allows for the implementation of new\nagents, safe interaction with sandboxed environments for code execution,\ncoordination between multiple agents, and incorporation of evaluation\nbenchmarks. Based on our currently incorporated benchmarks, we perform an\nevaluation of agents over 15 challenging tasks, including software engineering\n(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released\nunder the permissive MIT license, OpenDevin is a community project spanning\nacademia and industry with more than 1.3K contributions from over 160\ncontributors and will improve going forward.",
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "The capability gap between open-source and closed-source large language\nmodels (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we\nintroduce a synthetic data approach that combines data produced by larger, more\npowerful models (strong models) with error information data generated by\nsmaller, not well-aligned models (weak models). The method not only enhances\nthe domain generalization of text-to-SQL models but also explores the potential\nof error data supervision through preference learning. Furthermore, we employ\nthe synthetic data approach for instruction tuning on open-source LLMs,\nresulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is\ndemonstrated through state-of-the-art results on the SPIDER and BIRD\nbenchmarks, bridging the performance gap between open-source models and methods\nprompted by closed-source models.",
        "pos": [
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from\nliterature text has been challenging but crucial for the logical design of new\nMOFs with desirable functionality. The recent advent of large language models\n(LLMs) provides disruptively new solution to this long-standing problem and\nlatest researches have reported over 90% F1 in extracting correct conditions\nfrom MOFs literature. We argue in this paper that most existing synthesis\nextraction practices with LLMs stay with the primitive zero-shot learning,\nwhich could lead to downgraded extraction and application performance due to\nthe lack of specialized knowledge. This work pioneers and optimizes the\nfew-shot in-context learning paradigm for LLM extraction of material synthesis\nconditions. First, we propose a human-AI joint data curation process to secure\nhigh-quality ground-truth demonstrations for few-shot learning. Second, we\napply a BM25 algorithm based on the retrieval-augmented generation (RAG)\ntechnique to adaptively select few-shot demonstrations for each MOF's\nextraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the\nproposed few-shot method achieves much higher average F1 performance (0.93 vs.\n0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under\nfully automatic evaluation that are more objective than the previous human\nevaluation. The proposed method is further validated through real-world\nmaterial experiments: compared with the baseline zero-shot LLM, the proposed\nfew-shot approach increases the MOFs structural inference performance (R^2) by\n29.4% in average.",
        "pos": [
            "Detecting hallucinations in large language model (LLM) outputs is pivotal,\nyet traditional fine-tuning for this classification task is impeded by the\nexpensive and quickly outdated annotation process, especially across numerous\nvertical domains and in the face of rapid LLM advancements. In this study, we\nintroduce an approach that automatically generates both faithful and\nhallucinated outputs by rewriting system responses. Experimental findings\ndemonstrate that a T5-base model, fine-tuned on our generated dataset,\nsurpasses state-of-the-art zero-shot detectors and existing synthetic\ngeneration methods in both accuracy and latency, indicating efficacy of our\napproach.",
            "Sign language understanding has made significant strides; however, there is\nstill no viable solution for generating sign sequences directly from entire\nspoken content, e.g., text or speech. In this paper, we propose a unified\nframework for continuous sign language production, easing communication between\nsign and non-sign language users. In particular, a sequence diffusion model,\nutilizing embeddings extracted from text or speech, is crafted to generate sign\npredictions step by step. Moreover, by creating a joint embedding space for\ntext, audio, and sign, we bind these modalities and leverage the semantic\nconsistency among them to provide informative feedback for the model training.\nThis embedding-consistency learning strategy minimizes the reliance on sign\ntriplets and ensures continuous model refinement, even with a missing audio\nmodality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our\nmodel achieves competitive performance in sign language production."
        ],
        "neg": []
    },
    {
        "query": "The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from\nliterature text has been challenging but crucial for the logical design of new\nMOFs with desirable functionality. The recent advent of large language models\n(LLMs) provides disruptively new solution to this long-standing problem and\nlatest researches have reported over 90% F1 in extracting correct conditions\nfrom MOFs literature. We argue in this paper that most existing synthesis\nextraction practices with LLMs stay with the primitive zero-shot learning,\nwhich could lead to downgraded extraction and application performance due to\nthe lack of specialized knowledge. This work pioneers and optimizes the\nfew-shot in-context learning paradigm for LLM extraction of material synthesis\nconditions. First, we propose a human-AI joint data curation process to secure\nhigh-quality ground-truth demonstrations for few-shot learning. Second, we\napply a BM25 algorithm based on the retrieval-augmented generation (RAG)\ntechnique to adaptively select few-shot demonstrations for each MOF's\nextraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the\nproposed few-shot method achieves much higher average F1 performance (0.93 vs.\n0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under\nfully automatic evaluation that are more objective than the previous human\nevaluation. The proposed method is further validated through real-world\nmaterial experiments: compared with the baseline zero-shot LLM, the proposed\nfew-shot approach increases the MOFs structural inference performance (R^2) by\n29.4% in average.",
        "pos": [
            "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
        ],
        "neg": []
    },
    {
        "query": "The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from\nliterature text has been challenging but crucial for the logical design of new\nMOFs with desirable functionality. The recent advent of large language models\n(LLMs) provides disruptively new solution to this long-standing problem and\nlatest researches have reported over 90% F1 in extracting correct conditions\nfrom MOFs literature. We argue in this paper that most existing synthesis\nextraction practices with LLMs stay with the primitive zero-shot learning,\nwhich could lead to downgraded extraction and application performance due to\nthe lack of specialized knowledge. This work pioneers and optimizes the\nfew-shot in-context learning paradigm for LLM extraction of material synthesis\nconditions. First, we propose a human-AI joint data curation process to secure\nhigh-quality ground-truth demonstrations for few-shot learning. Second, we\napply a BM25 algorithm based on the retrieval-augmented generation (RAG)\ntechnique to adaptively select few-shot demonstrations for each MOF's\nextraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the\nproposed few-shot method achieves much higher average F1 performance (0.93 vs.\n0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under\nfully automatic evaluation that are more objective than the previous human\nevaluation. The proposed method is further validated through real-world\nmaterial experiments: compared with the baseline zero-shot LLM, the proposed\nfew-shot approach increases the MOFs structural inference performance (R^2) by\n29.4% in average.",
        "pos": [
            "Time series models, typically trained on numerical data, are designed to\nforecast future values. These models often rely on weighted averaging\ntechniques over time intervals. However, real-world time series data is seldom\nisolated and is frequently influenced by non-numeric factors. For instance,\nstock price fluctuations are impacted by daily random events in the broader\nworld, with each event exerting a unique influence on price signals.\nPreviously, forecasts in financial markets have been approached in two main\nways: either as time-series problems over price sequence or sentiment analysis\ntasks. The sentiment analysis tasks aim to determine whether news events will\nhave a positive or negative impact on stock prices, often categorizing them\ninto discrete labels. Recognizing the need for a more comprehensive approach to\naccurately model time series prediction, we propose a collaborative modeling\nframework that incorporates textual information about relevant events for\npredictions. Specifically, we leverage the intuition of large language models\nabout future changes to update real number time series predictions. We\nevaluated the effectiveness of our approach on financial market data."
        ],
        "neg": []
    },
    {
        "query": "Multi-Label Text Classification (MLTC) is a practical yet challenging task\nthat involves assigning multiple non-exclusive labels to each document.\nPrevious studies primarily focus on capturing label correlations to assist\nlabel prediction by introducing special labeling schemes, designing specific\nmodel structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor\n($k$NN) framework has shown promise by retrieving labeled samples as references\nto mine label co-occurrence information in the embedding space. However, two\ncritical biases, namely embedding alignment bias and confidence estimation\nbias, are often overlooked, adversely affecting prediction performance. In this\npaper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC,\nspecifically designed to mitigate these biases. To address embedding alignment\nbias, we propose a debiased contrastive learning strategy, enhancing neighbor\nconsistency on label co-occurrence. For confidence estimation bias, we present\na debiased confidence estimation strategy, improving the adaptive combination\nof predictions from $k$NN and inductive binary classifications. Extensive\nexperiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2,\nAmazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method.\nBesides, our method does not introduce any extra parameters.",
        "pos": [
            "Anomaly detection in computational workflows is critical for ensuring system\nreliability and security. However, traditional rule-based methods struggle to\ndetect novel anomalies. This paper leverages large language models (LLMs) for\nworkflow anomaly detection by exploiting their ability to learn complex data\npatterns. Two approaches are investigated: 1) supervised fine-tuning (SFT),\nwhere pre-trained LLMs are fine-tuned on labeled data for sentence\nclassification to identify anomalies, and 2) in-context learning (ICL) where\nprompts containing task descriptions and examples guide LLMs in few-shot\nanomaly detection without fine-tuning. The paper evaluates the performance,\nefficiency, generalization of SFT models, and explores zero-shot and few-shot\nICL prompts and interpretability enhancement via chain-of-thought prompting.\nExperiments across multiple workflow datasets demonstrate the promising\npotential of LLMs for effective anomaly detection in complex executions."
        ],
        "neg": []
    },
    {
        "query": "With the surge in digital content in low-resource languages, there is an\nescalating demand for advanced Natural Language Processing (NLP) techniques\ntailored to these languages. BERT (Bidirectional Encoder Representations from\nTransformers), serving as the foundational framework for numerous NLP\narchitectures and language models, is increasingly employed for the development\nof low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method\nfor fine-tuning Large Language Models (LLMs) and reducing the training\nparameters to some extent to decrease the computational costs needed for\ntraining the model and achieve results comparable to a fully fine-tuned model.\nIn this work, we present a study of PEFT methods for the Indic low-resource\nlanguage Marathi. We conduct a comprehensive analysis of PEFT methods applied\nto various monolingual and multilingual Marathi BERT models. These approaches\nare evaluated on prominent text classification datasets like MahaSent,\nMahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to\nsignificantly expedite the training speed of the models, addressing a critical\naspect of model development and deployment. In this study, we explore Low-Rank\nAdaptation of Large Language Models (LoRA) and adapter methods for low-resource\ntext classification. We show that these methods are competitive with full\nfine-tuning and can be used without loss in accuracy. This study contributes\nvaluable insights into the effectiveness of Marathi BERT models, offering a\nfoundation for the continued advancement of NLP capabilities in Marathi and\nsimilar Indic languages.",
        "pos": [
            "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub."
        ],
        "neg": []
    },
    {
        "query": "The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.",
        "pos": [
            "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Augmented Fake News Detection\n(DAFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DAFND first identifies the keywords of each news\narticle through a Detection Module. Subsequently, DAFND creatively designs an\nInvestigation Module to retrieve inside and outside valuable information\nconcerning to the current news, followed by another Judge Module to derive its\nrespective two prediction results. Finally, a Determination Module further\nintegrates these two predictions and derives the final result. Extensive\nexperiments on two publicly available datasets show the efficacy of our\nproposed method, particularly in low-resource settings."
        ],
        "neg": []
    },
    {
        "query": "The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.",
        "pos": [
            "The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping, while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce CompBench, a benchmark designed to evaluate the comparative reasoning\ncapability of multimodal large language models (MLLMs). CompBench mines and\npairs images through visually oriented questions covering eight dimensions of\nrelative comparison: visual attribute, existence, state, emotion, temporality,\nspatiality, quantity, and quality. We curate a collection of around 40K image\npairs using metadata from diverse vision datasets and CLIP similarity scores.\nThese image pairs span a broad array of visual domains, including animals,\nfashion, sports, and both outdoor and indoor scenes. The questions are\ncarefully crafted to discern relative characteristics between two images and\nare labeled by human annotators for accuracy and relevance. We use CompBench to\nevaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our\nresults reveal notable shortcomings in their comparative abilities. We believe\nCompBench not only sheds light on these limitations but also establishes a\nsolid foundation for future enhancements in the comparative capability of\nMLLMs.",
            "Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)\nthat utilizes limited training instances, appeals to more researchers in\nNatural Language Processing (NLP) due to its capability to extract textual\ninformation in extremely low-resource scenarios. The primary methodologies\nemployed for FSRE have been fine-tuning or prompt tuning techniques based on\nPre-trained Language Models (PLMs). Recently, the emergence of Large Language\nModels (LLMs) has prompted numerous researchers to explore FSRE through\nIn-Context Learning (ICL). However, there are substantial limitations\nassociated with methods based on either traditional RE models or LLMs.\nTraditional RE models are hampered by a lack of necessary prior knowledge,\nwhile LLMs fall short in their task-specific capabilities for RE. To address\nthese shortcomings, we propose a Dual-System Augmented Relation Extractor\n(DSARE), which synergistically combines traditional RE models with LLMs.\nSpecifically, DSARE innovatively injects the prior knowledge of LLMs into\ntraditional RE models, and conversely enhances LLMs' task-specific aptitude for\nRE through relation extraction augmentation. Moreover, an Integrated Prediction\nmodule is employed to jointly consider these two respective predictions and\nderive the final results. Extensive experiments demonstrate the efficacy of our\nproposed method.",
            "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Augmented Fake News Detection\n(DAFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DAFND first identifies the keywords of each news\narticle through a Detection Module. Subsequently, DAFND creatively designs an\nInvestigation Module to retrieve inside and outside valuable information\nconcerning to the current news, followed by another Judge Module to derive its\nrespective two prediction results. Finally, a Determination Module further\nintegrates these two predictions and derives the final result. Extensive\nexperiments on two publicly available datasets show the efficacy of our\nproposed method, particularly in low-resource settings."
        ],
        "neg": []
    },
    {
        "query": "The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.",
        "pos": [
            "Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)\nthat utilizes limited training instances, appeals to more researchers in\nNatural Language Processing (NLP) due to its capability to extract textual\ninformation in extremely low-resource scenarios. The primary methodologies\nemployed for FSRE have been fine-tuning or prompt tuning techniques based on\nPre-trained Language Models (PLMs). Recently, the emergence of Large Language\nModels (LLMs) has prompted numerous researchers to explore FSRE through\nIn-Context Learning (ICL). However, there are substantial limitations\nassociated with methods based on either traditional RE models or LLMs.\nTraditional RE models are hampered by a lack of necessary prior knowledge,\nwhile LLMs fall short in their task-specific capabilities for RE. To address\nthese shortcomings, we propose a Dual-System Augmented Relation Extractor\n(DSARE), which synergistically combines traditional RE models with LLMs.\nSpecifically, DSARE innovatively injects the prior knowledge of LLMs into\ntraditional RE models, and conversely enhances LLMs' task-specific aptitude for\nRE through relation extraction augmentation. Moreover, an Integrated Prediction\nmodule is employed to jointly consider these two respective predictions and\nderive the final results. Extensive experiments demonstrate the efficacy of our\nproposed method.",
            "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Augmented Fake News Detection\n(DAFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DAFND first identifies the keywords of each news\narticle through a Detection Module. Subsequently, DAFND creatively designs an\nInvestigation Module to retrieve inside and outside valuable information\nconcerning to the current news, followed by another Judge Module to derive its\nrespective two prediction results. Finally, a Determination Module further\nintegrates these two predictions and derives the final result. Extensive\nexperiments on two publicly available datasets show the efficacy of our\nproposed method, particularly in low-resource settings.",
            "The effectiveness of large language models (LLMs) is often hindered by\nduplicated data in their extensive pre-training datasets. Current approaches\nprimarily focus on detecting and removing duplicates, which risks the loss of\nvaluable information and neglects the varying degrees of duplication. To\naddress this, we propose a soft deduplication method that maintains dataset\nintegrity while selectively reducing the sampling weight of data with high\ncommonness. Central to our approach is the concept of \"data commonness\", a\nmetric we introduce to quantify the degree of duplication by measuring the\noccurrence probabilities of samples using an n-gram model. Empirical analysis\nshows that this method significantly improves training efficiency, achieving\ncomparable perplexity scores with at least a 26% reduction in required training\nsteps. Additionally, it enhances average few-shot downstream accuracy by 1.77%\nwhen trained for an equivalent duration. Importantly, this approach\nconsistently improves performance, even on rigorously deduplicated datasets,\nindicating its potential to complement existing methods and become a standard\npre-training process for LLMs."
        ],
        "neg": []
    },
    {
        "query": "The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.",
        "pos": [
            "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Augmented Fake News Detection\n(DAFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DAFND first identifies the keywords of each news\narticle through a Detection Module. Subsequently, DAFND creatively designs an\nInvestigation Module to retrieve inside and outside valuable information\nconcerning to the current news, followed by another Judge Module to derive its\nrespective two prediction results. Finally, a Determination Module further\nintegrates these two predictions and derives the final result. Extensive\nexperiments on two publicly available datasets show the efficacy of our\nproposed method, particularly in low-resource settings."
        ],
        "neg": []
    },
    {
        "query": "The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.",
        "pos": [
            "Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)\nthat utilizes limited training instances, appeals to more researchers in\nNatural Language Processing (NLP) due to its capability to extract textual\ninformation in extremely low-resource scenarios. The primary methodologies\nemployed for FSRE have been fine-tuning or prompt tuning techniques based on\nPre-trained Language Models (PLMs). Recently, the emergence of Large Language\nModels (LLMs) has prompted numerous researchers to explore FSRE through\nIn-Context Learning (ICL). However, there are substantial limitations\nassociated with methods based on either traditional RE models or LLMs.\nTraditional RE models are hampered by a lack of necessary prior knowledge,\nwhile LLMs fall short in their task-specific capabilities for RE. To address\nthese shortcomings, we propose a Dual-System Augmented Relation Extractor\n(DSARE), which synergistically combines traditional RE models with LLMs.\nSpecifically, DSARE innovatively injects the prior knowledge of LLMs into\ntraditional RE models, and conversely enhances LLMs' task-specific aptitude for\nRE through relation extraction augmentation. Moreover, an Integrated Prediction\nmodule is employed to jointly consider these two respective predictions and\nderive the final results. Extensive experiments demonstrate the efficacy of our\nproposed method.",
            "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Augmented Fake News Detection\n(DAFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DAFND first identifies the keywords of each news\narticle through a Detection Module. Subsequently, DAFND creatively designs an\nInvestigation Module to retrieve inside and outside valuable information\nconcerning to the current news, followed by another Judge Module to derive its\nrespective two prediction results. Finally, a Determination Module further\nintegrates these two predictions and derives the final result. Extensive\nexperiments on two publicly available datasets show the efficacy of our\nproposed method, particularly in low-resource settings."
        ],
        "neg": []
    },
    {
        "query": "The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.",
        "pos": [
            "Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)\nthat utilizes limited training instances, appeals to more researchers in\nNatural Language Processing (NLP) due to its capability to extract textual\ninformation in extremely low-resource scenarios. The primary methodologies\nemployed for FSRE have been fine-tuning or prompt tuning techniques based on\nPre-trained Language Models (PLMs). Recently, the emergence of Large Language\nModels (LLMs) has prompted numerous researchers to explore FSRE through\nIn-Context Learning (ICL). However, there are substantial limitations\nassociated with methods based on either traditional RE models or LLMs.\nTraditional RE models are hampered by a lack of necessary prior knowledge,\nwhile LLMs fall short in their task-specific capabilities for RE. To address\nthese shortcomings, we propose a Dual-System Augmented Relation Extractor\n(DSARE), which synergistically combines traditional RE models with LLMs.\nSpecifically, DSARE innovatively injects the prior knowledge of LLMs into\ntraditional RE models, and conversely enhances LLMs' task-specific aptitude for\nRE through relation extraction augmentation. Moreover, an Integrated Prediction\nmodule is employed to jointly consider these two respective predictions and\nderive the final results. Extensive experiments demonstrate the efficacy of our\nproposed method.",
            "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Augmented Fake News Detection\n(DAFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DAFND first identifies the keywords of each news\narticle through a Detection Module. Subsequently, DAFND creatively designs an\nInvestigation Module to retrieve inside and outside valuable information\nconcerning to the current news, followed by another Judge Module to derive its\nrespective two prediction results. Finally, a Determination Module further\nintegrates these two predictions and derives the final result. Extensive\nexperiments on two publicly available datasets show the efficacy of our\nproposed method, particularly in low-resource settings."
        ],
        "neg": []
    },
    {
        "query": "Large language models are ubiquitous in natural language processing because\nthey can adapt to new tasks without retraining. However, their sheer scale and\ncomplexity present unique challenges and opportunities, prompting researchers\nand practitioners to explore novel model training, optimization, and deployment\nmethods. This literature review focuses on various techniques for reducing\nresource requirements and compressing large language models, including\nquantization, pruning, knowledge distillation, and architectural optimizations.\nThe primary objective is to explore each method in-depth and highlight its\nunique challenges and practical applications. The discussed methods are\ncategorized into a taxonomy that presents an overview of the optimization\nlandscape and helps navigate it to understand the research trajectory better.",
        "pos": [
            "Speaker diarization answers the question \"who spoke when\" for an audio file.\nIn some diarization scenarios, low latency is required for transcription.\nSpeaker diarization with low latency is referred to as online speaker\ndiarization. The DIART pipeline is an online speaker diarization system. It\nconsists of a segmentation and an embedding model. The embedding model has the\nlargest share of the overall latency. The aim of this paper is to optimize the\ninference latency of the DIART pipeline. Different inference optimization\nmethods such as knowledge distilation, pruning, quantization and layer fusion\nare applied to the embedding model of the pipeline. It turns out that knowledge\ndistillation optimizes the latency, but has a negative effect on the accuracy.\nQuantization and layer fusion also have a positive influence on the latency\nwithout worsening the accuracy. Pruning, on the other hand, does not improve\nlatency.",
            "In this paper, different online speaker diarization systems are evaluated on\nthe same hardware with the same test data with regard to their latency. The\nlatency is the time span from audio input to the output of the corresponding\nspeaker label. As part of the evaluation, various model combinations within the\nDIART framework, a diarization system based on the online clustering algorithm\nUIS-RNN-SML, and the end-to-end online diarization system FS-EEND are compared.\nThe lowest latency is achieved for the DIART-pipeline with the embedding model\npyannote/embedding and the segmentation model pyannote/segmentation. The\nFS-EEND system shows a similarly good latency. In general there is currently no\npublished research that compares several online diarization systems in terms of\ntheir latency. This makes this work even more relevant."
        ],
        "neg": []
    },
    {
        "query": "As the NLP community increasingly addresses challenges associated with\nmultilingualism, robust annotation tools are essential to handle multilingual\ndatasets efficiently. In this paper, we introduce a code-mixed multilingual\ntext annotation framework, COMMENTATOR, specifically designed for annotating\ncode-mixed text. The tool demonstrates its effectiveness in token-level and\nsentence-level language annotation tasks for Hinglish text. We perform robust\nqualitative human-based evaluations to showcase COMMENTATOR led to 5x faster\nannotations than the best baseline. Our code is publicly available at\n\\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is\navailable at \\url{https://bit.ly/commentator_video}.",
        "pos": [
            "Global sustainable fund universe encompasses open-end funds and\nexchange-traded funds (ETF) that, by prospectus or other regulatory filings,\nclaim to focus on Environment, Social and Governance (ESG). Challengingly, the\nclaims can only be confirmed by examining the textual disclosures to check if\nthere is presence of intentionality and ESG focus on its investment strategy.\nCurrently, there is no regulation to enforce sustainability in ESG products\nspace. This paper proposes a unique method and system to classify and score the\nfund prospectuses in the sustainable universe regarding specificity and\ntransparency of language. We aim to employ few-shot learners to identify\nspecific, ambiguous, and generic sustainable investment-related language.\nAdditionally, we construct a ratio metric to determine language score and\nrating to rank products and quantify sustainability claims for US sustainable\nuniverse. As a by-product, we publish manually annotated quality training\ndataset on Hugging Face (ESG-Prospectus-Clarity-Category under cc-by-nc-sa-4.0)\nof more than 1K ESG textual statements. The performance of the few-shot\nfinetuning approach is compared with zero-shot models e.g., Llama-13B, GPT 3.5\nTurbo etc. We found that prompting large language models are not accurate for\ndomain specific tasks due to misalignment issues. The few-shot finetuning\ntechniques outperform zero-shot models by large margins of more than absolute\n~30% in precision, recall and F1 metrics on completely unseen ESG languages\n(test set). Overall, the paper attempts to establish a systematic and scalable\napproach to measure and rate sustainability intention quantitatively for\nsustainable funds using texts in prospectus. Regulatory bodies, investors, and\nadvisors may utilize the findings of this research to reduce cognitive load in\ninvestigating or screening of ESG funds which accurately reflects the ESG\nintention."
        ],
        "neg": []
    },
    {
        "query": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous\nLLMs into one with all the capabilities. Ideally, any LLMs sharing the same\nbackbone should be mergeable, irrespective of whether they are Fine-Tuned (FT)\nwith minor parameter changes or Pre-Trained (PT) with substantial parameter\nshifts. However, existing methods often manually assign the model importance,\nrendering them feasible only for LLMs with similar parameter alterations, such\nas multiple FT LLMs. The diverse parameter changed ranges between FT and PT\nLLMs pose challenges for current solutions in empirically determining the\noptimal combination. In this paper, we make a pioneering effort to broaden the\napplicability of merging techniques from FT to PT LLMs. We initially examine\nthe efficacy of current methods in merging FT and PT LLMs, discovering that\nthey struggle to deal with PT LLMs. Subsequently, we introduce an approach\nbased on WeIght DisENtanglement (WIDEN) to effectively extend the merging\nscope, which first disentangles model weights into magnitude and direction\ncomponents, and then performs adaptive fusion by considering their respective\ncontributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with\ninstruction-following skills) with Sailor (a PT LLM with multilingual\nabilities) across 7B and 14B model scales. Results reveal that: (1) existing\nsolutions usually fail when merging Sailor, either losing both abilities or\nonly retaining instruction-following skills; (2) WIDEN successfully injects the\nmultilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in\nSoutheast Asian languages, achieving enhancements in the fundamental\ncapabilities. In light of previous research, we also merge multiple 13B FT LLMs\nand observe that WIDEN achieves a balanced amalgamation of instruction\nfollowing, mathematical reasoning, and code generation skills.",
        "pos": [
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.",
        "pos": [
            "To improve the performance of large language models (LLMs), researchers have\nexplored providing LLMs with textual task-solving experience via prompts.\nHowever, they rely on manual efforts to acquire and apply such experience for\neach task, which is not feasible for the growing demand for LLMs and the\nvariety of user questions. To address this issue, we design a lifelong\nautonomous experiential learning framework based on LLMs to explore whether\nLLMs can imitate human ability for learning and utilizing experience. It\nautonomously learns and accumulates experience through experience transfer and\ninduction, categorizing the types of input questions to select which\naccumulated experience to employ for them. Experimental results on six widely\nused NLP datasets show that our framework performs reliably in each\nintermediate step and effectively improves the performance of GPT-3.5 and\nGPT-4. This validates the feasibility of using LLMs to mimic human experiential\nlearning and application capabilities. Additionally, we provide a detailed\nanalysis of the behavior of our framework at each step."
        ],
        "neg": []
    },
    {
        "query": "Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.",
        "pos": [
            "To improve the performance of large language models (LLMs), researchers have\nexplored providing LLMs with textual task-solving experience via prompts.\nHowever, they rely on manual efforts to acquire and apply such experience for\neach task, which is not feasible for the growing demand for LLMs and the\nvariety of user questions. To address this issue, we design a lifelong\nautonomous experiential learning framework based on LLMs to explore whether\nLLMs can imitate human ability for learning and utilizing experience. It\nautonomously learns and accumulates experience through experience transfer and\ninduction, categorizing the types of input questions to select which\naccumulated experience to employ for them. Experimental results on six widely\nused NLP datasets show that our framework performs reliably in each\nintermediate step and effectively improves the performance of GPT-3.5 and\nGPT-4. This validates the feasibility of using LLMs to mimic human experiential\nlearning and application capabilities. Additionally, we provide a detailed\nanalysis of the behavior of our framework at each step."
        ],
        "neg": []
    },
    {
        "query": "Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.",
        "pos": [
            "Through reading the documentation in the context, tool-using language models\ncan dynamically extend their capability using external tools. The cost is that\nwe have to input lengthy documentation every time the model needs to use the\ntool, occupying the input window as well as slowing down the decoding process.\n  Given the progress in general-purpose compression, soft context compression\nis a suitable approach to alleviate the problem. However, when compressing tool\ndocumentation, existing methods suffer from the weaknesses of key information\nloss (specifically, tool/parameter name errors) and difficulty in adjusting the\nlength of compressed sequences based on documentation lengths.\n  To address these problems, we propose two strategies for compressing tool\ndocumentation into concise and precise summary sequences for tool-using\nlanguage models. 1) Selective compression strategy mitigates key information\nloss by deliberately retaining key information as raw text tokens. 2) Block\ncompression strategy involves dividing tool documentation into short chunks and\nthen employing a fixed-length compression model to achieve variable-length\ncompression. This strategy facilitates the flexible adjustment of the\ncompression ratio.\n  Results on API-Bank and APIBench show that our approach reaches a performance\ncomparable to the upper-bound baseline under up to 16x compression ratio."
        ],
        "neg": []
    },
    {
        "query": "Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.",
        "pos": [
            "To improve the performance of large language models (LLMs), researchers have\nexplored providing LLMs with textual task-solving experience via prompts.\nHowever, they rely on manual efforts to acquire and apply such experience for\neach task, which is not feasible for the growing demand for LLMs and the\nvariety of user questions. To address this issue, we design a lifelong\nautonomous experiential learning framework based on LLMs to explore whether\nLLMs can imitate human ability for learning and utilizing experience. It\nautonomously learns and accumulates experience through experience transfer and\ninduction, categorizing the types of input questions to select which\naccumulated experience to employ for them. Experimental results on six widely\nused NLP datasets show that our framework performs reliably in each\nintermediate step and effectively improves the performance of GPT-3.5 and\nGPT-4. This validates the feasibility of using LLMs to mimic human experiential\nlearning and application capabilities. Additionally, we provide a detailed\nanalysis of the behavior of our framework at each step."
        ],
        "neg": []
    },
    {
        "query": "Code comments provide important information for understanding the source\ncode. They can help developers understand the overall purpose of a function or\nclass, as well as identify bugs and technical debt. However, an overabundance\nof comments is meaningless and counterproductive. As a result, it is critical\nto automatically filter out these comments for specific purposes. In this\npaper, we present Dopamin, a Transformer-based tool for dealing with this\nissue. Our model excels not only in presenting knowledge sharing of common\ncategories across multiple languages, but also in achieving robust performance\nin comment classification by improving comment representation. As a result, it\noutperforms the STACC baseline by 3% on the NLBSE'24 Tool Competition dataset\nin terms of average F1-score, while maintaining a comparable inference time for\npractical use. The source code is publicity available at\nhttps://github.com/FSoft-AI4Code/Dopamin.",
        "pos": [
            "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in Large Language Models (LLMs) have showcased their\nproficiency in answering natural language queries. However, their effectiveness\nis hindered by limited domain-specific knowledge, raising concerns about the\nreliability of their responses. We introduce a hybrid system that augments LLMs\nwith domain-specific knowledge graphs (KGs), thereby aiming to enhance factual\ncorrectness using a KG-based retrieval approach. We focus on a medical KG to\ndemonstrate our methodology, which includes (1) pre-processing, (2) Cypher\nquery generation, (3) Cypher query processing, (4) KG retrieval, and (5)\nLLM-enhanced response generation. We evaluate our system on a curated dataset\nof 69 samples, achieving a precision of 78\\% in retrieving correct KG nodes.\nOur findings indicate that the hybrid system surpasses a standalone LLM in\naccuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.\nThis positions the system as a promising tool for applications that demand\nfactual correctness and completeness, such as target identification -- a\ncritical process in pinpointing biological entities for disease treatment or\ncrop enhancement. Moreover, its intuitive search interface and ability to\nprovide accurate responses within seconds make it well-suited for\ntime-sensitive, precision-focused research contexts. We publish the source code\ntogether with the dataset and the prompt templates used.",
        "pos": [
            "While preliminary findings indicate that multilingual LLMs exhibit reduced\nbias compared to monolingual ones, a comprehensive understanding of the effect\nof multilingual training on bias mitigation, is lacking. This study addresses\nthis gap by systematically training six LLMs of identical size (2.6B\nparameters) and architecture: five monolingual models (English, German, French,\nItalian, and Spanish) and one multilingual model trained on an equal\ndistribution of data across these languages, all using publicly available data.\nTo ensure robust evaluation, standard bias benchmarks were automatically\ntranslated into the five target languages and verified for both translation\nquality and bias preservation by human annotators. Our results consistently\ndemonstrate that multilingual training effectively mitigates bias. Moreover, we\nobserve that multilingual models achieve not only lower bias but also superior\nprediction accuracy when compared to monolingual models with the same amount of\ntraining data, model architecture, and size."
        ],
        "neg": []
    },
    {
        "query": "Empathetic response generation, aiming at understanding the user's situation\nand feelings and respond empathically, is crucial in building human-like\ndialogue systems. Previous methods mainly focus on using maximum likelihood\nestimation as the optimization objective for training response generation\nmodels, without taking into account the empathy level alignment between\ngenerated responses and target responses. To this end, we propose an empathetic\nresponse generation using reinforcement learning (EmpRL) framework. The\nframework designs an effective empathy reward function and generates empathetic\nresponses by maximizing the expected reward through reinforcement learning.\nGiven the powerful text generation capability of pre-trained language models,\nEmpRL utilizes the pre-trained T5 model as the generator and conducts further\ntraining to initialize the policy. To align the empathy level between generated\nresponses and target responses in the context, an empathy reward function\ncontaining three empathy communication mechanisms, i.e., emotional reaction,\ninterpretation, and exploration, is constructed using pre-designed and\npre-trained empathy identifiers. Finally, the proximal policy optimization\nalgorithm is used to further train the policy to produce empathetic responses.\nBoth automatic and manual evaluations demonstrate that the proposed EmpRL\nframework can improve the quality of generated responses, enhance the empathy\nlevel similarity between generated and target responses, and produce empathetic\nresponses covering both affective and cognitive aspects.",
        "pos": [
            "Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising generation, evidence judgment, and refinement. We augment this\nprocess with real-world psychological case backgrounds extracted from online\nplatforms, enhancing the relevance and applicability of the generated data.\nAdditionally, to compare the performance of PsycoLLM with other LLMs, we\ndevelop a comprehensive psychological benchmark based on authoritative\npsychological counseling examinations in China, which includes assessments of\nprofessional ethics, theoretical proficiency, and case analysis. The\nexperimental results on the benchmark illustrates the effectiveness of\nPsycoLLM, which demonstrates superior performance compared to other LLMs."
        ],
        "neg": []
    },
    {
        "query": "Empathetic response generation, aiming at understanding the user's situation\nand feelings and respond empathically, is crucial in building human-like\ndialogue systems. Previous methods mainly focus on using maximum likelihood\nestimation as the optimization objective for training response generation\nmodels, without taking into account the empathy level alignment between\ngenerated responses and target responses. To this end, we propose an empathetic\nresponse generation using reinforcement learning (EmpRL) framework. The\nframework designs an effective empathy reward function and generates empathetic\nresponses by maximizing the expected reward through reinforcement learning.\nGiven the powerful text generation capability of pre-trained language models,\nEmpRL utilizes the pre-trained T5 model as the generator and conducts further\ntraining to initialize the policy. To align the empathy level between generated\nresponses and target responses in the context, an empathy reward function\ncontaining three empathy communication mechanisms, i.e., emotional reaction,\ninterpretation, and exploration, is constructed using pre-designed and\npre-trained empathy identifiers. Finally, the proximal policy optimization\nalgorithm is used to further train the policy to produce empathetic responses.\nBoth automatic and manual evaluations demonstrate that the proposed EmpRL\nframework can improve the quality of generated responses, enhance the empathy\nlevel similarity between generated and target responses, and produce empathetic\nresponses covering both affective and cognitive aspects.",
        "pos": [
            "Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising generation, evidence judgment, and refinement. We augment this\nprocess with real-world psychological case backgrounds extracted from online\nplatforms, enhancing the relevance and applicability of the generated data.\nAdditionally, to compare the performance of PsycoLLM with other LLMs, we\ndevelop a comprehensive psychological benchmark based on authoritative\npsychological counseling examinations in China, which includes assessments of\nprofessional ethics, theoretical proficiency, and case analysis. The\nexperimental results on the benchmark illustrates the effectiveness of\nPsycoLLM, which demonstrates superior performance compared to other LLMs."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have attracted considerable attention in various\nfields for their cost-effective solutions to diverse challenges, especially\nwith advancements in instruction tuning and quantization. E-commerce, with its\ncomplex tasks and extensive product-user interactions, presents a promising\napplication area for LLMs. However, the domain-specific concepts and knowledge\ninherent in e-commerce pose significant challenges for adapting general LLMs.\nTo address this issue, we developed EC-Guide\n\\href{https://github.com/fzp0424/EC-Guide-KDDUP-2024}, a comprehensive\ne-commerce guide for instruction tuning and quantization of LLMs. We also\nheuristically integrated Chain-of-Thought (CoT) during inference to enhance\narithmetic performance. Our approach achieved the 2nd place in Track 2 and 5th\nplace in Track 5 at the Amazon KDD Cup'24\n\\href{https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms}.\nAdditionally, our solution is model-agnostic, enabling effective scalability\nacross larger systems.",
        "pos": [
            "Evaluating the bias in Large Language Models (LLMs) becomes increasingly\ncrucial with their rapid development. However, existing evaluation methods rely\non fixed-form outputs and cannot adapt to the flexible open-text generation\nscenarios of LLMs (e.g., sentence completion and question answering). To\naddress this, we introduce BiasAlert, a plug-and-play tool designed to detect\nsocial bias in open-text generations of LLMs. BiasAlert integrates external\nhuman knowledge with inherent reasoning capabilities to detect bias reliably.\nExtensive experiments demonstrate that BiasAlert significantly outperforms\nexisting state-of-the-art methods like GPT4-as-A-Judge in detecting bias.\nFurthermore, through application studies, we demonstrate the utility of\nBiasAlert in reliable LLM bias evaluation and bias mitigation across various\nscenarios. Model and code will be publicly released."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) are fundamentally transforming human-facing\napplications in the health and well-being domains: boosting patient engagement,\naccelerating clinical decision-making, and facilitating medical education.\nAlthough state-of-the-art LLMs have shown superior performance in several\nconversational applications, evaluations within nutrition and diet applications\nare still insufficient. In this paper, we propose to employ the Registered\nDietitian (RD) exam to conduct a standard and comprehensive evaluation of\nstate-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing\nboth accuracy and consistency in nutrition queries. Our evaluation includes\n1050 RD exam questions encompassing several nutrition topics and proficiency\nlevels. In addition, for the first time, we examine the impact of Zero-Shot\n(ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC),\nand Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the\nresponses. Our findings revealed that while these LLMs obtained acceptable\noverall performance, their results varied considerably with different prompts\nand question domains. GPT-4o with CoT-SC prompting outperformed the other\napproaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.\nFor GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both\naccuracy and consistency. RAP was particularly effective for GPT-4o to answer\nExpert level questions. Consequently, choosing the appropriate LLM and\nprompting technique, tailored to the proficiency level and specific domain, can\nmitigate errors and potential risks in diet and nutrition chatbots.",
        "pos": [
            "Objective: This study aims to develop and validate an evaluation framework to\nensure the safety and reliability of mental health chatbots, which are\nincreasingly popular due to their accessibility, human-like interactions, and\ncontext-aware support. Materials and Methods: We created an evaluation\nframework with 100 benchmark questions and ideal responses, and five guideline\nquestions for chatbot responses. This framework, validated by mental health\nexperts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation\nmethods explored included large language model (LLM)-based scoring, an agentic\napproach using real-time data, and embedding models to compare chatbot\nresponses against ground truth standards. Results: The results highlight the\nimportance of guidelines and ground truth for improving LLM evaluation\naccuracy. The agentic method, dynamically accessing reliable information,\ndemonstrated the best alignment with human assessments. Adherence to a\nstandardized, expert-validated framework significantly enhanced chatbot\nresponse safety and reliability. Discussion: Our findings emphasize the need\nfor comprehensive, expert-tailored safety evaluation metrics for mental health\nchatbots. While LLMs have significant potential, careful implementation is\nnecessary to mitigate risks. The superior performance of the agentic approach\nunderscores the importance of real-time data access in enhancing chatbot\nreliability. Conclusion: The study validated an evaluation framework for mental\nhealth chatbots, proving its effectiveness in improving safety and reliability.\nFuture work should extend evaluations to accuracy, bias, empathy, and privacy\nto ensure holistic assessment and responsible integration into healthcare.\nStandardized evaluations will build trust among users and professionals,\nfacilitating broader adoption and improved mental health support through\ntechnology."
        ],
        "neg": []
    },
    {
        "query": "Self-supervised learning, such as with the wav2vec 2.0 framework\nsignificantly improves the accuracy of end-to-end automatic speech recognition\n(ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In\nthis work, we explored a self-supervised learning method for a multi-channel\nend-to-end ASR model based on the wav2vec 2.0 framework. As the multi-channel\nend-to-end ASR model, we focused on a multi-channel neural transducer. In\npre-training, we compared three different methods for feature quantization to\ntrain a multi-channel conformer audio encoder: joint quantization, feature-wise\nquantization and channel-wise quantization. In fine-tuning, we trained the\nmulti-channel conformer-transducer. All experiments were conducted using the\nfar-field in-house and CHiME-4 datasets. The results of the experiments showed\nthat feature-wise quantization was the most effective among the methods. We\nobserved a 66% relative reduction in character error rate compared with the\nmodel without any pre-training for the far-field in-house dataset.",
        "pos": [
            "We propose the intermediate direct preference optimization (DPO) method to\ncalculate the DPO loss at selected intermediate layers as an auxiliary loss for\nfinetuning large language models (LLMs). The conventional DPO method fine-tunes\na supervised fine-tuning (SFT) model by calculating the DPO loss using logits\nfrom the final layer. In our intermediate DPO approach, DPO losses are\ncalculated using the logits from K-selected intermediate layers and averaged to\nobtain the intermediate DPO loss. For training the intermediate DPO model, the\nfinal loss is obtained by calculating the weighted sum of the DPO and\nintermediate DPO losses. During inference, the intermediate DPO model decodes\nusing the final layer logits similarly to the conventional DPO model. In\nexperiments using the ultrafeedback dataset, the performance of the\nintermediate DPO model was evaluated using GPT-4. As a result, the intermediate\nDPO model trained using the intermediate DPO loss calculated at the 22nd layer\nof a 32-layer SFT model achieved win rates of 52.5% and 67.5% against the\nconventional DPO and SFT models, respectively, demonstrating the effectiveness\nof the proposed method. Furthermore, we report the relationships among the\nposition of the selected intermediate layers, the number of layers, and\nperformance."
        ],
        "neg": []
    },
    {
        "query": "Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.",
        "pos": [
            "Large language models (LLMs) have demonstrated notable potential in\nconducting complex tasks and are increasingly utilized in various financial\napplications. However, high-quality sequential financial investment\ndecision-making remains challenging. These tasks require multiple interactions\nwith a volatile environment for every decision, demanding sufficient\nintelligence to maximize returns and manage risks. Although LLMs have been used\nto develop agent systems that surpass human teams and yield impressive\ninvestment returns, opportunities to enhance multi-sourced information\nsynthesis and optimize decision-making outcomes through timely experience\nrefinement remain unexplored. Here, we introduce the FinCon, an LLM-based\nmulti-agent framework with CONceptual verbal reinforcement tailored for diverse\nFINancial tasks. Inspired by effective real-world investment firm\norganizational structures, FinCon utilizes a manager-analyst communication\nhierarchy. This structure allows for synchronized cross-functional agent\ncollaboration towards unified goals through natural language interactions and\nequips each agent with greater memory capacity than humans. Additionally, a\nrisk-control component in FinCon enhances decision quality by episodically\ninitiating a self-critiquing mechanism to update systematic investment beliefs.\nThe conceptualized beliefs serve as verbal reinforcement for the future agent's\nbehavior and can be selectively propagated to the appropriate node that\nrequires knowledge updates. This feature significantly improves performance\nwhile reducing unnecessary peer-to-peer communication costs. Moreover, FinCon\ndemonstrates strong generalization capabilities in various financial tasks,\nincluding single stock trading and portfolio management."
        ],
        "neg": []
    },
    {
        "query": "Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.",
        "pos": [
            "Large language models (LLMs) have demonstrated notable potential in\nconducting complex tasks and are increasingly utilized in various financial\napplications. However, high-quality sequential financial investment\ndecision-making remains challenging. These tasks require multiple interactions\nwith a volatile environment for every decision, demanding sufficient\nintelligence to maximize returns and manage risks. Although LLMs have been used\nto develop agent systems that surpass human teams and yield impressive\ninvestment returns, opportunities to enhance multi-sourced information\nsynthesis and optimize decision-making outcomes through timely experience\nrefinement remain unexplored. Here, we introduce the FinCon, an LLM-based\nmulti-agent framework with CONceptual verbal reinforcement tailored for diverse\nFINancial tasks. Inspired by effective real-world investment firm\norganizational structures, FinCon utilizes a manager-analyst communication\nhierarchy. This structure allows for synchronized cross-functional agent\ncollaboration towards unified goals through natural language interactions and\nequips each agent with greater memory capacity than humans. Additionally, a\nrisk-control component in FinCon enhances decision quality by episodically\ninitiating a self-critiquing mechanism to update systematic investment beliefs.\nThe conceptualized beliefs serve as verbal reinforcement for the future agent's\nbehavior and can be selectively propagated to the appropriate node that\nrequires knowledge updates. This feature significantly improves performance\nwhile reducing unnecessary peer-to-peer communication costs. Moreover, FinCon\ndemonstrates strong generalization capabilities in various financial tasks,\nincluding single stock trading and portfolio management."
        ],
        "neg": []
    },
    {
        "query": "Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.",
        "pos": [
            "In this paper we present the results of the AI-Debater 2023 Challenge held by\nthe Chinese Conference on Affect Computing (CCAC 2023), and introduce the\nrelated datasets. We organize two tracks to handle the argumentative generation\ntasks in different scenarios, namely, Counter-Argument Generation (Track 1) and\nClaim-based Argument Generation (Track 2). Each track is equipped with its\ndistinct dataset and baseline model respectively. In total, 32 competing teams\nregister for the challenge, from which we received 11 successful submissions.\nIn this paper, we will present the results of the challenge and a summary of\nthe systems, highlighting commonalities and innovations among participating\nsystems. Datasets and baseline models of the AI-Debater 2023 Challenge have\nbeen already released and can be accessed through the official website of the\nchallenge."
        ],
        "neg": []
    },
    {
        "query": "Model checklists (Ribeiro et al., 2020) have emerged as a useful tool for\nunderstanding the behavior of LLMs, analogous to unit-testing in software\nengineering. However, despite datasets being a key determinant of model\nbehavior, evaluating datasets, e.g., for the existence of annotation artifacts,\nis largely done ad hoc, once a problem in model behavior has already been found\ndownstream. In this work, we take a more principled approach to unit-testing\ndatasets by proposing a taxonomy based on the V-information literature. We call\na collection of such unit tests a data checklist. Using a checklist, not only\nare we able to recover known artifacts in well-known datasets such as SNLI, but\nwe also discover previously unknown artifacts in preference datasets for LLM\nalignment. Data checklists further enable a new kind of data filtering, which\nwe use to improve the efficacy and data efficiency of preference alignment.",
        "pos": [
            "The reconfiguration of human-LM interactions from simple sentence completions\nto complex, multi-domain, humanlike engagements necessitates new methodologies\nto understand how humans choose to rely on LMs. In our work, we contend that\nreliance is influenced by numerous factors within the interactional context of\na generation, a departure from prior work that used verbalized confidence\n(e.g., \"I'm certain the answer is...\") as the key determinant of reliance.\nHere, we introduce Rel-A.I., an in situ, system-level evaluation approach to\nmeasure human reliance on LM-generated epistemic markers (e.g., \"I think\nit's..\", \"Undoubtedly it's...\"). Using this methodology, we measure reliance\nrates in three emergent human-LM interaction settings: long-term interactions,\nanthropomorphic generations, and variable subject matter. Our findings reveal\nthat reliance is not solely based on verbalized confidence but is significantly\naffected by other features of the interaction context. Prior interactions,\nanthropomorphic cues, and subject domain all contribute to reliance\nvariability. An expression such as, \"I'm pretty sure it's...\", can vary up to\n20% in reliance frequency depending on its interactional context. Our work\nunderscores the importance of context in understanding human reliance and\noffers future designers and researchers with a methodology to conduct such\nmeasurements."
        ],
        "neg": []
    },
    {
        "query": "Retrieving external knowledge and prompting large language models with\nrelevant information is an effective paradigm to enhance the performance of\nquestion-answering tasks. Previous research typically handles paragraphs from\nexternal documents in isolation, resulting in a lack of context and ambiguous\nreferences, particularly in multi-document and complex tasks. To overcome these\nchallenges, we propose a new retrieval framework IIER, that leverages\nInter-chunk Interactions to Enhance Retrieval. This framework captures the\ninternal connections between document chunks by considering three types of\ninteractions: structural, keyword, and semantic. We then construct a unified\nChunk-Interaction Graph to represent all external documents comprehensively.\nAdditionally, we design a graph-based evidence chain retriever that utilizes\nprevious paths and chunk interactions to guide the retrieval process. It\nidentifies multiple seed nodes based on the target question and iteratively\nsearches for relevant chunks to gather supporting evidence. This retrieval\nprocess refines the context and reasoning chain, aiding the large language\nmodel in reasoning and answer generation. Extensive experiments demonstrate\nthat IIER outperforms strong baselines across four datasets, highlighting its\neffectiveness in improving retrieval and reasoning capabilities.",
        "pos": [
            "Large Language Model (LLMs) such as ChatGPT that exhibit generative AI\ncapabilities are facing accelerated adoption and innovation. The increased\npresence of Generative AI (GAI) inevitably raises concerns about the risks and\nsafety associated with these models. This article provides an up-to-date survey\nof recent trends in AI safety research of GAI-LLMs from a computer scientist's\nperspective: specific and technical. In this survey, we explore the background\nand motivation for the identified harms and risks in the context of LLMs being\ngenerative language models; our survey differentiates by emphasising the need\nfor unified theories of the distinct safety challenges in the research\ndevelopment and applications of LLMs. We start our discussion with a concise\nintroduction to the workings of LLMs, supported by relevant literature. Then we\ndiscuss earlier research that has pointed out the fundamental constraints of\ngenerative models, or lack of understanding thereof (e.g., performance and\nsafety trade-offs as LLMs scale in number of parameters). We provide a\nsufficient coverage of LLM alignment -- delving into various approaches,\ncontending methods and present challenges associated with aligning LLMs with\nhuman preferences. By highlighting the gaps in the literature and possible\nimplementation oversights, our aim is to create a comprehensive analysis that\nprovides insights for addressing AI safety in LLMs and encourages the\ndevelopment of aligned and secure models. We conclude our survey by discussing\nfuture directions of LLMs for AI safety, offering insights into ongoing\nresearch in this critical area."
        ],
        "neg": []
    },
    {
        "query": "Retrieving external knowledge and prompting large language models with\nrelevant information is an effective paradigm to enhance the performance of\nquestion-answering tasks. Previous research typically handles paragraphs from\nexternal documents in isolation, resulting in a lack of context and ambiguous\nreferences, particularly in multi-document and complex tasks. To overcome these\nchallenges, we propose a new retrieval framework IIER, that leverages\nInter-chunk Interactions to Enhance Retrieval. This framework captures the\ninternal connections between document chunks by considering three types of\ninteractions: structural, keyword, and semantic. We then construct a unified\nChunk-Interaction Graph to represent all external documents comprehensively.\nAdditionally, we design a graph-based evidence chain retriever that utilizes\nprevious paths and chunk interactions to guide the retrieval process. It\nidentifies multiple seed nodes based on the target question and iteratively\nsearches for relevant chunks to gather supporting evidence. This retrieval\nprocess refines the context and reasoning chain, aiding the large language\nmodel in reasoning and answer generation. Extensive experiments demonstrate\nthat IIER outperforms strong baselines across four datasets, highlighting its\neffectiveness in improving retrieval and reasoning capabilities.",
        "pos": [
            "Despite large language models (LLMs) have demonstrated impressive performance\nin various tasks, they are still suffering from the factual inconsistency\nproblem called hallucinations. For instance, LLMs occasionally generate content\nthat diverges from source article, and prefer to extract information that\nappears at the beginning and end of the context, especially in long document\nsummarization. Inspired by these findings, we propose to improve the\nfaithfulness of LLMs in summarization by impelling them to process the entire\narticle more fairly and faithfully. We present a novel summary generation\nstrategy, namely SliSum, which exploits the ideas of sliding windows and\nself-consistency. Specifically, SliSum divides the source article into\noverlapping windows, and utilizes LLM to generate local summaries for the\ncontent in the windows. Finally, SliSum aggregates all local summaries using\nclustering and majority voting algorithm to produce more faithful summary of\nentire article. Extensive experiments demonstrate that SliSum significantly\nimproves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and\nGPT-3.5 in both short and long text summarization, while maintaining their\nfluency and informativeness and without additional fine-tuning and resources.\nWe further conduct qualitative and quantitative studies to investigate why\nSliSum works and impacts of hyperparameters in SliSum on performance."
        ],
        "neg": []
    },
    {
        "query": "Enabling Large Language Models (LLMs) to generate citations in\nQuestion-Answering (QA) tasks is an emerging paradigm aimed at enhancing the\nverifiability of their responses when LLMs are utilizing external references to\ngenerate an answer. However, there is currently no unified framework to\nstandardize and fairly compare different citation generation methods, leading\nto difficulties in reproducing different methods and a comprehensive\nassessment. To cope with the problems above, we introduce \\name, an open-source\nand modular toolkit designed to facilitate the implementation and evaluation of\nexisting citation generation methods, while also fostering the development of\nnew approaches to improve citation quality in LLM outputs. This tool is highly\nextensible, allowing users to utilize 4 main modules and 14 components to\nconstruct a pipeline, evaluating an existing method or innovative designs. Our\nexperiments with two state-of-the-art LLMs and 11 citation generation baselines\ndemonstrate varying strengths of different modules in answer accuracy and\ncitation quality improvement, as well as the challenge of enhancing\ngranularity. Based on our analysis of the effectiveness of components, we\npropose a new method, self-RAG \\snippet, obtaining a balanced answer accuracy\nand citation quality. Citekit is released at\nhttps://github.com/SjJ1017/Citekit.",
        "pos": [
            "As Large Language Models (LLMs) are increasingly deployed to handle various\nnatural language processing (NLP) tasks, concerns regarding the potential\nnegative societal impacts of LLM-generated content have also arisen. To\nevaluate the biases exhibited by LLMs, researchers have recently proposed a\nvariety of datasets. However, existing bias evaluation efforts often focus on\nonly a particular type of bias and employ inconsistent evaluation metrics,\nleading to difficulties in comparison across different datasets and LLMs. To\naddress these limitations, we collect a variety of datasets designed for the\nbias evaluation of LLMs, and further propose CEB, a Compositional Evaluation\nBenchmark that covers different types of bias across different social groups\nand tasks. The curation of CEB is based on our newly proposed compositional\ntaxonomy, which characterizes each dataset from three dimensions: bias types,\nsocial groups, and tasks. By combining the three dimensions, we develop a\ncomprehensive evaluation strategy for the bias in LLMs. Our experiments\ndemonstrate that the levels of bias vary across these dimensions, thereby\nproviding guidance for the development of specific bias mitigation methods."
        ],
        "neg": []
    },
    {
        "query": "Stock embedding is a method for vector representation of stocks. There is a\ngrowing demand for vector representations of stock, i.e., stock embedding, in\nwealth management sectors, and the method has been applied to various tasks\nsuch as stock price prediction, portfolio optimization, and similar fund\nidentifications. Stock embeddings have the advantage of enabling the\nquantification of relative relationships between stocks, and they can extract\nuseful information from unstructured data such as text and network data. In\nthis study, we propose stock embedding enhanced with textual and network\ninformation (SETN) using a domain-adaptive pre-trained transformer-based model\nto embed textual information and a graph neural network model to grasp network\ninformation. We evaluate the performance of our proposed model on related\ncompany information extraction tasks. We also demonstrate that stock embeddings\nobtained from the proposed model perform better in creating thematic funds than\nthose obtained from baseline methods, providing a promising pathway for various\napplications in the wealth management industry.",
        "pos": [
            "What would happen if temperatures were subdued and result in a cool summer?\nOne can easily imagine that air conditioner, ice cream or beer sales would be\nsuppressed as a result of this. Less obvious is that agricultural shipments\nmight be delayed, or that sound proofing material sales might decrease. The\nability to extract such causal knowledge is important, but it is also important\nto distinguish between cause-effect pairs that are known and those that are\nlikely to be unknown, or rare. Therefore, in this paper, we propose a method\nfor extracting rare causal knowledge from Japanese financial statement\nsummaries produced by companies. Our method consists of three steps. First, it\nextracts sentences that include causal knowledge from the summaries using a\nmachine learning method based on an extended language ontology. Second, it\nobtains causal knowledge from the extracted sentences using syntactic patterns.\nFinally, it extracts the rarest causal knowledge from the knowledge it has\nobtained.",
            "In this study, we propose a methodology to extract, index, and visualize\n``climate change narratives'' (stories about the connection between causal and\nconsequential events related to climate change). We use two natural language\nprocessing methods, BERT (Bidirectional Encoder Representations from\nTransformers) and causal extraction, to textually analyze newspaper articles on\nclimate change to extract ``climate change narratives.'' The novelty of the\nmethodology could extract and quantify the causal relationships assumed by the\nnewspaper's writers. Looking at the extracted climate change narratives over\ntime, we find that since 2018, an increasing number of narratives suggest the\nimpact of the development of climate change policy discussion and the\nimplementation of climate change-related policies on corporate behaviors,\nmacroeconomics, and price dynamics. We also observed the recent emergence of\nnarratives focusing on the linkages between climate change-related policies and\nmonetary policy. Furthermore, there is a growing awareness of the negative\nimpacts of natural disasters (e.g., abnormal weather and severe floods) related\nto climate change on economic activities, and this issue might be perceived as\na new challenge for companies and governments. The methodology of this study is\nexpected to be applied to a wide range of fields, as it can analyze causal\nrelationships among various economic topics, including analysis of inflation\nexpectation or monetary policy communication strategy.",
            "In this paper, we attempt to summarize monthly reports as investment reports.\nFund managers have a wide range of tasks, one of which is the preparation of\ninvestment reports. In addition to preparing monthly reports on fund\nmanagement, fund managers prepare management reports that summarize these\nmonthly reports every six months or once a year. The preparation of fund\nreports is a labor-intensive and time-consuming task. Therefore, in this paper,\nwe tackle investment summarization from monthly reports using transformer-based\nmodels. There are two main types of summarization methods: extractive\nsummarization and abstractive summarization, and this study constructs both\nmethods and examines which is more useful in summarizing investment reports.",
            "Many natural language processing (NLP) tasks in English or general domains\nare widely available and are often used to evaluate pre-trained language\nmodels. In contrast, there are fewer tasks available for languages other than\nEnglish and for the financial domain. In particular, tasks in Japanese and the\nfinancial domain are limited. We construct two large datasets using materials\npublished by a Japanese central government agency. The datasets provide three\nJapanese financial NLP tasks, which include a 3-class and 12-class\nclassification for categorizing sentences, as well as a 5-class classification\ntask for sentiment analysis. Our datasets are designed to be comprehensive and\nup-to-date, leveraging an automatic update framework that ensures the latest\ntask datasets are publicly available anytime."
        ],
        "neg": []
    },
    {
        "query": "Stock embedding is a method for vector representation of stocks. There is a\ngrowing demand for vector representations of stock, i.e., stock embedding, in\nwealth management sectors, and the method has been applied to various tasks\nsuch as stock price prediction, portfolio optimization, and similar fund\nidentifications. Stock embeddings have the advantage of enabling the\nquantification of relative relationships between stocks, and they can extract\nuseful information from unstructured data such as text and network data. In\nthis study, we propose stock embedding enhanced with textual and network\ninformation (SETN) using a domain-adaptive pre-trained transformer-based model\nto embed textual information and a graph neural network model to grasp network\ninformation. We evaluate the performance of our proposed model on related\ncompany information extraction tasks. We also demonstrate that stock embeddings\nobtained from the proposed model perform better in creating thematic funds than\nthose obtained from baseline methods, providing a promising pathway for various\napplications in the wealth management industry.",
        "pos": [
            "What would happen if temperatures were subdued and result in a cool summer?\nOne can easily imagine that air conditioner, ice cream or beer sales would be\nsuppressed as a result of this. Less obvious is that agricultural shipments\nmight be delayed, or that sound proofing material sales might decrease. The\nability to extract such causal knowledge is important, but it is also important\nto distinguish between cause-effect pairs that are known and those that are\nlikely to be unknown, or rare. Therefore, in this paper, we propose a method\nfor extracting rare causal knowledge from Japanese financial statement\nsummaries produced by companies. Our method consists of three steps. First, it\nextracts sentences that include causal knowledge from the summaries using a\nmachine learning method based on an extended language ontology. Second, it\nobtains causal knowledge from the extracted sentences using syntactic patterns.\nFinally, it extracts the rarest causal knowledge from the knowledge it has\nobtained.",
            "In this paper, we attempt to summarize monthly reports as investment reports.\nFund managers have a wide range of tasks, one of which is the preparation of\ninvestment reports. In addition to preparing monthly reports on fund\nmanagement, fund managers prepare management reports that summarize these\nmonthly reports every six months or once a year. The preparation of fund\nreports is a labor-intensive and time-consuming task. Therefore, in this paper,\nwe tackle investment summarization from monthly reports using transformer-based\nmodels. There are two main types of summarization methods: extractive\nsummarization and abstractive summarization, and this study constructs both\nmethods and examines which is more useful in summarizing investment reports."
        ],
        "neg": []
    },
    {
        "query": "The need for improved diagnostic methods in ophthalmology is acute,\nespecially in the less developed regions with limited access to specialists and\nadvanced equipment. Therefore, we introduce VisionUnite, a novel\nvision-language foundation model for ophthalmology enhanced with clinical\nknowledge. VisionUnite has been pretrained on an extensive dataset comprising\n1.24 million image-text pairs, and further refined using our proposed MMFundus\ndataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate\nthat VisionUnite outperforms existing generative foundation models such as\nGPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable\nto junior ophthalmologists. VisionUnite performs well in various clinical\nscenarios including open-ended multi-disease diagnosis, clinical explanation,\nand patient interaction, making it a highly versatile tool for initial\nophthalmic disease screening. VisionUnite can also serve as an educational aid\nfor junior ophthalmologists, accelerating their acquisition of knowledge\nregarding both common and rare ophthalmic conditions. VisionUnite represents a\nsignificant advancement in ophthalmology, with broad implications for\ndiagnostics, medical education, and understanding of disease mechanisms.",
        "pos": [
            "In contemporary society, the issue of psychological health has become\nincreasingly prominent, characterized by the diversification, complexity, and\nuniversality of mental disorders. Cognitive Behavioral Therapy (CBT), currently\nthe most influential and clinically effective psychological treatment method\nwith no side effects, has limited coverage and poor quality in most countries.\nIn recent years, researches on the recognition and intervention of emotional\ndisorders using large language models (LLMs) have been validated, providing new\npossibilities for psychological assistance therapy. However, are LLMs truly\npossible to conduct cognitive behavioral therapy? Many concerns have been\nraised by mental health experts regarding the use of LLMs for therapy. Seeking\nto answer this question, we collected real CBT corpus from online video\nwebsites, designed and conducted a targeted automatic evaluation framework\ninvolving the evaluation of emotion tendency of generated text, structured\ndialogue pattern and proactive inquiry ability. For emotion tendency, we\ncalculate the emotion tendency score of the CBT dialogue text generated by each\nmodel. For structured dialogue pattern, we use a diverse range of automatic\nevaluation metrics to compare speaking style, the ability to maintain\nconsistency of topic and the use of technology in CBT between different models\n. As for inquiring to guide the patient, we utilize PQA (Proactive Questioning\nAbility) metric. We also evaluated the CBT ability of the LLM after integrating\na CBT knowledge base to explore the help of introducing additional knowledge to\nenhance the model's CBT counseling ability. Four LLM variants with excellent\nperformance on natural language processing are evaluated, and the experimental\nresult shows the great potential of LLMs in psychological counseling realm,\nespecially after combining with other technological means."
        ],
        "neg": []
    },
    {
        "query": "The need for improved diagnostic methods in ophthalmology is acute,\nespecially in the less developed regions with limited access to specialists and\nadvanced equipment. Therefore, we introduce VisionUnite, a novel\nvision-language foundation model for ophthalmology enhanced with clinical\nknowledge. VisionUnite has been pretrained on an extensive dataset comprising\n1.24 million image-text pairs, and further refined using our proposed MMFundus\ndataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate\nthat VisionUnite outperforms existing generative foundation models such as\nGPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable\nto junior ophthalmologists. VisionUnite performs well in various clinical\nscenarios including open-ended multi-disease diagnosis, clinical explanation,\nand patient interaction, making it a highly versatile tool for initial\nophthalmic disease screening. VisionUnite can also serve as an educational aid\nfor junior ophthalmologists, accelerating their acquisition of knowledge\nregarding both common and rare ophthalmic conditions. VisionUnite represents a\nsignificant advancement in ophthalmology, with broad implications for\ndiagnostics, medical education, and understanding of disease mechanisms.",
        "pos": [
            "Fine-grained sentiment analysis involves extracting and organizing sentiment\nelements from textual data. However, existing approaches often overlook issues\nof category semantic inclusion and overlap, as well as inherent structural\npatterns within the target sequence. This study introduces a generative\nsentiment analysis model. To address the challenges related to category\nsemantic inclusion and overlap, a latent category distribution variable is\nintroduced. By reconstructing the input of a variational autoencoder, the model\nlearns the intensity of the relationship between categories and text, thereby\nimproving sequence generation. Additionally, a trie data structure and\nconstrained decoding strategy are utilized to exploit structural patterns,\nwhich in turn reduces the search space and regularizes the generation process.\nExperimental results on the Restaurant-ACOS and Laptop-ACOS datasets\ndemonstrate a significant performance improvement compared to baseline models.\nAblation experiments further confirm the effectiveness of latent category\ndistribution and constrained decoding strategy."
        ],
        "neg": []
    },
    {
        "query": "The need for improved diagnostic methods in ophthalmology is acute,\nespecially in the less developed regions with limited access to specialists and\nadvanced equipment. Therefore, we introduce VisionUnite, a novel\nvision-language foundation model for ophthalmology enhanced with clinical\nknowledge. VisionUnite has been pretrained on an extensive dataset comprising\n1.24 million image-text pairs, and further refined using our proposed MMFundus\ndataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate\nthat VisionUnite outperforms existing generative foundation models such as\nGPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable\nto junior ophthalmologists. VisionUnite performs well in various clinical\nscenarios including open-ended multi-disease diagnosis, clinical explanation,\nand patient interaction, making it a highly versatile tool for initial\nophthalmic disease screening. VisionUnite can also serve as an educational aid\nfor junior ophthalmologists, accelerating their acquisition of knowledge\nregarding both common and rare ophthalmic conditions. VisionUnite represents a\nsignificant advancement in ophthalmology, with broad implications for\ndiagnostics, medical education, and understanding of disease mechanisms.",
        "pos": [
            "Personality psychologists have analyzed the relationship between personality\nand safety behaviors in human society. Although Large Language Models (LLMs)\ndemonstrate personality traits, the relationship between personality traits and\nsafety abilities in LLMs still remains a mystery. In this paper, we discover\nthat LLMs' personality traits are closely related to their safety abilities,\ni.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.\nMeanwhile, the safety alignment generally increases various LLMs' Extraversion,\nSensing, and Judging traits. According to such findings, we can edit LLMs'\npersonality traits and improve their safety performance, e.g., inducing\npersonality from ISTJ to ISTP resulted in a relative improvement of\napproximately 43% and 10% in privacy and fairness performance, respectively.\nAdditionally, we find that LLMs with different personality traits are\ndifferentially susceptible to jailbreak. This study pioneers the investigation\nof LLM safety from a personality perspective, providing new insights into LLM\nsafety enhancement.",
            "Large Language Models (LLMs) are increasingly integrated into diverse\nindustries, posing substantial security risks due to unauthorized replication\nand misuse. To mitigate these concerns, robust identification mechanisms are\nwidely acknowledged as an effective strategy. Identification systems for LLMs\nnow rely heavily on watermarking technology to manage and protect intellectual\nproperty and ensure data security. However, previous studies have primarily\nconcentrated on the basic principles of algorithms and lacked a comprehensive\nanalysis of watermarking theory and practice from the perspective of\nintelligent identification. To bridge this gap, firstly, we explore how a\nrobust identity recognition system can be effectively implemented and managed\nwithin LLMs by various participants using watermarking technology. Secondly, we\npropose a mathematical framework based on mutual information theory, which\nsystematizes the identification process to achieve more precise and customized\nwatermarking. Additionally, we present a comprehensive evaluation of\nperformance metrics for LLM watermarking, reflecting participant preferences\nand advancing discussions on its identification applications. Lastly, we\noutline the existing challenges in current watermarking technologies and\ntheoretical frameworks, and provide directional guidance to address these\nchallenges. Our systematic classification and detailed exposition aim to\nenhance the comparison and evaluation of various methods, fostering further\nresearch and development toward a transparent, secure, and equitable LLM\necosystem.",
            "Large language models (LLMs) are integral to modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it demands substantial training\nresources to optimize model weights and quantization parameters. To address\nthis, we propose Efficient Quantization-Aware Training (EfficientQAT), a novel\nquantization technique for compressing LLMs. EfficientQAT involves two\nconsecutive phases: Block-wise training of all parameters (Block-AP) and\nend-to-end training of quantization parameters (E2E-QP). Block-AP sequentially\nconducts quantization-aware training for all parameters in each transformer\nblock with block-wise reconstruction, maintaining efficiency by avoiding\ntraining the entire LLM. Initialized with quantized model, E2E-QP then trains\nonly quantization parameters (step sizes) end-to-end, enhancing efficiency with\na fixed quantized backbone and reduced trainable parameter count. Extensive\nexperiments demonstrate that EfficientQAT outperforms previous quantization\nmethods across a range of models, including base LLMs, instruction-tuned LLMs,\nand multimodal LLMs, with scales from 7B to 70B parameters at various\nquantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model\non a single A100-80GB GPU in 41 hours, with less than 3\\% accuracy degradation\ncompared to the full precision (69.48 vs. 72.41). Notably, this INT2 quantized\n70B model obtains a 1.67 accuracy gain over the Llama-2-13B model (69.48 vs.\n67.81) while requiring less memory (19.2GB vs. 24.2GB). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.",
            "Large Language Models~(LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting\nextensive multilingual continual pre-training on the LLaMA series models,\nenabling translation support across more than 100 languages. Through a\ncomprehensive analysis of training strategies, such as vocabulary expansion and\ndata augmentation, we develop LLaMAX. Remarkably, without sacrificing its\ngeneralization ability, LLaMAX achieves significantly higher translation\nperformance compared to existing open-source LLMs~(by more than 10 spBLEU\npoints) and performs on-par with specialized translation model~(M2M-100-12B) on\nthe Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve\nas a robust multilingual foundation model. The\ncode~\\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and\nmodels~\\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available.",
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have been applied to a wide range of tasks,\nincluding text summarization, web navigation, and chatbots. They have\nbenefitted from supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) following an unsupervised pretraining. These datasets can\nbe difficult to collect, limited in scope, and vary in sample quality.\nAdditionally, datasets can vary extensively in supervision format, from\nnumerical to binary as well as multi-dimensional with many different values. We\npresent a framework for fine-tuning LLMs using heterogeneous feedback, which\nhas two main components. First, we combine the heterogeneous feedback data into\na single supervision format, compatible with methods like SFT and RLHF. Next,\ngiven this unified feedback dataset, we extract a high-quality and diverse\nsubset to obtain performance increases potentially exceeding the full dataset.\nWe conduct extensive experiments to understand the effectiveness of these\ntechniques for incorporating heterogeneous feedback, and demonstrate\nimprovements from using a high-quality and diverse subset of the data. We find\nthat our framework is able to improve models in multiple areas simultaneously,\nsuch as in instruction following and bias reduction.",
        "pos": [
            "Extractive summarization plays a pivotal role in natural language processing\ndue to its wide-range applications in summarizing diverse content efficiently,\nwhile also being faithful to the original content. Despite significant\nadvancement achieved in extractive summarization by Large Language Models\n(LLMs), these summaries frequently exhibit incoherence. An important aspect of\nthe coherent summary is its readability for intended users. Although there have\nbeen many datasets and benchmarks proposed for creating coherent extractive\nsummaries, none of them currently incorporate user intent to improve coherence\nin extractive summarization. Motivated by this, we propose a systematically\ncreated human-annotated dataset consisting of coherent summaries for five\npublicly available datasets and natural language user feedback, offering\nvaluable insights into how to improve coherence in extractive summaries. We\nutilize this dataset for aligning LLMs through supervised fine-tuning with\nnatural language human feedback to enhance the coherence of their generated\nsummaries. Preliminary experiments with Falcon-40B and Llama-2-13B show\nsignificant performance improvements (~10% Rouge-L) in terms of producing\ncoherent summaries. We further utilize human feedback to benchmark results over\ninstruction-tuned models such as FLAN-T5 which resulted in several interesting\nfindings. Data and source code are available at\nhttps://github.com/Mihir3009/Extract-AI.",
            "Large Language Models (LLMs) have been achieving competent performance on a\nwide range of downstream tasks, yet existing work shows that inference on\nstructured data is challenging for LLMs. This is because LLMs need to either\nunderstand long structured data or select the most relevant evidence before\ninference, and both approaches are not trivial. This paper proposes a\nframework, Learning to Reduce, that fine-tunes a language model with On-Policy\nLearning to generate a reduced version of an input structured data. When\ncompared to state-of-the-art LLMs like GPT-4, Learning to Reduce not only\nachieves outstanding performance in reducing the input, but shows\ngeneralizability on different datasets. We further show that the model\nfine-tuned with our framework helps LLMs better perform on table QA tasks\nespecially when the context is longer."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have been applied to a wide range of tasks,\nincluding text summarization, web navigation, and chatbots. They have\nbenefitted from supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) following an unsupervised pretraining. These datasets can\nbe difficult to collect, limited in scope, and vary in sample quality.\nAdditionally, datasets can vary extensively in supervision format, from\nnumerical to binary as well as multi-dimensional with many different values. We\npresent a framework for fine-tuning LLMs using heterogeneous feedback, which\nhas two main components. First, we combine the heterogeneous feedback data into\na single supervision format, compatible with methods like SFT and RLHF. Next,\ngiven this unified feedback dataset, we extract a high-quality and diverse\nsubset to obtain performance increases potentially exceeding the full dataset.\nWe conduct extensive experiments to understand the effectiveness of these\ntechniques for incorporating heterogeneous feedback, and demonstrate\nimprovements from using a high-quality and diverse subset of the data. We find\nthat our framework is able to improve models in multiple areas simultaneously,\nsuch as in instruction following and bias reduction.",
        "pos": [
            "We introduce an approach to identifying speaker names in dialogue\ntranscripts, a crucial task for enhancing content accessibility and\nsearchability in digital media archives. Despite the advancements in speech\nrecognition, the task of text-based speaker identification (SpeakerID) has\nreceived limited attention, lacking large-scale, diverse datasets for effective\nmodel training. Addressing these gaps, we present a novel, large-scale dataset\nderived from the MediaSum corpus, encompassing transcripts from a wide range of\nmedia sources. We propose novel transformer-based models tailored for\nSpeakerID, leveraging contextual cues within dialogues to accurately attribute\nspeaker names. Through extensive experiments, our best model achieves a great\nprecision of 80.3\\%, setting a new benchmark for SpeakerID. The data and code\nare publicly available here:\n\\url{https://github.com/adobe-research/speaker-identification}",
            "Extractive summarization plays a pivotal role in natural language processing\ndue to its wide-range applications in summarizing diverse content efficiently,\nwhile also being faithful to the original content. Despite significant\nadvancement achieved in extractive summarization by Large Language Models\n(LLMs), these summaries frequently exhibit incoherence. An important aspect of\nthe coherent summary is its readability for intended users. Although there have\nbeen many datasets and benchmarks proposed for creating coherent extractive\nsummaries, none of them currently incorporate user intent to improve coherence\nin extractive summarization. Motivated by this, we propose a systematically\ncreated human-annotated dataset consisting of coherent summaries for five\npublicly available datasets and natural language user feedback, offering\nvaluable insights into how to improve coherence in extractive summaries. We\nutilize this dataset for aligning LLMs through supervised fine-tuning with\nnatural language human feedback to enhance the coherence of their generated\nsummaries. Preliminary experiments with Falcon-40B and Llama-2-13B show\nsignificant performance improvements (~10% Rouge-L) in terms of producing\ncoherent summaries. We further utilize human feedback to benchmark results over\ninstruction-tuned models such as FLAN-T5 which resulted in several interesting\nfindings. Data and source code are available at\nhttps://github.com/Mihir3009/Extract-AI."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have been applied to a wide range of tasks,\nincluding text summarization, web navigation, and chatbots. They have\nbenefitted from supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) following an unsupervised pretraining. These datasets can\nbe difficult to collect, limited in scope, and vary in sample quality.\nAdditionally, datasets can vary extensively in supervision format, from\nnumerical to binary as well as multi-dimensional with many different values. We\npresent a framework for fine-tuning LLMs using heterogeneous feedback, which\nhas two main components. First, we combine the heterogeneous feedback data into\na single supervision format, compatible with methods like SFT and RLHF. Next,\ngiven this unified feedback dataset, we extract a high-quality and diverse\nsubset to obtain performance increases potentially exceeding the full dataset.\nWe conduct extensive experiments to understand the effectiveness of these\ntechniques for incorporating heterogeneous feedback, and demonstrate\nimprovements from using a high-quality and diverse subset of the data. We find\nthat our framework is able to improve models in multiple areas simultaneously,\nsuch as in instruction following and bias reduction.",
        "pos": [
            "Instruction tuning in multimodal large language models (MLLMs) aims to\nsmoothly integrate a backbone LLM with a pre-trained feature encoder for\ndownstream tasks. The major challenge is how to efficiently find the synergy\nthrough cooperative learning where LLMs adapt their reasoning abilities in\ndownstream tasks while feature encoders adjust their encoding to provide more\nrelevant modal information. In this paper, we analyze the MLLM instruction\ntuning from both theoretical and empirical perspectives, where we find\nunbalanced learning between the two components, i.e., the feature encoder and\nthe LLM, can cause diminishing learning gradients that slow the model\nconvergence and often lead to sub-optimal results due to insufficient learning.\nInspired by our findings, we propose a measurement to quantitatively evaluate\nthe learning balance, based on which we further design a dynamic learning\nscheduler that better coordinates the learning. In addition, we introduce an\nauxiliary loss regularization method to promote updating of the generation\ndistribution of MLLMs considering the learning state of each model component,\nwhich potentially prevents each component from gradient diminishing and enables\na more accurate estimation of the learning balance coefficient. We conduct\nexperiments with multiple LLM backbones and feature encoders, where our\ntechniques are model-agnostic and can be generically integrated with various\nMLLM backbones. Experiment results on multiple downstream tasks and modalities\nin vision and audio, demonstrate the proposed method's better efficiency and\neffectiveness in MLLM instruction tuning.",
            "Large Language Models (LLMs) have been achieving competent performance on a\nwide range of downstream tasks, yet existing work shows that inference on\nstructured data is challenging for LLMs. This is because LLMs need to either\nunderstand long structured data or select the most relevant evidence before\ninference, and both approaches are not trivial. This paper proposes a\nframework, Learning to Reduce, that fine-tunes a language model with On-Policy\nLearning to generate a reduced version of an input structured data. When\ncompared to state-of-the-art LLMs like GPT-4, Learning to Reduce not only\nachieves outstanding performance in reducing the input, but shows\ngeneralizability on different datasets. We further show that the model\nfine-tuned with our framework helps LLMs better perform on table QA tasks\nespecially when the context is longer."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have been applied to a wide range of tasks,\nincluding text summarization, web navigation, and chatbots. They have\nbenefitted from supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) following an unsupervised pretraining. These datasets can\nbe difficult to collect, limited in scope, and vary in sample quality.\nAdditionally, datasets can vary extensively in supervision format, from\nnumerical to binary as well as multi-dimensional with many different values. We\npresent a framework for fine-tuning LLMs using heterogeneous feedback, which\nhas two main components. First, we combine the heterogeneous feedback data into\na single supervision format, compatible with methods like SFT and RLHF. Next,\ngiven this unified feedback dataset, we extract a high-quality and diverse\nsubset to obtain performance increases potentially exceeding the full dataset.\nWe conduct extensive experiments to understand the effectiveness of these\ntechniques for incorporating heterogeneous feedback, and demonstrate\nimprovements from using a high-quality and diverse subset of the data. We find\nthat our framework is able to improve models in multiple areas simultaneously,\nsuch as in instruction following and bias reduction.",
        "pos": [
            "Instruction tuning in multimodal large language models (MLLMs) aims to\nsmoothly integrate a backbone LLM with a pre-trained feature encoder for\ndownstream tasks. The major challenge is how to efficiently find the synergy\nthrough cooperative learning where LLMs adapt their reasoning abilities in\ndownstream tasks while feature encoders adjust their encoding to provide more\nrelevant modal information. In this paper, we analyze the MLLM instruction\ntuning from both theoretical and empirical perspectives, where we find\nunbalanced learning between the two components, i.e., the feature encoder and\nthe LLM, can cause diminishing learning gradients that slow the model\nconvergence and often lead to sub-optimal results due to insufficient learning.\nInspired by our findings, we propose a measurement to quantitatively evaluate\nthe learning balance, based on which we further design a dynamic learning\nscheduler that better coordinates the learning. In addition, we introduce an\nauxiliary loss regularization method to promote updating of the generation\ndistribution of MLLMs considering the learning state of each model component,\nwhich potentially prevents each component from gradient diminishing and enables\na more accurate estimation of the learning balance coefficient. We conduct\nexperiments with multiple LLM backbones and feature encoders, where our\ntechniques are model-agnostic and can be generically integrated with various\nMLLM backbones. Experiment results on multiple downstream tasks and modalities\nin vision and audio, demonstrate the proposed method's better efficiency and\neffectiveness in MLLM instruction tuning.",
            "Large Language Models (LLMs) have been achieving competent performance on a\nwide range of downstream tasks, yet existing work shows that inference on\nstructured data is challenging for LLMs. This is because LLMs need to either\nunderstand long structured data or select the most relevant evidence before\ninference, and both approaches are not trivial. This paper proposes a\nframework, Learning to Reduce, that fine-tunes a language model with On-Policy\nLearning to generate a reduced version of an input structured data. When\ncompared to state-of-the-art LLMs like GPT-4, Learning to Reduce not only\nachieves outstanding performance in reducing the input, but shows\ngeneralizability on different datasets. We further show that the model\nfine-tuned with our framework helps LLMs better perform on table QA tasks\nespecially when the context is longer."
        ],
        "neg": []
    },
    {
        "query": "Humans are not homo economicus (i.e., rational economic beings). As humans,\nwe exhibit systematic behavioral biases such as loss aversion, anchoring,\nframing, etc., which lead us to make suboptimal economic decisions. Insofar as\nsuch biases may be embedded in text data on which large language models (LLMs)\nare trained, to what extent are LLMs prone to the same behavioral biases?\nUnderstanding these biases in LLMs is crucial for deploying LLMs to support\nhuman decision-making. We propose utility theory-a paradigm at the core of\nmodern economic theory-as an approach to evaluate the economic biases of LLMs.\nUtility theory enables the quantification and comparison of economic behavior\nagainst benchmarks such as perfect rationality or human behavior. To\ndemonstrate our approach, we quantify and compare the economic behavior of a\nvariety of open- and closed-source LLMs. We find that the economic behavior of\ncurrent LLMs is neither entirely human-like nor entirely economicus-like. We\nalso find that most current LLMs struggle to maintain consistent economic\nbehavior across settings. Finally, we illustrate how our approach can measure\nthe effect of interventions such as prompting on economic biases.",
        "pos": [
            "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.",
            "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task."
        ],
        "neg": []
    },
    {
        "query": "Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.",
        "pos": [
            "Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well.",
            "Large language models (LLMs), known for their exceptional reasoning\ncapabilities, generalizability, and fluency across diverse domains, present a\npromising avenue for enhancing speech-related tasks. In this paper, we focus on\nintegrating decoder-only LLMs to the task of speech-to-text translation (S2TT).\nWe propose a decoder-only architecture that enables the LLM to directly consume\nthe encoded speech representation and generate the text translation.\nAdditionally, we investigate the effects of different parameter-efficient\nfine-tuning techniques and task formulation. Our model achieves\nstate-of-the-art performance on CoVoST 2 and FLEURS among models trained\nwithout proprietary data. We also conduct analyses to validate the design\nchoices of our proposed model and bring insights to the integration of LLMs to\nS2TT."
        ],
        "neg": []
    },
    {
        "query": "Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.",
        "pos": [
            "Large Language Models (LLMs) are rapidly surpassing human knowledge in many\ndomains. While improving these models traditionally relies on costly human\ndata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs\ncan improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation\nduring iterative training. To address this issue, we introduce a novel\nMeta-Rewarding step to the self-improvement process, where the model judges its\nown judgements and uses that feedback to refine its judgment skills.\nSurprisingly, this unsupervised approach improves the model's ability to judge\n{\\em and} follow instructions, as demonstrated by a win rate improvement of\nLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on\nArena-Hard. These results strongly suggest the potential for self-improving\nmodels without human supervision."
        ],
        "neg": []
    },
    {
        "query": "Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.",
        "pos": [
            "Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well."
        ],
        "neg": []
    },
    {
        "query": "Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.",
        "pos": [
            "Large Language Models (LLMs) are rapidly surpassing human knowledge in many\ndomains. While improving these models traditionally relies on costly human\ndata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs\ncan improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation\nduring iterative training. To address this issue, we introduce a novel\nMeta-Rewarding step to the self-improvement process, where the model judges its\nown judgements and uses that feedback to refine its judgment skills.\nSurprisingly, this unsupervised approach improves the model's ability to judge\n{\\em and} follow instructions, as demonstrated by a win rate improvement of\nLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on\nArena-Hard. These results strongly suggest the potential for self-improving\nmodels without human supervision."
        ],
        "neg": []
    },
    {
        "query": "As large language models (LLMs) continue to advance in capability and\ninfluence, ensuring their security and preventing harmful outputs has become\ncrucial. A promising approach to address these concerns involves training\nmodels to automatically generate adversarial prompts for red teaming. However,\nthe evolving subtlety of vulnerabilities in LLMs challenges the effectiveness\nof current adversarial methods, which struggle to specifically target and\nexplore the weaknesses of these models. To tackle these challenges, we\nintroduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving\n}\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$\noptimization framework, which enhances security by leveraging data generated by\nthe model itself. SEAS operates through three iterative stages: Initialization,\nAttack, and Adversarial Optimization, refining both the Red Team and Target\nmodels to improve robustness and safety. This framework reduces reliance on\nmanual testing and significantly enhances the security capabilities of LLMs.\nOur contributions include a novel adversarial framework, a comprehensive safety\ndataset, and after three iterations, the Target model achieves a security level\ncomparable to GPT-4, while the Red Team model shows a marked increase in attack\nsuccess rate (ASR) against advanced models.",
        "pos": [
            "Speculative decoding has emerged as a promising technique to accelerate the\ninference of Large Language Models (LLMs) by employing a small language model\nto draft a hypothesis sequence, which is then validated by the LLM. The\neffectiveness of this approach heavily relies on the balance between\nperformance and efficiency of the draft model. In our research, we focus on\nenhancing the proportion of draft tokens that are accepted to the final output\nby generating multiple hypotheses instead of just one. This allows the LLM more\noptions to choose from and select the longest sequence that meets its\nstandards. Our analysis reveals that hypotheses produced by the draft model\nshare many common token sequences, suggesting a potential for optimizing\ncomputation. Leveraging this observation, we introduce an innovative approach\nutilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This\nstructure enables us to efficiently predict and merge recurring token\nsequences, vastly reducing the computational demands of the draft model. We\nterm this approach Graph-structured Speculative Decoding (GSD). We apply GSD\nacross a range of LLMs, including a 70-billion parameter LLaMA-2 model, and\nobserve a remarkable speedup of 1.73$\\times$ to 1.96$\\times$, significantly\nsurpassing standard speculative decoding.",
            "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems."
        ],
        "neg": []
    },
    {
        "query": "As large language models (LLMs) continue to advance in capability and\ninfluence, ensuring their security and preventing harmful outputs has become\ncrucial. A promising approach to address these concerns involves training\nmodels to automatically generate adversarial prompts for red teaming. However,\nthe evolving subtlety of vulnerabilities in LLMs challenges the effectiveness\nof current adversarial methods, which struggle to specifically target and\nexplore the weaknesses of these models. To tackle these challenges, we\nintroduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving\n}\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$\noptimization framework, which enhances security by leveraging data generated by\nthe model itself. SEAS operates through three iterative stages: Initialization,\nAttack, and Adversarial Optimization, refining both the Red Team and Target\nmodels to improve robustness and safety. This framework reduces reliance on\nmanual testing and significantly enhances the security capabilities of LLMs.\nOur contributions include a novel adversarial framework, a comprehensive safety\ndataset, and after three iterations, the Target model achieves a security level\ncomparable to GPT-4, while the Red Team model shows a marked increase in attack\nsuccess rate (ASR) against advanced models.",
        "pos": [
            "Speculative decoding has emerged as a promising technique to accelerate the\ninference of Large Language Models (LLMs) by employing a small language model\nto draft a hypothesis sequence, which is then validated by the LLM. The\neffectiveness of this approach heavily relies on the balance between\nperformance and efficiency of the draft model. In our research, we focus on\nenhancing the proportion of draft tokens that are accepted to the final output\nby generating multiple hypotheses instead of just one. This allows the LLM more\noptions to choose from and select the longest sequence that meets its\nstandards. Our analysis reveals that hypotheses produced by the draft model\nshare many common token sequences, suggesting a potential for optimizing\ncomputation. Leveraging this observation, we introduce an innovative approach\nutilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This\nstructure enables us to efficiently predict and merge recurring token\nsequences, vastly reducing the computational demands of the draft model. We\nterm this approach Graph-structured Speculative Decoding (GSD). We apply GSD\nacross a range of LLMs, including a 70-billion parameter LLaMA-2 model, and\nobserve a remarkable speedup of 1.73$\\times$ to 1.96$\\times$, significantly\nsurpassing standard speculative decoding.",
            "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems."
        ],
        "neg": []
    },
    {
        "query": "Dialogue serves as the most natural manner of human-computer interaction\n(HCI). Recent advancements in speech language models (SLM) have significantly\nenhanced speech-based conversational AI. However, these models are limited to\nturn-based conversation, lacking the ability to interact with humans in\nreal-time spoken scenarios, for example, being interrupted when the generated\ncontent is not satisfactory. To address these limitations, we explore full\nduplex modeling (FDM) in interactive speech language models (iSLM), focusing on\nenhancing real-time interaction and, more explicitly, exploring the\nquintessential ability of interruption. We introduce a novel model design,\nnamely listening-while-speaking language model (LSLM), an end-to-end system\nequipped with both listening and speaking channels. Our LSLM employs a\ntoken-based decoder-only TTS for speech generation and a streaming\nself-supervised learning (SSL) encoder for real-time audio input. LSLM fuses\nboth channels for autoregressive generation and detects turn-taking in real\ntime. Three fusion strategies -- early fusion, middle fusion, and late fusion\n-- are explored, with middle fusion achieving an optimal balance between speech\ngeneration and real-time interaction. Two experimental settings, command-based\nFDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity\nto diverse instructions. Our results highlight LSLM's capability to achieve\nduplex communication with minimal impact on existing systems. This study aims\nto advance the development of interactive speech dialogue systems, enhancing\ntheir applicability in real-world contexts.",
        "pos": [
            "Retrieval-Augmented Generation (RAG) is applied to solve hallucination\nproblems and real-time constraints of large language models, but it also\ninduces vulnerabilities against retrieval corruption attacks. Existing research\nmainly explores the unreliability of RAG in white-box and closed-domain QA\ntasks. In this paper, we aim to reveal the vulnerabilities of\nRetrieval-Enhanced Generative (RAG) models when faced with black-box attacks\nfor opinion manipulation. We explore the impact of such attacks on user\ncognition and decision-making, providing new insight to enhance the reliability\nand security of RAG models. We manipulate the ranking results of the retrieval\nmodel in RAG with instruction and use these results as data to train a\nsurrogate model. By employing adversarial retrieval attack methods to the\nsurrogate model, black-box transfer attacks on RAG are further realized.\nExperiments conducted on opinion datasets across multiple topics show that the\nproposed attack strategy can significantly alter the opinion polarity of the\ncontent generated by RAG. This demonstrates the model's vulnerability and, more\nimportantly, reveals the potential negative impact on user cognition and\ndecision-making, making it easier to mislead users into accepting incorrect or\nbiased information."
        ],
        "neg": []
    },
    {
        "query": "The advancement of natural language processing (NLP) in biology hinges on\nmodels' ability to interpret intricate biomedical literature. Traditional\nmodels often struggle with the complex and domain-specific language in this\nfield. In this paper, we present BioMamba, a pre-trained model specifically\ndesigned for biomedical text mining. BioMamba builds upon the Mamba\narchitecture and is pre-trained on an extensive corpus of biomedical\nliterature. Our empirical studies demonstrate that BioMamba significantly\noutperforms models like BioBERT and general-domain Mamba across various\nbiomedical tasks. For instance, BioMamba achieves a 100 times reduction in\nperplexity and a 4 times reduction in cross-entropy loss on the BioASQ test\nset. We provide an overview of the model architecture, pre-training process,\nand fine-tuning techniques. Additionally, we release the code and trained model\nto facilitate further research.",
        "pos": [
            "Clinical trials need to recruit a sufficient number of volunteer patients to\ndemonstrate the statistical power of the treatment (e.g., a new drug) in curing\na certain disease. Clinical trial recruitment has a significant impact on trial\nsuccess. Forecasting whether the recruitment process would be successful before\nwe run the trial would save many resources and time. This paper develops a\nnovel deep & cross network with large language model (LLM)-augmented text\nfeature that learns semantic information from trial eligibility criteria and\npredicts enrollment success. The proposed method enables interpretability by\nunderstanding which sentence/word in eligibility criteria contributes heavily\nto prediction. We also demonstrate the empirical superiority of the proposed\nmethod (0.7002 PR-AUC) over a bunch of well-established machine learning\nmethods. The code and curated dataset are publicly available at\nhttps://anonymous.4open.science/r/TrialEnroll-7E12."
        ],
        "neg": []
    },
    {
        "query": "The advancement of natural language processing (NLP) in biology hinges on\nmodels' ability to interpret intricate biomedical literature. Traditional\nmodels often struggle with the complex and domain-specific language in this\nfield. In this paper, we present BioMamba, a pre-trained model specifically\ndesigned for biomedical text mining. BioMamba builds upon the Mamba\narchitecture and is pre-trained on an extensive corpus of biomedical\nliterature. Our empirical studies demonstrate that BioMamba significantly\noutperforms models like BioBERT and general-domain Mamba across various\nbiomedical tasks. For instance, BioMamba achieves a 100 times reduction in\nperplexity and a 4 times reduction in cross-entropy loss on the BioASQ test\nset. We provide an overview of the model architecture, pre-training process,\nand fine-tuning techniques. Additionally, we release the code and trained model\nto facilitate further research.",
        "pos": [
            "Clinical trials need to recruit a sufficient number of volunteer patients to\ndemonstrate the statistical power of the treatment (e.g., a new drug) in curing\na certain disease. Clinical trial recruitment has a significant impact on trial\nsuccess. Forecasting whether the recruitment process would be successful before\nwe run the trial would save many resources and time. This paper develops a\nnovel deep & cross network with large language model (LLM)-augmented text\nfeature that learns semantic information from trial eligibility criteria and\npredicts enrollment success. The proposed method enables interpretability by\nunderstanding which sentence/word in eligibility criteria contributes heavily\nto prediction. We also demonstrate the empirical superiority of the proposed\nmethod (0.7002 PR-AUC) over a bunch of well-established machine learning\nmethods. The code and curated dataset are publicly available at\nhttps://anonymous.4open.science/r/TrialEnroll-7E12."
        ],
        "neg": []
    },
    {
        "query": "The advancement of natural language processing (NLP) in biology hinges on\nmodels' ability to interpret intricate biomedical literature. Traditional\nmodels often struggle with the complex and domain-specific language in this\nfield. In this paper, we present BioMamba, a pre-trained model specifically\ndesigned for biomedical text mining. BioMamba builds upon the Mamba\narchitecture and is pre-trained on an extensive corpus of biomedical\nliterature. Our empirical studies demonstrate that BioMamba significantly\noutperforms models like BioBERT and general-domain Mamba across various\nbiomedical tasks. For instance, BioMamba achieves a 100 times reduction in\nperplexity and a 4 times reduction in cross-entropy loss on the BioASQ test\nset. We provide an overview of the model architecture, pre-training process,\nand fine-tuning techniques. Additionally, we release the code and trained model\nto facilitate further research.",
        "pos": [
            "Clinical trials need to recruit a sufficient number of volunteer patients to\ndemonstrate the statistical power of the treatment (e.g., a new drug) in curing\na certain disease. Clinical trial recruitment has a significant impact on trial\nsuccess. Forecasting whether the recruitment process would be successful before\nwe run the trial would save many resources and time. This paper develops a\nnovel deep & cross network with large language model (LLM)-augmented text\nfeature that learns semantic information from trial eligibility criteria and\npredicts enrollment success. The proposed method enables interpretability by\nunderstanding which sentence/word in eligibility criteria contributes heavily\nto prediction. We also demonstrate the empirical superiority of the proposed\nmethod (0.7002 PR-AUC) over a bunch of well-established machine learning\nmethods. The code and curated dataset are publicly available at\nhttps://anonymous.4open.science/r/TrialEnroll-7E12."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models have demonstrated impressive capabilities in various\nlanguage tasks but may produce content that misaligns with human expectations,\nraising ethical and legal concerns. Therefore, it is important to explore the\nlimitations and implement restrictions on the models to ensure safety and\ncompliance, with Reinforcement Learning from Human Feedback (RLHF) being the\nprimary method. Due to challenges in stability and scalability with the RLHF\nstages, researchers are exploring alternative methods to achieve effects\ncomparable to those of RLHF. However, these methods often depend on large\nhigh-quality datasets and inefficiently utilize generated data. To deal with\nthis problem, we propose PSLE, i.e., Progressively Selective Label Enhancement\nfor Language Model Alignment, a framework that fully utilizes all generated\ndata by guiding the model with principles to align outputs with human\nexpectations. Using a dynamically updated threshold, our approach ensures\nefficient data utilization by incorporating all generated responses and\nweighting them based on their corresponding reward scores. Experimental results\non multiple datasets demonstrate the effectiveness of PSLE compared to existing\nlanguage model alignment methods.",
        "pos": [
            "Symptom phenotypes are one of the key types of manifestations for diagnosis\nand treatment of various disease conditions. However, the diversity of symptom\nterminologies is one of the major obstacles hindering the analysis and\nknowledge sharing of various types of symptom-related medical data particularly\nin the fields of Traditional Chinese Medicine (TCM). Objective: This study\naimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to\nsupport the data mining of Chinese EMRs and real-world study in TCM field.\nMethods: To construct an integrated ontology of symptom phenotypes (ISPO), we\nmanually annotated classical TCM textbooks and large-scale Chinese electronic\nmedical records (EMRs) to collect symptom terms with support from a medical\ntext annotation system. Furthermore, to facilitate the semantic\ninteroperability between different terminologies, we incorporated public\navailable biomedical vocabularies by manual mapping between Chinese terms and\nEnglish terms with cross-references to source vocabularies. In addition, we\nevaluated the ISPO using independent clinical EMRs to provide a high-usable\nmedical ontology for clinical data analysis. Results: By integrating 78,696\ninpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and\ndictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition\nor contextual texts. Adhering to the taxonomical structure of the related\nanatomical systems of symptom phenotypes, ISPO provides 12 top-level categories\nand 79 middle-level sub-categories. The validation of data analysis showed the\nISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with\noccurrence rates of 0.5% in additional three independent curated clinical\ndatasets, which can demonstrate the significant value of ISPO in mapping\nclinical terms to ontologies."
        ],
        "neg": []
    },
    {
        "query": "The ever-increasing volume of digital information necessitates efficient\nmethods for users to extract key insights from lengthy documents. Aspect-based\nsummarization offers a targeted approach, generating summaries focused on\nspecific aspects within a document. Despite advancements in aspect-based\nsummarization research, there is a continuous quest for improved model\nperformance. Given that large language models (LLMs) have demonstrated the\npotential to revolutionize diverse tasks within natural language processing,\nparticularly in the problem of summarization, this paper explores the potential\nof fine-tuning LLMs for the aspect-based summarization task. We evaluate the\nimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,\nGemma and Aya, on a publicly available domain-specific aspect based summary\ndataset. We hypothesize that this approach will enable these models to\neffectively identify and extract aspect-related information, leading to\nsuperior quality aspect-based summaries compared to the state-of-the-art. We\nestablish a comprehensive evaluation framework to compare the performance of\nfine-tuned LLMs against competing aspect-based summarization methods and\nvanilla counterparts of the fine-tuned LLMs. Our work contributes to the field\nof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs\nfor generating high-quality aspect-based summaries. Furthermore, it opens doors\nfor further exploration of using LLMs for targeted information extraction tasks\nacross various NLP domains.",
        "pos": [
            "A common way to extend the memory of large language models (LLMs) is by\nretrieval augmented generation (RAG), which inserts text retrieved from a\nlarger memory into an LLM's context window. However, the context window is\ntypically limited to several thousand tokens, which limits the number of\nretrieved passages that can inform a model's response. For this reason, it's\nimportant to avoid occupying context window space with redundant information by\nensuring a degree of diversity among retrieved passages. At the same time, the\ninformation should also be relevant to the current task. Most prior methods\nthat encourage diversity among retrieved results, such as Maximal Marginal\nRelevance (MMR), do so by incorporating an objective that explicitly trades off\ndiversity and relevance. We propose a novel simple optimization metric based on\nrelevant information gain, a probabilistic measure of the total information\nrelevant to a query for a set of retrieved results. By optimizing this metric,\ndiversity organically emerges from our system. When used as a drop-in\nreplacement for the retrieval component of a RAG system, this method yields\nstate-of-the-art performance on question answering tasks from the Retrieval\nAugmented Generation Benchmark (RGB), outperforming existing metrics that\ndirectly optimize for relevance and diversity."
        ],
        "neg": []
    },
    {
        "query": "The ever-increasing volume of digital information necessitates efficient\nmethods for users to extract key insights from lengthy documents. Aspect-based\nsummarization offers a targeted approach, generating summaries focused on\nspecific aspects within a document. Despite advancements in aspect-based\nsummarization research, there is a continuous quest for improved model\nperformance. Given that large language models (LLMs) have demonstrated the\npotential to revolutionize diverse tasks within natural language processing,\nparticularly in the problem of summarization, this paper explores the potential\nof fine-tuning LLMs for the aspect-based summarization task. We evaluate the\nimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,\nGemma and Aya, on a publicly available domain-specific aspect based summary\ndataset. We hypothesize that this approach will enable these models to\neffectively identify and extract aspect-related information, leading to\nsuperior quality aspect-based summaries compared to the state-of-the-art. We\nestablish a comprehensive evaluation framework to compare the performance of\nfine-tuned LLMs against competing aspect-based summarization methods and\nvanilla counterparts of the fine-tuned LLMs. Our work contributes to the field\nof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs\nfor generating high-quality aspect-based summaries. Furthermore, it opens doors\nfor further exploration of using LLMs for targeted information extraction tasks\nacross various NLP domains.",
        "pos": [
            "A common way to extend the memory of large language models (LLMs) is by\nretrieval augmented generation (RAG), which inserts text retrieved from a\nlarger memory into an LLM's context window. However, the context window is\ntypically limited to several thousand tokens, which limits the number of\nretrieved passages that can inform a model's response. For this reason, it's\nimportant to avoid occupying context window space with redundant information by\nensuring a degree of diversity among retrieved passages. At the same time, the\ninformation should also be relevant to the current task. Most prior methods\nthat encourage diversity among retrieved results, such as Maximal Marginal\nRelevance (MMR), do so by incorporating an objective that explicitly trades off\ndiversity and relevance. We propose a novel simple optimization metric based on\nrelevant information gain, a probabilistic measure of the total information\nrelevant to a query for a set of retrieved results. By optimizing this metric,\ndiversity organically emerges from our system. When used as a drop-in\nreplacement for the retrieval component of a RAG system, this method yields\nstate-of-the-art performance on question answering tasks from the Retrieval\nAugmented Generation Benchmark (RGB), outperforming existing metrics that\ndirectly optimize for relevance and diversity."
        ],
        "neg": []
    },
    {
        "query": "The ever-increasing volume of digital information necessitates efficient\nmethods for users to extract key insights from lengthy documents. Aspect-based\nsummarization offers a targeted approach, generating summaries focused on\nspecific aspects within a document. Despite advancements in aspect-based\nsummarization research, there is a continuous quest for improved model\nperformance. Given that large language models (LLMs) have demonstrated the\npotential to revolutionize diverse tasks within natural language processing,\nparticularly in the problem of summarization, this paper explores the potential\nof fine-tuning LLMs for the aspect-based summarization task. We evaluate the\nimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,\nGemma and Aya, on a publicly available domain-specific aspect based summary\ndataset. We hypothesize that this approach will enable these models to\neffectively identify and extract aspect-related information, leading to\nsuperior quality aspect-based summaries compared to the state-of-the-art. We\nestablish a comprehensive evaluation framework to compare the performance of\nfine-tuned LLMs against competing aspect-based summarization methods and\nvanilla counterparts of the fine-tuned LLMs. Our work contributes to the field\nof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs\nfor generating high-quality aspect-based summaries. Furthermore, it opens doors\nfor further exploration of using LLMs for targeted information extraction tasks\nacross various NLP domains.",
        "pos": [
            "Assessing the quality of Natural Language Generation (NLG) outputs, such as\nthose produced by large language models (LLMs), poses significant challenges.\nTraditional approaches involve either resource-intensive human evaluations or\nautomatic metrics, which often exhibit a low correlation with human judgment.\nIn this study, we propose Review-Feedback-Reason (ReFeR), a novel evaluation\nframework for NLG using LLM agents. We rigorously test ReFeR using two\npre-existing benchmark datasets on diverse NLG tasks. The proposed framework\nnot only enhances the accuracy of NLG evaluation, surpassing previous\nbenchmarks by $\\sim$20\\%, but also generates constructive feedback and\nsignificantly improves collective reasoning. This feedback is then leveraged\nfor the creation of instruction-tuning datasets, which, when used to fine-tune\nsmaller models like Mistral-7B, makes them extremely good evaluators, yielding\na better correlation with human evaluations and performance nearly on par with\nGPT-3.5. We highlight the effectiveness of our methodology through its\napplication on three reasoning benchmarks, where it outperforms most of the\nstate-of-the-art methods, and also outperforms the reasoning capabilities of\nmodels like GPT-3.5 Turbo by $\\sim$11.67\\% and GPT-4 by $\\sim$1\\% on an\naverage.",
            "Neural Machine Translation (NMT) remains a formidable challenge, especially\nwhen dealing with low-resource languages. Pre-trained sequence-to-sequence\n(seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive\nperformance in various low-resource NMT tasks. However, their pre-training has\nbeen confined to 50 languages, leaving out support for numerous low-resource\nlanguages, particularly those spoken in the Indian subcontinent. Expanding\nmBART-50's language support requires complex pre-training, risking performance\ndecline due to catastrophic forgetting. Considering these expanding challenges,\nthis paper explores a framework that leverages the benefits of a pre-trained\nlanguage model along with knowledge distillation in a seq2seq architecture to\nfacilitate translation for low-resource languages, including those not covered\nby mBART-50. The proposed framework employs a multilingual encoder-based\nseq2seq model as the foundational architecture and subsequently uses\ncomplementary knowledge distillation techniques to mitigate the impact of\nimbalanced training. Our framework is evaluated on three low-resource Indic\nlanguages in four Indic-to-Indic directions, yielding significant BLEU-4 and\nchrF improvements over baselines. Further, we conduct human evaluation to\nconfirm effectiveness of our approach. Our code is publicly available at\nhttps://github.com/raypretam/Two-step-low-res-NMT.",
            "Legal systems worldwide are inundated with exponential growth in cases and\ndocuments. There is an imminent need to develop NLP and ML techniques for\nautomatically processing and understanding legal documents to streamline the\nlegal system. However, evaluating and comparing various NLP models designed\nspecifically for the legal domain is challenging. This paper addresses this\nchallenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding\nand Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual\n(9 Indian languages) domain-specific tasks that address different aspects of\nthe legal system from the point of view of understanding and reasoning over\nIndian legal documents. We present baseline models (including LLM-based) for\neach task, outlining the gap between models and the ground truth. To foster\nfurther research in the legal domain, we create a leaderboard (available at:\nhttps://exploration-lab.github.io/IL-TUR/) where the research community can\nupload and compare legal text understanding systems."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown success in handling simple games with\nimperfect information and enabling multi-agent coordination, but their ability\nto facilitate practical collaboration against other agents in complex,\nimperfect information environments, especially in a non-English environment,\nstill needs to be explored. This study investigates the applicability of\nknowledge acquired by open-source and API-based LLMs to sophisticated\ntext-based games requiring agent collaboration under imperfect information,\ncomparing their performance to established baselines using other types of\nagents. We propose a Theory of Mind (ToM) planning technique that allows LLM\nagents to adapt their strategy against various adversaries using only game\nrules, current state, and historical context as input. An external tool was\nincorporated to mitigate the challenge of dynamic and extensive action spaces\nin this card game. Our results show that although a performance gap exists\nbetween current LLMs and state-of-the-art reinforcement learning (RL) models,\nLLMs demonstrate ToM capabilities in this game setting. It consistently\nimproves their performance against opposing agents, suggesting their ability to\nunderstand the actions of allies and adversaries and establish collaboration\nwith allies. To encourage further research and understanding, we have made our\ncodebase openly accessible.",
        "pos": [
            "While large language models (LLMs) have demonstrated impressive capabilities\nacross various natural language processing tasks by acquiring rich factual\nknowledge from their broad training data, their ability to synthesize and\nlogically reason with this knowledge in complex ways remains underexplored. In\nthis work, we present a systematic evaluation of state-of-the-art LLMs' complex\nlogical reasoning abilities through a novel benchmark of automatically\ngenerated complex reasoning questions over general domain and biomedical\nknowledge graphs. Our extensive experiments, employing diverse in-context\nlearning techniques, reveal that LLMs excel at reasoning over general world\nknowledge but face significant challenges with specialized domain-specific\nknowledge. We find that prompting with explicit Chain-of-Thought demonstrations\ncan substantially improve LLM performance on complex logical reasoning tasks\nwith diverse logical operations. Interestingly, our controlled evaluations\nuncover an asymmetry where LLMs display proficiency at set union operations,\nbut struggle considerably with set intersections - a key building block of\nlogical reasoning. To foster further work, we will publicly release our\nevaluation benchmark and code."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown success in handling simple games with\nimperfect information and enabling multi-agent coordination, but their ability\nto facilitate practical collaboration against other agents in complex,\nimperfect information environments, especially in a non-English environment,\nstill needs to be explored. This study investigates the applicability of\nknowledge acquired by open-source and API-based LLMs to sophisticated\ntext-based games requiring agent collaboration under imperfect information,\ncomparing their performance to established baselines using other types of\nagents. We propose a Theory of Mind (ToM) planning technique that allows LLM\nagents to adapt their strategy against various adversaries using only game\nrules, current state, and historical context as input. An external tool was\nincorporated to mitigate the challenge of dynamic and extensive action spaces\nin this card game. Our results show that although a performance gap exists\nbetween current LLMs and state-of-the-art reinforcement learning (RL) models,\nLLMs demonstrate ToM capabilities in this game setting. It consistently\nimproves their performance against opposing agents, suggesting their ability to\nunderstand the actions of allies and adversaries and establish collaboration\nwith allies. To encourage further research and understanding, we have made our\ncodebase openly accessible.",
        "pos": [
            "While large language models (LLMs) have demonstrated impressive capabilities\nacross various natural language processing tasks by acquiring rich factual\nknowledge from their broad training data, their ability to synthesize and\nlogically reason with this knowledge in complex ways remains underexplored. In\nthis work, we present a systematic evaluation of state-of-the-art LLMs' complex\nlogical reasoning abilities through a novel benchmark of automatically\ngenerated complex reasoning questions over general domain and biomedical\nknowledge graphs. Our extensive experiments, employing diverse in-context\nlearning techniques, reveal that LLMs excel at reasoning over general world\nknowledge but face significant challenges with specialized domain-specific\nknowledge. We find that prompting with explicit Chain-of-Thought demonstrations\ncan substantially improve LLM performance on complex logical reasoning tasks\nwith diverse logical operations. Interestingly, our controlled evaluations\nuncover an asymmetry where LLMs display proficiency at set union operations,\nbut struggle considerably with set intersections - a key building block of\nlogical reasoning. To foster further work, we will publicly release our\nevaluation benchmark and code."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in the graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general setting is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing our simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness\n(i.e., action accuracy) of multimodal agents, our findings indicate that these\nagents are prone to environmental distractions, resulting in unfaithful\nbehaviors. Furthermore, we switch to the adversarial perspective and implement\nenvironment injection, demonstrating that such unfaithfulness can be exploited,\nleading to unexpected risks.",
        "pos": [
            "The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in the graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general setting is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing our simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness\n(i.e., action accuracy) of multimodal agents, our findings indicate that these\nagents are prone to environmental distractions, resulting in unfaithful\nbehaviors. Furthermore, we switch to the adversarial perspective and implement\nenvironment injection, demonstrating that such unfaithfulness can be exploited,\nleading to unexpected risks.",
        "pos": [
            "The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in the graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general setting is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing our simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness\n(i.e., action accuracy) of multimodal agents, our findings indicate that these\nagents are prone to environmental distractions, resulting in unfaithful\nbehaviors. Furthermore, we switch to the adversarial perspective and implement\nenvironment injection, demonstrating that such unfaithfulness can be exploited,\nleading to unexpected risks.",
        "pos": [
            "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in the graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general setting is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing our simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness\n(i.e., action accuracy) of multimodal agents, our findings indicate that these\nagents are prone to environmental distractions, resulting in unfaithful\nbehaviors. Furthermore, we switch to the adversarial perspective and implement\nenvironment injection, demonstrating that such unfaithfulness can be exploited,\nleading to unexpected risks.",
        "pos": [
            "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in the graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general setting is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing our simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness\n(i.e., action accuracy) of multimodal agents, our findings indicate that these\nagents are prone to environmental distractions, resulting in unfaithful\nbehaviors. Furthermore, we switch to the adversarial perspective and implement\nenvironment injection, demonstrating that such unfaithfulness can be exploited,\nleading to unexpected risks.",
        "pos": [
            "Recently, there has been a growing interest among large language model (LLM)\ndevelopers in LLM-based document reading systems, which enable users to upload\ntheir own documents and pose questions related to the document contents, going\nbeyond simple reading comprehension tasks. Consequently, these systems have\nbeen carefully designed to tackle challenges such as file parsing, metadata\nextraction, multi-modal information understanding and long-context reading.\nHowever, no current benchmark exists to evaluate their performance in such\nscenarios, where a raw file and questions are provided as input, and a\ncorresponding response is expected as output. In this paper, we introduce\nDocBench, a new benchmark designed to evaluate LLM-based document reading\nsystems. Our benchmark involves a meticulously crafted process, including the\nrecruitment of human annotators and the generation of synthetic questions. It\nincludes 229 real documents and 1,102 questions, spanning across five different\ndomains and four major types of questions. We evaluate both proprietary\nLLM-based systems accessible via web interfaces or APIs, and a parse-then-read\npipeline employing open-source LLMs. Our evaluations reveal noticeable gaps\nbetween existing LLM-based document reading systems and human performance,\nunderscoring the challenges of developing proficient systems. To summarize,\nDocBench aims to establish a standardized benchmark for evaluating LLM-based\ndocument reading systems under diverse real-world scenarios, thereby guiding\nfuture advancements in this research area.",
            "The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in the graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general setting is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing our simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness\n(i.e., action accuracy) of multimodal agents, our findings indicate that these\nagents are prone to environmental distractions, resulting in unfaithful\nbehaviors. Furthermore, we switch to the adversarial perspective and implement\nenvironment injection, demonstrating that such unfaithfulness can be exploited,\nleading to unexpected risks.",
        "pos": [
            "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
            "Recently, there has been a growing interest among large language model (LLM)\ndevelopers in LLM-based document reading systems, which enable users to upload\ntheir own documents and pose questions related to the document contents, going\nbeyond simple reading comprehension tasks. Consequently, these systems have\nbeen carefully designed to tackle challenges such as file parsing, metadata\nextraction, multi-modal information understanding and long-context reading.\nHowever, no current benchmark exists to evaluate their performance in such\nscenarios, where a raw file and questions are provided as input, and a\ncorresponding response is expected as output. In this paper, we introduce\nDocBench, a new benchmark designed to evaluate LLM-based document reading\nsystems. Our benchmark involves a meticulously crafted process, including the\nrecruitment of human annotators and the generation of synthetic questions. It\nincludes 229 real documents and 1,102 questions, spanning across five different\ndomains and four major types of questions. We evaluate both proprietary\nLLM-based systems accessible via web interfaces or APIs, and a parse-then-read\npipeline employing open-source LLMs. Our evaluations reveal noticeable gaps\nbetween existing LLM-based document reading systems and human performance,\nunderscoring the challenges of developing proficient systems. To summarize,\nDocBench aims to establish a standardized benchmark for evaluating LLM-based\ndocument reading systems under diverse real-world scenarios, thereby guiding\nfuture advancements in this research area."
        ],
        "neg": []
    },
    {
        "query": "Significant advancements has recently been achieved in the field of\nmulti-modal large language models (MLLMs), demonstrating their remarkable\ncapabilities in understanding and reasoning across diverse tasks. However,\nthese models are often trained for specific tasks and rely on task-specific\ninput-output formats, limiting their applicability to a broader range of tasks.\nThis raises a fundamental question: Can we develop a unified approach to\nrepresent and handle different multi-modal tasks to maximize the\ngeneralizability of MLLMs? In this paper, we propose UnifiedMLLM, a\ncomprehensive model designed to represent various tasks using a unified\nrepresentation. Our model exhibits strong capabilities in comprehending the\nimplicit intent of user instructions and preforming reasoning. In addition to\ngenerating textual responses, our model also outputs task tokens and grounding\ntokens, serving as indicators of task types and task granularity. These outputs\nare subsequently routed through the task router and directed to specific expert\nmodels for task completion. To train our model, we construct a task-specific\ndataset and an 100k multi-task dataset encompassing complex scenarios.\nEmploying a three-stage training strategy, we equip our model with robust\nreasoning and task processing capabilities while preserving its generalization\ncapacity and knowledge reservoir. Extensive experiments showcase the impressive\nperformance of our unified representation approach across various tasks,\nsurpassing existing methodologies. Furthermore, our approach exhibits\nexceptional scalability and generality. Our code, model, and dataset will be\navailable at \\url{https://github.com/lzw-lzw/UnifiedMLLM}.",
        "pos": [
            "Complex reasoning is an impressive ability shown by large language models\n(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought\nprompting or iterative tool-using to solve challenging tasks step-by-step. In\nthis paper, we hope to focus on evaluating and teaching LLMs to conduct\ninductive reasoning, that is, LLMs are supposed to infer underlying rules by\nobserving examples or sequential transformations. However, collecting\nlarge-scale and diverse human-generated inductive data is challenging. We focus\non data synthesis in the code domain and propose a \\textbf{Case2Code} task by\nexploiting the expressiveness and correctness of programs. Specifically, we\ncollect a diverse set of executable programs, synthesize input-output\ntransformations for each program, and force LLMs to infer the underlying code\nimplementations based on the synthetic I/O cases. We first evaluate\nrepresentative LLMs on the synthesized Case2Code task and demonstrate that the\nCase-to-code induction is challenging for LLMs. Then, we synthesize large-scale\nCase2Code training samples to train LLMs to perform inductive reasoning.\nExperimental results show that such induction training benefits not only in\ndistribution Case2Code performance but also enhances various coding abilities\nof trained LLMs, demonstrating the great potential of learning inductive\nreasoning via synthetic data."
        ],
        "neg": []
    },
    {
        "query": "Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs'\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs' performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs' reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks.",
        "pos": [
            "In this study, we explore the proactive ability of LLMs to seek user support,\nusing text-to-SQL generation as a case study. We propose metrics to evaluate\nthe trade-off between performance improvements and user burden, and investigate\nwhether LLMs can determine when to request help and examine their performance\nwith varying levels of information availability. Our experiments reveal that\nwithout external feedback, many LLMs struggle to recognize their need for\nadditional support. Our findings highlight the importance of external signals\nand provide insights for future research on improving support-seeking\nstrategies."
        ],
        "neg": []
    },
    {
        "query": "Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs'\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs' performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs' reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks.",
        "pos": [
            "In this study, we explore the proactive ability of LLMs to seek user support,\nusing text-to-SQL generation as a case study. We propose metrics to evaluate\nthe trade-off between performance improvements and user burden, and investigate\nwhether LLMs can determine when to request help and examine their performance\nwith varying levels of information availability. Our experiments reveal that\nwithout external feedback, many LLMs struggle to recognize their need for\nadditional support. Our findings highlight the importance of external signals\nand provide insights for future research on improving support-seeking\nstrategies."
        ],
        "neg": []
    },
    {
        "query": "Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs'\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs' performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs' reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks.",
        "pos": [
            "In this study, we explore the proactive ability of LLMs to seek user support,\nusing text-to-SQL generation as a case study. We propose metrics to evaluate\nthe trade-off between performance improvements and user burden, and investigate\nwhether LLMs can determine when to request help and examine their performance\nwith varying levels of information availability. Our experiments reveal that\nwithout external feedback, many LLMs struggle to recognize their need for\nadditional support. Our findings highlight the importance of external signals\nand provide insights for future research on improving support-seeking\nstrategies."
        ],
        "neg": []
    },
    {
        "query": "Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs'\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs' performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs' reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks.",
        "pos": [
            "In this study, we explore the proactive ability of LLMs to seek user support,\nusing text-to-SQL generation as a case study. We propose metrics to evaluate\nthe trade-off between performance improvements and user burden, and investigate\nwhether LLMs can determine when to request help and examine their performance\nwith varying levels of information availability. Our experiments reveal that\nwithout external feedback, many LLMs struggle to recognize their need for\nadditional support. Our findings highlight the importance of external signals\nand provide insights for future research on improving support-seeking\nstrategies.",
            "Recent advancements in dialogue systems have highlighted the significance of\nintegrating multimodal responses, which enable conveying ideas through diverse\nmodalities rather than solely relying on text-based interactions. This\nenrichment not only improves overall communicative efficacy but also enhances\nthe quality of conversational experiences. However, existing methods for\ndialogue-to-image retrieval face limitations due to the constraints of\npre-trained vision language models (VLMs) in comprehending complex dialogues\naccurately. To address this, we present a novel approach leveraging the robust\nreasoning capabilities of large language models (LLMs) to generate precise\ndialogue-associated visual descriptors, facilitating seamless connection with\nimages. Extensive experiments conducted on benchmark data validate the\neffectiveness of our proposed approach in deriving concise and accurate visual\ndescriptors, leading to significant enhancements in dialogue-to-image retrieval\nperformance. Furthermore, our findings demonstrate the method's\ngeneralizability across diverse visual cues, various LLMs, and different\ndatasets, underscoring its practicality and potential impact in real-world\napplications."
        ],
        "neg": []
    },
    {
        "query": "The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at \\url{https://github.com/liangzid/PromptExtractionEval}.",
        "pos": [
            "Question Answering (QA) effectively evaluates language models' reasoning and\nknowledge depth. While QA datasets are plentiful in areas like general domain\nand biomedicine, academic chemistry is less explored. Chemical QA plays a\ncrucial role in both education and research by effectively translating complex\nchemical information into readily understandable format. Addressing this gap,\nwe introduce ScholarChemQA, a large-scale QA dataset constructed from chemical\npapers. This dataset reflects typical real-world challenges, including an\nimbalanced data distribution and a substantial amount of unlabeled data that\ncan be potentially useful. Correspondingly, we introduce a QAMatch model,\nspecifically designed to effectively answer chemical questions by fully\nleveraging our collected data. We first address the issue of imbalanced label\ndistribution by re-weighting the instance-wise loss based on the inverse\nfrequency of each class, ensuring minority classes are not dominated by\nmajority ones during optimization. Next, we utilize the unlabeled data to\nenrich the learning process, generating a variety of augmentations based on a\nSoftMix operation and ensuring their predictions align with the same target,\ni.e., pseudo-labels. To ensure the quality of the pseudo-labels, we propose a\ncalibration procedure aimed at closely aligning the pseudo-label estimates of\nindividual samples with a desired ground truth distribution. Experiments show\nthat our QAMatch significantly outperforms the recent similar-scale baselines\nand Large Language Models (LLMs) not only on our ScholarChemQA dataset but also\non four benchmark datasets. We hope our benchmark and model can facilitate and\npromote more research on chemical QA."
        ],
        "neg": []
    },
    {
        "query": "Speaker diarization answers the question \"who spoke when\" for an audio file.\nIn some diarization scenarios, low latency is required for transcription.\nSpeaker diarization with low latency is referred to as online speaker\ndiarization. The DIART pipeline is an online speaker diarization system. It\nconsists of a segmentation and an embedding model. The embedding model has the\nlargest share of the overall latency. The aim of this paper is to optimize the\ninference latency of the DIART pipeline. Different inference optimization\nmethods such as knowledge distilation, pruning, quantization and layer fusion\nare applied to the embedding model of the pipeline. It turns out that knowledge\ndistillation optimizes the latency, but has a negative effect on the accuracy.\nQuantization and layer fusion also have a positive influence on the latency\nwithout worsening the accuracy. Pruning, on the other hand, does not improve\nlatency.",
        "pos": [
            "In this paper, different online speaker diarization systems are evaluated on\nthe same hardware with the same test data with regard to their latency. The\nlatency is the time span from audio input to the output of the corresponding\nspeaker label. As part of the evaluation, various model combinations within the\nDIART framework, a diarization system based on the online clustering algorithm\nUIS-RNN-SML, and the end-to-end online diarization system FS-EEND are compared.\nThe lowest latency is achieved for the DIART-pipeline with the embedding model\npyannote/embedding and the segmentation model pyannote/segmentation. The\nFS-EEND system shows a similarly good latency. In general there is currently no\npublished research that compares several online diarization systems in terms of\ntheir latency. This makes this work even more relevant."
        ],
        "neg": []
    },
    {
        "query": "Speaker diarization answers the question \"who spoke when\" for an audio file.\nIn some diarization scenarios, low latency is required for transcription.\nSpeaker diarization with low latency is referred to as online speaker\ndiarization. The DIART pipeline is an online speaker diarization system. It\nconsists of a segmentation and an embedding model. The embedding model has the\nlargest share of the overall latency. The aim of this paper is to optimize the\ninference latency of the DIART pipeline. Different inference optimization\nmethods such as knowledge distilation, pruning, quantization and layer fusion\nare applied to the embedding model of the pipeline. It turns out that knowledge\ndistillation optimizes the latency, but has a negative effect on the accuracy.\nQuantization and layer fusion also have a positive influence on the latency\nwithout worsening the accuracy. Pruning, on the other hand, does not improve\nlatency.",
        "pos": [
            "In this paper, different online speaker diarization systems are evaluated on\nthe same hardware with the same test data with regard to their latency. The\nlatency is the time span from audio input to the output of the corresponding\nspeaker label. As part of the evaluation, various model combinations within the\nDIART framework, a diarization system based on the online clustering algorithm\nUIS-RNN-SML, and the end-to-end online diarization system FS-EEND are compared.\nThe lowest latency is achieved for the DIART-pipeline with the embedding model\npyannote/embedding and the segmentation model pyannote/segmentation. The\nFS-EEND system shows a similarly good latency. In general there is currently no\npublished research that compares several online diarization systems in terms of\ntheir latency. This makes this work even more relevant."
        ],
        "neg": []
    },
    {
        "query": "Multilingual neural machine translation systems learn to map sentences of\ndifferent languages into a common representation space. Intuitively, with a\ngrowing number of seen languages the encoder sentence representation grows more\nflexible and easily adaptable to new languages. In this work, we test this\nhypothesis by zero-shot translating from unseen languages. To deal with unknown\nvocabularies from unknown languages we propose a setup where we decouple\nlearning of vocabulary and syntax, i.e. for each language we learn word\nrepresentations in a separate step (using cross-lingual word embeddings), and\nthen train to translate while keeping those word representations frozen. We\ndemonstrate that this setup enables zero-shot translation from entirely unseen\nlanguages. Zero-shot translating with a model trained on Germanic and Romance\nlanguages we achieve scores of 42.6 BLEU for Portuguese-English and 20.7 BLEU\nfor Russian-English on TED domain. We explore how this zero-shot translation\ncapability develops with varying number of languages seen by the encoder.\nLastly, we explore the effectiveness of our decoupled learning strategy for\nunsupervised machine translation. By exploiting our model's zero-shot\ntranslation capability for iterative back-translation we attain near parity\nwith a supervised setting.",
        "pos": [
            "This paper addresses the problem of correctly formatting numeric expressions\nin automatic speech recognition (ASR) transcripts. This is challenging since\nthe expected transcript format depends on the context, e.g., 1945 (year) vs.\n19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize\nand format numeric expression, such as years, timestamps, currency amounts, and\nquantities. For the end-to-end approach we employed a data generation strategy\nusing a large language model (LLM) together with a text to speech (TTS) model\nto generate adaptation data. The results on our test dataset show that while\napproaches based on LLMs perform well on recognizing formatted numeric\nexpressions, adapted end-to-end models offer competitive performance with the\nadvantage of lower latency and inference cost."
        ],
        "neg": []
    },
    {
        "query": "This paper explores the task of automatic prediction of text spans in a legal\nproblem description that support a legal area label. We use a corpus of problem\ndescriptions written by laypeople in English that is annotated by practising\nlawyers. Inherent subjectivity exists in our task because legal area\ncategorisation is a complex task, and lawyers often have different views on a\nproblem, especially in the face of legally-imprecise descriptions of issues.\nExperiments show that training on majority-voted spans outperforms training on\ndisaggregated ones.",
        "pos": [
            "We propose selective debiasing -- an inference-time safety mechanism that\naims to increase the overall quality of models in terms of prediction\nperformance and fairness in the situation when re-training a model is\nprohibitive. The method is inspired by selective prediction, where some\npredictions that are considered low quality are discarded at inference time. In\nour approach, we identify the potentially biased model predictions and, instead\nof discarding them, we debias them using LEACE -- a post-processing debiasing\nmethod. To select problematic predictions, we propose a bias quantification\napproach based on KL divergence, which achieves better results than standard UQ\nmethods. Experiments with text classification datasets demonstrate that\nselective debiasing helps to close the performance gap between post-processing\nmethods and at-training and pre-processing debiasing techniques."
        ],
        "neg": []
    },
    {
        "query": "This paper explores the task of automatic prediction of text spans in a legal\nproblem description that support a legal area label. We use a corpus of problem\ndescriptions written by laypeople in English that is annotated by practising\nlawyers. Inherent subjectivity exists in our task because legal area\ncategorisation is a complex task, and lawyers often have different views on a\nproblem, especially in the face of legally-imprecise descriptions of issues.\nExperiments show that training on majority-voted spans outperforms training on\ndisaggregated ones.",
        "pos": [
            "We present an evaluation framework for interactive dialogue assessment in the\ncontext of English as a Second Language (ESL) speakers. Our framework collects\ndialogue-level interactivity labels (e.g., topic management; 4 labels in total)\nand micro-level span features (e.g., backchannels; 17 features in total). Given\nour annotated data, we study how the micro-level features influence the (higher\nlevel) interactivity quality of ESL dialogues by constructing various machine\nlearning-based models. Our results demonstrate that certain micro-level\nfeatures strongly correlate with interactivity quality, like reference word\n(e.g., she, her, he), revealing new insights about the interaction between\nhigher-level dialogue quality and lower-level linguistic signals. Our framework\nalso provides a means to assess ESL communication, which is useful for language\nassessment."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have garnered significant interest in natural\nlanguage processing (NLP), particularly their remarkable performance in various\ndownstream tasks in resource-rich languages. Recent studies have highlighted\nthe limitations of LLMs in low-resource languages, primarily focusing on binary\nclassification tasks and giving minimal attention to South Asian languages.\nThese limitations are primarily attributed to constraints such as dataset\nscarcity, computational costs, and research gaps specific to low-resource\nlanguages. To address this gap, we present datasets for sentiment and hate\nspeech tasks by translating from English to Bangla, Hindi, and Urdu,\nfacilitating research in low-resource language processing. Further, we\ncomprehensively examine zero-shot learning using multiple LLMs in English and\nwidely spoken South Asian languages. Our findings indicate that GPT-4\nconsistently outperforms Llama 2 and Gemini, with English consistently\ndemonstrating superior performance across diverse tasks compared to\nlow-resource languages. Furthermore, our analysis reveals that natural language\ninference (NLI) exhibits the highest performance among the evaluated tasks,\nwith GPT-4 demonstrating superior capabilities.",
        "pos": [
            "We present an overview of the second edition of the ArAIEval shared task,\norganized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In\nthis edition, ArAIEval offers two tasks: (i) detection of propagandistic\ntextual spans with persuasion techniques identification in tweets and news\narticles, and (ii) distinguishing between propagandistic and non-propagandistic\nmemes. A total of 14 teams participated in the final evaluation phase, with 6\nand 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams\nsubmitted system description papers. Across both tasks, we observed that\nfine-tuning transformer models such as AraBERT was at the core of the majority\nof the participating systems. We provide a description of the task setup,\nincluding a description of the dataset construction and the evaluation setup.\nWe further provide a brief overview of the participating systems. All datasets\nand evaluation scripts are released to the research community\n(https://araieval.gitlab.io/). We hope this will enable further research on\nthese important tasks in Arabic."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have garnered significant interest in natural\nlanguage processing (NLP), particularly their remarkable performance in various\ndownstream tasks in resource-rich languages. Recent studies have highlighted\nthe limitations of LLMs in low-resource languages, primarily focusing on binary\nclassification tasks and giving minimal attention to South Asian languages.\nThese limitations are primarily attributed to constraints such as dataset\nscarcity, computational costs, and research gaps specific to low-resource\nlanguages. To address this gap, we present datasets for sentiment and hate\nspeech tasks by translating from English to Bangla, Hindi, and Urdu,\nfacilitating research in low-resource language processing. Further, we\ncomprehensively examine zero-shot learning using multiple LLMs in English and\nwidely spoken South Asian languages. Our findings indicate that GPT-4\nconsistently outperforms Llama 2 and Gemini, with English consistently\ndemonstrating superior performance across diverse tasks compared to\nlow-resource languages. Furthermore, our analysis reveals that natural language\ninference (NLI) exhibits the highest performance among the evaluated tasks,\nwith GPT-4 demonstrating superior capabilities.",
        "pos": [
            "Social media posts provide valuable insight into the narrative of users and\ntheir intentions, including providing an opportunity to automatically model\nwhether a social media user is depressed or not. The challenge lies in\nfaithfully modelling user narratives from their online social media posts,\nwhich could potentially be useful in several different applications. We have\ndeveloped a novel and effective model called \\texttt{NarrationDep}, which\nfocuses on detecting narratives associated with depression. By analyzing a\nuser's tweets, \\texttt{NarrationDep} accurately identifies crucial narratives.\n\\texttt{NarrationDep} is a deep learning framework that jointly models\nindividual user tweet representations and clusters of users' tweets. As a\nresult, \\texttt{NarrationDep} is characterized by a novel two-layer deep\nlearning model: the first layer models using social media text posts, and the\nsecond layer learns semantic representations of tweets associated with a\ncluster. To faithfully model these cluster representations, the second layer\nincorporates a novel component that hierarchically learns from users' posts.\nThe results demonstrate that our framework outperforms other comparative models\nincluding recently developed models on a variety of datasets."
        ],
        "neg": []
    },
    {
        "query": "Legal charge prediction, an essential task in legal AI, seeks to assign\naccurate charge labels to case descriptions, attracting significant recent\ninterest. Existing methods primarily employ diverse neural network structures\nfor modeling case descriptions directly, failing to effectively leverage\nmulti-source external knowledge. We propose a prompt learning framework-based\nmethod that simultaneously leverages multi-source heterogeneous external\nknowledge from a legal knowledge base, a conversational LLM, and related legal\narticles. Specifically, we match knowledge snippets in case descriptions via\nthe legal knowledge base and encapsulate them into the input through a hard\nprompt template. Additionally, we retrieve legal articles related to a given\ncase description through contrastive learning, and then obtain factual elements\nwithin the case description through a conversational LLM. We fuse the embedding\nvectors of soft prompt tokens with the encoding vector of factual elements to\nachieve knowledge-enhanced model forward inference. Experimental results show\nthat our method achieved state-of-the-art results on CAIL-2018, the largest\nlegal charge prediction dataset, and our method has lower data dependency. Case\nstudies also demonstrate our method's strong interpretability.",
        "pos": [
            "Large Language Models (LLMs) demonstrate substantial potential in delivering\nlegal consultation services to users without a legal background, attributed to\ntheir superior text comprehension and generation capabilities. Nonetheless,\nexisting Chinese legal LLMs limit interaction to a single model-user dialogue,\nunlike the collaborative consultations typical of law firms, where multiple\nstaff members contribute to a single consultation. This limitation prevents an\nauthentic consultation experience. Additionally, extant Chinese legal LLMs\nsuffer from critical limitations: (1) insufficient control over the quality of\ninstruction fine-tuning data; (2) increased model hallucination resulting from\nusers' ambiguous queries; and (3) a reduction in the model's ability to follow\ninstructions over multiple dialogue turns. In response to these challenges, we\npropose a novel legal dialogue framework that leverages the collaborative\ncapabilities of multiple LLM agents, termed LawLuo. This framework encompasses\nfour agents: a receptionist, a lawyer, a secretary, and a boss, each\nresponsible for different functionalities, collaboratively providing a\ncomprehensive legal consultation to users. Additionally, we constructed two\nhigh-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned\nChatGLM-3-6b using these datasets. We propose a legal query clarification\nalgorithm called ToLC. Experimental results demonstrate that LawLuo outperforms\nbaseline LLMs, including GPT-4, across three dimensions: lawyer-like language\nstyle, the usefulness of legal advice, and the accuracy of legal knowledge. Our\ncode and datasets are available at https://github.com/NEFUJing/LawLuo."
        ],
        "neg": []
    },
    {
        "query": "Legal charge prediction, an essential task in legal AI, seeks to assign\naccurate charge labels to case descriptions, attracting significant recent\ninterest. Existing methods primarily employ diverse neural network structures\nfor modeling case descriptions directly, failing to effectively leverage\nmulti-source external knowledge. We propose a prompt learning framework-based\nmethod that simultaneously leverages multi-source heterogeneous external\nknowledge from a legal knowledge base, a conversational LLM, and related legal\narticles. Specifically, we match knowledge snippets in case descriptions via\nthe legal knowledge base and encapsulate them into the input through a hard\nprompt template. Additionally, we retrieve legal articles related to a given\ncase description through contrastive learning, and then obtain factual elements\nwithin the case description through a conversational LLM. We fuse the embedding\nvectors of soft prompt tokens with the encoding vector of factual elements to\nachieve knowledge-enhanced model forward inference. Experimental results show\nthat our method achieved state-of-the-art results on CAIL-2018, the largest\nlegal charge prediction dataset, and our method has lower data dependency. Case\nstudies also demonstrate our method's strong interpretability.",
        "pos": [
            "Large Language Models (LLMs) demonstrate substantial potential in delivering\nlegal consultation services to users without a legal background, attributed to\ntheir superior text comprehension and generation capabilities. Nonetheless,\nexisting Chinese legal LLMs limit interaction to a single model-user dialogue,\nunlike the collaborative consultations typical of law firms, where multiple\nstaff members contribute to a single consultation. This limitation prevents an\nauthentic consultation experience. Additionally, extant Chinese legal LLMs\nsuffer from critical limitations: (1) insufficient control over the quality of\ninstruction fine-tuning data; (2) increased model hallucination resulting from\nusers' ambiguous queries; and (3) a reduction in the model's ability to follow\ninstructions over multiple dialogue turns. In response to these challenges, we\npropose a novel legal dialogue framework that leverages the collaborative\ncapabilities of multiple LLM agents, termed LawLuo. This framework encompasses\nfour agents: a receptionist, a lawyer, a secretary, and a boss, each\nresponsible for different functionalities, collaboratively providing a\ncomprehensive legal consultation to users. Additionally, we constructed two\nhigh-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned\nChatGLM-3-6b using these datasets. We propose a legal query clarification\nalgorithm called ToLC. Experimental results demonstrate that LawLuo outperforms\nbaseline LLMs, including GPT-4, across three dimensions: lawyer-like language\nstyle, the usefulness of legal advice, and the accuracy of legal knowledge. Our\ncode and datasets are available at https://github.com/NEFUJing/LawLuo."
        ],
        "neg": []
    },
    {
        "query": "The use of large language models (LLMs) is expanding rapidly, and open-source\nversions are becoming available, offering users safer and more adaptable\noptions. These models enable users to protect data privacy by eliminating the\nneed to provide data to third parties and can be customized for specific tasks.\nIn this study, we compare the performance of various language models on the\nSustainable Development Goal (SDG) mapping task, using the output of GPT-4o as\nthe baseline. The selected open-source models for comparison include Mixtral,\nLLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more\nspecialized version of GPT-4o, was included to extend the comparison. Given the\nmulti-label nature of the SDG mapping task, we employed metrics such as F1\nscore, precision, and recall with micro-averaging to evaluate different aspects\nof the models' performance. These metrics are derived from the confusion matrix\nto ensure a comprehensive evaluation. We provide a clear observation and\nanalysis of each model's performance by plotting curves based on F1 score,\nprecision, and recall at different thresholds. According to the results of this\nexperiment, LLaMA 2 and Gemma still have significant room for improvement. The\nother four models do not exhibit particularly large differences in performance.\nThe outputs from all seven models are available on Zenodo:\nhttps://doi.org/10.5281/zenodo.12789375.",
        "pos": [
            "In recent years, Natural Language Processing (NLP) has played a significant\nrole in various Artificial Intelligence (AI) applications such as chatbots,\ntext generation, and language translation. The emergence of large language\nmodels (LLMs) has greatly improved the performance of these applications,\nshowing astonishing results in language understanding and generation. However,\nthey still show some disadvantages, such as hallucinations and lack of\ndomain-specific knowledge, that affect their performance in real-world tasks.\nThese issues can be effectively mitigated by incorporating knowledge graphs\n(KGs), which organise information in structured formats that capture\nrelationships between entities in a versatile and interpretable fashion.\nLikewise, the construction and validation of KGs present challenges that LLMs\ncan help resolve. The complementary relationship between LLMs and KGs has led\nto a trend that combines these technologies to achieve trustworthy results.\nThis work collected 28 papers outlining methods for KG-powered LLMs, LLM-based\nKGs, and LLM-KG hybrid approaches. We systematically analysed and compared\nthese approaches to provide a comprehensive overview highlighting key trends,\ninnovative techniques, and common challenges. This synthesis will benefit\nresearchers new to the field and those seeking to deepen their understanding of\nhow KGs and LLMs can be effectively combined to enhance AI applications\ncapabilities."
        ],
        "neg": []
    },
    {
        "query": "The use of large language models (LLMs) is expanding rapidly, and open-source\nversions are becoming available, offering users safer and more adaptable\noptions. These models enable users to protect data privacy by eliminating the\nneed to provide data to third parties and can be customized for specific tasks.\nIn this study, we compare the performance of various language models on the\nSustainable Development Goal (SDG) mapping task, using the output of GPT-4o as\nthe baseline. The selected open-source models for comparison include Mixtral,\nLLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more\nspecialized version of GPT-4o, was included to extend the comparison. Given the\nmulti-label nature of the SDG mapping task, we employed metrics such as F1\nscore, precision, and recall with micro-averaging to evaluate different aspects\nof the models' performance. These metrics are derived from the confusion matrix\nto ensure a comprehensive evaluation. We provide a clear observation and\nanalysis of each model's performance by plotting curves based on F1 score,\nprecision, and recall at different thresholds. According to the results of this\nexperiment, LLaMA 2 and Gemma still have significant room for improvement. The\nother four models do not exhibit particularly large differences in performance.\nThe outputs from all seven models are available on Zenodo:\nhttps://doi.org/10.5281/zenodo.12789375.",
        "pos": [
            "In recent years, Natural Language Processing (NLP) has played a significant\nrole in various Artificial Intelligence (AI) applications such as chatbots,\ntext generation, and language translation. The emergence of large language\nmodels (LLMs) has greatly improved the performance of these applications,\nshowing astonishing results in language understanding and generation. However,\nthey still show some disadvantages, such as hallucinations and lack of\ndomain-specific knowledge, that affect their performance in real-world tasks.\nThese issues can be effectively mitigated by incorporating knowledge graphs\n(KGs), which organise information in structured formats that capture\nrelationships between entities in a versatile and interpretable fashion.\nLikewise, the construction and validation of KGs present challenges that LLMs\ncan help resolve. The complementary relationship between LLMs and KGs has led\nto a trend that combines these technologies to achieve trustworthy results.\nThis work collected 28 papers outlining methods for KG-powered LLMs, LLM-based\nKGs, and LLM-KG hybrid approaches. We systematically analysed and compared\nthese approaches to provide a comprehensive overview highlighting key trends,\ninnovative techniques, and common challenges. This synthesis will benefit\nresearchers new to the field and those seeking to deepen their understanding of\nhow KGs and LLMs can be effectively combined to enhance AI applications\ncapabilities."
        ],
        "neg": []
    },
    {
        "query": "Existing generative retrieval (GR) approaches rely on training-based\nindexing, i.e., fine-tuning a model to memorise the associations between a\nquery and the document identifier (docid) of a relevant document.\nTraining-based indexing has three limitations: high training overhead,\nunder-utilization of the pre-trained knowledge of large language models (LLMs),\nand challenges in adapting to a dynamic document corpus. To address the above\nissues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).\nIt has a novel few-shot indexing process, where we prompt an LLM to generate\ndocids for all documents in a corpus, ultimately creating a docid bank for the\nentire corpus. During retrieval, we feed a query to the same LLM and constrain\nit to generate a docid within the docid bank created during indexing, and then\nmap the generated docid back to its corresponding document. Few-Shot GR relies\nsolely on prompting an LLM without requiring any training, making it more\nefficient. Moreover, we devise few-shot indexing with one-to-many mapping to\nfurther enhance Few-Shot GR. Experiments show that Few-Shot GR achieves\nsuperior performance to state-of-the-art GR methods that require heavy\ntraining.",
        "pos": [
            "The legal landscape encompasses a wide array of lawsuit types, presenting\nlawyers with challenges in delivering timely and accurate information to\nclients, particularly concerning critical aspects like potential imprisonment\nduration or financial repercussions. Compounded by the scarcity of legal\nexperts, there's an urgent need to enhance the efficiency of traditional legal\nworkflows. Recent advances in deep learning, especially Large Language Models\n(LLMs), offer promising solutions to this challenge. Leveraging LLMs'\nmathematical reasoning capabilities, we propose a novel approach integrating\nLLM-based methodologies with specially designed prompts to address precision\nrequirements in legal Artificial Intelligence (LegalAI) applications. The\nproposed work seeks to bridge the gap between traditional legal practices and\nmodern technological advancements, paving the way for a more accessible,\nefficient, and equitable legal system. To validate this method, we introduce a\ncurated dataset tailored to precision-oriented LegalAI tasks, serving as a\nbenchmark for evaluating LLM-based approaches. Extensive experimentation\nconfirms the efficacy of our methodology in generating accurate numerical\nestimates within the legal domain, emphasizing the role of LLMs in streamlining\nlegal processes and meeting the evolving demands of LegalAI.",
            "Large language models are extensively applied across a wide range of tasks,\nsuch as customer support, content creation, educational tutoring, and providing\nfinancial guidance. However, a well-known drawback is their predisposition to\ngenerate hallucinations. This damages the trustworthiness of the information\nthese models provide, impacting decision-making and user confidence. We propose\na method to detect hallucinations by looking at the structure of the latent\nspace and finding associations within hallucinated and non-hallucinated\ngenerations. We create a graph structure that connects generations that lie\nclosely in the embedding space. Moreover, we employ a Graph Attention Network\nwhich utilizes message passing to aggregate information from neighboring nodes\nand assigns varying degrees of importance to each neighbor based on their\nrelevance. Our findings show that 1) there exists a structure in the latent\nspace that differentiates between hallucinated and non-hallucinated\ngenerations, 2) Graph Attention Networks can learn this structure and\ngeneralize it to unseen generations, and 3) the robustness of our method is\nenhanced when incorporating contrastive learning. When evaluated against\nevidence-based benchmarks, our model performs similarly without access to\nsearch-based methods."
        ],
        "neg": []
    },
    {
        "query": "Existing generative retrieval (GR) approaches rely on training-based\nindexing, i.e., fine-tuning a model to memorise the associations between a\nquery and the document identifier (docid) of a relevant document.\nTraining-based indexing has three limitations: high training overhead,\nunder-utilization of the pre-trained knowledge of large language models (LLMs),\nand challenges in adapting to a dynamic document corpus. To address the above\nissues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).\nIt has a novel few-shot indexing process, where we prompt an LLM to generate\ndocids for all documents in a corpus, ultimately creating a docid bank for the\nentire corpus. During retrieval, we feed a query to the same LLM and constrain\nit to generate a docid within the docid bank created during indexing, and then\nmap the generated docid back to its corresponding document. Few-Shot GR relies\nsolely on prompting an LLM without requiring any training, making it more\nefficient. Moreover, we devise few-shot indexing with one-to-many mapping to\nfurther enhance Few-Shot GR. Experiments show that Few-Shot GR achieves\nsuperior performance to state-of-the-art GR methods that require heavy\ntraining.",
        "pos": [
            "Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative \"System 1\" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong \"System 2\" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have gained widespread global adoption,\nshowcasing advanced linguistic capabilities across multiple of languages. There\nis a growing interest in academia to use these models to simulate and study\nhuman behaviors. However, it is crucial to acknowledge that an LLM's\nproficiency in a specific language might not fully encapsulate the norms and\nvalues associated with its culture. Concerns have emerged regarding potential\nbiases towards Anglo-centric cultures and values due to the predominance of\nWestern and US-based training data. This study focuses on analyzing the\ncultural representations of emotions in LLMs, in the specific case of\nmixed-emotion situations. Our methodology is based on the studies of Miyamoto\net al. (2010), which identified distinctive emotional indicators in Japanese\nand American human responses. We first administer their mixed emotion survey to\nfive different LLMs and analyze their outputs. Second, we experiment with\ncontextual variables to explore variations in responses considering both\nlanguage and speaker origin. Thirdly, we expand our investigation to encompass\nadditional East Asian and Western European origin languages to gauge their\nalignment with their respective cultures, anticipating a closer fit. We find\nthat (1) models have limited alignment with the evidence in the literature; (2)\nwritten language has greater effect on LLMs' response than information on\nparticipants origin; and (3) LLMs responses were found more similar for East\nAsian languages than Western European languages.",
        "pos": [
            "Low-resource languages often face challenges in acquiring high-quality\nlanguage data due to the reliance on translation-based methods, which can\nintroduce the translationese effect. This phenomenon results in translated\nsentences that lack fluency and naturalness in the target language. In this\npaper, we propose a novel approach for data collection by leveraging\nstoryboards to elicit more fluent and natural sentences. Our method involves\npresenting native speakers with visual stimuli in the form of storyboards and\ncollecting their descriptions without direct exposure to the source text. We\nconducted a comprehensive evaluation comparing our storyboard-based approach\nwith traditional text translation-based methods in terms of accuracy and\nfluency. Human annotators and quantitative metrics were used to assess\ntranslation quality. The results indicate a preference for text translation in\nterms of accuracy, while our method demonstrates worse accuracy but better\nfluency in the language focused."
        ],
        "neg": []
    },
    {
        "query": "In-context learning (ICL) is a few-shot learning paradigm that involves\nlearning mappings through input-output pairs and appropriately applying them to\nnew instances. Despite the remarkable ICL capabilities demonstrated by Large\nLanguage Models (LLMs), existing works are highly dependent on large-scale\nlabeled support sets, not always feasible in practical scenarios. To refine\nthis approach, we focus primarily on an innovative selective annotation\nmechanism, which precedes the standard demonstration retrieval. We introduce\nthe Language Model-based Determinant Point Process (LM-DPP) that simultaneously\nconsiders the uncertainty and diversity of unlabeled instances for optimal\nselection. Consequently, this yields a subset for annotation that strikes a\ntrade-off between the two factors. We apply LM-DPP to various language models,\nincluding GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2\nGeneration datasets demonstrate that LM-DPP can effectively select canonical\nexamples. Further analysis reveals that LLMs benefit most significantly from\nsubsets that are both low uncertainty and high diversity.",
        "pos": [
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
            "Recently, various pre-trained language models (PLMs) have been proposed to\nprove their impressive performances on a wide range of few-shot tasks. However,\nlimited by the unstructured prior knowledge in PLMs, it is difficult to\nmaintain consistent performance on complex structured scenarios, such as\nhierarchical text classification (HTC), especially when the downstream data is\nextremely scarce. The main challenge is how to transfer the unstructured\nsemantic space in PLMs to the downstream domain hierarchy. Unlike previous work\non HTC which directly performs multi-label classification or uses graph neural\nnetwork (GNN) to inject label hierarchy, in this work, we study the HTC problem\nunder a few-shot setting to adapt knowledge in PLMs from an unstructured manner\nto the downstream hierarchy. Technically, we design a simple yet effective\nmethod named Hierarchical Iterative Conditional Random Field (HierICRF) to\nsearch the most domain-challenging directions and exquisitely crafts\ndomain-hierarchy adaptation as a hierarchical iterative language modeling\nproblem, and then it encourages the model to make hierarchical consistency\nself-correction during the inference, thereby achieving knowledge transfer with\nhierarchical consistency preservation. We perform HierICRF on various\narchitectures, and extensive experiments on two popular HTC datasets\ndemonstrate that prompt with HierICRF significantly boosts the few-shot HTC\nperformance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%\nto 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot\nsettings, while remaining SOTA hierarchical consistency performance.",
            "As Large Language Models (LLMs) are increasingly deployed to handle various\nnatural language processing (NLP) tasks, concerns regarding the potential\nnegative societal impacts of LLM-generated content have also arisen. To\nevaluate the biases exhibited by LLMs, researchers have recently proposed a\nvariety of datasets. However, existing bias evaluation efforts often focus on\nonly a particular type of bias and employ inconsistent evaluation metrics,\nleading to difficulties in comparison across different datasets and LLMs. To\naddress these limitations, we collect a variety of datasets designed for the\nbias evaluation of LLMs, and further propose CEB, a Compositional Evaluation\nBenchmark that covers different types of bias across different social groups\nand tasks. The curation of CEB is based on our newly proposed compositional\ntaxonomy, which characterizes each dataset from three dimensions: bias types,\nsocial groups, and tasks. By combining the three dimensions, we develop a\ncomprehensive evaluation strategy for the bias in LLMs. Our experiments\ndemonstrate that the levels of bias vary across these dimensions, thereby\nproviding guidance for the development of specific bias mitigation methods."
        ],
        "neg": []
    },
    {
        "query": "In-context learning (ICL) is a few-shot learning paradigm that involves\nlearning mappings through input-output pairs and appropriately applying them to\nnew instances. Despite the remarkable ICL capabilities demonstrated by Large\nLanguage Models (LLMs), existing works are highly dependent on large-scale\nlabeled support sets, not always feasible in practical scenarios. To refine\nthis approach, we focus primarily on an innovative selective annotation\nmechanism, which precedes the standard demonstration retrieval. We introduce\nthe Language Model-based Determinant Point Process (LM-DPP) that simultaneously\nconsiders the uncertainty and diversity of unlabeled instances for optimal\nselection. Consequently, this yields a subset for annotation that strikes a\ntrade-off between the two factors. We apply LM-DPP to various language models,\nincluding GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2\nGeneration datasets demonstrate that LM-DPP can effectively select canonical\nexamples. Further analysis reveals that LLMs benefit most significantly from\nsubsets that are both low uncertainty and high diversity.",
        "pos": [
            "Syntactic Transformer language models aim to achieve better generalization\nthrough simultaneously modeling syntax trees and sentences. While prior work\nhas been focusing on adding constituency-based structures to Transformers, we\nintroduce Dependency Transformer Grammars (DTGs), a new class of Transformer\nlanguage model with explicit dependency-based inductive bias. DTGs simulate\ndependency transition systems with constrained attention patterns by modifying\nattention masks, incorporate the stack information through relative positional\nencoding, and augment dependency arc representation with a combination of token\nembeddings and operation embeddings. When trained on a dataset of sentences\nannotated with dependency trees, DTGs achieve better generalization while\nmaintaining comparable perplexity with Transformer language model baselines.\nDTGs also outperform recent constituency-based models, showing that dependency\ncan better guide Transformer language models. Our code is released at\nhttps://github.com/zhaoyd1/Dep_Transformer_Grammars."
        ],
        "neg": []
    },
    {
        "query": "In-context learning (ICL) is a few-shot learning paradigm that involves\nlearning mappings through input-output pairs and appropriately applying them to\nnew instances. Despite the remarkable ICL capabilities demonstrated by Large\nLanguage Models (LLMs), existing works are highly dependent on large-scale\nlabeled support sets, not always feasible in practical scenarios. To refine\nthis approach, we focus primarily on an innovative selective annotation\nmechanism, which precedes the standard demonstration retrieval. We introduce\nthe Language Model-based Determinant Point Process (LM-DPP) that simultaneously\nconsiders the uncertainty and diversity of unlabeled instances for optimal\nselection. Consequently, this yields a subset for annotation that strikes a\ntrade-off between the two factors. We apply LM-DPP to various language models,\nincluding GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2\nGeneration datasets demonstrate that LM-DPP can effectively select canonical\nexamples. Further analysis reveals that LLMs benefit most significantly from\nsubsets that are both low uncertainty and high diversity.",
        "pos": [
            "We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.",
            "Named entity recognition (NER) models often struggle with noisy inputs, such\nas those with spelling mistakes or errors generated by Optical Character\nRecognition processes, and learning a robust NER model is challenging. Existing\nrobust NER models utilize both noisy text and its corresponding gold text for\ntraining, which is infeasible in many real-world applications in which gold\ntext is not available. In this paper, we consider a more realistic setting in\nwhich only noisy text and its NER labels are available. We propose to retrieve\nrelevant text of the noisy text from a knowledge corpus and use it to enhance\nthe representation of the original noisy input. We design three retrieval\nmethods: sparse retrieval based on lexicon similarity, dense retrieval based on\nsemantic similarity, and self-retrieval based on task-specific text. After\nretrieving relevant text, we concatenate the retrieved text with the original\nnoisy text and encode them with a transformer network, utilizing self-attention\nto enhance the contextual token representations of the noisy text using the\nretrieved text. We further employ a multi-view training framework that improves\nrobust NER without retrieving text during inference. Experiments show that our\nretrieval-augmented model achieves significant improvements in various noisy\nNER settings."
        ],
        "neg": []
    },
    {
        "query": "In-context learning (ICL) is a few-shot learning paradigm that involves\nlearning mappings through input-output pairs and appropriately applying them to\nnew instances. Despite the remarkable ICL capabilities demonstrated by Large\nLanguage Models (LLMs), existing works are highly dependent on large-scale\nlabeled support sets, not always feasible in practical scenarios. To refine\nthis approach, we focus primarily on an innovative selective annotation\nmechanism, which precedes the standard demonstration retrieval. We introduce\nthe Language Model-based Determinant Point Process (LM-DPP) that simultaneously\nconsiders the uncertainty and diversity of unlabeled instances for optimal\nselection. Consequently, this yields a subset for annotation that strikes a\ntrade-off between the two factors. We apply LM-DPP to various language models,\nincluding GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2\nGeneration datasets demonstrate that LM-DPP can effectively select canonical\nexamples. Further analysis reveals that LLMs benefit most significantly from\nsubsets that are both low uncertainty and high diversity.",
        "pos": [
            "Named entity recognition (NER) models often struggle with noisy inputs, such\nas those with spelling mistakes or errors generated by Optical Character\nRecognition processes, and learning a robust NER model is challenging. Existing\nrobust NER models utilize both noisy text and its corresponding gold text for\ntraining, which is infeasible in many real-world applications in which gold\ntext is not available. In this paper, we consider a more realistic setting in\nwhich only noisy text and its NER labels are available. We propose to retrieve\nrelevant text of the noisy text from a knowledge corpus and use it to enhance\nthe representation of the original noisy input. We design three retrieval\nmethods: sparse retrieval based on lexicon similarity, dense retrieval based on\nsemantic similarity, and self-retrieval based on task-specific text. After\nretrieving relevant text, we concatenate the retrieved text with the original\nnoisy text and encode them with a transformer network, utilizing self-attention\nto enhance the contextual token representations of the noisy text using the\nretrieved text. We further employ a multi-view training framework that improves\nrobust NER without retrieving text during inference. Experiments show that our\nretrieval-augmented model achieves significant improvements in various noisy\nNER settings."
        ],
        "neg": []
    },
    {
        "query": "Harnessing the power of Large Language Models (LLMs), this study explores the\nuse of three state-of-the-art LLMs, specifically GPT-3.5-turbo, LLaMA3-8B, and\nLLaMA3-70B, for crash severity inference, framing it as a classification task.\nWe generate textual narratives from original traffic crash tabular data using a\npre-built template infused with domain knowledge. Additionally, we incorporated\nChain-of-Thought (CoT) reasoning to guide the LLMs in analyzing the crash\ncauses and then inferring the severity. This study also examine the impact of\nprompt engineering specifically designed for crash severity inference. The LLMs\nwere tasked with crash severity inference to: (1) evaluate the models'\ncapabilities in crash severity analysis, (2) assess the effectiveness of CoT\nand domain-informed prompt engineering, and (3) examine the reasoning abilities\nwith the CoT framework. Our results showed that LLaMA3-70B consistently\noutperformed the other models, particularly in zero-shot settings. The CoT and\nPrompt Engineering techniques significantly enhanced performance, improving\nlogical reasoning and addressing alignment issues. Notably, the CoT offers\nvaluable insights into LLMs' reasoning processes, unleashing their capacity to\nconsider diverse factors such as environmental conditions, driver behavior, and\nvehicle characteristics in severity analysis and inference.",
        "pos": [
            "Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans, or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results caution against the simplistic application of\nLLMs in science education and highlight the importance of aligning LLM outputs\nwith human expectations to ensure efficient and accurate automatic scoring."
        ],
        "neg": []
    },
    {
        "query": "Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.",
        "pos": [
            "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has\nenhanced medical diagnosis. However, current Med-LVLMs frequently encounter\nfactual issues, often generating responses that do not align with established\nmedical facts. Retrieval-Augmented Generation (RAG), which utilizes external\nknowledge, can improve the factual accuracy of these models but introduces two\nmajor challenges. First, limited retrieved contexts might not cover all\nnecessary information, while excessive retrieval can introduce irrelevant and\ninaccurate references, interfering with the model's generation. Second, in\ncases where the model originally responds correctly, applying RAG can lead to\nan over-reliance on retrieved contexts, resulting in incorrect answers. To\naddress these issues, we propose RULE, which consists of two components. First,\nwe introduce a provably effective strategy for controlling factuality risk\nthrough the calibrated selection of the number of retrieved contexts. Second,\nbased on samples where over-reliance on retrieved contexts led to errors, we\ncurate a preference dataset to fine-tune the model, balancing its dependence on\ninherent knowledge and retrieved contexts for generation. We demonstrate the\neffectiveness of RULE on three medical VQA datasets, achieving an average\nimprovement of 20.8% in factual accuracy. We publicly release our benchmark and\ncode in https://github.com/richard-peng-xia/RULE."
        ],
        "neg": []
    },
    {
        "query": "Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.",
        "pos": [
            "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
            "Multilingual pretraining for transfer learning significantly boosts the\nrobustness of low-resource monolingual ASR models. This study systematically\ninvestigates three main aspects: (a) the impact of transfer learning on model\nperformance during initial training or fine-tuning, (b) the influence of\ntransfer learning across dataset domains and languages, and (c) the effect on\nrare-word recognition compared to non-rare words. Our finding suggests that\nRNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word\nError Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across\nlanguages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8%\ncompared to monolingual baselines for MLS and in-house datasets. Out-of-domain\npretraining leads to 28% higher WERR than in-domain pretraining. Both rare and\nnon-rare words benefit, with rare words showing greater improvements with\nout-of-domain pretraining, and non-rare words with in-domain pretraining.",
            "The structures of RNA sequences play a vital role in various cellular\nprocesses, while existing genomic foundation models (FMs) have struggled with\nprecise sequence-structure alignment, due to the complexity of exponential\ncombinations of nucleotide bases. In this study, we introduce OmniGenome, a\nfoundation model that addresses this critical challenge of sequence-structure\nalignment in RNA FMs. OmniGenome bridges the sequences with secondary\nstructures using structure-contextualized modeling, enabling hard in-silico\ngenomic tasks that existing FMs cannot handle, e.g., RNA design tasks. The\nresults on two comprehensive genomic benchmarks show that OmniGenome achieves\nstate-of-the-art performance on complex RNA subtasks. For example, OmniGenome\nsolved 74% of complex puzzles, compared to SpliceBERT which solved only 3% of\nthe puzzles. Besides, OmniGenome solves most of the puzzles within $1$ hour,\nwhile the existing methods usually allocate $24$ hours for each puzzle.\nOverall, OmniGenome establishes wide genomic application cases and offers\nprofound insights into biological mechanisms from the perspective of\nsequence-structure alignment."
        ],
        "neg": []
    },
    {
        "query": "This research introduces the Multilevel Embedding Association Test (ML-EAT),\na method designed for interpretable and transparent measurement of intrinsic\nbias in language technologies. The ML-EAT addresses issues of ambiguity and\ndifficulty in interpreting the traditional EAT measurement by quantifying bias\nat three levels of increasing granularity: the differential association between\ntwo target concepts with two attribute concepts; the individual effect size of\neach target concept with two attribute concepts; and the association between\neach individual target concept and each individual attribute concept. Using the\nML-EAT, this research defines a taxonomy of EAT patterns describing the nine\npossible outcomes of an embedding association test, each of which is associated\nwith a unique EAT-Map, a novel four-quadrant visualization for interpreting the\nML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2\nlanguage models, and a CLIP language-and-image model shows that EAT patterns\nadd otherwise unobservable information about the component biases that make up\nan EAT; reveal the effects of prompting in zero-shot models; and can also\nidentify situations when cosine similarity is an ineffective metric, rendering\nan EAT unreliable. Our work contributes a method for rendering bias more\nobservable and interpretable, improving the transparency of computational\ninvestigations into human minds and societies.",
        "pos": [
            "Calls to use open generative language models in academic research have\nhighlighted the need for reproducibility and transparency in scientific\nresearch. However, the impact of generative AI extends well beyond academia, as\ncorporations and public interest organizations have begun integrating these\nmodels into their data science pipelines. We expand this lens to include the\nimpact of open models on organizations, focusing specifically on fact-checking\norganizations, which use AI to observe and analyze large volumes of circulating\nmisinformation, yet must also ensure the reproducibility and impartiality of\ntheir work. We wanted to understand where fact-checking organizations use open\nmodels in their data science pipelines; what motivates their use of open models\nor proprietary models; and how their use of open or proprietary models can\ninform research on the societal impact of generative AI. To answer these\nquestions, we conducted an interview study with N=24 professionals at 20\nfact-checking organizations on six continents. Based on these interviews, we\noffer a five-component conceptual model of where fact-checking organizations\nemploy generative AI to support or automate parts of their data science\npipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data\nDelivery, and Data Sharing. We then provide taxonomies of fact-checking\norganizations' motivations for using open models and the limitations that\nprevent them for further adopting open models, finding that they prefer open\nmodels for Organizational Autonomy, Data Privacy and Ownership, Application\nSpecificity, and Capability Transparency. However, they nonetheless use\nproprietary models due to perceived advantages in Performance, Usability, and\nSafety, as well as Opportunity Costs related to participation in emerging\ngenerative AI ecosystems. Our work provides novel perspective on open models in\ndata-driven organizations.",
            "Popular and news media often portray teenagers with sensationalism, as both a\nrisk to society and at risk from society. As AI begins to absorb some of the\nepistemic functions of traditional media, we study how teenagers in two\ncountries speaking two languages: 1) are depicted by AI, and 2) how they would\nprefer to be depicted. Specifically, we study the biases about teenagers\nlearned by static word embeddings (SWEs) and generative language models (GLMs),\ncomparing these with the perspectives of adolescents living in the U.S. and\nNepal. We find English-language SWEs associate teenagers with societal\nproblems, and more than 50% of the 1,000 words most associated with teenagers\nin the pretrained GloVe SWE reflect such problems. Given prompts about\nteenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss\nsocietal problems, most commonly violence, but also drug use, mental illness,\nand sexual taboo. Nepali models, while not free of such associations, are less\ndominated by social problems. Data from workshops with N=13 U.S. adolescents\nand N=18 Nepalese adolescents show that AI presentations are disconnected from\nteenage life, which revolves around activities like school and friendship.\nParticipant ratings of how well 20 trait words describe teens are decorrelated\nfrom SWE associations, with Pearson's r=.02, n.s. in English FastText and\nr=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in\nGloVe. U.S. participants suggested AI could fairly present teens by\nhighlighting diversity, while Nepalese participants centered positivity.\nParticipants were optimistic that, if it learned from adolescents, rather than\nmedia sources, AI could help mitigate stereotypes. Our work offers an\nunderstanding of the ways SWEs and GLMs misrepresent a developmentally\nvulnerable group and provides a template for less sensationalized\ncharacterization.",
            "Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting."
        ],
        "neg": []
    },
    {
        "query": "This research introduces the Multilevel Embedding Association Test (ML-EAT),\na method designed for interpretable and transparent measurement of intrinsic\nbias in language technologies. The ML-EAT addresses issues of ambiguity and\ndifficulty in interpreting the traditional EAT measurement by quantifying bias\nat three levels of increasing granularity: the differential association between\ntwo target concepts with two attribute concepts; the individual effect size of\neach target concept with two attribute concepts; and the association between\neach individual target concept and each individual attribute concept. Using the\nML-EAT, this research defines a taxonomy of EAT patterns describing the nine\npossible outcomes of an embedding association test, each of which is associated\nwith a unique EAT-Map, a novel four-quadrant visualization for interpreting the\nML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2\nlanguage models, and a CLIP language-and-image model shows that EAT patterns\nadd otherwise unobservable information about the component biases that make up\nan EAT; reveal the effects of prompting in zero-shot models; and can also\nidentify situations when cosine similarity is an ineffective metric, rendering\nan EAT unreliable. Our work contributes a method for rendering bias more\nobservable and interpretable, improving the transparency of computational\ninvestigations into human minds and societies.",
        "pos": [
            "Popular and news media often portray teenagers with sensationalism, as both a\nrisk to society and at risk from society. As AI begins to absorb some of the\nepistemic functions of traditional media, we study how teenagers in two\ncountries speaking two languages: 1) are depicted by AI, and 2) how they would\nprefer to be depicted. Specifically, we study the biases about teenagers\nlearned by static word embeddings (SWEs) and generative language models (GLMs),\ncomparing these with the perspectives of adolescents living in the U.S. and\nNepal. We find English-language SWEs associate teenagers with societal\nproblems, and more than 50% of the 1,000 words most associated with teenagers\nin the pretrained GloVe SWE reflect such problems. Given prompts about\nteenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss\nsocietal problems, most commonly violence, but also drug use, mental illness,\nand sexual taboo. Nepali models, while not free of such associations, are less\ndominated by social problems. Data from workshops with N=13 U.S. adolescents\nand N=18 Nepalese adolescents show that AI presentations are disconnected from\nteenage life, which revolves around activities like school and friendship.\nParticipant ratings of how well 20 trait words describe teens are decorrelated\nfrom SWE associations, with Pearson's r=.02, n.s. in English FastText and\nr=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in\nGloVe. U.S. participants suggested AI could fairly present teens by\nhighlighting diversity, while Nepalese participants centered positivity.\nParticipants were optimistic that, if it learned from adolescents, rather than\nmedia sources, AI could help mitigate stereotypes. Our work offers an\nunderstanding of the ways SWEs and GLMs misrepresent a developmentally\nvulnerable group and provides a template for less sensationalized\ncharacterization.",
            "Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting."
        ],
        "neg": []
    },
    {
        "query": "This research introduces the Multilevel Embedding Association Test (ML-EAT),\na method designed for interpretable and transparent measurement of intrinsic\nbias in language technologies. The ML-EAT addresses issues of ambiguity and\ndifficulty in interpreting the traditional EAT measurement by quantifying bias\nat three levels of increasing granularity: the differential association between\ntwo target concepts with two attribute concepts; the individual effect size of\neach target concept with two attribute concepts; and the association between\neach individual target concept and each individual attribute concept. Using the\nML-EAT, this research defines a taxonomy of EAT patterns describing the nine\npossible outcomes of an embedding association test, each of which is associated\nwith a unique EAT-Map, a novel four-quadrant visualization for interpreting the\nML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2\nlanguage models, and a CLIP language-and-image model shows that EAT patterns\nadd otherwise unobservable information about the component biases that make up\nan EAT; reveal the effects of prompting in zero-shot models; and can also\nidentify situations when cosine similarity is an ineffective metric, rendering\nan EAT unreliable. Our work contributes a method for rendering bias more\nobservable and interpretable, improving the transparency of computational\ninvestigations into human minds and societies.",
        "pos": [
            "Popular and news media often portray teenagers with sensationalism, as both a\nrisk to society and at risk from society. As AI begins to absorb some of the\nepistemic functions of traditional media, we study how teenagers in two\ncountries speaking two languages: 1) are depicted by AI, and 2) how they would\nprefer to be depicted. Specifically, we study the biases about teenagers\nlearned by static word embeddings (SWEs) and generative language models (GLMs),\ncomparing these with the perspectives of adolescents living in the U.S. and\nNepal. We find English-language SWEs associate teenagers with societal\nproblems, and more than 50% of the 1,000 words most associated with teenagers\nin the pretrained GloVe SWE reflect such problems. Given prompts about\nteenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss\nsocietal problems, most commonly violence, but also drug use, mental illness,\nand sexual taboo. Nepali models, while not free of such associations, are less\ndominated by social problems. Data from workshops with N=13 U.S. adolescents\nand N=18 Nepalese adolescents show that AI presentations are disconnected from\nteenage life, which revolves around activities like school and friendship.\nParticipant ratings of how well 20 trait words describe teens are decorrelated\nfrom SWE associations, with Pearson's r=.02, n.s. in English FastText and\nr=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in\nGloVe. U.S. participants suggested AI could fairly present teens by\nhighlighting diversity, while Nepalese participants centered positivity.\nParticipants were optimistic that, if it learned from adolescents, rather than\nmedia sources, AI could help mitigate stereotypes. Our work offers an\nunderstanding of the ways SWEs and GLMs misrepresent a developmentally\nvulnerable group and provides a template for less sensationalized\ncharacterization.",
            "Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.",
            "Equitable urban transportation applications require high-fidelity digital\nrepresentations of the built environment: not just streets and sidewalks, but\nbike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions,\ntraffic signals, signage, street markings, potholes, and more. Direct\ninspections and manual annotations are prohibitively expensive at scale.\nConventional machine learning methods require substantial annotated training\ndata for adequate performance. In this paper, we consider vision language\nmodels as a mechanism for annotating diverse urban features from satellite\nimages, reducing the dependence on human annotation to produce large training\nsets. While these models have achieved impressive results in describing common\nobjects in images captured from a human perspective, their training sets are\nless likely to include strong signals for esoteric features in the built\nenvironment, and their performance in these settings is therefore unclear. We\ndemonstrate proof-of-concept combining a state-of-the-art vision language model\nand variants of a prompting strategy that asks the model to consider segmented\nelements independently of the original image. Experiments on two urban features\n-- stop lines and raised tables -- show that while direct zero-shot prompting\ncorrectly annotates nearly zero images, the pre-segmentation strategies can\nannotate images with near 40% intersection-over-union accuracy. We describe how\nthese results inform a new research agenda in automatic annotation of the built\nenvironment to improve equity, accessibility, and safety at broad scale and in\ndiverse environments.",
            "Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future work, centered\naround whether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, while still providing opportunities to optimize\nabstention abilities based on context."
        ],
        "neg": []
    },
    {
        "query": "We evaluate the robustness of several large language models on multiple\ndatasets. Robustness here refers to the relative insensitivity of the model's\nanswers to meaning-preserving variants of their input. Benchmark datasets are\nconstructed by introducing naturally-occurring, non-malicious perturbations, or\nby generating semantically equivalent paraphrases of input questions or\nstatements. We further propose a novel metric for assessing a model robustness,\nand demonstrate its benefits in the non-adversarial scenario by empirical\nevaluation of several models on the created datasets.",
        "pos": [
            "We present a novel approach to automating the identification of risk factors\nfor diseases from medical literature, leveraging pre-trained models in the\nbio-medical domain, while tuning them for the specific task. Faced with the\nchallenges of the diverse and unstructured nature of medical articles, our\nstudy introduces a multi-step system to first identify relevant articles, then\nclassify them based on the presence of risk factor discussions and, finally,\nextract specific risk factor information for a disease through a\nquestion-answering model.\n  Our contributions include the development of a comprehensive pipeline for the\nautomated extraction of risk factors and the compilation of several datasets,\nwhich can serve as valuable resources for further research in this area. These\ndatasets encompass a wide range of diseases, as well as their associated risk\nfactors, meticulously identified and validated through a fine-grained\nevaluation scheme. We conducted both automatic and thorough manual evaluation,\ndemonstrating encouraging results. We also highlight the importance of\nimproving models and expanding dataset comprehensiveness to keep pace with the\nrapidly evolving field of medical research."
        ],
        "neg": []
    },
    {
        "query": "Calls to use open generative language models in academic research have\nhighlighted the need for reproducibility and transparency in scientific\nresearch. However, the impact of generative AI extends well beyond academia, as\ncorporations and public interest organizations have begun integrating these\nmodels into their data science pipelines. We expand this lens to include the\nimpact of open models on organizations, focusing specifically on fact-checking\norganizations, which use AI to observe and analyze large volumes of circulating\nmisinformation, yet must also ensure the reproducibility and impartiality of\ntheir work. We wanted to understand where fact-checking organizations use open\nmodels in their data science pipelines; what motivates their use of open models\nor proprietary models; and how their use of open or proprietary models can\ninform research on the societal impact of generative AI. To answer these\nquestions, we conducted an interview study with N=24 professionals at 20\nfact-checking organizations on six continents. Based on these interviews, we\noffer a five-component conceptual model of where fact-checking organizations\nemploy generative AI to support or automate parts of their data science\npipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data\nDelivery, and Data Sharing. We then provide taxonomies of fact-checking\norganizations' motivations for using open models and the limitations that\nprevent them for further adopting open models, finding that they prefer open\nmodels for Organizational Autonomy, Data Privacy and Ownership, Application\nSpecificity, and Capability Transparency. However, they nonetheless use\nproprietary models due to perceived advantages in Performance, Usability, and\nSafety, as well as Opportunity Costs related to participation in emerging\ngenerative AI ecosystems. Our work provides novel perspective on open models in\ndata-driven organizations.",
        "pos": [
            "This study introduces ValueScope, a framework leveraging language models to\nquantify social norms and values within online communities, grounded in social\nscience perspectives on normative structures. We employ ValueScope to dissect\nand analyze linguistic and stylistic expressions across 13 Reddit communities\ncategorized under gender, politics, science, and finance. Our analysis provides\na quantitative foundation showing that even closely related communities exhibit\nremarkably diverse norms. This diversity supports existing theories and adds a\nnew dimension--community preference--to understanding community interactions.\nValueScope not only delineates differing social norms among communities but\nalso effectively traces their evolution and the influence of significant\nexternal events like the U.S. presidential elections and the emergence of new\nsub-communities. The framework thus highlights the pivotal role of social norms\nin shaping online interactions, presenting a substantial advance in both the\ntheory and application of social norm studies in digital spaces."
        ],
        "neg": []
    },
    {
        "query": "Popular and news media often portray teenagers with sensationalism, as both a\nrisk to society and at risk from society. As AI begins to absorb some of the\nepistemic functions of traditional media, we study how teenagers in two\ncountries speaking two languages: 1) are depicted by AI, and 2) how they would\nprefer to be depicted. Specifically, we study the biases about teenagers\nlearned by static word embeddings (SWEs) and generative language models (GLMs),\ncomparing these with the perspectives of adolescents living in the U.S. and\nNepal. We find English-language SWEs associate teenagers with societal\nproblems, and more than 50% of the 1,000 words most associated with teenagers\nin the pretrained GloVe SWE reflect such problems. Given prompts about\nteenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss\nsocietal problems, most commonly violence, but also drug use, mental illness,\nand sexual taboo. Nepali models, while not free of such associations, are less\ndominated by social problems. Data from workshops with N=13 U.S. adolescents\nand N=18 Nepalese adolescents show that AI presentations are disconnected from\nteenage life, which revolves around activities like school and friendship.\nParticipant ratings of how well 20 trait words describe teens are decorrelated\nfrom SWE associations, with Pearson's r=.02, n.s. in English FastText and\nr=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in\nGloVe. U.S. participants suggested AI could fairly present teens by\nhighlighting diversity, while Nepalese participants centered positivity.\nParticipants were optimistic that, if it learned from adolescents, rather than\nmedia sources, AI could help mitigate stereotypes. Our work offers an\nunderstanding of the ways SWEs and GLMs misrepresent a developmentally\nvulnerable group and provides a template for less sensationalized\ncharacterization.",
        "pos": [
            "Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.",
        "pos": [
            "Large Language Models (LLMs) exhibit various emergent abilities. Among these\nabilities, some might reveal the internal working mechanisms of models. In this\npaper, we uncover a novel emergent capability in models: the intrinsic ability\nto perform extended sequences of calculations without relying on\nchain-of-thought step-by-step solutions. Remarkably, the most advanced models\ncan directly output the results of two-digit number additions with lengths\nextending up to 15 addends. We hypothesize that the model emerges Implicit\nDiscrete State Representations (IDSRs) within its hidden states and performs\nsymbolic calculations internally. To test this hypothesis, we design a sequence\nof experiments that look into the hidden states. Specifically, we first confirm\nthat IDSRs exist. Then, we provide interesting observations about the formation\nof IDSRs from layer, digit, and sequence perspectives. Finally, we confirm that\nmodels indeed use IDSRs to produce the final answers. However, we also discover\nthat these state representations are far from lossless in current open-sourced\nmodels, leading to inaccuracies in their final performance. Our work presents a\nnovel exploration of LLMs' symbolic calculation abilities and the underlying\nmechanisms."
        ],
        "neg": []
    },
    {
        "query": "Query intent classification is an essential module for customers to find\ndesired products on the e-commerce application quickly. Most existing query\nintent classification methods rely on the users' click behavior as a supervised\nsignal to construct training samples. However, these methods based entirely on\nposterior labels may lead to serious category imbalance problems because of the\nMatthew effect in click samples. Compared with popular categories, it is\ndifficult for products under long-tail categories to obtain traffic and user\nclicks, which makes the models unable to detect users' intent for products\nunder long-tail categories. This in turn aggravates the problem that long-tail\ncategories cannot obtain traffic, forming a vicious circle. In addition, due to\nthe randomness of the user's click, the posterior label is unstable for the\nquery with similar semantics, which makes the model very sensitive to the\ninput, leading to an unstable and incomplete recall of categories.\n  In this paper, we propose a novel Semi-supervised Multi-channel Graph\nConvolutional Network (SMGCN) to address the above problems from the\nperspective of label association and semi-supervised learning. SMGCN extends\ncategory information and enhances the posterior label by utilizing the\nsimilarity score between the query and categories. Furthermore, it leverages\nthe co-occurrence and semantic similarity graph of categories to strengthen the\nrelations among labels and weaken the influence of posterior label instability.\nWe conduct extensive offline and online A/B experiments, and the experimental\nresults show that SMGCN significantly outperforms the strong baselines, which\nshows its effectiveness and practicality.",
        "pos": [
            "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.",
            "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%.\nFinally, we demonstrate the maturity of IncogniText for real-world applications\nby distilling its anonymization capability into a set of LoRA parameters\nassociated with an on-device model.",
            "The latest and most impactful advances in large models stem from their\nincreased size. Unfortunately, this translates into an improved memorization\ncapacity, raising data privacy concerns. Specifically, it has been shown that\nmodels can output personal identifiable information (PII) contained in their\ntraining data. However, reported PIII extraction performance varies widely, and\nthere is no consensus on the optimal methodology to evaluate this risk,\nresulting in underestimating realistic adversaries. In this work, we\nempirically demonstrate that it is possible to improve the extractability of\nPII by over ten-fold by grounding the prefix of the manually constructed\nextraction prompt with in-domain data. Our approach, PII-Compass, achieves\nphone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308\nqueries, respectively, i.e., the phone number of 1 person in 15 is extractable."
        ],
        "neg": []
    },
    {
        "query": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
        "pos": [
            "Imposing constraints on machine translation systems presents a challenging\nissue because these systems are not trained to make use of constraints in\ngenerating adequate, fluent translations. In this paper, we leverage the\ncapabilities of large language models (LLMs) for constrained translation, given\nthat LLMs can easily adapt to this task by taking translation instructions and\nconstraints as prompts. However, LLMs cannot always guarantee the adequacy of\ntranslation, and, in some cases, ignore the given constraints. This is in part\nbecause LLMs might be overly confident in their predictions, overriding the\ninfluence of the constraints. To overcome this overiding behaviour, we propose\nto add a revision process that encourages LLMs to correct the outputs by\nprompting them about the constraints that have not yet been met. We evaluate\nour approach on four constrained translation tasks, encompassing both lexical\nand structural constraints in multiple constraint domains. Experiments show\n15\\% improvement in constraint-based translation accuracy over standard LLMs\nand the approach also significantly outperforms neural machine translation\n(NMT) state-of-the-art methods."
        ],
        "neg": []
    },
    {
        "query": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
        "pos": [
            "Imposing constraints on machine translation systems presents a challenging\nissue because these systems are not trained to make use of constraints in\ngenerating adequate, fluent translations. In this paper, we leverage the\ncapabilities of large language models (LLMs) for constrained translation, given\nthat LLMs can easily adapt to this task by taking translation instructions and\nconstraints as prompts. However, LLMs cannot always guarantee the adequacy of\ntranslation, and, in some cases, ignore the given constraints. This is in part\nbecause LLMs might be overly confident in their predictions, overriding the\ninfluence of the constraints. To overcome this overiding behaviour, we propose\nto add a revision process that encourages LLMs to correct the outputs by\nprompting them about the constraints that have not yet been met. We evaluate\nour approach on four constrained translation tasks, encompassing both lexical\nand structural constraints in multiple constraint domains. Experiments show\n15\\% improvement in constraint-based translation accuracy over standard LLMs\nand the approach also significantly outperforms neural machine translation\n(NMT) state-of-the-art methods."
        ],
        "neg": []
    },
    {
        "query": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
        "pos": [
            "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
            "Imposing constraints on machine translation systems presents a challenging\nissue because these systems are not trained to make use of constraints in\ngenerating adequate, fluent translations. In this paper, we leverage the\ncapabilities of large language models (LLMs) for constrained translation, given\nthat LLMs can easily adapt to this task by taking translation instructions and\nconstraints as prompts. However, LLMs cannot always guarantee the adequacy of\ntranslation, and, in some cases, ignore the given constraints. This is in part\nbecause LLMs might be overly confident in their predictions, overriding the\ninfluence of the constraints. To overcome this overiding behaviour, we propose\nto add a revision process that encourages LLMs to correct the outputs by\nprompting them about the constraints that have not yet been met. We evaluate\nour approach on four constrained translation tasks, encompassing both lexical\nand structural constraints in multiple constraint domains. Experiments show\n15\\% improvement in constraint-based translation accuracy over standard LLMs\nand the approach also significantly outperforms neural machine translation\n(NMT) state-of-the-art methods."
        ],
        "neg": []
    },
    {
        "query": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
        "pos": [
            "Imposing constraints on machine translation systems presents a challenging\nissue because these systems are not trained to make use of constraints in\ngenerating adequate, fluent translations. In this paper, we leverage the\ncapabilities of large language models (LLMs) for constrained translation, given\nthat LLMs can easily adapt to this task by taking translation instructions and\nconstraints as prompts. However, LLMs cannot always guarantee the adequacy of\ntranslation, and, in some cases, ignore the given constraints. This is in part\nbecause LLMs might be overly confident in their predictions, overriding the\ninfluence of the constraints. To overcome this overiding behaviour, we propose\nto add a revision process that encourages LLMs to correct the outputs by\nprompting them about the constraints that have not yet been met. We evaluate\nour approach on four constrained translation tasks, encompassing both lexical\nand structural constraints in multiple constraint domains. Experiments show\n15\\% improvement in constraint-based translation accuracy over standard LLMs\nand the approach also significantly outperforms neural machine translation\n(NMT) state-of-the-art methods."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in large language models (LLMs) have enabled autonomous\nagents with complex reasoning and task-fulfillment capabilities using a wide\nrange of tools. However, effectively identifying the most relevant tools for a\ngiven task becomes a key bottleneck as the toolset size grows, hindering\nreliable tool utilization. To address this, we introduce Re-Invoke, an\nunsupervised tool retrieval method designed to scale effectively to large\ntoolsets without training. Specifically, we first generate a diverse set of\nsynthetic queries that comprehensively cover different aspects of the query\nspace associated with each tool document during the tool indexing phase.\nSecond, we leverage LLM's query understanding capabilities to extract key\ntool-related context and underlying intents from user queries during the\ninference phase. Finally, we employ a novel multi-view similarity ranking\nstrategy based on intents to pinpoint the most relevant tools for each query.\nOur evaluation demonstrates that Re-Invoke significantly outperforms\nstate-of-the-art alternatives in both single-tool and multi-tool scenarios, all\nwithin a fully unsupervised setting. Notably, on the ToolE datasets, we achieve\na 20% relative improvement in nDCG@5 for single-tool retrieval and a 39%\nimprovement for multi-tool retrieval.",
        "pos": [
            "Embeddings from Large Language Models (LLMs) have emerged as critical\ncomponents in various applications, particularly for information retrieval.\nWhile high-dimensional embeddings generally demonstrate superior performance as\nthey contain more salient information, their practical application is\nfrequently hindered by elevated computational latency and the associated higher\ncost. To address these challenges, we propose Matryoshka-Adaptor, a novel\ntuning framework designed for the customization of LLM embeddings.\nMatryoshka-Adaptor facilitates substantial dimensionality reduction while\nmaintaining comparable performance levels, thereby achieving a significant\nenhancement in computational efficiency and cost-effectiveness. Our framework\ndirectly modifies the embeddings from pre-trained LLMs which is designed to be\nseamlessly integrated with any LLM architecture, encompassing those accessible\nexclusively through black-box APIs. Also, it exhibits efficacy in both\nunsupervised and supervised learning settings. A rigorous evaluation conducted\nacross a diverse corpus of English, multilingual, and multimodal datasets\nconsistently reveals substantial gains with Matryoshka-Adaptor. Notably, with\nGoogle and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in\ndimensionality ranging from two- to twelve-fold without compromising\nperformance across multiple BEIR datasets.",
            "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in large language models (LLMs) have enabled autonomous\nagents with complex reasoning and task-fulfillment capabilities using a wide\nrange of tools. However, effectively identifying the most relevant tools for a\ngiven task becomes a key bottleneck as the toolset size grows, hindering\nreliable tool utilization. To address this, we introduce Re-Invoke, an\nunsupervised tool retrieval method designed to scale effectively to large\ntoolsets without training. Specifically, we first generate a diverse set of\nsynthetic queries that comprehensively cover different aspects of the query\nspace associated with each tool document during the tool indexing phase.\nSecond, we leverage LLM's query understanding capabilities to extract key\ntool-related context and underlying intents from user queries during the\ninference phase. Finally, we employ a novel multi-view similarity ranking\nstrategy based on intents to pinpoint the most relevant tools for each query.\nOur evaluation demonstrates that Re-Invoke significantly outperforms\nstate-of-the-art alternatives in both single-tool and multi-tool scenarios, all\nwithin a fully unsupervised setting. Notably, on the ToolE datasets, we achieve\na 20% relative improvement in nDCG@5 for single-tool retrieval and a 39%\nimprovement for multi-tool retrieval.",
        "pos": [
            "Retrieval augmented generation (RAG) combines the generative abilities of\nlarge language models (LLMs) with external knowledge sources to provide more\naccurate and up-to-date responses. Recent RAG advancements focus on improving\nretrieval outcomes through iterative LLM refinement or self-critique\ncapabilities acquired through additional instruction tuning of LLMs. In this\nwork, we introduce Speculative RAG - a framework that leverages a larger\ngeneralist LM to efficiently verify multiple RAG drafts produced in parallel by\na smaller, distilled specialist LM. Each draft is generated from a distinct\nsubset of retrieved documents, offering diverse perspectives on the evidence\nwhile reducing input token counts per draft. This approach enhances\ncomprehension of each subset and mitigates potential position bias over long\ncontext. Our method accelerates RAG by delegating drafting to the smaller\nspecialist LM, with the larger generalist LM performing a single verification\npass over the drafts. Extensive experiments demonstrate that Speculative RAG\nachieves state-of-the-art performance with reduced latency on TriviaQA,\nMuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy\nby up to 12.97% while reducing latency by 51% compared to conventional RAG\nsystems on PubHealth."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and analyzing lengthy sequential inputs, owing to their extensive\ncontext windows that allow processing millions of tokens in a single forward\npass. However, this paper uncovers a surprising limitation: LLMs fall short\nwhen handling long input sequences. We investigate this issue using three\ndatasets and two tasks (sentiment analysis and news categorization) across\nvarious LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,\nand Mistral Instruct models. To address this limitation, we propose and\nevaluate ad-hoc solutions that substantially enhance LLMs' performance on long\ninput sequences by up to 50%, while reducing API cost and latency by up to 93%\nand 50%, respectively.",
        "pos": [
            "How similar are politicians to those who vote for them? This is a critical\nquestion at the heart of democratic representation and particularly relevant at\ntimes when political dissatisfaction and populism are on the rise. To answer\nthis question we compare the online discourse of elected politicians and their\nconstituents. We collect a two and a half years (September 2020 - February\n2023) constituency-level dataset for USA and UK that includes: (i) the Twitter\ntimelines (5.6 Million tweets) of elected political representatives (595 UK\nMembers of Parliament and 433 USA Representatives), (ii) the Nextdoor posts\n(21.8 Million posts) of the constituency (98.4% USA and 91.5% UK\nconstituencies). We find that elected politicians tend to be equally similar to\ntheir constituents in terms of content and style regardless of whether a\nconstituency elects a right or left-wing politician. The size of the electoral\nvictory and the level of income of a constituency shows a nuanced picture. The\nnarrower the electoral victory, the more similar the style and the more\ndissimilar the content is. The lower the income of a constituency, the more\nsimilar the content is. In terms of style, poorer constituencies tend to have a\nmore similar sentiment and more dissimilar psychological text traits (i.e.\nmeasured with LIWC categories)."
        ],
        "neg": []
    },
    {
        "query": "Objective: This study aims to develop and validate an evaluation framework to\nensure the safety and reliability of mental health chatbots, which are\nincreasingly popular due to their accessibility, human-like interactions, and\ncontext-aware support. Materials and Methods: We created an evaluation\nframework with 100 benchmark questions and ideal responses, and five guideline\nquestions for chatbot responses. This framework, validated by mental health\nexperts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation\nmethods explored included large language model (LLM)-based scoring, an agentic\napproach using real-time data, and embedding models to compare chatbot\nresponses against ground truth standards. Results: The results highlight the\nimportance of guidelines and ground truth for improving LLM evaluation\naccuracy. The agentic method, dynamically accessing reliable information,\ndemonstrated the best alignment with human assessments. Adherence to a\nstandardized, expert-validated framework significantly enhanced chatbot\nresponse safety and reliability. Discussion: Our findings emphasize the need\nfor comprehensive, expert-tailored safety evaluation metrics for mental health\nchatbots. While LLMs have significant potential, careful implementation is\nnecessary to mitigate risks. The superior performance of the agentic approach\nunderscores the importance of real-time data access in enhancing chatbot\nreliability. Conclusion: The study validated an evaluation framework for mental\nhealth chatbots, proving its effectiveness in improving safety and reliability.\nFuture work should extend evaluations to accuracy, bias, empathy, and privacy\nto ensure holistic assessment and responsible integration into healthcare.\nStandardized evaluations will build trust among users and professionals,\nfacilitating broader adoption and improved mental health support through\ntechnology.",
        "pos": [
            "Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large\nfoundation models, leveraging low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$\nto represent weight changes (\\textit{i.e.,} $\\Delta \\mathbf{W} = \\mathbf{B}\n\\mathbf{A}$). This method reduces trainable parameters and mitigates heavy\nmemory consumption associated with full delta matrices by sequentially\nmultiplying $\\mathbf{A}$ and $\\mathbf{B}$ with the activation. Despite its\nsuccess, the intrinsic low-rank characteristic may limit its performance.\nAlthough several variants have been proposed to address this issue, they often\noverlook the crucial computational and memory efficiency brought by LoRA. In\nthis paper, we propose \\underline{C}ir\\underline{c}ular \\underline{C}onvolution\n\\underline{A}daptation (C$^3$A), which not only achieves high-rank adaptation\nwith enhanced performance but also excels in both computational power and\nmemory utilization. Extensive experiments demonstrate that C$^3$A consistently\noutperforms LoRA and its variants across various fine-tuning tasks.",
            "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
            "The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench."
        ],
        "neg": []
    },
    {
        "query": "Stance detection is an active task in natural language processing (NLP) that\naims to identify the author's stance towards a particular target within a text.\nGiven the remarkable language understanding capabilities and encyclopedic prior\nknowledge of large language models (LLMs), how to explore the potential of LLMs\nin stance detection has received significant attention. Unlike existing\nLLM-based approaches that focus solely on fine-tuning with large-scale\ndatasets, we propose a new prompting method, called \\textit{Chain of Stance}\n(CoS). In particular, it positions LLMs as expert stance detectors by\ndecomposing the stance detection process into a series of intermediate,\nstance-related assertions that culminate in the final judgment. This approach\nleads to significant improvements in classification performance. We conducted\nextensive experiments using four SOTA LLMs on the SemEval 2016 dataset,\ncovering the zero-shot and few-shot learning setups. The results indicate that\nthe proposed method achieves state-of-the-art results with an F1 score of 79.84\nin the few-shot setting.",
        "pos": [
            "Personalized Dialogue Generation (PDG) aims to create coherent responses\naccording to roles or personas. Traditional PDG relies on external role data,\nwhich can be scarce and raise privacy concerns. Approaches address these issues\nby extracting role information from dialogue history, which often fail to\ngenerically model roles in continuous space. To overcome these limitations, we\nintroduce a novel framework \\textbf{MO}dels \\textbf{R}oles from\n\\textbf{P}ersonalized Dialogue \\textbf{H}istory by \\textbf{E}xploring and\n\\textbf{U}tilizing Latent \\textbf{S}pace (MORPHEUS) through a three-stage\ntraining process. Specifically, we create a persona codebook to represent roles\nin latent space compactly, and this codebook is used to construct a posterior\ndistribution of role information. This method enables the model to generalize\nacross roles, allowing the generation of personalized dialogues even for unseen\nroles. Experiments on both Chinese and English datasets demonstrate that\nMORPHEUS enhances the extraction of role information, and improves response\ngeneration without external role data. Additionally, MORPHEUS can be considered\nan efficient fine-tuning for large language models."
        ],
        "neg": []
    },
    {
        "query": "Stance detection is an active task in natural language processing (NLP) that\naims to identify the author's stance towards a particular target within a text.\nGiven the remarkable language understanding capabilities and encyclopedic prior\nknowledge of large language models (LLMs), how to explore the potential of LLMs\nin stance detection has received significant attention. Unlike existing\nLLM-based approaches that focus solely on fine-tuning with large-scale\ndatasets, we propose a new prompting method, called \\textit{Chain of Stance}\n(CoS). In particular, it positions LLMs as expert stance detectors by\ndecomposing the stance detection process into a series of intermediate,\nstance-related assertions that culminate in the final judgment. This approach\nleads to significant improvements in classification performance. We conducted\nextensive experiments using four SOTA LLMs on the SemEval 2016 dataset,\ncovering the zero-shot and few-shot learning setups. The results indicate that\nthe proposed method achieves state-of-the-art results with an F1 score of 79.84\nin the few-shot setting.",
        "pos": [
            "Elaborating a series of intermediate reasoning steps significantly improves\nthe ability of large language models (LLMs) to solve complex problems, as such\nsteps would evoke LLMs to think sequentially. However, human sarcasm\nunderstanding is often considered an intuitive and holistic cognitive process,\nin which various linguistic, contextual, and emotional cues are integrated to\nform a comprehensive understanding of the speaker's true intention, which is\nargued not be limited to a step-by-step reasoning process. To verify this\nargument, we introduce a new prompting framework called SarcasmCue, which\ncontains four prompting strategies, $viz.$ chain of contradiction (CoC), graph\nof cues (GoC), bagging of cues (BoC) and tensor of cues (ToC), which elicits\nLLMs to detect human sarcasm by considering sequential and non-sequential\nprompting methods. Through a comprehensive empirical comparison on four\nbenchmarking datasets, we show that the proposed four prompting methods\noutperforms standard IO prompting, CoT and ToT with a considerable margin, and\nnon-sequential prompting generally outperforms sequential prompting."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we present STBLLM, the first structural binarization framework\nfor compressing Large Language Models (LLMs) to less than 1-bit precision. LLMs\nhave achieved remarkable performance, but their heavy memory requirements have\nhindered widespread adoption, particularly on resource-constrained devices.\nBinarization, which quantifies weights to a mere 1-bit, achieves a milestone in\nincreasing computational efficiency. However, we observe that some weights in\nbinarized LLMs can be randomly flipped without significant performance\ndegradation, indicating the potential for further compression. To exploit this,\nour STBLLM employs an N:M sparsity to perform structural binarization of the\nweights. First, we introduce a new Standardized Importance (SI) metric that\nconsiders weight magnitude and input feature norm to better evaluate weight\nsignificance. Then, we propose a layer-wise approach where different layers of\nthe LLM can be sparsified with varying N:M ratios, balancing compression and\naccuracy. Finally, we use residual approximation with double binarization to\npreserve information for salient weights. In addition, we utilize a\nfine-grained grouping strategy for less important weights that applies\ndifferent quantization schemes to sparse, intermediate, and dense regions. We\nconduct extensive experiments on various language models, including the\nLLaMA-1/2/3, OPT family, and Mistral, to evaluate the effectiveness of STBLLM.\nThe results demonstrate that our approach performs better than other compressed\nbinarization LLM methods while significantly reducing memory requirements.",
        "pos": [
            "Symbolic Music, akin to language, can be encoded in discrete symbols. Recent\nresearch has extended the application of large language models (LLMs) such as\nGPT-4 and Llama2 to the symbolic music domain including understanding and\ngeneration. Yet scant research explores the details of how these LLMs perform\non advanced music understanding and conditioned generation, especially from the\nmulti-step reasoning perspective, which is a critical aspect in the\nconditioned, editable, and interactive human-computer co-creation process. This\nstudy conducts a thorough investigation of LLMs' capability and limitations in\nsymbolic music processing. We identify that current LLMs exhibit poor\nperformance in song-level multi-step music reasoning, and typically fail to\nleverage learned music knowledge when addressing complex musical tasks. An\nanalysis of LLMs' responses highlights distinctly their pros and cons. Our\nfindings suggest achieving advanced musical capability is not intrinsically\nobtained by LLMs, and future research should focus more on bridging the gap\nbetween music knowledge and reasoning, to improve the co-creation experience\nfor musicians."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we present STBLLM, the first structural binarization framework\nfor compressing Large Language Models (LLMs) to less than 1-bit precision. LLMs\nhave achieved remarkable performance, but their heavy memory requirements have\nhindered widespread adoption, particularly on resource-constrained devices.\nBinarization, which quantifies weights to a mere 1-bit, achieves a milestone in\nincreasing computational efficiency. However, we observe that some weights in\nbinarized LLMs can be randomly flipped without significant performance\ndegradation, indicating the potential for further compression. To exploit this,\nour STBLLM employs an N:M sparsity to perform structural binarization of the\nweights. First, we introduce a new Standardized Importance (SI) metric that\nconsiders weight magnitude and input feature norm to better evaluate weight\nsignificance. Then, we propose a layer-wise approach where different layers of\nthe LLM can be sparsified with varying N:M ratios, balancing compression and\naccuracy. Finally, we use residual approximation with double binarization to\npreserve information for salient weights. In addition, we utilize a\nfine-grained grouping strategy for less important weights that applies\ndifferent quantization schemes to sparse, intermediate, and dense regions. We\nconduct extensive experiments on various language models, including the\nLLaMA-1/2/3, OPT family, and Mistral, to evaluate the effectiveness of STBLLM.\nThe results demonstrate that our approach performs better than other compressed\nbinarization LLM methods while significantly reducing memory requirements.",
        "pos": [
            "Symbolic Music, akin to language, can be encoded in discrete symbols. Recent\nresearch has extended the application of large language models (LLMs) such as\nGPT-4 and Llama2 to the symbolic music domain including understanding and\ngeneration. Yet scant research explores the details of how these LLMs perform\non advanced music understanding and conditioned generation, especially from the\nmulti-step reasoning perspective, which is a critical aspect in the\nconditioned, editable, and interactive human-computer co-creation process. This\nstudy conducts a thorough investigation of LLMs' capability and limitations in\nsymbolic music processing. We identify that current LLMs exhibit poor\nperformance in song-level multi-step music reasoning, and typically fail to\nleverage learned music knowledge when addressing complex musical tasks. An\nanalysis of LLMs' responses highlights distinctly their pros and cons. Our\nfindings suggest achieving advanced musical capability is not intrinsically\nobtained by LLMs, and future research should focus more on bridging the gap\nbetween music knowledge and reasoning, to improve the co-creation experience\nfor musicians."
        ],
        "neg": []
    },
    {
        "query": "With the development of artificial intelligence (AI), large language models\n(LLM) are widely used in many fields. However, the reasoning ability of LLM is\nstill very limited when it comes to mathematical reasoning. Mathematics plays\nan important role in all aspects of human society and is a technical guarantee\nin the fields of healthcare, transport and aerospace, for this reason, the\ndevelopment of AI big language models in the field of mathematics has great\npotential significance. To improve the mathematical reasoning ability of large\nlanguage models, we proposed an agent framework for learning to solve\nmathematical problems based on inductive reasoning. By emulating the human\nlearning process of generalization of learned information and effective\napplication of previous knowledge in new reasoning tasks, this framework has\ngreat performance in the mathematical reasoning process. It improves global\naccuracy over the baseline method (chain-of-thought) by 20.96% and solves\n17.54% of the mathematical problems that the baseline cannot solve. Benefiting\nfrom the efficient RETRIEVAL method, our model improves the ability of large\nlanguage models to efficiently use external knowledge, i.e., the mathematical\ncomputation of the model can be based on written procedures. In education, our\nmodel can be used as a personalised learning aid, thus reducing the inequality\nof educational resources.",
        "pos": [
            "This technical report presents the implementation of a state-of-the-art video\nencoder for video-text modal alignment and a video conversation framework\ncalled HiLight, which features dual visual towers. The work is divided into two\nmain parts: 1.alignment of video and text modalities; 2.convenient and\nefficient way to interact with users. Our goal is to address the task of video\ncomprehension in the context of billiards. The report includes a discussion of\nthe concepts and the final solution developed during the task's implementation."
        ],
        "neg": []
    },
    {
        "query": "The challenge of defining a slot schema to represent the state of a\ntask-oriented dialogue system is addressed by Slot Schema Induction (SSI),\nwhich aims to automatically induce slots from unlabeled dialogue data. Whereas\nprevious approaches induce slots by clustering value spans extracted directly\nfrom the dialogue text, we demonstrate the power of discovering slots using a\ngenerative approach. By training a model to generate slot names and values that\nsummarize key dialogue information with no prior task knowledge, our SSI method\ndiscovers high-quality candidate information for representing dialogue state.\nThese discovered slot-value candidates can be easily clustered into unified\nslot schemas that align well with human-authored schemas. Experimental\ncomparisons on the MultiWOZ and SGD datasets demonstrate that Generative\nDialogue State Inference (GenDSI) outperforms the previous state-of-the-art on\nmultiple aspects of the SSI task.",
        "pos": [
            "The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. Despite several challenges, recent models\nhave made remarkable advancements in this task using large language models\n(LLMs). Interestingly, we find that LLM-based models without fine-tuning\nexhibit distinct natures compared to their fine-tuned counterparts, leading to\ninadequacies in current evaluation metrics to accurately convey their\nperformance. Thus, we analyze the two primary metrics, Test Suite Execution\nAccuracy (EXE) and Exact Set Matching Accuracy (ESM), to examine their\nrobustness for this task and address shortcomings. We compare the performance\nof 9 LLM-based models using EXE, the original ESM, and our improved ESM (called\nESM+). Our results show that EXE and ESM have high false positive and negative\nrates of 11.3% and 13.9%, while ESM+ gives those of 0.1% and 2.6% respectively,\nproviding a significantly more stable evaluation. We release the ESM+ script as\nopen-source for the community to contribute, while enjoying a more reliable\nassessment of Text-to-SQL."
        ],
        "neg": []
    },
    {
        "query": "When engaging in conversations, dialogue agents in a virtual simulation\nenvironment may exhibit their own emotional states that are unrelated to the\nimmediate conversational context, a phenomenon known as self-emotion. This\nstudy explores how such self-emotion affects the agents' behaviors in dialogue\nstrategies and decision-making within a large language model (LLM)-driven\nsimulation framework. In a dialogue strategy prediction experiment, we analyze\nthe dialogue strategy choices employed by agents both with and without\nself-emotion, comparing them to those of humans. The results show that\nincorporating self-emotion helps agents exhibit more human-like dialogue\nstrategies. In an independent experiment comparing the performance of models\nfine-tuned on GPT-4 generated dialogue datasets, we demonstrate that\nself-emotion can lead to better overall naturalness and humanness. Finally, in\na virtual simulation environment where agents have discussions on multiple\ntopics, we show that self-emotion of agents can significantly influence the\ndecision-making process of the agents, leading to approximately a 50% change in\ndecisions.",
        "pos": [
            "Traditional spoken language processing involves cascading an automatic speech\nrecognition (ASR) system into text processing models. In contrast, \"textless\"\nmethods process speech representations without ASR systems, enabling the direct\nuse of acoustic speech features. Although their effectiveness is shown in\ncapturing acoustic features, it is unclear in capturing lexical knowledge. This\npaper proposes a textless method for dependency parsing, examining its\neffectiveness and limitations. Our proposed method predicts a dependency tree\nfrom a speech signal without transcribing, representing the tree as a labeled\nsequence. scading method outperforms the textless method in overall parsing\naccuracy, the latter excels in instances with important acoustic features. Our\nfindings highlight the importance of fusing word-level representations and\nsentence-level prosody for enhanced parsing performance. The code and models\nare made publicly available: https://github.com/mynlp/SpeechParser."
        ],
        "neg": []
    },
    {
        "query": "When engaging in conversations, dialogue agents in a virtual simulation\nenvironment may exhibit their own emotional states that are unrelated to the\nimmediate conversational context, a phenomenon known as self-emotion. This\nstudy explores how such self-emotion affects the agents' behaviors in dialogue\nstrategies and decision-making within a large language model (LLM)-driven\nsimulation framework. In a dialogue strategy prediction experiment, we analyze\nthe dialogue strategy choices employed by agents both with and without\nself-emotion, comparing them to those of humans. The results show that\nincorporating self-emotion helps agents exhibit more human-like dialogue\nstrategies. In an independent experiment comparing the performance of models\nfine-tuned on GPT-4 generated dialogue datasets, we demonstrate that\nself-emotion can lead to better overall naturalness and humanness. Finally, in\na virtual simulation environment where agents have discussions on multiple\ntopics, we show that self-emotion of agents can significantly influence the\ndecision-making process of the agents, leading to approximately a 50% change in\ndecisions.",
        "pos": [
            "Traditional spoken language processing involves cascading an automatic speech\nrecognition (ASR) system into text processing models. In contrast, \"textless\"\nmethods process speech representations without ASR systems, enabling the direct\nuse of acoustic speech features. Although their effectiveness is shown in\ncapturing acoustic features, it is unclear in capturing lexical knowledge. This\npaper proposes a textless method for dependency parsing, examining its\neffectiveness and limitations. Our proposed method predicts a dependency tree\nfrom a speech signal without transcribing, representing the tree as a labeled\nsequence. scading method outperforms the textless method in overall parsing\naccuracy, the latter excels in instances with important acoustic features. Our\nfindings highlight the importance of fusing word-level representations and\nsentence-level prosody for enhanced parsing performance. The code and models\nare made publicly available: https://github.com/mynlp/SpeechParser.",
            "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
        ],
        "neg": []
    },
    {
        "query": "LLM-driven dialog systems are used in a diverse set of applications, ranging\nfrom healthcare to customer service. However, given their generalization\ncapability, it is difficult to ensure that these chatbots stay within the\nboundaries of the specialized domains, potentially resulting in inaccurate\ninformation and irrelevant responses. This paper introduces an unsupervised\napproach for automatically inducing domain-specific dialog flows that can be\nused to constrain LLM-based chatbots. We introduce two variants of dialog flow\nbased on the availability of in-domain conversation instances. Through human\nand automatic evaluation over various dialog domains, we demonstrate that our\nhigh-quality data-guided dialog flows achieve better domain coverage, thereby\novercoming the need for extensive manual crafting of such flows.",
        "pos": [
            "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenDevin, a platform for the development of powerful and\nflexible AI agents that interact with the world in similar ways to those of a\nhuman developer: by writing code, interacting with a command line, and browsing\nthe web. We describe how the platform allows for the implementation of new\nagents, safe interaction with sandboxed environments for code execution,\ncoordination between multiple agents, and incorporation of evaluation\nbenchmarks. Based on our currently incorporated benchmarks, we perform an\nevaluation of agents over 15 challenging tasks, including software engineering\n(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released\nunder the permissive MIT license, OpenDevin is a community project spanning\nacademia and industry with more than 1.3K contributions from over 160\ncontributors and will improve going forward.",
            "Language models are known to encode a great amount of factual knowledge\nthrough pretraining. However, such knowledge might be insufficient to cater to\nuser requests, requiring the model to integrate external knowledge sources and\nadhere to user-provided specifications. When answering questions about ongoing\nevents, the model should use recent news articles to update its response; when\nasked to provide recommendations, the model should prioritize user\nspecifications over retrieved product reviews; when some facts are edited in\nthe model, the updated facts should override all prior knowledge learned by the\nmodel even if they are conflicting. In all of the cases above, the model faces\na decision between its own parametric knowledge, (retrieved) contextual\nknowledge, and user instruction knowledge. In this paper, we (1) unify such\nsettings into the problem of knowledge preference and define a three-level\npreference hierarchy over these knowledge sources; (2) compile a collection of\nexisting datasets IfQA, MQuAKE, and MRQA covering a combination of settings\n(with/without user specifications, with/without context documents) to\nsystematically evaluate how well models obey the intended knowledge preference;\nand (3) propose a dataset synthesis method that composes diverse\nquestion-answer pairs with user assumptions and related context to directly\nfine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a\n7B model, fine-tuned on only a few thousand examples automatically generated by\nour proposed method, effectively achieves superior performance (more than 18%\nimprovement across all evaluation benchmarks) in adhering to the desired\nknowledge preference hierarchy.",
            "Hallucination is often regarded as a major impediment for using large\nlanguage models (LLMs), especially for knowledge-intensive tasks. Even when the\ntraining corpus consists solely of true statements, language models still\ngenerate hallucinations in the form of amalgamations of multiple facts. We coin\nthis phenomenon as ``knowledge overshadowing'': when we query knowledge from a\nlanguage model with multiple conditions, some conditions overshadow others,\nleading to hallucinated outputs. This phenomenon partially stems from training\ndata imbalance, which we verify on both pretrained models and fine-tuned\nmodels, over a wide range of LM model families and sizes.From a theoretical\npoint of view, knowledge overshadowing can be interpreted as\nover-generalization of the dominant conditions (patterns). We show that the\nhallucination rate grows with both the imbalance ratio (between the popular and\nunpopular condition) and the length of dominant condition description,\nconsistent with our derived generalization bound. Finally, we propose to\nutilize overshadowing conditions as a signal to catch hallucination before it\nis produced, along with a training-free self-contrastive decoding method to\nalleviate hallucination during inference. Our proposed approach showcases up to\n82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,\nwith different models and datasets.",
            "Extensive previous research has focused on post-training knowledge editing\n(KE) for language models (LMs) to ensure that knowledge remains accurate and\nup-to-date. One desired property and open question in KE is to let edited LMs\ncorrectly handle ripple effects, where LM is expected to answer its logically\nrelated knowledge accurately. In this paper, we answer the question of why most\nKE methods still create messy ripple effects. We conduct extensive analysis and\nidentify a salient indicator, GradSim, that effectively reveals when and why\nupdated knowledge ripples in LMs. GradSim is computed by the cosine similarity\nbetween gradients of the original fact and its related knowledge. We observe a\nstrong positive correlation between ripple effect performance and GradSim\nacross different LMs, KE methods, and evaluation metrics. Further\ninvestigations into three counter-intuitive failure cases (Negation,\nOver-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures\nare often associated with very low GradSim. This finding validates that GradSim\nis an effective indicator of when knowledge ripples in LMs."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) are trained on a deluge of text data with\nlimited quality control. As a result, LLMs can exhibit unintended or even\nharmful behaviours, such as leaking information, fake news or hate speech.\nCountermeasures, commonly referred to as preference alignment, include\nfine-tuning the pretrained LLMs with carefully crafted text examples of desired\nbehaviour. Even then, empirical evidence shows preference aligned LLMs can be\nenticed to harmful behaviour. This so called jailbreaking of LLMs is typically\nachieved by adversarially modifying the input prompt to the LLM. Our paper\nprovides theoretical insights into the phenomenon of preference alignment and\njailbreaking from a statistical perspective. Under our framework, we first show\nthat pretrained LLMs will mimic harmful behaviour if present in the training\ncorpus. Under that same framework, we then introduce a statistical notion of\nalignment, and lower-bound the jailbreaking probability, showing that it is\nunpreventable under reasonable assumptions. Based on our insights, we propose\nan alteration to the currently prevalent alignment strategy RLHF. Specifically,\nwe introduce a simple modification to the RLHF objective, we call E-RLHF, that\naims to increase the likelihood of safe responses. E-RLHF brings no additional\ntraining cost, and is compatible with other methods. Empirically, we\ndemonstrate that E-RLHF outperforms RLHF on all alignment problems put forward\nby the AdvBench and HarmBench project without sacrificing model performance as\nmeasured by the MT-Bench project.",
        "pos": [
            "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource."
        ],
        "neg": []
    },
    {
        "query": "The rise of large language models (LLMs) has enabled us to seek answers to\ninherently debatable questions on LLM chatbots, necessitating a reliable way to\nevaluate their ability. However, traditional QA benchmarks assume fixed answers\nare inadequate for this purpose. To address this, we introduce DebateQA, a\ndataset of 2,941 debatable questions, each accompanied by multiple\nhuman-annotated partial answers that capture a variety of perspectives. We\ndevelop two metrics: Perspective Diversity, which evaluates the\ncomprehensiveness of perspectives, and Dispute Awareness, which assesses if the\nLLM acknowledges the question's debatable nature. Experiments demonstrate that\nboth metrics align with human preferences and are stable across different\nunderlying models. Using DebateQA with two metrics, we assess 12 popular LLMs\nand retrieval-augmented generation methods. Our findings reveal that while LLMs\ngenerally excel at recognizing debatable issues, their ability to provide\ncomprehensive answers encompassing diverse perspectives varies considerably.",
        "pos": [
            "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
            "The common toxicity and societal bias in contents generated by large language\nmodels (LLMs) necessitate strategies to reduce harm. Present solutions often\ndemand white-box access to the model or substantial training, which is\nimpractical for cutting-edge commercial LLMs. Moreover, prevailing prompting\nmethods depend on external tool feedback and fail to simultaneously lessen\ntoxicity and bias. Motivated by social psychology principles, we propose a\nnovel strategy named \\textbf{perspective-taking prompting (\\textsc{PeT})} that\ninspires LLMs to integrate diverse human perspectives and self-regulate their\nresponses. This self-correction mechanism can significantly diminish toxicity\n(up to $89\\%$) and bias (up to $73\\%$) in LLMs' responses. Rigorous evaluations\nand ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and\nthree open-source LLMs, revealing \\textsc{PeT}'s superiority in producing less\nharmful responses, outperforming five strong baselines."
        ],
        "neg": []
    },
    {
        "query": "The rise of large language models (LLMs) has enabled us to seek answers to\ninherently debatable questions on LLM chatbots, necessitating a reliable way to\nevaluate their ability. However, traditional QA benchmarks assume fixed answers\nare inadequate for this purpose. To address this, we introduce DebateQA, a\ndataset of 2,941 debatable questions, each accompanied by multiple\nhuman-annotated partial answers that capture a variety of perspectives. We\ndevelop two metrics: Perspective Diversity, which evaluates the\ncomprehensiveness of perspectives, and Dispute Awareness, which assesses if the\nLLM acknowledges the question's debatable nature. Experiments demonstrate that\nboth metrics align with human preferences and are stable across different\nunderlying models. Using DebateQA with two metrics, we assess 12 popular LLMs\nand retrieval-augmented generation methods. Our findings reveal that while LLMs\ngenerally excel at recognizing debatable issues, their ability to provide\ncomprehensive answers encompassing diverse perspectives varies considerably.",
        "pos": [
            "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
            "The common toxicity and societal bias in contents generated by large language\nmodels (LLMs) necessitate strategies to reduce harm. Present solutions often\ndemand white-box access to the model or substantial training, which is\nimpractical for cutting-edge commercial LLMs. Moreover, prevailing prompting\nmethods depend on external tool feedback and fail to simultaneously lessen\ntoxicity and bias. Motivated by social psychology principles, we propose a\nnovel strategy named \\textbf{perspective-taking prompting (\\textsc{PeT})} that\ninspires LLMs to integrate diverse human perspectives and self-regulate their\nresponses. This self-correction mechanism can significantly diminish toxicity\n(up to $89\\%$) and bias (up to $73\\%$) in LLMs' responses. Rigorous evaluations\nand ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and\nthree open-source LLMs, revealing \\textsc{PeT}'s superiority in producing less\nharmful responses, outperforming five strong baselines.",
            "While instruction fine-tuned LLMs are effective text generators, sensitivity\nto prompt construction makes performance unstable and sub-optimal in practice.\nRelying on a single \"best\" prompt cannot capture all differing approaches to a\ngeneration problem. Using this observation, we propose multi-prompt decoding,\nwhere many candidate generations are decoded from a prompt bank at\ninference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR)\ndecoding, which selects a final output using a trained value metric. We show\nmulti-prompt improves MBR across a comprehensive set of conditional generation\ntasks, and show this is a result of estimating a more diverse and higher\nquality candidate space than that of a single prompt. Further experiments\nconfirm multi-prompt improves generation across tasks, models and metrics.",
            "Prompt recovery, a crucial task in natural language processing, entails the\nreconstruction of prompts or instructions that language models use to convert\ninput text into a specific output. Although pivotal, the design and\neffectiveness of prompts represent a challenging and relatively untapped field\nwithin NLP research. This paper delves into an exhaustive investigation of\nprompt recovery methodologies, employing a spectrum of pre-trained language\nmodels and strategies. Our study is a comparative analysis aimed at gauging the\nefficacy of various models on a benchmark dataset, with the goal of pinpointing\nthe most proficient approach for prompt recovery. Through meticulous\nexperimentation and detailed analysis, we elucidate the outstanding performance\nof the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its\ncounterparts, showcasing its exceptional capability in accurately\nreconstructing prompts for text transformation tasks. Our findings offer a\nsignificant contribution to the existing knowledge on prompt recovery, shedding\nlight on the intricacies of prompt design and offering insightful perspectives\nfor future innovations in text rewriting and the broader field of natural\nlanguage processing.",
            "Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building."
        ],
        "neg": []
    },
    {
        "query": "Multimodal models that jointly process audio and language hold great promise\nin audio understanding and are increasingly being adopted in the music domain.\nBy allowing users to query via text and obtain information about a given audio\ninput, these models have the potential to enable a variety of music\nunderstanding tasks via language-based interfaces. However, their evaluation\nposes considerable challenges, and it remains unclear how to effectively assess\ntheir ability to correctly interpret music-related inputs with current methods.\nMotivated by this, we introduce MuChoMusic, a benchmark for evaluating music\nunderstanding in multimodal language models focused on audio. MuChoMusic\ncomprises 1,187 multiple-choice questions, all validated by human annotators,\non 644 music tracks sourced from two publicly available music datasets, and\ncovering a wide variety of genres. Questions in the benchmark are crafted to\nassess knowledge and reasoning abilities across several dimensions that cover\nfundamental musical concepts and their relation to cultural and functional\ncontexts. Through the holistic analysis afforded by the benchmark, we evaluate\nfive open-source models and identify several pitfalls, including an\nover-reliance on the language modality, pointing to a need for better\nmultimodal integration. Data and code are open-sourced.",
        "pos": [
            "Symbolic Music, akin to language, can be encoded in discrete symbols. Recent\nresearch has extended the application of large language models (LLMs) such as\nGPT-4 and Llama2 to the symbolic music domain including understanding and\ngeneration. Yet scant research explores the details of how these LLMs perform\non advanced music understanding and conditioned generation, especially from the\nmulti-step reasoning perspective, which is a critical aspect in the\nconditioned, editable, and interactive human-computer co-creation process. This\nstudy conducts a thorough investigation of LLMs' capability and limitations in\nsymbolic music processing. We identify that current LLMs exhibit poor\nperformance in song-level multi-step music reasoning, and typically fail to\nleverage learned music knowledge when addressing complex musical tasks. An\nanalysis of LLMs' responses highlights distinctly their pros and cons. Our\nfindings suggest achieving advanced musical capability is not intrinsically\nobtained by LLMs, and future research should focus more on bridging the gap\nbetween music knowledge and reasoning, to improve the co-creation experience\nfor musicians."
        ],
        "neg": []
    },
    {
        "query": "Learning token embeddings based on token co-occurrence statistics has proven\neffective for both pre-training and fine-tuning in natural language processing.\nHowever, recent studies have pointed out the distribution of learned embeddings\ndegenerates into anisotropy, and even pre-trained language models (PLMs) suffer\nfrom a loss of semantics-related information in embeddings for low-frequency\ntokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large,\nand demonstrates its robustness against degeneration. On the basis of this\nfinding, we propose DefinitionEMB, a method that utilizes definitions to\nconstruct isotropically distributed and semantics-related token embeddings for\nPLMs while maintaining original robustness during fine-tuning. Our experiments\ndemonstrate the effectiveness of leveraging definitions from Wiktionary to\nconstruct such embeddings for RoBERTa-base and BART-large. Furthermore, the\nconstructed embeddings for low-frequency tokens improve the performance of\nthese models across various GLUE and four text summarization datasets.",
        "pos": [
            "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
            "A large number of studies have emerged for Multimodal Knowledge Graph\nCompletion (MKGC) to predict the missing links in MKGs. However, fewer studies\nhave been proposed to study the inductive MKGC (IMKGC) involving emerging\nentities unseen during training. Existing inductive approaches focus on\nlearning textual entity representations, which neglect rich semantic\ninformation in visual modality. Moreover, they focus on aggregating structural\nneighbors from existing KGs, which of emerging entities are usually limited.\nHowever, the semantic neighbors are decoupled from the topology linkage and\nusually imply the true target entity. In this paper, we propose the IMKGC task\nand a semantic neighbor retrieval-enhanced IMKGC framework CMR, where the\ncontrast brings the helpful semantic neighbors close, and then the memorize\nsupports semantic neighbor retrieval to enhance inference. Specifically, we\nfirst propose a unified cross-modal contrastive learning to simultaneously\ncapture the textual-visual and textual-textual correlations of query-entity\npairs in a unified representation space. The contrastive learning increases the\nsimilarity of positive query-entity pairs, therefore making the representations\nof helpful semantic neighbors close. Then, we explicitly memorize the knowledge\nrepresentations to support the semantic neighbor retrieval. At test time, we\nretrieve the nearest semantic neighbors and interpolate them to the\nquery-entity similarity distribution to augment the final prediction. Extensive\nexperiments validate the effectiveness of CMR on three inductive MKGC datasets.\nCodes are available at https://github.com/OreOZhao/CMR."
        ],
        "neg": []
    },
    {
        "query": "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
        "pos": [
            "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
            "Adapting general large language models (LLMs) to specialized domains presents\ngreat challenges due to varied data distributions. This adaptation typically\nrequires continual pre-training on massive domain-specific corpora to\nfacilitate knowledge memorization, followed by training to apply this knowledge\nfollowing human instructions and preferences. However, this method may result\nin inefficient knowledge memorization due to a lack of awareness of knowledge\nutilization and imposes substantial demands on LLMs to simultaneously learn\nknowledge utilization and format alignment with limited training samples. To\nfacilitate the domain adaptation of LLM, we revise this process and propose a\nnew domain adaptation framework including domain knowledge learning and general\nformat alignment, called Mix-CPT. Specifically, we first conduct a knowledge\nmixture continual pre-training that concurrently focuses on knowledge\nmemorization and utilization, allowing for mutual reinforcement. To avoid\ncatastrophic forgetting during the continual pre-training process, we further\nincorporate a logit swap self-distillation constraint. Subsequently, leveraging\nthe knowledge and capabilities acquired during continual pre-training, we\nefficiently perform instruction tuning and alignment with a few general\ntraining samples to achieve format alignment. Extensive experiments demonstrate\nthat our proposed Mix-CPT framework can simultaneously improve the task-solving\ncapabilities of LLMs on the target and general domains compared to the\ntraditional adaptation methods.",
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering."
        ],
        "neg": []
    },
    {
        "query": "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench",
        "pos": [
            "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering."
        ],
        "neg": []
    },
    {
        "query": "Prompt tuning is a modular and efficient solution for training large language\nmodels (LLMs). One of its main advantages is task modularity, making it\nsuitable for multi-task problems. However, current soft-prompt-based methods\noften sacrifice multi-task modularity, requiring the training process to be\nfully or partially repeated for each newly added task. While recent work on\ntask vectors applied arithmetic operations on full model weights to achieve the\ndesired multi-task performance, a similar approach for soft-prompts is still\nmissing. To this end, we introduce Task Prompt Vectors, created by element-wise\ndifference between weights of tuned soft-prompts and their random\ninitialization. Experimental results on 12 NLU datasets show that task prompt\nvectors can be used in low-resource settings to effectively initialize prompt\ntuning on similar tasks. In addition, we show that task prompt vectors are\nindependent of the random initialization of prompt tuning. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, by\narithmetic addition of task prompt vectors from multiple tasks, we are able to\noutperform a state-of-the-art baseline in some cases.",
        "pos": [
            "Prompt injection (both direct and indirect) and jailbreaking are now\nrecognized as significant issues for large language models (LLMs), particularly\ndue to their potential for harm in application-integrated contexts. This\nextended abstract explores a novel approach to protecting LLMs from such\nattacks, termed \"soft begging.\" This method involves training soft prompts to\ncounteract the effects of corrupted prompts on the LLM's output. We provide an\noverview of prompt injections and jailbreaking, introduce the theoretical basis\nof the \"soft begging\" technique, and discuss an evaluation of its\neffectiveness."
        ],
        "neg": []
    },
    {
        "query": "Knowledge models are fundamental to dialogue systems for enabling\nconversational interactions, which require handling domain-specific knowledge.\nEnsuring effective communication in information-providing conversations entails\naligning user understanding with the knowledge available to the system.\nHowever, dialogue systems often face challenges arising from semantic\ninconsistencies in how information is expressed in natural language compared to\nhow it is represented within the system's internal knowledge. To address this\nproblem, we study the potential of large language models for conversational\ngrounding, a mechanism to bridge information gaps by establishing shared\nknowledge between dialogue participants. Our approach involves annotating human\nconversations across five knowledge domains to create a new dialogue corpus\ncalled BridgeKG. Through a series of experiments on this dataset, we\nempirically evaluate the capabilities of large language models in classifying\ngrounding acts and identifying grounded information items within a knowledge\ngraph structure. Our findings offer insights into how these models use\nin-context learning for conversational grounding tasks and common prediction\nerrors, which we illustrate with examples from challenging dialogues. We\ndiscuss how the models handle knowledge graphs as a semantic layer between\nunstructured dialogue utterances and structured information items.",
        "pos": [
            "The task of $\\textit{keyword extraction}$ is often an important initial step\nin unsupervised information extraction, forming the basis for tasks such as\ntopic modeling or document classification. While recent methods have proven to\nbe quite effective in the extraction of keywords, the identification of\n$\\textit{class-specific}$ keywords, or only those pertaining to a predefined\nclass, remains challenging. In this work, we propose an improved method for\nclass-specific keyword extraction, which builds upon the popular\n$\\textbf{KeyBERT}$ library to identify only keywords related to a class\ndescribed by $\\textit{seed keywords}$. We test this method using a dataset of\nGerman business registry entries, where the goal is to classify each business\naccording to an economic sector. Our results reveal that our method greatly\nimproves upon previous approaches, setting a new standard for\n$\\textit{class-specific}$ keyword extraction.",
            "Despite the advances in the abstractive summarization task using Large\nLanguage Models (LLM), there is a lack of research that asses their abilities\nto easily adapt to different domains. We evaluate the domain adaptation\nabilities of a wide range of LLMs on the summarization task across various\ndomains in both fine-tuning and in-context learning settings. We also present\nAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a\ndomain benchmark and a set of metrics to facilitate the analysis of domain\nadaptation. Our results demonstrate that LLMs exhibit comparable performance in\nthe in-context learning setting, regardless of their parameter scale."
        ],
        "neg": []
    },
    {
        "query": "When using large language models (LLMs) in knowledge-intensive tasks, such as\nopen-domain question answering, external context can bridge a gap between\nexternal knowledge and LLM's parametric knowledge. Recent research has been\ndeveloped to amplify contextual knowledge over the parametric knowledge of LLM\nwith contrastive decoding approaches. While these approaches could yield\ntruthful responses when relevant context is provided, they are prone to\nvulnerabilities when faced with noisy contexts. We extend the scope of previous\nstudies to encompass noisy contexts and propose adaptive contrastive decoding\n(ACD) to leverage contextual influence effectively. ACD demonstrates\nimprovements in open-domain question answering tasks compared to baselines,\nespecially in robustness by remaining undistracted by noisy contexts in\nretrieval-augmented generation.",
        "pos": [
            "Large Language Models (LLMs) have demonstrated impressive problem-solving\ncapabilities in mathematics through step-by-step reasoning chains. However,\nthey are susceptible to reasoning errors that impact the quality of subsequent\nreasoning chains and the final answer due to language models' autoregressive\ntoken-by-token generating nature. Recent works have proposed adopting external\nverifiers to guide the generation of reasoning paths, but existing works\nutilize models that have been trained with step-by-step labels to assess the\ncorrectness of token-by-token reasoning chains. Consequently, they struggle to\nrecognize discriminative details of tokens within a reasoning path and lack the\nability to evaluate whether an intermediate reasoning path is on a promising\ntrack toward the correct final answer. To amend the lack of sound and\ntoken-grained math-verification signals, we devise a novel training scheme for\nverifiers that apply token-level supervision with the expected cumulative\nreward (i.e., value). Furthermore, we propose a practical formulation of the\ncumulative reward by reducing it to finding the probability of future\ncorrectness of the final answer and thereby enabling the empirical estimation\nof the value. Experimental results on mathematical reasoning benchmarks show\nthat Token-Supervised Value Model (TVM) can outperform step-by-step verifiers\non GSM8K and MATH with Mistral and Llama."
        ],
        "neg": []
    },
    {
        "query": "When using large language models (LLMs) in knowledge-intensive tasks, such as\nopen-domain question answering, external context can bridge a gap between\nexternal knowledge and LLM's parametric knowledge. Recent research has been\ndeveloped to amplify contextual knowledge over the parametric knowledge of LLM\nwith contrastive decoding approaches. While these approaches could yield\ntruthful responses when relevant context is provided, they are prone to\nvulnerabilities when faced with noisy contexts. We extend the scope of previous\nstudies to encompass noisy contexts and propose adaptive contrastive decoding\n(ACD) to leverage contextual influence effectively. ACD demonstrates\nimprovements in open-domain question answering tasks compared to baselines,\nespecially in robustness by remaining undistracted by noisy contexts in\nretrieval-augmented generation.",
        "pos": [
            "Fine-tuning pre-trained language models (PLMs) has recently shown a potential\nto improve knowledge graph completion (KGC). However, most PLM-based methods\nencode only textual information, neglecting various topological structures of\nknowledge graphs (KGs). In this paper, we empirically validate the significant\nrelations between the structural properties of KGs and the performance of the\nPLM-based methods. To leverage the structural knowledge, we propose a\nSubgraph-Aware Training framework for KGC (SATKGC) that combines (i)\nsubgraph-aware mini-batching to encourage hard negative sampling, and (ii) a\nnew contrastive learning method to focus more on harder entities and harder\nnegative triples in terms of the structural properties. To the best of our\nknowledge, this is the first study to comprehensively incorporate the\nstructural inductive bias of the subgraphs into fine-tuning PLMs. Extensive\nexperiments on four KGC benchmarks demonstrate the superiority of SATKGC. Our\ncode is available.",
            "Modular programming, which aims to construct the final program by integrating\nsmaller, independent building blocks, has been regarded as a desirable practice\nin software development. However, with the rise of recent code generation\nagents built upon large language models (LLMs), a question emerges: is this\ntraditional practice equally effective for these new tools? In this work, we\nassess the impact of modularity in code generation by introducing a novel\nmetric for its quantitative measurement. Surprisingly, unlike conventional\nwisdom on the topic, we find that modularity is not a core factor for improving\nthe performance of code generation models. We also explore potential\nexplanations for why LLMs do not exhibit a preference for modular code compared\nto non-modular code."
        ],
        "neg": []
    },
    {
        "query": "Question Under Discussion (QUD) is a discourse framework that uses implicit\nquestions to reveal discourse relationships between sentences. In QUD parsing,\neach sentence is viewed as an answer to a question triggered by an anchor\nsentence in prior context. The resulting QUD structure is required to conform\nto several theoretical criteria like answer compatibility (how well the\nquestion is answered), making QUD parsing a challenging task. Previous works\nconstruct QUD parsers in a pipelined manner (i.e. detect the trigger sentence\nin context and then generate the question). However, these parsers lack a\nholistic view of the task and can hardly satisfy all the criteria. In this\nwork, we introduce QUDSELECT, a joint-training framework that selectively\ndecodes the QUD dependency structures considering the QUD criteria. Using\ninstruction-tuning, we train models to simultaneously predict the anchor\nsentence and generate the associated question. To explicitly incorporate the\ncriteria, we adopt a selective decoding strategy of sampling multiple QUD\ncandidates during inference, followed by selecting the best one with criteria\nscorers. Our method outperforms the state-of-the-art baseline models by 9% in\nhuman evaluation and 4% in automatic evaluation, demonstrating the\neffectiveness of our framework.",
        "pos": [
            "This paper investigates the capability of LLMs in storytelling, focusing on\nnarrative development and plot progression. We introduce a novel computational\nframework to analyze narratives through three discourse-level aspects: i) story\narcs, ii) turning points, and iii) affective dimensions, including arousal and\nvalence. By leveraging expert and automatic annotations, we uncover significant\ndiscrepancies between the LLM- and human- written stories. While human-written\nstories are suspenseful, arousing, and diverse in narrative structures, LLM\nstories are homogeneously positive and lack tension. Next, we measure narrative\nreasoning skills as a precursor to generative capacities, concluding that most\nLLMs fall short of human abilities in discourse understanding. Finally, we show\nthat explicit integration of aforementioned discourse features can enhance\nstorytelling, as is demonstrated by over 40% improvement in neural storytelling\nin terms of diversity, suspense, and arousal."
        ],
        "neg": []
    },
    {
        "query": "The recognition of named entities in visually-rich documents (VrD-NER) plays\na critical role in various real-world scenarios and applications. However, the\nresearch in VrD-NER faces three major challenges: complex document layouts,\nincorrect reading orders, and unsuitable task formulations. To address these\nchallenges, we propose a query-aware entity extraction head, namely UNER, to\ncollaborate with existing multi-modal document transformers to develop more\nrobust VrD-NER models. The UNER head considers the VrD-NER task as a\ncombination of sequence labeling and reading order prediction, effectively\naddressing the issues of discontinuous entities in documents. Experimental\nevaluations on diverse datasets demonstrate the effectiveness of UNER in\nimproving entity extraction performance. Moreover, the UNER head enables a\nsupervised pre-training stage on various VrD-NER datasets to enhance the\ndocument transformer backbones and exhibits substantial knowledge transfer from\nthe pre-training stage to the fine-tuning stage. By incorporating universal\nlayout understanding, a pre-trained UNER-based model demonstrates significant\nadvantages in few-shot and cross-linguistic scenarios and exhibits zero-shot\nentity extraction abilities.",
        "pos": [
            "The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we open-source SAFETY-J's training protocols,\ndatasets, and code at https://github.com/GAIR-NLP/Safety-J."
        ],
        "neg": []
    },
    {
        "query": "The recognition of named entities in visually-rich documents (VrD-NER) plays\na critical role in various real-world scenarios and applications. However, the\nresearch in VrD-NER faces three major challenges: complex document layouts,\nincorrect reading orders, and unsuitable task formulations. To address these\nchallenges, we propose a query-aware entity extraction head, namely UNER, to\ncollaborate with existing multi-modal document transformers to develop more\nrobust VrD-NER models. The UNER head considers the VrD-NER task as a\ncombination of sequence labeling and reading order prediction, effectively\naddressing the issues of discontinuous entities in documents. Experimental\nevaluations on diverse datasets demonstrate the effectiveness of UNER in\nimproving entity extraction performance. Moreover, the UNER head enables a\nsupervised pre-training stage on various VrD-NER datasets to enhance the\ndocument transformer backbones and exhibits substantial knowledge transfer from\nthe pre-training stage to the fine-tuning stage. By incorporating universal\nlayout understanding, a pre-trained UNER-based model demonstrates significant\nadvantages in few-shot and cross-linguistic scenarios and exhibits zero-shot\nentity extraction abilities.",
        "pos": [
            "The rapid development of the Internet has profoundly changed human life.\nHumans are increasingly expressing themselves and interacting with others on\nsocial media platforms. However, although artificial intelligence technology\nhas been widely used in many aspects of life, its application in social media\ncontent creation is still blank. To solve this problem, we propose a new prompt\nword generation framework based on multi-modal information fusion, which\ncombines multiple tasks including topic classification, sentiment analysis,\nscene recognition and keyword extraction to generate more comprehensive prompt\nwords. Subsequently, we use a template containing a set of prompt words to\nguide ChatGPT to generate high-quality tweets. Furthermore, in the absence of\neffective and objective evaluation criteria in the field of content generation,\nwe use the ChatGPT tool to evaluate the results generated by the algorithm,\nmaking large-scale evaluation of content generation algorithms possible.\nEvaluation results on extensive content generation demonstrate that our cue\nword generation framework generates higher quality content compared to manual\nmethods and other cueing techniques, while topic classification, sentiment\nanalysis, and scene recognition significantly enhance content clarity and its\nconsistency with the image."
        ],
        "neg": []
    },
    {
        "query": "Developers create pull request (PR) descriptions to provide an overview of\ntheir changes and explain the motivations behind them. These descriptions help\nreviewers and fellow developers quickly understand the updates. Despite their\nimportance, some developers omit these descriptions. To tackle this problem, we\npropose an automated method for generating PR descriptions based on commit\nmessages and source code comments. This method frames the task as a text\nsummarization problem, for which we utilized the T5 text-to-text transfer\nmodel. We fine-tuned a pre-trained T5 model using a dataset containing 33,466\nPRs. The model's effectiveness was assessed using ROUGE metrics, which are\nrecognized for their strong alignment with human evaluations. Our findings\nreveal that the T5 model significantly outperforms LexRank, which served as our\nbaseline for comparison.",
        "pos": [
            "Recent advancements in Large Language Models (LLMs), such as ChatGPT and\nLLaMA, have significantly transformed Natural Language Processing (NLP) with\ntheir outstanding abilities in text generation, summarization, and\nclassification. Nevertheless, their widespread adoption introduces numerous\nchallenges, including issues related to academic integrity, copyright,\nenvironmental impacts, and ethical considerations such as data bias, fairness,\nand privacy. The rapid evolution of LLMs also raises concerns regarding the\nreliability and generalizability of their evaluations. This paper offers a\ncomprehensive survey of the literature on these subjects, systematically\ngathered and synthesized from Google Scholar. Our study provides an in-depth\nanalysis of the risks associated with specific LLMs, identifying sub-risks,\ntheir causes, and potential solutions. Furthermore, we explore the broader\nchallenges related to LLMs, detailing their causes and proposing mitigation\nstrategies. Through this literature analysis, our survey aims to deepen the\nunderstanding of the implications and complexities surrounding these powerful\nmodels."
        ],
        "neg": []
    },
    {
        "query": "Developers create pull request (PR) descriptions to provide an overview of\ntheir changes and explain the motivations behind them. These descriptions help\nreviewers and fellow developers quickly understand the updates. Despite their\nimportance, some developers omit these descriptions. To tackle this problem, we\npropose an automated method for generating PR descriptions based on commit\nmessages and source code comments. This method frames the task as a text\nsummarization problem, for which we utilized the T5 text-to-text transfer\nmodel. We fine-tuned a pre-trained T5 model using a dataset containing 33,466\nPRs. The model's effectiveness was assessed using ROUGE metrics, which are\nrecognized for their strong alignment with human evaluations. Our findings\nreveal that the T5 model significantly outperforms LexRank, which served as our\nbaseline for comparison.",
        "pos": [
            "Recent advancements in Large Language Models (LLMs), such as ChatGPT and\nLLaMA, have significantly transformed Natural Language Processing (NLP) with\ntheir outstanding abilities in text generation, summarization, and\nclassification. Nevertheless, their widespread adoption introduces numerous\nchallenges, including issues related to academic integrity, copyright,\nenvironmental impacts, and ethical considerations such as data bias, fairness,\nand privacy. The rapid evolution of LLMs also raises concerns regarding the\nreliability and generalizability of their evaluations. This paper offers a\ncomprehensive survey of the literature on these subjects, systematically\ngathered and synthesized from Google Scholar. Our study provides an in-depth\nanalysis of the risks associated with specific LLMs, identifying sub-risks,\ntheir causes, and potential solutions. Furthermore, we explore the broader\nchallenges related to LLMs, detailing their causes and proposing mitigation\nstrategies. Through this literature analysis, our survey aims to deepen the\nunderstanding of the implications and complexities surrounding these powerful\nmodels."
        ],
        "neg": []
    },
    {
        "query": "Developers create pull request (PR) descriptions to provide an overview of\ntheir changes and explain the motivations behind them. These descriptions help\nreviewers and fellow developers quickly understand the updates. Despite their\nimportance, some developers omit these descriptions. To tackle this problem, we\npropose an automated method for generating PR descriptions based on commit\nmessages and source code comments. This method frames the task as a text\nsummarization problem, for which we utilized the T5 text-to-text transfer\nmodel. We fine-tuned a pre-trained T5 model using a dataset containing 33,466\nPRs. The model's effectiveness was assessed using ROUGE metrics, which are\nrecognized for their strong alignment with human evaluations. Our findings\nreveal that the T5 model significantly outperforms LexRank, which served as our\nbaseline for comparison.",
        "pos": [
            "Recent advancements in Large Language Models (LLMs), such as ChatGPT and\nLLaMA, have significantly transformed Natural Language Processing (NLP) with\ntheir outstanding abilities in text generation, summarization, and\nclassification. Nevertheless, their widespread adoption introduces numerous\nchallenges, including issues related to academic integrity, copyright,\nenvironmental impacts, and ethical considerations such as data bias, fairness,\nand privacy. The rapid evolution of LLMs also raises concerns regarding the\nreliability and generalizability of their evaluations. This paper offers a\ncomprehensive survey of the literature on these subjects, systematically\ngathered and synthesized from Google Scholar. Our study provides an in-depth\nanalysis of the risks associated with specific LLMs, identifying sub-risks,\ntheir causes, and potential solutions. Furthermore, we explore the broader\nchallenges related to LLMs, detailing their causes and proposing mitigation\nstrategies. Through this literature analysis, our survey aims to deepen the\nunderstanding of the implications and complexities surrounding these powerful\nmodels."
        ],
        "neg": []
    },
    {
        "query": "Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.",
        "pos": [
            "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe same result that the optimal vocabulary size depends on the available\ncompute budget and that larger models deserve larger vocabularies. However,\nmost LLMs use too small vocabulary sizes. For example, we predict that the\noptimal vocabulary size of Llama2-70B should have been at least 216K, 7 times\nlarger than its vocabulary of 32K. We validate our predictions empirically by\ntraining models with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly\nconsidering model parameters and vocabulary size for efficient scaling.",
            "Large language models (LLMs) are integral to modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it demands substantial training\nresources to optimize model weights and quantization parameters. To address\nthis, we propose Efficient Quantization-Aware Training (EfficientQAT), a novel\nquantization technique for compressing LLMs. EfficientQAT involves two\nconsecutive phases: Block-wise training of all parameters (Block-AP) and\nend-to-end training of quantization parameters (E2E-QP). Block-AP sequentially\nconducts quantization-aware training for all parameters in each transformer\nblock with block-wise reconstruction, maintaining efficiency by avoiding\ntraining the entire LLM. Initialized with quantized model, E2E-QP then trains\nonly quantization parameters (step sizes) end-to-end, enhancing efficiency with\na fixed quantized backbone and reduced trainable parameter count. Extensive\nexperiments demonstrate that EfficientQAT outperforms previous quantization\nmethods across a range of models, including base LLMs, instruction-tuned LLMs,\nand multimodal LLMs, with scales from 7B to 70B parameters at various\nquantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model\non a single A100-80GB GPU in 41 hours, with less than 3\\% accuracy degradation\ncompared to the full precision (69.48 vs. 72.41). Notably, this INT2 quantized\n70B model obtains a 1.67 accuracy gain over the Llama-2-13B model (69.48 vs.\n67.81) while requiring less memory (19.2GB vs. 24.2GB). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT."
        ],
        "neg": []
    },
    {
        "query": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.",
        "pos": [
            "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with upstream model capabilities, potentially\nenabling \"safetywashing\" -- where capability improvements are misrepresented as\nsafety advancements. Based on these findings, we propose an empirical\nfoundation for developing more meaningful safety metrics and define AI safety\nin a machine learning research context as a set of clearly delineated research\ngoals that are empirically separable from generic capabilities advancements. In\ndoing so, we aim to provide a more rigorous framework for AI safety research,\nadvancing the science of safety evaluations and clarifying the path towards\nmeasurable progress."
        ],
        "neg": []
    },
    {
        "query": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.",
        "pos": [
            "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with upstream model capabilities, potentially\nenabling \"safetywashing\" -- where capability improvements are misrepresented as\nsafety advancements. Based on these findings, we propose an empirical\nfoundation for developing more meaningful safety metrics and define AI safety\nin a machine learning research context as a set of clearly delineated research\ngoals that are empirically separable from generic capabilities advancements. In\ndoing so, we aim to provide a more rigorous framework for AI safety research,\nadvancing the science of safety evaluations and clarifying the path towards\nmeasurable progress."
        ],
        "neg": []
    },
    {
        "query": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.",
        "pos": [
            "Knowledge editing techniques have been increasingly adopted to efficiently\ncorrect the false or outdated knowledge in Large Language Models (LLMs), due to\nthe high cost of retraining from scratch. Meanwhile, one critical but\nunder-explored question is: can knowledge editing be used to inject harm into\nLLMs? In this paper, we propose to reformulate knowledge editing as a new type\nof safety threat for LLMs, namely Editing Attack, and conduct a systematic\ninvestigation with a newly constructed dataset EditAttack. Specifically, we\nfocus on two typical safety risks of Editing Attack including Misinformation\nInjection and Bias Injection. For the risk of misinformation injection, we\nfirst categorize it into commonsense misinformation injection and long-tail\nmisinformation injection. Then, we find that editing attacks can inject both\ntypes of misinformation into LLMs, and the effectiveness is particularly high\nfor commonsense misinformation injection. For the risk of bias injection, we\ndiscover that not only can biased sentences be injected into LLMs with high\neffectiveness, but also one single biased sentence injection can cause a bias\nincrease in general outputs of LLMs, which are even highly irrelevant to the\ninjected sentence, indicating a catastrophic impact on the overall fairness of\nLLMs. Then, we further illustrate the high stealthiness of editing attacks,\nmeasured by their impact on the general knowledge and reasoning capacities of\nLLMs, and show the hardness of defending editing attacks with empirical\nevidence. Our discoveries demonstrate the emerging misuse risks of knowledge\nediting techniques on compromising the safety alignment of LLMs.",
            "To ensure performance on a diverse set of downstream tasks, LLMs are\npretrained via data mixtures over different domains. In this work, we\ndemonstrate that the optimal data composition for a fixed compute budget varies\ndepending on the scale of the training data, suggesting that the common\npractice of empirically determining an optimal composition using small-scale\nexperiments will not yield the optimal data mixtures when scaling up to the\nfinal model. To address this challenge, we propose *AutoScale*, an automated\ntool that finds a compute-optimal data composition for training at any desired\ntarget scale. AutoScale first determines the optimal composition at a small\nscale using a novel bilevel optimization framework, Direct Data Optimization\n(*DDO*), and then fits a predictor to estimate the optimal composition at\nlarger scales. The predictor's design is inspired by our theoretical analysis\nof scaling laws related to data composition, which could be of independent\ninterest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2\nLarge) on RedPajama dataset, AutoScale decreases validation perplexity at least\n25% faster than any baseline with up to 38% speed up compared to without\nreweighting, achieving the best overall performance across downstream tasks. On\npre-training Encoder-only LMs (BERT) with masked language modeling, DDO is\nshown to decrease loss on all domains while visibly improving average task\nperformance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by\n5.9% compared with without reweighting. AutoScale speeds up training by up to\n28%. Our codes are open-sourced."
        ],
        "neg": []
    },
    {
        "query": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.",
        "pos": [
            "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with upstream model capabilities, potentially\nenabling \"safetywashing\" -- where capability improvements are misrepresented as\nsafety advancements. Based on these findings, we propose an empirical\nfoundation for developing more meaningful safety metrics and define AI safety\nin a machine learning research context as a set of clearly delineated research\ngoals that are empirically separable from generic capabilities advancements. In\ndoing so, we aim to provide a more rigorous framework for AI safety research,\nadvancing the science of safety evaluations and clarifying the path towards\nmeasurable progress."
        ],
        "neg": []
    },
    {
        "query": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.",
        "pos": [
            "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with upstream model capabilities, potentially\nenabling \"safetywashing\" -- where capability improvements are misrepresented as\nsafety advancements. Based on these findings, we propose an empirical\nfoundation for developing more meaningful safety metrics and define AI safety\nin a machine learning research context as a set of clearly delineated research\ngoals that are empirically separable from generic capabilities advancements. In\ndoing so, we aim to provide a more rigorous framework for AI safety research,\nadvancing the science of safety evaluations and clarifying the path towards\nmeasurable progress."
        ],
        "neg": []
    },
    {
        "query": "The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a vanilla RAG system and\nthey will be further used to guide the query generation in the next iteration.\nOur experiments show the improved performance of various LLMs brought by\ni-MedRAG compared with vanilla RAG on complex questions from clinical vignettes\nin the United States Medical Licensing Examination (USMLE), as well as various\nknowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.\nNotably, our zero-shot i-MedRAG outperforms all existing prompt engineering and\nfine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\\% on the MedQA\ndataset. In addition, we characterize the scaling properties of i-MedRAG with\ndifferent iterations of follow-up queries and different numbers of queries per\niteration. Our case studies show that i-MedRAG can flexibly ask follow-up\nqueries to form reasoning chains, providing an in-depth analysis of medical\nquestions. To the best of our knowledge, this is the first-of-its-kind study on\nincorporating follow-up queries into medical RAG.",
        "pos": [
            "Large language models (LLMs) hold great promise in summarizing medical\nevidence. Most recent studies focus on the application of proprietary LLMs.\nUsing proprietary LLMs introduces multiple risk factors, including a lack of\ntransparency and vendor dependency. While open-source LLMs allow better\ntransparency and customization, their performance falls short compared to\nproprietary ones. In this study, we investigated to what extent fine-tuning\nopen-source LLMs can further improve their performance in summarizing medical\nevidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs\nof systematic reviews and summaries, we fine-tuned three broadly-used,\nopen-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned\nLLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:\n8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and\n15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of\nfine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,\nsmaller fine-tuned models sometimes even demonstrated superior performance\ncompared to larger zero-shot models. The above trends of improvement were also\nmanifested in both human and GPT4-simulated evaluations. Our results can be\napplied to guide model selection for tasks demanding particular domain\nknowledge, such as medical evidence summarization."
        ],
        "neg": []
    },
    {
        "query": "The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a vanilla RAG system and\nthey will be further used to guide the query generation in the next iteration.\nOur experiments show the improved performance of various LLMs brought by\ni-MedRAG compared with vanilla RAG on complex questions from clinical vignettes\nin the United States Medical Licensing Examination (USMLE), as well as various\nknowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.\nNotably, our zero-shot i-MedRAG outperforms all existing prompt engineering and\nfine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\\% on the MedQA\ndataset. In addition, we characterize the scaling properties of i-MedRAG with\ndifferent iterations of follow-up queries and different numbers of queries per\niteration. Our case studies show that i-MedRAG can flexibly ask follow-up\nqueries to form reasoning chains, providing an in-depth analysis of medical\nquestions. To the best of our knowledge, this is the first-of-its-kind study on\nincorporating follow-up queries into medical RAG.",
        "pos": [
            "PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation."
        ],
        "neg": []
    },
    {
        "query": "The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a vanilla RAG system and\nthey will be further used to guide the query generation in the next iteration.\nOur experiments show the improved performance of various LLMs brought by\ni-MedRAG compared with vanilla RAG on complex questions from clinical vignettes\nin the United States Medical Licensing Examination (USMLE), as well as various\nknowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.\nNotably, our zero-shot i-MedRAG outperforms all existing prompt engineering and\nfine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\\% on the MedQA\ndataset. In addition, we characterize the scaling properties of i-MedRAG with\ndifferent iterations of follow-up queries and different numbers of queries per\niteration. Our case studies show that i-MedRAG can flexibly ask follow-up\nqueries to form reasoning chains, providing an in-depth analysis of medical\nquestions. To the best of our knowledge, this is the first-of-its-kind study on\nincorporating follow-up queries into medical RAG.",
        "pos": [
            "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
        ],
        "neg": []
    },
    {
        "query": "The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a vanilla RAG system and\nthey will be further used to guide the query generation in the next iteration.\nOur experiments show the improved performance of various LLMs brought by\ni-MedRAG compared with vanilla RAG on complex questions from clinical vignettes\nin the United States Medical Licensing Examination (USMLE), as well as various\nknowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.\nNotably, our zero-shot i-MedRAG outperforms all existing prompt engineering and\nfine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\\% on the MedQA\ndataset. In addition, we characterize the scaling properties of i-MedRAG with\ndifferent iterations of follow-up queries and different numbers of queries per\niteration. Our case studies show that i-MedRAG can flexibly ask follow-up\nqueries to form reasoning chains, providing an in-depth analysis of medical\nquestions. To the best of our knowledge, this is the first-of-its-kind study on\nincorporating follow-up queries into medical RAG.",
        "pos": [
            "Large language models (LLMs) hold great promise in summarizing medical\nevidence. Most recent studies focus on the application of proprietary LLMs.\nUsing proprietary LLMs introduces multiple risk factors, including a lack of\ntransparency and vendor dependency. While open-source LLMs allow better\ntransparency and customization, their performance falls short compared to\nproprietary ones. In this study, we investigated to what extent fine-tuning\nopen-source LLMs can further improve their performance in summarizing medical\nevidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs\nof systematic reviews and summaries, we fine-tuned three broadly-used,\nopen-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned\nLLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:\n8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and\n15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of\nfine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,\nsmaller fine-tuned models sometimes even demonstrated superior performance\ncompared to larger zero-shot models. The above trends of improvement were also\nmanifested in both human and GPT4-simulated evaluations. Our results can be\napplied to guide model selection for tasks demanding particular domain\nknowledge, such as medical evidence summarization."
        ],
        "neg": []
    },
    {
        "query": "While Large Language Models show remarkable performance in natural language\nunderstanding, their resource-intensive nature makes them less accessible. In\ncontrast, smaller language models such as MiniCPM offer more sustainable\nscalability, but often underperform without specialized optimization. In this\npaper, we explore the enhancement of smaller language models through the\nimprovement of their text embeddings. We select three language models, MiniCPM,\nPhi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Our\nresults demonstrate that this fine-tuning method enhances the quality of text\nembeddings for all three models across various benchmarks, with MiniCPM showing\nthe most significant improvements of an average 56.33% performance gain. The\ncontrastive fine-tuning code is publicly available at\nhttps://github.com/trapoom555/Language-Model-STS-CFT.",
        "pos": [
            "Entity Linking (EL) models are well-trained at mapping mentions to their\ncorresponding entities according to a given context. However, EL models\nstruggle to disambiguate long-tail entities due to their limited training data.\nMeanwhile, large language models (LLMs) are more robust at interpreting\nuncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at\ngenerating correct entity IDs. Furthermore, training an LLM to perform EL is\ncost-intensive. Building upon these insights, we introduce LLM-Augmented Entity\nLinking LLMAEL, a plug-and-play approach to enhance entity linking through LLM\ndata augmentation. We leverage LLMs as knowledgeable context augmenters,\ngenerating mention-centered descriptions as additional input, while preserving\ntraditional EL models for task specific processing. Experiments on 6 standard\ndatasets show that the vanilla LLMAEL outperforms baseline EL models in most\ncases, while the fine-tuned LLMAEL set the new state-of-the-art results across\nall 6 benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Entity alignment (EA) is to identify equivalent entities across different\nknowledge graphs (KGs), which can help fuse these KGs into a more comprehensive\none. Previous EA methods mainly focus on aligning a pair of KGs, and to the\nbest of our knowledge, no existing EA method considers aligning multiple (more\nthan two) KGs. To fill this research gap, in this work, we study a novel\nproblem of aligning multiple KGs and propose an effective framework named\nMultiEA to solve the problem. First, we embed the entities of all the candidate\nKGs into a common feature space by a shared KG encoder. Then, we explore three\nalignment strategies to minimize the distances among pre-aligned entities. In\nparticular, we propose an innovative inference enhancement technique to improve\nthe alignment performance by incorporating high-order similarities. Finally, to\nverify the effectiveness of MultiEA, we construct two new real-world benchmark\ndatasets and conduct extensive experiments on them. The results show that our\nMultiEA can effectively and efficiently align multiple KGs in a single pass.",
        "pos": [
            "The advent of vision-language models fosters the interactive conversations\nbetween AI-enabled models and humans. Yet applying these models into clinics\nmust deal with daunting challenges around large-scale training data, financial,\nand computational resources. Here we propose a cost-effective instruction\nlearning framework for conversational pathology named as CLOVER. CLOVER only\ntrains a lightweight module and uses instruction tuning while freezing the\nparameters of the large language model. Instead of using costly GPT-4, we\npropose well-designed prompts on GPT-3.5 for building generation-based\ninstructions, emphasizing the utility of pathological knowledge derived from\nthe Internet source. To augment the use of instructions, we construct a\nhigh-quality set of template-based instructions in the context of digital\npathology. From two benchmark datasets, our findings reveal the strength of\nhybrid-form instructions in the visual question-answer in pathology. Extensive\nresults show the cost-effectiveness of CLOVER in answering both open-ended and\nclosed-ended questions, where CLOVER outperforms strong baselines that possess\n37 times more training parameters and use instruction data generated from\nGPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot\nlearning in the external clinical dataset. These findings demonstrate that\ncost-effective modeling of CLOVER could accelerate the adoption of rapid\nconversational applications in the landscape of digital pathology."
        ],
        "neg": []
    },
    {
        "query": "Entity alignment (EA) is to identify equivalent entities across different\nknowledge graphs (KGs), which can help fuse these KGs into a more comprehensive\none. Previous EA methods mainly focus on aligning a pair of KGs, and to the\nbest of our knowledge, no existing EA method considers aligning multiple (more\nthan two) KGs. To fill this research gap, in this work, we study a novel\nproblem of aligning multiple KGs and propose an effective framework named\nMultiEA to solve the problem. First, we embed the entities of all the candidate\nKGs into a common feature space by a shared KG encoder. Then, we explore three\nalignment strategies to minimize the distances among pre-aligned entities. In\nparticular, we propose an innovative inference enhancement technique to improve\nthe alignment performance by incorporating high-order similarities. Finally, to\nverify the effectiveness of MultiEA, we construct two new real-world benchmark\ndatasets and conduct extensive experiments on them. The results show that our\nMultiEA can effectively and efficiently align multiple KGs in a single pass.",
        "pos": [
            "Everlasting contact between language communities leads to constant changes in\nlanguages over time, and gives rise to language varieties and dialects.\nHowever, the communities speaking non-standard language are often overlooked by\nnon-inclusive NLP technologies. Recently, there has been a surge of interest in\nstudying diatopic and diachronic changes in dialect NLP, but there is currently\nno research exploring the intersection of both. Our work aims to fill this gap\nby systematically reviewing diachronic and diatopic papers from a unified\nperspective. In this work, we critically assess nine tasks and datasets across\nfive dialects from three language families (Slavic, Romance, and Germanic) in\nboth spoken and written modalities. The tasks covered are diverse, including\ncorpus construction, dialect distance estimation, and dialect geolocation\nprediction, among others. Moreover, we outline five open challenges regarding\nchanges in dialect use over time, the reliability of dialect datasets, the\nimportance of speaker characteristics, limited coverage of dialects, and\nethical considerations in data collection. We hope that our work sheds light on\nfuture research towards inclusive computational methods and datasets for\nlanguage varieties and dialects."
        ],
        "neg": []
    },
    {
        "query": "Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aiming at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), which includes a Sentence\nEncoder to compress multiple tokens in a sentence into a single token, and a\nSentence Decoder to reconstruct it. By integrating SentenceVAE into the input\nand output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference method. In addition, the SentenceVAE module of\nSLLMs can maintain the integrity of the original semantic content by segmenting\nthe context into sentences, thereby improving accuracy while boosting inference\nspeed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over\nequivalent context length, significantly reducing memory demands for\nself-attention computation and facilitating the handling of longer context.\nExtensive experiments on Wanjuan dataset have revealed that the proposed method\ncan accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%\nof its original metric, and decrease memory overhead by 86~91% for the\nequivalent context length, compared to previous token-by-token methods.",
        "pos": [
            "Large language models (LLMs) with in-context learning have significantly\nimproved the performance of text-to-SQL task. Previous works generally focus on\nusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.\nHowever, they are mostly hard to handle large databases with numerous tables\nand columns, and usually ignore the significance of pre-processing database and\nextracting valuable information for more efficient prompt engineering. Based on\nabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework for\nin-context prompt engineering, which consists of three modules that retrieve\nconcise tables and columns as schema, and targeted examples for in-context\nlearning. Experiment results demonstrate that our model achieves better\nperformance than several competitive baselines on public datasets BIRD and\nSpider."
        ],
        "neg": []
    },
    {
        "query": "The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model.",
        "pos": [
            "Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels."
        ],
        "neg": []
    },
    {
        "query": "The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model.",
        "pos": [
            "Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels."
        ],
        "neg": []
    },
    {
        "query": "Rebuses are puzzles requiring constrained multi-step reasoning to identify a\nhidden phrase from a set of images and letters. In this work, we introduce a\nlarge collection of verbalized rebuses for the Italian language and use it to\nassess the rebus-solving capabilities of state-of-the-art large language\nmodels. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly\non this task, ad-hoc fine-tuning seems to improve models' performance. However,\nwe find that performance gains from training are largely motivated by\nmemorization. Our results suggest that rebus solving remains a challenging test\nbed to evaluate large language models' linguistic proficiency and sequential\ninstruction-following skills.",
        "pos": [
            "Estimating the difficulty of multiple-choice questions would be great help\nfor educators who must spend substantial time creating and piloting stimuli for\ntheir tests, and for learners who want to practice. Supervised approaches to\ndifficulty estimation have yielded to date mixed results. In this contribution\nwe leverage an aspect of generative large models which might be seen as a\nweakness when answering questions, namely their uncertainty, and exploit it\ntowards exploring correlations between two different metrics of uncertainty,\nand the actual student response distribution. While we observe some present but\nweak correlations, we also discover that the models' behaviour is different in\nthe case of correct vs wrong answers, and that correlations differ\nsubstantially according to the different question types which are included in\nour fine-grained, previously unused dataset of 451 questions from a\nBiopsychology course. In discussing our findings, we also suggest potential\navenues to further leverage model uncertainty as an additional proxy for item\ndifficulty."
        ],
        "neg": []
    },
    {
        "query": "Rebuses are puzzles requiring constrained multi-step reasoning to identify a\nhidden phrase from a set of images and letters. In this work, we introduce a\nlarge collection of verbalized rebuses for the Italian language and use it to\nassess the rebus-solving capabilities of state-of-the-art large language\nmodels. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly\non this task, ad-hoc fine-tuning seems to improve models' performance. However,\nwe find that performance gains from training are largely motivated by\nmemorization. Our results suggest that rebus solving remains a challenging test\nbed to evaluate large language models' linguistic proficiency and sequential\ninstruction-following skills.",
        "pos": [
            "Recent advances in computational linguistics include simulating the emergence\nof human-like languages with interacting neural network agents, starting from\nsets of random symbols. The recently introduced NeLLCom framework (Lian et al.,\n2023) allows agents to first learn an artificial language and then use it to\ncommunicate, with the aim of studying the emergence of specific linguistics\nproperties. We extend this framework (NeLLCom-X) by introducing more realistic\nrole-alternating agents and group communication in order to investigate the\ninterplay between language learnability, communication pressures, and group\nsize effects. We validate NeLLCom-X by replicating key findings from prior\nresearch simulating the emergence of a word-order/case-marking trade-off. Next,\nwe investigate how interaction affects linguistic convergence and emergence of\nthe trade-off. The novel framework facilitates future simulations of diverse\nlinguistic aspects, emphasizing the importance of interaction and group\ndynamics in language evolution."
        ],
        "neg": []
    },
    {
        "query": "Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.",
        "pos": [
            "While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR"
        ],
        "neg": []
    },
    {
        "query": "Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.",
        "pos": [
            "While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR",
            "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community."
        ],
        "neg": []
    },
    {
        "query": "Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.",
        "pos": [
            "While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR"
        ],
        "neg": []
    },
    {
        "query": "While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR",
        "pos": [
            "While large language models (LLMs) have demonstrated remarkable abilities\nacross various fields, hallucination remains a significant challenge. Recent\nstudies have explored hallucinations through the lens of internal\nrepresentations, proposing mechanisms to decipher LLMs' adherence to facts.\nHowever, these approaches often fail to generalize to out-of-distribution data,\nleading to concerns about whether internal representation patterns reflect\nfundamental factual awareness, or only overfit spurious correlations on the\nspecific datasets. In this work, we investigate whether a universal\ntruthfulness hyperplane that distinguishes the model's factually correct and\nincorrect outputs exists within the model. To this end, we scale up the number\nof training datasets and conduct an extensive evaluation -- we train the\ntruthfulness hyperplane on a diverse collection of over 40 datasets and examine\nits cross-task, cross-domain, and in-domain generalization. Our results\nindicate that increasing the diversity of the training datasets significantly\nenhances the performance in all scenarios, while the volume of data samples\nplays a less critical role. This finding supports the optimistic hypothesis\nthat a universal truthfulness hyperplane may indeed exist within the model,\noffering promising directions for future research.",
            "Recent advancements in Vision-Language Models (VLMs) have led to the\ndevelopment of Vision-Language Generalists (VLGs) capable of understanding and\ngenerating interleaved images and text. Despite these advances, VLGs still\nstruggle to follow user instructions for interleaved text and image generation.\nTo address this issue, we introduce LeafInstruct, the first open-sourced\ninterleaved instruction tuning data with over 30,000 high-quality instances\nacross more than 10 domains. Due to the extensive size of existing VLGs, we opt\nfor parameter-efficient tuning. However, we observe that VLGs tuned with a\nstandard LoRA typically exhibit inferior performance in interleaved text-image\ngeneration. We attribute this problem to modality interference and the lack of\nmodality-specialized adaptation design. Hence, we propose Lateralization LoRA,\na novel modality-specialized adaptation method inspired by the concept of brain\nlateralization. Lateralization LoRA employs a hybrid approach, combining the\ntraditional linear LoRA and a Convolutional LoRA for generating text and\nimages, enabling the generation of high-quality text and images by leveraging\nmodality-specific structures and parameter sets. We perform instruction tuning\nof the VLG (i.e., EMU2) using Lateralization LoRA on the LeafInstruct dataset.\nExtensive experiments demonstrate that EMU2 tuned with Lateralization LoRA\nachieve state-of-the-art performance, significantly surpassing baseline models\nin complex interleaved tasks.",
            "In this paper, we introduce Dynamic Layer Operations (DLO), a novel approach\nfor vertically scaling transformer-based Large Language Models (LLMs) by\ndynamically expanding, activating, or skipping layers using a sophisticated\nrouting policy based on layerwise feature similarity. Unlike traditional\nMixture-of-Experts (MoE) methods that focus on extending the model width, our\napproach targets model depth, addressing the redundancy observed across layer\nrepresentations for various input samples. Our framework is integrated with the\nSupervised Fine-Tuning (SFT) stage, eliminating the need for resource-intensive\nContinual Pre-Training (CPT). Experimental results demonstrate that DLO not\nonly outperforms the original unscaled models but also achieves comparable\nresults to densely expanded models with significantly improved efficiency. Our\nwork offers a promising direction for building efficient yet powerful LLMs. We\nwill release our implementation and model weights upon acceptance."
        ],
        "neg": []
    },
    {
        "query": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
        "pos": [
            "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon.",
            "The advent of clinical language models integrated into electronic health\nrecords (EHR) for clinical decision support has marked a significant\nadvancement, leveraging the depth of clinical notes for improved\ndecision-making. Despite their success, the potential vulnerabilities of these\nmodels remain largely unexplored. This paper delves into the realm of backdoor\nattacks on clinical language models, introducing an innovative attention-based\nbackdoor attack method, BadCLM (Bad Clinical Language Models). This technique\nclandestinely embeds a backdoor within the models, causing them to produce\nincorrect predictions when a pre-defined trigger is present in inputs, while\nfunctioning accurately otherwise. We demonstrate the efficacy of BadCLM through\nan in-hospital mortality prediction task with MIMIC III dataset, showcasing its\npotential to compromise model integrity. Our findings illuminate a significant\nsecurity risk in clinical decision support systems and pave the way for future\nendeavors in fortifying clinical language models against such vulnerabilities."
        ],
        "neg": []
    },
    {
        "query": "YouTube is a major social media platform that plays a significant role in\ndigital culture, with content creators at its core. These creators often engage\nin controversial behaviour to drive engagement, which can foster toxicity. This\npaper presents a quantitative analysis of controversial content on YouTube,\nfocusing on the relationship between controversy, toxicity, and monetisation.\nWe introduce a curated dataset comprising 20 controversial YouTube channels\nextracted from Reddit discussions, including 16,349 videos and more than 105\nmillion comments. We identify and categorise monetisation cues from video\ndescriptions into various models, including affiliate marketing and direct\nselling, using lists of URLs and keywords. Additionally, we train a machine\nlearning model to measure the toxicity of comments in these videos. Our\nfindings reveal that while toxic comments correlate with higher engagement,\nthey negatively impact monetisation, indicating that controversy-driven\ninteraction does not necessarily lead to financial gain. We also observed\nsignificant variation in monetisation strategies, with some creators showing\nextensive monetisation despite high toxicity levels. Our study introduces a\ncurated dataset, lists of URLs and keywords to categorise monetisation, a\nmachine learning model to measure toxicity, and is a significant step towards\nunderstanding the complex relationship between controversy, engagement, and\nmonetisation on YouTube. The lists used for detecting and categorising\nmonetisation cues are available on https://github.com/thalesbertaglia/toxmon.",
        "pos": [
            "Content monetization on social media fuels a growing influencer economy.\nInfluencer marketing remains largely undisclosed or inappropriately disclosed\non social media. Non-disclosure issues have become a priority for national and\nsupranational authorities worldwide, who are starting to impose increasingly\nharsher sanctions on them. This paper proposes a transparent methodology for\nmeasuring whether and how influencers comply with disclosures based on legal\nstandards. We introduce a novel distinction between disclosures that are\nlegally sufficient (green) and legally insufficient (yellow). We apply this\nmethodology to an original dataset reflecting the content of 150 Dutch\ninfluencers publicly registered with the Dutch Media Authority based on\nrecently introduced registration obligations. The dataset consists of 292,315\nposts and is multi-language (English and Dutch) and cross-platform (Instagram,\nYouTube and TikTok). We find that influencer marketing remains generally\nunderdisclosed on social media, and that bigger influencers are not necessarily\nmore compliant with disclosure standards."
        ],
        "neg": []
    },
    {
        "query": "YouTube is a major social media platform that plays a significant role in\ndigital culture, with content creators at its core. These creators often engage\nin controversial behaviour to drive engagement, which can foster toxicity. This\npaper presents a quantitative analysis of controversial content on YouTube,\nfocusing on the relationship between controversy, toxicity, and monetisation.\nWe introduce a curated dataset comprising 20 controversial YouTube channels\nextracted from Reddit discussions, including 16,349 videos and more than 105\nmillion comments. We identify and categorise monetisation cues from video\ndescriptions into various models, including affiliate marketing and direct\nselling, using lists of URLs and keywords. Additionally, we train a machine\nlearning model to measure the toxicity of comments in these videos. Our\nfindings reveal that while toxic comments correlate with higher engagement,\nthey negatively impact monetisation, indicating that controversy-driven\ninteraction does not necessarily lead to financial gain. We also observed\nsignificant variation in monetisation strategies, with some creators showing\nextensive monetisation despite high toxicity levels. Our study introduces a\ncurated dataset, lists of URLs and keywords to categorise monetisation, a\nmachine learning model to measure toxicity, and is a significant step towards\nunderstanding the complex relationship between controversy, engagement, and\nmonetisation on YouTube. The lists used for detecting and categorising\nmonetisation cues are available on https://github.com/thalesbertaglia/toxmon.",
        "pos": [
            "Content monetization on social media fuels a growing influencer economy.\nInfluencer marketing remains largely undisclosed or inappropriately disclosed\non social media. Non-disclosure issues have become a priority for national and\nsupranational authorities worldwide, who are starting to impose increasingly\nharsher sanctions on them. This paper proposes a transparent methodology for\nmeasuring whether and how influencers comply with disclosures based on legal\nstandards. We introduce a novel distinction between disclosures that are\nlegally sufficient (green) and legally insufficient (yellow). We apply this\nmethodology to an original dataset reflecting the content of 150 Dutch\ninfluencers publicly registered with the Dutch Media Authority based on\nrecently introduced registration obligations. The dataset consists of 292,315\nposts and is multi-language (English and Dutch) and cross-platform (Instagram,\nYouTube and TikTok). We find that influencer marketing remains generally\nunderdisclosed on social media, and that bigger influencers are not necessarily\nmore compliant with disclosure standards."
        ],
        "neg": []
    },
    {
        "query": "Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.",
        "pos": [
            "Data quality stands at the forefront of deciding the effectiveness of\nvideo-language representation learning. However, video-text pairs in previous\ndata typically do not align perfectly with each other, which might lead to\nvideo-language representations that do not accurately reflect cross-modal\nsemantics. Moreover, previous data also possess an uneven distribution of\nconcepts, thereby hampering the downstream performance across unpopular\nsubjects. To address these problems, we propose a contrastive objective with a\nsubtractive angular margin to regularize cross-modal representations in their\neffort to reach perfect similarity. Furthermore, to adapt to the non-uniform\nconcept distribution, we propose a multi-layer perceptron (MLP)-parameterized\nweighting function that maps loss values to sample weights which enable dynamic\nadjustment of the model's focus throughout the training. With the training\nguided by a small amount of unbiased meta-data and augmented by video-text data\ngenerated by large vision-language model, we improve video-language\nrepresentations and achieve superior performances on commonly used video\nquestion answering and text-video retrieval datasets."
        ],
        "neg": []
    },
    {
        "query": "Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.",
        "pos": [
            "Large Language Models (LLMs) have shown remarkable abilities across various\ntasks, yet their development has predominantly centered on high-resource\nlanguages like English and Chinese, leaving low-resource languages underserved.\nTo address this disparity, we present SeaLLMs 3, the latest iteration of the\nSeaLLMs model family, tailored for Southeast Asian languages. This region,\ncharacterized by its rich linguistic diversity, has lacked adequate language\ntechnology support. SeaLLMs 3 aims to bridge this gap by covering a\ncomprehensive range of languages spoken in this region, including English,\nChinese, Indonesian, Vietnamese, Thai, Tagalog, Malay, Burmese, Khmer, Lao,\nTamil, and Javanese. Leveraging efficient language enhancement techniques and a\nspecially constructed instruction tuning dataset, SeaLLMs 3 significantly\nreduces training costs while maintaining high performance and versatility. Our\nmodel excels in tasks such as world knowledge, mathematical reasoning,\ntranslation, and instruction following, achieving state-of-the-art performance\namong similarly sized models. Additionally, we prioritized safety and\nreliability by addressing both general and culture-specific considerations and\nincorporated mechanisms to reduce hallucinations. This work underscores the\nimportance of inclusive AI, showing that advanced LLM capabilities can benefit\nunderserved linguistic and cultural communities."
        ],
        "neg": []
    },
    {
        "query": "Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.",
        "pos": [
            "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
        ],
        "neg": []
    },
    {
        "query": "Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.",
        "pos": [
            "In this paper, we introduce the second-place solution in the KDD-2024\nOAG-Challenge paper source tracing track. Our solution is mainly based on two\nmethods, BERT and GCN, and combines the reasoning results of BERT and GCN in\nthe final submission to achieve complementary performance. In the BERT\nsolution, we focus on processing the fragments that appear in the references of\nthe paper, and use a variety of operations to reduce the redundant interference\nin the fragments, so that the information received by BERT is more refined. In\nthe GCN solution, we map information such as paper fragments, abstracts, and\ntitles to a high-dimensional semantic space through an embedding model, and try\nto build edges between titles, abstracts, and fragments to integrate contextual\nrelationships for judgment. In the end, our solution achieved a remarkable\nscore of 0.47691 in the competition."
        ],
        "neg": []
    },
    {
        "query": "Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.",
        "pos": [
            "The rapid evolution of large language models (LLMs) represents a substantial\nleap forward in natural language understanding and generation. However,\nalongside these advancements come significant challenges related to the\naccountability and transparency of LLM responses. Reliable source attribution\nis essential to adhering to stringent legal and regulatory standards, including\nthose set forth by the General Data Protection Regulation. Despite the\nwell-established methods in source attribution within the computer vision\ndomain, the application of robust attribution frameworks to natural language\nprocessing remains underexplored. To bridge this gap, we propose a novel and\nversatile TRansformer-based Attribution framework using Contrastive Embeddings\ncalled TRACE that, in particular, exploits contrastive learning for source\nattribution. We perform an extensive empirical evaluation to demonstrate the\nperformance and efficiency of TRACE in various settings and show that TRACE\nsignificantly improves the ability to attribute sources accurately, making it a\nvaluable tool for enhancing the reliability and trustworthiness of LLMs.",
            "Data quality stands at the forefront of deciding the effectiveness of\nvideo-language representation learning. However, video-text pairs in previous\ndata typically do not align perfectly with each other, which might lead to\nvideo-language representations that do not accurately reflect cross-modal\nsemantics. Moreover, previous data also possess an uneven distribution of\nconcepts, thereby hampering the downstream performance across unpopular\nsubjects. To address these problems, we propose a contrastive objective with a\nsubtractive angular margin to regularize cross-modal representations in their\neffort to reach perfect similarity. Furthermore, to adapt to the non-uniform\nconcept distribution, we propose a multi-layer perceptron (MLP)-parameterized\nweighting function that maps loss values to sample weights which enable dynamic\nadjustment of the model's focus throughout the training. With the training\nguided by a small amount of unbiased meta-data and augmented by video-text data\ngenerated by large vision-language model, we improve video-language\nrepresentations and achieve superior performances on commonly used video\nquestion answering and text-video retrieval datasets."
        ],
        "neg": []
    },
    {
        "query": "The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. In this paper, we focus\non machine translation (MT), a task that has been shown to benefit from\nin-context translation examples. However no systematic studies have been\npublished on how best to select examples, and mixed results have been reported\non the usefulness of similarity-based selection over random selection. We\nprovide a study covering multiple LLMs and multiple in-context example\nretrieval strategies, comparing multilingual sentence embeddings. We cover\nseveral language directions, representing different levels of language\nresourcedness (English into French, German, Swahili and Wolof). Contrarily to\npreviously published results, we find that sentence embedding similarity can\nimprove MT, especially for low-resource language directions, and discuss the\nbalance between selection pool diversity and quality. We also highlight\npotential problems with the evaluation of LLM-based MT and suggest a more\nappropriate evaluation protocol, adapting the COMET metric to the evaluation of\nLLMs. Code and outputs are freely available at\nhttps://github.com/ArmelRandy/ICL-MT.",
        "pos": [
            "This is the preliminary ranking of WMT24 General MT systems based on\nautomatic metrics. The official ranking will be a human evaluation, which is\nsuperior to the automatic ranking and supersedes it. The purpose of this report\nis not to interpret any findings but only provide preliminary results to the\nparticipants of the General MT task that may be useful during the writing of\nthe system submission.",
            "Current multimodal machine translation (MMT) systems rely on fully supervised\ndata (i.e models are trained on sentences with their translations and\naccompanying images). However, this type of data is costly to collect, limiting\nthe extension of MMT to other language pairs for which such data does not\nexist. In this work, we propose a method to bypass the need for fully\nsupervised data to train MMT systems, using multimodal English data only. Our\nmethod, called ZeroMMT, consists in adapting a strong text-only machine\ntranslation (MT) model by training it on a mixture of two objectives: visually\nconditioned masked language modelling and the Kullback-Leibler divergence\nbetween the original and new MMT outputs. We evaluate on standard MMT\nbenchmarks and the recently released CoMMuTE, a contrastive benchmark aiming to\nevaluate how well models use images to disambiguate English sentences. We\nobtain disambiguation performance close to state-of-the-art MMT models trained\nadditionally on fully supervised examples. To prove that our method generalizes\nto languages with no fully supervised training data available, we extend the\nCoMMuTE evaluation dataset to three new languages: Arabic, Russian and Chinese.\nWe further show that we can control the trade-off between disambiguation\ncapabilities and translation fidelity at inference time using classifier-free\nguidance and without any additional data. Our code, data and trained models are\npublicly accessible."
        ],
        "neg": []
    },
    {
        "query": "Traditional legal retrieval systems designed to retrieve legal documents,\nstatutes, precedents, and other legal information are unable to give\nsatisfactory answers due to lack of semantic understanding of specific\nquestions. Large Language Models (LLMs) have achieved excellent results in a\nvariety of natural language processing tasks, which inspired us that we train a\nLLM in the legal domain to help legal retrieval. However, in the Chinese legal\ndomain, due to the complexity of legal questions and the rigour of legal\narticles, there is no legal large model with satisfactory practical application\nyet. In this paper, we present DeliLaw, a Chinese legal counselling system\nbased on a large language model. DeliLaw integrates a legal retrieval module\nand a case retrieval module to overcome the model hallucination. Users can\nconsult professional legal questions, search for legal articles and relevant\njudgement cases, etc. on the DeliLaw system in a dialogue mode. In addition,\nDeliLaw supports the use of English for counseling. we provide the address of\nthe system: https://data.delilegal.com/lawQuestion.",
        "pos": [
            "The Mutual Reinforcement Effect (MRE) represents a promising avenue in\ninformation extraction and multitasking research. Nevertheless, its\napplicability has been constrained due to the exclusive availability of MRE mix\ndatasets in Japanese, thereby limiting comprehensive exploration by the global\nresearch community. To address this limitation, we introduce a Multilingual MRE\nmix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, and\nChinese. In this paper, we also propose a method for dataset translation\nassisted by Large Language Models (LLMs), which significantly reduces the\nmanual annotation time required for dataset construction by leveraging LLMs to\ntranslate the original Japanese datasets. Additionally, we have enriched the\ndataset by incorporating open-domain Named Entity Recognition (NER) and\nsentence classification tasks. Utilizing this expanded dataset, we developed a\nunified input-output framework to train an Open-domain Information Extraction\nLarge Language Model (OIELLM). The OIELLM model demonstrates the capability to\neffectively process novel MMM datasets, exhibiting significant improvements in\nperformance."
        ],
        "neg": []
    },
    {
        "query": "This research investigates biases in text-to-image (TTI) models for the Indic\nlanguages widely spoken across India. It evaluates and compares the generative\nperformance and cultural relevance of leading TTI models in these languages\nagainst their performance in English. Using the proposed IndicTTI benchmark, we\ncomprehensively assess the performance of 30 Indic languages with two\nopen-source diffusion models and two commercial generation APIs. The primary\nobjective of this benchmark is to evaluate the support for Indic languages in\nthese models and identify areas needing improvement. Given the linguistic\ndiversity of 30 languages spoken by over 1.4 billion people, this benchmark\naims to provide a detailed and insightful analysis of TTI models' effectiveness\nwithin the Indic linguistic landscape. The data and code for the IndicTTI\nbenchmark can be accessed at\nhttps://iab-rubric.org/resources/other-databases/indictti.",
        "pos": [
            "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment."
        ],
        "neg": []
    },
    {
        "query": "In-context learning (ICL) capabilities are foundational to the success of\nlarge language models (LLMs). Recently, context compression has attracted\ngrowing interest since it can largely reduce reasoning complexities and\ncomputation costs of LLMs. In this paper, we introduce a novel Query-gUIded\naTtention cOmpression (QUITO) method, which leverages attention of the question\nover the contexts to filter useless information. Specifically, we take a\ntrigger token to calculate the attention distribution of the context in\nresponse to the question. Based on the distribution, we propose three different\nfiltering methods to satisfy the budget constraints of the context length. We\nevaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and\nASQA. Experimental results demonstrate that QUITO significantly outperforms\nestablished baselines across various datasets and downstream LLMs, underscoring\nits effectiveness. Our code is available at\nhttps://github.com/Wenshansilvia/attention_compressor.",
        "pos": [
            "Recent advances in neural information retrieval (IR) models have\nsignificantly enhanced their effectiveness over various IR tasks. The\nrobustness of these models, essential for ensuring their reliability in\npractice, has also garnered significant attention. With a wide array of\nresearch on robust IR being proposed, we believe it is the opportune moment to\nconsolidate the current status, glean insights from existing methodologies, and\nlay the groundwork for future development. We view the robustness of IR to be a\nmultifaceted concept, emphasizing its necessity against adversarial attacks,\nout-of-distribution (OOD) scenarios and performance variance. With a focus on\nadversarial and OOD robustness, we dissect robustness solutions for dense\nretrieval models (DRMs) and neural ranking models (NRMs), respectively,\nrecognizing them as pivotal components of the neural IR pipeline. We provide an\nin-depth discussion of existing methods, datasets, and evaluation metrics,\nshedding light on challenges and future directions in the era of large language\nmodels. To the best of our knowledge, this is the first comprehensive survey on\nthe robustness of neural IR models, and we will also be giving our first\ntutorial presentation at SIGIR 2024\n\\url{https://sigir2024-robust-information-retrieval.github.io}. Along with the\norganization of existing work, we introduce a Benchmark for robust IR (BestIR),\na heterogeneous evaluation benchmark for robust neural information retrieval,\nwhich is publicly available at \\url{https://github.com/Davion-Liu/BestIR}. We\nhope that this study provides useful clues for future research on the\nrobustness of IR models and helps to develop trustworthy search engines\n\\url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}."
        ],
        "neg": []
    },
    {
        "query": "In-context learning (ICL) capabilities are foundational to the success of\nlarge language models (LLMs). Recently, context compression has attracted\ngrowing interest since it can largely reduce reasoning complexities and\ncomputation costs of LLMs. In this paper, we introduce a novel Query-gUIded\naTtention cOmpression (QUITO) method, which leverages attention of the question\nover the contexts to filter useless information. Specifically, we take a\ntrigger token to calculate the attention distribution of the context in\nresponse to the question. Based on the distribution, we propose three different\nfiltering methods to satisfy the budget constraints of the context length. We\nevaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and\nASQA. Experimental results demonstrate that QUITO significantly outperforms\nestablished baselines across various datasets and downstream LLMs, underscoring\nits effectiveness. Our code is available at\nhttps://github.com/Wenshansilvia/attention_compressor.",
        "pos": [
            "Recent advances in neural information retrieval (IR) models have\nsignificantly enhanced their effectiveness over various IR tasks. The\nrobustness of these models, essential for ensuring their reliability in\npractice, has also garnered significant attention. With a wide array of\nresearch on robust IR being proposed, we believe it is the opportune moment to\nconsolidate the current status, glean insights from existing methodologies, and\nlay the groundwork for future development. We view the robustness of IR to be a\nmultifaceted concept, emphasizing its necessity against adversarial attacks,\nout-of-distribution (OOD) scenarios and performance variance. With a focus on\nadversarial and OOD robustness, we dissect robustness solutions for dense\nretrieval models (DRMs) and neural ranking models (NRMs), respectively,\nrecognizing them as pivotal components of the neural IR pipeline. We provide an\nin-depth discussion of existing methods, datasets, and evaluation metrics,\nshedding light on challenges and future directions in the era of large language\nmodels. To the best of our knowledge, this is the first comprehensive survey on\nthe robustness of neural IR models, and we will also be giving our first\ntutorial presentation at SIGIR 2024\n\\url{https://sigir2024-robust-information-retrieval.github.io}. Along with the\norganization of existing work, we introduce a Benchmark for robust IR (BestIR),\na heterogeneous evaluation benchmark for robust neural information retrieval,\nwhich is publicly available at \\url{https://github.com/Davion-Liu/BestIR}. We\nhope that this study provides useful clues for future research on the\nrobustness of IR models and helps to develop trustworthy search engines\n\\url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}."
        ],
        "neg": []
    },
    {
        "query": "The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.",
        "pos": [
            "Synthetic data is becoming increasingly important for accelerating the\ndevelopment of language models, both large and small. Despite several\nsuccessful use cases, researchers also raised concerns around model collapse\nand drawbacks of imitating other models. This discrepancy can be attributed to\nthe fact that synthetic data varies in quality and diversity. Effective use of\nsynthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data\nby powerful models to teach a new skill or behavior to another model, we refer\nto this setting as Generative Teaching. We introduce AgentInstruct, an\nextensible agentic framework for automatically creating large amounts of\ndiverse and high-quality synthetic data. AgentInstruct can create both the\nprompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post\ntraining dataset of 25M pairs to teach language models different skills, such\nas text editing, creative writing, tool usage, coding, reading comprehension,\netc. The dataset can be used for instruction tuning of any base model. We\npost-train Mistral-7b with the data. When comparing the resulting model Orca-3\nto Mistral-7b-Instruct (which uses the same base model), we observe significant\nimprovements across many benchmarks. For example, 40% improvement on AGIEval,\n19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and\n45% improvement on AlpacaEval. Additionally, it consistently outperforms other\nmodels such as LLAMA-8B-instruct and GPT-3.5-turbo."
        ],
        "neg": []
    },
    {
        "query": "The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.",
        "pos": [
            "Reinforcement learning with human feedback (RLHF), as a widely adopted\napproach in current large language model pipelines, is \\textit{bottlenecked by\nthe size of human preference data}. While traditional methods rely on offline\npreference dataset constructions, recent approaches have shifted towards online\nsettings, where a learner uses a small amount of labeled seed data and a large\npool of unlabeled prompts to iteratively construct new preference data through\nself-generated responses and high-quality reward/preference feedback. However,\nmost current online algorithms still focus on preference labeling during policy\nmodel updating with given feedback oracles, which incurs significant expert\nquery costs. \\textit{We are the first to explore cost-effective proxy reward\noracles construction strategies for further labeling preferences or rewards\nwith extremely limited labeled data and expert query budgets}. Our approach\nintroduces two key innovations: (1) on-policy query to avoid OOD and imbalance\nissues in seed data, and (2) active learning to select the most informative\ndata for preference queries. Using these methods, we train a evaluation model\nwith minimal expert-labeled data, which then effectively labels nine times more\npreference pairs for further RLHF training. For instance, our model using\nDirect Preference Optimization (DPO) gains around over 1% average improvement\non AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our\nmethodology is orthogonal to other direct expert query-based strategies and\ntherefore might be integrated with them to further reduce query costs."
        ],
        "neg": []
    },
    {
        "query": "The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.",
        "pos": [
            "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and\nMistral-7B and then evaluating them across a comprehensive suite of\nmultilingual benchmarks that test reasoning, question answering, and reading\ncomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with\nsPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared\nto the baselines. We also devise a strategy to incorporate N-shot examples in\neach fine-tuning sample which further boosts the performance of these models by\n3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other\nmultilingual instruction tuning datasets on the same benchmarks along with\nbeing sample efficient and diverse, thereby reducing dataset creation costs.\nAdditionally, instruction tuning with sPhinX does not lead to regression on\nmost standard LLM benchmarks.",
            "Synthetic data is becoming increasingly important for accelerating the\ndevelopment of language models, both large and small. Despite several\nsuccessful use cases, researchers also raised concerns around model collapse\nand drawbacks of imitating other models. This discrepancy can be attributed to\nthe fact that synthetic data varies in quality and diversity. Effective use of\nsynthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data\nby powerful models to teach a new skill or behavior to another model, we refer\nto this setting as Generative Teaching. We introduce AgentInstruct, an\nextensible agentic framework for automatically creating large amounts of\ndiverse and high-quality synthetic data. AgentInstruct can create both the\nprompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post\ntraining dataset of 25M pairs to teach language models different skills, such\nas text editing, creative writing, tool usage, coding, reading comprehension,\netc. The dataset can be used for instruction tuning of any base model. We\npost-train Mistral-7b with the data. When comparing the resulting model Orca-3\nto Mistral-7b-Instruct (which uses the same base model), we observe significant\nimprovements across many benchmarks. For example, 40% improvement on AGIEval,\n19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and\n45% improvement on AlpacaEval. Additionally, it consistently outperforms other\nmodels such as LLAMA-8B-instruct and GPT-3.5-turbo."
        ],
        "neg": []
    },
    {
        "query": "This study introduces a taxonomy of stereotype content in contemporary large\nlanguage models (LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three\npowerful and widely used LLMs, for the characteristics associated with 87\nsocial categories (e.g., gender, race, occupations). We identify 14 stereotype\ndimensions (e.g., Morality, Ability, Health, Beliefs, Emotions), accounting for\n~90% of LLM stereotype associations. Warmth and Competence facets were the most\nfrequent content, but all other dimensions were significantly prevalent.\nStereotypes were more positive in LLMs (vs. humans), but there was significant\nvariability across categories and dimensions. Finally, the taxonomy predicted\nthe LLMs' internal evaluations of social categories (e.g., how\npositively/negatively the categories were represented), supporting the\nrelevance of a multidimensional taxonomy for characterizing LLM stereotypes.\nOur findings suggest that high-dimensional human stereotypes are reflected in\nLLMs and must be considered in AI auditing and debiasing to minimize\nunidentified harms from reliance in low-dimensional views of bias in LLMs.",
        "pos": [
            "Artificial intelligence (AI) hiring tools have revolutionized resume\nscreening, and large language models (LLMs) have the potential to do the same.\nHowever, given the biases which are embedded within LLMs, it is unclear whether\nthey can be used in this scenario without disadvantaging groups based on their\nprotected attributes. In this work, we investigate the possibilities of using\nLLMs in a resume screening setting via a document retrieval framework that\nsimulates job candidate selection. Using that framework, we then perform a\nresume audit study to determine whether a selection of Massive Text Embedding\n(MTE) models are biased in resume screening scenarios. We simulate this for\nnine occupations, using a collection of over 500 publicly available resumes and\n500 job descriptions. We find that the MTEs are biased, significantly favoring\nWhite-associated names in 85.1\\% of cases and female-associated names in only\n11.1\\% of cases, with a minority of cases showing no statistically significant\ndifferences. Further analyses show that Black males are disadvantaged in up to\n100\\% of cases, replicating real-world patterns of bias in employment settings,\nand validate three hypotheses of intersectionality. We also find an impact of\ndocument length as well as the corpus frequency of names in the selection of\nresumes. These findings have implications for widely used AI tools that are\nautomating employment, fairness, and tech policy.",
            "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices\nin their training data and reinforcing societal stereotypes and inequalities.\nOur work explores the potential of the Contact Hypothesis, a concept from\nsocial psychology for debiasing LLMs. We simulate various forms of social\ncontact through LLM prompting to measure their influence on the model's biases,\nmirroring how intergroup interactions can reduce prejudices in social contexts.\nWe create a dataset of 108,000 prompts following a principled approach\nreplicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and\nNousHermes) across 13 social bias dimensions. We propose a unique debiasing\ntechnique, Social Contact Debiasing (SCD), that instruction-tunes these models\nwith unbiased responses to prompts. Our research demonstrates that LLM\nresponses exhibit social biases when subject to contact probing, but more\nimportantly, these biases can be significantly reduced by up to 40% in 1 epoch\nof instruction tuning LLaMA 2 following our SCD strategy. Our code and data are\navailable at https://github.com/chahatraj/breakingbias."
        ],
        "neg": []
    },
    {
        "query": "This paper presents a shared task that we organized at the Foundations of\nLanguage Technology (FoLT) course in 2023/2024 at the Technical University of\nDarmstadt, which focuses on evaluating the output of Large Language Models\n(LLMs) in generating harmful answers to health-related clinical questions. We\ndescribe the task design considerations and report the feedback we received\nfrom the students. We expect the task and the findings reported in this paper\nto be relevant for instructors teaching natural language processing (NLP) and\ndesigning course assignments.",
        "pos": [
            "Large language models (LLMs) bring unprecedented flexibility in defining and\nexecuting complex, creative natural language generation (NLG) tasks. Yet, this\nflexibility brings new challenges, as it introduces new degrees of freedom in\nformulating the task inputs and instructions and in evaluating model\nperformance. To facilitate the exploration of creative NLG tasks, we propose a\nthree-component research framework that consists of systematic input\nmanipulation, reference data, and output measurement. We use this framework to\nexplore citation text generation -- a popular scholarly NLP task that lacks\nconsensus on the task definition and evaluation metric and has not yet been\ntackled within the LLM paradigm. Our results highlight the importance of\nsystematically investigating both task instruction and input configuration when\nprompting LLMs, and reveal non-trivial relationships between different\nevaluation metrics used for citation text generation. Additional human\ngeneration and human evaluation experiments provide new qualitative insights\ninto the task to guide future research in citation text generation. We make our\ncode and data publicly available."
        ],
        "neg": []
    },
    {
        "query": "This paper presents a shared task that we organized at the Foundations of\nLanguage Technology (FoLT) course in 2023/2024 at the Technical University of\nDarmstadt, which focuses on evaluating the output of Large Language Models\n(LLMs) in generating harmful answers to health-related clinical questions. We\ndescribe the task design considerations and report the feedback we received\nfrom the students. We expect the task and the findings reported in this paper\nto be relevant for instructors teaching natural language processing (NLP) and\ndesigning course assignments.",
        "pos": [
            "Applying differential privacy (DP) by means of the DP-SGD algorithm to\nprotect individual data points during training is becoming increasingly popular\nin NLP. However, the choice of granularity at which DP is applied is often\nneglected. For example, neural machine translation (NMT) typically operates on\nthe sentence-level granularity. From the perspective of DP, this setup assumes\nthat each sentence belongs to a single person and any two sentences in the\ntraining dataset are independent. This assumption is however violated in many\nreal-world NMT datasets, e.g. those including dialogues. For proper application\nof DP we thus must shift from sentences to entire documents. In this paper, we\ninvestigate NMT at both the sentence and document levels, analyzing the\nprivacy/utility trade-off for both scenarios, and evaluating the risks of not\nusing the appropriate privacy granularity in terms of leaking personally\nidentifiable information (PII). Our findings indicate that the document-level\nNMT system is more resistant to membership inference attacks, emphasizing the\nsignificance of using the appropriate granularity when working with DP."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge here is to\ndevelop steerable language models that trade-off multiple (conflicting)\nobjectives in a flexible and efficient manner. This paper presents Conditioned\nLanguage Policy (CLP), a general framework for finetuning language models on\nmultiple objectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP can learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through an extensive set of experiments and ablations,\nwe show that the CLP framework learns steerable models that outperform and\nPareto-dominate the current state-of-the-art approaches for multi-objective\nfinetuning.",
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge here is to\ndevelop steerable language models that trade-off multiple (conflicting)\nobjectives in a flexible and efficient manner. This paper presents Conditioned\nLanguage Policy (CLP), a general framework for finetuning language models on\nmultiple objectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP can learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through an extensive set of experiments and ablations,\nwe show that the CLP framework learns steerable models that outperform and\nPareto-dominate the current state-of-the-art approaches for multi-objective\nfinetuning.",
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge here is to\ndevelop steerable language models that trade-off multiple (conflicting)\nobjectives in a flexible and efficient manner. This paper presents Conditioned\nLanguage Policy (CLP), a general framework for finetuning language models on\nmultiple objectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP can learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through an extensive set of experiments and ablations,\nwe show that the CLP framework learns steerable models that outperform and\nPareto-dominate the current state-of-the-art approaches for multi-objective\nfinetuning.",
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge here is to\ndevelop steerable language models that trade-off multiple (conflicting)\nobjectives in a flexible and efficient manner. This paper presents Conditioned\nLanguage Policy (CLP), a general framework for finetuning language models on\nmultiple objectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP can learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through an extensive set of experiments and ablations,\nwe show that the CLP framework learns steerable models that outperform and\nPareto-dominate the current state-of-the-art approaches for multi-objective\nfinetuning.",
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Large language models (LLMs) currently dominate the field of natural language\nprocessing (NLP), representing the state-of-the-art across a diverse array of\ntasks. Developing a model of this nature, from training to inference, requires\nmaking numerous decisions which define a combinatorial search problem. For\nexample, selecting the optimal pre-trained LLM, prompt, or hyperparameters to\nattain the best performance for a task often requires evaluating multiple\ncandidates on an entire test set. This exhaustive evaluation can be\ntime-consuming and costly, as both inference and metric computation with LLMs\nare resource-intensive. In this paper, we address the challenge of identifying\nthe best method within a limited budget for evaluating methods on test\nexamples. By leveraging the well-studied multi-armed bandit framework, which\nsequentially selects the next method-example pair to evaluate, our approach,\ncombining multi-armed bandit algorithms with low-rank factorization,\nsignificantly reduces the required resources. Experiments show that our\nalgorithms can identify the top-performing method using only 5-15\\% of the\ntypically needed resources, resulting in an 85-95\\% reduction in cost.",
            "This paper presents a novel approach to aligning large language models (LLMs)\nwith individual human preferences, sometimes referred to as Reinforcement\nLearning from \\textit{Personalized} Human Feedback (RLPHF). Given stated\npreferences along multiple dimensions, such as helpfulness, conciseness, or\nhumor, the goal is to create an LLM without re-training that best adheres to\nthis specification. Starting from specialized expert LLMs, each trained for one\nsuch particular preference dimension, we propose a black-box method that merges\ntheir outputs on a per-token level. We train a lightweight Preference Control\nModel (PCM) that dynamically translates the preference description and current\ncontext into next-token prediction weights. By combining the expert models'\noutputs at the token level, our approach dynamically generates text that\noptimizes the given preference. Empirical tests show that our method matches or\nsurpasses existing preference merging techniques, providing a scalable,\nefficient alternative to fine-tuning LLMs for individual personalization."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Web authors frequently embed social media to support and enrich their\ncontent, creating the potential to derive web-based, cross-platform social\nmedia representations that can enable more effective social media retrieval\nsystems and richer scientific analyses. As step toward such capabilities, we\nintroduce a novel language modeling framework that enables automatic annotation\nof roles that social media entities play in their embedded web context. Using\nrelated communication theory, we liken social media embeddings to quotes,\nformalize the page context as structured natural language signals, and identify\na taxonomy of roles for quotes within the page context. We release\nSocialQuotes, a new data set built from the Common Crawl of over 32 million\nsocial quotes, 8.3k of them with crowdsourced quote annotations. Using\nSocialQuotes and the accompanying annotations, we provide a role classification\ncase study, showing reasonable performance with modern-day LLMs, and exposing\nexplainable aspects of our framework via page content ablations. We also\nclassify a large batch of un-annotated quotes, revealing interesting\ncross-domain, cross-platform role distributions on the web."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "A critical component in knowledge distillation is the means of coupling the\nteacher and student. The predominant sequence knowledge distillation method\ninvolves supervised learning of the student against teacher-decoded outputs,\nand is exemplified by the current state of the art, which incorporates minimum\nBayes risk (MBR) decoding. In this paper we seek to integrate MBR more tightly\nin distillation training, specifically by using several high scoring MBR\ntranslations, rather than a single selected sequence, thus capturing a rich\ndiversity of teacher outputs. Our experiments on English to German and English\nto Japanese translation show consistent improvements over strong baseline\nmethods for both tasks and with varying model sizes. Additionally, we conduct a\ndetailed analysis focusing on data efficiency and capacity curse aspects to\nelucidate MBR-n and explore its further potential."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "As Large Language Models (LLMs) achieve remarkable performance across various\nNLP tasks, their reliability becomes essential for widespread adoption. This\npaper focuses on Abstention Ability (AA), a critical yet under explored aspect\nof reliability - the ability of LLMs to refrain from answering questions when\nthey are uncertain or when definitive answer is not possible, while maintaining\nquestion-answering (QA) task performance. While previous works have focused on\nunderstanding the recollection abilities of LLMs or their ability to identify\nimponderable/unanswerable questions, we believe there is a need for an\neffective AA evaluation method. Therefore, we propose a black-box evaluation\nmethodology to examine and understand the AA of LLMs across a variety of\nmultiple-choice QA tasks. We measure AA by rewarding models for abstaining from\nanswering when their predictions are incorrect or when the questions are\ninherently unanswerable. We investigate three strategies, Strict Prompting,\nVerbal Confidence Thresholding, and Chain-of-Thought (CoT), to understand their\nimpact on abstention across different LLMs. Our findings reveal that while even\nstate-of-the-art LLMs like GPT-4 struggle with abstention, strategic prompting\nsuch as CoT, can significantly enhance this ability. Furthermore, we\ndemonstrate that improving AA also leads to better overall QA task performance,\nunderscoring the importance of evaluating AA in LLMs."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "An important issue with Large Language Models (LLMs) is their undesired\nability to generate toxic language. In this work, we show that the neurons\nresponsible for toxicity can be determined by their power to discriminate toxic\nsentences, and that toxic language can be mitigated by reducing their\nactivation levels proportionally to this power. We propose AUROC adaptation\n(AurA), an intervention that can be applied to any pre-trained LLM to mitigate\ntoxicity. As the intervention is proportional to the ability of each neuron to\ndiscriminate toxic content, it is free of any model-dependent hyperparameters.\nWe show that AurA can achieve up to $2.2 \\times$ reduction in toxicity with\nonly a $0.72$ perplexity increase. We also show that AurA is effective with\nmodels of different scale (from 1.5B to 40B parameters), and its effectiveness\nin mitigating toxic language, while preserving common-sense zero-shot\nabilities, holds across all scales. AurA can be combined with pre-prompting\nstrategies, boosting its average mitigation potential from $1.28\\times$ to\n$2.35\\times$. Moreover, AurA can counteract adversarial pre-prompts that\nmaliciously elicit toxic content, making it an effective method for deploying\nsafer and less toxic models."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in\nNatural Language Processing, serving as critical components in a wide range of\napplications. In this paper, we propose ReLiK, a Retriever-Reader architecture\nfor both EL and RE, where, given an input text, the Retriever module undertakes\nthe identification of candidate entities or relations that could potentially\nappear within the text. Subsequently, the Reader module is tasked to discern\nthe pertinent retrieved entities or relations and establish their alignment\nwith the corresponding textual spans. Notably, we put forward an innovative\ninput representation that incorporates the candidate entities or relations\nalongside the text, making it possible to link entities or extract relations in\na single forward pass and to fully leverage pre-trained language models\ncontextualization capabilities, in contrast with previous\nRetriever-Reader-based methods, which require a forward pass for each\ncandidate. Our formulation of EL and RE achieves state-of-the-art performance\nin both in-domain and out-of-domain benchmarks while using academic budget\ntraining and with up to 40x inference speed compared to competitors. Finally,\nwe show how our architecture can be used seamlessly for Information Extraction\n(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared\nReader that simultaneously extracts entities and relations.",
        "pos": [
            "Large autoregressive generative models have emerged as the cornerstone for\nachieving the highest performance across several Natural Language Processing\ntasks. However, the urge to attain superior results has, at times, led to the\npremature replacement of carefully designed task-specific approaches without\nexhaustive experimentation. The Coreference Resolution task is no exception;\nall recent state-of-the-art solutions adopt large generative autoregressive\nmodels that outperform encoder-based discriminative systems. In this work,we\nchallenge this recent trend by introducing Maverick, a carefully designed - yet\nsimple - pipeline, which enables running a state-of-the-art Coreference\nResolution system within the constraints of an academic budget, outperforming\nmodels with up to 13 billion parameters with as few as 500 million parameters.\nMaverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,\ntraining with up to 0.006x the memory resources and obtaining a 170x faster\ninference compared to previous state-of-the-art systems. We extensively\nvalidate the robustness of the Maverick framework with an array of diverse\nexperiments, reporting improvements over prior systems in data-scarce,\nlong-document, and out-of-domain settings. We release our code and models for\nresearch purposes at https://github.com/SapienzaNLP/maverick-coref."
        ],
        "neg": []
    },
    {
        "query": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in\nNatural Language Processing, serving as critical components in a wide range of\napplications. In this paper, we propose ReLiK, a Retriever-Reader architecture\nfor both EL and RE, where, given an input text, the Retriever module undertakes\nthe identification of candidate entities or relations that could potentially\nappear within the text. Subsequently, the Reader module is tasked to discern\nthe pertinent retrieved entities or relations and establish their alignment\nwith the corresponding textual spans. Notably, we put forward an innovative\ninput representation that incorporates the candidate entities or relations\nalongside the text, making it possible to link entities or extract relations in\na single forward pass and to fully leverage pre-trained language models\ncontextualization capabilities, in contrast with previous\nRetriever-Reader-based methods, which require a forward pass for each\ncandidate. Our formulation of EL and RE achieves state-of-the-art performance\nin both in-domain and out-of-domain benchmarks while using academic budget\ntraining and with up to 40x inference speed compared to competitors. Finally,\nwe show how our architecture can be used seamlessly for Information Extraction\n(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared\nReader that simultaneously extracts entities and relations.",
        "pos": [
            "Large autoregressive generative models have emerged as the cornerstone for\nachieving the highest performance across several Natural Language Processing\ntasks. However, the urge to attain superior results has, at times, led to the\npremature replacement of carefully designed task-specific approaches without\nexhaustive experimentation. The Coreference Resolution task is no exception;\nall recent state-of-the-art solutions adopt large generative autoregressive\nmodels that outperform encoder-based discriminative systems. In this work,we\nchallenge this recent trend by introducing Maverick, a carefully designed - yet\nsimple - pipeline, which enables running a state-of-the-art Coreference\nResolution system within the constraints of an academic budget, outperforming\nmodels with up to 13 billion parameters with as few as 500 million parameters.\nMaverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,\ntraining with up to 0.006x the memory resources and obtaining a 170x faster\ninference compared to previous state-of-the-art systems. We extensively\nvalidate the robustness of the Maverick framework with an array of diverse\nexperiments, reporting improvements over prior systems in data-scarce,\nlong-document, and out-of-domain settings. We release our code and models for\nresearch purposes at https://github.com/SapienzaNLP/maverick-coref."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "Multilingual pretraining for transfer learning significantly boosts the\nrobustness of low-resource monolingual ASR models. This study systematically\ninvestigates three main aspects: (a) the impact of transfer learning on model\nperformance during initial training or fine-tuning, (b) the influence of\ntransfer learning across dataset domains and languages, and (c) the effect on\nrare-word recognition compared to non-rare words. Our finding suggests that\nRNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word\nError Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across\nlanguages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8%\ncompared to monolingual baselines for MLS and in-house datasets. Out-of-domain\npretraining leads to 28% higher WERR than in-domain pretraining. Both rare and\nnon-rare words benefit, with rare words showing greater improvements with\nout-of-domain pretraining, and non-rare words with in-domain pretraining."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "Large Language Models (LLMs) have been increasingly used in real-world\nsettings, yet their strategic abilities remain largely unexplored. Game theory\nprovides a good framework for assessing the decision-making abilities of LLMs\nin interactions with other agents. Although prior studies have shown that LLMs\ncan solve these tasks with carefully curated prompts, they fail when the\nproblem setting or prompt changes. In this work we investigate LLMs' behaviour\nin strategic games, Stag Hunt and Prisoner Dilemma, analyzing performance\nvariations under different settings and prompts. Our results show that the\ntested state-of-the-art LLMs exhibit at least one of the following systematic\nbiases: (1) positional bias, (2) payoff bias, or (3) behavioural bias.\nSubsequently, we observed that the LLMs' performance drops when the game\nconfiguration is misaligned with the affecting biases. Performance is assessed\nbased on the selection of the correct action, one which agrees with the\nprompted preferred behaviours of both players. Alignment refers to whether the\nLLM's bias aligns with the correct action. For example, GPT-4o's average\nperformance drops by 34% when misaligned. Additionally, the current trend of\n\"bigger and newer is better\" does not hold for the above, where GPT-4o (the\ncurrent best-performing LLM) suffers the most substantial performance drop.\nLastly, we note that while chain-of-thought prompting does reduce the effect of\nthe biases on most models, it is far from solving the problem at the\nfundamental level."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "Knowledge-based Visual Question Answering (KVQA) requires both image and\nworld knowledge to answer questions. Current methods first retrieve knowledge\nfrom the image and external knowledge base with the original complex question,\nthen generate answers with Large Language Models (LLMs). However, since the\noriginal question contains complex elements that require knowledge from\ndifferent sources, acquiring different kinds of knowledge in a coupled manner\nmay confuse models and hinder them from retrieving precise knowledge.\nFurthermore, the ``forward-only'' answering process fails to explicitly capture\nthe knowledge needs of LLMs, which can further hurt answering quality. To cope\nwith the above limitations, we propose DKA: Disentangled Knowledge Acquisition\nfrom LLM feedback, a training-free framework that disentangles knowledge\nacquisition to avoid confusion and uses LLM's feedback to specify the required\nknowledge. Specifically, DKA requires LLMs to specify what knowledge they need\nto answer the question and decompose the original complex question into two\nsimple sub-questions: Image-based sub-question and Knowledge-based\nsub-question. Then we use the two sub-questions to retrieve knowledge from the\nimage and knowledge base, respectively. In this way, two knowledge acquisition\nmodels can focus on the content that corresponds to them and avoid disturbance\nof irrelevant elements in the original complex question, which can help to\nprovide more precise knowledge and better align the knowledge needs of LLMs to\nyield correct answers. Experiments on benchmark datasets show that DKA\nsignificantly outperforms SOTA models. To facilitate future research, our data\nand code are available at \\url{https://github.com/Lackel/DKA}."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "The shift from professionally generated content (PGC) to user-generated\ncontent (UGC) has revolutionized various media formats, from text to video.\nWith the rapid advancements in generative AI, a similar shift is set to\ntransform the game industry, particularly in the realm of role-playing games\n(RPGs). This paper introduces a new framework for a text-to-game engine that\nutilizes foundation models to convert simple textual inputs into complex,\ninteractive RPG experiences. The engine dynamically renders the game story in a\nmulti-modal format and adjusts the game character, environment, and mechanics\nin real-time in response to player actions. Using this framework, we developed\nthe \"Zagii\" game engine, which has successfully supported hundreds of RPG games\nacross a diverse range of genres and facilitated tens of thousands of online\nuser gameplay instances. This validates the effectiveness of our frame-work.\nOur work showcases the potential for a more open and democratized gaming\nparadigm, highlighting the transformative impact of generative AI on the game\nlife cycle."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "Language, as an information medium created by advanced organisms, has always\nbeen a concern of neuroscience regarding how it is represented in the brain.\nDecoding linguistic representations in the evoked brain has shown\ngroundbreaking achievements, thanks to the rapid improvement of neuroimaging,\nmedical technology, life sciences and artificial intelligence. In this work, we\npresent a taxonomy of brain-to-language decoding of both textual and speech\nformats. This work integrates two types of research: neuroscience focusing on\nlanguage understanding and deep learning-based brain decoding. Generating\ndiscernible language information from brain activity could not only help those\nwith limited articulation, especially amyotrophic lateral sclerosis (ALS)\npatients but also open up a new way for the next generation's brain-computer\ninterface (BCI). This article will help brain scientists and deep-learning\nresearchers to gain a bird's eye view of fine-grained language perception, and\nthus facilitate their further investigation and research of neural process and\nlanguage decoding."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "A central piece in enabling intelligent agentic behavior in foundation models\nis to make them capable of introspecting upon their behavior, reasoning, and\ncorrecting their mistakes as more computation or interaction is available. Even\nthe strongest proprietary large language models (LLMs) do not quite exhibit the\nability of continually improving their responses sequentially, even in\nscenarios where they are explicitly told that they are making a mistake. In\nthis paper, we develop RISE: Recursive IntroSpEction, an approach for\nfine-tuning LLMs to introduce this capability, despite prior work hypothesizing\nthat this capability may not be possible to attain. Our approach prescribes an\niterative fine-tuning procedure, which attempts to teach the model how to alter\nits response after having executed previously unsuccessful attempts to solve a\nhard test-time problem, with optionally additional environment feedback. RISE\nposes fine-tuning for a single-turn prompt as solving a multi-turn Markov\ndecision process (MDP), where the initial state is the prompt. Inspired by\nprinciples in online imitation learning and reinforcement learning, we propose\nstrategies for multi-turn data collection and training so as to imbue an LLM\nwith the capability to recursively detect and correct its previous mistakes in\nsubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,\nand Mistral models to improve themselves with more turns on math reasoning\ntasks, outperforming several single-turn strategies given an equal amount of\ninference-time computation. We also find that RISE scales well, often attaining\nlarger benefits with more capable models. Our analysis shows that RISE makes\nmeaningful improvements to responses to arrive at the correct solution for\nchallenging prompts, without disrupting one-turn abilities as a result of\nexpressing more complex distributions."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "Knowledge distillation plays a key role in compressing the Large Language\nModels (LLMs), which boosts a small-size student model under large teacher\nmodels' guidance. However, existing LLM distillation methods overly rely on\nstudent-generated outputs, which may introduce generation errors and misguide\nthe distillation process. Moreover, the distillation loss functions introduced\nin previous art struggle to align the most informative part due to the complex\ndistribution of LLMs' outputs. To address these problems, we propose a\nmulti-granularity semantic revision method for LLM distillation. At the\nsequence level, we propose a sequence correction and re-generation (SCRG)\nstrategy. SCRG first calculates the semantic cognitive difference between the\nteacher and student to detect the error token, then corrects it with the\nteacher-generated one, and re-generates the sequence to reduce generation\nerrors and enhance generation diversity. At the token level, we design a\ndistribution adaptive clipping Kullback-Leibler (DAC-KL) loss as the\ndistillation objective function. DAC-KL loss exploits a learnable sub-network\nto adaptively extract semantically dense areas from the teacher's output,\navoiding the interference of redundant information in the distillation process.\nFinally, at the span level, we leverage the span priors of a sequence to\ncompute the probability correlations within spans, and constrain the teacher\nand student's probability correlations to be consistent, further enhancing the\ntransfer of semantic information. Extensive experiments across different model\nfamilies with parameters ranging from 0.1B to 13B demonstrate the superiority\nof our method compared to existing methods.",
            "With the rapid development of the large model domain, research related to\nfine-tuning has concurrently seen significant advancement, given that\nfine-tuning is a constituent part of the training process for large-scale\nmodels. Data engineering plays a fundamental role in the training process of\nmodels, which includes data infrastructure, data processing, etc. Data during\nfine-tuning likewise forms the base for large models. In order to embrace the\npower and explore new possibilities of fine-tuning datasets, this paper reviews\ncurrent public fine-tuning datasets from the perspective of data construction.\nAn overview of public fine-tuning datasets from two sides: evolution and\ntaxonomy, is provided in this review, aiming to chart the development\ntrajectory. Construction techniques and methods for public fine-tuning datasets\nof Large Language Models (LLMs), including data generation and data\naugmentation among others, are detailed. This elaboration follows the\naforementioned taxonomy, specifically across demonstration, comparison, and\ngeneralist categories. Additionally, a category tree of data generation\ntechniques has been abstracted in our review to assist researchers in gaining a\ndeeper understanding of fine-tuning datasets from the construction dimension.\nOur review also summarizes the construction features in different data\npreparation phases of current practices in this field, aiming to provide a\ncomprehensive overview and inform future research. Fine-tuning dataset\npractices, encompassing various data modalities, are also discussed from a\nconstruction perspective in our review. Towards the end of the article, we\noffer insights and considerations regarding the future construction and\ndevelopments of fine-tuning datasets.",
            "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT",
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer."
        ],
        "neg": []
    },
    {
        "query": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "pos": [
            "We tackle the task of text-to-speech (TTS) in Hebrew. Traditional Hebrew\ncontains Diacritics, which dictate the way individuals should pronounce given\nwords, however, modern Hebrew rarely uses them. The lack of diacritics in\nmodern Hebrew results in readers expected to conclude the correct pronunciation\nand understand which phonemes to use based on the context. This imposes a\nfundamental challenge on TTS systems to accurately map between text-to-speech.\nIn this work, we propose to adopt a language modeling Diacritics-Free approach,\nfor the task of Hebrew TTS. The model operates on discrete speech\nrepresentations and is conditioned on a word-piece tokenizer. We optimize the\nproposed method using in-the-wild weakly supervised data and compare it to\nseveral diacritic-based TTS systems. Results suggest the proposed method is\nsuperior to the evaluated baselines considering both content preservation and\nnaturalness of the generated speech. Samples can be found under the following\nlink: pages.cs.huji.ac.il/adiyoss-lab/HebTTS/",
            "We present HebDB, a weakly supervised dataset for spoken language processing\nin the Hebrew language. HebDB offers roughly 2500 hours of natural and\nspontaneous speech recordings in the Hebrew language, consisting of a large\nvariety of speakers and topics. We provide raw recordings together with a\npre-processed, weakly supervised, and filtered version. The goal of HebDB is to\nfurther enhance research and development of spoken language processing tools\nfor the Hebrew language. Hence, we additionally provide two baseline systems\nfor Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a\nfully supervised model. We present the performance of these two methods\noptimized on HebDB and compare them to current multi-lingual ASR alternatives.\nResults suggest the proposed method reaches better results than the evaluated\nbaselines considering similar model sizes. Dataset, code, and models are\npublicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/."
        ],
        "neg": []
    },
    {
        "query": "We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers.",
        "pos": [
            "Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents."
        ],
        "neg": []
    },
    {
        "query": "In recent years, with the rapid advancements in large language models (LLMs),\nachieving excellent empathetic response capabilities has become a crucial\nprerequisite. Consequently, managing and understanding empathetic datasets have\ngained increasing significance. However, empathetic data are typically\nhuman-labeled, leading to insufficient datasets and wasted human labor. In this\nwork, we present Synth-Empathy, an LLM-based data generation and quality and\ndiversity selection pipeline that automatically generates high-quality\nempathetic data while discarding low-quality data. With the data generated from\na low empathetic model, we are able to further improve empathetic response\nperformance and achieve state-of-the-art (SoTA) results across multiple\nbenchmarks. Moreover, our model achieves SoTA performance on various human\nevaluation benchmarks, demonstrating its effectiveness and robustness in\nreal-world applications. Furthermore, we show the trade-off between data\nquantity and quality, providing insights into empathetic data generation and\nselection.",
        "pos": [
            "Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).",
            "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient\nFine-Tuning (PEFT)method, significantly enhances the training efficiency by\nupdating only a small portion of the weights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques have also been applied to LoRA\nmethods to reduce the memory footprint of fine-tuning. However, applying\nweight-activation quantization to the LoRA pipeline is under-explored, and we\nobserve substantial performance degradation primarily due to the presence of\nactivation outliers. In this work, we propose RoLoRA, the first LoRA-based\nscheme for effective weight-activation quantization. RoLoRA utilizes rotation\nfor outlier elimination and proposes rotation-aware fine-tuning to preserve the\noutlier-free characteristics in rotated LLMs. Experimental results show RoLoRA\nconsistently improves low-bit LoRA convergence and post-training quantization\nrobustness in weight-activation settings. We evaluate RoLoRA across\nLLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks\ncompared to LoRA baseline. We further demonstrate its effectiveness on Large\nMultimodal Models (LLaVA-1.5-7B). Codes are available at\nhttps://github.com/HuangOwen/RoLoRA",
            "Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets."
        ],
        "neg": []
    },
    {
        "query": "In recent years, with the rapid advancements in large language models (LLMs),\nachieving excellent empathetic response capabilities has become a crucial\nprerequisite. Consequently, managing and understanding empathetic datasets have\ngained increasing significance. However, empathetic data are typically\nhuman-labeled, leading to insufficient datasets and wasted human labor. In this\nwork, we present Synth-Empathy, an LLM-based data generation and quality and\ndiversity selection pipeline that automatically generates high-quality\nempathetic data while discarding low-quality data. With the data generated from\na low empathetic model, we are able to further improve empathetic response\nperformance and achieve state-of-the-art (SoTA) results across multiple\nbenchmarks. Moreover, our model achieves SoTA performance on various human\nevaluation benchmarks, demonstrating its effectiveness and robustness in\nreal-world applications. Furthermore, we show the trade-off between data\nquantity and quality, providing insights into empathetic data generation and\nselection.",
        "pos": [
            "Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).",
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.",
            "Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets."
        ],
        "neg": []
    },
    {
        "query": "Authorship obfuscation aims to disguise the identity of an author within a\ntext by altering the writing style, vocabulary, syntax, and other linguistic\nfeatures associated with the text author. This alteration needs to balance\nprivacy and utility. While strong obfuscation techniques can effectively hide\nthe author's identity, they often degrade the quality and usefulness of the\ntext for its intended purpose. Conversely, maintaining high utility tends to\nprovide insufficient privacy, making it easier for an adversary to de-anonymize\nthe author. Thus, achieving an optimal trade-off between these two conflicting\nobjectives is crucial. In this paper, we propose TAROT: Task-Oriented\nAuthorship Obfuscation Using Policy Optimization, a new unsupervised authorship\nobfuscation method whose goal is to optimize the privacy-utility trade-off by\nregenerating the entire text considering its downstream utility. Our approach\nleverages policy optimization as a fine-tuning paradigm over small language\nmodels in order to rewrite texts by preserving author identity and downstream\ntask utility. We show that our approach largely reduce the accuracy of\nattackers while preserving utility. We make our code and models publicly\navailable.",
        "pos": [
            "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research.",
            "Large language models (LLMs) can suggest missing elements from items listed\nin a prompt, which can be used for list completion or recommendations based on\nusers' history. However, their performance degrades when presented with too\nmany items, as they start to suggest items already included in the input list.\nThis occurs at around 100 items for mid-2024 flagship LLMs. We evaluate this\nphenomenon on both synthetic problems (e.g., finding missing numbers in a given\nrange of shuffled integers) and realistic movie recommendation scenarios. We\nrefer to this issue as \\textit{attention overflow}, as preventing repetition\nrequires attending to all items simultaneously. Although iterative loops can\nmitigate this problem, their costs increase with the repetition rate, affecting\nthe language models' ability to derive novelty from lengthy inputs."
        ],
        "neg": []
    },
    {
        "query": "Symbolic Music, akin to language, can be encoded in discrete symbols. Recent\nresearch has extended the application of large language models (LLMs) such as\nGPT-4 and Llama2 to the symbolic music domain including understanding and\ngeneration. Yet scant research explores the details of how these LLMs perform\non advanced music understanding and conditioned generation, especially from the\nmulti-step reasoning perspective, which is a critical aspect in the\nconditioned, editable, and interactive human-computer co-creation process. This\nstudy conducts a thorough investigation of LLMs' capability and limitations in\nsymbolic music processing. We identify that current LLMs exhibit poor\nperformance in song-level multi-step music reasoning, and typically fail to\nleverage learned music knowledge when addressing complex musical tasks. An\nanalysis of LLMs' responses highlights distinctly their pros and cons. Our\nfindings suggest achieving advanced musical capability is not intrinsically\nobtained by LLMs, and future research should focus more on bridging the gap\nbetween music knowledge and reasoning, to improve the co-creation experience\nfor musicians.",
        "pos": [
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer."
        ],
        "neg": []
    },
    {
        "query": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.",
        "pos": [
            "Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models."
        ],
        "neg": []
    },
    {
        "query": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.",
        "pos": [
            "Deep learning provides powerful methods to impute structured information from\nlarge-scale, unstructured text and image datasets. For example, economists\nmight wish to detect the presence of economic activity in satellite images, or\nto measure the topics or entities mentioned in social media, the congressional\nrecord, or firm filings. This review introduces deep neural networks, covering\nmethods such as classifiers, regression models, generative AI, and embedding\nmodels. Applications include classification, document digitization, record\nlinkage, and methods for data exploration in massive scale text and image\ncorpora. When suitable methods are used, deep learning models can be cheap to\ntune and can scale affordably to problems involving millions or billions of\ndata points.. The review is accompanied by a companion website, EconDL, with\nuser-friendly demo notebooks, software resources, and a knowledge base that\nprovides technical details and additional applications."
        ],
        "neg": []
    },
    {
        "query": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.",
        "pos": [
            "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference."
        ],
        "neg": []
    },
    {
        "query": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.",
        "pos": [
            "The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we open-source SAFETY-J's training protocols,\ndatasets, and code at https://github.com/GAIR-NLP/Safety-J.",
            "When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervisions for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.",
            "Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .",
            "Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.",
            "Self-improvement through post-training methods such as iterative preference\nlearning has been acclaimed for enhancing the problem-solving capabilities\n(e.g., mathematical reasoning) of Large Language Models (LLMs) without human\nintervention. However, as exploration deepens, it becomes crucial to assess\nwhether these improvements genuinely signify progress in solving more\nchallenging problems or if they could lead to unintended regressions. To\naddress this, we propose a comprehensive evaluative framework that goes beyond\nthe superficial pass@1 metric to scrutinize the underlying enhancements of\npost-training paradigms for self-improvement. Through rigorous experimentation\nand analysis across diverse problem-solving tasks, the empirical results point\nout the phenomenon of \\emph{self-improvement reversal}, where models showing\nimproved performance across benchmarks will paradoxically exhibit declines in\nbroader, essential capabilities, like output diversity and out-of-distribution\n(OOD) generalization. These findings indicate that current self-improvement\npractices through post-training are inadequate for equipping models to tackle\nmore complex problems. Furthermore, they underscore the necessity of our\ncritical evaluation metrics in discerning the \\emph{progress or regress}\ndichotomy for self-improving LLMs."
        ],
        "neg": []
    },
    {
        "query": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.",
        "pos": [
            "Thousands of new scientific papers are published each month. Such information\noverload complicates researcher efforts to stay current with the\nstate-of-the-art as well as to verify and correctly attribute claims. We pose\nthe following research question: Given a text excerpt referencing a paper,\ncould an LM act as a research assistant to correctly identify the referenced\npaper? We advance efforts to answer this question by building a benchmark that\nevaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,\nconsists of text excerpts from recent machine learning papers, each referencing\na single other paper. CiteME use reveals a large gap between frontier LMs and\nhuman performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.\nWe close this gap by introducing CiteAgent, an autonomous system built on the\nGPT-4o LM that can also search and read papers, which achieves an accuracy of\n35.3\\% on CiteME. Overall, CiteME serves as a challenging testbed for\nopen-ended claim attribution, driving the research community towards a future\nwhere any claim made by an LM can be automatically verified and discarded if\nfound to be incorrect."
        ],
        "neg": []
    },
    {
        "query": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.",
        "pos": [
            "Thousands of new scientific papers are published each month. Such information\noverload complicates researcher efforts to stay current with the\nstate-of-the-art as well as to verify and correctly attribute claims. We pose\nthe following research question: Given a text excerpt referencing a paper,\ncould an LM act as a research assistant to correctly identify the referenced\npaper? We advance efforts to answer this question by building a benchmark that\nevaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,\nconsists of text excerpts from recent machine learning papers, each referencing\na single other paper. CiteME use reveals a large gap between frontier LMs and\nhuman performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.\nWe close this gap by introducing CiteAgent, an autonomous system built on the\nGPT-4o LM that can also search and read papers, which achieves an accuracy of\n35.3\\% on CiteME. Overall, CiteME serves as a challenging testbed for\nopen-ended claim attribution, driving the research community towards a future\nwhere any claim made by an LM can be automatically verified and discarded if\nfound to be incorrect."
        ],
        "neg": []
    },
    {
        "query": "Conversational Speech Synthesis (CSS) aims to express a target utterance with\nthe proper speaking style in a user-agent conversation setting. Existing CSS\nmethods employ effective multi-modal context modeling techniques to achieve\nempathy understanding and expression. However, they often need to design\ncomplex network architectures and meticulously optimize the modules within\nthem. In addition, due to the limitations of small-scale datasets containing\nscripted recording styles, they often fail to simulate real natural\nconversational styles. To address the above issues, we propose a novel\ngenerative expressive CSS system, termed GPT-Talker.We transform the multimodal\ninformation of the multi-turn dialogue history into discrete token sequences\nand seamlessly integrate them to form a comprehensive user-agent dialogue\ncontext. Leveraging the power of GPT, we predict the token sequence, that\nincludes both semantic and style knowledge, of response for the agent. After\nthat, the expressive conversational speech is synthesized by the\nconversation-enriched VITS to deliver feedback to the user.Furthermore, we\npropose a large-scale Natural CSS Dataset called NCSSD, that includes both\nnaturally recorded conversational speech in improvised styles and dialogues\nextracted from TV shows. It encompasses both Chinese and English languages,\nwith a total duration of 236 hours.We conducted comprehensive experiments on\nthe reliability of the NCSSD and the effectiveness of our GPT-Talker. Both\nsubjective and objective evaluations demonstrate that our model outperforms\nother state-of-the-art CSS systems significantly in terms of naturalness and\nexpressiveness. The Code, Dataset, and Pre-trained Model are available at:\nhttps://github.com/AI-S2-Lab/GPT-Talker.",
        "pos": [
            "Emotion and Intent Joint Understanding in Multimodal Conversation (MC-EIU)\naims to decode the semantic information manifested in a multimodal\nconversational history, while inferring the emotions and intents simultaneously\nfor the current utterance. MC-EIU is enabling technology for many\nhuman-computer interfaces. However, there is a lack of available datasets in\nterms of annotation, modality, language diversity, and accessibility. In this\nwork, we propose an MC-EIU dataset, which features 7 emotion categories, 9\nintent categories, 3 modalities, i.e., textual, acoustic, and visual content,\nand two languages, i.e., English and Mandarin. Furthermore, it is completely\nopen-source for free access. To our knowledge, MC-EIU is the first\ncomprehensive and rich emotion and intent joint understanding dataset for\nmultimodal conversation. Together with the release of the dataset, we also\ndevelop an Emotion and Intent Interaction (EI$^2$) network as a reference\nsystem by modeling the deep correlation between emotion and intent in the\nmultimodal conversation. With comparative experiments and ablation studies, we\ndemonstrate the effectiveness of the proposed EI$^2$ method on the MC-EIU\ndataset. The dataset and codes will be made available at:\nhttps://github.com/MC-EIU/MC-EIU."
        ],
        "neg": []
    },
    {
        "query": "Conversational Speech Synthesis (CSS) aims to express a target utterance with\nthe proper speaking style in a user-agent conversation setting. Existing CSS\nmethods employ effective multi-modal context modeling techniques to achieve\nempathy understanding and expression. However, they often need to design\ncomplex network architectures and meticulously optimize the modules within\nthem. In addition, due to the limitations of small-scale datasets containing\nscripted recording styles, they often fail to simulate real natural\nconversational styles. To address the above issues, we propose a novel\ngenerative expressive CSS system, termed GPT-Talker.We transform the multimodal\ninformation of the multi-turn dialogue history into discrete token sequences\nand seamlessly integrate them to form a comprehensive user-agent dialogue\ncontext. Leveraging the power of GPT, we predict the token sequence, that\nincludes both semantic and style knowledge, of response for the agent. After\nthat, the expressive conversational speech is synthesized by the\nconversation-enriched VITS to deliver feedback to the user.Furthermore, we\npropose a large-scale Natural CSS Dataset called NCSSD, that includes both\nnaturally recorded conversational speech in improvised styles and dialogues\nextracted from TV shows. It encompasses both Chinese and English languages,\nwith a total duration of 236 hours.We conducted comprehensive experiments on\nthe reliability of the NCSSD and the effectiveness of our GPT-Talker. Both\nsubjective and objective evaluations demonstrate that our model outperforms\nother state-of-the-art CSS systems significantly in terms of naturalness and\nexpressiveness. The Code, Dataset, and Pre-trained Model are available at:\nhttps://github.com/AI-S2-Lab/GPT-Talker.",
        "pos": [
            "Learning dynamics, which describes how the learning of specific training\nexamples influences the model's prediction of other examples, give us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during finetuning, by analyzing\nthe step-wise decomposition and accumulated influence among different\nresponses. Our framework allows a uniform interpretation of many interesting\nobservations about the training of popular algorithms for both instruction\ntuning and preference tuning. The analysis not only explains where the benefits\nof these methods come from but also inspires a simple, effective method to\nfurther improve the alignment performance. Code for experiments is available at\nhttps://github.com/Joshua-Ren/Learning_dynamics_LLM."
        ],
        "neg": []
    },
    {
        "query": "Conversational Speech Synthesis (CSS) aims to express a target utterance with\nthe proper speaking style in a user-agent conversation setting. Existing CSS\nmethods employ effective multi-modal context modeling techniques to achieve\nempathy understanding and expression. However, they often need to design\ncomplex network architectures and meticulously optimize the modules within\nthem. In addition, due to the limitations of small-scale datasets containing\nscripted recording styles, they often fail to simulate real natural\nconversational styles. To address the above issues, we propose a novel\ngenerative expressive CSS system, termed GPT-Talker.We transform the multimodal\ninformation of the multi-turn dialogue history into discrete token sequences\nand seamlessly integrate them to form a comprehensive user-agent dialogue\ncontext. Leveraging the power of GPT, we predict the token sequence, that\nincludes both semantic and style knowledge, of response for the agent. After\nthat, the expressive conversational speech is synthesized by the\nconversation-enriched VITS to deliver feedback to the user.Furthermore, we\npropose a large-scale Natural CSS Dataset called NCSSD, that includes both\nnaturally recorded conversational speech in improvised styles and dialogues\nextracted from TV shows. It encompasses both Chinese and English languages,\nwith a total duration of 236 hours.We conducted comprehensive experiments on\nthe reliability of the NCSSD and the effectiveness of our GPT-Talker. Both\nsubjective and objective evaluations demonstrate that our model outperforms\nother state-of-the-art CSS systems significantly in terms of naturalness and\nexpressiveness. The Code, Dataset, and Pre-trained Model are available at:\nhttps://github.com/AI-S2-Lab/GPT-Talker.",
        "pos": [
            "Emotion and Intent Joint Understanding in Multimodal Conversation (MC-EIU)\naims to decode the semantic information manifested in a multimodal\nconversational history, while inferring the emotions and intents simultaneously\nfor the current utterance. MC-EIU is enabling technology for many\nhuman-computer interfaces. However, there is a lack of available datasets in\nterms of annotation, modality, language diversity, and accessibility. In this\nwork, we propose an MC-EIU dataset, which features 7 emotion categories, 9\nintent categories, 3 modalities, i.e., textual, acoustic, and visual content,\nand two languages, i.e., English and Mandarin. Furthermore, it is completely\nopen-source for free access. To our knowledge, MC-EIU is the first\ncomprehensive and rich emotion and intent joint understanding dataset for\nmultimodal conversation. Together with the release of the dataset, we also\ndevelop an Emotion and Intent Interaction (EI$^2$) network as a reference\nsystem by modeling the deep correlation between emotion and intent in the\nmultimodal conversation. With comparative experiments and ablation studies, we\ndemonstrate the effectiveness of the proposed EI$^2$ method on the MC-EIU\ndataset. The dataset and codes will be made available at:\nhttps://github.com/MC-EIU/MC-EIU."
        ],
        "neg": []
    },
    {
        "query": "The rapid development of neural text-to-speech (TTS) systems enabled its\nusage in other areas of natural language processing such as automatic speech\nrecognition (ASR) or spoken language translation (SLT). Due to the large number\nof different TTS architectures and their extensions, selecting which TTS\nsystems to use for synthetic data creation is not an easy task. We use the\ncomparison of five different TTS decoder architectures in the scope of\nsynthetic data generation to show the impact on CTC-based speech recognition\ntraining. We compare the recognition results to computable metrics like NISQA\nMOS and intelligibility, finding that there are no clear relations to the ASR\nperformance. We also observe that for data generation auto-regressive decoding\nperforms better than non-autoregressive decoding, and propose an approach to\nquantify TTS generalization capabilities.",
        "pos": [
            "In this work we evaluate the utility of synthetic data for training automatic\nspeech recognition (ASR). We use the ASR training data to train a\ntext-to-speech (TTS) system similar to FastSpeech-2. With this TTS we reproduce\nthe original training data, training ASR systems solely on synthetic data. For\nASR, we use three different architectures, attention-based encoder-decoder,\nhybrid deep neural network hidden Markov model and a Gaussian mixture hidden\nMarkov model, showing the different sensitivity of the models to synthetic data\ngeneration. In order to extend previous work, we present a number of ablation\nstudies on the effectiveness of synthetic vs. real training data for ASR. In\nparticular we focus on how the gap between training on synthetic and real data\nchanges by varying the speaker embedding or by scaling the model size. For the\nlatter we show that the TTS models generalize well, even when training scores\nindicate overfitting."
        ],
        "neg": []
    },
    {
        "query": "The rapid development of neural text-to-speech (TTS) systems enabled its\nusage in other areas of natural language processing such as automatic speech\nrecognition (ASR) or spoken language translation (SLT). Due to the large number\nof different TTS architectures and their extensions, selecting which TTS\nsystems to use for synthetic data creation is not an easy task. We use the\ncomparison of five different TTS decoder architectures in the scope of\nsynthetic data generation to show the impact on CTC-based speech recognition\ntraining. We compare the recognition results to computable metrics like NISQA\nMOS and intelligibility, finding that there are no clear relations to the ASR\nperformance. We also observe that for data generation auto-regressive decoding\nperforms better than non-autoregressive decoding, and propose an approach to\nquantify TTS generalization capabilities.",
        "pos": [
            "In this work we evaluate the utility of synthetic data for training automatic\nspeech recognition (ASR). We use the ASR training data to train a\ntext-to-speech (TTS) system similar to FastSpeech-2. With this TTS we reproduce\nthe original training data, training ASR systems solely on synthetic data. For\nASR, we use three different architectures, attention-based encoder-decoder,\nhybrid deep neural network hidden Markov model and a Gaussian mixture hidden\nMarkov model, showing the different sensitivity of the models to synthetic data\ngeneration. In order to extend previous work, we present a number of ablation\nstudies on the effectiveness of synthetic vs. real training data for ASR. In\nparticular we focus on how the gap between training on synthetic and real data\nchanges by varying the speaker embedding or by scaling the model size. For the\nlatter we show that the TTS models generalize well, even when training scores\nindicate overfitting."
        ],
        "neg": []
    },
    {
        "query": "The rapid development of neural text-to-speech (TTS) systems enabled its\nusage in other areas of natural language processing such as automatic speech\nrecognition (ASR) or spoken language translation (SLT). Due to the large number\nof different TTS architectures and their extensions, selecting which TTS\nsystems to use for synthetic data creation is not an easy task. We use the\ncomparison of five different TTS decoder architectures in the scope of\nsynthetic data generation to show the impact on CTC-based speech recognition\ntraining. We compare the recognition results to computable metrics like NISQA\nMOS and intelligibility, finding that there are no clear relations to the ASR\nperformance. We also observe that for data generation auto-regressive decoding\nperforms better than non-autoregressive decoding, and propose an approach to\nquantify TTS generalization capabilities.",
        "pos": [
            "Recent advances in simultaneous speech translation (SST) focus on the\ndecision policies that enable the use of offline-trained ST models for\nsimultaneous inference. These decision policies not only control the\nquality-latency trade-off in SST but also mitigate the impact of unstable\npredictions on translation quality by delaying translation for more context or\ndiscarding these predictions through stable hypothesis detection. However,\nthese policies often overlook the potential benefits of utilizing unstable\npredictions. We introduce the contrastive feedback mechanism (CFM) for SST, a\nnovel method that leverages these unstable predictions as feedback to improve\ntranslation quality. CFM guides the system to eliminate undesired model\nbehaviors from these predictions through a contrastive objective. The\nexperiments on 3 state-of-the-art decision policies across 8 languages in the\nMuST-C v1.0 dataset show that CFM effectively improves the performance of SST."
        ],
        "neg": []
    },
    {
        "query": "Real-world navigation often involves dealing with unexpected obstructions\nsuch as closed doors, moved objects, and unpredictable entities. However,\nmainstream Vision-and-Language Navigation (VLN) tasks typically assume\ninstructions perfectly align with the fixed and predefined navigation graphs\nwithout any obstructions. This assumption overlooks potential discrepancies in\nactual navigation graphs and given instructions, which can cause major failures\nfor both indoor and outdoor agents. To address this issue, we integrate diverse\nobstructions into the R2R dataset by modifying both the navigation graphs and\nvisual observations, introducing an innovative dataset and task, R2R with\nUNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers\nof path obstructions to generate instruction-reality mismatches for VLN\nresearch. Experiments on R2R-UNO reveal that state-of-the-art VLN methods\ninevitably encounter significant challenges when facing such mismatches,\nindicating that they rigidly follow instructions rather than navigate\nadaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),\nwhich includes a curriculum training strategy and virtual graph construction to\nhelp agents effectively adapt to obstructed environments. Empirical results\nshow that ObVLN not only maintains robust performance in unobstructed scenarios\nbut also achieves a substantial performance advantage with unexpected\nobstructions.",
        "pos": [
            "Capitalizing on the remarkable advancements in Large Language Models (LLMs),\nthere is a burgeoning initiative to harness LLMs for instruction following\nrobotic navigation. Such a trend underscores the potential of LLMs to\ngeneralize navigational reasoning and diverse language understanding. However,\na significant discrepancy in agent performance is observed when integrating\nLLMs in the Vision-and-Language navigation (VLN) tasks compared to previous\ndownstream specialist models. Furthermore, the inherent capacity of language to\ninterpret and facilitate communication in agent interactions is often\nunderutilized in these integrations. In this work, we strive to bridge the\ndivide between VLN-specialized models and LLM-based navigation paradigms, while\nmaintaining the interpretative prowess of LLMs in generating linguistic\nnavigational reasoning. By aligning visual content in a frozen LLM, we\nencompass visual observation comprehension for LLMs and exploit a way to\nincorporate LLMs and navigation policy networks for effective action\npredictions and navigational reasoning. We demonstrate the data efficiency of\nthe proposed methods and eliminate the gap between LM-based agents and\nstate-of-the-art VLN specialists."
        ],
        "neg": []
    },
    {
        "query": "Real-world navigation often involves dealing with unexpected obstructions\nsuch as closed doors, moved objects, and unpredictable entities. However,\nmainstream Vision-and-Language Navigation (VLN) tasks typically assume\ninstructions perfectly align with the fixed and predefined navigation graphs\nwithout any obstructions. This assumption overlooks potential discrepancies in\nactual navigation graphs and given instructions, which can cause major failures\nfor both indoor and outdoor agents. To address this issue, we integrate diverse\nobstructions into the R2R dataset by modifying both the navigation graphs and\nvisual observations, introducing an innovative dataset and task, R2R with\nUNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers\nof path obstructions to generate instruction-reality mismatches for VLN\nresearch. Experiments on R2R-UNO reveal that state-of-the-art VLN methods\ninevitably encounter significant challenges when facing such mismatches,\nindicating that they rigidly follow instructions rather than navigate\nadaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),\nwhich includes a curriculum training strategy and virtual graph construction to\nhelp agents effectively adapt to obstructed environments. Empirical results\nshow that ObVLN not only maintains robust performance in unobstructed scenarios\nbut also achieves a substantial performance advantage with unexpected\nobstructions.",
        "pos": [
            "Recently, various pre-trained language models (PLMs) have been proposed to\nprove their impressive performances on a wide range of few-shot tasks. However,\nlimited by the unstructured prior knowledge in PLMs, it is difficult to\nmaintain consistent performance on complex structured scenarios, such as\nhierarchical text classification (HTC), especially when the downstream data is\nextremely scarce. The main challenge is how to transfer the unstructured\nsemantic space in PLMs to the downstream domain hierarchy. Unlike previous work\non HTC which directly performs multi-label classification or uses graph neural\nnetwork (GNN) to inject label hierarchy, in this work, we study the HTC problem\nunder a few-shot setting to adapt knowledge in PLMs from an unstructured manner\nto the downstream hierarchy. Technically, we design a simple yet effective\nmethod named Hierarchical Iterative Conditional Random Field (HierICRF) to\nsearch the most domain-challenging directions and exquisitely crafts\ndomain-hierarchy adaptation as a hierarchical iterative language modeling\nproblem, and then it encourages the model to make hierarchical consistency\nself-correction during the inference, thereby achieving knowledge transfer with\nhierarchical consistency preservation. We perform HierICRF on various\narchitectures, and extensive experiments on two popular HTC datasets\ndemonstrate that prompt with HierICRF significantly boosts the few-shot HTC\nperformance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%\nto 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot\nsettings, while remaining SOTA hierarchical consistency performance."
        ],
        "neg": []
    },
    {
        "query": "Modern language models (LMs) need to follow human instructions while being\nfaithful; yet, they often fail to achieve both. Here, we provide concrete\nevidence of a trade-off between instruction following (i.e., follow open-ended\ninstructions) and faithfulness (i.e., ground responses in given context) when\ntraining LMs with these objectives. For instance, fine-tuning LLaMA-7B on\ninstruction following datasets renders it less faithful. Conversely,\ninstruction-tuned Vicuna-7B shows degraded performance at following\ninstructions when further optimized on tasks that require contextual grounding.\nOne common remedy is multi-task learning (MTL) with data mixing, yet it remains\nfar from achieving a synergic outcome. We propose a simple yet effective method\nthat relies on Rejection Sampling for Continued Self-instruction Tuning\n(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find\nthat less is more, as training ReSet with high-quality, yet substantially\nsmaller data (three-fold less) yields superior results. Our findings offer a\nbetter understanding of objective discrepancies in alignment training of LMs.",
        "pos": [
            "Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research."
        ],
        "neg": []
    },
    {
        "query": "Modern language models (LMs) need to follow human instructions while being\nfaithful; yet, they often fail to achieve both. Here, we provide concrete\nevidence of a trade-off between instruction following (i.e., follow open-ended\ninstructions) and faithfulness (i.e., ground responses in given context) when\ntraining LMs with these objectives. For instance, fine-tuning LLaMA-7B on\ninstruction following datasets renders it less faithful. Conversely,\ninstruction-tuned Vicuna-7B shows degraded performance at following\ninstructions when further optimized on tasks that require contextual grounding.\nOne common remedy is multi-task learning (MTL) with data mixing, yet it remains\nfar from achieving a synergic outcome. We propose a simple yet effective method\nthat relies on Rejection Sampling for Continued Self-instruction Tuning\n(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find\nthat less is more, as training ReSet with high-quality, yet substantially\nsmaller data (three-fold less) yields superior results. Our findings offer a\nbetter understanding of objective discrepancies in alignment training of LMs.",
        "pos": [
            "Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research."
        ],
        "neg": []
    },
    {
        "query": "Modern language models (LMs) need to follow human instructions while being\nfaithful; yet, they often fail to achieve both. Here, we provide concrete\nevidence of a trade-off between instruction following (i.e., follow open-ended\ninstructions) and faithfulness (i.e., ground responses in given context) when\ntraining LMs with these objectives. For instance, fine-tuning LLaMA-7B on\ninstruction following datasets renders it less faithful. Conversely,\ninstruction-tuned Vicuna-7B shows degraded performance at following\ninstructions when further optimized on tasks that require contextual grounding.\nOne common remedy is multi-task learning (MTL) with data mixing, yet it remains\nfar from achieving a synergic outcome. We propose a simple yet effective method\nthat relies on Rejection Sampling for Continued Self-instruction Tuning\n(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find\nthat less is more, as training ReSet with high-quality, yet substantially\nsmaller data (three-fold less) yields superior results. Our findings offer a\nbetter understanding of objective discrepancies in alignment training of LMs.",
        "pos": [
            "Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research."
        ],
        "neg": []
    },
    {
        "query": "Modern language models (LMs) need to follow human instructions while being\nfaithful; yet, they often fail to achieve both. Here, we provide concrete\nevidence of a trade-off between instruction following (i.e., follow open-ended\ninstructions) and faithfulness (i.e., ground responses in given context) when\ntraining LMs with these objectives. For instance, fine-tuning LLaMA-7B on\ninstruction following datasets renders it less faithful. Conversely,\ninstruction-tuned Vicuna-7B shows degraded performance at following\ninstructions when further optimized on tasks that require contextual grounding.\nOne common remedy is multi-task learning (MTL) with data mixing, yet it remains\nfar from achieving a synergic outcome. We propose a simple yet effective method\nthat relies on Rejection Sampling for Continued Self-instruction Tuning\n(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find\nthat less is more, as training ReSet with high-quality, yet substantially\nsmaller data (three-fold less) yields superior results. Our findings offer a\nbetter understanding of objective discrepancies in alignment training of LMs.",
        "pos": [
            "Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research."
        ],
        "neg": []
    },
    {
        "query": "Modern language models (LMs) need to follow human instructions while being\nfaithful; yet, they often fail to achieve both. Here, we provide concrete\nevidence of a trade-off between instruction following (i.e., follow open-ended\ninstructions) and faithfulness (i.e., ground responses in given context) when\ntraining LMs with these objectives. For instance, fine-tuning LLaMA-7B on\ninstruction following datasets renders it less faithful. Conversely,\ninstruction-tuned Vicuna-7B shows degraded performance at following\ninstructions when further optimized on tasks that require contextual grounding.\nOne common remedy is multi-task learning (MTL) with data mixing, yet it remains\nfar from achieving a synergic outcome. We propose a simple yet effective method\nthat relies on Rejection Sampling for Continued Self-instruction Tuning\n(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find\nthat less is more, as training ReSet with high-quality, yet substantially\nsmaller data (three-fold less) yields superior results. Our findings offer a\nbetter understanding of objective discrepancies in alignment training of LMs.",
        "pos": [
            "Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research."
        ],
        "neg": []
    },
    {
        "query": "Model attribution for LLM-generated disinformation poses a significant\nchallenge in understanding its origins and mitigating its spread. This task is\nespecially challenging because modern large language models (LLMs) produce\ndisinformation with human-like quality. Additionally, the diversity in\nprompting methods used to generate disinformation complicates accurate source\nattribution. These methods introduce domain-specific features that can mask the\nfundamental characteristics of the models. In this paper, we introduce the\nconcept of model attribution as a domain generalization problem, where each\nprompting method represents a unique domain. We argue that an effective\nattribution model must be invariant to these domain-specific features. It\nshould also be proficient in identifying the originating models across all\nscenarios, reflecting real-world detection challenges. To address this, we\nintroduce a novel approach based on Supervised Contrastive Learning. This\nmethod is designed to enhance the model's robustness to variations in prompts\nand focuses on distinguishing between different source LLMs. We evaluate our\nmodel through rigorous experiments involving three common prompting methods:\n``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:\n``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the\neffectiveness of our approach in model attribution tasks, achieving\nstate-of-the-art performance across diverse and unseen datasets.",
        "pos": [
            "In this paper, we introduce Dynamic Layer Operations (DLO), a novel approach\nfor vertically scaling transformer-based Large Language Models (LLMs) by\ndynamically expanding, activating, or skipping layers using a sophisticated\nrouting policy based on layerwise feature similarity. Unlike traditional\nMixture-of-Experts (MoE) methods that focus on extending the model width, our\napproach targets model depth, addressing the redundancy observed across layer\nrepresentations for various input samples. Our framework is integrated with the\nSupervised Fine-Tuning (SFT) stage, eliminating the need for resource-intensive\nContinual Pre-Training (CPT). Experimental results demonstrate that DLO not\nonly outperforms the original unscaled models but also achieves comparable\nresults to densely expanded models with significantly improved efficiency. Our\nwork offers a promising direction for building efficient yet powerful LLMs. We\nwill release our implementation and model weights upon acceptance.",
            "As Large Language Models (LLMs) are increasingly deployed to handle various\nnatural language processing (NLP) tasks, concerns regarding the potential\nnegative societal impacts of LLM-generated content have also arisen. To\nevaluate the biases exhibited by LLMs, researchers have recently proposed a\nvariety of datasets. However, existing bias evaluation efforts often focus on\nonly a particular type of bias and employ inconsistent evaluation metrics,\nleading to difficulties in comparison across different datasets and LLMs. To\naddress these limitations, we collect a variety of datasets designed for the\nbias evaluation of LLMs, and further propose CEB, a Compositional Evaluation\nBenchmark that covers different types of bias across different social groups\nand tasks. The curation of CEB is based on our newly proposed compositional\ntaxonomy, which characterizes each dataset from three dimensions: bias types,\nsocial groups, and tasks. By combining the three dimensions, we develop a\ncomprehensive evaluation strategy for the bias in LLMs. Our experiments\ndemonstrate that the levels of bias vary across these dimensions, thereby\nproviding guidance for the development of specific bias mitigation methods."
        ],
        "neg": []
    },
    {
        "query": "Model attribution for LLM-generated disinformation poses a significant\nchallenge in understanding its origins and mitigating its spread. This task is\nespecially challenging because modern large language models (LLMs) produce\ndisinformation with human-like quality. Additionally, the diversity in\nprompting methods used to generate disinformation complicates accurate source\nattribution. These methods introduce domain-specific features that can mask the\nfundamental characteristics of the models. In this paper, we introduce the\nconcept of model attribution as a domain generalization problem, where each\nprompting method represents a unique domain. We argue that an effective\nattribution model must be invariant to these domain-specific features. It\nshould also be proficient in identifying the originating models across all\nscenarios, reflecting real-world detection challenges. To address this, we\nintroduce a novel approach based on Supervised Contrastive Learning. This\nmethod is designed to enhance the model's robustness to variations in prompts\nand focuses on distinguishing between different source LLMs. We evaluate our\nmodel through rigorous experiments involving three common prompting methods:\n``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:\n``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the\neffectiveness of our approach in model attribution tasks, achieving\nstate-of-the-art performance across diverse and unseen datasets.",
        "pos": [
            "Knowledge editing techniques have been increasingly adopted to efficiently\ncorrect the false or outdated knowledge in Large Language Models (LLMs), due to\nthe high cost of retraining from scratch. Meanwhile, one critical but\nunder-explored question is: can knowledge editing be used to inject harm into\nLLMs? In this paper, we propose to reformulate knowledge editing as a new type\nof safety threat for LLMs, namely Editing Attack, and conduct a systematic\ninvestigation with a newly constructed dataset EditAttack. Specifically, we\nfocus on two typical safety risks of Editing Attack including Misinformation\nInjection and Bias Injection. For the risk of misinformation injection, we\nfirst categorize it into commonsense misinformation injection and long-tail\nmisinformation injection. Then, we find that editing attacks can inject both\ntypes of misinformation into LLMs, and the effectiveness is particularly high\nfor commonsense misinformation injection. For the risk of bias injection, we\ndiscover that not only can biased sentences be injected into LLMs with high\neffectiveness, but also one single biased sentence injection can cause a bias\nincrease in general outputs of LLMs, which are even highly irrelevant to the\ninjected sentence, indicating a catastrophic impact on the overall fairness of\nLLMs. Then, we further illustrate the high stealthiness of editing attacks,\nmeasured by their impact on the general knowledge and reasoning capacities of\nLLMs, and show the hardness of defending editing attacks with empirical\nevidence. Our discoveries demonstrate the emerging misuse risks of knowledge\nediting techniques on compromising the safety alignment of LLMs.",
            "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench."
        ],
        "neg": []
    },
    {
        "query": "Model attribution for LLM-generated disinformation poses a significant\nchallenge in understanding its origins and mitigating its spread. This task is\nespecially challenging because modern large language models (LLMs) produce\ndisinformation with human-like quality. Additionally, the diversity in\nprompting methods used to generate disinformation complicates accurate source\nattribution. These methods introduce domain-specific features that can mask the\nfundamental characteristics of the models. In this paper, we introduce the\nconcept of model attribution as a domain generalization problem, where each\nprompting method represents a unique domain. We argue that an effective\nattribution model must be invariant to these domain-specific features. It\nshould also be proficient in identifying the originating models across all\nscenarios, reflecting real-world detection challenges. To address this, we\nintroduce a novel approach based on Supervised Contrastive Learning. This\nmethod is designed to enhance the model's robustness to variations in prompts\nand focuses on distinguishing between different source LLMs. We evaluate our\nmodel through rigorous experiments involving three common prompting methods:\n``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:\n``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the\neffectiveness of our approach in model attribution tasks, achieving\nstate-of-the-art performance across diverse and unseen datasets.",
        "pos": [
            "Knowledge editing techniques have been increasingly adopted to efficiently\ncorrect the false or outdated knowledge in Large Language Models (LLMs), due to\nthe high cost of retraining from scratch. Meanwhile, one critical but\nunder-explored question is: can knowledge editing be used to inject harm into\nLLMs? In this paper, we propose to reformulate knowledge editing as a new type\nof safety threat for LLMs, namely Editing Attack, and conduct a systematic\ninvestigation with a newly constructed dataset EditAttack. Specifically, we\nfocus on two typical safety risks of Editing Attack including Misinformation\nInjection and Bias Injection. For the risk of misinformation injection, we\nfirst categorize it into commonsense misinformation injection and long-tail\nmisinformation injection. Then, we find that editing attacks can inject both\ntypes of misinformation into LLMs, and the effectiveness is particularly high\nfor commonsense misinformation injection. For the risk of bias injection, we\ndiscover that not only can biased sentences be injected into LLMs with high\neffectiveness, but also one single biased sentence injection can cause a bias\nincrease in general outputs of LLMs, which are even highly irrelevant to the\ninjected sentence, indicating a catastrophic impact on the overall fairness of\nLLMs. Then, we further illustrate the high stealthiness of editing attacks,\nmeasured by their impact on the general knowledge and reasoning capacities of\nLLMs, and show the hardness of defending editing attacks with empirical\nevidence. Our discoveries demonstrate the emerging misuse risks of knowledge\nediting techniques on compromising the safety alignment of LLMs."
        ],
        "neg": []
    },
    {
        "query": "Visual Question Answering (VQA) has recently emerged as a potential research\ndomain, captivating the interest of many in the field of artificial\nintelligence and computer vision. Despite the prevalence of approaches in\nEnglish, there is a notable lack of systems specifically developed for certain\nlanguages, particularly Vietnamese. This study aims to bridge this gap by\nconducting comprehensive experiments on the Vietnamese Visual Question\nAnswering (ViVQA) dataset, demonstrating the effectiveness of our proposed\nmodel. In response to community interest, we have developed a model that\nenhances image representation capabilities, thereby improving overall\nperformance in the ViVQA system. Specifically, our model integrates the\nBootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2)\nand the convolutional neural network EfficientNet to extract and process both\nlocal and global features from images. This integration leverages the strengths\nof transformer-based architectures for capturing comprehensive contextual\ninformation and convolutional networks for detailed local features. By freezing\nthe parameters of these pre-trained models, we significantly reduce the\ncomputational cost and training time, while maintaining high performance. This\napproach significantly improves image representation and enhances the\nperformance of existing VQA systems. We then leverage a multi-modal fusion\nmodule based on a general-purpose multi-modal foundation model (BEiT-3) to fuse\nthe information between visual and textual features. Our experimental findings\ndemonstrate that our model surpasses competing baselines, achieving promising\nperformance. This is particularly evident in its accuracy of $71.04\\%$ on the\ntest set of the ViVQA dataset, marking a significant advancement in our\nresearch area. The code is available at https://github.com/nngocson2002/ViVQA.",
        "pos": [
            "Vision-language models have been extensively explored across a wide range of\ntasks, achieving satisfactory performance; however, their application in\nmedical imaging remains underexplored. In this work, we propose a unified\nframework - LiteGPT - for the medical imaging. We leverage multiple pre-trained\nvisual encoders to enrich information and enhance the performance of\nvision-language models. To the best of our knowledge, this is the first study\nto utilize vision-language models for the novel task of joint localization and\nclassification in medical images. Besides, we are pioneers in providing\nbaselines for disease localization in chest X-rays. Finally, we set new\nstate-of-the-art performance in the image classification task on the\nwell-benchmarked VinDr-CXR dataset. All code and models are publicly available\nonline: https://github.com/leduckhai/LiteGPT"
        ],
        "neg": []
    },
    {
        "query": "Event-argument extraction is a challenging task, particularly in Arabic due\nto sparse linguistic resources. To fill this gap, we introduce the \\hadath\ncorpus ($550$k tokens) as an extension of Wojood, enriched with event-argument\nannotations. We used three types of event arguments: $agent$, $location$, and\n$date$, which we annotated as relation types. Our inter-annotator agreement\nevaluation resulted in $82.23\\%$ $Kappa$ score and $87.2\\%$ $F_1$-score.\nAdditionally, we propose a novel method for event relation extraction using\nBERT, in which we treat the task as text entailment. This method achieves an\n$F_1$-score of $94.01\\%$. To further evaluate the generalization of our\nproposed method, we collected and annotated another out-of-domain corpus (about\n$80$k tokens) called \\testNLI and used it as a second test set, on which our\napproach achieved promising results ($83.59\\%$ $F_1$-score). Last but not\nleast, we propose an end-to-end system for event-arguments extraction. This\nsystem is implemented as part of SinaTools, and both corpora are publicly\navailable at {\\small \\url{https://sina.birzeit.edu/wojood}}",
        "pos": [
            "This paper presents an overview of the Arabic Natural Language Understanding\n(ArabicNLU 2024) shared task, focusing on two subtasks: Word Sense\nDisambiguation (WSD) and Location Mention Disambiguation (LMD). The task aimed\nto evaluate the ability of automated systems to resolve word ambiguity and\nidentify locations mentioned in Arabic text. We provided participants with\nnovel datasets, including a sense-annotated corpus for WSD, called SALMA with\napproximately 34k annotated tokens, and the IDRISI-DA dataset with 3,893\nannotations and 763 unique location mentions. These are challenging tasks. Out\nof the 38 registered teams, only three teams participated in the final\nevaluation phase, with the highest accuracy being 77.8% for WSD and the highest\nMRR@1 being 95.0% for LMD. The shared task not only facilitated the evaluation\nand comparison of different techniques, but also provided valuable insights and\nresources for the continued advancement of Arabic NLU technologies."
        ],
        "neg": []
    },
    {
        "query": "Event-argument extraction is a challenging task, particularly in Arabic due\nto sparse linguistic resources. To fill this gap, we introduce the \\hadath\ncorpus ($550$k tokens) as an extension of Wojood, enriched with event-argument\nannotations. We used three types of event arguments: $agent$, $location$, and\n$date$, which we annotated as relation types. Our inter-annotator agreement\nevaluation resulted in $82.23\\%$ $Kappa$ score and $87.2\\%$ $F_1$-score.\nAdditionally, we propose a novel method for event relation extraction using\nBERT, in which we treat the task as text entailment. This method achieves an\n$F_1$-score of $94.01\\%$. To further evaluate the generalization of our\nproposed method, we collected and annotated another out-of-domain corpus (about\n$80$k tokens) called \\testNLI and used it as a second test set, on which our\napproach achieved promising results ($83.59\\%$ $F_1$-score). Last but not\nleast, we propose an end-to-end system for event-arguments extraction. This\nsystem is implemented as part of SinaTools, and both corpora are publicly\navailable at {\\small \\url{https://sina.birzeit.edu/wojood}}",
        "pos": [
            "The proliferation of bias and propaganda on social media is an increasingly\nsignificant concern, leading to the development of techniques for automatic\ndetection. This article presents a multilingual corpus of 12, 000 Facebook\nposts fully annotated for bias and propaganda. The corpus was created as part\nof the FigNews 2024 Shared Task on News Media Narratives for framing the\nIsraeli War on Gaza. It covers various events during the War from October 7,\n2023 to January 31, 2024. The corpus comprises 12, 000 posts in five languages\n(Arabic, Hebrew, English, French, and Hindi), with 2, 400 posts for each\nlanguage. The annotation process involved 10 graduate students specializing in\nLaw. The Inter-Annotator Agreement (IAA) was used to evaluate the annotations\nof the corpus, with an average IAA of 80.8% for bias and 70.15% for propaganda\nannotations. Our team was ranked among the bestperforming teams in both Bias\nand Propaganda subtasks. The corpus is open-source and available at\nhttps://sina.birzeit.edu/fada"
        ],
        "neg": []
    },
    {
        "query": "Event-argument extraction is a challenging task, particularly in Arabic due\nto sparse linguistic resources. To fill this gap, we introduce the \\hadath\ncorpus ($550$k tokens) as an extension of Wojood, enriched with event-argument\nannotations. We used three types of event arguments: $agent$, $location$, and\n$date$, which we annotated as relation types. Our inter-annotator agreement\nevaluation resulted in $82.23\\%$ $Kappa$ score and $87.2\\%$ $F_1$-score.\nAdditionally, we propose a novel method for event relation extraction using\nBERT, in which we treat the task as text entailment. This method achieves an\n$F_1$-score of $94.01\\%$. To further evaluate the generalization of our\nproposed method, we collected and annotated another out-of-domain corpus (about\n$80$k tokens) called \\testNLI and used it as a second test set, on which our\napproach achieved promising results ($83.59\\%$ $F_1$-score). Last but not\nleast, we propose an end-to-end system for event-arguments extraction. This\nsystem is implemented as part of SinaTools, and both corpora are publicly\navailable at {\\small \\url{https://sina.birzeit.edu/wojood}}",
        "pos": [
            "This paper presents an overview of the Arabic Natural Language Understanding\n(ArabicNLU 2024) shared task, focusing on two subtasks: Word Sense\nDisambiguation (WSD) and Location Mention Disambiguation (LMD). The task aimed\nto evaluate the ability of automated systems to resolve word ambiguity and\nidentify locations mentioned in Arabic text. We provided participants with\nnovel datasets, including a sense-annotated corpus for WSD, called SALMA with\napproximately 34k annotated tokens, and the IDRISI-DA dataset with 3,893\nannotations and 763 unique location mentions. These are challenging tasks. Out\nof the 38 registered teams, only three teams participated in the final\nevaluation phase, with the highest accuracy being 77.8% for WSD and the highest\nMRR@1 being 95.0% for LMD. The shared task not only facilitated the evaluation\nand comparison of different techniques, but also provided valuable insights and\nresources for the continued advancement of Arabic NLU technologies.",
            "We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed.",
            "We present WojoodNER-2024, the second Arabic Named Entity Recognition (NER)\nShared Task. In WojoodNER-2024, we focus on fine-grained Arabic NER. We\nprovided participants with a new Arabic fine-grained NER dataset called\nwojoodfine, annotated with subtypes of entities. WojoodNER-2024 encompassed\nthree subtasks: (i) Closed-Track Flat Fine-Grained NER, (ii) Closed-Track\nNested Fine-Grained NER, and (iii) an Open-Track NER for the Israeli War on\nGaza. A total of 43 unique teams registered for this shared task. Five teams\nparticipated in the Flat Fine-Grained Subtask, among which two teams tackled\nthe Nested Fine-Grained Subtask and one team participated in the Open-Track NER\nSubtask. The winning teams achieved F-1 scores of 91% and 92% in the Flat\nFine-Grained and Nested Fine-Grained Subtasks, respectively. The sole team in\nthe Open-Track Subtask achieved an F-1 score of 73.7%.",
            "The proliferation of bias and propaganda on social media is an increasingly\nsignificant concern, leading to the development of techniques for automatic\ndetection. This article presents a multilingual corpus of 12, 000 Facebook\nposts fully annotated for bias and propaganda. The corpus was created as part\nof the FigNews 2024 Shared Task on News Media Narratives for framing the\nIsraeli War on Gaza. It covers various events during the War from October 7,\n2023 to January 31, 2024. The corpus comprises 12, 000 posts in five languages\n(Arabic, Hebrew, English, French, and Hindi), with 2, 400 posts for each\nlanguage. The annotation process involved 10 graduate students specializing in\nLaw. The Inter-Annotator Agreement (IAA) was used to evaluate the annotations\nof the corpus, with an average IAA of 80.8% for bias and 70.15% for propaganda\nannotations. Our team was ranked among the bestperforming teams in both Bias\nand Propaganda subtasks. The corpus is open-source and available at\nhttps://sina.birzeit.edu/fada"
        ],
        "neg": []
    },
    {
        "query": "Event-argument extraction is a challenging task, particularly in Arabic due\nto sparse linguistic resources. To fill this gap, we introduce the \\hadath\ncorpus ($550$k tokens) as an extension of Wojood, enriched with event-argument\nannotations. We used three types of event arguments: $agent$, $location$, and\n$date$, which we annotated as relation types. Our inter-annotator agreement\nevaluation resulted in $82.23\\%$ $Kappa$ score and $87.2\\%$ $F_1$-score.\nAdditionally, we propose a novel method for event relation extraction using\nBERT, in which we treat the task as text entailment. This method achieves an\n$F_1$-score of $94.01\\%$. To further evaluate the generalization of our\nproposed method, we collected and annotated another out-of-domain corpus (about\n$80$k tokens) called \\testNLI and used it as a second test set, on which our\napproach achieved promising results ($83.59\\%$ $F_1$-score). Last but not\nleast, we propose an end-to-end system for event-arguments extraction. This\nsystem is implemented as part of SinaTools, and both corpora are publicly\navailable at {\\small \\url{https://sina.birzeit.edu/wojood}}",
        "pos": [
            "This paper presents an overview of the Arabic Natural Language Understanding\n(ArabicNLU 2024) shared task, focusing on two subtasks: Word Sense\nDisambiguation (WSD) and Location Mention Disambiguation (LMD). The task aimed\nto evaluate the ability of automated systems to resolve word ambiguity and\nidentify locations mentioned in Arabic text. We provided participants with\nnovel datasets, including a sense-annotated corpus for WSD, called SALMA with\napproximately 34k annotated tokens, and the IDRISI-DA dataset with 3,893\nannotations and 763 unique location mentions. These are challenging tasks. Out\nof the 38 registered teams, only three teams participated in the final\nevaluation phase, with the highest accuracy being 77.8% for WSD and the highest\nMRR@1 being 95.0% for LMD. The shared task not only facilitated the evaluation\nand comparison of different techniques, but also provided valuable insights and\nresources for the continued advancement of Arabic NLU technologies.",
            "We present WojoodNER-2024, the second Arabic Named Entity Recognition (NER)\nShared Task. In WojoodNER-2024, we focus on fine-grained Arabic NER. We\nprovided participants with a new Arabic fine-grained NER dataset called\nwojoodfine, annotated with subtypes of entities. WojoodNER-2024 encompassed\nthree subtasks: (i) Closed-Track Flat Fine-Grained NER, (ii) Closed-Track\nNested Fine-Grained NER, and (iii) an Open-Track NER for the Israeli War on\nGaza. A total of 43 unique teams registered for this shared task. Five teams\nparticipated in the Flat Fine-Grained Subtask, among which two teams tackled\nthe Nested Fine-Grained Subtask and one team participated in the Open-Track NER\nSubtask. The winning teams achieved F-1 scores of 91% and 92% in the Flat\nFine-Grained and Nested Fine-Grained Subtasks, respectively. The sole team in\nthe Open-Track Subtask achieved an F-1 score of 73.7%."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
        "pos": [
            "Large Language Models (LLMs) showcase remarkable performance and robust\ndeductive capabilities, yet their expansive size complicates deployment and\nraises environmental concerns due to substantial resource consumption. The\nrecent development of a quantization technique known as Learnable\nSingular-value Increment (LSI) has addressed some of these quantization\nchallenges. Leveraging insights from LSI and our extensive research, we have\ndeveloped innovative methods that enhance the performance of quantized LLMs,\nparticularly in low-bit settings. Our methods consistently deliver\nstate-of-the-art results across various quantization scenarios and offer deep\ntheoretical insights into the quantization process, elucidating the potential\nof quantized models for widespread application."
        ],
        "neg": []
    },
    {
        "query": "Recent advances show that two-stream approaches have achieved outstanding\nperformance in hateful meme detection. However, hateful memes constantly evolve\nas new memes emerge by fusing progressive cultural ideas, making existing\nmethods obsolete or ineffective. In this work, we explore the potential of\nLarge Multimodal Models (LMMs) for hateful meme detection. To this end, we\npropose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE)\nPrompting, by integrating the evolution attribute and in-context information of\nmemes. Specifically, Evolver simulates the evolving and expressing process of\nmemes and reasons through LMMs in a step-by-step manner. First, an evolutionary\npair mining module retrieves the top-k most similar memes in the external\ncurated meme set with the input meme. Second, an evolutionary information\nextractor is designed to summarize the semantic regularities between the paired\nmemes for prompting. Finally, a contextual relevance amplifier enhances the\nin-context hatefulness information to boost the search for evolutionary\nprocesses. Extensive experiments on public FHM, MAMI, and HarM datasets show\nthat CoE prompting can be incorporated into existing LMMs to improve their\nperformance. More encouragingly, it can serve as an interpretive tool to\npromote the understanding of the evolution of social memes.",
        "pos": [
            "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe same result that the optimal vocabulary size depends on the available\ncompute budget and that larger models deserve larger vocabularies. However,\nmost LLMs use too small vocabulary sizes. For example, we predict that the\noptimal vocabulary size of Llama2-70B should have been at least 216K, 7 times\nlarger than its vocabulary of 32K. We validate our predictions empirically by\ntraining models with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly\nconsidering model parameters and vocabulary size for efficient scaling."
        ],
        "neg": []
    },
    {
        "query": "Recent advances show that two-stream approaches have achieved outstanding\nperformance in hateful meme detection. However, hateful memes constantly evolve\nas new memes emerge by fusing progressive cultural ideas, making existing\nmethods obsolete or ineffective. In this work, we explore the potential of\nLarge Multimodal Models (LMMs) for hateful meme detection. To this end, we\npropose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE)\nPrompting, by integrating the evolution attribute and in-context information of\nmemes. Specifically, Evolver simulates the evolving and expressing process of\nmemes and reasons through LMMs in a step-by-step manner. First, an evolutionary\npair mining module retrieves the top-k most similar memes in the external\ncurated meme set with the input meme. Second, an evolutionary information\nextractor is designed to summarize the semantic regularities between the paired\nmemes for prompting. Finally, a contextual relevance amplifier enhances the\nin-context hatefulness information to boost the search for evolutionary\nprocesses. Extensive experiments on public FHM, MAMI, and HarM datasets show\nthat CoE prompting can be incorporated into existing LMMs to improve their\nperformance. More encouragingly, it can serve as an interpretive tool to\npromote the understanding of the evolution of social memes.",
        "pos": [
            "This study seeks to identify and quantify biases in simulating political\nsamples with Large Language Models, specifically focusing on vote choice and\npublic opinion. Using the GPT-3.5-Turbo model, we leverage data from the\nAmerican National Election Studies, German Longitudinal Election Study, Zuobiao\nDataset, and China Family Panel Studies to simulate voting behaviors and public\nopinions. This methodology enables us to examine three types of representation\nbias: disparities based on the the country's language, demographic groups, and\npolitical regime types. The findings reveal that simulation performance is\ngenerally better for vote choice than for public opinions, more accurate in\nEnglish-speaking countries, more effective in bipartisan systems than in\nmulti-partisan systems, and stronger in democratic settings than in\nauthoritarian regimes. These results contribute to enhancing our understanding\nand developing strategies to mitigate biases in AI applications within the\nfield of computational social science."
        ],
        "neg": []
    },
    {
        "query": "Recent advances show that two-stream approaches have achieved outstanding\nperformance in hateful meme detection. However, hateful memes constantly evolve\nas new memes emerge by fusing progressive cultural ideas, making existing\nmethods obsolete or ineffective. In this work, we explore the potential of\nLarge Multimodal Models (LMMs) for hateful meme detection. To this end, we\npropose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE)\nPrompting, by integrating the evolution attribute and in-context information of\nmemes. Specifically, Evolver simulates the evolving and expressing process of\nmemes and reasons through LMMs in a step-by-step manner. First, an evolutionary\npair mining module retrieves the top-k most similar memes in the external\ncurated meme set with the input meme. Second, an evolutionary information\nextractor is designed to summarize the semantic regularities between the paired\nmemes for prompting. Finally, a contextual relevance amplifier enhances the\nin-context hatefulness information to boost the search for evolutionary\nprocesses. Extensive experiments on public FHM, MAMI, and HarM datasets show\nthat CoE prompting can be incorporated into existing LMMs to improve their\nperformance. More encouragingly, it can serve as an interpretive tool to\npromote the understanding of the evolution of social memes.",
        "pos": [
            "This study seeks to identify and quantify biases in simulating political\nsamples with Large Language Models, specifically focusing on vote choice and\npublic opinion. Using the GPT-3.5-Turbo model, we leverage data from the\nAmerican National Election Studies, German Longitudinal Election Study, Zuobiao\nDataset, and China Family Panel Studies to simulate voting behaviors and public\nopinions. This methodology enables us to examine three types of representation\nbias: disparities based on the the country's language, demographic groups, and\npolitical regime types. The findings reveal that simulation performance is\ngenerally better for vote choice than for public opinions, more accurate in\nEnglish-speaking countries, more effective in bipartisan systems than in\nmulti-partisan systems, and stronger in democratic settings than in\nauthoritarian regimes. These results contribute to enhancing our understanding\nand developing strategies to mitigate biases in AI applications within the\nfield of computational social science."
        ],
        "neg": []
    },
    {
        "query": "Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).",
        "pos": [
            "Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets."
        ],
        "neg": []
    },
    {
        "query": "CultureVo, Inc. has developed the Integrated Culture Learning Suite (ICLS) to\ndeliver foundational knowledge of world cultures through a combination of\ninteractive lessons and gamified experiences. This paper explores how\nGenerative AI powered by open source Large Langauge Models are utilized within\nthe ICLS to enhance cultural intelligence. The suite employs Generative AI\ntechniques to automate the assessment of learner knowledge, analyze behavioral\npatterns, and manage interactions with non-player characters using real time\nlearner assessment. Additionally, ICLS provides contextual hint and recommend\ncourse content by assessing learner proficiency, while Generative AI\nfacilitates the automated creation and validation of educational content.",
        "pos": [
            "This paper introduces a new hyper-parameter for Retrieval-Augmented\nGeneration (RAG) systems called Context Window Utilization. RAG systems enhance\ngenerative models by incorporating relevant information retrieved from external\nknowledge bases, improving the factual accuracy and contextual relevance of\ngenerated responses. The size of the text chunks retrieved and processed is a\ncritical factor influencing RAG performance. This study aims to identify the\noptimal chunk size that maximizes answer generation quality. Through systematic\nexperimentation, we analyze the effects of varying chunk sizes on the\nefficiency and effectiveness of RAG frameworks. Our findings reveal that an\noptimal chunk size balances the trade-off between providing sufficient context\nand minimizing irrelevant information. These insights are crucial for enhancing\nthe design and implementation of RAG systems, underscoring the importance of\nselecting an appropriate chunk size to achieve superior performance."
        ],
        "neg": []
    },
    {
        "query": "Multi-label few-shot aspect category detection aims at identifying multiple\naspect categories from sentences with a limited number of training instances.\nThe representation of sentences and categories is a key issue in this task.\nMost of current methods extract keywords for the sentence representations and\nthe category representations. Sentences often contain many category-independent\nwords, which leads to suboptimal performance of keyword-based methods. Instead\nof directly extracting keywords, we propose a label-guided prompt method to\nrepresent sentences and categories. To be specific, we design label-specific\nprompts to represent sentences by combining crucial contextual and semantic\ninformation. Further, the label is introduced into a prompt to obtain category\ndescriptions by utilizing a large language model. This kind of category\ndescriptions contain the characteristics of the aspect categories, guiding the\nconstruction of discriminative category prototypes. Experimental results on two\npublic datasets show that our method outperforms current state-of-the-art\nmethods with a 3.86% - 4.75% improvement in the Macro-F1 score.",
        "pos": [
            "Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process."
        ],
        "neg": []
    },
    {
        "query": "This paper presents an overview of the Arabic Natural Language Understanding\n(ArabicNLU 2024) shared task, focusing on two subtasks: Word Sense\nDisambiguation (WSD) and Location Mention Disambiguation (LMD). The task aimed\nto evaluate the ability of automated systems to resolve word ambiguity and\nidentify locations mentioned in Arabic text. We provided participants with\nnovel datasets, including a sense-annotated corpus for WSD, called SALMA with\napproximately 34k annotated tokens, and the IDRISI-DA dataset with 3,893\nannotations and 763 unique location mentions. These are challenging tasks. Out\nof the 38 registered teams, only three teams participated in the final\nevaluation phase, with the highest accuracy being 77.8% for WSD and the highest\nMRR@1 being 95.0% for LMD. The shared task not only facilitated the evaluation\nand comparison of different techniques, but also provided valuable insights and\nresources for the continued advancement of Arabic NLU technologies.",
        "pos": [
            "We present an overview of the second edition of the ArAIEval shared task,\norganized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In\nthis edition, ArAIEval offers two tasks: (i) detection of propagandistic\ntextual spans with persuasion techniques identification in tweets and news\narticles, and (ii) distinguishing between propagandistic and non-propagandistic\nmemes. A total of 14 teams participated in the final evaluation phase, with 6\nand 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams\nsubmitted system description papers. Across both tasks, we observed that\nfine-tuning transformer models such as AraBERT was at the core of the majority\nof the participating systems. We provide a description of the task setup,\nincluding a description of the dataset construction and the evaluation setup.\nWe further provide a brief overview of the participating systems. All datasets\nand evaluation scripts are released to the research community\n(https://araieval.gitlab.io/). We hope this will enable further research on\nthese important tasks in Arabic."
        ],
        "neg": []
    },
    {
        "query": "This paper presents an overview of the Arabic Natural Language Understanding\n(ArabicNLU 2024) shared task, focusing on two subtasks: Word Sense\nDisambiguation (WSD) and Location Mention Disambiguation (LMD). The task aimed\nto evaluate the ability of automated systems to resolve word ambiguity and\nidentify locations mentioned in Arabic text. We provided participants with\nnovel datasets, including a sense-annotated corpus for WSD, called SALMA with\napproximately 34k annotated tokens, and the IDRISI-DA dataset with 3,893\nannotations and 763 unique location mentions. These are challenging tasks. Out\nof the 38 registered teams, only three teams participated in the final\nevaluation phase, with the highest accuracy being 77.8% for WSD and the highest\nMRR@1 being 95.0% for LMD. The shared task not only facilitated the evaluation\nand comparison of different techniques, but also provided valuable insights and\nresources for the continued advancement of Arabic NLU technologies.",
        "pos": [
            "We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed."
        ],
        "neg": []
    },
    {
        "query": "Transformer-based Large Language Models (LLMs) have demonstrated remarkable\nsuccess across various challenging tasks. However, the deployment of LLMs is\nhindered by their substantial parameter count and memory consumption. Recently,\nnumerous studies have attempted to compress LLMs by pruning them using\ntraining-free methods. However, these pruned models often experience\nsignificant performance degradation on complex tasks. To address this issue, we\npropose a novel training pipeline for semi-structured sparse models, named\nAdaptive Sparse Trainer (AST). By distilling the knowledge stored in its dense\ncounterpart, we prevent the sparse model from overfitting and ensure a stable\ntraining process. Moreover, AST allows the model to adaptively select better\nlottery tickets (e.g., masks) during training. Additionally, we discovered that\nadding extra well-initialized parameters can further enhance model performance\nwith only a small increase in memory footprint. Our method significantly\nnarrows the performance gap between dense and sparse models while maintaining\nlimited computational cost. Furthermore, when combined with existing\nquantization methods, AST can compress language models by up to 16x compared to\ndense FP32 precision models with minimal performance loss. AST outperforms\nprevious state-of-the-art methods by reducing the zero-shot accuracy gap\nbetween dense and semi-structured sparse models to 1.12% across multiple\nzero-shot tasks on Llama2-7B, using less than 0.4% of the pretraining tokens.",
        "pos": [
            "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset using LLMs and jailbreaking prompt attacks. Based on our evaluation\nresults, we draw several important findings, including: 1) no single model\nexcels in all aspects, with different models showing various strengths; 2) the\ncorrelation between GPT-4 assessments and manual reviews is generally high; 3)\nthere is a trade-off between the usability and safety of text-to-video\ngenerative models. This indicates that as the field of video generation rapidly\nadvances, safety risks are set to surge, highlighting the urgency of\nprioritizing video safety. We hope that T2VSafetyBench can provide insights for\nbetter understanding the safety of video generation in the era of generative\nAI."
        ],
        "neg": []
    },
    {
        "query": "The demand for social robots in fields like healthcare, education, and\nentertainment increases due to their emotional adaptation features. These\nrobots leverage multimodal communication, incorporating speech, facial\nexpressions, and gestures to enhance user engagement and emotional support. The\nunderstanding of design paradigms of social robots is obstructed by the\ncomplexity of the system and the necessity to tune it to a specific task. This\narticle provides a structured review of social robot design paradigms,\ncategorizing them into cognitive architectures, role design models, linguistic\nmodels, communication flow, activity system models, and integrated design\nmodels. By breaking down the articles on social robot design and application\nbased on these paradigms, we highlight the strengths and areas for improvement\nin current approaches. We further propose our original integrated design model\nthat combines the most important aspects of the design of social robots. Our\napproach shows the importance of integrating operational, communicational, and\nemotional dimensions to create more adaptive and empathetic interactions\nbetween robots and humans.",
        "pos": [
            "ERIT is a novel multimodal dataset designed to facilitate research in a\nlightweight multimodal fusion. It contains text and image data collected from\nvideos of elderly individuals reacting to various situations, as well as seven\nemotion labels for each data sample. Because of the use of labeled images of\nelderly users reacting emotionally, it is also facilitating research on emotion\nrecognition in an underrepresented age group in machine learning visual emotion\nrecognition. The dataset is validated through comprehensive experiments\nindicating its importance in neural multimodal fusion research."
        ],
        "neg": []
    },
    {
        "query": "The demand for social robots in fields like healthcare, education, and\nentertainment increases due to their emotional adaptation features. These\nrobots leverage multimodal communication, incorporating speech, facial\nexpressions, and gestures to enhance user engagement and emotional support. The\nunderstanding of design paradigms of social robots is obstructed by the\ncomplexity of the system and the necessity to tune it to a specific task. This\narticle provides a structured review of social robot design paradigms,\ncategorizing them into cognitive architectures, role design models, linguistic\nmodels, communication flow, activity system models, and integrated design\nmodels. By breaking down the articles on social robot design and application\nbased on these paradigms, we highlight the strengths and areas for improvement\nin current approaches. We further propose our original integrated design model\nthat combines the most important aspects of the design of social robots. Our\napproach shows the importance of integrating operational, communicational, and\nemotional dimensions to create more adaptive and empathetic interactions\nbetween robots and humans.",
        "pos": [
            "ERIT is a novel multimodal dataset designed to facilitate research in a\nlightweight multimodal fusion. It contains text and image data collected from\nvideos of elderly individuals reacting to various situations, as well as seven\nemotion labels for each data sample. Because of the use of labeled images of\nelderly users reacting emotionally, it is also facilitating research on emotion\nrecognition in an underrepresented age group in machine learning visual emotion\nrecognition. The dataset is validated through comprehensive experiments\nindicating its importance in neural multimodal fusion research."
        ],
        "neg": []
    },
    {
        "query": "Instruction tuning in multimodal large language models (MLLMs) aims to\nsmoothly integrate a backbone LLM with a pre-trained feature encoder for\ndownstream tasks. The major challenge is how to efficiently find the synergy\nthrough cooperative learning where LLMs adapt their reasoning abilities in\ndownstream tasks while feature encoders adjust their encoding to provide more\nrelevant modal information. In this paper, we analyze the MLLM instruction\ntuning from both theoretical and empirical perspectives, where we find\nunbalanced learning between the two components, i.e., the feature encoder and\nthe LLM, can cause diminishing learning gradients that slow the model\nconvergence and often lead to sub-optimal results due to insufficient learning.\nInspired by our findings, we propose a measurement to quantitatively evaluate\nthe learning balance, based on which we further design a dynamic learning\nscheduler that better coordinates the learning. In addition, we introduce an\nauxiliary loss regularization method to promote updating of the generation\ndistribution of MLLMs considering the learning state of each model component,\nwhich potentially prevents each component from gradient diminishing and enables\na more accurate estimation of the learning balance coefficient. We conduct\nexperiments with multiple LLM backbones and feature encoders, where our\ntechniques are model-agnostic and can be generically integrated with various\nMLLM backbones. Experiment results on multiple downstream tasks and modalities\nin vision and audio, demonstrate the proposed method's better efficiency and\neffectiveness in MLLM instruction tuning.",
        "pos": [
            "We study open-world multi-label text classification under extremely weak\nsupervision (XWS), where the user only provides a brief description for\nclassification objectives without any labels or ground-truth label space.\nSimilar single-label XWS settings have been explored recently, however, these\nmethods cannot be easily adapted for multi-label. We observe that (1) most\ndocuments have a dominant class covering the majority of content and (2)\nlong-tail labels would appear in some documents as a dominant class. Therefore,\nwe first utilize the user description to prompt a large language model (LLM)\nfor dominant keyphrases of a subset of raw documents, and then construct a\n(initial) label space via clustering. We further apply a zero-shot multi-label\nclassifier to locate the documents with small top predicted scores, so we can\nrevisit their dominant keyphrases for more long-tail labels. We iterate this\nprocess to discover a comprehensive label space and construct a multi-label\nclassifier as a novel method, X-MLClass. X-MLClass exhibits a remarkable\nincrease in ground-truth label space coverage on various datasets, for example,\na 40% improvement on the AAPD dataset over topic modeling and keyword\nextraction methods. Moreover, X-MLClass achieves the best end-to-end\nmulti-label classification accuracy."
        ],
        "neg": []
    },
    {
        "query": "Instruction tuning in multimodal large language models (MLLMs) aims to\nsmoothly integrate a backbone LLM with a pre-trained feature encoder for\ndownstream tasks. The major challenge is how to efficiently find the synergy\nthrough cooperative learning where LLMs adapt their reasoning abilities in\ndownstream tasks while feature encoders adjust their encoding to provide more\nrelevant modal information. In this paper, we analyze the MLLM instruction\ntuning from both theoretical and empirical perspectives, where we find\nunbalanced learning between the two components, i.e., the feature encoder and\nthe LLM, can cause diminishing learning gradients that slow the model\nconvergence and often lead to sub-optimal results due to insufficient learning.\nInspired by our findings, we propose a measurement to quantitatively evaluate\nthe learning balance, based on which we further design a dynamic learning\nscheduler that better coordinates the learning. In addition, we introduce an\nauxiliary loss regularization method to promote updating of the generation\ndistribution of MLLMs considering the learning state of each model component,\nwhich potentially prevents each component from gradient diminishing and enables\na more accurate estimation of the learning balance coefficient. We conduct\nexperiments with multiple LLM backbones and feature encoders, where our\ntechniques are model-agnostic and can be generically integrated with various\nMLLM backbones. Experiment results on multiple downstream tasks and modalities\nin vision and audio, demonstrate the proposed method's better efficiency and\neffectiveness in MLLM instruction tuning.",
        "pos": [
            "Large Language Model (LLMs) such as ChatGPT that exhibit generative AI\ncapabilities are facing accelerated adoption and innovation. The increased\npresence of Generative AI (GAI) inevitably raises concerns about the risks and\nsafety associated with these models. This article provides an up-to-date survey\nof recent trends in AI safety research of GAI-LLMs from a computer scientist's\nperspective: specific and technical. In this survey, we explore the background\nand motivation for the identified harms and risks in the context of LLMs being\ngenerative language models; our survey differentiates by emphasising the need\nfor unified theories of the distinct safety challenges in the research\ndevelopment and applications of LLMs. We start our discussion with a concise\nintroduction to the workings of LLMs, supported by relevant literature. Then we\ndiscuss earlier research that has pointed out the fundamental constraints of\ngenerative models, or lack of understanding thereof (e.g., performance and\nsafety trade-offs as LLMs scale in number of parameters). We provide a\nsufficient coverage of LLM alignment -- delving into various approaches,\ncontending methods and present challenges associated with aligning LLMs with\nhuman preferences. By highlighting the gaps in the literature and possible\nimplementation oversights, our aim is to create a comprehensive analysis that\nprovides insights for addressing AI safety in LLMs and encourages the\ndevelopment of aligned and secure models. We conclude our survey by discussing\nfuture directions of LLMs for AI safety, offering insights into ongoing\nresearch in this critical area."
        ],
        "neg": []
    },
    {
        "query": "Instruction tuning in multimodal large language models (MLLMs) aims to\nsmoothly integrate a backbone LLM with a pre-trained feature encoder for\ndownstream tasks. The major challenge is how to efficiently find the synergy\nthrough cooperative learning where LLMs adapt their reasoning abilities in\ndownstream tasks while feature encoders adjust their encoding to provide more\nrelevant modal information. In this paper, we analyze the MLLM instruction\ntuning from both theoretical and empirical perspectives, where we find\nunbalanced learning between the two components, i.e., the feature encoder and\nthe LLM, can cause diminishing learning gradients that slow the model\nconvergence and often lead to sub-optimal results due to insufficient learning.\nInspired by our findings, we propose a measurement to quantitatively evaluate\nthe learning balance, based on which we further design a dynamic learning\nscheduler that better coordinates the learning. In addition, we introduce an\nauxiliary loss regularization method to promote updating of the generation\ndistribution of MLLMs considering the learning state of each model component,\nwhich potentially prevents each component from gradient diminishing and enables\na more accurate estimation of the learning balance coefficient. We conduct\nexperiments with multiple LLM backbones and feature encoders, where our\ntechniques are model-agnostic and can be generically integrated with various\nMLLM backbones. Experiment results on multiple downstream tasks and modalities\nin vision and audio, demonstrate the proposed method's better efficiency and\neffectiveness in MLLM instruction tuning.",
        "pos": [
            "Office automation significantly enhances human productivity by automatically\nfinishing routine tasks in the workflow. Beyond the basic information\nextraction studied in much of the prior document AI literature, the office\nautomation research should be extended to more realistic office tasks which\nrequire to integrate various information sources in the office system and\nproduce outputs through a series of decision-making processes. We introduce\nOfficeBench, one of the first office automation benchmarks for evaluating\ncurrent LLM agents' capability to address office tasks in realistic office\nworkflows. OfficeBench requires LLM agents to perform feasible long-horizon\nplanning, proficiently switch between applications in a timely manner, and\naccurately ground their actions within a large combined action space, based on\nthe contextual demands of the workflow. Applying our customized evaluation\nmethods on each task, we find that GPT-4 Omni achieves the highest pass rate of\n47.00%, demonstrating a decent performance in handling office tasks. However,\nthis is still far below the human performance and accuracy standards required\nby real-world office workflows. We further observe that most issues are related\nto operation redundancy and hallucinations, as well as limitations in switching\nbetween multiple applications, which may provide valuable insights for\ndeveloping effective agent frameworks for office automation.",
            "Retrieval augmented generation (RAG) combines the generative abilities of\nlarge language models (LLMs) with external knowledge sources to provide more\naccurate and up-to-date responses. Recent RAG advancements focus on improving\nretrieval outcomes through iterative LLM refinement or self-critique\ncapabilities acquired through additional instruction tuning of LLMs. In this\nwork, we introduce Speculative RAG - a framework that leverages a larger\ngeneralist LM to efficiently verify multiple RAG drafts produced in parallel by\na smaller, distilled specialist LM. Each draft is generated from a distinct\nsubset of retrieved documents, offering diverse perspectives on the evidence\nwhile reducing input token counts per draft. This approach enhances\ncomprehension of each subset and mitigates potential position bias over long\ncontext. Our method accelerates RAG by delegating drafting to the smaller\nspecialist LM, with the larger generalist LM performing a single verification\npass over the drafts. Extensive experiments demonstrate that Speculative RAG\nachieves state-of-the-art performance with reduced latency on TriviaQA,\nMuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy\nby up to 12.97% while reducing latency by 51% compared to conventional RAG\nsystems on PubHealth.",
            "We study open-world multi-label text classification under extremely weak\nsupervision (XWS), where the user only provides a brief description for\nclassification objectives without any labels or ground-truth label space.\nSimilar single-label XWS settings have been explored recently, however, these\nmethods cannot be easily adapted for multi-label. We observe that (1) most\ndocuments have a dominant class covering the majority of content and (2)\nlong-tail labels would appear in some documents as a dominant class. Therefore,\nwe first utilize the user description to prompt a large language model (LLM)\nfor dominant keyphrases of a subset of raw documents, and then construct a\n(initial) label space via clustering. We further apply a zero-shot multi-label\nclassifier to locate the documents with small top predicted scores, so we can\nrevisit their dominant keyphrases for more long-tail labels. We iterate this\nprocess to discover a comprehensive label space and construct a multi-label\nclassifier as a novel method, X-MLClass. X-MLClass exhibits a remarkable\nincrease in ground-truth label space coverage on various datasets, for example,\na 40% improvement on the AAPD dataset over topic modeling and keyword\nextraction methods. Moreover, X-MLClass achieves the best end-to-end\nmulti-label classification accuracy."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) rely on instruction samples for alignment, but\ncreating these datasets poses challenges, particularly in expert-dependent\ntasks like coding, which can be cost-prohibitive. One approach to mitigate\nthese challenges is synthesizing data using another LLM. In this paper, we\nintroduce a scalable method for generating synthetic instructions to enhance\nthe code generation capability of LLMs. The proposed algorithm,\nGenetic-Instruct, mimics evolutionary processes, utilizing self-instruction to\ncreate numerous synthetic samples from a limited number of seeds.\nGenetic-Instruct is designed for efficient scaling of the generation process.\nFine-tuning multiple coding LLMs with the synthetic samples demonstrates a\nsignificant improvement in their code generation accuracy compared to the\nbaselines.",
        "pos": [
            "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis of between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) rely on instruction samples for alignment, but\ncreating these datasets poses challenges, particularly in expert-dependent\ntasks like coding, which can be cost-prohibitive. One approach to mitigate\nthese challenges is synthesizing data using another LLM. In this paper, we\nintroduce a scalable method for generating synthetic instructions to enhance\nthe code generation capability of LLMs. The proposed algorithm,\nGenetic-Instruct, mimics evolutionary processes, utilizing self-instruction to\ncreate numerous synthetic samples from a limited number of seeds.\nGenetic-Instruct is designed for efficient scaling of the generation process.\nFine-tuning multiple coding LLMs with the synthetic samples demonstrates a\nsignificant improvement in their code generation accuracy compared to the\nbaselines.",
        "pos": [
            "Discrete speech representations have garnered recent attention for their\nefficacy in training transformer-based models for various speech-related tasks\nsuch as automatic speech recognition (ASR), translation, speaker verification,\nand joint speech-text foundational models. In this work, we present a\ncomprehensive analysis on building ASR systems with discrete codes. We\ninvestigate different methods for codec training such as quantization schemes\nand time-domain vs spectral feature encodings. We further explore ASR training\ntechniques aimed at enhancing performance, training efficiency, and noise\nrobustness. Drawing upon our findings, we introduce a codec ASR pipeline that\noutperforms Encodec at similar bit-rate. Remarkably, it also surpasses the\nstate-of-the-art results achieved by strong self-supervised models on the 143\nlanguages ML-SUPERB benchmark despite being smaller in size and pretrained on\nsignificantly less data."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) rely on instruction samples for alignment, but\ncreating these datasets poses challenges, particularly in expert-dependent\ntasks like coding, which can be cost-prohibitive. One approach to mitigate\nthese challenges is synthesizing data using another LLM. In this paper, we\nintroduce a scalable method for generating synthetic instructions to enhance\nthe code generation capability of LLMs. The proposed algorithm,\nGenetic-Instruct, mimics evolutionary processes, utilizing self-instruction to\ncreate numerous synthetic samples from a limited number of seeds.\nGenetic-Instruct is designed for efficient scaling of the generation process.\nFine-tuning multiple coding LLMs with the synthetic samples demonstrates a\nsignificant improvement in their code generation accuracy compared to the\nbaselines.",
        "pos": [
            "We introduce romanization encoding for script-heavy languages to optimize\nmultilingual and code-switching Automatic Speech Recognition (ASR) systems. By\nadopting romanization encoding alongside a balanced concatenated tokenizer\nwithin a FastConformer-RNNT framework equipped with a Roman2Char module, we\nsignificantly reduce vocabulary and output dimensions, enabling larger training\nbatches and reduced memory consumption. Our method decouples acoustic modeling\nand language modeling, enhancing the flexibility and adaptability of the\nsystem. In our study, applying this method to Mandarin-English ASR resulted in\na remarkable 63.51% vocabulary reduction and notable performance gains of\n13.72% and 15.03% on SEAME code-switching benchmarks. Ablation studies on\nMandarin-Korean and Mandarin-Japanese highlight our method's strong capability\nto address the complexities of other script-heavy languages, paving the way for\nmore versatile and effective multilingual ASR systems.",
            "Discrete speech representations have garnered recent attention for their\nefficacy in training transformer-based models for various speech-related tasks\nsuch as automatic speech recognition (ASR), translation, speaker verification,\nand joint speech-text foundational models. In this work, we present a\ncomprehensive analysis on building ASR systems with discrete codes. We\ninvestigate different methods for codec training such as quantization schemes\nand time-domain vs spectral feature encodings. We further explore ASR training\ntechniques aimed at enhancing performance, training efficiency, and noise\nrobustness. Drawing upon our findings, we introduce a codec ASR pipeline that\noutperforms Encodec at similar bit-rate. Remarkably, it also surpasses the\nstate-of-the-art results achieved by strong self-supervised models on the 143\nlanguages ML-SUPERB benchmark despite being smaller in size and pretrained on\nsignificantly less data."
        ],
        "neg": []
    },
    {
        "query": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
        "pos": [
            "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development."
        ],
        "neg": []
    },
    {
        "query": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
        "pos": [
            "In this paper, we propose reverse inference optimization (RIO), a simple and\neffective method designed to enhance the robustness of\nautoregressive-model-based zero-shot text-to-speech (TTS) systems using\nreinforcement learning from human feedback (RLHF). To assess the quality of\nspeech produced by the TTS system without human annotations, RIO introduces a\nnovel concept termed as reverse inference based on the Bayesian principle,\nwhich suggests that a high-quality generated speech should be able to be used\nas a prompt for subsequent generation using the same TTS model. By leveraging\nreverse inference as the standard to select exemplars used in RLHF from the\nspeech samples generated by the TTS system itself, RIO steers the subsequent\noptimization towards a direction of enhancing the TTS robustness. The RIO\nframework, comprising sampling, automatic annotating, and learning, obviates\nthe need for a reward model or pairwise preference data, and significantly\nimproves the stability of zero-shot TTS performance by reducing the\ndiscrepancies between training and inference conditions. Our experimental\nresults verify that RIO can effectively improve both subjective and objective\nmetrics, including mean opinion scores, word error rates, and speaker\nsimilarity. Remarkably, RIO can also diminish the incidence of bad outputs to\nnearly zero percent, rivalling the robustness when using ground-truth speech as\nthe prompt."
        ],
        "neg": []
    },
    {
        "query": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
        "pos": [
            "Large Language Models (LLMs) have shown remarkable abilities across various\ntasks, yet their development has predominantly centered on high-resource\nlanguages like English and Chinese, leaving low-resource languages underserved.\nTo address this disparity, we present SeaLLMs 3, the latest iteration of the\nSeaLLMs model family, tailored for Southeast Asian languages. This region,\ncharacterized by its rich linguistic diversity, has lacked adequate language\ntechnology support. SeaLLMs 3 aims to bridge this gap by covering a\ncomprehensive range of languages spoken in this region, including English,\nChinese, Indonesian, Vietnamese, Thai, Tagalog, Malay, Burmese, Khmer, Lao,\nTamil, and Javanese. Leveraging efficient language enhancement techniques and a\nspecially constructed instruction tuning dataset, SeaLLMs 3 significantly\nreduces training costs while maintaining high performance and versatility. Our\nmodel excels in tasks such as world knowledge, mathematical reasoning,\ntranslation, and instruction following, achieving state-of-the-art performance\namong similarly sized models. Additionally, we prioritized safety and\nreliability by addressing both general and culture-specific considerations and\nincorporated mechanisms to reduce hallucinations. This work underscores the\nimportance of inclusive AI, showing that advanced LLM capabilities can benefit\nunderserved linguistic and cultural communities."
        ],
        "neg": []
    },
    {
        "query": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
        "pos": [
            "The process of human speech production involves coordinated respiratory\naction to elicit acoustic speech signals. Typically, speech is produced when\nair is forced from the lungs and is modulated by the vocal tract, where such\nactions are interspersed by moments of breathing in air (inhalation) to refill\nthe lungs again. Respiratory rate (RR) is a vital metric that is used to assess\nthe overall health, fitness, and general well-being of an individual. Existing\napproaches to measure RR (number of breaths one takes in a minute) are\nperformed using specialized equipment or training. Studies have demonstrated\nthat machine learning algorithms can be used to estimate RR using bio-sensor\nsignals as input. Speech-based estimation of RR can offer an effective approach\nto measure the vital metric without requiring any specialized equipment or\nsensors. This work investigates a machine learning based approach to estimate\nRR from speech segments obtained from subjects speaking to a close-talking\nmicrophone device. Data were collected from N=26 individuals, where the\ngroundtruth RR was obtained through commercial grade chest-belts and then\nmanually corrected for any errors. A convolutional long-short term memory\nnetwork (Conv-LSTM) is proposed to estimate respiration time-series data from\nthe speech signal. We demonstrate that the use of pre-trained representations\nobtained from a foundation model, such as Wav2Vec2, can be used to estimate\nrespiration-time-series with low root-mean-squared error and high correlation\ncoefficient, when compared with the baseline. The model-driven time series can\nbe used to estimate $RR$ with a low mean absolute error (MAE) ~ 1.6\nbreaths/min."
        ],
        "neg": []
    },
    {
        "query": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
        "pos": [
            "Privacy-preserving voice protection approaches primarily suppress\nprivacy-related information derived from paralinguistic attributes while\npreserving the linguistic content. Existing solutions focus on single-speaker\nscenarios. However, they lack practicality for real-world applications, i.e.,\nmulti-speaker scenarios. In this paper, we present an initial attempt to\nprovide a multi-speaker anonymization benchmark by defining the task and\nevaluation protocol, proposing benchmarking solutions, and discussing the\nprivacy leakage of overlapping conversations. Specifically, ideal multi-speaker\nanonymization should preserve the number of speakers and the turn-taking\nstructure of the conversation, ensuring accurate context conveyance while\nmaintaining privacy. To achieve that, a cascaded system uses speaker\ndiarization to aggregate the speech of each speaker and speaker anonymization\nto conceal speaker privacy and preserve speech content. Additionally, we\npropose two conversation-level speaker vector anonymization methods to improve\nthe utility further. Both methods aim to make the original and corresponding\npseudo-speaker identities of each speaker unlinkable while preserving or even\nimproving the distinguishability among pseudo-speakers in a conversation. The\nfirst method minimizes the differential similarity across speaker pairs in the\noriginal and anonymized conversations to maintain original speaker\nrelationships in the anonymized version. The other method minimizes the\naggregated similarity across anonymized speakers to achieve better\ndifferentiation between speakers. Experiments conducted on both non-overlap\nsimulated and real-world datasets demonstrate the effectiveness of the\nmulti-speaker anonymization system with the proposed speaker anonymizers.\nAdditionally, we analyzed overlapping speech regarding privacy leakage and\nprovide potential solutions."
        ],
        "neg": []
    },
    {
        "query": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
        "pos": [
            "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
        ],
        "neg": []
    },
    {
        "query": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
        "pos": [
            "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
        ],
        "neg": []
    },
    {
        "query": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
        "pos": [
            "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
        ],
        "neg": []
    },
    {
        "query": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
        "pos": [
            "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
        ],
        "neg": []
    },
    {
        "query": "Knowledge editing techniques have been increasingly adopted to efficiently\ncorrect the false or outdated knowledge in Large Language Models (LLMs), due to\nthe high cost of retraining from scratch. Meanwhile, one critical but\nunder-explored question is: can knowledge editing be used to inject harm into\nLLMs? In this paper, we propose to reformulate knowledge editing as a new type\nof safety threat for LLMs, namely Editing Attack, and conduct a systematic\ninvestigation with a newly constructed dataset EditAttack. Specifically, we\nfocus on two typical safety risks of Editing Attack including Misinformation\nInjection and Bias Injection. For the risk of misinformation injection, we\nfirst categorize it into commonsense misinformation injection and long-tail\nmisinformation injection. Then, we find that editing attacks can inject both\ntypes of misinformation into LLMs, and the effectiveness is particularly high\nfor commonsense misinformation injection. For the risk of bias injection, we\ndiscover that not only can biased sentences be injected into LLMs with high\neffectiveness, but also one single biased sentence injection can cause a bias\nincrease in general outputs of LLMs, which are even highly irrelevant to the\ninjected sentence, indicating a catastrophic impact on the overall fairness of\nLLMs. Then, we further illustrate the high stealthiness of editing attacks,\nmeasured by their impact on the general knowledge and reasoning capacities of\nLLMs, and show the hardness of defending editing attacks with empirical\nevidence. Our discoveries demonstrate the emerging misuse risks of knowledge\nediting techniques on compromising the safety alignment of LLMs.",
        "pos": [
            "The rapid advancement of Large Language Models (LLMs) and Large Multimodal\nModels (LMMs) has heightened the demand for AI-based scientific assistants\ncapable of understanding scientific articles and figures. Despite progress,\nthere remains a significant gap in evaluating models' comprehension of\nprofessional, graduate-level, and even PhD-level scientific content. Current\ndatasets and benchmarks primarily focus on relatively simple scientific tasks\nand figures, lacking comprehensive assessments across diverse advanced\nscientific disciplines. To bridge this gap, we collected a multimodal,\nmultidisciplinary dataset from open-access scientific articles published in\nNature Communications journals. This dataset spans 72 scientific disciplines,\nensuring both diversity and quality. We created benchmarks with various tasks\nand settings to comprehensively evaluate LMMs' capabilities in understanding\nscientific figures and content. Our evaluation revealed that these tasks are\nhighly challenging: many open-source models struggled significantly, and even\nGPT-4V and GPT-4o faced difficulties. We also explored using our dataset as\ntraining resources by constructing visual instruction-following data, enabling\nthe 7B LLaVA model to achieve performance comparable to GPT-4V/o on our\nbenchmark. Additionally, we investigated the use of our interleaved article\ntexts and figure images for pre-training LMMs, resulting in improvements on the\nmaterial generation task. The source dataset, including articles, figures,\nconstructed benchmarks, and visual instruction-following data, is open-sourced."
        ],
        "neg": []
    },
    {
        "query": "Knowledge editing techniques have been increasingly adopted to efficiently\ncorrect the false or outdated knowledge in Large Language Models (LLMs), due to\nthe high cost of retraining from scratch. Meanwhile, one critical but\nunder-explored question is: can knowledge editing be used to inject harm into\nLLMs? In this paper, we propose to reformulate knowledge editing as a new type\nof safety threat for LLMs, namely Editing Attack, and conduct a systematic\ninvestigation with a newly constructed dataset EditAttack. Specifically, we\nfocus on two typical safety risks of Editing Attack including Misinformation\nInjection and Bias Injection. For the risk of misinformation injection, we\nfirst categorize it into commonsense misinformation injection and long-tail\nmisinformation injection. Then, we find that editing attacks can inject both\ntypes of misinformation into LLMs, and the effectiveness is particularly high\nfor commonsense misinformation injection. For the risk of bias injection, we\ndiscover that not only can biased sentences be injected into LLMs with high\neffectiveness, but also one single biased sentence injection can cause a bias\nincrease in general outputs of LLMs, which are even highly irrelevant to the\ninjected sentence, indicating a catastrophic impact on the overall fairness of\nLLMs. Then, we further illustrate the high stealthiness of editing attacks,\nmeasured by their impact on the general knowledge and reasoning capacities of\nLLMs, and show the hardness of defending editing attacks with empirical\nevidence. Our discoveries demonstrate the emerging misuse risks of knowledge\nediting techniques on compromising the safety alignment of LLMs.",
        "pos": [
            "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench."
        ],
        "neg": []
    },
    {
        "query": "Knowledge editing techniques have been increasingly adopted to efficiently\ncorrect the false or outdated knowledge in Large Language Models (LLMs), due to\nthe high cost of retraining from scratch. Meanwhile, one critical but\nunder-explored question is: can knowledge editing be used to inject harm into\nLLMs? In this paper, we propose to reformulate knowledge editing as a new type\nof safety threat for LLMs, namely Editing Attack, and conduct a systematic\ninvestigation with a newly constructed dataset EditAttack. Specifically, we\nfocus on two typical safety risks of Editing Attack including Misinformation\nInjection and Bias Injection. For the risk of misinformation injection, we\nfirst categorize it into commonsense misinformation injection and long-tail\nmisinformation injection. Then, we find that editing attacks can inject both\ntypes of misinformation into LLMs, and the effectiveness is particularly high\nfor commonsense misinformation injection. For the risk of bias injection, we\ndiscover that not only can biased sentences be injected into LLMs with high\neffectiveness, but also one single biased sentence injection can cause a bias\nincrease in general outputs of LLMs, which are even highly irrelevant to the\ninjected sentence, indicating a catastrophic impact on the overall fairness of\nLLMs. Then, we further illustrate the high stealthiness of editing attacks,\nmeasured by their impact on the general knowledge and reasoning capacities of\nLLMs, and show the hardness of defending editing attacks with empirical\nevidence. Our discoveries demonstrate the emerging misuse risks of knowledge\nediting techniques on compromising the safety alignment of LLMs.",
        "pos": [
            "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has\nenhanced medical diagnosis. However, current Med-LVLMs frequently encounter\nfactual issues, often generating responses that do not align with established\nmedical facts. Retrieval-Augmented Generation (RAG), which utilizes external\nknowledge, can improve the factual accuracy of these models but introduces two\nmajor challenges. First, limited retrieved contexts might not cover all\nnecessary information, while excessive retrieval can introduce irrelevant and\ninaccurate references, interfering with the model's generation. Second, in\ncases where the model originally responds correctly, applying RAG can lead to\nan over-reliance on retrieved contexts, resulting in incorrect answers. To\naddress these issues, we propose RULE, which consists of two components. First,\nwe introduce a provably effective strategy for controlling factuality risk\nthrough the calibrated selection of the number of retrieved contexts. Second,\nbased on samples where over-reliance on retrieved contexts led to errors, we\ncurate a preference dataset to fine-tune the model, balancing its dependence on\ninherent knowledge and retrieved contexts for generation. We demonstrate the\neffectiveness of RULE on three medical VQA datasets, achieving an average\nimprovement of 20.8% in factual accuracy. We publicly release our benchmark and\ncode in https://github.com/richard-peng-xia/RULE.",
            "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench."
        ],
        "neg": []
    },
    {
        "query": "Knowledge editing techniques have been increasingly adopted to efficiently\ncorrect the false or outdated knowledge in Large Language Models (LLMs), due to\nthe high cost of retraining from scratch. Meanwhile, one critical but\nunder-explored question is: can knowledge editing be used to inject harm into\nLLMs? In this paper, we propose to reformulate knowledge editing as a new type\nof safety threat for LLMs, namely Editing Attack, and conduct a systematic\ninvestigation with a newly constructed dataset EditAttack. Specifically, we\nfocus on two typical safety risks of Editing Attack including Misinformation\nInjection and Bias Injection. For the risk of misinformation injection, we\nfirst categorize it into commonsense misinformation injection and long-tail\nmisinformation injection. Then, we find that editing attacks can inject both\ntypes of misinformation into LLMs, and the effectiveness is particularly high\nfor commonsense misinformation injection. For the risk of bias injection, we\ndiscover that not only can biased sentences be injected into LLMs with high\neffectiveness, but also one single biased sentence injection can cause a bias\nincrease in general outputs of LLMs, which are even highly irrelevant to the\ninjected sentence, indicating a catastrophic impact on the overall fairness of\nLLMs. Then, we further illustrate the high stealthiness of editing attacks,\nmeasured by their impact on the general knowledge and reasoning capacities of\nLLMs, and show the hardness of defending editing attacks with empirical\nevidence. Our discoveries demonstrate the emerging misuse risks of knowledge\nediting techniques on compromising the safety alignment of LLMs.",
        "pos": [
            "The rapid advancement of Large Language Models (LLMs) and Large Multimodal\nModels (LMMs) has heightened the demand for AI-based scientific assistants\ncapable of understanding scientific articles and figures. Despite progress,\nthere remains a significant gap in evaluating models' comprehension of\nprofessional, graduate-level, and even PhD-level scientific content. Current\ndatasets and benchmarks primarily focus on relatively simple scientific tasks\nand figures, lacking comprehensive assessments across diverse advanced\nscientific disciplines. To bridge this gap, we collected a multimodal,\nmultidisciplinary dataset from open-access scientific articles published in\nNature Communications journals. This dataset spans 72 scientific disciplines,\nensuring both diversity and quality. We created benchmarks with various tasks\nand settings to comprehensively evaluate LMMs' capabilities in understanding\nscientific figures and content. Our evaluation revealed that these tasks are\nhighly challenging: many open-source models struggled significantly, and even\nGPT-4V and GPT-4o faced difficulties. We also explored using our dataset as\ntraining resources by constructing visual instruction-following data, enabling\nthe 7B LLaVA model to achieve performance comparable to GPT-4V/o on our\nbenchmark. Additionally, we investigated the use of our interleaved article\ntexts and figure images for pre-training LMMs, resulting in improvements on the\nmaterial generation task. The source dataset, including articles, figures,\nconstructed benchmarks, and visual instruction-following data, is open-sourced."
        ],
        "neg": []
    },
    {
        "query": "Knowledge editing techniques have been increasingly adopted to efficiently\ncorrect the false or outdated knowledge in Large Language Models (LLMs), due to\nthe high cost of retraining from scratch. Meanwhile, one critical but\nunder-explored question is: can knowledge editing be used to inject harm into\nLLMs? In this paper, we propose to reformulate knowledge editing as a new type\nof safety threat for LLMs, namely Editing Attack, and conduct a systematic\ninvestigation with a newly constructed dataset EditAttack. Specifically, we\nfocus on two typical safety risks of Editing Attack including Misinformation\nInjection and Bias Injection. For the risk of misinformation injection, we\nfirst categorize it into commonsense misinformation injection and long-tail\nmisinformation injection. Then, we find that editing attacks can inject both\ntypes of misinformation into LLMs, and the effectiveness is particularly high\nfor commonsense misinformation injection. For the risk of bias injection, we\ndiscover that not only can biased sentences be injected into LLMs with high\neffectiveness, but also one single biased sentence injection can cause a bias\nincrease in general outputs of LLMs, which are even highly irrelevant to the\ninjected sentence, indicating a catastrophic impact on the overall fairness of\nLLMs. Then, we further illustrate the high stealthiness of editing attacks,\nmeasured by their impact on the general knowledge and reasoning capacities of\nLLMs, and show the hardness of defending editing attacks with empirical\nevidence. Our discoveries demonstrate the emerging misuse risks of knowledge\nediting techniques on compromising the safety alignment of LLMs.",
        "pos": [
            "Modern language models (LMs) pose a new challenge in capability assessment.\nStatic benchmarks inevitably saturate without providing confidence in the\ndeployment tolerances of LM-based systems, but developers nonetheless claim\nthat their models have generalized traits such as reasoning or open-domain\nlanguage understanding based on these flawed metrics. The science and practice\nof LMs requires a new approach to benchmarking which measures specific\ncapabilities with dynamic assessments. To be confident in our metrics, we need\na new discipline of model metrology -- one which focuses on how to generate\nbenchmarks that predict performance under deployment. Motivated by our\nevaluation criteria, we outline how building a community of model metrology\npractitioners -- one focused on building tools and studying how to measure\nsystem capabilities -- is the best way to meet these needs to and add clarity\nto the AI discussion.",
            "Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models.",
            "Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research.",
            "To enhance Large Language Model (LLM) capabilities, multi-agent debates have\nbeen introduced, where multiple LLMs discuss solutions to a problem over\nseveral rounds of debate. However, LLMs often produce incorrect responses that\nappear deceptively confident, which can mislead other agents. This is partly\nbecause agents do not express their confidence levels during standard debates.\nTo address this, we introduce DebUnc, a multi-agent debate framework that uses\nuncertainty metrics to assess agent confidence levels. We adapted the LLM\nattention mechanism to adjust token weights based on confidence levels and also\nexplored using textual prompts to convey confidence. Our evaluations across\nvarious benchmarks show that attention-based methods are particularly\neffective, and that as uncertainty metrics evolve, performance will continue to\nincrease. The code is available at https://github.com/lukeyoffe/debunc",
            "The rapid advancement of Large Language Models (LLMs) and Large Multimodal\nModels (LMMs) has heightened the demand for AI-based scientific assistants\ncapable of understanding scientific articles and figures. Despite progress,\nthere remains a significant gap in evaluating models' comprehension of\nprofessional, graduate-level, and even PhD-level scientific content. Current\ndatasets and benchmarks primarily focus on relatively simple scientific tasks\nand figures, lacking comprehensive assessments across diverse advanced\nscientific disciplines. To bridge this gap, we collected a multimodal,\nmultidisciplinary dataset from open-access scientific articles published in\nNature Communications journals. This dataset spans 72 scientific disciplines,\nensuring both diversity and quality. We created benchmarks with various tasks\nand settings to comprehensively evaluate LMMs' capabilities in understanding\nscientific figures and content. Our evaluation revealed that these tasks are\nhighly challenging: many open-source models struggled significantly, and even\nGPT-4V and GPT-4o faced difficulties. We also explored using our dataset as\ntraining resources by constructing visual instruction-following data, enabling\nthe 7B LLaVA model to achieve performance comparable to GPT-4V/o on our\nbenchmark. Additionally, we investigated the use of our interleaved article\ntexts and figure images for pre-training LMMs, resulting in improvements on the\nmaterial generation task. The source dataset, including articles, figures,\nconstructed benchmarks, and visual instruction-following data, is open-sourced."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in language models have demonstrated their capability to\nsolve mathematical reasoning problems, achieving near-perfect accuracy on\ngrade-school level math benchmarks like GSM8K. In this paper, we formally study\nhow language models solve these problems. We design a series of controlled\nexperiments to address several fundamental questions: (1) Can language models\ntruly develop reasoning skills, or do they simply memorize templates? (2) What\nis the model's hidden (mental) reasoning process? (3) Do models solve math\nquestions using skills similar to or different from humans? (4) Do models\ntrained on GSM8K-like datasets develop reasoning skills beyond those necessary\nfor solving GSM8K problems? (5) What mental process causes models to make\nreasoning mistakes? (6) How large or deep must a model be to effectively solve\nGSM8K-level math questions?\n  Our study uncovers many hidden mechanisms by which language models solve\nmathematical questions, providing insights that extend beyond current\nunderstandings of LLMs.",
        "pos": [
            "Natural images captured by mobile devices often suffer from multiple types of\ndegradation, such as noise, blur, and low light. Traditional image restoration\nmethods require manual selection of specific tasks, algorithms, and execution\nsequences, which is time-consuming and may yield suboptimal results. All-in-one\nmodels, though capable of handling multiple tasks, typically support only a\nlimited range and often produce overly smooth, low-fidelity outcomes due to\ntheir broad data distribution fitting. To address these challenges, we first\ndefine a new pipeline for restoring images with multiple degradations, and then\nintroduce RestoreAgent, an intelligent image restoration system leveraging\nmultimodal large language models. RestoreAgent autonomously assesses the type\nand extent of degradation in input images and performs restoration through (1)\ndetermining the appropriate restoration tasks, (2) optimizing the task\nsequence, (3) selecting the most suitable models, and (4) executing the\nrestoration. Experimental results demonstrate the superior performance of\nRestoreAgent in handling complex degradation, surpassing human experts.\nFurthermore, the system modular design facilitates the fast integration of new\ntasks and models, enhancing its flexibility and scalability for various\napplications."
        ],
        "neg": []
    },
    {
        "query": "Conversational search supports multi-turn user-system interactions to solve\ncomplex information needs. Different from the traditional single-turn ad-hoc\nsearch, conversational search encounters a more challenging problem of\ncontext-dependent query understanding with the lengthy and long-tail\nconversational history context. While conversational query rewriting methods\nleverage explicit rewritten queries to train a rewriting model to transform the\ncontext-dependent query into a stand-stone search query, this is usually done\nwithout considering the quality of search results. Conversational dense\nretrieval methods use fine-tuning to improve a pre-trained ad-hoc query\nencoder, but they are limited by the conversational search data available for\ntraining. In this paper, we leverage both rewritten queries and relevance\njudgments in the conversational search data to train a better query\nrepresentation model. The key idea is to align the query representation with\nthose of rewritten queries and relevant documents. The proposed model -- Query\nRepresentation Alignment Conversational Dense Retriever, QRACDR, is tested on\neight datasets, including various settings in conversational search and ad-hoc\nsearch. The results demonstrate the strong performance of QRACDR compared with\nstate-of-the-art methods, and confirm the effectiveness of representation\nalignment.",
        "pos": [
            "Personalized conversational information retrieval (CIR) combines\nconversational and personalizable elements to satisfy various users' complex\ninformation needs through multi-turn interaction based on their backgrounds.\nThe key promise is that the personal textual knowledge base (PTKB) can improve\nthe CIR effectiveness because the retrieval results can be more related to the\nuser's background. However, PTKB is noisy: not every piece of knowledge in PTKB\nis relevant to the specific query at hand. In this paper, we explore and test\nseveral ways to select knowledge from PTKB and use it for query reformulation\nby using a large language model (LLM). The experimental results show the PTKB\nmight not always improve the search results when used alone, but LLM can help\ngenerate a more appropriate personalized query when high-quality guidance is\nprovided.",
            "Document-level biomedical concept extraction is the task of identifying\nbiomedical concepts mentioned in a given document. Recent advancements have\nadapted pre-trained language models for this task. However, the scarcity of\ndomain-specific data and the deviation of concepts from their canonical names\noften hinder these models' effectiveness. To tackle this issue, we employ\nMetaMapLite, an existing rule-based concept mapping system, to generate\nadditional pseudo-annotated data from PubMed and PMC. The annotated data are\nused to augment the limited training data. Through extensive experiments, this\nstudy demonstrates the utility of a manually crafted concept mapping tool for\ntraining a better concept extraction model."
        ],
        "neg": []
    },
    {
        "query": "Conversational search supports multi-turn user-system interactions to solve\ncomplex information needs. Different from the traditional single-turn ad-hoc\nsearch, conversational search encounters a more challenging problem of\ncontext-dependent query understanding with the lengthy and long-tail\nconversational history context. While conversational query rewriting methods\nleverage explicit rewritten queries to train a rewriting model to transform the\ncontext-dependent query into a stand-stone search query, this is usually done\nwithout considering the quality of search results. Conversational dense\nretrieval methods use fine-tuning to improve a pre-trained ad-hoc query\nencoder, but they are limited by the conversational search data available for\ntraining. In this paper, we leverage both rewritten queries and relevance\njudgments in the conversational search data to train a better query\nrepresentation model. The key idea is to align the query representation with\nthose of rewritten queries and relevant documents. The proposed model -- Query\nRepresentation Alignment Conversational Dense Retriever, QRACDR, is tested on\neight datasets, including various settings in conversational search and ad-hoc\nsearch. The results demonstrate the strong performance of QRACDR compared with\nstate-of-the-art methods, and confirm the effectiveness of representation\nalignment.",
        "pos": [
            "Personalized conversational information retrieval (CIR) combines\nconversational and personalizable elements to satisfy various users' complex\ninformation needs through multi-turn interaction based on their backgrounds.\nThe key promise is that the personal textual knowledge base (PTKB) can improve\nthe CIR effectiveness because the retrieval results can be more related to the\nuser's background. However, PTKB is noisy: not every piece of knowledge in PTKB\nis relevant to the specific query at hand. In this paper, we explore and test\nseveral ways to select knowledge from PTKB and use it for query reformulation\nby using a large language model (LLM). The experimental results show the PTKB\nmight not always improve the search results when used alone, but LLM can help\ngenerate a more appropriate personalized query when high-quality guidance is\nprovided."
        ],
        "neg": []
    },
    {
        "query": "Conversational search supports multi-turn user-system interactions to solve\ncomplex information needs. Different from the traditional single-turn ad-hoc\nsearch, conversational search encounters a more challenging problem of\ncontext-dependent query understanding with the lengthy and long-tail\nconversational history context. While conversational query rewriting methods\nleverage explicit rewritten queries to train a rewriting model to transform the\ncontext-dependent query into a stand-stone search query, this is usually done\nwithout considering the quality of search results. Conversational dense\nretrieval methods use fine-tuning to improve a pre-trained ad-hoc query\nencoder, but they are limited by the conversational search data available for\ntraining. In this paper, we leverage both rewritten queries and relevance\njudgments in the conversational search data to train a better query\nrepresentation model. The key idea is to align the query representation with\nthose of rewritten queries and relevant documents. The proposed model -- Query\nRepresentation Alignment Conversational Dense Retriever, QRACDR, is tested on\neight datasets, including various settings in conversational search and ad-hoc\nsearch. The results demonstrate the strong performance of QRACDR compared with\nstate-of-the-art methods, and confirm the effectiveness of representation\nalignment.",
        "pos": [
            "Personalized conversational information retrieval (CIR) combines\nconversational and personalizable elements to satisfy various users' complex\ninformation needs through multi-turn interaction based on their backgrounds.\nThe key promise is that the personal textual knowledge base (PTKB) can improve\nthe CIR effectiveness because the retrieval results can be more related to the\nuser's background. However, PTKB is noisy: not every piece of knowledge in PTKB\nis relevant to the specific query at hand. In this paper, we explore and test\nseveral ways to select knowledge from PTKB and use it for query reformulation\nby using a large language model (LLM). The experimental results show the PTKB\nmight not always improve the search results when used alone, but LLM can help\ngenerate a more appropriate personalized query when high-quality guidance is\nprovided.",
            "Document-level biomedical concept extraction is the task of identifying\nbiomedical concepts mentioned in a given document. Recent advancements have\nadapted pre-trained language models for this task. However, the scarcity of\ndomain-specific data and the deviation of concepts from their canonical names\noften hinder these models' effectiveness. To tackle this issue, we employ\nMetaMapLite, an existing rule-based concept mapping system, to generate\nadditional pseudo-annotated data from PubMed and PMC. The annotated data are\nused to augment the limited training data. Through extensive experiments, this\nstudy demonstrates the utility of a manually crafted concept mapping tool for\ntraining a better concept extraction model."
        ],
        "neg": []
    },
    {
        "query": "Information seeking and integration is a complex cognitive task that consumes\nenormous time and effort. Inspired by the remarkable progress of Large Language\nModels, recent works attempt to solve this task by combining LLMs and search\nengines. However, these methods still obtain unsatisfying performance due to\nthree challenges: (1) complex requests often cannot be accurately and\ncompletely retrieved by the search engine once (2) corresponding information to\nbe integrated is spread over multiple web pages along with massive noise, and\n(3) a large number of web pages with long contents may quickly exceed the\nmaximum context length of LLMs. Inspired by the cognitive process when humans\nsolve these problems, we introduce MindSearch to mimic the human minds in web\ninformation seeking and integration, which can be instantiated by a simple yet\neffective LLM-based multi-agent framework. The WebPlanner models the human mind\nof multi-step information seeking as a dynamic graph construction process: it\ndecomposes the user query into atomic sub-questions as nodes in the graph and\nprogressively extends the graph based on the search result from WebSearcher.\nTasked with each sub-question, WebSearcher performs hierarchical information\nretrieval with search engines and collects valuable information for WebPlanner.\nThe multi-agent design of MindSearch enables the whole framework to seek and\nintegrate information parallelly from larger-scale (e.g., more than 300) web\npages in 3 minutes, which is worth 3 hours of human effort. MindSearch\ndemonstrates significant improvement in the response quality in terms of depth\nand breadth, on both close-set and open-set QA problems. Besides, responses\nfrom MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web\nand Perplexity.ai applications, which implies that MindSearch can already\ndeliver a competitive solution to the proprietary AI search engine.",
        "pos": [
            "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization."
        ],
        "neg": []
    },
    {
        "query": "Information seeking and integration is a complex cognitive task that consumes\nenormous time and effort. Inspired by the remarkable progress of Large Language\nModels, recent works attempt to solve this task by combining LLMs and search\nengines. However, these methods still obtain unsatisfying performance due to\nthree challenges: (1) complex requests often cannot be accurately and\ncompletely retrieved by the search engine once (2) corresponding information to\nbe integrated is spread over multiple web pages along with massive noise, and\n(3) a large number of web pages with long contents may quickly exceed the\nmaximum context length of LLMs. Inspired by the cognitive process when humans\nsolve these problems, we introduce MindSearch to mimic the human minds in web\ninformation seeking and integration, which can be instantiated by a simple yet\neffective LLM-based multi-agent framework. The WebPlanner models the human mind\nof multi-step information seeking as a dynamic graph construction process: it\ndecomposes the user query into atomic sub-questions as nodes in the graph and\nprogressively extends the graph based on the search result from WebSearcher.\nTasked with each sub-question, WebSearcher performs hierarchical information\nretrieval with search engines and collects valuable information for WebPlanner.\nThe multi-agent design of MindSearch enables the whole framework to seek and\nintegrate information parallelly from larger-scale (e.g., more than 300) web\npages in 3 minutes, which is worth 3 hours of human effort. MindSearch\ndemonstrates significant improvement in the response quality in terms of depth\nand breadth, on both close-set and open-set QA problems. Besides, responses\nfrom MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web\nand Perplexity.ai applications, which implies that MindSearch can already\ndeliver a competitive solution to the proprietary AI search engine.",
        "pos": [
            "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.",
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer."
        ],
        "neg": []
    },
    {
        "query": "Information seeking and integration is a complex cognitive task that consumes\nenormous time and effort. Inspired by the remarkable progress of Large Language\nModels, recent works attempt to solve this task by combining LLMs and search\nengines. However, these methods still obtain unsatisfying performance due to\nthree challenges: (1) complex requests often cannot be accurately and\ncompletely retrieved by the search engine once (2) corresponding information to\nbe integrated is spread over multiple web pages along with massive noise, and\n(3) a large number of web pages with long contents may quickly exceed the\nmaximum context length of LLMs. Inspired by the cognitive process when humans\nsolve these problems, we introduce MindSearch to mimic the human minds in web\ninformation seeking and integration, which can be instantiated by a simple yet\neffective LLM-based multi-agent framework. The WebPlanner models the human mind\nof multi-step information seeking as a dynamic graph construction process: it\ndecomposes the user query into atomic sub-questions as nodes in the graph and\nprogressively extends the graph based on the search result from WebSearcher.\nTasked with each sub-question, WebSearcher performs hierarchical information\nretrieval with search engines and collects valuable information for WebPlanner.\nThe multi-agent design of MindSearch enables the whole framework to seek and\nintegrate information parallelly from larger-scale (e.g., more than 300) web\npages in 3 minutes, which is worth 3 hours of human effort. MindSearch\ndemonstrates significant improvement in the response quality in terms of depth\nand breadth, on both close-set and open-set QA problems. Besides, responses\nfrom MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web\nand Perplexity.ai applications, which implies that MindSearch can already\ndeliver a competitive solution to the proprietary AI search engine.",
        "pos": [
            "Recently, large language models have presented promising results in aiding\nformal mathematical reasoning. However, their performance is restricted due to\nthe scarcity of formal theorem-proving data, which requires additional effort\nto be extracted from raw formal language corpora. Meanwhile, a significant\namount of human-written formal language corpora remains underutilized. To\naddress this issue, we propose LEAN-GitHub, a dataset consisting of large-scale\nformal data extracted from almost all Lean 4 repositories on GitHub. After\nfine-tuning InternLM-math-plus on this dataset, our model achieved accuracies\nof 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F\ntest, surpassing state-of-the-art method at 52%. And it also achieves\nstate-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting\ndifferent fields/levels of math. These results demonstrate that our proposed\ndataset is beneficial for formal reasoning on a wide range of math topics. We\nopen-source our model at https://GitHub. com/InternLM/InternLM-Math and our\ndata at https://huggingface.co/ datasets/InternLM/Lean-GitHub",
            "We introduces LLaST, a framework for building high-performance Large Language\nmodel based Speech-to-text Translation systems. We address the limitations of\nend-to-end speech translation(E2E ST) models by exploring model architecture\ndesign and optimization techniques tailored for LLMs. Our approach includes\nLLM-based speech translation architecture design, ASR-augmented training,\nmultilingual data augmentation, and dual-LoRA optimization. Our approach\ndemonstrates superior performance on the CoVoST-2 benchmark and showcases\nexceptional scaling capabilities powered by LLMs. We believe this effective\nmethod will serve as a strong baseline for speech translation and provide\ninsights for future improvements of the LLM-based speech translation framework.\nWe release the data, code and models in https://github.com/openaudiolab/LLaST.",
            "In evaluating the long-context capabilities of large language models (LLMs),\nidentifying content relevant to a user's query from original long documents is\na crucial prerequisite for any LLM to answer questions based on long text. We\npresent NeedleBench, a framework consisting of a series of progressively more\nchallenging tasks for assessing bilingual long-context capabilities, spanning\nmultiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and\ndifferent depth ranges, allowing the strategic insertion of critical data\npoints in different text depth zones to rigorously test the retrieval and\nreasoning capabilities of models in diverse contexts. We use the NeedleBench\nframework to assess how well the leading open-source models can identify key\ninformation relevant to the question and apply that information to reasoning in\nbilingual long texts. Furthermore, we propose the Ancestral Trace Challenge\n(ATC) to mimic the complexity of logical reasoning challenges that are likely\nto be present in real-world long-context tasks, providing a simple method for\nevaluating LLMs in dealing with complex long-context situations. Our results\nsuggest that current LLMs have significant room for improvement in practical\nlong-context applications, as they struggle with the complexity of logical\nreasoning challenges that are likely to be present in real-world long-context\ntasks. All codes and resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass.",
            "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.",
            "Recently, speech generation models have made significant progress by using\nlarge-scale training data. However, the research community struggle to produce\nhighly spontaneous and human-like speech due to the lack of large-scale,\ndiverse, and spontaneous speech data. This paper present Emilia, the first\nmultilingual speech generation dataset from in-the-wild speech data, and\nEmilia-Pipe, the first open-source preprocessing pipeline designed to transform\nin-the-wild speech data into high-quality training data with annotations for\nspeech generation. Emilia starts with over 101k hours of speech in six\nlanguages and features diverse speech with varied speaking styles. To\nfacilitate the scale-up of Emilia, the open-source pipeline Emilia-Pipe can\nprocess one hour of raw speech data ready for model training in a few mins,\nwhich enables the research community to collaborate on large-scale speech\ngeneration research. Experimental results validate the effectiveness of Emilia.\nDemos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.",
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer."
        ],
        "neg": []
    },
    {
        "query": "To ensure performance on a diverse set of downstream tasks, LLMs are\npretrained via data mixtures over different domains. In this work, we\ndemonstrate that the optimal data composition for a fixed compute budget varies\ndepending on the scale of the training data, suggesting that the common\npractice of empirically determining an optimal composition using small-scale\nexperiments will not yield the optimal data mixtures when scaling up to the\nfinal model. To address this challenge, we propose *AutoScale*, an automated\ntool that finds a compute-optimal data composition for training at any desired\ntarget scale. AutoScale first determines the optimal composition at a small\nscale using a novel bilevel optimization framework, Direct Data Optimization\n(*DDO*), and then fits a predictor to estimate the optimal composition at\nlarger scales. The predictor's design is inspired by our theoretical analysis\nof scaling laws related to data composition, which could be of independent\ninterest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2\nLarge) on RedPajama dataset, AutoScale decreases validation perplexity at least\n25% faster than any baseline with up to 38% speed up compared to without\nreweighting, achieving the best overall performance across downstream tasks. On\npre-training Encoder-only LMs (BERT) with masked language modeling, DDO is\nshown to decrease loss on all domains while visibly improving average task\nperformance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by\n5.9% compared with without reweighting. AutoScale speeds up training by up to\n28%. Our codes are open-sourced.",
        "pos": [
            "Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future work, centered\naround whether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, while still providing opportunities to optimize\nabstention abilities based on context."
        ],
        "neg": []
    },
    {
        "query": "Word-level AutoCompletion(WLAC) is a rewarding yet challenging task in\nComputer-aided Translation. Existing work addresses this task through a\nclassification model based on a neural network that maps the hidden vector of\nthe input context into its corresponding label (i.e., the candidate target word\nis treated as a label). Since the context hidden vector itself does not take\nthe label into account and it is projected to the label through a linear\nclassifier, the model can not sufficiently leverage valuable information from\nthe source sentence as verified in our experiments, which eventually hinders\nits overall performance. To alleviate this issue, this work proposes an\nenergy-based model for WLAC, which enables the context hidden vector to capture\ncrucial information from the source sentence. Unfortunately, training and\ninference suffer from efficiency and effectiveness challenges, thereby we\nemploy three simple yet effective strategies to put our model into practice.\nExperiments on four standard benchmarks demonstrate that our reranking-based\napproach achieves substantial improvements (about 6.07%) over the previous\nstate-of-the-art model. Further analyses show that each strategy of our\napproach contributes to the final performance.",
        "pos": [
            "Targeted adversarial attacks are widely used to evaluate the robustness of\nneural machine translation systems. Unfortunately, this paper first identifies\na critical issue in the existing settings of NMT targeted adversarial attacks,\nwhere their attacking results are largely overestimated. To this end, this\npaper presents a new setting for NMT targeted adversarial attacks that could\nlead to reliable attacking results. Under the new setting, it then proposes a\nTargeted Word Gradient adversarial Attack (TWGA) method to craft adversarial\nexamples. Experimental results demonstrate that our proposed setting could\nprovide faithful attacking results for targeted adversarial attacks on NMT\nsystems, and the proposed TWGA method can effectively attack such victim NMT\nsystems. In-depth analyses on a large-scale dataset further illustrate some\nvaluable findings. 1 Our code and data are available at\nhttps://github.com/wujunjie1998/TWGA."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) with billions of parameters are known for their\nimpressive predicting capabilities but require lots of resources to run. With\ntheir massive rise in popularity, even a small reduction in required resources\ncould have an impact on environment. On the other hand, smaller models require\nfewer resources but may sacrifice accuracy. In this work, we are proposing an\nimplementation of ``stairs'' assisted greedy generation. It is a modified\nassisted generation methodology that makes use of a smaller model's fast\ngeneration, large model's batch prediction, and \"stairs\" validation in order to\nachieve a speed up in prediction generation. Results show between 9.58 and\n17.24 percent inference time reduction compared to a stand-alone large LLM\nprediction in a text generation task without a loss in accuracy.",
        "pos": [
            "Sentiment analysis is a widely researched area within Natural Language\nProcessing (NLP), attracting significant interest due to the advent of\nautomated solutions. Despite this, the task remains challenging because of the\ninherent complexity of languages and the subjective nature of sentiments. It is\neven more challenging for less-studied and less-resourced languages such as\nLithuanian. Our review of existing Lithuanian NLP research reveals that\ntraditional machine learning methods and classification algorithms have limited\neffectiveness for the task. In this work, we address sentiment analysis of\nLithuanian five-star-based online reviews from multiple domains that we collect\nand clean. We apply transformer models to this task for the first time,\nexploring the capabilities of pre-trained multilingual Large Language Models\n(LLMs), specifically focusing on fine-tuning BERT and T5 models. Given the\ninherent difficulty of the task, the fine-tuned models perform quite well,\nespecially when the sentiments themselves are less ambiguous: 80.74% and 89.61%\ntesting recognition accuracy of the most popular one- and five-star reviews\nrespectively. They significantly outperform current commercial state-of-the-art\ngeneral-purpose LLM GPT-4. We openly share our fine-tuned LLMs online."
        ],
        "neg": []
    },
    {
        "query": "This is the preliminary ranking of WMT24 General MT systems based on\nautomatic metrics. The official ranking will be a human evaluation, which is\nsuperior to the automatic ranking and supersedes it. The purpose of this report\nis not to interpret any findings but only provide preliminary results to the\nparticipants of the General MT task that may be useful during the writing of\nthe system submission.",
        "pos": [
            "Following the rapid progress in natural language processing (NLP) models,\nlanguage models are applied to increasingly more complex interactive tasks such\nas negotiations and conversation moderations. Having human evaluators directly\ninteract with these NLP models is essential for adequately evaluating the\nperformance on such interactive tasks. We develop BotEval, an easily\ncustomizable, open-source, evaluation toolkit that focuses on enabling\nhuman-bot interactions as part of the evaluation process, as opposed to human\nevaluators making judgements for a static input. BotEval balances flexibility\nfor customization and user-friendliness by providing templates for common use\ncases that span various degrees of complexity and built-in compatibility with\npopular crowdsourcing platforms. We showcase the numerous useful features of\nBotEval through a study that evaluates the performance of various chatbots on\ntheir effectiveness for conversational moderation and discuss how BotEval\ndiffers from other annotation tools."
        ],
        "neg": []
    },
    {
        "query": "This is the preliminary ranking of WMT24 General MT systems based on\nautomatic metrics. The official ranking will be a human evaluation, which is\nsuperior to the automatic ranking and supersedes it. The purpose of this report\nis not to interpret any findings but only provide preliminary results to the\nparticipants of the General MT task that may be useful during the writing of\nthe system submission.",
        "pos": [
            "Using questions in written text is an effective strategy to enhance\nreadability. However, what makes an active reading question good, what the\nlinguistic role of these questions is, and what is their impact on human\nreading remains understudied. We introduce GuidingQ, a dataset of 10K in-text\nquestions from textbooks and scientific articles. By analyzing the dataset, we\npresent a comprehensive understanding of the use, distribution, and linguistic\ncharacteristics of these questions. Then, we explore various approaches to\ngenerate such questions using language models. Our results highlight the\nimportance of capturing inter-question relationships and the challenge of\nquestion position identification in generating these questions. Finally, we\nconduct a human study to understand the implication of such questions on\nreading comprehension. We find that the generated questions are of high quality\nand are almost as effective as human-written questions in terms of improving\nreaders' memorization and comprehension."
        ],
        "neg": []
    },
    {
        "query": "The Retrieval-Augmented Language Model (RALM) has shown remarkable\nperformance on knowledge-intensive tasks by incorporating external knowledge\nduring inference, which mitigates the factual hallucinations inherited in large\nlanguage models (LLMs). Despite these advancements, challenges persist in the\nimplementation of RALMs, particularly concerning their reliability and\ntraceability. To be specific, the irrelevant document retrieval may result in\nunhelpful response generation or even deteriorate the performance of LLMs,\nwhile the lack of proper citations in generated outputs complicates efforts to\nverify the trustworthiness of the models. To this end, we propose a novel\nself-reasoning framework aimed at improving the reliability and traceability of\nRALMs, whose core idea is to leverage reasoning trajectories generated by the\nLLM itself. The framework involves constructing self-reason trajectories with\nthree processes: a relevance-aware process, an evidence-aware selective\nprocess, and a trajectory analysis process. We have evaluated our framework\nacross four public datasets (two short-form QA datasets, one long-form QA\ndataset, and one fact verification dataset) to demonstrate the superiority of\nour method, which can outperform existing state-of-art models and can achieve\ncomparable performance with GPT-4, while only using 2,000 training samples.",
        "pos": [
            "Recent advancements in artificial intelligence (AI) have precipitated\nsignificant breakthroughs in healthcare, particularly in refining diagnostic\nprocedures. However, previous studies have often been constrained to limited\nfunctionalities. This study introduces MiniGPT-Med, a vision-language model\nderived from large-scale language models and tailored for medical applications.\nMiniGPT-Med demonstrates remarkable versatility across various imaging\nmodalities, including X-rays, CT scans, and MRIs, enhancing its utility. The\nmodel is capable of performing tasks such as medical report generation, visual\nquestion answering (VQA), and disease identification within medical imagery.\nIts integrated processing of both image and textual clinical data markedly\nimproves diagnostic accuracy. Our empirical assessments confirm MiniGPT-Med's\nsuperior performance in disease grounding, medical report generation, and VQA\nbenchmarks, representing a significant step towards reducing the gap in\nassisting radiology practice. Furthermore, it achieves state-of-the-art\nperformance on medical report generation, higher than the previous best model\nby 19\\% accuracy. MiniGPT-Med promises to become a general interface for\nradiology diagnoses, enhancing diagnostic efficiency across a wide range of\nmedical imaging applications."
        ],
        "neg": []
    },
    {
        "query": "We focus on the problem of fusing two or more heterogeneous large language\nmodels (LLMs) to facilitate their complementary strengths. One of the\nchallenges on model fusion is high computational load, i.e. to fine-tune or to\nalign vocabularies via combinatorial optimization. To this end, we propose\n\\emph{Cool-Fusion}, a simple yet effective approach that fuses the knowledge of\nheterogeneous source LLMs to leverage their complementary strengths.\n\\emph{Cool-Fusion} is the first method that does not require any type of\ntraining like the ensemble approaches. But unlike ensemble methods, it is\napplicable to any set of source LLMs that have different vocabularies. The\nbasic idea is to have each source LLM individually generate tokens until the\ntokens can be decoded into a text segment that ends at word boundaries common\nto all source LLMs. Then, the source LLMs jointly rerank the generated text\nsegment and select the best one, which is the fused text generation in one\nstep. Extensive experiments are conducted across a variety of benchmark\ndatasets. On \\emph{GSM8K}, \\emph{Cool-Fusion} increases accuracy from three\nstrong source LLMs by a significant 8\\%-17.8\\%.",
        "pos": [
            "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE."
        ],
        "neg": []
    },
    {
        "query": "This paper presents teaching materials, particularly assignments and ideas\nfor classroom activities, from a new course on large language models (LLMs)\ntaught at Charles University. The assignments include experiments with LLM\ninference for weather report generation and machine translation. The classroom\nactivities include class quizzes, focused research on downstream tasks and\ndatasets, and an interactive \"best paper\" session aimed at reading and\ncomprehension of research papers.",
        "pos": [
            "We present factgenie: a framework for annotating and visualizing word spans\nin textual model outputs. Annotations can capture various span-based phenomena\nsuch as semantic inaccuracies or irrelevant text. With factgenie, the\nannotations can be collected both from human crowdworkers and large language\nmodels. Our framework consists of a web interface for data visualization and\ngathering text annotations, powered by an easily extensible codebase."
        ],
        "neg": []
    },
    {
        "query": "This paper presents teaching materials, particularly assignments and ideas\nfor classroom activities, from a new course on large language models (LLMs)\ntaught at Charles University. The assignments include experiments with LLM\ninference for weather report generation and machine translation. The classroom\nactivities include class quizzes, focused research on downstream tasks and\ndatasets, and an interactive \"best paper\" session aimed at reading and\ncomprehension of research papers.",
        "pos": [
            "We present factgenie: a framework for annotating and visualizing word spans\nin textual model outputs. Annotations can capture various span-based phenomena\nsuch as semantic inaccuracies or irrelevant text. With factgenie, the\nannotations can be collected both from human crowdworkers and large language\nmodels. Our framework consists of a web interface for data visualization and\ngathering text annotations, powered by an easily extensible codebase."
        ],
        "neg": []
    },
    {
        "query": "This paper presents teaching materials, particularly assignments and ideas\nfor classroom activities, from a new course on large language models (LLMs)\ntaught at Charles University. The assignments include experiments with LLM\ninference for weather report generation and machine translation. The classroom\nactivities include class quizzes, focused research on downstream tasks and\ndatasets, and an interactive \"best paper\" session aimed at reading and\ncomprehension of research papers.",
        "pos": [
            "In multilingual settings, non-Latin scripts and low-resource languages are\nusually disadvantaged in terms of language models' utility, efficiency, and\ncost. Specifically, previous studies have reported multiple modeling biases\nthat the current tokenization algorithms introduce to non-Latin script\nlanguages, the main one being over-segmentation. In this work, we propose\nMAGNET; multilingual adaptive gradient-based tokenization to reduce\nover-segmentation via adaptive gradient-based subword tokenization. MAGNET\nlearns to predict segment boundaries between byte tokens in a sequence via\nsub-modules within the model, which act as internal boundary predictors\n(tokenizers). Previous gradient-based tokenization methods aimed for uniform\ncompression across sequences by integrating a single boundary predictor during\ntraining and optimizing it end-to-end through stochastic reparameterization\nalongside the next token prediction objective. However, this approach still\nresults in over-segmentation for non-Latin script languages in multilingual\nsettings. In contrast, MAGNET offers a customizable architecture where\nbyte-level sequences are routed through language-script-specific predictors,\neach optimized for its respective language script. This modularity enforces\nequitable segmentation granularity across different language scripts compared\nto previous methods. Through extensive experiments, we demonstrate that in\naddition to reducing segmentation disparities, MAGNET also enables faster\nlanguage modelling and improves downstream utility."
        ],
        "neg": []
    },
    {
        "query": "Large language models are able to generate code for visualisations in\nresponse to user requests. This is a useful application, and an appealing one\nfor NLP research because plots of data provide grounding for language. However,\nthere are relatively few benchmarks, and it is unknown whether those that exist\nare representative of what people do in practice. This paper aims to answer\nthat question through an empirical study comparing benchmark datasets and code\nfrom public repositories. Our findings reveal a substantial gap in datasets,\nwith evaluations not testing the same distribution of chart types, attributes,\nand the number of actions. The only representative dataset requires\nmodification to become an end-to-end and practical benchmark. This shows that\nnew, more benchmarks are needed to support the development of systems that\ntruly address users' visualisation needs. These observations will guide future\ndata creation, highlighting which features hold genuine significance for users.",
        "pos": [
            "The significance of mental health classification is paramount in contemporary\nsociety, where digital platforms serve as crucial sources for monitoring\nindividuals' well-being. However, existing social media mental health datasets\nprimarily consist of text-only samples, potentially limiting the efficacy of\nmodels trained on such data. Recognising that humans utilise cross-modal\ninformation to comprehend complex situations or issues, we present a novel\napproach to address the limitations of current methodologies. In this work, we\nintroduce a Multimodal and Multi-Teacher Knowledge Distillation model for\nMental Health Classification, leveraging insights from cross-modal human\nunderstanding. Unlike conventional approaches that often rely on simple\nconcatenation to integrate diverse features, our model addresses the challenge\nof appropriately representing inputs of varying natures (e.g., texts and\nsounds). To mitigate the computational complexity associated with integrating\nall features into a single model, we employ a multimodal and multi-teacher\narchitecture. By distributing the learning process across multiple teachers,\neach specialising in a particular feature extraction aspect, we enhance the\noverall mental health classification performance. Through experimental\nvalidation, we demonstrate the efficacy of our model in achieving improved\nperformance."
        ],
        "neg": []
    },
    {
        "query": "The rapid progress in Large Language Models (LLMs) has prompted the creation\nof numerous benchmarks to evaluate their capabilities.This study focuses on the\nComprehensive Medical Benchmark in Chinese (CMB), showcasing how dataset\ndiversity and distribution in supervised fine-tuning (SFT) may enhance LLM\nperformance.Remarkably, We successfully trained a smaller base model to achieve\nscores comparable to larger models, indicating that a diverse and\nwell-distributed dataset can optimize performance regardless of model size.This\nstudy suggests that even smaller models may reach high performance levels with\ncarefully curated and varied datasets. By integrating a wide range of\ninstructional content, our approach addresses potential issues such as data\nquality inconsistencies. Our results imply that a broader spectrum of training\ndata may enhance a model's ability to generalize and perform effectively across\ndifferent medical scenarios, highlighting the importance of dataset quality and\ndiversity in fine-tuning processes. We open-source the model for future\nresearch at https://github.com/CAS-SIAT-XinHai/CollectiveSFT",
        "pos": [
            "Empathetic response generation is designed to comprehend the emotions of\nothers and select the most appropriate strategies to assist them in resolving\nemotional challenges. Empathy can be categorized into cognitive empathy and\naffective empathy. The former pertains to the ability to understand and discern\nthe emotional issues and situations of others, while the latter involves the\ncapacity to provide comfort. To enhance one's empathetic abilities, it is\nessential to develop both these aspects. Therefore, we develop an innovative\nframework that combines retrieval augmentation and emotional support strategy\nintegration. Our framework starts with the introduction of a comprehensive\nemotional palette for empathy. We then apply appraisal theory to decompose this\npalette and create a database of empathetic responses. This database serves as\nan external resource and enhances the LLM's empathy by integrating semantic\nretrieval mechanisms. Moreover, our framework places a strong emphasis on the\nproper articulation of response strategies. By incorporating emotional support\nstrategies, we aim to enrich the model's capabilities in both cognitive and\naffective empathy, leading to a more nuanced and comprehensive empathetic\nresponse. Finally, we extract datasets ED and ET from the empathetic dialogue\ndataset \\textsc{EmpatheticDialogues} and ExTES based on dialogue length.\nExperiments demonstrate that our framework can enhance the empathy ability of\nLLMs from both cognitive and affective empathy perspectives. Our code is\nreleased at https://github.com/CAS-SIAT-XinHai/APTNESS."
        ],
        "neg": []
    },
    {
        "query": "We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.",
        "pos": [
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.",
        "pos": [
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.",
        "pos": [
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
        ],
        "neg": []
    },
    {
        "query": "We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.",
        "pos": [
            "Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.",
            "In-image machine translation (IIMT) aims to translate an image containing\ntexts in source language into an image containing translations in target\nlanguage. In this regard, conventional cascaded methods suffer from issues such\nas error propagation, massive parameters, and difficulties in deployment and\nretaining visual characteristics of the input image. Thus, constructing\nend-to-end models has become an option, which, however, faces two main\nchallenges: 1) the huge modeling burden, as it is required to simultaneously\nlearn alignment across languages and preserve the visual characteristics of the\ninput image; 2) the difficulties of directly predicting excessively lengthy\npixel sequences. In this paper, we propose \\textit{Translatotron-V(ision)}, an\nend-to-end IIMT model consisting of four modules. In addition to an image\nencoder, and an image decoder, our model contains a target text decoder and an\nimage tokenizer. Among them, the target text decoder is used to alleviate the\nlanguage alignment burden, and the image tokenizer converts long sequences of\npixels into shorter sequences of visual tokens, preventing the model from\nfocusing on low-level visual features. Besides, we present a two-stage training\nframework for our model to assist the model in learning alignment across\nmodalities and languages. Finally, we propose a location-aware evaluation\nmetric called Structure-BLEU to assess the translation quality of the generated\nimages. Experimental results demonstrate that our model achieves competitive\nperformance compared to cascaded models with only 70.9\\% of parameters, and\nsignificantly outperforms the pixel-level end-to-end IIMT model.",
            "Through reading the documentation in the context, tool-using language models\ncan dynamically extend their capability using external tools. The cost is that\nwe have to input lengthy documentation every time the model needs to use the\ntool, occupying the input window as well as slowing down the decoding process.\n  Given the progress in general-purpose compression, soft context compression\nis a suitable approach to alleviate the problem. However, when compressing tool\ndocumentation, existing methods suffer from the weaknesses of key information\nloss (specifically, tool/parameter name errors) and difficulty in adjusting the\nlength of compressed sequences based on documentation lengths.\n  To address these problems, we propose two strategies for compressing tool\ndocumentation into concise and precise summary sequences for tool-using\nlanguage models. 1) Selective compression strategy mitigates key information\nloss by deliberately retaining key information as raw text tokens. 2) Block\ncompression strategy involves dividing tool documentation into short chunks and\nthen employing a fixed-length compression model to achieve variable-length\ncompression. This strategy facilitates the flexible adjustment of the\ncompression ratio.\n  Results on API-Bank and APIBench show that our approach reaches a performance\ncomparable to the upper-bound baseline under up to 16x compression ratio."
        ],
        "neg": []
    },
    {
        "query": "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween two multi-modal knowledge graphs (MMKGs), whose entities can be\nassociated with relational triples and related images. Most previous studies\ntreat the graph structure as a special modality, and fuse different modality\ninformation with separate uni-modal encoders, neglecting valuable relational\nassociations in modalities. Other studies refine each uni-modal information\nwith graph structures, but may introduce unnecessary relations in specific\nmodalities. To this end, we propose a novel local-to-global interaction network\nfor MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal\ninteractions to generate holistic entity semantics and then refine them with\nglobal relational interactions of entity neighbors. In this design, the\nuni-modal information is fused adaptively, and can be refined with relations\naccordingly. To enrich local interactions of multi-modal entity information, we\ndevice modality weights and low-rank interactive fusion, allowing diverse\nimpacts and element-level interactions among modalities. To capture global\ninteractions of graph structures, we adopt relation reflection graph attention\nnetworks, which fully capture relational associations between entities.\nExtensive experiments demonstrate superior results of our method over 5\ncross-KG or bilingual benchmark datasets, indicating the effectiveness of\ncapturing local and global interactions.",
        "pos": [
            "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween multi-modal knowledge graphs (MMKGs), where the entities can be\nassociated with related images. Most existing studies integrate multi-modal\ninformation heavily relying on the automatically-learned fusion module, rarely\nsuppressing the redundant information for MMEA explicitly. To this end, we\nexplore variational information bottleneck for multi-modal entity alignment\n(IBMEA), which emphasizes the alignment-relevant information and suppresses the\nalignment-irrelevant information in generating entity representations.\nSpecifically, we devise multi-modal variational encoders to generate\nmodal-specific entity representations as probability distributions. Then, we\npropose four modal-specific information bottleneck regularizers, limiting the\nmisleading clues in refining modal-specific entity representations. Finally, we\npropose a modal-hybrid information contrastive regularizer to integrate all the\nrefined modal-specific representations, enhancing the entity similarity between\nMMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and\nthree bilingual MMEA datasets. Experimental results demonstrate that our model\nconsistently outperforms previous state-of-the-art methods, and also shows\npromising and robust performance in low-resource and high-noise data scenarios."
        ],
        "neg": []
    },
    {
        "query": "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween two multi-modal knowledge graphs (MMKGs), whose entities can be\nassociated with relational triples and related images. Most previous studies\ntreat the graph structure as a special modality, and fuse different modality\ninformation with separate uni-modal encoders, neglecting valuable relational\nassociations in modalities. Other studies refine each uni-modal information\nwith graph structures, but may introduce unnecessary relations in specific\nmodalities. To this end, we propose a novel local-to-global interaction network\nfor MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal\ninteractions to generate holistic entity semantics and then refine them with\nglobal relational interactions of entity neighbors. In this design, the\nuni-modal information is fused adaptively, and can be refined with relations\naccordingly. To enrich local interactions of multi-modal entity information, we\ndevice modality weights and low-rank interactive fusion, allowing diverse\nimpacts and element-level interactions among modalities. To capture global\ninteractions of graph structures, we adopt relation reflection graph attention\nnetworks, which fully capture relational associations between entities.\nExtensive experiments demonstrate superior results of our method over 5\ncross-KG or bilingual benchmark datasets, indicating the effectiveness of\ncapturing local and global interactions.",
        "pos": [
            "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween multi-modal knowledge graphs (MMKGs), where the entities can be\nassociated with related images. Most existing studies integrate multi-modal\ninformation heavily relying on the automatically-learned fusion module, rarely\nsuppressing the redundant information for MMEA explicitly. To this end, we\nexplore variational information bottleneck for multi-modal entity alignment\n(IBMEA), which emphasizes the alignment-relevant information and suppresses the\nalignment-irrelevant information in generating entity representations.\nSpecifically, we devise multi-modal variational encoders to generate\nmodal-specific entity representations as probability distributions. Then, we\npropose four modal-specific information bottleneck regularizers, limiting the\nmisleading clues in refining modal-specific entity representations. Finally, we\npropose a modal-hybrid information contrastive regularizer to integrate all the\nrefined modal-specific representations, enhancing the entity similarity between\nMMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and\nthree bilingual MMEA datasets. Experimental results demonstrate that our model\nconsistently outperforms previous state-of-the-art methods, and also shows\npromising and robust performance in low-resource and high-noise data scenarios."
        ],
        "neg": []
    },
    {
        "query": "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween two multi-modal knowledge graphs (MMKGs), whose entities can be\nassociated with relational triples and related images. Most previous studies\ntreat the graph structure as a special modality, and fuse different modality\ninformation with separate uni-modal encoders, neglecting valuable relational\nassociations in modalities. Other studies refine each uni-modal information\nwith graph structures, but may introduce unnecessary relations in specific\nmodalities. To this end, we propose a novel local-to-global interaction network\nfor MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal\ninteractions to generate holistic entity semantics and then refine them with\nglobal relational interactions of entity neighbors. In this design, the\nuni-modal information is fused adaptively, and can be refined with relations\naccordingly. To enrich local interactions of multi-modal entity information, we\ndevice modality weights and low-rank interactive fusion, allowing diverse\nimpacts and element-level interactions among modalities. To capture global\ninteractions of graph structures, we adopt relation reflection graph attention\nnetworks, which fully capture relational associations between entities.\nExtensive experiments demonstrate superior results of our method over 5\ncross-KG or bilingual benchmark datasets, indicating the effectiveness of\ncapturing local and global interactions.",
        "pos": [
            "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween multi-modal knowledge graphs (MMKGs), where the entities can be\nassociated with related images. Most existing studies integrate multi-modal\ninformation heavily relying on the automatically-learned fusion module, rarely\nsuppressing the redundant information for MMEA explicitly. To this end, we\nexplore variational information bottleneck for multi-modal entity alignment\n(IBMEA), which emphasizes the alignment-relevant information and suppresses the\nalignment-irrelevant information in generating entity representations.\nSpecifically, we devise multi-modal variational encoders to generate\nmodal-specific entity representations as probability distributions. Then, we\npropose four modal-specific information bottleneck regularizers, limiting the\nmisleading clues in refining modal-specific entity representations. Finally, we\npropose a modal-hybrid information contrastive regularizer to integrate all the\nrefined modal-specific representations, enhancing the entity similarity between\nMMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and\nthree bilingual MMEA datasets. Experimental results demonstrate that our model\nconsistently outperforms previous state-of-the-art methods, and also shows\npromising and robust performance in low-resource and high-noise data scenarios."
        ],
        "neg": []
    },
    {
        "query": "In this paper, I apply linguistic methods of analysis to non-linguistic data,\nchess plays, metaphorically equating one with the other and seeking analogies.\nChess game notations are also a kind of text, and one can consider the records\nof moves or positions of pieces as words and statements in a certain language.\nIn this article I show how word embeddings (word2vec) can work on chess game\ntexts instead of natural language texts. I don't see how this representation of\nchess data can be used productively. It's unlikely that these vector models\nwill help engines or people choose the best move. But in a purely academic\nsense, it's clear that such methods of information representation capture\nsomething important about the very nature of the game, which doesn't\nnecessarily lead to a win.",
        "pos": [
            "The stylo package includes a frequency table that can be used to calculate\ndistances between texts and thus independently solve the problem of attribution\nof The Cuckoo's Calling, a novel that J.K. Rowling said she wrote. However, the\nset of texts for this table is very vulnerable to criticism. The authors there\nare not modern, they wrote in a different genre. I set out to test the\nperformance of the method on texts that are more relevant to the research\nquestion.",
            "Burrows' Delta was introduced in 2002 and has proven to be an effective tool\nfor author attribution. Despite the fact that these are different languages,\nthey mostly belong to the same grammatical type and use the same graphic\nprinciple to convey speech in writing: a phonemic alphabet with word separation\nusing spaces. The question I want to address in this article is how well this\nattribution method works with texts in a language with a different grammatical\nstructure and a script based on different principles. There are fewer studies\nanalyzing the effectiveness of the Delta method on Chinese texts than on texts\nin European languages. I believe that such a low level of attention to Delta\nfrom sinologists is due to the structure of the scientific field dedicated to\nmedieval Chinese poetry. Clustering based on intertextual distances worked\nflawlessly. Delta produced results where clustering showed that the samples of\none author were most similar to each other, and Delta never confused different\npoets. Despite the fact that I used an unconventional approach and applied the\nDelta method to a language poorly suited for it, the method demonstrated its\neffectiveness. Tang dynasty poets are correctly identified using Delta, and the\nempirical pattern observed for authors writing in European standard languages\nhas been confirmed once again."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many\ndomains. While improving these models traditionally relies on costly human\ndata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs\ncan improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation\nduring iterative training. To address this issue, we introduce a novel\nMeta-Rewarding step to the self-improvement process, where the model judges its\nown judgements and uses that feedback to refine its judgment skills.\nSurprisingly, this unsupervised approach improves the model's ability to judge\n{\\em and} follow instructions, as demonstrated by a win rate improvement of\nLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on\nArena-Hard. These results strongly suggest the potential for self-improving\nmodels without human supervision.",
        "pos": [
            "Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well."
        ],
        "neg": []
    },
    {
        "query": "Training with larger mini-batches improves the performance and convergence\nrate of training machine learning models. However, training with large\nmini-batches becomes prohibitive for Large Language Models (LLMs) with billions\nof parameters, due to the large GPU memory requirement. To address this\nproblem, we propose finding small mini-batches that simulate the dynamics of\ntraining with larger mini-batches. Specifically, we formulate selecting smaller\nmini-batches of examples that closely capture gradients of large mini-batches\nas a submodular maximization problem. Nevertheless, the very large\ndimensionality of the gradients makes the problem very challenging to solve. To\naddress this, we leverage ideas from zeroth-order optimization and neural\nnetwork pruning to find lower-dimensional gradient estimates that allow finding\nhigh-quality subsets effectively with a limited amount of memory. We prove the\nsuperior convergence rate of training on the small mini-batches found by our\nmethod and empirically show its effectiveness. Our method can effectively\nreduce the memory requirement by 2x and speed up training by 1.3x, as we\nconfirm for fine-tuning Phi-2 on MathInstruct. Our method can be easily stacked\nwith LoRA and other memory-efficient methods to further reduce the memory\nrequirements of training LLMs.",
        "pos": [
            "GPT-4V's purported strong multimodal abilities raise interests in using it to\nautomate radiology report writing, but there lacks thorough evaluations. In\nthis work, we perform a systematic evaluation of GPT-4V in generating radiology\nreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt\nto directly generate reports using GPT-4V through different prompting\nstrategies and find that it fails terribly in both lexical metrics and clinical\nefficacy metrics. To understand the low performance, we decompose the task into\ntwo steps: 1) the medical image reasoning step of predicting medical condition\nlabels from images; and 2) the report synthesis step of generating reports from\n(groundtruth) conditions. We show that GPT-4V's performance in image reasoning\nis consistently low across different prompts. In fact, the distributions of\nmodel-predicted labels remain constant regardless of which groundtruth\nconditions are present on the image, suggesting that the model is not\ninterpreting chest X-rays meaningfully. Even when given groundtruth conditions\nin report synthesis, its generated reports are less correct and less\nnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt\non the viability of using GPT-4V in a radiology workflow."
        ],
        "neg": []
    },
    {
        "query": "To be included into chatbot systems, Large language models (LLMs) must be\naligned with human conversational conventions. However, being trained mainly on\nweb-scraped data gives existing LLMs a voice closer to informational text than\nactual human speech. In this paper, we examine the effect of decoding methods\non the alignment between LLM-generated and human conversations, including Beam\nSearch, Top K Sampling, and Nucleus Sampling. We present new measures of\nalignment in substance, style, and psychometric orientation, and experiment\nwith two conversation datasets. Our results provide subtle insights: better\nalignment is attributed to fewer beams in Beam Search and lower values of P in\nNucleus Sampling. We also find that task-oriented and open-ended datasets\nperform differently in terms of alignment, indicating the significance of\ntaking into account the context of the interaction.",
        "pos": [
            "For the WASSA 2024 Empathy and Personality Prediction Shared Task, we propose\na novel turn-level empathy detection method that decomposes empathy into six\npsychological indicators: Emotional Language, Perspective-Taking, Sympathy and\nCompassion, Extroversion, Openness, and Agreeableness. A pipeline of text\nenrichment using a Large Language Model (LLM) followed by DeBERTA fine-tuning\ndemonstrates a significant improvement in the Pearson Correlation Coefficient\nand F1 scores for empathy detection, highlighting the effectiveness of our\napproach. Our system officially ranked 7th at the CONV-turn track.",
            "Supervised machine-learning models for predicting user behavior offer a\nchallenging classification problem with lower average prediction performance\nscores than other text classification tasks. This study evaluates multi-task\nlearning frameworks grounded in Cognitive Appraisal Theory to predict user\nbehavior as a function of users' self-expression and psychological attributes.\nOur experiments show that users' language and traits improve predictions above\nand beyond models predicting only from text. Our findings highlight the\nimportance of integrating psychological constructs into NLP to enhance the\nunderstanding and prediction of user actions. We close with a discussion of the\nimplications for future applications of large language models for computational\npsychology."
        ],
        "neg": []
    },
    {
        "query": "To be included into chatbot systems, Large language models (LLMs) must be\naligned with human conversational conventions. However, being trained mainly on\nweb-scraped data gives existing LLMs a voice closer to informational text than\nactual human speech. In this paper, we examine the effect of decoding methods\non the alignment between LLM-generated and human conversations, including Beam\nSearch, Top K Sampling, and Nucleus Sampling. We present new measures of\nalignment in substance, style, and psychometric orientation, and experiment\nwith two conversation datasets. Our results provide subtle insights: better\nalignment is attributed to fewer beams in Beam Search and lower values of P in\nNucleus Sampling. We also find that task-oriented and open-ended datasets\nperform differently in terms of alignment, indicating the significance of\ntaking into account the context of the interaction.",
        "pos": [
            "For the WASSA 2024 Empathy and Personality Prediction Shared Task, we propose\na novel turn-level empathy detection method that decomposes empathy into six\npsychological indicators: Emotional Language, Perspective-Taking, Sympathy and\nCompassion, Extroversion, Openness, and Agreeableness. A pipeline of text\nenrichment using a Large Language Model (LLM) followed by DeBERTA fine-tuning\ndemonstrates a significant improvement in the Pearson Correlation Coefficient\nand F1 scores for empathy detection, highlighting the effectiveness of our\napproach. Our system officially ranked 7th at the CONV-turn track.",
            "Supervised machine-learning models for predicting user behavior offer a\nchallenging classification problem with lower average prediction performance\nscores than other text classification tasks. This study evaluates multi-task\nlearning frameworks grounded in Cognitive Appraisal Theory to predict user\nbehavior as a function of users' self-expression and psychological attributes.\nOur experiments show that users' language and traits improve predictions above\nand beyond models predicting only from text. Our findings highlight the\nimportance of integrating psychological constructs into NLP to enhance the\nunderstanding and prediction of user actions. We close with a discussion of the\nimplications for future applications of large language models for computational\npsychology."
        ],
        "neg": []
    },
    {
        "query": "Imagine observing someone scratching their arm; to understand why, additional\ncontext would be necessary. However, spotting a mosquito nearby would\nimmediately offer a likely explanation for the person's discomfort, thereby\nalleviating the need for further information. This example illustrates how\nsubtle visual cues can challenge our cognitive skills and demonstrates the\ncomplexity of interpreting visual scenarios. To study these skills, we present\nVisual Riddles, a benchmark aimed to test vision and language models on visual\nriddles requiring commonsense and world knowledge. The benchmark comprises 400\nvisual riddles, each featuring a unique image created by a variety of\ntext-to-image models, question, ground-truth answer, textual hint, and\nattribution. Human evaluation reveals that existing models lag significantly\nbehind human performance, which is at 82\\% accuracy, with Gemini-Pro-1.5\nleading with 40\\% accuracy. Our benchmark comes with automatic evaluation tasks\nto make assessment scalable. These findings underscore the potential of Visual\nRiddles as a valuable resource for enhancing vision and language models'\ncapabilities in interpreting complex visual scenarios.",
        "pos": [
            "Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.",
            "Values are a basic driving force underlying human behavior. Large Language\nModels (LLM) technology is constantly improving towards human-like dialogue.\nHowever, little research has been done to study the values exhibited in text\ngenerated by LLMs. Here we study this question by turning to the rich\nliterature on value structure in psychology. We ask whether LLMs exhibit the\nsame value structure that has been demonstrated in humans, including the\nranking of values, and correlation between values. We show that the results of\nthis analysis strongly depend on how the LLM is prompted, and that under a\nparticular prompting strategy (referred to as 'Value Anchoring') the agreement\nwith human data is quite compelling. Our results serve both to improve our\nunderstanding of values in LLMs, as well as introduce novel methods for\nassessing consistency in LLM responses."
        ],
        "neg": []
    },
    {
        "query": "Imagine observing someone scratching their arm; to understand why, additional\ncontext would be necessary. However, spotting a mosquito nearby would\nimmediately offer a likely explanation for the person's discomfort, thereby\nalleviating the need for further information. This example illustrates how\nsubtle visual cues can challenge our cognitive skills and demonstrates the\ncomplexity of interpreting visual scenarios. To study these skills, we present\nVisual Riddles, a benchmark aimed to test vision and language models on visual\nriddles requiring commonsense and world knowledge. The benchmark comprises 400\nvisual riddles, each featuring a unique image created by a variety of\ntext-to-image models, question, ground-truth answer, textual hint, and\nattribution. Human evaluation reveals that existing models lag significantly\nbehind human performance, which is at 82\\% accuracy, with Gemini-Pro-1.5\nleading with 40\\% accuracy. Our benchmark comes with automatic evaluation tasks\nto make assessment scalable. These findings underscore the potential of Visual\nRiddles as a valuable resource for enhancing vision and language models'\ncapabilities in interpreting complex visual scenarios.",
        "pos": [
            "As the number and sophistication of cyber attacks have increased, threat\nhunting has become a critical aspect of active security, enabling proactive\ndetection and mitigation of threats before they cause significant harm.\nOpen-source cyber threat intelligence (OS-CTI) is a valuable resource for\nthreat hunters, however, it often comes in unstructured formats that require\nfurther manual analysis. Previous studies aimed at automating OSCTI analysis\nare limited since (1) they failed to provide actionable outputs, (2) they did\nnot take advantage of images present in OSCTI sources, and (3) they focused on\non-premises environments, overlooking the growing importance of cloud\nenvironments. To address these gaps, we propose LLMCloudHunter, a novel\nframework that leverages large language models (LLMs) to automatically generate\ngeneric-signature detection rule candidates from textual and visual OSCTI data.\nWe evaluated the quality of the rules generated by the proposed framework using\n12 annotated real-world cloud threat reports. The results show that our\nframework achieved a precision of 92% and recall of 98% for the task of\naccurately extracting API calls made by the threat actor and a precision of 99%\nwith a recall of 98% for IoCs. Additionally, 99.18% of the generated detection\nrule candidates were successfully compiled and converted into Splunk queries."
        ],
        "neg": []
    },
    {
        "query": "We provide a detailed overview of various approaches to word segmentation of\nAsian Languages, specifically Chinese, Korean, and Japanese languages. For each\nlanguage, approaches to deal with word segmentation differs. We also include\nour analysis about certain advantages and disadvantages to each method. In\naddition, there is room for future work in this field.",
        "pos": [
            "With the introduction of large language models (LLMs), automatic math\nreasoning has seen tremendous success. However, current methods primarily focus\non providing solutions or using techniques like Chain-of-Thought to enhance\nproblem-solving accuracy. In this paper, we focus on improving the capability\nof mathematics teaching via a Socratic teaching-based LLM\n(\\texttt{SocraticLLM}), which guides learners toward profound thinking with\nclarity and self-discovery via conversation. We collect and release a\nhigh-quality mathematical teaching dataset, named \\texttt{SocraticMATH}, which\nprovides Socratic-style conversations of problems with extra knowledge. Also,\nwe propose a knowledge-enhanced LLM as a strong baseline to generate reliable\nresponses with review, guidance/heuristic, rectification, and summarization.\nExperimental results show the great advantages of \\texttt{SocraticLLM} by\ncomparing it with several strong generative models. The codes and datasets are\navailable on \\url{https://github.com/ECNU-ICALK/SocraticMath}."
        ],
        "neg": []
    },
    {
        "query": "In the rapidly evolving field of legal analytics, finding relevant cases and\naccurately predicting judicial outcomes are challenging because of the\ncomplexity of legal language, which often includes specialized terminology,\ncomplex syntax, and historical context. Moreover, the subtle distinctions\nbetween similar and precedent cases require a deep understanding of legal\nknowledge. Researchers often conflate these concepts, making it difficult to\ndevelop specialized techniques to effectively address these nuanced tasks. In\nthis paper, we introduce the Law Large Language Model (LawLLM), a multi-task\nmodel specifically designed for the US legal domain to address these\nchallenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case\nRecommendation (PCR), and Legal Judgment Prediction (LJP). By clearly\ndistinguishing between precedent and similar cases, we provide essential\nclarity, guiding future research in developing specialized strategies for these\ntasks. We propose customized data preprocessing techniques for each task that\ntransform raw legal data into a trainable format. Furthermore, we also use\ntechniques such as in-context learning (ICL) and advanced information retrieval\nmethods in LawLLM. The evaluation results demonstrate that LawLLM consistently\noutperforms existing baselines in both zero-shot and few-shot scenarios,\noffering unparalleled multi-task capabilities and filling critical gaps in the\nlegal domain.",
        "pos": [
            "Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.",
            "Vector retrieval algorithms are vital for semantic queries in the evolving\nlandscape of Large Language Models (LLMs). Retrieving vectors that\nsimultaneously meet criteria for both similarity and diversity significantly\nenhances the capabilities of LLM-based agents. Despite the widespread use of\nthe Maximal Marginal Relevance (MMR) in retrieval scenarios with relevance and\ndiversity requirements, fluctuations caused by variations in the parameter $\n\\lambda $ within the MMR complicate the determination of the optimization\ntrajectory in vector spaces, thus obscuring the direction of enhancement.\nMoreover, there is a lack of a robust theoretical analysis for the constraints\nof similarity and diversity in retrieval processes. This paper introduces a\nnovel approach to characterizing both constraints through the relationship\nbetween the sum vector and the query vector. The proximity of these vectors\naddresses the similarity constraint, while necessitating that individual\nvectors within the sum vector divergently align with the query vector to\nsatisfy the diversity constraint. We also formulate a new combinatorial\noptimization challenge, taking a selection of $k$ vectors from a set of\ncandidates such that their sum vector maximally aligns with the query vector, a\nproblem we demonstrate to be NP-complete. This establishes the profound\ndifficulty of pursuing similarity and diversity simultaneously in vector\nretrieval and lays a theoretical groundwork for further research. Additionally,\nwe present the heuristic algorithm Vectors Retrieval with Similarity and\nDiversity (VRSD) which not only has a definitive optimization goal and eschews\nthe need for preset parameters but also offers a modest reduction in time\ncomplexity compared to MMR. Empirical validation further confirm that VRSD\nsignificantly surpasses MMR across various datasets."
        ],
        "neg": []
    },
    {
        "query": "Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large\nfoundation models, leveraging low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$\nto represent weight changes (\\textit{i.e.,} $\\Delta \\mathbf{W} = \\mathbf{B}\n\\mathbf{A}$). This method reduces trainable parameters and mitigates heavy\nmemory consumption associated with full delta matrices by sequentially\nmultiplying $\\mathbf{A}$ and $\\mathbf{B}$ with the activation. Despite its\nsuccess, the intrinsic low-rank characteristic may limit its performance.\nAlthough several variants have been proposed to address this issue, they often\noverlook the crucial computational and memory efficiency brought by LoRA. In\nthis paper, we propose \\underline{C}ir\\underline{c}ular \\underline{C}onvolution\n\\underline{A}daptation (C$^3$A), which not only achieves high-rank adaptation\nwith enhanced performance but also excels in both computational power and\nmemory utilization. Extensive experiments demonstrate that C$^3$A consistently\noutperforms LoRA and its variants across various fine-tuning tasks.",
        "pos": [
            "The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench."
        ],
        "neg": []
    },
    {
        "query": "Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large\nfoundation models, leveraging low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$\nto represent weight changes (\\textit{i.e.,} $\\Delta \\mathbf{W} = \\mathbf{B}\n\\mathbf{A}$). This method reduces trainable parameters and mitigates heavy\nmemory consumption associated with full delta matrices by sequentially\nmultiplying $\\mathbf{A}$ and $\\mathbf{B}$ with the activation. Despite its\nsuccess, the intrinsic low-rank characteristic may limit its performance.\nAlthough several variants have been proposed to address this issue, they often\noverlook the crucial computational and memory efficiency brought by LoRA. In\nthis paper, we propose \\underline{C}ir\\underline{c}ular \\underline{C}onvolution\n\\underline{A}daptation (C$^3$A), which not only achieves high-rank adaptation\nwith enhanced performance but also excels in both computational power and\nmemory utilization. Extensive experiments demonstrate that C$^3$A consistently\noutperforms LoRA and its variants across various fine-tuning tasks.",
        "pos": [
            "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir potential applications across various fields. This paper embarked on a\npivotal inquiry: Can existing LLMs effectively serve as \"water expert models\"\nfor water engineering and research tasks? This study was the first to evaluate\nLLMs' contributions across various water engineering and research tasks by\nestablishing a domain-specific benchmark suite, namely, WaterER. Herein, we\nprepared 983 tasks related to water engineering and research, categorized into\n\"wastewater treatment\", \"environmental restoration\", \"drinking water treatment\nand distribution\", \"sanitation\", \"anaerobic digestion\" and \"contaminants\nassessment\". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5,\nGemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the\nstrengths of GPT-4 in handling diverse and complex tasks of water engineering\nand water research, the specialized capabilities of Gemini in academic\ncontexts, Llama3's strongest capacity to answer Chinese water engineering\nquestions and the competitive performance of Chinese-oriented models like\nGLM-4, ERNIE and QWEN in some water engineering tasks. More specifically,\ncurrent LLMs excelled particularly in generating precise research gaps for\npapers on \"contaminants and related water quality monitoring and assessment\".\nAdditionally, they were more adept at creating appropriate titles for research\npapers on \"treatment processes for wastewaters\", \"environmental restoration\",\nand \"drinking water treatment\". Overall, this study pioneered evaluating LLMs\nin water engineering and research by introducing the WaterER benchmark to\nassess the trustworthiness of their predictions. This standardized evaluation\nframework would also drive future advancements in LLM technology by using\ntargeting datasets, propelling these models towards becoming true \"water\nexpert\".",
            "Alignment is a crucial step to enhance the instruction-following and\nconversational abilities of language models. Despite many recent work proposing\nnew algorithms, datasets, and training pipelines, there is a lack of\ncomprehensive studies measuring the impact of various design choices throughout\nthe whole training process. We first conduct a rigorous analysis over a\nthree-stage training pipeline consisting of supervised fine-tuning, offline\npreference learning, and online preference learning. We have found that using\ntechniques like sequence packing, loss masking in SFT, increasing the\npreference dataset size in DPO, and online DPO training can significantly\nimprove the performance of language models. We then train from Gemma-2b-base\nand LLama-3-8b-base, and find that our best models exceed the performance of\nthe official instruct models tuned with closed-source data and algorithms. Our\ncode and models can be found at\nhttps://github.com/Columbia-NLP-Lab/LionAlignment."
        ],
        "neg": []
    },
    {
        "query": "Humans appear to have a critical period (CP) for language acquisition: Second\nlanguage (L2) acquisition becomes harder after early childhood, and ceasing\nexposure to a first language (L1) after this period (but not before) typically\ndoes not lead to substantial loss of L1 proficiency. It is unknown whether\nthese CP effects result from innately determined brain maturation or as a\nstabilization of neural connections naturally induced by experience. In this\nstudy, we use language models (LMs) to test the extent to which these phenomena\nare peculiar to humans, or shared by a broader class of language learners. We\nvary the age of exposure by training LMs on language pairs in various\nexperimental conditions, and find that LMs, which lack any direct analog to\ninnate maturational stages, do not show CP effects when trained sequentially on\nL1 and L2. Our results contradict the claim that CP effects are an inevitable\nresult of learning in statistical learners, and they are consistent with an\ninnate mechanism for CP effects. We show that we can reverse-engineer the CP by\nintroducing a regularizer partway through training to simulate a maturational\ndecrease in plasticity. All in all, our results suggest that L1 learning on its\nown may not be enough to induce a CP, and additional engineering is necessary\nto make language models more cognitively plausible.",
        "pos": [
            "Tokenization - the practice of converting strings of characters over an\nalphabet into sequences of tokens over a vocabulary - is a critical yet\nunder-theorized step in the NLP pipeline. Notably, it remains the only major\nstep not fully integrated into widely used end-to-end neural models. This paper\naims to address this theoretical gap by laying the foundations of tokenization\nfrom a formal perspective. By articulating and extending basic properties about\nthe category of stochastic maps, we propose a unified framework for\nrepresenting and analyzing tokenizer models. This framework allows us to\nestablish general conditions for the use of tokenizers. In particular, we\nformally establish the necessary and sufficient conditions for a tokenizer\nmodel to preserve the consistency of statistical estimators. Additionally, we\ndiscuss statistical and computational concerns crucial for the design and\nimplementation of tokenizer models. The framework and results advanced in this\npaper represent a step toward a robust theoretical foundation for neural\nlanguage modeling.",
            "Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on a controlled\ngeneration task suggest that while variational BoN is not as effective as BoN\nin aligning language models, it is close to BoN performance as vBoN appears\nmore often on the Pareto frontier of reward and KL divergence compared to\nmodels trained with KL-constrained RL objective."
        ],
        "neg": []
    },
    {
        "query": "Understanding whether and to what extent large language models (LLMs) have\nmemorised training data has important implications for the reliability of their\noutput and the privacy of their training data. In order to cleanly measure and\ndisentangle memorisation from other phenomena (e.g. in-context learning), we\ncreate an experimental framework that is based on repeatedly exposing LLMs to\nrandom strings. Our framework allows us to better understand the dynamics,\ni.e., the behaviour of the model, when repeatedly exposing it to random\nstrings. Using our framework, we make several striking observations: (a) we\nfind consistent phases of the dynamics across families of models (Pythia, Phi\nand Llama2), (b) we identify factors that make some strings easier to memorise\nthan others, and (c) we identify the role of local prefixes and global context\nin memorisation. We also show that sequential exposition to different random\nstrings has a significant effect on memorisation. Our results, often\nsurprising, have significant downstream implications in the study and usage of\nLLMs.",
        "pos": [
            "Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal tasks remains limited. We conduct a comprehensive study of 260\nlegal text classification tasks, nearly all new to the machine learning\ncommunity. Starting from GPT-4 as a baseline, we show that it has non-trivial\nbut highly varied zero-shot accuracy, often exhibiting performance that may be\ninsufficient for legal work. We then demonstrate that a lightly fine-tuned\nLlama 3 model vastly outperforms GPT-4 on almost all tasks, typically by\ndouble-digit percentage points. We find that larger models respond better to\nfine-tuning than smaller models. A few tens to hundreds of examples suffice to\nachieve high classification accuracy. Notably, we can fine-tune a single model\non all 260 tasks simultaneously at a small loss in accuracy relative to having\na separate model for each task. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal tasks\nwith some available labeled data, researchers are better off using a fine-tuned\nopen-source model."
        ],
        "neg": []
    },
    {
        "query": "This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, in recent years, a series of theorems have\nbeen proven to show that given an input sample, a small number of interactions\nbetween input variables can be considered as primitive inference patterns,\nwhich can faithfully represent every detailed inference logic of the DNN on\nthis sample. Particularly, it has been observed that various DNNs all learn\ninteractions of different complexities with two-phase dynamics, and this well\nexplains how a DNN's generalization power changes from under-fitting to\nover-fitting. Therefore, in this study, we prove the dynamics of a DNN\ngradually encoding interactions of different complexities, which provides a\ntheoretically grounded mechanism for the over-fitting of a DNN. Experiments\nshow that our theory well predicts the real learning dynamics of various DNNs\non different tasks.",
        "pos": [
            "Through reading the documentation in the context, tool-using language models\ncan dynamically extend their capability using external tools. The cost is that\nwe have to input lengthy documentation every time the model needs to use the\ntool, occupying the input window as well as slowing down the decoding process.\n  Given the progress in general-purpose compression, soft context compression\nis a suitable approach to alleviate the problem. However, when compressing tool\ndocumentation, existing methods suffer from the weaknesses of key information\nloss (specifically, tool/parameter name errors) and difficulty in adjusting the\nlength of compressed sequences based on documentation lengths.\n  To address these problems, we propose two strategies for compressing tool\ndocumentation into concise and precise summary sequences for tool-using\nlanguage models. 1) Selective compression strategy mitigates key information\nloss by deliberately retaining key information as raw text tokens. 2) Block\ncompression strategy involves dividing tool documentation into short chunks and\nthen employing a fixed-length compression model to achieve variable-length\ncompression. This strategy facilitates the flexible adjustment of the\ncompression ratio.\n  Results on API-Bank and APIBench show that our approach reaches a performance\ncomparable to the upper-bound baseline under up to 16x compression ratio."
        ],
        "neg": []
    },
    {
        "query": "This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, in recent years, a series of theorems have\nbeen proven to show that given an input sample, a small number of interactions\nbetween input variables can be considered as primitive inference patterns,\nwhich can faithfully represent every detailed inference logic of the DNN on\nthis sample. Particularly, it has been observed that various DNNs all learn\ninteractions of different complexities with two-phase dynamics, and this well\nexplains how a DNN's generalization power changes from under-fitting to\nover-fitting. Therefore, in this study, we prove the dynamics of a DNN\ngradually encoding interactions of different complexities, which provides a\ntheoretically grounded mechanism for the over-fitting of a DNN. Experiments\nshow that our theory well predicts the real learning dynamics of various DNNs\non different tasks.",
        "pos": [
            "Personality psychologists have analyzed the relationship between personality\nand safety behaviors in human society. Although Large Language Models (LLMs)\ndemonstrate personality traits, the relationship between personality traits and\nsafety abilities in LLMs still remains a mystery. In this paper, we discover\nthat LLMs' personality traits are closely related to their safety abilities,\ni.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.\nMeanwhile, the safety alignment generally increases various LLMs' Extraversion,\nSensing, and Judging traits. According to such findings, we can edit LLMs'\npersonality traits and improve their safety performance, e.g., inducing\npersonality from ISTJ to ISTP resulted in a relative improvement of\napproximately 43% and 10% in privacy and fairness performance, respectively.\nAdditionally, we find that LLMs with different personality traits are\ndifferentially susceptible to jailbreak. This study pioneers the investigation\nof LLM safety from a personality perspective, providing new insights into LLM\nsafety enhancement."
        ],
        "neg": []
    },
    {
        "query": "Various social media platforms, e.g., Twitter and Reddit, allow people to\ndisseminate a plethora of information more efficiently and conveniently.\nHowever, they are inevitably full of misinformation, causing damage to diverse\naspects of our daily lives. To reduce the negative impact, timely\nidentification of misinformation, namely Misinformation Detection (MD), has\nbecome an active research topic receiving widespread attention. As a complex\nphenomenon, the veracity of an article is influenced by various aspects. In\nthis paper, we are inspired by the opposition of intents between misinformation\nand real information. Accordingly, we propose to reason the intent of articles\nand form the corresponding intent features to promote the veracity\ndiscrimination of article features. To achieve this, we build a hierarchy of a\nset of intents for both misinformation and real information by referring to the\nexisting psychological theories, and we apply it to reason the intent of\narticles by progressively generating binary answers with an encoder-decoder\nstructure. We form the corresponding intent features and integrate it with the\ntoken features to achieve more discriminative article features for MD. Upon\nthese ideas, we suggest a novel MD method, namely Detecting Misinformation by\nIntegrating Intent featuRes (DM-INTER). To evaluate the performance of\nDM-INTER, we conduct extensive experiments on benchmark MD datasets. The\nexperimental results validate that DM-INTER can outperform the existing\nbaseline MD methods.",
        "pos": [
            "Nowadays, misinformation is widely spreading over various social media\nplatforms and causes extremely negative impacts on society. To combat this\nissue, automatically identifying misinformation, especially those containing\nmultimodal content, has attracted growing attention from the academic and\nindustrial communities, and induced an active research topic named Multimodal\nMisinformation Detection (MMD). Typically, existing MMD methods capture the\nsemantic correlation and inconsistency between multiple modalities, but neglect\nsome potential clues in multimodal content. Recent studies suggest that\nmanipulated traces of the images in articles are non-trivial clues for\ndetecting misinformation. Meanwhile, we find that the underlying intentions\nbehind the manipulation, e.g., harmful and harmless, also matter in MMD.\nAccordingly, in this work, we propose to detect misinformation by learning\nmanipulation features that indicate whether the image has been manipulated, as\nwell as intention features regarding the harmful and harmless intentions of the\nmanipulation. Unfortunately, the manipulation and intention labels that make\nthese features discriminative are unknown. To overcome the problem, we propose\ntwo weakly supervised signals as alternatives by introducing additional\ndatasets on image manipulation detection and formulating two classification\ntasks as positive and unlabeled learning problems. Based on these ideas, we\npropose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD\n(HAMI-M3D). Extensive experiments across three benchmark datasets can\ndemonstrate that HAMI-M3D can consistently improve the performance of any MMD\nbaselines."
        ],
        "neg": []
    },
    {
        "query": "Various social media platforms, e.g., Twitter and Reddit, allow people to\ndisseminate a plethora of information more efficiently and conveniently.\nHowever, they are inevitably full of misinformation, causing damage to diverse\naspects of our daily lives. To reduce the negative impact, timely\nidentification of misinformation, namely Misinformation Detection (MD), has\nbecome an active research topic receiving widespread attention. As a complex\nphenomenon, the veracity of an article is influenced by various aspects. In\nthis paper, we are inspired by the opposition of intents between misinformation\nand real information. Accordingly, we propose to reason the intent of articles\nand form the corresponding intent features to promote the veracity\ndiscrimination of article features. To achieve this, we build a hierarchy of a\nset of intents for both misinformation and real information by referring to the\nexisting psychological theories, and we apply it to reason the intent of\narticles by progressively generating binary answers with an encoder-decoder\nstructure. We form the corresponding intent features and integrate it with the\ntoken features to achieve more discriminative article features for MD. Upon\nthese ideas, we suggest a novel MD method, namely Detecting Misinformation by\nIntegrating Intent featuRes (DM-INTER). To evaluate the performance of\nDM-INTER, we conduct extensive experiments on benchmark MD datasets. The\nexperimental results validate that DM-INTER can outperform the existing\nbaseline MD methods.",
        "pos": [
            "Nowadays, misinformation is widely spreading over various social media\nplatforms and causes extremely negative impacts on society. To combat this\nissue, automatically identifying misinformation, especially those containing\nmultimodal content, has attracted growing attention from the academic and\nindustrial communities, and induced an active research topic named Multimodal\nMisinformation Detection (MMD). Typically, existing MMD methods capture the\nsemantic correlation and inconsistency between multiple modalities, but neglect\nsome potential clues in multimodal content. Recent studies suggest that\nmanipulated traces of the images in articles are non-trivial clues for\ndetecting misinformation. Meanwhile, we find that the underlying intentions\nbehind the manipulation, e.g., harmful and harmless, also matter in MMD.\nAccordingly, in this work, we propose to detect misinformation by learning\nmanipulation features that indicate whether the image has been manipulated, as\nwell as intention features regarding the harmful and harmless intentions of the\nmanipulation. Unfortunately, the manipulation and intention labels that make\nthese features discriminative are unknown. To overcome the problem, we propose\ntwo weakly supervised signals as alternatives by introducing additional\ndatasets on image manipulation detection and formulating two classification\ntasks as positive and unlabeled learning problems. Based on these ideas, we\npropose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD\n(HAMI-M3D). Extensive experiments across three benchmark datasets can\ndemonstrate that HAMI-M3D can consistently improve the performance of any MMD\nbaselines."
        ],
        "neg": []
    },
    {
        "query": "Various social media platforms, e.g., Twitter and Reddit, allow people to\ndisseminate a plethora of information more efficiently and conveniently.\nHowever, they are inevitably full of misinformation, causing damage to diverse\naspects of our daily lives. To reduce the negative impact, timely\nidentification of misinformation, namely Misinformation Detection (MD), has\nbecome an active research topic receiving widespread attention. As a complex\nphenomenon, the veracity of an article is influenced by various aspects. In\nthis paper, we are inspired by the opposition of intents between misinformation\nand real information. Accordingly, we propose to reason the intent of articles\nand form the corresponding intent features to promote the veracity\ndiscrimination of article features. To achieve this, we build a hierarchy of a\nset of intents for both misinformation and real information by referring to the\nexisting psychological theories, and we apply it to reason the intent of\narticles by progressively generating binary answers with an encoder-decoder\nstructure. We form the corresponding intent features and integrate it with the\ntoken features to achieve more discriminative article features for MD. Upon\nthese ideas, we suggest a novel MD method, namely Detecting Misinformation by\nIntegrating Intent featuRes (DM-INTER). To evaluate the performance of\nDM-INTER, we conduct extensive experiments on benchmark MD datasets. The\nexperimental results validate that DM-INTER can outperform the existing\nbaseline MD methods.",
        "pos": [
            "Nowadays, misinformation is widely spreading over various social media\nplatforms and causes extremely negative impacts on society. To combat this\nissue, automatically identifying misinformation, especially those containing\nmultimodal content, has attracted growing attention from the academic and\nindustrial communities, and induced an active research topic named Multimodal\nMisinformation Detection (MMD). Typically, existing MMD methods capture the\nsemantic correlation and inconsistency between multiple modalities, but neglect\nsome potential clues in multimodal content. Recent studies suggest that\nmanipulated traces of the images in articles are non-trivial clues for\ndetecting misinformation. Meanwhile, we find that the underlying intentions\nbehind the manipulation, e.g., harmful and harmless, also matter in MMD.\nAccordingly, in this work, we propose to detect misinformation by learning\nmanipulation features that indicate whether the image has been manipulated, as\nwell as intention features regarding the harmful and harmless intentions of the\nmanipulation. Unfortunately, the manipulation and intention labels that make\nthese features discriminative are unknown. To overcome the problem, we propose\ntwo weakly supervised signals as alternatives by introducing additional\ndatasets on image manipulation detection and formulating two classification\ntasks as positive and unlabeled learning problems. Based on these ideas, we\npropose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD\n(HAMI-M3D). Extensive experiments across three benchmark datasets can\ndemonstrate that HAMI-M3D can consistently improve the performance of any MMD\nbaselines."
        ],
        "neg": []
    },
    {
        "query": "Various social media platforms, e.g., Twitter and Reddit, allow people to\ndisseminate a plethora of information more efficiently and conveniently.\nHowever, they are inevitably full of misinformation, causing damage to diverse\naspects of our daily lives. To reduce the negative impact, timely\nidentification of misinformation, namely Misinformation Detection (MD), has\nbecome an active research topic receiving widespread attention. As a complex\nphenomenon, the veracity of an article is influenced by various aspects. In\nthis paper, we are inspired by the opposition of intents between misinformation\nand real information. Accordingly, we propose to reason the intent of articles\nand form the corresponding intent features to promote the veracity\ndiscrimination of article features. To achieve this, we build a hierarchy of a\nset of intents for both misinformation and real information by referring to the\nexisting psychological theories, and we apply it to reason the intent of\narticles by progressively generating binary answers with an encoder-decoder\nstructure. We form the corresponding intent features and integrate it with the\ntoken features to achieve more discriminative article features for MD. Upon\nthese ideas, we suggest a novel MD method, namely Detecting Misinformation by\nIntegrating Intent featuRes (DM-INTER). To evaluate the performance of\nDM-INTER, we conduct extensive experiments on benchmark MD datasets. The\nexperimental results validate that DM-INTER can outperform the existing\nbaseline MD methods.",
        "pos": [
            "Nowadays, misinformation is widely spreading over various social media\nplatforms and causes extremely negative impacts on society. To combat this\nissue, automatically identifying misinformation, especially those containing\nmultimodal content, has attracted growing attention from the academic and\nindustrial communities, and induced an active research topic named Multimodal\nMisinformation Detection (MMD). Typically, existing MMD methods capture the\nsemantic correlation and inconsistency between multiple modalities, but neglect\nsome potential clues in multimodal content. Recent studies suggest that\nmanipulated traces of the images in articles are non-trivial clues for\ndetecting misinformation. Meanwhile, we find that the underlying intentions\nbehind the manipulation, e.g., harmful and harmless, also matter in MMD.\nAccordingly, in this work, we propose to detect misinformation by learning\nmanipulation features that indicate whether the image has been manipulated, as\nwell as intention features regarding the harmful and harmless intentions of the\nmanipulation. Unfortunately, the manipulation and intention labels that make\nthese features discriminative are unknown. To overcome the problem, we propose\ntwo weakly supervised signals as alternatives by introducing additional\ndatasets on image manipulation detection and formulating two classification\ntasks as positive and unlabeled learning problems. Based on these ideas, we\npropose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD\n(HAMI-M3D). Extensive experiments across three benchmark datasets can\ndemonstrate that HAMI-M3D can consistently improve the performance of any MMD\nbaselines."
        ],
        "neg": []
    },
    {
        "query": "The proliferation of large language models (LLMs) requires robust evaluation\nof their alignment with local values and ethical standards, especially as\nexisting benchmarks often reflect the cultural, legal, and ideological values\nof their creators. \\textsc{LocalValueBench}, introduced in this paper, is an\nextensible benchmark designed to assess LLMs' adherence to Australian values,\nand provides a framework for regulators worldwide to develop their own LLM\nbenchmarks for local value alignment. Employing a novel typology for ethical\nreasoning and an interrogation approach, we curated comprehensive questions and\nutilized prompt engineering strategies to probe LLMs' value alignment. Our\nevaluation criteria quantified deviations from local values, ensuring a\nrigorous assessment process. Comparative analysis of three commercial LLMs by\nUSA vendors revealed significant insights into their effectiveness and\nlimitations, demonstrating the critical importance of value alignment. This\nstudy offers valuable tools and methodologies for regulators to create tailored\nbenchmarks, highlighting avenues for future research to enhance ethical AI\ndevelopment.",
        "pos": [
            "Addressing school bullying effectively and promptly is crucial for the mental\nhealth of students. This study examined the potential of large language models\n(LLMs) to empower students by discerning between bullying and joking in school\npeer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus,\nevaluating their effectiveness through human review. Our results revealed that\nnot all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the\nmost promise. We observed variations in LLM outputs, possibly influenced by\npolitical overcorrectness, context window limitations, and pre-existing bias in\ntheir training data. ChatGPT-4 excelled in context-specific accuracy after\nimplementing the agentic approach, highlighting its potential to provide\ncontinuous, real-time support to vulnerable students. This study underlines the\nsignificant social impact of using agentic AI in educational settings, offering\na new avenue for reducing the negative consequences of bullying and enhancing\nstudent well-being."
        ],
        "neg": []
    },
    {
        "query": "The proliferation of large language models (LLMs) requires robust evaluation\nof their alignment with local values and ethical standards, especially as\nexisting benchmarks often reflect the cultural, legal, and ideological values\nof their creators. \\textsc{LocalValueBench}, introduced in this paper, is an\nextensible benchmark designed to assess LLMs' adherence to Australian values,\nand provides a framework for regulators worldwide to develop their own LLM\nbenchmarks for local value alignment. Employing a novel typology for ethical\nreasoning and an interrogation approach, we curated comprehensive questions and\nutilized prompt engineering strategies to probe LLMs' value alignment. Our\nevaluation criteria quantified deviations from local values, ensuring a\nrigorous assessment process. Comparative analysis of three commercial LLMs by\nUSA vendors revealed significant insights into their effectiveness and\nlimitations, demonstrating the critical importance of value alignment. This\nstudy offers valuable tools and methodologies for regulators to create tailored\nbenchmarks, highlighting avenues for future research to enhance ethical AI\ndevelopment.",
        "pos": [
            "Addressing school bullying effectively and promptly is crucial for the mental\nhealth of students. This study examined the potential of large language models\n(LLMs) to empower students by discerning between bullying and joking in school\npeer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus,\nevaluating their effectiveness through human review. Our results revealed that\nnot all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the\nmost promise. We observed variations in LLM outputs, possibly influenced by\npolitical overcorrectness, context window limitations, and pre-existing bias in\ntheir training data. ChatGPT-4 excelled in context-specific accuracy after\nimplementing the agentic approach, highlighting its potential to provide\ncontinuous, real-time support to vulnerable students. This study underlines the\nsignificant social impact of using agentic AI in educational settings, offering\na new avenue for reducing the negative consequences of bullying and enhancing\nstudent well-being."
        ],
        "neg": []
    },
    {
        "query": "The proliferation of large language models (LLMs) requires robust evaluation\nof their alignment with local values and ethical standards, especially as\nexisting benchmarks often reflect the cultural, legal, and ideological values\nof their creators. \\textsc{LocalValueBench}, introduced in this paper, is an\nextensible benchmark designed to assess LLMs' adherence to Australian values,\nand provides a framework for regulators worldwide to develop their own LLM\nbenchmarks for local value alignment. Employing a novel typology for ethical\nreasoning and an interrogation approach, we curated comprehensive questions and\nutilized prompt engineering strategies to probe LLMs' value alignment. Our\nevaluation criteria quantified deviations from local values, ensuring a\nrigorous assessment process. Comparative analysis of three commercial LLMs by\nUSA vendors revealed significant insights into their effectiveness and\nlimitations, demonstrating the critical importance of value alignment. This\nstudy offers valuable tools and methodologies for regulators to create tailored\nbenchmarks, highlighting avenues for future research to enhance ethical AI\ndevelopment.",
        "pos": [
            "Addressing school bullying effectively and promptly is crucial for the mental\nhealth of students. This study examined the potential of large language models\n(LLMs) to empower students by discerning between bullying and joking in school\npeer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus,\nevaluating their effectiveness through human review. Our results revealed that\nnot all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the\nmost promise. We observed variations in LLM outputs, possibly influenced by\npolitical overcorrectness, context window limitations, and pre-existing bias in\ntheir training data. ChatGPT-4 excelled in context-specific accuracy after\nimplementing the agentic approach, highlighting its potential to provide\ncontinuous, real-time support to vulnerable students. This study underlines the\nsignificant social impact of using agentic AI in educational settings, offering\na new avenue for reducing the negative consequences of bullying and enhancing\nstudent well-being."
        ],
        "neg": []
    },
    {
        "query": "The proliferation of large language models (LLMs) requires robust evaluation\nof their alignment with local values and ethical standards, especially as\nexisting benchmarks often reflect the cultural, legal, and ideological values\nof their creators. \\textsc{LocalValueBench}, introduced in this paper, is an\nextensible benchmark designed to assess LLMs' adherence to Australian values,\nand provides a framework for regulators worldwide to develop their own LLM\nbenchmarks for local value alignment. Employing a novel typology for ethical\nreasoning and an interrogation approach, we curated comprehensive questions and\nutilized prompt engineering strategies to probe LLMs' value alignment. Our\nevaluation criteria quantified deviations from local values, ensuring a\nrigorous assessment process. Comparative analysis of three commercial LLMs by\nUSA vendors revealed significant insights into their effectiveness and\nlimitations, demonstrating the critical importance of value alignment. This\nstudy offers valuable tools and methodologies for regulators to create tailored\nbenchmarks, highlighting avenues for future research to enhance ethical AI\ndevelopment.",
        "pos": [
            "Addressing school bullying effectively and promptly is crucial for the mental\nhealth of students. This study examined the potential of large language models\n(LLMs) to empower students by discerning between bullying and joking in school\npeer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus,\nevaluating their effectiveness through human review. Our results revealed that\nnot all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the\nmost promise. We observed variations in LLM outputs, possibly influenced by\npolitical overcorrectness, context window limitations, and pre-existing bias in\ntheir training data. ChatGPT-4 excelled in context-specific accuracy after\nimplementing the agentic approach, highlighting its potential to provide\ncontinuous, real-time support to vulnerable students. This study underlines the\nsignificant social impact of using agentic AI in educational settings, offering\na new avenue for reducing the negative consequences of bullying and enhancing\nstudent well-being."
        ],
        "neg": []
    },
    {
        "query": "The proliferation of large language models (LLMs) requires robust evaluation\nof their alignment with local values and ethical standards, especially as\nexisting benchmarks often reflect the cultural, legal, and ideological values\nof their creators. \\textsc{LocalValueBench}, introduced in this paper, is an\nextensible benchmark designed to assess LLMs' adherence to Australian values,\nand provides a framework for regulators worldwide to develop their own LLM\nbenchmarks for local value alignment. Employing a novel typology for ethical\nreasoning and an interrogation approach, we curated comprehensive questions and\nutilized prompt engineering strategies to probe LLMs' value alignment. Our\nevaluation criteria quantified deviations from local values, ensuring a\nrigorous assessment process. Comparative analysis of three commercial LLMs by\nUSA vendors revealed significant insights into their effectiveness and\nlimitations, demonstrating the critical importance of value alignment. This\nstudy offers valuable tools and methodologies for regulators to create tailored\nbenchmarks, highlighting avenues for future research to enhance ethical AI\ndevelopment.",
        "pos": [
            "Addressing school bullying effectively and promptly is crucial for the mental\nhealth of students. This study examined the potential of large language models\n(LLMs) to empower students by discerning between bullying and joking in school\npeer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus,\nevaluating their effectiveness through human review. Our results revealed that\nnot all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the\nmost promise. We observed variations in LLM outputs, possibly influenced by\npolitical overcorrectness, context window limitations, and pre-existing bias in\ntheir training data. ChatGPT-4 excelled in context-specific accuracy after\nimplementing the agentic approach, highlighting its potential to provide\ncontinuous, real-time support to vulnerable students. This study underlines the\nsignificant social impact of using agentic AI in educational settings, offering\na new avenue for reducing the negative consequences of bullying and enhancing\nstudent well-being."
        ],
        "neg": []
    },
    {
        "query": "Office automation significantly enhances human productivity by automatically\nfinishing routine tasks in the workflow. Beyond the basic information\nextraction studied in much of the prior document AI literature, the office\nautomation research should be extended to more realistic office tasks which\nrequire to integrate various information sources in the office system and\nproduce outputs through a series of decision-making processes. We introduce\nOfficeBench, one of the first office automation benchmarks for evaluating\ncurrent LLM agents' capability to address office tasks in realistic office\nworkflows. OfficeBench requires LLM agents to perform feasible long-horizon\nplanning, proficiently switch between applications in a timely manner, and\naccurately ground their actions within a large combined action space, based on\nthe contextual demands of the workflow. Applying our customized evaluation\nmethods on each task, we find that GPT-4 Omni achieves the highest pass rate of\n47.00%, demonstrating a decent performance in handling office tasks. However,\nthis is still far below the human performance and accuracy standards required\nby real-world office workflows. We further observe that most issues are related\nto operation redundancy and hallucinations, as well as limitations in switching\nbetween multiple applications, which may provide valuable insights for\ndeveloping effective agent frameworks for office automation.",
        "pos": [
            "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research."
        ],
        "neg": []
    },
    {
        "query": "Office automation significantly enhances human productivity by automatically\nfinishing routine tasks in the workflow. Beyond the basic information\nextraction studied in much of the prior document AI literature, the office\nautomation research should be extended to more realistic office tasks which\nrequire to integrate various information sources in the office system and\nproduce outputs through a series of decision-making processes. We introduce\nOfficeBench, one of the first office automation benchmarks for evaluating\ncurrent LLM agents' capability to address office tasks in realistic office\nworkflows. OfficeBench requires LLM agents to perform feasible long-horizon\nplanning, proficiently switch between applications in a timely manner, and\naccurately ground their actions within a large combined action space, based on\nthe contextual demands of the workflow. Applying our customized evaluation\nmethods on each task, we find that GPT-4 Omni achieves the highest pass rate of\n47.00%, demonstrating a decent performance in handling office tasks. However,\nthis is still far below the human performance and accuracy standards required\nby real-world office workflows. We further observe that most issues are related\nto operation redundancy and hallucinations, as well as limitations in switching\nbetween multiple applications, which may provide valuable insights for\ndeveloping effective agent frameworks for office automation.",
        "pos": [
            "Current evaluations of large language models (LLMs) often overlook\nnon-determinism, typically focusing on a single output per example. This limits\nour understanding of LLM performance variability in real-world applications.\nOur study addresses this issue by exploring key questions about the performance\ndifferences between greedy decoding and sampling, identifying benchmarks'\nconsistency regarding non-determinism, and examining unique model behaviors.\nThrough extensive experiments, we observe that greedy decoding generally\noutperforms sampling methods for most evaluated tasks. We also observe\nconsistent performance across different LLM sizes and alignment methods, noting\nthat alignment can reduce sampling variance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can match or surpass larger models such\nas GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This\nresearch shows the importance of considering non-determinism in LLM evaluations\nand provides insights for future LLM development and evaluation."
        ],
        "neg": []
    },
    {
        "query": "We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.",
        "pos": [
            "Physically-simulated models for human motion can generate high-quality\nresponsive character animations, often in real-time. Natural language serves as\na flexible interface for controlling these models, allowing expert and\nnon-expert users to quickly create and edit their animations. Many recent\nphysics-based animation methods, including those that use text interfaces,\ntrain control policies using reinforcement learning (RL). However, scaling\nthese methods beyond several hundred motions has remained challenging.\nMeanwhile, kinematic animation models are able to successfully learn from\nthousands of diverse motions by leveraging supervised learning methods.\nInspired by these successes, in this work we introduce SuperPADL, a scalable\nframework for physics-based text-to-motion that leverages both RL and\nsupervised learning to train controllers on thousands of diverse motion clips.\nSuperPADL is trained in stages using progressive distillation, starting with a\nlarge number of specialized experts using RL. These experts are then\niteratively distilled into larger, more robust policies using a combination of\nreinforcement learning and supervised learning. Our final SuperPADL controller\nis trained on a dataset containing over 5000 skills and runs in real time on a\nconsumer GPU. Moreover, our policy can naturally transition between skills,\nallowing for users to interactively craft multi-stage animations. We\nexperimentally demonstrate that SuperPADL significantly outperforms RL-based\nbaselines at this large data scale."
        ],
        "neg": []
    },
    {
        "query": "Autonomous agents that address day-to-day digital tasks (e.g., ordering\ngroceries for a household), must not only operate multiple apps (e.g., notes,\nmessaging, shopping app) via APIs, but also generate rich code with complex\ncontrol flow in an iterative manner based on their interaction with the\nenvironment. However, existing benchmarks for tool use are inadequate, as they\nonly cover tasks that require a simple sequence of API calls.\n  To remedy this gap, we built $\\textbf{AppWorld Engine}$, a high-quality\nexecution environment (60K lines of code) of 9 day-to-day apps operable via 457\nAPIs and populated with realistic digital activities simulating the lives of\n~100 fictitious users. We then created $\\textbf{AppWorld Benchmark}$ (40K lines\nof code), a suite of 750 natural, diverse, and challenging autonomous agent\ntasks requiring rich and interactive code generation. It supports robust\nprogrammatic evaluation with state-based unit tests, allowing for different\nways of completing a task while also checking for unexpected changes, i.e.,\ncollateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our\n'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least\n16% fewer. This highlights the benchmark's difficulty and AppWorld's potential\nto push the frontiers of interactive coding agents. The project website is\navailable at https://appworld.dev/.",
        "pos": [
            "Multiple-choice question answering (MCQA) is a key competence of performant\ntransformer language models that is tested by mainstream benchmarks. However,\nrecent evidence shows that models can have quite a range of performance,\nparticularly when the task format is diversified slightly (such as by shuffling\nanswer choice order). In this work we ask: how do successful models perform\nformatted MCQA? We employ vocabulary projection and activation patching methods\nto localize key hidden states that encode relevant information for predicting\nthe correct answer. We find that prediction of a specific answer symbol is\ncausally attributed to a single middle layer, and specifically its multi-head\nself-attention mechanism. We show that subsequent layers increase the\nprobability of the predicted answer symbol in vocabulary space, and that this\nprobability increase is associated with a sparse set of attention heads with\nunique roles. We additionally uncover differences in how different models\nadjust to alternative symbols. Finally, we demonstrate that a synthetic task\ncan disentangle sources of model error to pinpoint when a model has learned\nformatted MCQA, and show that an inability to separate answer symbol tokens in\nvocabulary space is a property of models unable to perform formatted MCQA\ntasks."
        ],
        "neg": []
    },
    {
        "query": "Autonomous agents that address day-to-day digital tasks (e.g., ordering\ngroceries for a household), must not only operate multiple apps (e.g., notes,\nmessaging, shopping app) via APIs, but also generate rich code with complex\ncontrol flow in an iterative manner based on their interaction with the\nenvironment. However, existing benchmarks for tool use are inadequate, as they\nonly cover tasks that require a simple sequence of API calls.\n  To remedy this gap, we built $\\textbf{AppWorld Engine}$, a high-quality\nexecution environment (60K lines of code) of 9 day-to-day apps operable via 457\nAPIs and populated with realistic digital activities simulating the lives of\n~100 fictitious users. We then created $\\textbf{AppWorld Benchmark}$ (40K lines\nof code), a suite of 750 natural, diverse, and challenging autonomous agent\ntasks requiring rich and interactive code generation. It supports robust\nprogrammatic evaluation with state-based unit tests, allowing for different\nways of completing a task while also checking for unexpected changes, i.e.,\ncollateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our\n'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least\n16% fewer. This highlights the benchmark's difficulty and AppWorld's potential\nto push the frontiers of interactive coding agents. The project website is\navailable at https://appworld.dev/.",
        "pos": [
            "It has become common practice now to use random initialization schemes,\nrather than the pre-trained embeddings, when training transformer based models\nfrom scratch. Indeed, we find that pre-trained word embeddings from GloVe, and\nsome sub-word embeddings extracted from language models such as T5 and mT5 fare\nmuch worse compared to random initialization. This is counter-intuitive given\nthe well-known representational and transfer-learning advantages of\npre-training. Interestingly, we also find that BERT and mBERT embeddings fare\nbetter than random initialization, showing the advantages of pre-trained\nrepresentations. In this work, we posit two potential factors that contribute\nto these mixed results: the model sensitivity to parameter distribution and the\nembedding interactions with position encodings. We observe that pre-trained\nGloVe, T5, and mT5 embeddings have a wider distribution of values. As argued in\nthe initialization studies, such large value initializations can lead to poor\ntraining because of saturated outputs. Further, the larger embedding values\ncan, in effect, absorb the smaller position encoding values when added\ntogether, thus losing position information. Standardizing the pre-trained\nembeddings to a narrow range (e.g. as prescribed by Xavier) leads to\nsubstantial gains for Glove, T5, and mT5 embeddings. On the other hand, BERT\npre-trained embeddings, while larger, are still relatively closer to Xavier\ninitialization range which may allow it to effectively transfer the pre-trained\nknowledge."
        ],
        "neg": []
    },
    {
        "query": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.",
        "pos": [
            "Modeling contextual information in a search session has drawn more and more\nattention when understanding complex user intents. Recent methods are all\ndata-driven, i.e., they train different models on large-scale search log data\nto identify the relevance between search contexts and candidate documents. The\ncommon training paradigm is to pair the search context with different candidate\ndocuments and train the model to rank the clicked documents higher than the\nunclicked ones. However, this paradigm neglects the symmetric nature of the\nrelevance between the session context and document, i.e., the clicked documents\ncan also be paired with different search contexts when training. In this work,\nwe propose query-oriented data augmentation to enrich search logs and empower\nthe modeling. We generate supplemental training pairs by altering the most\nimportant part of a search context, i.e., the current query, and train our\nmodel to rank the generated sequence along with the original sequence. This\napproach enables models to learn that the relevance of a document may vary as\nthe session context changes, leading to a better understanding of users' search\npatterns. We develop several strategies to alter the current query, resulting\nin new training data with varying degrees of difficulty. Through\nexperimentation on two extensive public search logs, we have successfully\ndemonstrated the effectiveness of our model."
        ],
        "neg": []
    },
    {
        "query": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.",
        "pos": [
            "Adapting general large language models (LLMs) to specialized domains presents\ngreat challenges due to varied data distributions. This adaptation typically\nrequires continual pre-training on massive domain-specific corpora to\nfacilitate knowledge memorization, followed by training to apply this knowledge\nfollowing human instructions and preferences. However, this method may result\nin inefficient knowledge memorization due to a lack of awareness of knowledge\nutilization and imposes substantial demands on LLMs to simultaneously learn\nknowledge utilization and format alignment with limited training samples. To\nfacilitate the domain adaptation of LLM, we revise this process and propose a\nnew domain adaptation framework including domain knowledge learning and general\nformat alignment, called Mix-CPT. Specifically, we first conduct a knowledge\nmixture continual pre-training that concurrently focuses on knowledge\nmemorization and utilization, allowing for mutual reinforcement. To avoid\ncatastrophic forgetting during the continual pre-training process, we further\nincorporate a logit swap self-distillation constraint. Subsequently, leveraging\nthe knowledge and capabilities acquired during continual pre-training, we\nefficiently perform instruction tuning and alignment with a few general\ntraining samples to achieve format alignment. Extensive experiments demonstrate\nthat our proposed Mix-CPT framework can simultaneously improve the task-solving\ncapabilities of LLMs on the target and general domains compared to the\ntraditional adaptation methods."
        ],
        "neg": []
    },
    {
        "query": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.",
        "pos": [
            "Adapting general large language models (LLMs) to specialized domains presents\ngreat challenges due to varied data distributions. This adaptation typically\nrequires continual pre-training on massive domain-specific corpora to\nfacilitate knowledge memorization, followed by training to apply this knowledge\nfollowing human instructions and preferences. However, this method may result\nin inefficient knowledge memorization due to a lack of awareness of knowledge\nutilization and imposes substantial demands on LLMs to simultaneously learn\nknowledge utilization and format alignment with limited training samples. To\nfacilitate the domain adaptation of LLM, we revise this process and propose a\nnew domain adaptation framework including domain knowledge learning and general\nformat alignment, called Mix-CPT. Specifically, we first conduct a knowledge\nmixture continual pre-training that concurrently focuses on knowledge\nmemorization and utilization, allowing for mutual reinforcement. To avoid\ncatastrophic forgetting during the continual pre-training process, we further\nincorporate a logit swap self-distillation constraint. Subsequently, leveraging\nthe knowledge and capabilities acquired during continual pre-training, we\nefficiently perform instruction tuning and alignment with a few general\ntraining samples to achieve format alignment. Extensive experiments demonstrate\nthat our proposed Mix-CPT framework can simultaneously improve the task-solving\ncapabilities of LLMs on the target and general domains compared to the\ntraditional adaptation methods."
        ],
        "neg": []
    },
    {
        "query": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.",
        "pos": [
            "Modeling contextual information in a search session has drawn more and more\nattention when understanding complex user intents. Recent methods are all\ndata-driven, i.e., they train different models on large-scale search log data\nto identify the relevance between search contexts and candidate documents. The\ncommon training paradigm is to pair the search context with different candidate\ndocuments and train the model to rank the clicked documents higher than the\nunclicked ones. However, this paradigm neglects the symmetric nature of the\nrelevance between the session context and document, i.e., the clicked documents\ncan also be paired with different search contexts when training. In this work,\nwe propose query-oriented data augmentation to enrich search logs and empower\nthe modeling. We generate supplemental training pairs by altering the most\nimportant part of a search context, i.e., the current query, and train our\nmodel to rank the generated sequence along with the original sequence. This\napproach enables models to learn that the relevance of a document may vary as\nthe session context changes, leading to a better understanding of users' search\npatterns. We develop several strategies to alter the current query, resulting\nin new training data with varying degrees of difficulty. Through\nexperimentation on two extensive public search logs, we have successfully\ndemonstrated the effectiveness of our model."
        ],
        "neg": []
    },
    {
        "query": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.",
        "pos": [
            "Adapting general large language models (LLMs) to specialized domains presents\ngreat challenges due to varied data distributions. This adaptation typically\nrequires continual pre-training on massive domain-specific corpora to\nfacilitate knowledge memorization, followed by training to apply this knowledge\nfollowing human instructions and preferences. However, this method may result\nin inefficient knowledge memorization due to a lack of awareness of knowledge\nutilization and imposes substantial demands on LLMs to simultaneously learn\nknowledge utilization and format alignment with limited training samples. To\nfacilitate the domain adaptation of LLM, we revise this process and propose a\nnew domain adaptation framework including domain knowledge learning and general\nformat alignment, called Mix-CPT. Specifically, we first conduct a knowledge\nmixture continual pre-training that concurrently focuses on knowledge\nmemorization and utilization, allowing for mutual reinforcement. To avoid\ncatastrophic forgetting during the continual pre-training process, we further\nincorporate a logit swap self-distillation constraint. Subsequently, leveraging\nthe knowledge and capabilities acquired during continual pre-training, we\nefficiently perform instruction tuning and alignment with a few general\ntraining samples to achieve format alignment. Extensive experiments demonstrate\nthat our proposed Mix-CPT framework can simultaneously improve the task-solving\ncapabilities of LLMs on the target and general domains compared to the\ntraditional adaptation methods.",
            "Modeling contextual information in a search session has drawn more and more\nattention when understanding complex user intents. Recent methods are all\ndata-driven, i.e., they train different models on large-scale search log data\nto identify the relevance between search contexts and candidate documents. The\ncommon training paradigm is to pair the search context with different candidate\ndocuments and train the model to rank the clicked documents higher than the\nunclicked ones. However, this paradigm neglects the symmetric nature of the\nrelevance between the session context and document, i.e., the clicked documents\ncan also be paired with different search contexts when training. In this work,\nwe propose query-oriented data augmentation to enrich search logs and empower\nthe modeling. We generate supplemental training pairs by altering the most\nimportant part of a search context, i.e., the current query, and train our\nmodel to rank the generated sequence along with the original sequence. This\napproach enables models to learn that the relevance of a document may vary as\nthe session context changes, leading to a better understanding of users' search\npatterns. We develop several strategies to alter the current query, resulting\nin new training data with varying degrees of difficulty. Through\nexperimentation on two extensive public search logs, we have successfully\ndemonstrated the effectiveness of our model."
        ],
        "neg": []
    },
    {
        "query": "The deployment of language models brings challenges in generating reliable\ninformation, especially when these models are fine-tuned using human\npreferences. To extract encoded knowledge without (potentially) biased human\nlabels, unsupervised probing techniques like Contrast-Consistent Search (CCS)\nhave been developed (Burns et al., 2022). However, salient but unrelated\nfeatures in a given dataset can mislead these probes (Farquhar et al., 2023).\nAddressing this, we propose a cluster normalization method to minimize the\nimpact of such features by clustering and normalizing activations of contrast\npairs before applying unsupervised probing techniques. While this approach does\nnot address the issue of differentiating between knowledge in general and\nsimulated knowledge - a major issue in the literature of latent knowledge\nelicitation (Christiano et al., 2021) - it significantly improves the ability\nof unsupervised probes to identify the intended knowledge amidst distractions.",
        "pos": [
            "Are large language models (LLMs) biased towards text generated by LLMs over\ntext authored by humans, leading to possible anti-human bias? Utilizing a\nclassical experimental design inspired by employment discrimination studies, we\ntested widely-used LLMs, including GPT-3.5 and GPT4, in binary-choice\nscenarios. These involved LLM-based agents selecting between products and\nacademic papers described either by humans or LLMs under identical conditions.\nOur results show a consistent tendency for LLM-based AIs to prefer\nLLM-generated content. This suggests the possibility of AI systems implicitly\ndiscriminating against humans, giving AI agents an unfair advantage."
        ],
        "neg": []
    },
    {
        "query": "This paper tackles a key issue in the interpretation of scientific figures:\nthe fine-grained alignment of text and figures. It advances beyond prior\nresearch that primarily dealt with straightforward, data-driven visualizations\nsuch as bar and pie charts and only offered a basic understanding of diagrams\nthrough captioning and classification. We introduce a novel task, Figure\nIntegrity Verification, designed to evaluate the precision of technologies in\naligning textual knowledge with visual elements in scientific figures. To\nsupport this, we develop a semi-automated method for constructing a large-scale\ndataset, Figure-seg, specifically designed for this task. Additionally, we\npropose an innovative framework, Every Part Matters (EPM), which leverages\nMultimodal Large Language Models (MLLMs) to not only incrementally improve the\nalignment and verification of text-figure integrity but also enhance integrity\nthrough analogical reasoning. Our comprehensive experiments show that these\ninnovations substantially improve upon existing methods, allowing for more\nprecise and thorough analysis of complex scientific figures. This progress not\nonly enhances our understanding of multimodal technologies but also stimulates\nfurther research and practical applications across fields requiring the\naccurate interpretation of complex visual data.",
        "pos": [
            "Retrieval-Augmented Generation (RAG) is applied to solve hallucination\nproblems and real-time constraints of large language models, but it also\ninduces vulnerabilities against retrieval corruption attacks. Existing research\nmainly explores the unreliability of RAG in white-box and closed-domain QA\ntasks. In this paper, we aim to reveal the vulnerabilities of\nRetrieval-Enhanced Generative (RAG) models when faced with black-box attacks\nfor opinion manipulation. We explore the impact of such attacks on user\ncognition and decision-making, providing new insight to enhance the reliability\nand security of RAG models. We manipulate the ranking results of the retrieval\nmodel in RAG with instruction and use these results as data to train a\nsurrogate model. By employing adversarial retrieval attack methods to the\nsurrogate model, black-box transfer attacks on RAG are further realized.\nExperiments conducted on opinion datasets across multiple topics show that the\nproposed attack strategy can significantly alter the opinion polarity of the\ncontent generated by RAG. This demonstrates the model's vulnerability and, more\nimportantly, reveals the potential negative impact on user cognition and\ndecision-making, making it easier to mislead users into accepting incorrect or\nbiased information."
        ],
        "neg": []
    },
    {
        "query": "This paper tackles a key issue in the interpretation of scientific figures:\nthe fine-grained alignment of text and figures. It advances beyond prior\nresearch that primarily dealt with straightforward, data-driven visualizations\nsuch as bar and pie charts and only offered a basic understanding of diagrams\nthrough captioning and classification. We introduce a novel task, Figure\nIntegrity Verification, designed to evaluate the precision of technologies in\naligning textual knowledge with visual elements in scientific figures. To\nsupport this, we develop a semi-automated method for constructing a large-scale\ndataset, Figure-seg, specifically designed for this task. Additionally, we\npropose an innovative framework, Every Part Matters (EPM), which leverages\nMultimodal Large Language Models (MLLMs) to not only incrementally improve the\nalignment and verification of text-figure integrity but also enhance integrity\nthrough analogical reasoning. Our comprehensive experiments show that these\ninnovations substantially improve upon existing methods, allowing for more\nprecise and thorough analysis of complex scientific figures. This progress not\nonly enhances our understanding of multimodal technologies but also stimulates\nfurther research and practical applications across fields requiring the\naccurate interpretation of complex visual data.",
        "pos": [
            "Effective training of language models (LMs) for mathematical reasoning tasks\ndemands high-quality supervised fine-tuning data. Besides obtaining annotations\nfrom human experts, a common alternative is sampling from larger and more\npowerful LMs. However, this knowledge distillation approach can be costly and\nunstable, particularly when relying on closed-source, proprietary LMs like\nGPT-4, whose behaviors are often unpredictable. In this work, we demonstrate\nthat the reasoning abilities of small-scale LMs can be enhanced through\nself-training, a process where models learn from their own outputs. We also\nshow that the conventional self-training can be further augmented by a\npreference learning algorithm called Direct Preference Optimization (DPO). By\nintegrating DPO into self-training, we leverage preference data to guide LMs\ntowards more accurate and diverse chain-of-thought reasoning. We evaluate our\nmethod across various mathematical reasoning tasks using different base models.\nOur experiments show that this approach not only improves LMs' reasoning\nperformance but also offers a more cost-effective and scalable solution\ncompared to relying on large proprietary LMs.",
            "Retrieval-Augmented Generation (RAG) is applied to solve hallucination\nproblems and real-time constraints of large language models, but it also\ninduces vulnerabilities against retrieval corruption attacks. Existing research\nmainly explores the unreliability of RAG in white-box and closed-domain QA\ntasks. In this paper, we aim to reveal the vulnerabilities of\nRetrieval-Enhanced Generative (RAG) models when faced with black-box attacks\nfor opinion manipulation. We explore the impact of such attacks on user\ncognition and decision-making, providing new insight to enhance the reliability\nand security of RAG models. We manipulate the ranking results of the retrieval\nmodel in RAG with instruction and use these results as data to train a\nsurrogate model. By employing adversarial retrieval attack methods to the\nsurrogate model, black-box transfer attacks on RAG are further realized.\nExperiments conducted on opinion datasets across multiple topics show that the\nproposed attack strategy can significantly alter the opinion polarity of the\ncontent generated by RAG. This demonstrates the model's vulnerability and, more\nimportantly, reveals the potential negative impact on user cognition and\ndecision-making, making it easier to mislead users into accepting incorrect or\nbiased information."
        ],
        "neg": []
    },
    {
        "query": "Named entity recognition (NER) models often struggle with noisy inputs, such\nas those with spelling mistakes or errors generated by Optical Character\nRecognition processes, and learning a robust NER model is challenging. Existing\nrobust NER models utilize both noisy text and its corresponding gold text for\ntraining, which is infeasible in many real-world applications in which gold\ntext is not available. In this paper, we consider a more realistic setting in\nwhich only noisy text and its NER labels are available. We propose to retrieve\nrelevant text of the noisy text from a knowledge corpus and use it to enhance\nthe representation of the original noisy input. We design three retrieval\nmethods: sparse retrieval based on lexicon similarity, dense retrieval based on\nsemantic similarity, and self-retrieval based on task-specific text. After\nretrieving relevant text, we concatenate the retrieved text with the original\nnoisy text and encode them with a transformer network, utilizing self-attention\nto enhance the contextual token representations of the noisy text using the\nretrieved text. We further employ a multi-view training framework that improves\nrobust NER without retrieving text during inference. Experiments show that our\nretrieval-augmented model achieves significant improvements in various noisy\nNER settings.",
        "pos": [
            "Syntactic Transformer language models aim to achieve better generalization\nthrough simultaneously modeling syntax trees and sentences. While prior work\nhas been focusing on adding constituency-based structures to Transformers, we\nintroduce Dependency Transformer Grammars (DTGs), a new class of Transformer\nlanguage model with explicit dependency-based inductive bias. DTGs simulate\ndependency transition systems with constrained attention patterns by modifying\nattention masks, incorporate the stack information through relative positional\nencoding, and augment dependency arc representation with a combination of token\nembeddings and operation embeddings. When trained on a dataset of sentences\nannotated with dependency trees, DTGs achieve better generalization while\nmaintaining comparable perplexity with Transformer language model baselines.\nDTGs also outperform recent constituency-based models, showing that dependency\ncan better guide Transformer language models. Our code is released at\nhttps://github.com/zhaoyd1/Dep_Transformer_Grammars."
        ],
        "neg": []
    },
    {
        "query": "Based on the WASSA 2022 Shared Task on Empathy Detection and Emotion\nClassification, we predict the level of empathic concern and personal distress\ndisplayed in essays. For the first stage of this project we implemented a\nFeed-Forward Neural Network using sentence-level embeddings as features. We\nexperimented with four different embedding models for generating the inputs to\nthe neural network. The subsequent stage builds upon the previous work and we\nhave implemented three types of revisions. The first revision focuses on the\nenhancements to the model architecture and the training approach. The second\nrevision focuses on handling class imbalance using stratified data sampling.\nThe third revision focuses on leveraging lexical resources, where we apply four\ndifferent resources to enrich the features associated with the dataset. During\nthe final stage of this project, we have created the final end-to-end system\nfor the primary task using an ensemble of models to revise primary task\nperformance. Additionally, as part of the final stage, these approaches have\nbeen adapted to the WASSA 2023 Shared Task on Empathy Emotion and Personality\nDetection in Interactions, in which the empathic concern, emotion polarity, and\nemotion intensity in dyadic text conversations are predicted.",
        "pos": [
            "We introduce new dataset 'CORD-19-Vaccination' to cater to scientists\nspecifically looking into COVID-19 vaccine-related research. This dataset is\nextracted from CORD-19 dataset [Wang et al., 2020] and augmented with new\ncolumns for language detail, author demography, keywords, and topic per paper.\nFacebook's fastText model is used to identify languages [Joulin et al., 2016].\nTo establish author demography (author affiliation, lab/institution location,\nand lab/institution country columns) we processed the JSON file for each paper\nand then further enhanced using Google's search API to determine country\nvalues. 'Yake' was used to extract keywords from the title, abstract, and body\nof each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to\nadd topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset,\nwe demonstrate a question-answering task like the one used in the CORD-19\nKaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential\nsentence classification was performed on each paper's abstract using the model\nfrom Dernoncourt et al. [2016]. We partially hand annotated the training\ndataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination'\ncontains 30k research papers and can be immensely valuable for NLP research\nsuch as text mining, information extraction, and question answering, specific\nto the domain of COVID-19 vaccine research."
        ],
        "neg": []
    },
    {
        "query": "Based on the WASSA 2022 Shared Task on Empathy Detection and Emotion\nClassification, we predict the level of empathic concern and personal distress\ndisplayed in essays. For the first stage of this project we implemented a\nFeed-Forward Neural Network using sentence-level embeddings as features. We\nexperimented with four different embedding models for generating the inputs to\nthe neural network. The subsequent stage builds upon the previous work and we\nhave implemented three types of revisions. The first revision focuses on the\nenhancements to the model architecture and the training approach. The second\nrevision focuses on handling class imbalance using stratified data sampling.\nThe third revision focuses on leveraging lexical resources, where we apply four\ndifferent resources to enrich the features associated with the dataset. During\nthe final stage of this project, we have created the final end-to-end system\nfor the primary task using an ensemble of models to revise primary task\nperformance. Additionally, as part of the final stage, these approaches have\nbeen adapted to the WASSA 2023 Shared Task on Empathy Emotion and Personality\nDetection in Interactions, in which the empathic concern, emotion polarity, and\nemotion intensity in dyadic text conversations are predicted.",
        "pos": [
            "We introduce new dataset 'CORD-19-Vaccination' to cater to scientists\nspecifically looking into COVID-19 vaccine-related research. This dataset is\nextracted from CORD-19 dataset [Wang et al., 2020] and augmented with new\ncolumns for language detail, author demography, keywords, and topic per paper.\nFacebook's fastText model is used to identify languages [Joulin et al., 2016].\nTo establish author demography (author affiliation, lab/institution location,\nand lab/institution country columns) we processed the JSON file for each paper\nand then further enhanced using Google's search API to determine country\nvalues. 'Yake' was used to extract keywords from the title, abstract, and body\nof each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to\nadd topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset,\nwe demonstrate a question-answering task like the one used in the CORD-19\nKaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential\nsentence classification was performed on each paper's abstract using the model\nfrom Dernoncourt et al. [2016]. We partially hand annotated the training\ndataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination'\ncontains 30k research papers and can be immensely valuable for NLP research\nsuch as text mining, information extraction, and question answering, specific\nto the domain of COVID-19 vaccine research."
        ],
        "neg": []
    },
    {
        "query": "Based on the WASSA 2022 Shared Task on Empathy Detection and Emotion\nClassification, we predict the level of empathic concern and personal distress\ndisplayed in essays. For the first stage of this project we implemented a\nFeed-Forward Neural Network using sentence-level embeddings as features. We\nexperimented with four different embedding models for generating the inputs to\nthe neural network. The subsequent stage builds upon the previous work and we\nhave implemented three types of revisions. The first revision focuses on the\nenhancements to the model architecture and the training approach. The second\nrevision focuses on handling class imbalance using stratified data sampling.\nThe third revision focuses on leveraging lexical resources, where we apply four\ndifferent resources to enrich the features associated with the dataset. During\nthe final stage of this project, we have created the final end-to-end system\nfor the primary task using an ensemble of models to revise primary task\nperformance. Additionally, as part of the final stage, these approaches have\nbeen adapted to the WASSA 2023 Shared Task on Empathy Emotion and Personality\nDetection in Interactions, in which the empathic concern, emotion polarity, and\nemotion intensity in dyadic text conversations are predicted.",
        "pos": [
            "We introduce new dataset 'CORD-19-Vaccination' to cater to scientists\nspecifically looking into COVID-19 vaccine-related research. This dataset is\nextracted from CORD-19 dataset [Wang et al., 2020] and augmented with new\ncolumns for language detail, author demography, keywords, and topic per paper.\nFacebook's fastText model is used to identify languages [Joulin et al., 2016].\nTo establish author demography (author affiliation, lab/institution location,\nand lab/institution country columns) we processed the JSON file for each paper\nand then further enhanced using Google's search API to determine country\nvalues. 'Yake' was used to extract keywords from the title, abstract, and body\nof each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to\nadd topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset,\nwe demonstrate a question-answering task like the one used in the CORD-19\nKaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential\nsentence classification was performed on each paper's abstract using the model\nfrom Dernoncourt et al. [2016]. We partially hand annotated the training\ndataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination'\ncontains 30k research papers and can be immensely valuable for NLP research\nsuch as text mining, information extraction, and question answering, specific\nto the domain of COVID-19 vaccine research."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\nincreasing demands of application scenarios have driven the evolution of RAG,\nleading to the integration of advanced retrievers, LLMs and other complementary\ntechnologies, which in turn has amplified the intricacy of RAG systems.\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\nwith many methods struggling to be unified under the process of\n\"retrieve-then-generate\". In this context, this paper examines the limitations\nof the existing RAG paradigm and introduces the modular RAG framework. By\ndecomposing complex RAG systems into independent modules and specialized\noperators, it facilitates a highly reconfigurable framework. Modular RAG\ntranscends the traditional linear architecture, embracing a more advanced\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\nextensive research, this paper further identifies prevalent RAG\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\nanalysis of their respective implementation nuances. Modular RAG presents\ninnovative opportunities for the conceptualization and deployment of RAG\nsystems. Finally, the paper explores the potential emergence of new operators\nand paradigms, establishing a solid theoretical foundation and a practical\nroadmap for the continued evolution and practical deployment of RAG\ntechnologies.",
        "pos": [
            "Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising generation, evidence judgment, and refinement. We augment this\nprocess with real-world psychological case backgrounds extracted from online\nplatforms, enhancing the relevance and applicability of the generated data.\nAdditionally, to compare the performance of PsycoLLM with other LLMs, we\ndevelop a comprehensive psychological benchmark based on authoritative\npsychological counseling examinations in China, which includes assessments of\nprofessional ethics, theoretical proficiency, and case analysis. The\nexperimental results on the benchmark illustrates the effectiveness of\nPsycoLLM, which demonstrates superior performance compared to other LLMs."
        ],
        "neg": []
    },
    {
        "query": "As a branch of advanced artificial intelligence, dialogue systems are\nprospering. Multi-turn response selection is a general research problem in\ndialogue systems. With the assistance of background information and pre-trained\nlanguage models, the performance of state-of-the-art methods on this problem\ngains impressive improvement. However, existing studies neglect the importance\nof external commonsense knowledge. Hence, we design a Siamese network where a\npre-trained Language model merges with a Graph neural network (SinLG). SinLG\ntakes advantage of Pre-trained Language Models (PLMs) to catch the word\ncorrelations in the context and response candidates and utilizes a Graph Neural\nNetwork (GNN) to reason helpful common sense from an external knowledge graph.\nThe GNN aims to assist the PLM in fine-tuning, and arousing its related\nmemories to attain better performance. Specifically, we first extract related\nconcepts as nodes from an external knowledge graph to construct a subgraph with\nthe context response pair as a super node for each sample. Next, we learn two\nrepresentations for the context response pair via both the PLM and GNN. A\nsimilarity loss between the two representations is utilized to transfer the\ncommonsense knowledge from the GNN to the PLM. Then only the PLM is used to\ninfer online so that efficiency can be guaranteed. Finally, we conduct\nextensive experiments on two variants of the PERSONA-CHAT dataset, which proves\nthat our solution can not only improve the performance of the PLM but also\nachieve an efficient inference.",
        "pos": [
            "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain."
        ],
        "neg": []
    },
    {
        "query": "Dysarthric speech recognition (DSR) presents a formidable challenge due to\ninherent inter-speaker variability, leading to severe performance degradation\nwhen applying DSR models to new dysarthric speakers. Traditional speaker\nadaptation methodologies typically involve fine-tuning models for each speaker,\nbut this strategy is cost-prohibitive and inconvenient for disabled users,\nrequiring substantial data collection. To address this issue, we introduce a\nprototype-based approach that markedly improves DSR performance for unseen\ndysarthric speakers without additional fine-tuning. Our method employs a\nfeature extractor trained with HuBERT to produce per-word prototypes that\nencapsulate the characteristics of previously unseen speakers. These prototypes\nserve as the basis for classification. Additionally, we incorporate supervised\ncontrastive learning to refine feature extraction. By enhancing representation\nquality, we further improve DSR performance, enabling effective personalized\nDSR. We release our code at https://github.com/NKU-HLT/PB-DSR.",
        "pos": [
            "Recent advancements in LLMs have showcased their remarkable role-playing\ncapabilities, able to accurately simulate the dialogue styles and cognitive\nprocesses of various roles based on different instructions and contexts.\nStudies indicate that assigning LLMs the roles of experts, a strategy known as\nrole-play prompting, can enhance their performance in the corresponding\ndomains. However, the prompt needs to be manually designed for the given\nproblem, requiring certain expertise and iterative modifications. To this end,\nwe propose self-prompt tuning, making LLMs themselves generate role-play\nprompts through fine-tuning. Leveraging the LIMA dataset as our foundational\ncorpus, we employ GPT-4 to annotate role-play prompts for each data points,\nresulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like\nLlama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned\nLLMs can automatically generate expert role prompts for any given question. We\nextensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and\nopen-ended question test. Our empirical results illustrate that self-prompt\ntuned LLMs outperform standard instruction tuned baselines across most\ndatasets. This highlights the great potential of utilizing fine-tuning to\nenable LLMs to self-prompt, thereby automating complex prompting strategies. We\nrelease the dataset, models, and code at this\n\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}."
        ],
        "neg": []
    },
    {
        "query": "Dysarthric speech recognition (DSR) presents a formidable challenge due to\ninherent inter-speaker variability, leading to severe performance degradation\nwhen applying DSR models to new dysarthric speakers. Traditional speaker\nadaptation methodologies typically involve fine-tuning models for each speaker,\nbut this strategy is cost-prohibitive and inconvenient for disabled users,\nrequiring substantial data collection. To address this issue, we introduce a\nprototype-based approach that markedly improves DSR performance for unseen\ndysarthric speakers without additional fine-tuning. Our method employs a\nfeature extractor trained with HuBERT to produce per-word prototypes that\nencapsulate the characteristics of previously unseen speakers. These prototypes\nserve as the basis for classification. Additionally, we incorporate supervised\ncontrastive learning to refine feature extraction. By enhancing representation\nquality, we further improve DSR performance, enabling effective personalized\nDSR. We release our code at https://github.com/NKU-HLT/PB-DSR.",
        "pos": [
            "Recent advancements in LLMs have showcased their remarkable role-playing\ncapabilities, able to accurately simulate the dialogue styles and cognitive\nprocesses of various roles based on different instructions and contexts.\nStudies indicate that assigning LLMs the roles of experts, a strategy known as\nrole-play prompting, can enhance their performance in the corresponding\ndomains. However, the prompt needs to be manually designed for the given\nproblem, requiring certain expertise and iterative modifications. To this end,\nwe propose self-prompt tuning, making LLMs themselves generate role-play\nprompts through fine-tuning. Leveraging the LIMA dataset as our foundational\ncorpus, we employ GPT-4 to annotate role-play prompts for each data points,\nresulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like\nLlama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned\nLLMs can automatically generate expert role prompts for any given question. We\nextensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and\nopen-ended question test. Our empirical results illustrate that self-prompt\ntuned LLMs outperform standard instruction tuned baselines across most\ndatasets. This highlights the great potential of utilizing fine-tuning to\nenable LLMs to self-prompt, thereby automating complex prompting strategies. We\nrelease the dataset, models, and code at this\n\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}."
        ],
        "neg": []
    },
    {
        "query": "Dysarthric speech recognition (DSR) presents a formidable challenge due to\ninherent inter-speaker variability, leading to severe performance degradation\nwhen applying DSR models to new dysarthric speakers. Traditional speaker\nadaptation methodologies typically involve fine-tuning models for each speaker,\nbut this strategy is cost-prohibitive and inconvenient for disabled users,\nrequiring substantial data collection. To address this issue, we introduce a\nprototype-based approach that markedly improves DSR performance for unseen\ndysarthric speakers without additional fine-tuning. Our method employs a\nfeature extractor trained with HuBERT to produce per-word prototypes that\nencapsulate the characteristics of previously unseen speakers. These prototypes\nserve as the basis for classification. Additionally, we incorporate supervised\ncontrastive learning to refine feature extraction. By enhancing representation\nquality, we further improve DSR performance, enabling effective personalized\nDSR. We release our code at https://github.com/NKU-HLT/PB-DSR.",
        "pos": [
            "Recent advancements in LLMs have showcased their remarkable role-playing\ncapabilities, able to accurately simulate the dialogue styles and cognitive\nprocesses of various roles based on different instructions and contexts.\nStudies indicate that assigning LLMs the roles of experts, a strategy known as\nrole-play prompting, can enhance their performance in the corresponding\ndomains. However, the prompt needs to be manually designed for the given\nproblem, requiring certain expertise and iterative modifications. To this end,\nwe propose self-prompt tuning, making LLMs themselves generate role-play\nprompts through fine-tuning. Leveraging the LIMA dataset as our foundational\ncorpus, we employ GPT-4 to annotate role-play prompts for each data points,\nresulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like\nLlama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned\nLLMs can automatically generate expert role prompts for any given question. We\nextensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and\nopen-ended question test. Our empirical results illustrate that self-prompt\ntuned LLMs outperform standard instruction tuned baselines across most\ndatasets. This highlights the great potential of utilizing fine-tuning to\nenable LLMs to self-prompt, thereby automating complex prompting strategies. We\nrelease the dataset, models, and code at this\n\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}."
        ],
        "neg": []
    },
    {
        "query": "While the abundance of rich and vast datasets across numerous fields has\nfacilitated the advancement of natural language processing, sectors in need of\nspecialized data types continue to struggle with the challenge of finding\nquality data. Our study introduces a novel guidance data augmentation technique\nutilizing abstracted context and sentence structures to produce varied\nsentences while maintaining context-entity relationships, addressing data\nscarcity challenges. By fostering a closer relationship between context,\nsentence structure, and role of entities, our method enhances data\naugmentation's effectiveness. Consequently, by showcasing diversification in\nboth entity-related vocabulary and overall sentence structure, and\nsimultaneously improving the training performance of named entity recognition\ntask.",
        "pos": [
            "Recent advancements in large language models have heavily relied on the large\nreward model from reinforcement learning from human feedback for fine-tuning.\nHowever, the use of a single reward model across various domains may not always\nbe optimal, often requiring retraining from scratch when new domain data is\nintroduced. To address these challenges, we explore the utilization of small\nlanguage models operating in a domain-specific manner based on router\nmechanisms. Our three approaches are: 1) utilize mixture of experts to form a\nsingle reward model by modularizing an internal router and experts, 2)\nemploying external router to select the appropriate reward model from multiple\ndomain-specific models, and 3) the framework reduces parameter size by loading\nreward models and router adapters onto a single small language model using\nadapters. Experimental validation underscores the effectiveness of our\napproach, demonstrating performance comparable to baseline methods while also\nreducing the total parameter size."
        ],
        "neg": []
    },
    {
        "query": "While the abundance of rich and vast datasets across numerous fields has\nfacilitated the advancement of natural language processing, sectors in need of\nspecialized data types continue to struggle with the challenge of finding\nquality data. Our study introduces a novel guidance data augmentation technique\nutilizing abstracted context and sentence structures to produce varied\nsentences while maintaining context-entity relationships, addressing data\nscarcity challenges. By fostering a closer relationship between context,\nsentence structure, and role of entities, our method enhances data\naugmentation's effectiveness. Consequently, by showcasing diversification in\nboth entity-related vocabulary and overall sentence structure, and\nsimultaneously improving the training performance of named entity recognition\ntask.",
        "pos": [
            "Recent advancements in large language models have heavily relied on the large\nreward model from reinforcement learning from human feedback for fine-tuning.\nHowever, the use of a single reward model across various domains may not always\nbe optimal, often requiring retraining from scratch when new domain data is\nintroduced. To address these challenges, we explore the utilization of small\nlanguage models operating in a domain-specific manner based on router\nmechanisms. Our three approaches are: 1) utilize mixture of experts to form a\nsingle reward model by modularizing an internal router and experts, 2)\nemploying external router to select the appropriate reward model from multiple\ndomain-specific models, and 3) the framework reduces parameter size by loading\nreward models and router adapters onto a single small language model using\nadapters. Experimental validation underscores the effectiveness of our\napproach, demonstrating performance comparable to baseline methods while also\nreducing the total parameter size.",
            "Syntactic elements, such as word order and case markers, are fundamental in\nnatural language processing. Recent studies show that syntactic information\nboosts language model performance and offers clues for people to understand\ntheir learning mechanisms. Unlike languages with a fixed word order such as\nEnglish, Korean allows for varied word sequences, despite its canonical\nstructure, due to case markers that indicate the functions of sentence\ncomponents. This study explores whether Korean language models can accurately\ncapture this flexibility. We note that incomplete word orders and omitted case\nmarkers frequently appear in ordinary Korean communication. To investigate this\nfurther, we introduce the Syntactically Incomplete Korean (SIKO) dataset.\nThrough SIKO, we assessed Korean language models' flexibility with incomplete\nsyntax and confirmed the dataset's training value. Results indicate these\nmodels reflect Korean's inherent flexibility, accurately handling incomplete\ninputs. Moreover, fine-tuning with SIKO enhances the ability to handle common\nincomplete Korean syntactic forms. The dataset's simple construction process,\ncoupled with significant performance enhancements, solidifies its standing as\nan effective data augmentation technique."
        ],
        "neg": []
    },
    {
        "query": "Synthetic data has become an important tool in the fine-tuning of language\nmodels to follow instructions and solve complex problems. Nevertheless, the\nmajority of open data to date is often lacking multi-turn data and collected on\nclosed models, limiting progress on advancing open fine-tuning methods. We\nintroduce Self Directed Synthetic Dialogues (SDSD), an experimental dataset\nconsisting of guided conversations of language models talking to themselves.\nThe dataset consists of multi-turn conversations generated with DBRX, Llama 2\n70B, and Mistral Large, all instructed to follow a conversation plan generated\nprior to the conversation. We also explore including principles from\nConstitutional AI and other related works to create synthetic preference data\nvia revisions to the final conversation turn. We hope this work encourages\nfurther exploration in multi-turn data and the use of open models for expanding\nthe impact of synthetic data.",
        "pos": [
            "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research."
        ],
        "neg": []
    },
    {
        "query": "Synthetic data has become an important tool in the fine-tuning of language\nmodels to follow instructions and solve complex problems. Nevertheless, the\nmajority of open data to date is often lacking multi-turn data and collected on\nclosed models, limiting progress on advancing open fine-tuning methods. We\nintroduce Self Directed Synthetic Dialogues (SDSD), an experimental dataset\nconsisting of guided conversations of language models talking to themselves.\nThe dataset consists of multi-turn conversations generated with DBRX, Llama 2\n70B, and Mistral Large, all instructed to follow a conversation plan generated\nprior to the conversation. We also explore including principles from\nConstitutional AI and other related works to create synthetic preference data\nvia revisions to the final conversation turn. We hope this work encourages\nfurther exploration in multi-turn data and the use of open models for expanding\nthe impact of synthetic data.",
        "pos": [
            "The rapid advancement of language models (LMs) necessitates robust alignment\nwith diverse user values. However, current preference optimization approaches\noften fail to capture the plurality of user opinions, instead reinforcing\nmajority viewpoints and marginalizing minority perspectives. We introduce\nPERSONA, a reproducible test bed designed to evaluate and improve pluralistic\nalignment of LMs. We procedurally generate diverse user profiles from US census\ndata, resulting in 1,586 synthetic personas with varied demographic and\nidiosyncratic attributes. We then generate a large-scale evaluation dataset\ncontaining 3,868 prompts and 317,200 feedback pairs obtained from our synthetic\npersonas. Leveraging this dataset, we systematically evaluate LM capabilities\nin role-playing diverse users, verified through human judges, and the\nestablishment of both a benchmark, PERSONA Bench, for pluralistic alignment\napproaches as well as an extensive dataset to create new and future benchmarks.\nThe full dataset and benchmarks are available here:\nhttps://www.synthlabs.ai/research/persona."
        ],
        "neg": []
    },
    {
        "query": "Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future work, centered\naround whether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, while still providing opportunities to optimize\nabstention abilities based on context.",
        "pos": [
            "In multilingual settings, non-Latin scripts and low-resource languages are\nusually disadvantaged in terms of language models' utility, efficiency, and\ncost. Specifically, previous studies have reported multiple modeling biases\nthat the current tokenization algorithms introduce to non-Latin script\nlanguages, the main one being over-segmentation. In this work, we propose\nMAGNET; multilingual adaptive gradient-based tokenization to reduce\nover-segmentation via adaptive gradient-based subword tokenization. MAGNET\nlearns to predict segment boundaries between byte tokens in a sequence via\nsub-modules within the model, which act as internal boundary predictors\n(tokenizers). Previous gradient-based tokenization methods aimed for uniform\ncompression across sequences by integrating a single boundary predictor during\ntraining and optimizing it end-to-end through stochastic reparameterization\nalongside the next token prediction objective. However, this approach still\nresults in over-segmentation for non-Latin script languages in multilingual\nsettings. In contrast, MAGNET offers a customizable architecture where\nbyte-level sequences are routed through language-script-specific predictors,\neach optimized for its respective language script. This modularity enforces\nequitable segmentation granularity across different language scripts compared\nto previous methods. Through extensive experiments, we demonstrate that in\naddition to reducing segmentation disparities, MAGNET also enables faster\nlanguage modelling and improves downstream utility.",
            "This study introduces ValueScope, a framework leveraging language models to\nquantify social norms and values within online communities, grounded in social\nscience perspectives on normative structures. We employ ValueScope to dissect\nand analyze linguistic and stylistic expressions across 13 Reddit communities\ncategorized under gender, politics, science, and finance. Our analysis provides\na quantitative foundation showing that even closely related communities exhibit\nremarkably diverse norms. This diversity supports existing theories and adds a\nnew dimension--community preference--to understanding community interactions.\nValueScope not only delineates differing social norms among communities but\nalso effectively traces their evolution and the influence of significant\nexternal events like the U.S. presidential elections and the emergence of new\nsub-communities. The framework thus highlights the pivotal role of social norms\nin shaping online interactions, presenting a substantial advance in both the\ntheory and application of social norm studies in digital spaces.",
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future work, centered\naround whether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, while still providing opportunities to optimize\nabstention abilities based on context.",
        "pos": [
            "Literature review requires researchers to synthesize a large amount of\ninformation and is increasingly challenging as the scientific literature\nexpands. In this work, we investigate the potential of LLMs for producing\nhierarchical organizations of scientific studies to assist researchers with\nliterature review. We define hierarchical organizations as tree structures\nwhere nodes refer to topical categories and every node is linked to the studies\nassigned to that category. Our naive LLM-based pipeline for hierarchy\ngeneration from a set of studies produces promising yet imperfect hierarchies,\nmotivating us to collect CHIME, an expert-curated dataset for this task focused\non biomedicine. Given the challenging and time-consuming nature of building\nhierarchies from scratch, we use a human-in-the-loop process in which experts\ncorrect errors (both links between categories and study assignment) in\nLLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies\ncovering 472 topics, and expert-corrected hierarchies for a subset of 100\ntopics. Expert corrections allow us to quantify LLM performance, and we find\nthat while they are quite good at generating and organizing categories, their\nassignment of studies to categories could be improved. We attempt to train a\ncorrector model with human feedback which improves study assignment by 12.6 F1\npoints. We release our dataset and models to encourage research on developing\nbetter assistive tools for literature review."
        ],
        "neg": []
    },
    {
        "query": "While Large Language Models (LLM) have created a massive technological impact\nin the past decade, allowing for human-enabled applications, they can produce\noutput that contains stereotypes and biases, especially when using low-resource\nlanguages. This can be of great ethical concern when dealing with sensitive\ntopics such as religion. As a means toward making LLMS more fair, we explore\nbias from a religious perspective in Bengali, focusing specifically on two main\nreligious dialects: Hindu and Muslim-majority dialects. Here, we perform\ndifferent experiments and audit showing the comparative analysis of different\nsentences using three commonly used LLMs: ChatGPT, Gemini, and Microsoft\nCopilot, pertaining to the Hindu and Muslim dialects of specific words and\nshowcasing which ones catch the social biases and which do not. Furthermore, we\nanalyze our findings and relate them to potential reasons and evaluation\nperspectives, considering their global impact with over 300 million speakers\nworldwide. With this work, we hope to establish the rigor for creating more\nfairness in LLMs, as these are widely used as creative writing agents.",
        "pos": [
            "Integrating cognitive ergonomics with LLMs is essential for enhancing safety,\nreliability, and user satisfaction in human-AI interactions. Current LLM design\noften lacks this integration, leading to systems that may not fully align with\nhuman cognitive capabilities and limitations. Insufficient focus on\nincorporating cognitive science methods exacerbates biases in LLM outputs,\nwhile inconsistent application of user-centered design principles results in\nsub-optimal user experiences. To address these challenges, our position paper\nexplores the critical integration of cognitive ergonomics principles into LLM\ndesign, aiming to provide a comprehensive framework and practical guidelines\nfor ethical LLM development. Through our contributions, we seek to advance\nunderstanding and practice in integrating cognitive ergonomics into LLM\nsystems, fostering safer, more reliable, and ethically sound human-AI\ninteractions."
        ],
        "neg": []
    },
    {
        "query": "We present a principled approach to provide LLM-based evaluation with a\nrigorous guarantee of human agreement. We first propose that a reliable\nevaluation method should not uncritically rely on model preferences for\npairwise evaluation, but rather assess the confidence of judge models and\nselectively decide when to trust its judgement. We then show that under this\nselective evaluation framework, human agreement can be provably guaranteed --\nsuch that the model evaluation aligns with that of humans to a user-specified\nagreement level. As part of our framework, we also introduce Simulated\nAnnotators, a novel confidence estimation method that significantly improves\njudge calibration and thus enables high coverage of evaluated instances.\nFinally, we propose Cascaded Selective Evaluation, where we use cheaper models\nas initial judges and escalate to stronger models only when necessary -- again,\nwhile still providing a provable guarantee of human agreement. Experimental\nresults show that Cascaded Selective Evaluation guarantees strong alignment\nwith humans, far beyond what LLM judges could achieve without selective\nevaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never\nachieves 80% human agreement, our method, even while employing substantially\ncost-effective models such as Mistral-7B, guarantees over 80% human agreement\nwith almost 80% test coverage.",
        "pos": [
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "We present a principled approach to provide LLM-based evaluation with a\nrigorous guarantee of human agreement. We first propose that a reliable\nevaluation method should not uncritically rely on model preferences for\npairwise evaluation, but rather assess the confidence of judge models and\nselectively decide when to trust its judgement. We then show that under this\nselective evaluation framework, human agreement can be provably guaranteed --\nsuch that the model evaluation aligns with that of humans to a user-specified\nagreement level. As part of our framework, we also introduce Simulated\nAnnotators, a novel confidence estimation method that significantly improves\njudge calibration and thus enables high coverage of evaluated instances.\nFinally, we propose Cascaded Selective Evaluation, where we use cheaper models\nas initial judges and escalate to stronger models only when necessary -- again,\nwhile still providing a provable guarantee of human agreement. Experimental\nresults show that Cascaded Selective Evaluation guarantees strong alignment\nwith humans, far beyond what LLM judges could achieve without selective\nevaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never\nachieves 80% human agreement, our method, even while employing substantially\ncost-effective models such as Mistral-7B, guarantees over 80% human agreement\nwith almost 80% test coverage.",
        "pos": [
            "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.",
            "The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.",
            "While humans naturally develop theory of mind (ToM), the capability to\nunderstand other people's mental states and beliefs, state-of-the-art large\nlanguage models (LLMs) underperform on simple ToM benchmarks. We posit that we\ncan extend our understanding of LLMs' ToM abilities by evaluating key human ToM\nprecursors -- perception inference and perception-to-belief inference -- in\nLLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate\nthese precursory inferences for ToM in LLMs by annotating characters'\nperceptions on ToMi and FANToM, respectively. Our evaluation of eight\nstate-of-the-art LLMs reveals that the models generally perform well in\nperception inference while exhibiting limited capability in\nperception-to-belief inference (e.g., lack of inhibitory control). Based on\nthese results, we present PercepToM, a novel ToM method leveraging LLMs' strong\nperception inference capability while supplementing their limited\nperception-to-belief inference. Experimental results demonstrate that PercepToM\nsignificantly enhances LLM's performance, especially in false belief scenarios.",
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed.",
        "pos": [
            "Dialectal Arabic is the primary spoken language used by native Arabic\nspeakers in daily communication. The rise of social media platforms has notably\nexpanded its use as a written language. However, Arabic dialects do not have\nstandard orthographies. This, combined with the inherent noise in\nuser-generated content on social media, presents a major challenge to NLP\napplications dealing with Dialectal Arabic. In this paper, we explore and\nreport on the task of CODAfication, which aims to normalize Dialectal Arabic\ninto the Conventional Orthography for Dialectal Arabic (CODA). We work with a\nunique parallel corpus of multiple Arabic dialects focusing on five major city\ndialects. We benchmark newly developed pretrained sequence-to-sequence models\non the task of CODAfication. We further show that using dialect identification\ninformation improves the performance across all dialects. We make our code,\ndata, and pretrained models publicly available."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements have significantly enhanced the capabilities of\nMultimodal Large Language Models (MLLMs) in generating and understanding\nimage-to-text content. Despite these successes, progress is predominantly\nlimited to English due to the scarcity of high quality multimodal resources in\nother languages. This limitation impedes the development of competitive models\nin languages such as Arabic. To alleviate this situation, we introduce an\nefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced\nlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallah\ndemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning\nsix Arabic dialects, Dallah showcases its capability to handle complex\ndialectal interactions incorporating both textual and visual elements. The\nmodel excels in two benchmark tests: one evaluating its performance on Modern\nStandard Arabic (MSA) and another specifically designed to assess dialectal\nresponses. Beyond its robust performance in multimodal interaction tasks,\nDallah has the potential to pave the way for further development of\ndialect-aware Arabic MLLMs.",
        "pos": [
            "Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)\npose unique challenges due to the cursive and context-sensitive nature of the\nArabic script. This study introduces Qalam, a novel foundation model designed\nfor Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder\narchitecture. Our model significantly outperforms existing methods, achieving a\nWord Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We\ntrain Qalam on a diverse dataset, including over 4.5 million images from Arabic\nmanuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,\nQalam demonstrates exceptional handling of Arabic diacritics, a critical\nfeature in Arabic scripts. Furthermore, it shows a remarkable ability to\nprocess high-resolution inputs, addressing a common limitation in current OCR\nsystems. These advancements underscore Qalam's potential as a leading solution\nfor Arabic script recognition, offering a significant leap in accuracy and\nefficiency.",
            "Large language models (LLMs) have recently emerged as a powerful tool for a\nwide range of language generation tasks. Nevertheless, this progress has been\nslower in Arabic. In this work, we focus on the task of generating stories from\nLLMs. For our training, we use stories acquired through machine translation\n(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that\nensures we acquire high-quality stories. For our GPT-41 data, we introduce\ncrafted prompts that allow us to generate data well-suited to the Arabic\ncontext in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian\nand Moroccan). For example, we generate stories tailored to various Arab\ncountries on a wide host of topics. Our manual evaluation shows that our model\nfine-tuned on these training datasets can generate coherent stories that adhere\nto our instructions. We also conduct an extensive automatic and human\nevaluation comparing our models against state-of-the-art proprietary and\nopen-source models. Our datasets and models will be made publicly available at\nhttps: //github.com/UBC-NLP/arastories."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements have significantly enhanced the capabilities of\nMultimodal Large Language Models (MLLMs) in generating and understanding\nimage-to-text content. Despite these successes, progress is predominantly\nlimited to English due to the scarcity of high quality multimodal resources in\nother languages. This limitation impedes the development of competitive models\nin languages such as Arabic. To alleviate this situation, we introduce an\nefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced\nlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallah\ndemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning\nsix Arabic dialects, Dallah showcases its capability to handle complex\ndialectal interactions incorporating both textual and visual elements. The\nmodel excels in two benchmark tests: one evaluating its performance on Modern\nStandard Arabic (MSA) and another specifically designed to assess dialectal\nresponses. Beyond its robust performance in multimodal interaction tasks,\nDallah has the potential to pave the way for further development of\ndialect-aware Arabic MLLMs.",
        "pos": [
            "Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)\npose unique challenges due to the cursive and context-sensitive nature of the\nArabic script. This study introduces Qalam, a novel foundation model designed\nfor Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder\narchitecture. Our model significantly outperforms existing methods, achieving a\nWord Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We\ntrain Qalam on a diverse dataset, including over 4.5 million images from Arabic\nmanuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,\nQalam demonstrates exceptional handling of Arabic diacritics, a critical\nfeature in Arabic scripts. Furthermore, it shows a remarkable ability to\nprocess high-resolution inputs, addressing a common limitation in current OCR\nsystems. These advancements underscore Qalam's potential as a leading solution\nfor Arabic script recognition, offering a significant leap in accuracy and\nefficiency."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements have significantly enhanced the capabilities of\nMultimodal Large Language Models (MLLMs) in generating and understanding\nimage-to-text content. Despite these successes, progress is predominantly\nlimited to English due to the scarcity of high quality multimodal resources in\nother languages. This limitation impedes the development of competitive models\nin languages such as Arabic. To alleviate this situation, we introduce an\nefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced\nlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallah\ndemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning\nsix Arabic dialects, Dallah showcases its capability to handle complex\ndialectal interactions incorporating both textual and visual elements. The\nmodel excels in two benchmark tests: one evaluating its performance on Modern\nStandard Arabic (MSA) and another specifically designed to assess dialectal\nresponses. Beyond its robust performance in multimodal interaction tasks,\nDallah has the potential to pave the way for further development of\ndialect-aware Arabic MLLMs.",
        "pos": [
            "Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)\npose unique challenges due to the cursive and context-sensitive nature of the\nArabic script. This study introduces Qalam, a novel foundation model designed\nfor Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder\narchitecture. Our model significantly outperforms existing methods, achieving a\nWord Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We\ntrain Qalam on a diverse dataset, including over 4.5 million images from Arabic\nmanuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,\nQalam demonstrates exceptional handling of Arabic diacritics, a critical\nfeature in Arabic scripts. Furthermore, it shows a remarkable ability to\nprocess high-resolution inputs, addressing a common limitation in current OCR\nsystems. These advancements underscore Qalam's potential as a leading solution\nfor Arabic script recognition, offering a significant leap in accuracy and\nefficiency.",
            "We present WojoodNER-2024, the second Arabic Named Entity Recognition (NER)\nShared Task. In WojoodNER-2024, we focus on fine-grained Arabic NER. We\nprovided participants with a new Arabic fine-grained NER dataset called\nwojoodfine, annotated with subtypes of entities. WojoodNER-2024 encompassed\nthree subtasks: (i) Closed-Track Flat Fine-Grained NER, (ii) Closed-Track\nNested Fine-Grained NER, and (iii) an Open-Track NER for the Israeli War on\nGaza. A total of 43 unique teams registered for this shared task. Five teams\nparticipated in the Flat Fine-Grained Subtask, among which two teams tackled\nthe Nested Fine-Grained Subtask and one team participated in the Open-Track NER\nSubtask. The winning teams achieved F-1 scores of 91% and 92% in the Flat\nFine-Grained and Nested Fine-Grained Subtasks, respectively. The sole team in\nthe Open-Track Subtask achieved an F-1 score of 73.7%.",
            "Large language models (LLMs) have recently emerged as a powerful tool for a\nwide range of language generation tasks. Nevertheless, this progress has been\nslower in Arabic. In this work, we focus on the task of generating stories from\nLLMs. For our training, we use stories acquired through machine translation\n(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that\nensures we acquire high-quality stories. For our GPT-41 data, we introduce\ncrafted prompts that allow us to generate data well-suited to the Arabic\ncontext in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian\nand Moroccan). For example, we generate stories tailored to various Arab\ncountries on a wide host of topics. Our manual evaluation shows that our model\nfine-tuned on these training datasets can generate coherent stories that adhere\nto our instructions. We also conduct an extensive automatic and human\nevaluation comparing our models against state-of-the-art proprietary and\nopen-source models. Our datasets and models will be made publicly available at\nhttps: //github.com/UBC-NLP/arastories.",
            "We address a notable gap in Natural Language Processing (NLP) by introducing\na collection of resources designed to improve Machine Translation (MT) for\nlow-resource languages, with a specific focus on African languages. First, we\nintroduce two language models (LMs), Cheetah-1.2B and Cheetah-3.7B, with 1.2\nbillion and 3.7 billion parameters respectively. Next, we finetune the\naforementioned models to create toucan, an Afrocentric machine translation\nmodel designed to support 156 African language pairs. To evaluate Toucan, we\ncarefully develop an extensive machine translation benchmark, dubbed\nAfroLingu-MT, tailored for evaluating machine translation. Toucan significantly\noutperforms other models, showcasing its remarkable performance on MT for\nAfrican languages. Finally, we train a new model, spBLEU-1K, to enhance\ntranslation evaluation metrics, covering 1K languages, including 614 African\nlanguages. This work aims to advance the field of NLP, fostering cross-cultural\nunderstanding and knowledge exchange, particularly in regions with limited\nlanguage resources such as Africa. The GitHub repository for the Toucan project\nis available at https://github.com/UBC-NLP/Toucan."
        ],
        "neg": []
    },
    {
        "query": "The recent emergence of Large Language Models (LLMs) has heralded a new era\nof human-AI interaction. These sophisticated models, exemplified by Chat-GPT\nand its successors, have exhibited remarkable capabilities in language\nunderstanding. However, as these LLMs have undergone exponential growth, a\ncrucial dimension that remains understudied is the personalization of these\nmodels. Large foundation models such as GPT-3 etc. focus on creating a\nuniversal model that serves a broad range of tasks and users. This approach\nemphasizes the model's generalization capabilities, treating users as a\ncollective rather than as distinct individuals. While practical for many common\napplications, this one-size-fits-all approach often fails to address the rich\ntapestry of human diversity and individual needs. To explore this issue we\nintroduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP\nmodels for user personalization. \\datasetname{} consists of a series of\nuser-centered tasks containing diverse and individualized expressions where the\npreferences of users can potentially differ for the same input. Using PEFT-U,\nwe explore the challenge of efficiently personalizing LLMs to accommodate\nuser-specific preferences in the context of diverse user-centered tasks.",
        "pos": [
            "The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a\nviable and cost-effective alternative to API-based Large Language Models\n(LLMs), such as OpenAI's GPT-4, offering comparable performance and stability.\nHowever, SLAM also identified discrepancies between human preferences and\ntraditional auto-evaluators. This follow-up paper explores methods to align LLM\nevaluator preferences with human evaluations by addressing biases, particularly\ntoward higher token counts. We employed Bayesian statistics and a t-test to\nquantify this bias and developed a recalibration procedure to adjust the\nGPTScorer. Our findings significantly improve aligning the recalibrated LLM\nevaluator with human evaluations across multiple use cases. For instance,\nspearman's ranking correlation score in the Recommendation use case improved\nfrom -27.27 to 44.55. These results highlight the importance of accounting for\nbiases in automated evaluations to ensure fair and accurate model assessments.\nThe recalibration process enhances the reliability of automated evaluators,\nleading to better AI models that align with human values and expectations. This\nstudy provides a robust methodology for future research into bias correction\nand emphasizes the feasibility and benefits of developing human-aligned AI\nevaluation systems."
        ],
        "neg": []
    },
    {
        "query": "Humans have clear cross-modal preferences when matching certain novel words\nto visual shapes. Evidence suggests that these preferences play a prominent\nrole in our linguistic processing, language learning, and the origins of\nsignal-meaning mappings. With the rise of multimodal models in AI, such as\nvision- and-language (VLM) models, it becomes increasingly important to uncover\nthe kinds of visio-linguistic associations these models encode and whether they\nalign with human representations. Informed by experiments with humans, we probe\nand compare four VLMs for a well-known human cross-modal preference, the\nbouba-kiki effect. We do not find conclusive evidence for this effect but\nsuggest that results may depend on features of the models, such as architecture\ndesign, model size, and training details. Our findings inform discussions on\nthe origins of the bouba-kiki effect in human cognition and future developments\nof VLMs that align well with human cross-modal associations.",
        "pos": [
            "Natural language has the universal properties of being compositional and\ngrounded in reality. The emergence of linguistic properties is often\ninvestigated through simulations of emergent communication in referential\ngames. However, these experiments have yielded mixed results compared to\nsimilar experiments addressing linguistic properties of human language. Here we\naddress representational alignment as a potential contributing factor to these\nresults. Specifically, we assess the representational alignment between agent\nimage representations and between agent representations and input images. Doing\nso, we confirm that the emergent language does not appear to encode human-like\nconceptual visual features, since agent image representations drift away from\ninputs whilst inter-agent alignment increases. We moreover identify a strong\nrelationship between inter-agent alignment and topographic similarity, a common\nmetric for compositionality, and address its consequences. To address these\nissues, we introduce an alignment penalty that prevents representational drift\nbut interestingly does not improve performance on a compositional\ndiscrimination task. Together, our findings emphasise the key role\nrepresentational alignment plays in simulations of language emergence.",
            "Recent advances in computational linguistics include simulating the emergence\nof human-like languages with interacting neural network agents, starting from\nsets of random symbols. The recently introduced NeLLCom framework (Lian et al.,\n2023) allows agents to first learn an artificial language and then use it to\ncommunicate, with the aim of studying the emergence of specific linguistics\nproperties. We extend this framework (NeLLCom-X) by introducing more realistic\nrole-alternating agents and group communication in order to investigate the\ninterplay between language learnability, communication pressures, and group\nsize effects. We validate NeLLCom-X by replicating key findings from prior\nresearch simulating the emergence of a word-order/case-marking trade-off. Next,\nwe investigate how interaction affects linguistic convergence and emergence of\nthe trade-off. The novel framework facilitates future simulations of diverse\nlinguistic aspects, emphasizing the importance of interaction and group\ndynamics in language evolution."
        ],
        "neg": []
    },
    {
        "query": "Humans have clear cross-modal preferences when matching certain novel words\nto visual shapes. Evidence suggests that these preferences play a prominent\nrole in our linguistic processing, language learning, and the origins of\nsignal-meaning mappings. With the rise of multimodal models in AI, such as\nvision- and-language (VLM) models, it becomes increasingly important to uncover\nthe kinds of visio-linguistic associations these models encode and whether they\nalign with human representations. Informed by experiments with humans, we probe\nand compare four VLMs for a well-known human cross-modal preference, the\nbouba-kiki effect. We do not find conclusive evidence for this effect but\nsuggest that results may depend on features of the models, such as architecture\ndesign, model size, and training details. Our findings inform discussions on\nthe origins of the bouba-kiki effect in human cognition and future developments\nof VLMs that align well with human cross-modal associations.",
        "pos": [
            "Natural language has the universal properties of being compositional and\ngrounded in reality. The emergence of linguistic properties is often\ninvestigated through simulations of emergent communication in referential\ngames. However, these experiments have yielded mixed results compared to\nsimilar experiments addressing linguistic properties of human language. Here we\naddress representational alignment as a potential contributing factor to these\nresults. Specifically, we assess the representational alignment between agent\nimage representations and between agent representations and input images. Doing\nso, we confirm that the emergent language does not appear to encode human-like\nconceptual visual features, since agent image representations drift away from\ninputs whilst inter-agent alignment increases. We moreover identify a strong\nrelationship between inter-agent alignment and topographic similarity, a common\nmetric for compositionality, and address its consequences. To address these\nissues, we introduce an alignment penalty that prevents representational drift\nbut interestingly does not improve performance on a compositional\ndiscrimination task. Together, our findings emphasise the key role\nrepresentational alignment plays in simulations of language emergence."
        ],
        "neg": []
    },
    {
        "query": "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.",
        "pos": [
            "Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: https://github.com/IBM/BenchBench\n  Leaderboard: https://huggingface.co/spaces/per/BenchBench"
        ],
        "neg": []
    },
    {
        "query": "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.",
        "pos": [
            "Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: https://github.com/IBM/BenchBench\n  Leaderboard: https://huggingface.co/spaces/per/BenchBench"
        ],
        "neg": []
    },
    {
        "query": "Differing from sentiment transfer, positive reframing seeks to substitute\nnegative perspectives with positive expressions while preserving the original\nmeaning. With the emergence of pre-trained language models (PLMs), it is\npossible to achieve acceptable results by fine-tuning PLMs. Nevertheless,\ngenerating fluent, diverse and task-constrained reframing text remains a\nsignificant challenge. To tackle this issue, a \\textbf{m}ulti-\\textbf{s}trategy\n\\textbf{o}ptimization \\textbf{f}ramework (MSOF) is proposed in this paper.\nStarting from the objective of positive reframing, we first design positive\nsentiment reward and content preservation reward to encourage the model to\ntransform the negative expressions of the original text while ensuring the\nintegrity and consistency of the semantics. Then, different decoding\noptimization approaches are introduced to improve the quality of text\ngeneration. Finally, based on the modeling formula of positive reframing, we\npropose a multi-dimensional re-ranking method that further selects candidate\nsentences from three dimensions: strategy consistency, text similarity and\nfluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate\nour framework achieves significant improvements on unconstrained and controlled\npositive reframing tasks.",
        "pos": [
            "In this paper we present the results of the AI-Debater 2023 Challenge held by\nthe Chinese Conference on Affect Computing (CCAC 2023), and introduce the\nrelated datasets. We organize two tracks to handle the argumentative generation\ntasks in different scenarios, namely, Counter-Argument Generation (Track 1) and\nClaim-based Argument Generation (Track 2). Each track is equipped with its\ndistinct dataset and baseline model respectively. In total, 32 competing teams\nregister for the challenge, from which we received 11 successful submissions.\nIn this paper, we will present the results of the challenge and a summary of\nthe systems, highlighting commonalities and innovations among participating\nsystems. Datasets and baseline models of the AI-Debater 2023 Challenge have\nbeen already released and can be accessed through the official website of the\nchallenge."
        ],
        "neg": []
    },
    {
        "query": "Differing from sentiment transfer, positive reframing seeks to substitute\nnegative perspectives with positive expressions while preserving the original\nmeaning. With the emergence of pre-trained language models (PLMs), it is\npossible to achieve acceptable results by fine-tuning PLMs. Nevertheless,\ngenerating fluent, diverse and task-constrained reframing text remains a\nsignificant challenge. To tackle this issue, a \\textbf{m}ulti-\\textbf{s}trategy\n\\textbf{o}ptimization \\textbf{f}ramework (MSOF) is proposed in this paper.\nStarting from the objective of positive reframing, we first design positive\nsentiment reward and content preservation reward to encourage the model to\ntransform the negative expressions of the original text while ensuring the\nintegrity and consistency of the semantics. Then, different decoding\noptimization approaches are introduced to improve the quality of text\ngeneration. Finally, based on the modeling formula of positive reframing, we\npropose a multi-dimensional re-ranking method that further selects candidate\nsentences from three dimensions: strategy consistency, text similarity and\nfluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate\nour framework achieves significant improvements on unconstrained and controlled\npositive reframing tasks.",
        "pos": [
            "In this paper we present the results of the AI-Debater 2023 Challenge held by\nthe Chinese Conference on Affect Computing (CCAC 2023), and introduce the\nrelated datasets. We organize two tracks to handle the argumentative generation\ntasks in different scenarios, namely, Counter-Argument Generation (Track 1) and\nClaim-based Argument Generation (Track 2). Each track is equipped with its\ndistinct dataset and baseline model respectively. In total, 32 competing teams\nregister for the challenge, from which we received 11 successful submissions.\nIn this paper, we will present the results of the challenge and a summary of\nthe systems, highlighting commonalities and innovations among participating\nsystems. Datasets and baseline models of the AI-Debater 2023 Challenge have\nbeen already released and can be accessed through the official website of the\nchallenge."
        ],
        "neg": []
    },
    {
        "query": "Representations from deep neural networks (DNNs) have proven remarkably\npredictive of neural activity involved in both visual and linguistic\nprocessing. Despite these successes, most studies to date concern unimodal\nDNNs, encoding either visual or textual input but not both. Yet, there is\ngrowing evidence that human meaning representations integrate linguistic and\nsensory-motor information. Here we investigate whether the integration of\nmultimodal information operated by current vision-and-language DNN models\n(VLMs) leads to representations that are more aligned with human brain activity\nthan those obtained by language-only and vision-only DNNs. We focus on fMRI\nresponses recorded while participants read concept words in the context of\neither a full sentence or an accompanying picture. Our results reveal that VLM\nrepresentations correlate more strongly than language- and vision-only DNNs\nwith activations in brain areas functionally related to language processing. A\ncomparison between different types of visuo-linguistic architectures shows that\nrecent generative VLMs tend to be less brain-aligned than previous\narchitectures with lower performance on downstream applications. Moreover,\nthrough an additional analysis comparing brain vs. behavioural alignment across\nmultiple VLMs, we show that -- with one remarkable exception -- representations\nthat strongly align with behavioural judgments do not correlate highly with\nbrain responses. This indicates that brain similarity does not go hand in hand\nwith behavioural similarity, and vice versa.",
        "pos": [
            "Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition."
        ],
        "neg": []
    },
    {
        "query": "Representations from deep neural networks (DNNs) have proven remarkably\npredictive of neural activity involved in both visual and linguistic\nprocessing. Despite these successes, most studies to date concern unimodal\nDNNs, encoding either visual or textual input but not both. Yet, there is\ngrowing evidence that human meaning representations integrate linguistic and\nsensory-motor information. Here we investigate whether the integration of\nmultimodal information operated by current vision-and-language DNN models\n(VLMs) leads to representations that are more aligned with human brain activity\nthan those obtained by language-only and vision-only DNNs. We focus on fMRI\nresponses recorded while participants read concept words in the context of\neither a full sentence or an accompanying picture. Our results reveal that VLM\nrepresentations correlate more strongly than language- and vision-only DNNs\nwith activations in brain areas functionally related to language processing. A\ncomparison between different types of visuo-linguistic architectures shows that\nrecent generative VLMs tend to be less brain-aligned than previous\narchitectures with lower performance on downstream applications. Moreover,\nthrough an additional analysis comparing brain vs. behavioural alignment across\nmultiple VLMs, we show that -- with one remarkable exception -- representations\nthat strongly align with behavioural judgments do not correlate highly with\nbrain responses. This indicates that brain similarity does not go hand in hand\nwith behavioural similarity, and vice versa.",
        "pos": [
            "Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition."
        ],
        "neg": []
    },
    {
        "query": "The rise of social media and the exponential growth of multimodal\ncommunication necessitates advanced techniques for Multimodal Information\nExtraction (MIE). However, existing methodologies primarily rely on direct\nImage-Text interactions, a paradigm that often faces significant challenges due\nto semantic and modality gaps between images and text. In this paper, we\nintroduce a new paradigm of Image-Context-Text interaction, where large\nmultimodal models (LMMs) are utilized to generate descriptive textual context\nto bridge these gaps. In line with this paradigm, we propose a novel Shapley\nValue-based Contrastive Alignment (Shap-CA) method, which aligns both\ncontext-text and context-image pairs. Shap-CA initially applies the Shapley\nvalue concept from cooperative game theory to assess the individual\ncontribution of each element in the set of contexts, texts and images towards\ntotal semantic and modality overlaps. Following this quantitative evaluation, a\ncontrastive learning strategy is employed to enhance the interactive\ncontribution within context-text/image pairs, while minimizing the influence\nacross these pairs. Furthermore, we design an adaptive fusion module for\nselective cross-modal fusion. Extensive experiments across four MIE datasets\ndemonstrate that our method significantly outperforms existing state-of-the-art\nmethods.",
        "pos": [
            "Current evaluations of large language models (LLMs) often overlook\nnon-determinism, typically focusing on a single output per example. This limits\nour understanding of LLM performance variability in real-world applications.\nOur study addresses this issue by exploring key questions about the performance\ndifferences between greedy decoding and sampling, identifying benchmarks'\nconsistency regarding non-determinism, and examining unique model behaviors.\nThrough extensive experiments, we observe that greedy decoding generally\noutperforms sampling methods for most evaluated tasks. We also observe\nconsistent performance across different LLM sizes and alignment methods, noting\nthat alignment can reduce sampling variance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can match or surpass larger models such\nas GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This\nresearch shows the importance of considering non-determinism in LLM evaluations\nand provides insights for future LLM development and evaluation."
        ],
        "neg": []
    },
    {
        "query": "Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's\ngroundbreaking work. Although CLIP performs well, the typical direct latent\nfeature alignment lacks clarity in its representation and similarity scores. On\nthe other hand, lexical representation, a vector whose element represents the\nsimilarity between the sample and a word from the vocabulary, is a natural\nsparse representation and interpretable, providing exact matches for individual\nwords. However, lexical representations is difficult to learn due to no\nground-truth supervision and false-discovery issues, and thus requires complex\ndesign to train effectively. In this paper, we introduce LexVLA, a more\ninterpretable VLA framework by learning a unified lexical representation for\nboth modalities without complex design. We use DINOv2 as our visual model for\nits local-inclined features and Llama 2, a generative language model, to\nleverage its in-context lexical prediction ability. To avoid the false\ndiscovery, we propose an overuse penalty to refrain the lexical representation\nfrom falsely frequently activating meaningless words. We demonstrate that these\ntwo pre-trained uni-modal models can be well-aligned by fine-tuning on modest\nmulti-modal dataset and avoid intricate training configurations. On cross-modal\nretrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,\noutperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those\ntrained from scratch on even bigger datasets (e.g., 1.1B data, including\nCC-12M). We conduct extensive experiments to analyze LexVLA.",
        "pos": [
            "Large language models in the past have typically relied on some form of\nreinforcement learning with human feedback (RLHF) to better align model\nresponses with human preferences. However, because of oft-observed\ninstabilities when implementing these RLHF pipelines, various\nreparameterization techniques have recently been introduced to sidestep the\nneed for separately learning an RL reward model. Instead, directly fine-tuning\nfor human preferences is achieved via the minimization of a single closed-form\ntraining objective, a process originally referred to as direct preference\noptimization (DPO) and followed by several notable descendants. Although\neffective in certain real-world settings, we introduce new evaluation criteria\nthat serve to highlight unresolved shortcomings in the ability of existing DPO\nmethods to interpolate between a pre-trained reference model and empirical\nmeasures of human preferences, as well as unavoidable trade-offs in how low-\nand high-quality responses are regularized and constraints are handled. Our\ninsights then motivate an alternative DPO-like loss that provably mitigates\nthese limitations. Empirical results serve to corroborate notable aspects of\nour analyses."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) hold great promise in summarizing medical\nevidence. Most recent studies focus on the application of proprietary LLMs.\nUsing proprietary LLMs introduces multiple risk factors, including a lack of\ntransparency and vendor dependency. While open-source LLMs allow better\ntransparency and customization, their performance falls short compared to\nproprietary ones. In this study, we investigated to what extent fine-tuning\nopen-source LLMs can further improve their performance in summarizing medical\nevidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs\nof systematic reviews and summaries, we fine-tuned three broadly-used,\nopen-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned\nLLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:\n8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and\n15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of\nfine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,\nsmaller fine-tuned models sometimes even demonstrated superior performance\ncompared to larger zero-shot models. The above trends of improvement were also\nmanifested in both human and GPT4-simulated evaluations. Our results can be\napplied to guide model selection for tasks demanding particular domain\nknowledge, such as medical evidence summarization.",
        "pos": [
            "Extracting social determinants of health (SDoH) from unstructured medical\nnotes depends heavily on labor-intensive annotations, which are typically\ntask-specific, hampering reusability and limiting sharing. In this study we\nintroduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)\nmethod leveraging contrastive examples and concise instructions to extract SDoH\nwithout relying on extensive medical annotations or costly human intervention.\nIt achieved tenfold and twentyfold reductions in time and cost respectively,\nand superior consistency with human annotators measured by Cohen's kappa of up\nto 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the\nstrengths of both, ensuring high accuracy and computational efficiency while\nconsistently maintaining 0.90+ AUROC scores. Testing across three distinct\ndatasets has confirmed its robustness and accuracy. This study highlights the\npotential of leveraging LLMs to revolutionize medical note classification,\ndemonstrating their capability to achieve highly accurate classifications with\nsignificantly reduced time and cost.",
            "Hallucination, a phenomenon where large language models (LLMs) produce output\nthat is factually incorrect or unrelated to the input, is a major challenge for\nLLM applications that require accuracy and dependability. In this paper, we\nintroduce a reliable and high-speed production system aimed at detecting and\nrectifying the hallucination issue within LLMs. Our system encompasses named\nentity recognition (NER), natural language inference (NLI), span-based\ndetection (SBD), and an intricate decision tree-based process to reliably\ndetect a wide range of hallucinations in LLM responses. Furthermore, our team\nhas crafted a rewriting mechanism that maintains an optimal mix of precision,\nresponse time, and cost-effectiveness. We detail the core elements of our\nframework and underscore the paramount challenges tied to response time,\navailability, and performance metrics, which are crucial for real-world\ndeployment of these technologies. Our extensive evaluation, utilizing offline\ndata and live production traffic, confirms the efficacy of our proposed\nframework and service.",
            "As Large Language Models (LLMs) are increasingly deployed to handle various\nnatural language processing (NLP) tasks, concerns regarding the potential\nnegative societal impacts of LLM-generated content have also arisen. To\nevaluate the biases exhibited by LLMs, researchers have recently proposed a\nvariety of datasets. However, existing bias evaluation efforts often focus on\nonly a particular type of bias and employ inconsistent evaluation metrics,\nleading to difficulties in comparison across different datasets and LLMs. To\naddress these limitations, we collect a variety of datasets designed for the\nbias evaluation of LLMs, and further propose CEB, a Compositional Evaluation\nBenchmark that covers different types of bias across different social groups\nand tasks. The curation of CEB is based on our newly proposed compositional\ntaxonomy, which characterizes each dataset from three dimensions: bias types,\nsocial groups, and tasks. By combining the three dimensions, we develop a\ncomprehensive evaluation strategy for the bias in LLMs. Our experiments\ndemonstrate that the levels of bias vary across these dimensions, thereby\nproviding guidance for the development of specific bias mitigation methods."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) hold great promise in summarizing medical\nevidence. Most recent studies focus on the application of proprietary LLMs.\nUsing proprietary LLMs introduces multiple risk factors, including a lack of\ntransparency and vendor dependency. While open-source LLMs allow better\ntransparency and customization, their performance falls short compared to\nproprietary ones. In this study, we investigated to what extent fine-tuning\nopen-source LLMs can further improve their performance in summarizing medical\nevidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs\nof systematic reviews and summaries, we fine-tuned three broadly-used,\nopen-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned\nLLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:\n8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and\n15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of\nfine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,\nsmaller fine-tuned models sometimes even demonstrated superior performance\ncompared to larger zero-shot models. The above trends of improvement were also\nmanifested in both human and GPT4-simulated evaluations. Our results can be\napplied to guide model selection for tasks demanding particular domain\nknowledge, such as medical evidence summarization.",
        "pos": [
            "Extracting social determinants of health (SDoH) from unstructured medical\nnotes depends heavily on labor-intensive annotations, which are typically\ntask-specific, hampering reusability and limiting sharing. In this study we\nintroduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)\nmethod leveraging contrastive examples and concise instructions to extract SDoH\nwithout relying on extensive medical annotations or costly human intervention.\nIt achieved tenfold and twentyfold reductions in time and cost respectively,\nand superior consistency with human annotators measured by Cohen's kappa of up\nto 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the\nstrengths of both, ensuring high accuracy and computational efficiency while\nconsistently maintaining 0.90+ AUROC scores. Testing across three distinct\ndatasets has confirmed its robustness and accuracy. This study highlights the\npotential of leveraging LLMs to revolutionize medical note classification,\ndemonstrating their capability to achieve highly accurate classifications with\nsignificantly reduced time and cost.",
            "Convolutions have become essential in state-of-the-art end-to-end Automatic\nSpeech Recognition~(ASR) systems due to their efficient modelling of local\ncontext. Notably, its use in Conformers has led to superior performance\ncompared to vanilla Transformer-based ASR systems. While components other than\nthe convolution module in the Conformer have been reexamined, altering the\nconvolution module itself has been far less explored. Towards this, we\nintroduce Multi-Convformer that uses multiple convolution kernels within the\nconvolution module of the Conformer in conjunction with gating. This helps in\nimproved modeling of local dependencies at varying granularities. Our model\nrivals existing Conformer variants such as CgMLP and E-Branchformer in\nperformance, while being more parameter efficient. We empirically compare our\napproach with Conformer and its variants across four different datasets and\nthree different modelling paradigms and show up to 8% relative word error\nrate~(WER) improvements."
        ],
        "neg": []
    },
    {
        "query": "Following the rapid progress in natural language processing (NLP) models,\nlanguage models are applied to increasingly more complex interactive tasks such\nas negotiations and conversation moderations. Having human evaluators directly\ninteract with these NLP models is essential for adequately evaluating the\nperformance on such interactive tasks. We develop BotEval, an easily\ncustomizable, open-source, evaluation toolkit that focuses on enabling\nhuman-bot interactions as part of the evaluation process, as opposed to human\nevaluators making judgements for a static input. BotEval balances flexibility\nfor customization and user-friendliness by providing templates for common use\ncases that span various degrees of complexity and built-in compatibility with\npopular crowdsourcing platforms. We showcase the numerous useful features of\nBotEval through a study that evaluates the performance of various chatbots on\ntheir effectiveness for conversational moderation and discuss how BotEval\ndiffers from other annotation tools.",
        "pos": [
            "This paper investigates the capability of LLMs in storytelling, focusing on\nnarrative development and plot progression. We introduce a novel computational\nframework to analyze narratives through three discourse-level aspects: i) story\narcs, ii) turning points, and iii) affective dimensions, including arousal and\nvalence. By leveraging expert and automatic annotations, we uncover significant\ndiscrepancies between the LLM- and human- written stories. While human-written\nstories are suspenseful, arousing, and diverse in narrative structures, LLM\nstories are homogeneously positive and lack tension. Next, we measure narrative\nreasoning skills as a precursor to generative capacities, concluding that most\nLLMs fall short of human abilities in discourse understanding. Finally, we show\nthat explicit integration of aforementioned discourse features can enhance\nstorytelling, as is demonstrated by over 40% improvement in neural storytelling\nin terms of diversity, suspense, and arousal."
        ],
        "neg": []
    },
    {
        "query": "The advent of vision-language models fosters the interactive conversations\nbetween AI-enabled models and humans. Yet applying these models into clinics\nmust deal with daunting challenges around large-scale training data, financial,\nand computational resources. Here we propose a cost-effective instruction\nlearning framework for conversational pathology named as CLOVER. CLOVER only\ntrains a lightweight module and uses instruction tuning while freezing the\nparameters of the large language model. Instead of using costly GPT-4, we\npropose well-designed prompts on GPT-3.5 for building generation-based\ninstructions, emphasizing the utility of pathological knowledge derived from\nthe Internet source. To augment the use of instructions, we construct a\nhigh-quality set of template-based instructions in the context of digital\npathology. From two benchmark datasets, our findings reveal the strength of\nhybrid-form instructions in the visual question-answer in pathology. Extensive\nresults show the cost-effectiveness of CLOVER in answering both open-ended and\nclosed-ended questions, where CLOVER outperforms strong baselines that possess\n37 times more training parameters and use instruction data generated from\nGPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot\nlearning in the external clinical dataset. These findings demonstrate that\ncost-effective modeling of CLOVER could accelerate the adoption of rapid\nconversational applications in the landscape of digital pathology.",
        "pos": [
            "As safety remains a crucial concern throughout the development lifecycle of\nLarge Language Models (LLMs), researchers and industrial practitioners have\nincreasingly focused on safeguarding and aligning LLM behaviors with human\npreferences and ethical standards. LLMs, trained on extensive multilingual\ncorpora, exhibit powerful generalization abilities across diverse languages and\ndomains. However, current safety alignment practices predominantly focus on\nsingle-language scenarios, which leaves their effectiveness in complex\nmultilingual contexts, especially for those complex mixed-language formats,\nlargely unexplored. In this study, we introduce Multilingual Blending, a\nmixed-language query-response scheme designed to evaluate the safety alignment\nof various state-of-the-art LLMs (e.g., GPT-4o, GPT-3.5, Llama3) under\nsophisticated, multilingual conditions. We further investigate language\npatterns such as language availability, morphology, and language family that\ncould impact the effectiveness of Multilingual Blending in compromising the\nsafeguards of LLMs. Our experimental results show that, without meticulously\ncrafted prompt templates, Multilingual Blending significantly amplifies the\ndetriment of malicious queries, leading to dramatically increased bypass rates\nin LLM safety alignment (67.23% on GPT-3.5 and 40.34% on GPT-4o), far exceeding\nthose of single-language baselines. Moreover, the performance of Multilingual\nBlending varies notably based on intrinsic linguistic properties, with\nlanguages of different morphology and from diverse families being more prone to\nevading safety alignments. These findings underscore the necessity of\nevaluating LLMs and developing corresponding safety alignment strategies in a\ncomplex, multilingual context to align with their superior cross-language\ngeneralization capabilities."
        ],
        "neg": []
    },
    {
        "query": "In contemporary society, the issue of psychological health has become\nincreasingly prominent, characterized by the diversification, complexity, and\nuniversality of mental disorders. Cognitive Behavioral Therapy (CBT), currently\nthe most influential and clinically effective psychological treatment method\nwith no side effects, has limited coverage and poor quality in most countries.\nIn recent years, researches on the recognition and intervention of emotional\ndisorders using large language models (LLMs) have been validated, providing new\npossibilities for psychological assistance therapy. However, are LLMs truly\npossible to conduct cognitive behavioral therapy? Many concerns have been\nraised by mental health experts regarding the use of LLMs for therapy. Seeking\nto answer this question, we collected real CBT corpus from online video\nwebsites, designed and conducted a targeted automatic evaluation framework\ninvolving the evaluation of emotion tendency of generated text, structured\ndialogue pattern and proactive inquiry ability. For emotion tendency, we\ncalculate the emotion tendency score of the CBT dialogue text generated by each\nmodel. For structured dialogue pattern, we use a diverse range of automatic\nevaluation metrics to compare speaking style, the ability to maintain\nconsistency of topic and the use of technology in CBT between different models\n. As for inquiring to guide the patient, we utilize PQA (Proactive Questioning\nAbility) metric. We also evaluated the CBT ability of the LLM after integrating\na CBT knowledge base to explore the help of introducing additional knowledge to\nenhance the model's CBT counseling ability. Four LLM variants with excellent\nperformance on natural language processing are evaluated, and the experimental\nresult shows the great potential of LLMs in psychological counseling realm,\nespecially after combining with other technological means.",
        "pos": [
            "The mainstream automatic speech recognition (ASR) technology usually requires\nhundreds to thousands of hours of annotated speech data. Three approaches to\nlow-resourced ASR are phoneme or subword based supervised pre-training, and\nself-supervised pre-training over multilingual data. The Iu Mien language is\nthe main ethnic language of the Yao ethnic group in China and is low-resourced\nin the sense that the annotated speech is very limited. With less than 10 hours\nof transcribed Iu Mien language, this paper investigates and compares the three\napproaches for Iu Mien speech recognition. Our experiments are based on the\nrecently released, three backbone models pretrained over the 10 languages from\nthe CommonVoice dataset (CV-Lang10), which correspond to the three approaches\nfor low-resourced ASR. It is found that phoneme supervision can achieve better\nresults compared to subword supervision and self-supervision, thereby providing\nhigher data-efficiency. Particularly, the Whistle models, i.e., obtained by the\nweakly-supervised phoneme-based multilingual pre-training, obtain the most\ncompetitive results."
        ],
        "neg": []
    },
    {
        "query": "While large language models (LLMs) have been increasingly deployed across\ntasks in language understanding and interactive decision-making, their\nimpressive performance is largely due to the comprehensive and in-depth domain\nknowledge embedded within them. However, the extent of this knowledge can vary\nacross different domains. Existing methods often assume that LLMs already\npossess such comprehensive and in-depth knowledge of their environment,\noverlooking potential gaps in their understanding of actual world dynamics. To\naddress this gap, we introduce Discover, Verify, and Evolve (DiVE), a framework\nthat discovers world dynamics from a small number of demonstrations, verifies\nthe correctness of these dynamics, and evolves new, advanced dynamics tailored\nto the current situation. Through extensive evaluations, we analyze the impact\nof each component on performance and compare the automatically generated\ndynamics from DiVE with human-annotated world dynamics. Our results demonstrate\nthat LLMs guided by DiVE can make better decisions, achieving rewards\ncomparable to human players in the Crafter environment.",
        "pos": [
            "Humor understanding is an important and challenging research in natural\nlanguage processing. As the popularity of pre-trained language models (PLMs),\nsome recent work makes preliminary attempts to adopt PLMs for humor recognition\nand generation. However, these simple attempts do not substantially answer the\nquestion: {\\em whether PLMs are capable of humor understanding?} This paper is\nthe first work that systematically investigates the humor understanding ability\nof PLMs. For this purpose, a comprehensive framework with three evaluation\nsteps and four evaluation tasks is designed. We also construct a comprehensive\nChinese humor dataset, which can fully meet all the data requirements of the\nproposed evaluation framework. Our empirical study on the Chinese humor dataset\nyields some valuable observations, which are of great guiding value for future\noptimization of PLMs in humor understanding and generation."
        ],
        "neg": []
    },
    {
        "query": "Attention-based transformers have been remarkably successful at modeling\ngenerative processes across various domains and modalities. In this paper, we\nstudy the behavior of transformers on data drawn from \\kth Markov processes,\nwhere the conditional distribution of the next symbol in a sequence depends on\nthe previous $k$ symbols observed. We observe a surprising phenomenon\nempirically which contradicts previous findings: when trained for sufficiently\nlong, a transformer with a fixed depth and $1$ head per layer is able to\nachieve low test loss on sequences drawn from \\kth Markov sources, even as $k$\ngrows. Furthermore, this low test loss is achieved by the transformer's ability\nto represent and learn the in-context conditional empirical distribution. On\nthe theoretical side, our main result is that a transformer with a single head\nand three layers can represent the in-context conditional empirical\ndistribution for \\kth Markov sources, concurring with our empirical\nobservations. Along the way, we prove that \\textit{attention-only} transformers\nwith $O(\\log_2(k))$ layers can represent the in-context conditional empirical\ndistribution by composing induction heads to track the previous $k$ symbols in\nthe sequence. These results provide more insight into our current understanding\nof the mechanisms by which transformers learn to capture context, by\nunderstanding their behavior on Markov sources.",
        "pos": [
            "We formalize the problem of prompt compression for large language models\n(LLMs) and present a framework to unify token-level prompt compression methods\nwhich create hard prompts for black-box models. We derive the distortion-rate\nfunction for this setup as a linear program, and provide an efficient algorithm\nto compute this fundamental limit via the dual of the linear program. Using the\ndistortion-rate function as the baseline, we study the performance of existing\ncompression schemes on a synthetic dataset consisting of prompts generated from\na Markov chain, natural language queries, and their respective answers. Our\nempirical analysis demonstrates the criticality of query-aware prompt\ncompression, where the compressor has knowledge of the downstream task/query\nfor the black-box LLM. We show that there is a large gap between the\nperformance of current prompt compression methods and the optimal strategy, and\npropose a query-aware, variable-rate adaptation of a prior work to close the\ngap. We extend our experiments to a small natural language dataset to further\nconfirm our findings on our synthetic dataset."
        ],
        "neg": []
    },
    {
        "query": "Attention-based transformers have been remarkably successful at modeling\ngenerative processes across various domains and modalities. In this paper, we\nstudy the behavior of transformers on data drawn from \\kth Markov processes,\nwhere the conditional distribution of the next symbol in a sequence depends on\nthe previous $k$ symbols observed. We observe a surprising phenomenon\nempirically which contradicts previous findings: when trained for sufficiently\nlong, a transformer with a fixed depth and $1$ head per layer is able to\nachieve low test loss on sequences drawn from \\kth Markov sources, even as $k$\ngrows. Furthermore, this low test loss is achieved by the transformer's ability\nto represent and learn the in-context conditional empirical distribution. On\nthe theoretical side, our main result is that a transformer with a single head\nand three layers can represent the in-context conditional empirical\ndistribution for \\kth Markov sources, concurring with our empirical\nobservations. Along the way, we prove that \\textit{attention-only} transformers\nwith $O(\\log_2(k))$ layers can represent the in-context conditional empirical\ndistribution by composing induction heads to track the previous $k$ symbols in\nthe sequence. These results provide more insight into our current understanding\nof the mechanisms by which transformers learn to capture context, by\nunderstanding their behavior on Markov sources.",
        "pos": [
            "We formalize the problem of prompt compression for large language models\n(LLMs) and present a framework to unify token-level prompt compression methods\nwhich create hard prompts for black-box models. We derive the distortion-rate\nfunction for this setup as a linear program, and provide an efficient algorithm\nto compute this fundamental limit via the dual of the linear program. Using the\ndistortion-rate function as the baseline, we study the performance of existing\ncompression schemes on a synthetic dataset consisting of prompts generated from\na Markov chain, natural language queries, and their respective answers. Our\nempirical analysis demonstrates the criticality of query-aware prompt\ncompression, where the compressor has knowledge of the downstream task/query\nfor the black-box LLM. We show that there is a large gap between the\nperformance of current prompt compression methods and the optimal strategy, and\npropose a query-aware, variable-rate adaptation of a prior work to close the\ngap. We extend our experiments to a small natural language dataset to further\nconfirm our findings on our synthetic dataset."
        ],
        "neg": []
    },
    {
        "query": "Attention-based transformers have been remarkably successful at modeling\ngenerative processes across various domains and modalities. In this paper, we\nstudy the behavior of transformers on data drawn from \\kth Markov processes,\nwhere the conditional distribution of the next symbol in a sequence depends on\nthe previous $k$ symbols observed. We observe a surprising phenomenon\nempirically which contradicts previous findings: when trained for sufficiently\nlong, a transformer with a fixed depth and $1$ head per layer is able to\nachieve low test loss on sequences drawn from \\kth Markov sources, even as $k$\ngrows. Furthermore, this low test loss is achieved by the transformer's ability\nto represent and learn the in-context conditional empirical distribution. On\nthe theoretical side, our main result is that a transformer with a single head\nand three layers can represent the in-context conditional empirical\ndistribution for \\kth Markov sources, concurring with our empirical\nobservations. Along the way, we prove that \\textit{attention-only} transformers\nwith $O(\\log_2(k))$ layers can represent the in-context conditional empirical\ndistribution by composing induction heads to track the previous $k$ symbols in\nthe sequence. These results provide more insight into our current understanding\nof the mechanisms by which transformers learn to capture context, by\nunderstanding their behavior on Markov sources.",
        "pos": [
            "We formalize the problem of prompt compression for large language models\n(LLMs) and present a framework to unify token-level prompt compression methods\nwhich create hard prompts for black-box models. We derive the distortion-rate\nfunction for this setup as a linear program, and provide an efficient algorithm\nto compute this fundamental limit via the dual of the linear program. Using the\ndistortion-rate function as the baseline, we study the performance of existing\ncompression schemes on a synthetic dataset consisting of prompts generated from\na Markov chain, natural language queries, and their respective answers. Our\nempirical analysis demonstrates the criticality of query-aware prompt\ncompression, where the compressor has knowledge of the downstream task/query\nfor the black-box LLM. We show that there is a large gap between the\nperformance of current prompt compression methods and the optimal strategy, and\npropose a query-aware, variable-rate adaptation of a prior work to close the\ngap. We extend our experiments to a small natural language dataset to further\nconfirm our findings on our synthetic dataset."
        ],
        "neg": []
    },
    {
        "query": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
        "pos": [
            "With the growing utilization of large language models (LLMs) across domains,\nalignment towards human preferences has become one of the most critical aspects\nof training models. At the forefront of state-of-the-art human alignment\nmethods are preference optimization methods (*PO). However, prior research has\noften concentrated on identifying the best-performing method, typically\ninvolving a grid search over hyperparameters, which can be impractical for\ngeneral practitioners. In this paper, we aim to identify the algorithm that,\nwhile being performant, is simultaneously more robust to varying\nhyperparameters, thereby increasing the likelihood of achieving better results.\nWe focus on a realistic out-of-distribution (OOD) scenario that mirrors\nreal-world applications of human alignment, offering practical insights into\nthe strengths and weaknesses of these methods. Furthermore, to better\nunderstand the shortcomings of generations from the different methods, we\nanalyze the model generations through the lens of KL divergence of the SFT\nmodel and the response length statistics. Our analysis reveals that the widely\nadopted DPO method consistently produces lengthy responses of inferior quality\nthat are very close to the SFT responses. Motivated by these findings, we\npropose an embarrassingly simple extension to the DPO algorithm, LN-DPO,\nresulting in more concise responses without sacrificing quality compared to the\npolicy obtained by vanilla DPO."
        ],
        "neg": []
    },
    {
        "query": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
        "pos": [
            "With the growing utilization of large language models (LLMs) across domains,\nalignment towards human preferences has become one of the most critical aspects\nof training models. At the forefront of state-of-the-art human alignment\nmethods are preference optimization methods (*PO). However, prior research has\noften concentrated on identifying the best-performing method, typically\ninvolving a grid search over hyperparameters, which can be impractical for\ngeneral practitioners. In this paper, we aim to identify the algorithm that,\nwhile being performant, is simultaneously more robust to varying\nhyperparameters, thereby increasing the likelihood of achieving better results.\nWe focus on a realistic out-of-distribution (OOD) scenario that mirrors\nreal-world applications of human alignment, offering practical insights into\nthe strengths and weaknesses of these methods. Furthermore, to better\nunderstand the shortcomings of generations from the different methods, we\nanalyze the model generations through the lens of KL divergence of the SFT\nmodel and the response length statistics. Our analysis reveals that the widely\nadopted DPO method consistently produces lengthy responses of inferior quality\nthat are very close to the SFT responses. Motivated by these findings, we\npropose an embarrassingly simple extension to the DPO algorithm, LN-DPO,\nresulting in more concise responses without sacrificing quality compared to the\npolicy obtained by vanilla DPO.",
            "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and\nMistral-7B and then evaluating them across a comprehensive suite of\nmultilingual benchmarks that test reasoning, question answering, and reading\ncomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with\nsPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared\nto the baselines. We also devise a strategy to incorporate N-shot examples in\neach fine-tuning sample which further boosts the performance of these models by\n3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other\nmultilingual instruction tuning datasets on the same benchmarks along with\nbeing sample efficient and diverse, thereby reducing dataset creation costs.\nAdditionally, instruction tuning with sPhinX does not lead to regression on\nmost standard LLM benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
        "pos": [
            "With the growing utilization of large language models (LLMs) across domains,\nalignment towards human preferences has become one of the most critical aspects\nof training models. At the forefront of state-of-the-art human alignment\nmethods are preference optimization methods (*PO). However, prior research has\noften concentrated on identifying the best-performing method, typically\ninvolving a grid search over hyperparameters, which can be impractical for\ngeneral practitioners. In this paper, we aim to identify the algorithm that,\nwhile being performant, is simultaneously more robust to varying\nhyperparameters, thereby increasing the likelihood of achieving better results.\nWe focus on a realistic out-of-distribution (OOD) scenario that mirrors\nreal-world applications of human alignment, offering practical insights into\nthe strengths and weaknesses of these methods. Furthermore, to better\nunderstand the shortcomings of generations from the different methods, we\nanalyze the model generations through the lens of KL divergence of the SFT\nmodel and the response length statistics. Our analysis reveals that the widely\nadopted DPO method consistently produces lengthy responses of inferior quality\nthat are very close to the SFT responses. Motivated by these findings, we\npropose an embarrassingly simple extension to the DPO algorithm, LN-DPO,\nresulting in more concise responses without sacrificing quality compared to the\npolicy obtained by vanilla DPO.",
            "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and\nMistral-7B and then evaluating them across a comprehensive suite of\nmultilingual benchmarks that test reasoning, question answering, and reading\ncomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with\nsPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared\nto the baselines. We also devise a strategy to incorporate N-shot examples in\neach fine-tuning sample which further boosts the performance of these models by\n3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other\nmultilingual instruction tuning datasets on the same benchmarks along with\nbeing sample efficient and diverse, thereby reducing dataset creation costs.\nAdditionally, instruction tuning with sPhinX does not lead to regression on\nmost standard LLM benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
        "pos": [
            "With the growing utilization of large language models (LLMs) across domains,\nalignment towards human preferences has become one of the most critical aspects\nof training models. At the forefront of state-of-the-art human alignment\nmethods are preference optimization methods (*PO). However, prior research has\noften concentrated on identifying the best-performing method, typically\ninvolving a grid search over hyperparameters, which can be impractical for\ngeneral practitioners. In this paper, we aim to identify the algorithm that,\nwhile being performant, is simultaneously more robust to varying\nhyperparameters, thereby increasing the likelihood of achieving better results.\nWe focus on a realistic out-of-distribution (OOD) scenario that mirrors\nreal-world applications of human alignment, offering practical insights into\nthe strengths and weaknesses of these methods. Furthermore, to better\nunderstand the shortcomings of generations from the different methods, we\nanalyze the model generations through the lens of KL divergence of the SFT\nmodel and the response length statistics. Our analysis reveals that the widely\nadopted DPO method consistently produces lengthy responses of inferior quality\nthat are very close to the SFT responses. Motivated by these findings, we\npropose an embarrassingly simple extension to the DPO algorithm, LN-DPO,\nresulting in more concise responses without sacrificing quality compared to the\npolicy obtained by vanilla DPO."
        ],
        "neg": []
    },
    {
        "query": "This paper presents our proposed approach to the Discharge Me! shared task,\ncollocated with the 23th Workshop on Biomedical Natural Language Processing\n(BioNLP). In this work, we develop an LLM-based framework for solving the\nDischarge Summary Documentation (DSD) task, i.e., generating the two critical\ntarget sections `Brief Hospital Course' and `Discharge Instructions' in the\ndischarge summary. By streamlining the recent instruction-finetuning process on\nLLMs, we explore several prompting strategies for optimally adapting LLMs to\nspecific generation task of DSD. Experimental results show that providing a\nclear output structure, complimented by a set of comprehensive\nChain-of-Thoughts (CoT) questions, effectively improves the model's reasoning\ncapability, and thereby, enhancing the structural correctness and faithfulness\nof clinical information in the generated text. Source code is available at:\nhttps://github.com/antangrocket1312/Discharge_LLM",
        "pos": [
            "Key Point Analysis (KPA) aims for quantitative summarization that provides\nkey points (KPs) as succinct textual summaries and quantities measuring their\nprevalence. KPA studies for arguments and reviews have been reported in the\nliterature. A majority of KPA studies for reviews adopt supervised learning to\nextract short sentences as KPs before matching KPs to review comments for\nquantification of KP prevalence. Recent abstractive approaches still generate\nKPs based on sentences, often leading to KPs with overlapping and hallucinated\nopinions, and inaccurate quantification. In this paper, we propose Prompted\nAspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA\nemploys aspect sentiment analysis and prompted in-context learning with Large\nLanguage Models (LLMs) to generate and quantify KPs grounded in aspects for\nbusiness entities, which achieves faithful KPs with accurate quantification,\nand removes the need for large amounts of annotated data for supervised\ntraining. Experiments on the popular review dataset Yelp and the\naspect-oriented review summarization dataset SPACE show that our framework\nachieves state-of-the-art performance. Source code and data are available at:\nhttps://github.com/antangrocket1312/PAKPA"
        ],
        "neg": []
    },
    {
        "query": "This paper presents our proposed approach to the Discharge Me! shared task,\ncollocated with the 23th Workshop on Biomedical Natural Language Processing\n(BioNLP). In this work, we develop an LLM-based framework for solving the\nDischarge Summary Documentation (DSD) task, i.e., generating the two critical\ntarget sections `Brief Hospital Course' and `Discharge Instructions' in the\ndischarge summary. By streamlining the recent instruction-finetuning process on\nLLMs, we explore several prompting strategies for optimally adapting LLMs to\nspecific generation task of DSD. Experimental results show that providing a\nclear output structure, complimented by a set of comprehensive\nChain-of-Thoughts (CoT) questions, effectively improves the model's reasoning\ncapability, and thereby, enhancing the structural correctness and faithfulness\nof clinical information in the generated text. Source code is available at:\nhttps://github.com/antangrocket1312/Discharge_LLM",
        "pos": [
            "Key Point Analysis (KPA) aims for quantitative summarization that provides\nkey points (KPs) as succinct textual summaries and quantities measuring their\nprevalence. KPA studies for arguments and reviews have been reported in the\nliterature. A majority of KPA studies for reviews adopt supervised learning to\nextract short sentences as KPs before matching KPs to review comments for\nquantification of KP prevalence. Recent abstractive approaches still generate\nKPs based on sentences, often leading to KPs with overlapping and hallucinated\nopinions, and inaccurate quantification. In this paper, we propose Prompted\nAspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA\nemploys aspect sentiment analysis and prompted in-context learning with Large\nLanguage Models (LLMs) to generate and quantify KPs grounded in aspects for\nbusiness entities, which achieves faithful KPs with accurate quantification,\nand removes the need for large amounts of annotated data for supervised\ntraining. Experiments on the popular review dataset Yelp and the\naspect-oriented review summarization dataset SPACE show that our framework\nachieves state-of-the-art performance. Source code and data are available at:\nhttps://github.com/antangrocket1312/PAKPA"
        ],
        "neg": []
    },
    {
        "query": "This paper presents our proposed approach to the Discharge Me! shared task,\ncollocated with the 23th Workshop on Biomedical Natural Language Processing\n(BioNLP). In this work, we develop an LLM-based framework for solving the\nDischarge Summary Documentation (DSD) task, i.e., generating the two critical\ntarget sections `Brief Hospital Course' and `Discharge Instructions' in the\ndischarge summary. By streamlining the recent instruction-finetuning process on\nLLMs, we explore several prompting strategies for optimally adapting LLMs to\nspecific generation task of DSD. Experimental results show that providing a\nclear output structure, complimented by a set of comprehensive\nChain-of-Thoughts (CoT) questions, effectively improves the model's reasoning\ncapability, and thereby, enhancing the structural correctness and faithfulness\nof clinical information in the generated text. Source code is available at:\nhttps://github.com/antangrocket1312/Discharge_LLM",
        "pos": [
            "Key Point Analysis (KPA) aims for quantitative summarization that provides\nkey points (KPs) as succinct textual summaries and quantities measuring their\nprevalence. KPA studies for arguments and reviews have been reported in the\nliterature. A majority of KPA studies for reviews adopt supervised learning to\nextract short sentences as KPs before matching KPs to review comments for\nquantification of KP prevalence. Recent abstractive approaches still generate\nKPs based on sentences, often leading to KPs with overlapping and hallucinated\nopinions, and inaccurate quantification. In this paper, we propose Prompted\nAspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA\nemploys aspect sentiment analysis and prompted in-context learning with Large\nLanguage Models (LLMs) to generate and quantify KPs grounded in aspects for\nbusiness entities, which achieves faithful KPs with accurate quantification,\nand removes the need for large amounts of annotated data for supervised\ntraining. Experiments on the popular review dataset Yelp and the\naspect-oriented review summarization dataset SPACE show that our framework\nachieves state-of-the-art performance. Source code and data are available at:\nhttps://github.com/antangrocket1312/PAKPA"
        ],
        "neg": []
    },
    {
        "query": "When seeking information from unfamiliar documents, users frequently pose\nquestions that cannot be answered by the documents. While existing large\nlanguage models (LLMs) identify these unanswerable questions, they do not\nassist users in reformulating their questions, thereby reducing their overall\nutility. We curate CouldAsk, an evaluation benchmark composed of existing and\nnew datasets for document-grounded question answering, specifically designed to\nstudy reformulating unanswerable questions. We evaluate state-of-the-art\nopen-source and proprietary LLMs on CouldAsk. The results demonstrate the\nlimited capabilities of these models in reformulating questions. Specifically,\nGPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the\ntime, respectively. Error analysis shows that 62% of the unsuccessful\nreformulations stem from the models merely rephrasing the questions or even\ngenerating identical questions. We publicly release the benchmark and the code\nto reproduce the experiments.",
        "pos": [
            "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations."
        ],
        "neg": []
    },
    {
        "query": "When seeking information from unfamiliar documents, users frequently pose\nquestions that cannot be answered by the documents. While existing large\nlanguage models (LLMs) identify these unanswerable questions, they do not\nassist users in reformulating their questions, thereby reducing their overall\nutility. We curate CouldAsk, an evaluation benchmark composed of existing and\nnew datasets for document-grounded question answering, specifically designed to\nstudy reformulating unanswerable questions. We evaluate state-of-the-art\nopen-source and proprietary LLMs on CouldAsk. The results demonstrate the\nlimited capabilities of these models in reformulating questions. Specifically,\nGPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the\ntime, respectively. Error analysis shows that 62% of the unsuccessful\nreformulations stem from the models merely rephrasing the questions or even\ngenerating identical questions. We publicly release the benchmark and the code\nto reproduce the experiments.",
        "pos": [
            "Predicting emotions elicited by news headlines can be challenging as the task\nis largely influenced by the varying nature of people's interpretations and\nbackgrounds. Previous works have explored classifying discrete emotions\ndirectly from news headlines. We provide a different approach to tackling this\nproblem by utilizing people's explanations of their emotion, written in\nfree-text, on how they feel after reading a news headline. Using the dataset\nBU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the\nfree-text explanations have a strong correlation with the dominant emotion\nelicited by the headlines. The free-text explanations also contain more\nsentimental context than the news headlines alone and can serve as a better\ninput to emotion classification models. Therefore, in this work we explored\ngenerating emotion explanations from headlines by training a\nsequence-to-sequence transformer model and by using pretrained large language\nmodel, ChatGPT (GPT-4). We then used the generated emotion explanations for\nemotion classification. In addition, we also experimented with training the\npretrained T5 model for the intermediate task of explanation generation before\nfine-tuning it for emotion classification. Using McNemar's significance test,\nmethods that incorporate GPT-generated free-text emotion explanations\ndemonstrated significant improvement (P-value < 0.05) in emotion classification\nfrom headlines, compared to methods that only use headlines. This underscores\nthe value of using intermediate free-text explanations for emotion prediction\ntasks with headlines."
        ],
        "neg": []
    },
    {
        "query": "When seeking information from unfamiliar documents, users frequently pose\nquestions that cannot be answered by the documents. While existing large\nlanguage models (LLMs) identify these unanswerable questions, they do not\nassist users in reformulating their questions, thereby reducing their overall\nutility. We curate CouldAsk, an evaluation benchmark composed of existing and\nnew datasets for document-grounded question answering, specifically designed to\nstudy reformulating unanswerable questions. We evaluate state-of-the-art\nopen-source and proprietary LLMs on CouldAsk. The results demonstrate the\nlimited capabilities of these models in reformulating questions. Specifically,\nGPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the\ntime, respectively. Error analysis shows that 62% of the unsuccessful\nreformulations stem from the models merely rephrasing the questions or even\ngenerating identical questions. We publicly release the benchmark and the code\nto reproduce the experiments.",
        "pos": [
            "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations."
        ],
        "neg": []
    },
    {
        "query": "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.",
        "pos": [
            "Literature search questions, such as \"where can I find research on the\nevaluation of consistency in generated summaries?\" pose significant challenges\nfor modern search engines and retrieval systems. These questions often require\na deep understanding of research concepts and the ability to reason over entire\narticles. In this work, we introduce LitSearch, a retrieval benchmark\ncomprising 597 realistic literature search queries about recent ML and NLP\npapers. LitSearch is constructed using a combination of (1) questions generated\nby GPT-4 based on paragraphs containing inline citations from research papers\nand (2) questions about recently published papers, manually written by their\nauthors. All LitSearch questions were manually examined or edited by experts to\nensure high quality. We extensively benchmark state-of-the-art retrieval models\nand also evaluate two LLM-based reranking pipelines. We find a significant\nperformance gap between BM25 and state-of-the-art dense retrievers, with a\n24.8% difference in absolute recall@5. The LLM-based reranking strategies\nfurther improve the best-performing dense retriever by 4.4%. Additionally,\ncommercial search engines and research tools like Google Search perform poorly\non LitSearch, lagging behind the best dense retriever by 32 points. Taken\ntogether, these results show that LitSearch is an informative new testbed for\nretrieval systems while catering to a real-world use case."
        ],
        "neg": []
    },
    {
        "query": "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.",
        "pos": [
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.",
        "pos": [
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Therefore, if we value the balance between efficiency and\neffectiveness, CMR can be consider as the optimal mixture ratio.Through\nextensive experiments, we ascertain the predictability of CMR, and propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.",
        "pos": [
            "In this work, we report our efforts to advance the standard operation\nprocedure of developing Large Language Models (LLMs) or LLMs-based systems or\nservices in industry. We introduce the concept of Large Language Model\nDevelopment Lifecycle (LDLC) and then highlight the importance of consistency\ntest in ensuring the delivery quality. The principled solution of consistency\ntest, however, is usually overlooked by industrial practitioners and not urgent\nin academia, and current practical solutions are insufficiently rigours and\nlabor-intensive. We thus propose a simple yet effective consistency test\nprotocol, named SimCT. SimCT is mainly to proactively check the consistency\nacross different development stages of \"bare metal\" LLMs or associated services\nwithout accessing the model artifacts, in an attempt to expedite the delivery\nby reducing the back-and-forth alignment communications among multiple teams\ninvolved in different development stages.\n  Specifically, SimCT encompasses response-wise and model-wise tests. We\nimplement the protocol with LightGBM and Student's t-test for two components\nrespectively, and perform extensive experiments to substantiate the\neffectiveness of SimCT and the involved components."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Therefore, if we value the balance between efficiency and\neffectiveness, CMR can be consider as the optimal mixture ratio.Through\nextensive experiments, we ascertain the predictability of CMR, and propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.",
        "pos": [
            "In this work, we report our efforts to advance the standard operation\nprocedure of developing Large Language Models (LLMs) or LLMs-based systems or\nservices in industry. We introduce the concept of Large Language Model\nDevelopment Lifecycle (LDLC) and then highlight the importance of consistency\ntest in ensuring the delivery quality. The principled solution of consistency\ntest, however, is usually overlooked by industrial practitioners and not urgent\nin academia, and current practical solutions are insufficiently rigours and\nlabor-intensive. We thus propose a simple yet effective consistency test\nprotocol, named SimCT. SimCT is mainly to proactively check the consistency\nacross different development stages of \"bare metal\" LLMs or associated services\nwithout accessing the model artifacts, in an attempt to expedite the delivery\nby reducing the back-and-forth alignment communications among multiple teams\ninvolved in different development stages.\n  Specifically, SimCT encompasses response-wise and model-wise tests. We\nimplement the protocol with LightGBM and Student's t-test for two components\nrespectively, and perform extensive experiments to substantiate the\neffectiveness of SimCT and the involved components."
        ],
        "neg": []
    },
    {
        "query": "The rapid advancement of language models (LMs) necessitates robust alignment\nwith diverse user values. However, current preference optimization approaches\noften fail to capture the plurality of user opinions, instead reinforcing\nmajority viewpoints and marginalizing minority perspectives. We introduce\nPERSONA, a reproducible test bed designed to evaluate and improve pluralistic\nalignment of LMs. We procedurally generate diverse user profiles from US census\ndata, resulting in 1,586 synthetic personas with varied demographic and\nidiosyncratic attributes. We then generate a large-scale evaluation dataset\ncontaining 3,868 prompts and 317,200 feedback pairs obtained from our synthetic\npersonas. Leveraging this dataset, we systematically evaluate LM capabilities\nin role-playing diverse users, verified through human judges, and the\nestablishment of both a benchmark, PERSONA Bench, for pluralistic alignment\napproaches as well as an extensive dataset to create new and future benchmarks.\nThe full dataset and benchmarks are available here:\nhttps://www.synthlabs.ai/research/persona.",
        "pos": [
            "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench."
        ],
        "neg": []
    },
    {
        "query": "The rapid advancement of language models (LMs) necessitates robust alignment\nwith diverse user values. However, current preference optimization approaches\noften fail to capture the plurality of user opinions, instead reinforcing\nmajority viewpoints and marginalizing minority perspectives. We introduce\nPERSONA, a reproducible test bed designed to evaluate and improve pluralistic\nalignment of LMs. We procedurally generate diverse user profiles from US census\ndata, resulting in 1,586 synthetic personas with varied demographic and\nidiosyncratic attributes. We then generate a large-scale evaluation dataset\ncontaining 3,868 prompts and 317,200 feedback pairs obtained from our synthetic\npersonas. Leveraging this dataset, we systematically evaluate LM capabilities\nin role-playing diverse users, verified through human judges, and the\nestablishment of both a benchmark, PERSONA Bench, for pluralistic alignment\napproaches as well as an extensive dataset to create new and future benchmarks.\nThe full dataset and benchmarks are available here:\nhttps://www.synthlabs.ai/research/persona.",
        "pos": [
            "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench."
        ],
        "neg": []
    },
    {
        "query": "Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process.",
        "pos": [
            "Large multimodal models (LMMs) are processing increasingly longer and richer\ninputs. Albeit the progress, few public benchmark is available to measure such\ndevelopment. To mitigate this gap, we introduce LongVideoBench, a\nquestion-answering benchmark that features video-language interleaved inputs up\nto an hour long. Our benchmark includes 3,763 varying-length web-collected\nvideos with their subtitles across diverse themes, designed to comprehensively\nevaluate LMMs on long-term multimodal understanding. To achieve this, we\ninterpret the primary challenge as to accurately retrieve and reason over\ndetailed multimodal information from long inputs. As such, we formulate a novel\nvideo question-answering task termed referring reasoning. Specifically, as part\nof the question, it contains a referring query that references related video\ncontexts, called referred context. The model is then required to reason over\nrelevant video details from the referred context. Following the paradigm of\nreferring reasoning, we curate 6,678 human-annotated multiple-choice questions\nin 17 fine-grained categories, establishing one of the most comprehensive\nbenchmarks for long-form video understanding. Evaluations suggest that the\nLongVideoBench presents significant challenges even for the most advanced\nproprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their\nopen-source counterparts show an even larger performance gap. In addition, our\nresults indicate that model performance on the benchmark improves only when\nthey are capable of processing more frames, positioning LongVideoBench as a\nvaluable benchmark for evaluating future-generation long-context LMMs."
        ],
        "neg": []
    },
    {
        "query": "In this study, we address the growing issue of misleading charts, a prevalent\nproblem that undermines the integrity of information dissemination. Misleading\ncharts can distort the viewer's perception of data, leading to\nmisinterpretations and decisions based on false information. The development of\neffective automatic detection methods for misleading charts is an urgent field\nof research. The recent advancement of multimodal Large Language Models (LLMs)\nhas introduced a promising direction for addressing this challenge. We explored\nthe capabilities of these models in analyzing complex charts and assessing the\nimpact of different prompting strategies on the models' analyses. We utilized a\ndataset of misleading charts collected from the internet by prior research and\ncrafted nine distinct prompts, ranging from simple to complex, to test the\nability of four different multimodal LLMs in detecting over 21 different chart\nissues. Through three experiments--from initial exploration to detailed\nanalysis--we progressively gained insights into how to effectively prompt LLMs\nto identify misleading charts and developed strategies to address the\nscalability challenges encountered as we expanded our detection range from the\ninitial five issues to 21 issues in the final experiment. Our findings reveal\nthat multimodal LLMs possess a strong capability for chart comprehension and\ncritical thinking in data interpretation. There is significant potential in\nemploying multimodal LLMs to counter misleading information by supporting\ncritical thinking and enhancing visualization literacy. This study demonstrates\nthe applicability of LLMs in addressing the pressing concern of misleading\ncharts.",
        "pos": [
            "Large Language Models (LLMs) have gained significant attention but also\nraised concerns due to the risk of misuse. Jailbreak prompts, a popular type of\nadversarial attack towards LLMs, have appeared and constantly evolved to breach\nthe safety protocols of LLMs. To address this issue, LLMs are regularly updated\nwith safety patches based on reported jailbreak prompts. However, malicious\nusers often keep their successful jailbreak prompts private to exploit LLMs. To\nuncover these private jailbreak prompts, extensive analysis of large-scale\nconversational datasets is necessary to identify prompts that still manage to\nbypass the system's defenses. This task is highly challenging due to the\nimmense volume of conversation data, diverse characteristics of jailbreak\nprompts, and their presence in complex multi-turn conversations. To tackle\nthese challenges, we introduce JailbreakHunter, a visual analytics approach for\nidentifying jailbreak prompts in large-scale human-LLM conversational datasets.\nWe have designed a workflow with three analysis levels: group-level,\nconversation-level, and turn-level. Group-level analysis enables users to grasp\nthe distribution of conversations and identify suspicious conversations using\nmultiple criteria, such as similarity with reported jailbreak prompts in\nprevious research and attack success rates. Conversation-level analysis\nfacilitates the understanding of the progress of conversations and helps\ndiscover jailbreak prompts within their conversation contexts. Turn-level\nanalysis allows users to explore the semantic similarity and token overlap\nbetween a singleturn prompt and the reported jailbreak prompts, aiding in the\nidentification of new jailbreak strategies. The effectiveness and usability of\nthe system were verified through multiple case studies and expert interviews."
        ],
        "neg": []
    },
    {
        "query": "Recently, large language models have presented promising results in aiding\nformal mathematical reasoning. However, their performance is restricted due to\nthe scarcity of formal theorem-proving data, which requires additional effort\nto be extracted from raw formal language corpora. Meanwhile, a significant\namount of human-written formal language corpora remains underutilized. To\naddress this issue, we propose LEAN-GitHub, a dataset consisting of large-scale\nformal data extracted from almost all Lean 4 repositories on GitHub. After\nfine-tuning InternLM-math-plus on this dataset, our model achieved accuracies\nof 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F\ntest, surpassing state-of-the-art method at 52%. And it also achieves\nstate-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting\ndifferent fields/levels of math. These results demonstrate that our proposed\ndataset is beneficial for formal reasoning on a wide range of math topics. We\nopen-source our model at https://GitHub. com/InternLM/InternLM-Math and our\ndata at https://huggingface.co/ datasets/InternLM/Lean-GitHub",
        "pos": [
            "Complex reasoning is an impressive ability shown by large language models\n(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought\nprompting or iterative tool-using to solve challenging tasks step-by-step. In\nthis paper, we hope to focus on evaluating and teaching LLMs to conduct\ninductive reasoning, that is, LLMs are supposed to infer underlying rules by\nobserving examples or sequential transformations. However, collecting\nlarge-scale and diverse human-generated inductive data is challenging. We focus\non data synthesis in the code domain and propose a \\textbf{Case2Code} task by\nexploiting the expressiveness and correctness of programs. Specifically, we\ncollect a diverse set of executable programs, synthesize input-output\ntransformations for each program, and force LLMs to infer the underlying code\nimplementations based on the synthetic I/O cases. We first evaluate\nrepresentative LLMs on the synthesized Case2Code task and demonstrate that the\nCase-to-code induction is challenging for LLMs. Then, we synthesize large-scale\nCase2Code training samples to train LLMs to perform inductive reasoning.\nExperimental results show that such induction training benefits not only in\ndistribution Case2Code performance but also enhances various coding abilities\nof trained LLMs, demonstrating the great potential of learning inductive\nreasoning via synthetic data.",
            "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.",
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer."
        ],
        "neg": []
    },
    {
        "query": "Transparency in AI decision-making is crucial in healthcare due to the severe\nconsequences of errors, and this is important for building trust among AI and\nusers in sentiment analysis task. Incorporating reasoning capabilities helps\nLarge Language Models (LLMs) understand human emotions within broader contexts,\nhandle nuanced and ambiguous language, and infer underlying sentiments that may\nnot be explicitly stated. In this work, we introduce a new task - Sentiment\nReasoning - for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Our study showed that\nrationale-augmented training enhances model performance in sentiment\nclassification across both human transcript and ASR settings. Also, we found\nthat the generated rationales typically exhibit different vocabularies compared\nto human-generated rationales, but maintain similar semantics. All code, data\n(English-translated and Vietnamese) and models are published online:\nhttps://github.com/leduckhai/MultiMed",
        "pos": [
            "Vision-language models have been extensively explored across a wide range of\ntasks, achieving satisfactory performance; however, their application in\nmedical imaging remains underexplored. In this work, we propose a unified\nframework - LiteGPT - for the medical imaging. We leverage multiple pre-trained\nvisual encoders to enrich information and enhance the performance of\nvision-language models. To the best of our knowledge, this is the first study\nto utilize vision-language models for the novel task of joint localization and\nclassification in medical images. Besides, we are pioneers in providing\nbaselines for disease localization in chest X-rays. Finally, we set new\nstate-of-the-art performance in the image classification task on the\nwell-benchmarked VinDr-CXR dataset. All code and models are publicly available\nonline: https://github.com/leduckhai/LiteGPT"
        ],
        "neg": []
    },
    {
        "query": "Social media posts provide valuable insight into the narrative of users and\ntheir intentions, including providing an opportunity to automatically model\nwhether a social media user is depressed or not. The challenge lies in\nfaithfully modelling user narratives from their online social media posts,\nwhich could potentially be useful in several different applications. We have\ndeveloped a novel and effective model called \\texttt{NarrationDep}, which\nfocuses on detecting narratives associated with depression. By analyzing a\nuser's tweets, \\texttt{NarrationDep} accurately identifies crucial narratives.\n\\texttt{NarrationDep} is a deep learning framework that jointly models\nindividual user tweet representations and clusters of users' tweets. As a\nresult, \\texttt{NarrationDep} is characterized by a novel two-layer deep\nlearning model: the first layer models using social media text posts, and the\nsecond layer learns semantic representations of tweets associated with a\ncluster. To faithfully model these cluster representations, the second layer\nincorporates a novel component that hierarchically learns from users' posts.\nThe results demonstrate that our framework outperforms other comparative models\nincluding recently developed models on a variety of datasets.",
        "pos": [
            "With an increasing social demand for fine-grained sentiment analysis (SA),\nimplicit sentiment analysis (ISA) poses a significant challenge with the\nabsence of salient cue words in expressions. It necessitates reliable reasoning\nto understand how the sentiment is aroused and thus determine implicit\nsentiments. In the era of Large Language Models (LLMs), Encoder-Decoder (ED)\nLLMs have gained popularity to serve as backbone models for SA applications,\nconsidering impressive text comprehension and reasoning ability among diverse\ntasks. On the other hand, Decoder-only (DO) LLMs exhibit superior natural\nlanguage generation and in-context learning capabilities. However, their\nresponses may contain misleading or inaccurate information. To identify\nimplicit sentiment with reliable reasoning, this study proposes RVISA, a\ntwo-stage reasoning framework that harnesses the generation ability of DO LLMs\nand the reasoning ability of ED LLMs to train an enhanced reasoner.\nSpecifically, we adopt three-hop reasoning prompting to explicitly furnish\nsentiment elements as cues. The generated rationales are utilized to fine-tune\nan ED LLM into a skilled reasoner. Additionally, we develop a straightforward\nyet effective verification mechanism to ensure the reliability of the reasoning\nlearning. We evaluated the proposed method on two benchmark datasets and\nachieved state-of-the-art results in ISA performance."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we experimented with the SpeechT5 model pre-trained on\nlarge-scale datasets. We pre-trained the foundation model from scratch and\nfine-tuned it on a large-scale robust multi-speaker text-to-speech (TTS) task.\nWe tested the model capabilities in a zero- and few-shot scenario. Based on two\nlistening tests, we evaluated the synthetic audio quality and the similarity of\nhow synthetic voices resemble real voices. Our results showed that the SpeechT5\nmodel can generate a synthetic voice for any speaker using only one minute of\nthe target speaker's data. We successfully demonstrated the high quality and\nsimilarity of our synthetic voices on publicly known Czech politicians and\ncelebrities.",
        "pos": [
            "In this paper, we are comparing monolingual Wav2Vec 2.0 models with various\nmultilingual models to see whether we could improve speech recognition\nperformance on a unique oral history archive containing a lot of mixed-language\nsentences. Our main goal is to push forward research on this unique dataset,\nwhich is an extremely valuable part of our cultural heritage. Our results\nsuggest that monolingual speech recognition models are, in most cases, superior\nto multilingual models, even when processing the oral history archive full of\nmixed-language sentences from non-native speakers. We also performed the same\nexperiments on the public CommonVoice dataset to verify our results. We are\ncontributing to the research community by releasing our pre-trained models to\nthe public."
        ],
        "neg": []
    },
    {
        "query": "Extracting social determinants of health (SDoH) from unstructured medical\nnotes depends heavily on labor-intensive annotations, which are typically\ntask-specific, hampering reusability and limiting sharing. In this study we\nintroduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)\nmethod leveraging contrastive examples and concise instructions to extract SDoH\nwithout relying on extensive medical annotations or costly human intervention.\nIt achieved tenfold and twentyfold reductions in time and cost respectively,\nand superior consistency with human annotators measured by Cohen's kappa of up\nto 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the\nstrengths of both, ensuring high accuracy and computational efficiency while\nconsistently maintaining 0.90+ AUROC scores. Testing across three distinct\ndatasets has confirmed its robustness and accuracy. This study highlights the\npotential of leveraging LLMs to revolutionize medical note classification,\ndemonstrating their capability to achieve highly accurate classifications with\nsignificantly reduced time and cost.",
        "pos": [
            "In this paper, we introduce Dynamic Layer Operations (DLO), a novel approach\nfor vertically scaling transformer-based Large Language Models (LLMs) by\ndynamically expanding, activating, or skipping layers using a sophisticated\nrouting policy based on layerwise feature similarity. Unlike traditional\nMixture-of-Experts (MoE) methods that focus on extending the model width, our\napproach targets model depth, addressing the redundancy observed across layer\nrepresentations for various input samples. Our framework is integrated with the\nSupervised Fine-Tuning (SFT) stage, eliminating the need for resource-intensive\nContinual Pre-Training (CPT). Experimental results demonstrate that DLO not\nonly outperforms the original unscaled models but also achieves comparable\nresults to densely expanded models with significantly improved efficiency. Our\nwork offers a promising direction for building efficient yet powerful LLMs. We\nwill release our implementation and model weights upon acceptance.",
            "Recent work has explored the capability of large language models (LLMs) to\nidentify and correct errors in LLM-generated responses. These refinement\napproaches frequently evaluate what sizes of models are able to do refinement\nfor what problems, but less attention is paid to what effective feedback for\nrefinement looks like. In this work, we propose looking at refinement with\nfeedback as a composition of three distinct LLM competencies: (1)\nidentification of bad generations; (2) fine-grained natural language feedback\ngeneration; (3) refining with fine-grained feedback. The first step can be\nimplemented with a high-performing discriminative model and steps 2 and 3 can\nbe implemented either via prompted or fine-tuned LLMs. A key property of this\napproach is that the step 2 critique model can give fine-grained feedback about\nerrors, made possible by offloading the discrimination to a separate model in\nstep 1. We show that models of different capabilities benefit from refining\nwith this approach on the task of improving factual consistency of document\ngrounded summaries. Overall, our proposed method consistently outperforms\nexisting end-to-end refinement approaches and current trained models not\nfine-tuned for factuality critiquing."
        ],
        "neg": []
    },
    {
        "query": "One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. In this paper, we\nfocus on entity type ambiguity and analyze current state-of-the-art LLMs for\ntheir proficiency and consistency in applying their factual knowledge when\nprompted for entities under ambiguity. To do so, we propose an evaluation\nprotocol that disentangles knowing from applying knowledge, and test\nstate-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs perform\npoorly with ambiguous prompts, achieving only 80% accuracy. Our results further\ndemonstrate systematic discrepancies in LLM behavior and their failure to\nconsistently apply information, indicating that the models can exhibit\nknowledge without being able to utilize it, significant biases for preferred\nreadings, as well as self inconsistencies. Our study highlights the importance\nof handling entity ambiguity in future for more trustworthy LLMs",
        "pos": [
            "A diverse range of large language models (LLMs), e.g., ChatGPT, and visual\nquestion answering (VQA) models, e.g., BLIP, have been developed for solving\ntextual and visual question answering tasks. However, both LLMs and VQA models\nencounter challenges when applied to task-specific datasets. Fine-tuning these\nmodels is either difficult, as it requires access via APIs, rendering them as\nblack-boxes, or costly due to the need of tuning a large number of parameters.\nTo address this, we introduce InfoSel, a data-efficient and lightweight\nensemble method that learns to dynamically pick the winner from existing\nblack-box models for predictions on both textual and multimodal visual question\nanswering tasks. Unlike traditional ensemble models, InfoSel does not rely on\nprediction probabilities or confidences, which typically are not available in\nblack-box models. Experimental results on four datasets demonstrate that our\napproach achieves an absolute increase of up to +5.27% in the F1-score compared\nto standalone LLMs. Remarkably, this improvement is achieved by utilizing only\n1K training instances and 110M model parameters for training task-specific\nensemble models.",
            "One way to personalize and steer generations from large language models (LLM)\nis to assign a persona: a role that describes how the user expects the LLM to\nbehave (e.g., a helpful assistant, a teacher, a woman). This paper investigates\nhow personas affect diverse aspects of model behavior. We assign to seven LLMs\n162 personas from 12 categories spanning variables like gender, sexual\norientation, and occupation. We prompt them to answer questions from five\ndatasets covering objective (e.g., questions about math and history) and\nsubjective tasks (e.g., questions about beliefs and values). We also compare\npersona's generations to two baseline settings: a control persona setting with\n30 paraphrases of \"a helpful assistant\" to control for models' prompt\nsensitivity, and an empty persona setting where no persona is assigned. We find\nthat for all models and datasets, personas show greater variability than the\ncontrol setting and that some measures of persona behavior generalize across\nmodels."
        ],
        "neg": []
    },
    {
        "query": "Large language model (LLM)-based evaluation pipelines have demonstrated their\ncapability to robustly evaluate machine-generated text. Extending this\nmethodology to assess human-written text could significantly benefit\neducational settings by providing direct feedback to enhance writing skills,\nalthough this application is not straightforward. In this paper, we investigate\nwhether LLMs can effectively assess human-written text for educational\npurposes. We collected 100 texts from 32 Korean students across 15 types of\nwriting and employed GPT-4-Turbo to evaluate them using grammaticality,\nfluency, coherence, consistency, and relevance as criteria. Our analyses\nindicate that LLM evaluators can reliably assess grammaticality and fluency, as\nwell as more objective types of writing, though they struggle with other\ncriteria and types of writing. We publicly release our dataset and feedback.",
        "pos": [
            "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates Who's Harry Potter (WHP), a pioneering yet\ninsufficiently understood method for LLM unlearning. We explore it in two\nsteps. First, we introduce a new task of LLM targeted unlearning, where given\nan unlearning target (e.g., a person) and some unlearning documents, we aim to\nunlearn only the information about the target, rather than everything in the\nunlearning documents. We further argue that a successful unlearning should\nsatisfy criteria such as not outputting gibberish, not fabricating facts about\nthe unlearning target, and not releasing factual information under jailbreak\nattacks. Second, we construct a causal intervention framework for targeted\nunlearning, where the knowledge of the unlearning target is modeled as a\nconfounder between LLM input and output, and the unlearning process as a\ndeconfounding process. This framework justifies and extends WHP, deriving a\nsimple unlearning algorithm that includes WHP as a special case. Experiments on\nexisting and new datasets show that our approach, without explicitly optimizing\nfor the aforementioned criteria, achieves competitive performance in all of\nthem. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/causal_unlearn.git.",
        "pos": [
            "In-context learning (ICL) is a recent advancement in the capabilities of\nlarge language models (LLMs). This feature allows users to perform a new task\nwithout updating the model. Concretely, users can address tasks during the\ninference time by conditioning on a few input-label pair demonstrations along\nwith the test input. It is different than the conventional fine-tuning paradigm\nand offers more flexibility. However, this capability also introduces potential\nissues. For example, users may use the model on any data without restriction,\nsuch as performing tasks with improper or sensitive content, which might\nviolate the model policy or conflict with the model owner's interests. As a\nmodel owner, it is crucial to establish a mechanism to control the model's\nbehavior under ICL, depending on the model owner's requirements for various\ncontent. To this end, we introduce the concept of \"applicability authorization\"\ntailored for LLMs, particularly for ICL behavior, and propose a simple\napproach, ICLGuard. It is a fine-tuning framework designed to allow the model\nowner to regulate ICL behavior on different data. ICLGuard preserves the\noriginal LLM and fine-tunes only a minimal set of additional trainable\nparameters to \"guard\" the LLM. Empirical results show that the guarded LLM can\ndeactivate its ICL ability on target data without affecting its ICL ability on\nother data and its general functionality across all data.",
            "Open-source large language models (LLMs) have become increasingly popular\namong both the general public and industry, as they can be customized,\nfine-tuned, and freely used. However, some open-source LLMs require approval\nbefore usage, which has led to third parties publishing their own easily\naccessible versions. Similarly, third parties have been publishing fine-tuned\nor quantized variants of these LLMs. These versions are particularly appealing\nto users because of their ease of access and reduced computational resource\ndemands. This trend has increased the risk of training time attacks,\ncompromising the integrity and security of LLMs. In this work, we present a new\ntraining time attack, SOS, which is designed to be low in computational demand\nand does not require clean data or modification of the model weights, thereby\nmaintaining the model's utility intact. The attack addresses security issues in\nvarious scenarios, including the backdoor attack, jailbreak attack, and prompt\nstealing attack. Our experimental findings demonstrate that the proposed attack\nis effective across all evaluated targets. Furthermore, we present the other\nside of our SOS technique, namely the copyright token -- a novel technique that\nenables users to mark their copyrighted content and prevent models from using\nit."
        ],
        "neg": []
    },
    {
        "query": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.",
        "pos": [
            "Using questions in written text is an effective strategy to enhance\nreadability. However, what makes an active reading question good, what the\nlinguistic role of these questions is, and what is their impact on human\nreading remains understudied. We introduce GuidingQ, a dataset of 10K in-text\nquestions from textbooks and scientific articles. By analyzing the dataset, we\npresent a comprehensive understanding of the use, distribution, and linguistic\ncharacteristics of these questions. Then, we explore various approaches to\ngenerate such questions using language models. Our results highlight the\nimportance of capturing inter-question relationships and the challenge of\nquestion position identification in generating these questions. Finally, we\nconduct a human study to understand the implication of such questions on\nreading comprehension. We find that the generated questions are of high quality\nand are almost as effective as human-written questions in terms of improving\nreaders' memorization and comprehension.",
            "Large language models (LLMs) present an opportunity to scale high-quality\npersonalized education to all. A promising approach towards this means is to\nbuild dialog tutoring models that scaffold students' problem-solving. However,\neven though existing LLMs perform well in solving reasoning questions, they\nstruggle to precisely detect student's errors and tailor their feedback to\nthese errors. Inspired by real-world teaching practice where teachers identify\nstudent errors and customize their response based on them, we focus on\nverifying student solutions and show how grounding to such verification\nimproves the overall quality of tutor response generation. We collect a dataset\nof 1K stepwise math reasoning chains with the first error step annotated by\nteachers. We show empirically that finding the mistake in a student solution is\nchallenging for current models. We propose and evaluate several verifiers for\ndetecting these errors. Using both automatic and human evaluation we show that\nthe student solution verifiers steer the generation model towards highly\ntargeted responses to student errors which are more often correct with less\nhallucinations compared to existing baselines."
        ],
        "neg": []
    },
    {
        "query": "Despite the usefulness of machine learning approaches for the early screening\nof potential breakthrough technologies, their practicality is often hindered by\nopaque models. To address this, we propose an interpretable machine learning\napproach to predicting future citation counts from patent texts using a\npatent-specific hierarchical attention network (PatentHAN) model. Central to\nthis approach are (1) a patent-specific pre-trained language model, capturing\nthe meanings of technical words in patent claims, (2) a hierarchical network\nstructure, enabling detailed analysis at the claim level, and (3) a claim-wise\nself-attention mechanism, revealing pivotal claims during the screening\nprocess. A case study of 35,376 pharmaceutical patents demonstrates the\neffectiveness of our approach in early screening of potential breakthrough\ntechnologies while ensuring interpretability. Furthermore, we conduct\nadditional analyses using different language models and claim types to examine\nthe robustness of the approach. It is expected that the proposed approach will\nenhance expert-machine collaboration in identifying breakthrough technologies,\nproviding new insight derived from text mining into technological value.",
        "pos": [
            "Recent studies have increasingly applied natural language processing (NLP) to\nautomatically extract experimental research data from the extensive battery\nmaterials literature. Despite the complex process involved in battery\nmanufacturing -- from material synthesis to cell assembly -- there has been no\ncomprehensive study systematically organizing this information. In response, we\npropose a language modeling-based protocol, Text-to-Battery Recipe (T2BR), for\nthe automatic extraction of end-to-end battery recipes, validated using a case\nstudy on batteries containing LiFePO4 cathode material. We report machine\nlearning-based paper filtering models, screening 2,174 relevant papers from the\nkeyword-based search results, and unsupervised topic models to identify 2,876\nparagraphs related to cathode synthesis and 2,958 paragraphs related to cell\nassembly. Then, focusing on the two topics, two deep learning-based named\nentity recognition models are developed to extract a total of 30 entities --\nincluding precursors, active materials, and synthesis methods -- achieving F1\nscores of 88.18% and 94.61%. The accurate extraction of entities enables the\nsystematic generation of 165 end-toend recipes of LiFePO4 batteries. Our\nprotocol and results offer valuable insights into specific trends, such as\nassociations between precursor materials and synthesis methods, or combinations\nbetween different precursor materials. We anticipate that our findings will\nserve as a foundational knowledge base for facilitating battery-recipe\ninformation retrieval. The proposed protocol will significantly accelerate the\nreview of battery material literature and catalyze innovations in battery\ndesign and development."
        ],
        "neg": []
    },
    {
        "query": "Question Answering (QA) effectively evaluates language models' reasoning and\nknowledge depth. While QA datasets are plentiful in areas like general domain\nand biomedicine, academic chemistry is less explored. Chemical QA plays a\ncrucial role in both education and research by effectively translating complex\nchemical information into readily understandable format. Addressing this gap,\nwe introduce ScholarChemQA, a large-scale QA dataset constructed from chemical\npapers. This dataset reflects typical real-world challenges, including an\nimbalanced data distribution and a substantial amount of unlabeled data that\ncan be potentially useful. Correspondingly, we introduce a QAMatch model,\nspecifically designed to effectively answer chemical questions by fully\nleveraging our collected data. We first address the issue of imbalanced label\ndistribution by re-weighting the instance-wise loss based on the inverse\nfrequency of each class, ensuring minority classes are not dominated by\nmajority ones during optimization. Next, we utilize the unlabeled data to\nenrich the learning process, generating a variety of augmentations based on a\nSoftMix operation and ensuring their predictions align with the same target,\ni.e., pseudo-labels. To ensure the quality of the pseudo-labels, we propose a\ncalibration procedure aimed at closely aligning the pseudo-label estimates of\nindividual samples with a desired ground truth distribution. Experiments show\nthat our QAMatch significantly outperforms the recent similar-scale baselines\nand Large Language Models (LLMs) not only on our ScholarChemQA dataset but also\non four benchmark datasets. We hope our benchmark and model can facilitate and\npromote more research on chemical QA.",
        "pos": [
            "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenDevin, a platform for the development of powerful and\nflexible AI agents that interact with the world in similar ways to those of a\nhuman developer: by writing code, interacting with a command line, and browsing\nthe web. We describe how the platform allows for the implementation of new\nagents, safe interaction with sandboxed environments for code execution,\ncoordination between multiple agents, and incorporation of evaluation\nbenchmarks. Based on our currently incorporated benchmarks, we perform an\nevaluation of agents over 15 challenging tasks, including software engineering\n(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released\nunder the permissive MIT license, OpenDevin is a community project spanning\nacademia and industry with more than 1.3K contributions from over 160\ncontributors and will improve going forward."
        ],
        "neg": []
    },
    {
        "query": "Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.",
        "pos": [
            "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. They are trained using preference datasets where each\nexample consists of one input prompt, two responses, and a preference label. As\ncurating a high-quality human labeled preference dataset is both time-consuming\nand expensive, people often rely on existing powerful LLMs for preference label\ngeneration. This can potentially introduce noise and impede RM training. In\nthis work, we present RMBoost, a novel synthetic preference data generation\nparadigm to boost reward model quality. Unlike traditional methods, which\ngenerate two responses before obtaining the preference label, RMBoost first\ngenerates one response and selects a preference label, followed by generating\nthe second more (or less) preferred response conditioned on the pre-selected\npreference label and the first response. This approach offers two main\nadvantages. First, RMBoost reduces labeling noise since preference pairs are\nconstructed intentionally. Second, RMBoost facilitates the creation of more\ndiverse responses by incorporating various quality aspects (e.g., helpfulness,\nrelevance, completeness) into the prompts. We conduct extensive experiments\nacross three diverse datasets and demonstrate that RMBoost outperforms other\nsynthetic preference data generation techniques and significantly boosts the\nperformance of four distinct reward models.",
            "The popularity of automated news headline generation has surged with\nadvancements in pre-trained language models. However, these models often suffer\nfrom the ``hallucination'' problem, where the generated headline is not fully\nsupported by its source article. Efforts to address this issue have\npredominantly focused on English, using over-simplistic classification schemes\nthat overlook nuanced hallucination types. In this study, we introduce the\nfirst multilingual, fine-grained news headline hallucination detection dataset\nthat contains over 11 thousand pairs in 5 languages, each annotated with\ndetailed hallucination types by experts. We conduct extensive experiments on\nthis dataset under two settings. First, we implement several supervised\nfine-tuning approaches as preparatory solutions and demonstrate this dataset's\nchallenges and utilities. Second, we test various large language models'\nin-context learning abilities and propose two novel techniques,\nlanguage-dependent demonstration selection and coarse-to-fine prompting, to\nboost the few-shot hallucination detection performance in terms of the\nexample-F1 metric. We release this dataset to foster further research in\nmultilingual, fine-grained headline hallucination detection.",
            "Knowledge-intensive visual question answering requires models to effectively\nuse external knowledge to help answer visual questions. A typical pipeline\nincludes a knowledge retriever and an answer generator. However, a retriever\nthat utilizes local information, such as an image patch, may not provide\nreliable question-candidate relevance scores. Besides, the two-tower\narchitecture also limits the relevance score modeling of a retriever to select\ntop candidates for answer generator reasoning. In this paper, we introduce an\nadditional module, a multi-modal reranker, to improve the ranking quality of\nknowledge candidates for answer generation. Our reranking module takes\nmulti-modal information from both candidates and questions and performs\ncross-item interaction for better relevance score modeling. Experiments on\nOK-VQA and A-OKVQA show that multi-modal reranker from distant supervision\nprovides consistent improvements. We also find a training-testing discrepancy\nwith reranking in answer generation, where performance improves if training\nknowledge candidates are similar to or noisier than those used in testing."
        ],
        "neg": []
    },
    {
        "query": "Fine-grained understanding of objects, attributes, and relationships between\nobjects is crucial for visual-language models (VLMs). Existing benchmarks\nprimarily focus on evaluating VLMs' capability to distinguish between two very\nsimilar \\textit{captions} given an image. In this paper, we introduce a new,\nchallenging benchmark termed \\textbf{Vis}ual \\textbf{Min}imal-Change\nUnderstanding (VisMin), which requires models to predict the correct\nimage-caption match given two images and two captions. The image pair and\ncaption pair contain minimal changes, i.e., only one aspect changes at a time\nfrom among the following: \\textit{object}, \\textit{attribute}, \\textit{count},\nand \\textit{spatial relation}. These changes test the models' understanding of\nobjects, attributes (such as color, material, shape), counts, and spatial\nrelationships between objects. We built an automatic framework using large\nlanguage models and diffusion models, followed by a rigorous 4-step\nverification process by human annotators. Empirical experiments reveal that\ncurrent VLMs exhibit notable deficiencies in understanding spatial\nrelationships and counting abilities. We also generate a large-scale training\ndataset to finetune CLIP and Idefics2, showing significant improvements in\nfine-grained understanding across benchmarks and in CLIP's general image-text\nalignment. We release all resources, including the benchmark, training data,\nand finetuned model checkpoints, at \\url{https://vismin.net/}.",
        "pos": [
            "Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures."
        ],
        "neg": []
    },
    {
        "query": "Fine-grained understanding of objects, attributes, and relationships between\nobjects is crucial for visual-language models (VLMs). Existing benchmarks\nprimarily focus on evaluating VLMs' capability to distinguish between two very\nsimilar \\textit{captions} given an image. In this paper, we introduce a new,\nchallenging benchmark termed \\textbf{Vis}ual \\textbf{Min}imal-Change\nUnderstanding (VisMin), which requires models to predict the correct\nimage-caption match given two images and two captions. The image pair and\ncaption pair contain minimal changes, i.e., only one aspect changes at a time\nfrom among the following: \\textit{object}, \\textit{attribute}, \\textit{count},\nand \\textit{spatial relation}. These changes test the models' understanding of\nobjects, attributes (such as color, material, shape), counts, and spatial\nrelationships between objects. We built an automatic framework using large\nlanguage models and diffusion models, followed by a rigorous 4-step\nverification process by human annotators. Empirical experiments reveal that\ncurrent VLMs exhibit notable deficiencies in understanding spatial\nrelationships and counting abilities. We also generate a large-scale training\ndataset to finetune CLIP and Idefics2, showing significant improvements in\nfine-grained understanding across benchmarks and in CLIP's general image-text\nalignment. We release all resources, including the benchmark, training data,\nand finetuned model checkpoints, at \\url{https://vismin.net/}.",
        "pos": [
            "Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.",
            "Despite tremendous advancements, current state-of-the-art Vision-Language\nModels (VLMs) are still far from perfect. They tend to hallucinate and may\ngenerate biased responses. In such circumstances, having a way to assess the\nreliability of a given response generated by a VLM is quite useful. Existing\nmethods, such as estimating uncertainty using answer likelihoods or\nprompt-based confidence generation, often suffer from overconfidence. Other\nmethods use self-consistency comparison but are affected by confirmation\nbiases. To alleviate these, we propose \\textbf{De}compose and \\textbf{C}ompare\n\\textbf{C}onsistency (\\texttt{DeCC}) for reliability measurement. By comparing\nthe consistency between the direct answer generated using the VLM's internal\nreasoning process, and the indirect answers obtained by decomposing the\nquestion into sub-questions and reasoning over the sub-answers produced by the\nVLM, \\texttt{DeCC} measures the reliability of VLM's direct answer. Experiments\nacross six vision-language tasks with three VLMs show \\texttt{DeCC}'s\nreliability estimation achieves better correlation with task accuracy compared\nto the existing methods."
        ],
        "neg": []
    },
    {
        "query": "We introduce Lifelong ICL, a problem setting that challenges long-context\nlanguage models (LMs) to learn from a sequence of language tasks through\nin-context learning (ICL). We further introduce Task Haystack, an evaluation\nsuite dedicated to assessing and diagnosing how long-context LMs utilizes\ncontexts in Lifelong ICL. When given a task instruction and test inputs,\nlong-context LMs are expected to leverage the relevant demonstrations in the\nLifelong ICL prompt, avoid distraction and interference from other tasks, and\nachieve test accuracies that are not significantly worse than the Single-task\nICL baseline.\n  Task Haystack draws inspiration from the widely-adopted\n\"needle-in-a-haystack\" (NIAH) evaluation, but presents new and unique\nchallenges. It demands that models (1) utilize the contexts with deeper\nunderstanding, rather than resorting to simple copying and pasting; (2)\nnavigate through long streams of evolving topics and tasks, which closely\napproximates the complexities of real-world usage of long-context LMs.\nAdditionally, Task Haystack inherits the controllability aspect of NIAH,\nproviding model developers with tools and visualizations to identify model\nvulnerabilities effectively.\n  We benchmark 12 long-context LMs using Task Haystack. We find that\nstate-of-the-art closed models such as GPT-4o still struggle in this setting,\nfailing 15% of the cases on average, while all open-weight models we evaluate\nfurther lack behind by a large margin, failing up to 61% of the cases. In our\ncontrolled analysis, we identify factors such as distraction and recency bias\nas contributors to these failure cases. Further, we observe declines in\nperformance when task instructions are paraphrased at test time or when ICL\ndemonstrations are repeated excessively, raising concerns about the robustness,\ninstruction understanding, and true context utilization of current long-context\nLMs.",
        "pos": [
            "The reconfiguration of human-LM interactions from simple sentence completions\nto complex, multi-domain, humanlike engagements necessitates new methodologies\nto understand how humans choose to rely on LMs. In our work, we contend that\nreliance is influenced by numerous factors within the interactional context of\na generation, a departure from prior work that used verbalized confidence\n(e.g., \"I'm certain the answer is...\") as the key determinant of reliance.\nHere, we introduce Rel-A.I., an in situ, system-level evaluation approach to\nmeasure human reliance on LM-generated epistemic markers (e.g., \"I think\nit's..\", \"Undoubtedly it's...\"). Using this methodology, we measure reliance\nrates in three emergent human-LM interaction settings: long-term interactions,\nanthropomorphic generations, and variable subject matter. Our findings reveal\nthat reliance is not solely based on verbalized confidence but is significantly\naffected by other features of the interaction context. Prior interactions,\nanthropomorphic cues, and subject domain all contribute to reliance\nvariability. An expression such as, \"I'm pretty sure it's...\", can vary up to\n20% in reliance frequency depending on its interactional context. Our work\nunderscores the importance of context in understanding human reliance and\noffers future designers and researchers with a methodology to conduct such\nmeasurements."
        ],
        "neg": []
    },
    {
        "query": "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenDevin, a platform for the development of powerful and\nflexible AI agents that interact with the world in similar ways to those of a\nhuman developer: by writing code, interacting with a command line, and browsing\nthe web. We describe how the platform allows for the implementation of new\nagents, safe interaction with sandboxed environments for code execution,\ncoordination between multiple agents, and incorporation of evaluation\nbenchmarks. Based on our currently incorporated benchmarks, we perform an\nevaluation of agents over 15 challenging tasks, including software engineering\n(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released\nunder the permissive MIT license, OpenDevin is a community project spanning\nacademia and industry with more than 1.3K contributions from over 160\ncontributors and will improve going forward.",
        "pos": [
            "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research.",
            "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe same result that the optimal vocabulary size depends on the available\ncompute budget and that larger models deserve larger vocabularies. However,\nmost LLMs use too small vocabulary sizes. For example, we predict that the\noptimal vocabulary size of Llama2-70B should have been at least 216K, 7 times\nlarger than its vocabulary of 32K. We validate our predictions empirically by\ntraining models with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly\nconsidering model parameters and vocabulary size for efficient scaling.",
            "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io."
        ],
        "neg": []
    },
    {
        "query": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been\nintegrated into many real-world applications like Code Copilot. These\napplications have significantly expanded the attack surface of LLMs, exposing\nthem to a variety of threats. Among them, jailbreak attacks that induce toxic\nresponses through jailbreak prompts have raised critical safety concerns. To\nidentify these threats, a growing number of red teaming approaches simulate\npotential adversarial scenarios by crafting jailbreak prompts to test the\ntarget LLM. However, existing red teaming methods do not consider the unique\nvulnerabilities of LLM in different scenarios, making it difficult to adjust\nthe jailbreak prompts to find context-specific vulnerabilities. Meanwhile,\nthese methods are limited to refining jailbreak templates using a few mutation\noperations, lacking the automation and scalability to adapt to different\nscenarios. To enable context-aware and efficient red teaming, we abstract and\nmodel existing attacks into a coherent concept called \"jailbreak strategy\" and\npropose a multi-agent LLM system named RedAgent that leverages these strategies\nto generate context-aware jailbreak prompts. By self-reflecting on contextual\nfeedback in an additional memory buffer, RedAgent continuously learns how to\nleverage these strategies to achieve effective jailbreaks in specific contexts.\nExtensive experiments demonstrate that our system can jailbreak most black-box\nLLMs in just five queries, improving the efficiency of existing red teaming\nmethods by two times. Additionally, RedAgent can jailbreak customized LLM\napplications more efficiently. By generating context-aware jailbreak prompts\ntowards applications on GPTs, we discover 60 severe vulnerabilities of these\nreal-world applications with only two queries per vulnerability. We have\nreported all found issues and communicated with OpenAI and Meta for bug fixes.",
        "pos": [
            "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems."
        ],
        "neg": []
    },
    {
        "query": "Text style transfer (TST) is an important task in controllable text\ngeneration, which aims to control selected attributes of language use, such as\npoliteness, formality, or sentiment, without altering the style-independent\ncontent of the text. The field has received considerable research attention in\nrecent years and has already been covered in several reviews, but the focus has\nmostly been on the development of new algorithms and learning from different\ntypes of data (supervised, unsupervised, out-of-domain, etc.) and not so much\non the application side. However, TST-related technologies are gradually\nreaching a production- and deployment-ready level, and therefore, the inclusion\nof the application perspective in TST research becomes crucial. Similarly, the\noften overlooked ethical considerations of TST technology have become a\npressing issue. This paper presents a comprehensive review of TST applications\nthat have been researched over the years, using both traditional linguistic\napproaches and more recent deep learning methods. We discuss current\nchallenges, future research directions, and ethical implications of TST\napplications in text generation. By providing a holistic overview of the\nlandscape of TST applications, we hope to stimulate further research and\ncontribute to a better understanding of the potential as well as ethical\nconsiderations associated with TST.",
        "pos": [
            "Text Style Transfer (TST) is a pivotal task in natural language generation to\nmanipulate text style attributes while preserving style-independent content.\nThe attributes targeted in TST can vary widely, including politeness,\nauthorship, mitigation of offensive language, modification of feelings, and\nadjustment of text formality. TST has become a widely researched topic with\nsubstantial advancements in recent years. This paper provides an introductory\noverview of TST, addressing its challenges, existing approaches, datasets,\nevaluation measures, subtasks, and applications. This fundamental overview\nimproves understanding of the background and fundamentals of text style\ntransfer."
        ],
        "neg": []
    },
    {
        "query": "Text style transfer (TST) is an important task in controllable text\ngeneration, which aims to control selected attributes of language use, such as\npoliteness, formality, or sentiment, without altering the style-independent\ncontent of the text. The field has received considerable research attention in\nrecent years and has already been covered in several reviews, but the focus has\nmostly been on the development of new algorithms and learning from different\ntypes of data (supervised, unsupervised, out-of-domain, etc.) and not so much\non the application side. However, TST-related technologies are gradually\nreaching a production- and deployment-ready level, and therefore, the inclusion\nof the application perspective in TST research becomes crucial. Similarly, the\noften overlooked ethical considerations of TST technology have become a\npressing issue. This paper presents a comprehensive review of TST applications\nthat have been researched over the years, using both traditional linguistic\napproaches and more recent deep learning methods. We discuss current\nchallenges, future research directions, and ethical implications of TST\napplications in text generation. By providing a holistic overview of the\nlandscape of TST applications, we hope to stimulate further research and\ncontribute to a better understanding of the potential as well as ethical\nconsiderations associated with TST.",
        "pos": [
            "Text Style Transfer (TST) is a pivotal task in natural language generation to\nmanipulate text style attributes while preserving style-independent content.\nThe attributes targeted in TST can vary widely, including politeness,\nauthorship, mitigation of offensive language, modification of feelings, and\nadjustment of text formality. TST has become a widely researched topic with\nsubstantial advancements in recent years. This paper provides an introductory\noverview of TST, addressing its challenges, existing approaches, datasets,\nevaluation measures, subtasks, and applications. This fundamental overview\nimproves understanding of the background and fundamentals of text style\ntransfer."
        ],
        "neg": []
    },
    {
        "query": "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
        "pos": [
            "The mainstream automatic speech recognition (ASR) technology usually requires\nhundreds to thousands of hours of annotated speech data. Three approaches to\nlow-resourced ASR are phoneme or subword based supervised pre-training, and\nself-supervised pre-training over multilingual data. The Iu Mien language is\nthe main ethnic language of the Yao ethnic group in China and is low-resourced\nin the sense that the annotated speech is very limited. With less than 10 hours\nof transcribed Iu Mien language, this paper investigates and compares the three\napproaches for Iu Mien speech recognition. Our experiments are based on the\nrecently released, three backbone models pretrained over the 10 languages from\nthe CommonVoice dataset (CV-Lang10), which correspond to the three approaches\nfor low-resourced ASR. It is found that phoneme supervision can achieve better\nresults compared to subword supervision and self-supervision, thereby providing\nhigher data-efficiency. Particularly, the Whistle models, i.e., obtained by the\nweakly-supervised phoneme-based multilingual pre-training, obtain the most\ncompetitive results.",
            "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems."
        ],
        "neg": []
    },
    {
        "query": "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
        "pos": [
            "The common toxicity and societal bias in contents generated by large language\nmodels (LLMs) necessitate strategies to reduce harm. Present solutions often\ndemand white-box access to the model or substantial training, which is\nimpractical for cutting-edge commercial LLMs. Moreover, prevailing prompting\nmethods depend on external tool feedback and fail to simultaneously lessen\ntoxicity and bias. Motivated by social psychology principles, we propose a\nnovel strategy named \\textbf{perspective-taking prompting (\\textsc{PeT})} that\ninspires LLMs to integrate diverse human perspectives and self-regulate their\nresponses. This self-correction mechanism can significantly diminish toxicity\n(up to $89\\%$) and bias (up to $73\\%$) in LLMs' responses. Rigorous evaluations\nand ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and\nthree open-source LLMs, revealing \\textsc{PeT}'s superiority in producing less\nharmful responses, outperforming five strong baselines."
        ],
        "neg": []
    },
    {
        "query": "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
        "pos": [
            "The common toxicity and societal bias in contents generated by large language\nmodels (LLMs) necessitate strategies to reduce harm. Present solutions often\ndemand white-box access to the model or substantial training, which is\nimpractical for cutting-edge commercial LLMs. Moreover, prevailing prompting\nmethods depend on external tool feedback and fail to simultaneously lessen\ntoxicity and bias. Motivated by social psychology principles, we propose a\nnovel strategy named \\textbf{perspective-taking prompting (\\textsc{PeT})} that\ninspires LLMs to integrate diverse human perspectives and self-regulate their\nresponses. This self-correction mechanism can significantly diminish toxicity\n(up to $89\\%$) and bias (up to $73\\%$) in LLMs' responses. Rigorous evaluations\nand ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and\nthree open-source LLMs, revealing \\textsc{PeT}'s superiority in producing less\nharmful responses, outperforming five strong baselines."
        ],
        "neg": []
    },
    {
        "query": "Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal tasks remains limited. We conduct a comprehensive study of 260\nlegal text classification tasks, nearly all new to the machine learning\ncommunity. Starting from GPT-4 as a baseline, we show that it has non-trivial\nbut highly varied zero-shot accuracy, often exhibiting performance that may be\ninsufficient for legal work. We then demonstrate that a lightly fine-tuned\nLlama 3 model vastly outperforms GPT-4 on almost all tasks, typically by\ndouble-digit percentage points. We find that larger models respond better to\nfine-tuning than smaller models. A few tens to hundreds of examples suffice to\nachieve high classification accuracy. Notably, we can fine-tune a single model\non all 260 tasks simultaneously at a small loss in accuracy relative to having\na separate model for each task. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal tasks\nwith some available labeled data, researchers are better off using a fine-tuned\nopen-source model.",
        "pos": [
            "We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of techniques to\ninclude task-relevant data in the pretraining stage of a language model. We\ndemonstrate that training on the test task confounds both relative model\nevaluations and claims about emergent capabilities. We argue that the seeming\nsuperiority of one model family over another may be explained by a different\ndegree of training on the test task. To this end, we propose an effective\nmethod to adjust for training on the test task by fine-tuning each model under\ncomparison on the same task-relevant data before evaluation. We then show that\ninstances of emergent behavior largely vanish once we adjust for training on\nthe test task. This also applies to reported instances of emergent behavior\nthat cannot be explained by the choice of evaluation metric. Our work promotes\na new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities."
        ],
        "neg": []
    },
    {
        "query": "Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal tasks remains limited. We conduct a comprehensive study of 260\nlegal text classification tasks, nearly all new to the machine learning\ncommunity. Starting from GPT-4 as a baseline, we show that it has non-trivial\nbut highly varied zero-shot accuracy, often exhibiting performance that may be\ninsufficient for legal work. We then demonstrate that a lightly fine-tuned\nLlama 3 model vastly outperforms GPT-4 on almost all tasks, typically by\ndouble-digit percentage points. We find that larger models respond better to\nfine-tuning than smaller models. A few tens to hundreds of examples suffice to\nachieve high classification accuracy. Notably, we can fine-tune a single model\non all 260 tasks simultaneously at a small loss in accuracy relative to having\na separate model for each task. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal tasks\nwith some available labeled data, researchers are better off using a fine-tuned\nopen-source model.",
        "pos": [
            "We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of techniques to\ninclude task-relevant data in the pretraining stage of a language model. We\ndemonstrate that training on the test task confounds both relative model\nevaluations and claims about emergent capabilities. We argue that the seeming\nsuperiority of one model family over another may be explained by a different\ndegree of training on the test task. To this end, we propose an effective\nmethod to adjust for training on the test task by fine-tuning each model under\ncomparison on the same task-relevant data before evaluation. We then show that\ninstances of emergent behavior largely vanish once we adjust for training on\nthe test task. This also applies to reported instances of emergent behavior\nthat cannot be explained by the choice of evaluation metric. Our work promotes\na new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities.",
            "We study the predictability of online speech on social media, and whether\npredictability improves with information outside a user's own posts. Recent\nwork suggests that the predictive information contained in posts written by a\nuser's peers can surpass that of the user's own posts. Motivated by the success\nof large language models, we empirically test this hypothesis. We define\nunpredictability as a measure of the model's uncertainty, i.e., its negative\nlog-likelihood on future tokens given context. As the basis of our study, we\ncollect a corpus of 6.25M posts from more than five thousand X (previously\nTwitter) users and their peers. Across three large language models ranging in\nsize from 1 billion to 70 billion parameters, we find that predicting a user's\nposts from their peers' posts performs poorly. Moreover, the value of the\nuser's own posts for prediction is consistently higher than that of their\npeers'. Across the board, we find that the predictability of social media posts\nremains low, comparable to predicting financial news without context. We extend\nour investigation with a detailed analysis about the causes of unpredictability\nand the robustness of our findings. Specifically, we observe that a significant\namount of predictive uncertainty comes from hashtags and @-mentions. Moreover,\nour results replicate if instead of prompting the model with additional\ncontext, we finetune on additional context."
        ],
        "neg": []
    },
    {
        "query": "Despite the recent proliferation of large language models (LLMs), their\ntraining recipes -- model architecture, pre-training data and optimization\nalgorithm -- are often very similar. This naturally raises the question of the\nsimilarity among the resulting models. In this paper, we propose a novel\nsetting, imaginary question answering (IQA), to better understand model\nsimilarity. In IQA, we ask one model to generate purely imaginary questions\n(e.g., on completely made-up concepts in physics) and prompt another model to\nanswer. Surprisingly, despite the total fictionality of these questions, all\nmodels can answer each other's questions with remarkable success, suggesting a\n\"shared imagination space\" in which these models operate during such\nhallucinations. We conduct a series of investigations into this phenomenon and\ndiscuss implications on model homogeneity, hallucination, and computational\ncreativity.",
        "pos": [
            "Mechanistic interpretability (MI) is an emerging sub-field of\ninterpretability that seeks to understand a neural network model by\nreverse-engineering its internal computations. Recently, MI has garnered\nsignificant attention for interpreting transformer-based language models (LMs),\nresulting in many novel insights yet introducing new challenges. However, there\nhas not been work that comprehensively reviews these insights and challenges,\nparticularly as a guide for newcomers to this field. To fill this gap, we\npresent a comprehensive survey outlining fundamental objects of study in MI,\ntechniques that have been used for its investigation, approaches for evaluating\nMI results, and significant findings and applications stemming from the use of\nMI to understand LMs. In particular, we present a roadmap for beginners to\nnavigate the field and leverage MI for their benefit. Finally, we also identify\ncurrent gaps in the field and discuss potential future directions."
        ],
        "neg": []
    },
    {
        "query": "Recent surge in the accessibility of large language models (LLMs) to the\ngeneral population can lead to untrackable use of such models for\nmedical-related recommendations. Language generation via LLMs models has two\nkey problems: firstly, they are prone to hallucination and therefore, for any\nmedical purpose they require scientific and factual grounding; secondly, LLMs\npose tremendous challenge to computational resources due to their gigantic\nmodel size. In this work, we introduce pRAGe, a pipeline for Retrieval\nAugmented Generation and evaluation of medical paraphrases generation using\nSmall Language Models (SLM). We study the effectiveness of SLMs and the impact\nof external knowledge base for medical paraphrase generation in French.",
        "pos": [
            "The success of pretrained language models (PLMs) across a spate of use-cases\nhas led to significant investment from the NLP community towards building\ndomain-specific foundational models. On the other hand, in mission critical\nsettings such as biomedical applications, other aspects also factor in-chief of\nwhich is a model's ability to produce reasonable estimates of its own\nuncertainty. In the present study, we discuss these two desiderata through the\nlens of how they shape the entropy of a model's output probability\ndistribution. We find that domain specificity and uncertainty awareness can\noften be successfully combined, but the exact task at hand weighs in much more\nstrongly."
        ],
        "neg": []
    },
    {
        "query": "Recent surge in the accessibility of large language models (LLMs) to the\ngeneral population can lead to untrackable use of such models for\nmedical-related recommendations. Language generation via LLMs models has two\nkey problems: firstly, they are prone to hallucination and therefore, for any\nmedical purpose they require scientific and factual grounding; secondly, LLMs\npose tremendous challenge to computational resources due to their gigantic\nmodel size. In this work, we introduce pRAGe, a pipeline for Retrieval\nAugmented Generation and evaluation of medical paraphrases generation using\nSmall Language Models (SLM). We study the effectiveness of SLMs and the impact\nof external knowledge base for medical paraphrase generation in French.",
        "pos": [
            "The success of pretrained language models (PLMs) across a spate of use-cases\nhas led to significant investment from the NLP community towards building\ndomain-specific foundational models. On the other hand, in mission critical\nsettings such as biomedical applications, other aspects also factor in-chief of\nwhich is a model's ability to produce reasonable estimates of its own\nuncertainty. In the present study, we discuss these two desiderata through the\nlens of how they shape the entropy of a model's output probability\ndistribution. We find that domain specificity and uncertainty awareness can\noften be successfully combined, but the exact task at hand weighs in much more\nstrongly."
        ],
        "neg": []
    },
    {
        "query": "A long-standing question in automatic speech recognition research is how to\nattribute errors to the ability of a model to model the acoustics, versus its\nability to leverage higher-order context (lexicon, morphology, syntax,\nsemantics). We validate a novel approach which models error rates as a function\nof relative textual predictability, and yields a single number, $k$, which\nmeasures the effect of textual predictability on the recognizer. We use this\nmethod to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use\nof textual context than a hybrid ASR model, in spite of not using an explicit\nlanguage model, and also use it to shed light on recent results demonstrating\npoor performance of standard ASR systems on African-American English. We\ndemonstrate that these mostly represent failures of acoustic--phonetic\nmodelling. We show how this approach can be used straightforwardly in\ndiagnosing and improving ASR.",
        "pos": [
            "In this paper, we introduce a comprehensive reformulation of the task known\nas Circuit Discovery, along with DiscoGP, a novel and effective algorithm based\non differentiable masking for discovering circuits. Circuit discovery is the\ntask of interpreting the computational mechanisms of language models (LMs) by\ndissecting their functions and capabilities into sparse subnetworks (circuits).\nWe identified two major limitations in existing circuit discovery efforts: (1)\na dichotomy between weight-based and connection-edge-based approaches forces\nresearchers to choose between pruning connections or weights, thereby limiting\nthe scope of mechanistic interpretation of LMs; (2) algorithms based on\nactivation patching tend to identify circuits that are neither functionally\nfaithful nor complete. The performance of these identified circuits is\nsubstantially reduced, often resulting in near-random performance in isolation.\nFurthermore, the complement of the circuit -- i.e., the original LM with the\nidentified circuit removed -- still retains adequate performance, indicating\nthat essential components of a complete circuits are missed by existing\nmethods.\n  DiscoGP successfully addresses the two aforementioned issues and demonstrates\nstate-of-the-art faithfulness, completeness, and sparsity. The effectiveness of\nthe algorithm and its novel structure open up new avenues of gathering new\ninsights into the internal workings of generative AI."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in massively multilingual machine translation systems\nhave significantly enhanced translation accuracy; however, even the best\nperforming systems still generate hallucinations, severely impacting user\ntrust. Detecting hallucinations in Machine Translation (MT) remains a critical\nchallenge, particularly since existing methods excel with High-Resource\nLanguages (HRLs) but exhibit substantial limitations when applied to\nLow-Resource Languages (LRLs). This paper evaluates hallucination detection\napproaches using Large Language Models (LLMs) and semantic similarity within\nmassively multilingual embeddings. Our study spans 16 language directions,\ncovering HRLs, LRLs, with diverse scripts. We find that the choice of model is\nessential for performance. On average, for HRLs, Llama3-70B outperforms the\nprevious state of the art by as much as 0.16 MCC (Matthews Correlation\nCoefficient). However, for LRLs we observe that Claude Sonnet outperforms other\nLLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can\nachieve performance comparable or even better than previously proposed models,\ndespite not being explicitly trained for any machine translation task. However,\ntheir advantage is less significant for LRLs.",
        "pos": [
            "Low-resource languages often face challenges in acquiring high-quality\nlanguage data due to the reliance on translation-based methods, which can\nintroduce the translationese effect. This phenomenon results in translated\nsentences that lack fluency and naturalness in the target language. In this\npaper, we propose a novel approach for data collection by leveraging\nstoryboards to elicit more fluent and natural sentences. Our method involves\npresenting native speakers with visual stimuli in the form of storyboards and\ncollecting their descriptions without direct exposure to the source text. We\nconducted a comprehensive evaluation comparing our storyboard-based approach\nwith traditional text translation-based methods in terms of accuracy and\nfluency. Human annotators and quantitative metrics were used to assess\ntranslation quality. The results indicate a preference for text translation in\nterms of accuracy, while our method demonstrates worse accuracy but better\nfluency in the language focused."
        ],
        "neg": []
    },
    {
        "query": "This paper presents a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly minimizes the training corpus requirement to a\nmere 0.3% while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes for human\nstudents, particularly how structured domain knowledge from textbooks is\nabsorbed and then applied to tackle real-world challenges through specific\nexercises. Based on this, we propose a novel two-stage knowledge injection\nstrategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware\nSupervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data\ninto an auto-generated taxonomy of domain knowledge, enabling LLMs to\neffectively memorize textual segments linked to specific expertise within the\ntaxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt\nmodels to reveal the underlying knowledge structure in their outputs,\nleveraging this structured domain insight to address practical problems\nadeptly. Our ultimate method has undergone extensive evaluations across model\narchitectures and scales, using closed-book question-answering tasks on\nLongBench and MMedBench datasets. Remarkably, our method matches 50% of the\nimprovement displayed by the state-of-the-art MMedLM2 on MMedBench, but with\nonly 0.3% quantity of the training corpus. This breakthrough showcases the\npotential to scale up our StructTuning for stronger domain-specific LLMs. Code\nwill be made public soon.",
        "pos": [
            "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon.",
            "Large language models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, the conventional\nfixed-length data composition strategy for pretraining, which involves\nconcatenating and splitting documents, can introduce noise and limit the\nmodel's ability to capture long-range dependencies. To address this, we first\nintroduce three metrics for evaluating data composition quality: padding ratio,\ntruncation ratio, and concatenation ratio. We further propose a multi-bucket\ndata composition method that moves beyond the fixed-length paradigm, offering a\nmore flexible and efficient approach to pretraining. Extensive experiments\ndemonstrate that our proposed method could significantly improving both the\nefficiency and efficacy of LLMs pretraining. Our approach not only reduces\nnoise and preserves context but also accelerates training, making it a\npromising solution for LLMs pretraining."
        ],
        "neg": []
    },
    {
        "query": "This paper presents a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly minimizes the training corpus requirement to a\nmere 0.3% while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes for human\nstudents, particularly how structured domain knowledge from textbooks is\nabsorbed and then applied to tackle real-world challenges through specific\nexercises. Based on this, we propose a novel two-stage knowledge injection\nstrategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware\nSupervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data\ninto an auto-generated taxonomy of domain knowledge, enabling LLMs to\neffectively memorize textual segments linked to specific expertise within the\ntaxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt\nmodels to reveal the underlying knowledge structure in their outputs,\nleveraging this structured domain insight to address practical problems\nadeptly. Our ultimate method has undergone extensive evaluations across model\narchitectures and scales, using closed-book question-answering tasks on\nLongBench and MMedBench datasets. Remarkably, our method matches 50% of the\nimprovement displayed by the state-of-the-art MMedLM2 on MMedBench, but with\nonly 0.3% quantity of the training corpus. This breakthrough showcases the\npotential to scale up our StructTuning for stronger domain-specific LLMs. Code\nwill be made public soon.",
        "pos": [
            "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon."
        ],
        "neg": []
    },
    {
        "query": "This paper presents a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly minimizes the training corpus requirement to a\nmere 0.3% while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes for human\nstudents, particularly how structured domain knowledge from textbooks is\nabsorbed and then applied to tackle real-world challenges through specific\nexercises. Based on this, we propose a novel two-stage knowledge injection\nstrategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware\nSupervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data\ninto an auto-generated taxonomy of domain knowledge, enabling LLMs to\neffectively memorize textual segments linked to specific expertise within the\ntaxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt\nmodels to reveal the underlying knowledge structure in their outputs,\nleveraging this structured domain insight to address practical problems\nadeptly. Our ultimate method has undergone extensive evaluations across model\narchitectures and scales, using closed-book question-answering tasks on\nLongBench and MMedBench datasets. Remarkably, our method matches 50% of the\nimprovement displayed by the state-of-the-art MMedLM2 on MMedBench, but with\nonly 0.3% quantity of the training corpus. This breakthrough showcases the\npotential to scale up our StructTuning for stronger domain-specific LLMs. Code\nwill be made public soon.",
        "pos": [
            "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon."
        ],
        "neg": []
    },
    {
        "query": "This paper presents a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly minimizes the training corpus requirement to a\nmere 0.3% while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes for human\nstudents, particularly how structured domain knowledge from textbooks is\nabsorbed and then applied to tackle real-world challenges through specific\nexercises. Based on this, we propose a novel two-stage knowledge injection\nstrategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware\nSupervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data\ninto an auto-generated taxonomy of domain knowledge, enabling LLMs to\neffectively memorize textual segments linked to specific expertise within the\ntaxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt\nmodels to reveal the underlying knowledge structure in their outputs,\nleveraging this structured domain insight to address practical problems\nadeptly. Our ultimate method has undergone extensive evaluations across model\narchitectures and scales, using closed-book question-answering tasks on\nLongBench and MMedBench datasets. Remarkably, our method matches 50% of the\nimprovement displayed by the state-of-the-art MMedLM2 on MMedBench, but with\nonly 0.3% quantity of the training corpus. This breakthrough showcases the\npotential to scale up our StructTuning for stronger domain-specific LLMs. Code\nwill be made public soon.",
        "pos": [
            "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon."
        ],
        "neg": []
    },
    {
        "query": "This paper presents a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly minimizes the training corpus requirement to a\nmere 0.3% while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes for human\nstudents, particularly how structured domain knowledge from textbooks is\nabsorbed and then applied to tackle real-world challenges through specific\nexercises. Based on this, we propose a novel two-stage knowledge injection\nstrategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware\nSupervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data\ninto an auto-generated taxonomy of domain knowledge, enabling LLMs to\neffectively memorize textual segments linked to specific expertise within the\ntaxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt\nmodels to reveal the underlying knowledge structure in their outputs,\nleveraging this structured domain insight to address practical problems\nadeptly. Our ultimate method has undergone extensive evaluations across model\narchitectures and scales, using closed-book question-answering tasks on\nLongBench and MMedBench datasets. Remarkably, our method matches 50% of the\nimprovement displayed by the state-of-the-art MMedLM2 on MMedBench, but with\nonly 0.3% quantity of the training corpus. This breakthrough showcases the\npotential to scale up our StructTuning for stronger domain-specific LLMs. Code\nwill be made public soon.",
        "pos": [
            "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon.",
            "Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that\nare only partially available. This poses two main challenges: effectively\nleveraging multi-view information and mitigating the impact of missing views.\nPrevailing solutions employ cross-view contrastive learning and missing view\nrecovery techniques. However, they either neglect valuable complementary\ninformation by focusing only on consensus between views or provide unreliable\nrecovered views due to the absence of supervision. To address these\nlimitations, we propose a novel Unified and Robust Representation Learning for\nIncomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a\nunified embedding that is robust to view missing conditions by integrating\ninformation from multiple views and neighboring samples. Firstly, to overcome\nthe limitations of cross-view contrastive learning, URRL-IMVC incorporates an\nattention-based auto-encoder framework to fuse multi-view information and\ngenerate unified embeddings. Secondly, URRL-IMVC directly enhances the\nrobustness of the unified embedding against view-missing conditions through KNN\nimputation and data augmentation techniques, eliminating the need for explicit\nmissing view recovery. Finally, incremental improvements are introduced to\nfurther enhance the overall performance, such as the Clustering Module and the\ncustomization of the Encoder. We extensively evaluate the proposed URRL-IMVC\nframework on various benchmark datasets, demonstrating its state-of-the-art\nperformance. Furthermore, comprehensive ablation studies are performed to\nvalidate the effectiveness of our design."
        ],
        "neg": []
    },
    {
        "query": "This paper presents a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly minimizes the training corpus requirement to a\nmere 0.3% while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes for human\nstudents, particularly how structured domain knowledge from textbooks is\nabsorbed and then applied to tackle real-world challenges through specific\nexercises. Based on this, we propose a novel two-stage knowledge injection\nstrategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware\nSupervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data\ninto an auto-generated taxonomy of domain knowledge, enabling LLMs to\neffectively memorize textual segments linked to specific expertise within the\ntaxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt\nmodels to reveal the underlying knowledge structure in their outputs,\nleveraging this structured domain insight to address practical problems\nadeptly. Our ultimate method has undergone extensive evaluations across model\narchitectures and scales, using closed-book question-answering tasks on\nLongBench and MMedBench datasets. Remarkably, our method matches 50% of the\nimprovement displayed by the state-of-the-art MMedLM2 on MMedBench, but with\nonly 0.3% quantity of the training corpus. This breakthrough showcases the\npotential to scale up our StructTuning for stronger domain-specific LLMs. Code\nwill be made public soon.",
        "pos": [
            "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon."
        ],
        "neg": []
    },
    {
        "query": "This paper presents a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly minimizes the training corpus requirement to a\nmere 0.3% while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes for human\nstudents, particularly how structured domain knowledge from textbooks is\nabsorbed and then applied to tackle real-world challenges through specific\nexercises. Based on this, we propose a novel two-stage knowledge injection\nstrategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware\nSupervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data\ninto an auto-generated taxonomy of domain knowledge, enabling LLMs to\neffectively memorize textual segments linked to specific expertise within the\ntaxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt\nmodels to reveal the underlying knowledge structure in their outputs,\nleveraging this structured domain insight to address practical problems\nadeptly. Our ultimate method has undergone extensive evaluations across model\narchitectures and scales, using closed-book question-answering tasks on\nLongBench and MMedBench datasets. Remarkably, our method matches 50% of the\nimprovement displayed by the state-of-the-art MMedLM2 on MMedBench, but with\nonly 0.3% quantity of the training corpus. This breakthrough showcases the\npotential to scale up our StructTuning for stronger domain-specific LLMs. Code\nwill be made public soon.",
        "pos": [
            "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon.",
            "Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that\nare only partially available. This poses two main challenges: effectively\nleveraging multi-view information and mitigating the impact of missing views.\nPrevailing solutions employ cross-view contrastive learning and missing view\nrecovery techniques. However, they either neglect valuable complementary\ninformation by focusing only on consensus between views or provide unreliable\nrecovered views due to the absence of supervision. To address these\nlimitations, we propose a novel Unified and Robust Representation Learning for\nIncomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a\nunified embedding that is robust to view missing conditions by integrating\ninformation from multiple views and neighboring samples. Firstly, to overcome\nthe limitations of cross-view contrastive learning, URRL-IMVC incorporates an\nattention-based auto-encoder framework to fuse multi-view information and\ngenerate unified embeddings. Secondly, URRL-IMVC directly enhances the\nrobustness of the unified embedding against view-missing conditions through KNN\nimputation and data augmentation techniques, eliminating the need for explicit\nmissing view recovery. Finally, incremental improvements are introduced to\nfurther enhance the overall performance, such as the Clustering Module and the\ncustomization of the Encoder. We extensively evaluate the proposed URRL-IMVC\nframework on various benchmark datasets, demonstrating its state-of-the-art\nperformance. Furthermore, comprehensive ablation studies are performed to\nvalidate the effectiveness of our design."
        ],
        "neg": []
    },
    {
        "query": "Gender bias has been a focal point in the study of bias in machine\ntranslation and language models. Existing machine translation gender bias\nevaluations are primarily focused on male and female genders, limiting the\nscope of the evaluation. To assess gender bias accurately, these studies often\nrely on calculating the accuracy of gender pronouns or the masculine and\nfeminine attributes of grammatical gender via the stereotypes triggered by\noccupations or sentiment words ({\\em i.e.}, clear positive or negative\nattitude), which cannot extend to non-binary groups. This study presents a\nbenchmark AmbGIMT (Gender-Inclusive Machine Translation with Ambiguous attitude\nwords), which assesses gender bias beyond binary gender. Meanwhile, we propose\na novel process to evaluate gender bias based on the Emotional Attitude Score\n(EAS), which is used to quantify ambiguous attitude words. In evaluating three\nrecent and effective open-source LLMs and one powerful multilingual\ntranslation-specific model, our main observations are: (1) The translation\nperformance within non-binary gender contexts is markedly inferior in terms of\ntranslation quality and exhibits more negative attitudes than binary-gender\ncontexts. (2) The analysis experiments indicate that incorporating constraint\ncontext in prompts for gender identity terms can substantially reduce\ntranslation bias, while the bias remains evident despite the presence of the\nconstraints. The code is publicly available at\n\\url{https://github.com/pppa2019/ambGIMT}.",
        "pos": [
            "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}.",
            "In-image machine translation (IIMT) aims to translate an image containing\ntexts in source language into an image containing translations in target\nlanguage. In this regard, conventional cascaded methods suffer from issues such\nas error propagation, massive parameters, and difficulties in deployment and\nretaining visual characteristics of the input image. Thus, constructing\nend-to-end models has become an option, which, however, faces two main\nchallenges: 1) the huge modeling burden, as it is required to simultaneously\nlearn alignment across languages and preserve the visual characteristics of the\ninput image; 2) the difficulties of directly predicting excessively lengthy\npixel sequences. In this paper, we propose \\textit{Translatotron-V(ision)}, an\nend-to-end IIMT model consisting of four modules. In addition to an image\nencoder, and an image decoder, our model contains a target text decoder and an\nimage tokenizer. Among them, the target text decoder is used to alleviate the\nlanguage alignment burden, and the image tokenizer converts long sequences of\npixels into shorter sequences of visual tokens, preventing the model from\nfocusing on low-level visual features. Besides, we present a two-stage training\nframework for our model to assist the model in learning alignment across\nmodalities and languages. Finally, we propose a location-aware evaluation\nmetric called Structure-BLEU to assess the translation quality of the generated\nimages. Experimental results demonstrate that our model achieves competitive\nperformance compared to cascaded models with only 70.9\\% of parameters, and\nsignificantly outperforms the pixel-level end-to-end IIMT model."
        ],
        "neg": []
    },
    {
        "query": "Large language models demonstrate reasonable multilingual abilities, despite\npredominantly English-centric pretraining. However, the spontaneous\nmultilingual alignment in these models is shown to be weak, leading to\nunsatisfactory cross-lingual transfer and knowledge sharing. Previous works\nattempt to address this issue by explicitly injecting multilingual alignment\ninformation during or after pretraining. Thus for the early stage in\npretraining, the alignment is weak for sharing information or knowledge across\nlanguages. In this paper, we propose PreAlign, a framework that establishes\nmultilingual alignment prior to language model pretraining. PreAlign injects\nmultilingual alignment by initializing the model to generate similar\nrepresentations of aligned words and preserves this alignment using a\ncode-switching strategy during pretraining. Extensive experiments in a\nsynthetic English to English-Clone setting demonstrate that PreAlign\nsignificantly outperforms standard multilingual joint training in language\nmodeling, zero-shot cross-lingual transfer, and cross-lingual knowledge\napplication. Further experiments in real-world scenarios further validate\nPreAlign's effectiveness across various model sizes.",
        "pos": [
            "Decoding by contrasting layers (DoLa), is designed to improve the generation\nquality of large language models (LLMs) by contrasting the prediction\nprobabilities between an early exit output (amateur logits) and the final\noutput (expert logits). However, we find that this approach does not work well\non non-English tasks. Inspired by previous interpretability work on language\ntransition during the model's forward pass, we discover that this issue arises\nfrom a language mismatch between early exit output and final output. In this\nwork, we propose an improved contrastive decoding algorithm that is effective\nfor diverse languages beyond English. To obtain more helpful amateur logits, we\ndevise two strategies to skip a set of bottom, language-agnostic layers based\non our preliminary analysis. Experimental results on multilingual reasoning\nbenchmarks demonstrate that our proposed method outperforms previous\ncontrastive decoding baselines and substantially improves LLM's\nchain-of-thought reasoning accuracy across 11 languages. The project will be\navailable at: https://github.com/NJUNLP/SkipLayerCD."
        ],
        "neg": []
    },
    {
        "query": "Speculative decoding has emerged as a promising technique to accelerate the\ninference of Large Language Models (LLMs) by employing a small language model\nto draft a hypothesis sequence, which is then validated by the LLM. The\neffectiveness of this approach heavily relies on the balance between\nperformance and efficiency of the draft model. In our research, we focus on\nenhancing the proportion of draft tokens that are accepted to the final output\nby generating multiple hypotheses instead of just one. This allows the LLM more\noptions to choose from and select the longest sequence that meets its\nstandards. Our analysis reveals that hypotheses produced by the draft model\nshare many common token sequences, suggesting a potential for optimizing\ncomputation. Leveraging this observation, we introduce an innovative approach\nutilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This\nstructure enables us to efficiently predict and merge recurring token\nsequences, vastly reducing the computational demands of the draft model. We\nterm this approach Graph-structured Speculative Decoding (GSD). We apply GSD\nacross a range of LLMs, including a 70-billion parameter LLaMA-2 model, and\nobserve a remarkable speedup of 1.73$\\times$ to 1.96$\\times$, significantly\nsurpassing standard speculative decoding.",
        "pos": [
            "Is it always necessary to compute tokens from shallow to deep layers in\nTransformers? The continued success of vanilla Transformers and their variants\nsuggests an undoubted \"yes\". In this work, however, we attempt to break the\ndepth-ordered convention by proposing a novel architecture dubbed\nmixture-of-modules (MoM), which is motivated by an intuition that any layer,\nregardless of its position, can be used to compute a token as long as it\npossesses the needed processing capabilities. The construction of MoM starts\nfrom a finite set of modules defined by multi-head attention and feed-forward\nnetworks, each distinguished by its unique parameterization. Two routers then\niteratively select attention modules and feed-forward modules from the set to\nprocess a token. The selection dynamically expands the computation graph in the\nforward pass of the token, culminating in an assembly of modules. We show that\nMoM provides not only a unified framework for Transformers and their numerous\nvariants but also a flexible and learnable approach for reducing redundancy in\nTransformer parameterization. We pre-train various MoMs using OpenWebText.\nEmpirical results demonstrate that MoMs, of different parameter counts,\nconsistently outperform vanilla transformers on both GLUE and XSUM benchmarks.\nMore interestingly, with a fixed parameter budget, MoM-large enables an over\n38% increase in depth for computation graphs compared to GPT-2-large, resulting\nin absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large\nalso enables an over 60% reduction in depth while involving more modules per\nlayer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage\ncompared to GPT-2-large, while maintaining comparable performance."
        ],
        "neg": []
    },
    {
        "query": "Speculative decoding has emerged as a promising technique to accelerate the\ninference of Large Language Models (LLMs) by employing a small language model\nto draft a hypothesis sequence, which is then validated by the LLM. The\neffectiveness of this approach heavily relies on the balance between\nperformance and efficiency of the draft model. In our research, we focus on\nenhancing the proportion of draft tokens that are accepted to the final output\nby generating multiple hypotheses instead of just one. This allows the LLM more\noptions to choose from and select the longest sequence that meets its\nstandards. Our analysis reveals that hypotheses produced by the draft model\nshare many common token sequences, suggesting a potential for optimizing\ncomputation. Leveraging this observation, we introduce an innovative approach\nutilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This\nstructure enables us to efficiently predict and merge recurring token\nsequences, vastly reducing the computational demands of the draft model. We\nterm this approach Graph-structured Speculative Decoding (GSD). We apply GSD\nacross a range of LLMs, including a 70-billion parameter LLaMA-2 model, and\nobserve a remarkable speedup of 1.73$\\times$ to 1.96$\\times$, significantly\nsurpassing standard speculative decoding.",
        "pos": [
            "Video Question Answering (VideoQA) has emerged as a challenging frontier in\nthe field of multimedia processing, requiring intricate interactions between\nvisual and textual modalities. Simply uniformly sampling frames or\nindiscriminately aggregating frame-level visual features often falls short in\ncapturing the nuanced and relevant contexts of videos to well perform VideoQA.\nTo mitigate these issues, we propose VidF4, a novel VideoQA framework equipped\nwith tailored frame selection strategy for effective and efficient VideoQA. We\npropose three frame-scoring mechanisms that consider both question relevance\nand inter-frame similarity to evaluate the importance of each frame for a given\nquestion on the video. Furthermore, we design a differentiable adaptive frame\nsampling mechanism to facilitate end-to-end training for the frame selector and\nanswer generator. The experimental results across three widely adopted\nbenchmarks demonstrate that our model consistently outperforms existing VideoQA\nmethods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA\n(+1.0%). Furthermore, through both quantitative and qualitative analyses, we\nvalidate the effectiveness of each design choice.",
            "Is it always necessary to compute tokens from shallow to deep layers in\nTransformers? The continued success of vanilla Transformers and their variants\nsuggests an undoubted \"yes\". In this work, however, we attempt to break the\ndepth-ordered convention by proposing a novel architecture dubbed\nmixture-of-modules (MoM), which is motivated by an intuition that any layer,\nregardless of its position, can be used to compute a token as long as it\npossesses the needed processing capabilities. The construction of MoM starts\nfrom a finite set of modules defined by multi-head attention and feed-forward\nnetworks, each distinguished by its unique parameterization. Two routers then\niteratively select attention modules and feed-forward modules from the set to\nprocess a token. The selection dynamically expands the computation graph in the\nforward pass of the token, culminating in an assembly of modules. We show that\nMoM provides not only a unified framework for Transformers and their numerous\nvariants but also a flexible and learnable approach for reducing redundancy in\nTransformer parameterization. We pre-train various MoMs using OpenWebText.\nEmpirical results demonstrate that MoMs, of different parameter counts,\nconsistently outperform vanilla transformers on both GLUE and XSUM benchmarks.\nMore interestingly, with a fixed parameter budget, MoM-large enables an over\n38% increase in depth for computation graphs compared to GPT-2-large, resulting\nin absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large\nalso enables an over 60% reduction in depth while involving more modules per\nlayer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage\ncompared to GPT-2-large, while maintaining comparable performance.",
            "Adapting large language models (LLMs) to new languages typically involves\ncontinual pre-training (CT) followed by supervised fine-tuning (SFT). However,\nthis CT-then-SFT approach struggles with limited data in the context of\nlow-resource languages, failing to balance language modeling and task-solving\ncapabilities. We thus propose model merging as an alternative for low-resource\nlanguages, combining models with distinct capabilities into a single model\nwithout additional training. We use model merging to develop task-solving LLMs\nfor low-resource languages without SFT data in the target languages. Our\nexperiments based on Llama-2-7B demonstrate that model merging effectively\nendows LLMs for low-resource languages with task-solving abilities,\noutperforming CT-then-SFT in scenarios with extremely scarce data. Observing\nperformance saturation in model merging with more training tokens, we further\nanalyze the merging process and introduce a slack variable to the model merging\nalgorithm to mitigate the loss of important parameters, thereby enhancing\nperformance. We hope that model merging can benefit more human languages\nsuffering from data scarcity with its higher data efficiency."
        ],
        "neg": []
    },
    {
        "query": "The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these LLMs still have numerous inherent vulnerabilities,\nparticularly when faced with jailbreak attacks. By investigating jailbreak\nattacks, we can uncover hidden weaknesses in LLMs and inform the development of\nmore robust defense mechanisms to fortify their security. In this paper, we\nfurther explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analyzing-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse. The code is\npublicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This\npaper contains examples of LLMs that might be offensive or harmful.",
        "pos": [
            "Hallucination, a phenomenon where large language models (LLMs) produce output\nthat is factually incorrect or unrelated to the input, is a major challenge for\nLLM applications that require accuracy and dependability. In this paper, we\nintroduce a reliable and high-speed production system aimed at detecting and\nrectifying the hallucination issue within LLMs. Our system encompasses named\nentity recognition (NER), natural language inference (NLI), span-based\ndetection (SBD), and an intricate decision tree-based process to reliably\ndetect a wide range of hallucinations in LLM responses. Furthermore, our team\nhas crafted a rewriting mechanism that maintains an optimal mix of precision,\nresponse time, and cost-effectiveness. We detail the core elements of our\nframework and underscore the paramount challenges tied to response time,\navailability, and performance metrics, which are crucial for real-world\ndeployment of these technologies. Our extensive evaluation, utilizing offline\ndata and live production traffic, confirms the efficacy of our proposed\nframework and service."
        ],
        "neg": []
    },
    {
        "query": "This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks.",
        "pos": [
            "This research addresses the issue of missing structured data in dental\nrecords by extracting diagnostic information from unstructured text. The\nupdated periodontology classification system's complexity has increased\nincomplete or missing structured diagnoses. To tackle this, we use advanced AI\nand NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a\nRoBERTa model. This significantly enhances the model's ability to understand\nmedical and dental language. We evaluated the model using 120 randomly selected\nclinical notes from two datasets, demonstrating its improved diagnostic\nextraction accuracy. The results showed high accuracy in diagnosing periodontal\nstatus, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In\nthe subtype category, Site 2 achieved perfect scores, outperforming Site 1.\nThis method enhances extraction accuracy and broadens its use across dental\ncontexts. The study underscores AI and NLP's transformative impact on\nhealthcare delivery and management. Integrating AI and NLP technologies\nenhances documentation and simplifies administrative tasks by precisely\nextracting complex clinical information. This approach effectively addresses\nchallenges in dental diagnostics. Using synthetic training data from LLMs\noptimizes the training process, improving accuracy and efficiency in\nidentifying periodontal diagnoses from clinical notes. This innovative method\nholds promise for broader healthcare applications, potentially improving\npatient care quality."
        ],
        "neg": []
    },
    {
        "query": "This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks.",
        "pos": [
            "This research addresses the issue of missing structured data in dental\nrecords by extracting diagnostic information from unstructured text. The\nupdated periodontology classification system's complexity has increased\nincomplete or missing structured diagnoses. To tackle this, we use advanced AI\nand NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a\nRoBERTa model. This significantly enhances the model's ability to understand\nmedical and dental language. We evaluated the model using 120 randomly selected\nclinical notes from two datasets, demonstrating its improved diagnostic\nextraction accuracy. The results showed high accuracy in diagnosing periodontal\nstatus, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In\nthe subtype category, Site 2 achieved perfect scores, outperforming Site 1.\nThis method enhances extraction accuracy and broadens its use across dental\ncontexts. The study underscores AI and NLP's transformative impact on\nhealthcare delivery and management. Integrating AI and NLP technologies\nenhances documentation and simplifies administrative tasks by precisely\nextracting complex clinical information. This approach effectively addresses\nchallenges in dental diagnostics. Using synthetic training data from LLMs\noptimizes the training process, improving accuracy and efficiency in\nidentifying periodontal diagnoses from clinical notes. This innovative method\nholds promise for broader healthcare applications, potentially improving\npatient care quality."
        ],
        "neg": []
    },
    {
        "query": "Despite the advanced intelligence abilities of large language models (LLMs)\nin various applications, they still face significant computational and storage\ndemands. Knowledge Distillation (KD) has emerged as an effective strategy to\nimprove the performance of a smaller LLM (i.e., the student model) by\ntransferring knowledge from a high-performing LLM (i.e., the teacher model).\nPrevailing techniques in LLM distillation typically use a black-box model API\nto generate high-quality pretrained and aligned datasets, or utilize white-box\ndistillation by altering the loss function to better transfer knowledge from\nthe teacher LLM. However, these methods ignore the knowledge differences\nbetween the student and teacher LLMs across domains. This results in excessive\nfocus on domains with minimal performance gaps and insufficient attention to\ndomains with large gaps, reducing overall performance. In this paper, we\nintroduce a new LLM distillation framework called DDK, which dynamically\nadjusts the composition of the distillation dataset in a smooth manner\naccording to the domain performance differences between the teacher and student\nmodels, making the distillation process more stable and effective. Extensive\nevaluations show that DDK significantly improves the performance of student\nmodels, outperforming both continuously pretrained baselines and existing\nknowledge distillation methods by a large margin.",
        "pos": [
            "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
            "Text relevance or text matching of query and product is an essential\ntechnique for the e-commerce search system to ensure that the displayed\nproducts can match the intent of the query. Many studies focus on improving the\nperformance of the relevance model in search system. Recently, pre-trained\nlanguage models like BERT have achieved promising performance on the text\nrelevance task. While these models perform well on the offline test dataset,\nthere are still obstacles to deploy the pre-trained language model to the\nonline system as their high latency. The two-tower model is extensively\nemployed in industrial scenarios, owing to its ability to harmonize performance\nwith computational efficiency. Regrettably, such models present an opaque\n``black box'' nature, which prevents developers from making special\noptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an\nefficient and interpretable relevance architecture for Chinese e-commerce. Our\napproach proposes to encode the query and the product into the sparse BoW\nrepresentation, which is a set of word-weight pairs. The weight means the\nimportant or the relevant score between the corresponding word and the raw\ntext. The relevance score is measured by the accumulation of the matched word\nbetween the sparse BoW representation of the query and the product. Compared to\npopular dense distributed representation that usually suffers from the drawback\nof black-box, the most advantage of the proposed representation model is highly\nexplainable and interventionable, which is a superior advantage to the\ndeployment and operation of online search engines. Moreover, the online\nefficiency of the proposed model is even better than the most efficient inner\nproduct form of dense representation ..."
        ],
        "neg": []
    },
    {
        "query": "Literature review requires researchers to synthesize a large amount of\ninformation and is increasingly challenging as the scientific literature\nexpands. In this work, we investigate the potential of LLMs for producing\nhierarchical organizations of scientific studies to assist researchers with\nliterature review. We define hierarchical organizations as tree structures\nwhere nodes refer to topical categories and every node is linked to the studies\nassigned to that category. Our naive LLM-based pipeline for hierarchy\ngeneration from a set of studies produces promising yet imperfect hierarchies,\nmotivating us to collect CHIME, an expert-curated dataset for this task focused\non biomedicine. Given the challenging and time-consuming nature of building\nhierarchies from scratch, we use a human-in-the-loop process in which experts\ncorrect errors (both links between categories and study assignment) in\nLLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies\ncovering 472 topics, and expert-corrected hierarchies for a subset of 100\ntopics. Expert corrections allow us to quantify LLM performance, and we find\nthat while they are quite good at generating and organizing categories, their\nassignment of studies to categories could be improved. We attempt to train a\ncorrector model with human feedback which improves study assignment by 12.6 F1\npoints. We release our dataset and models to encourage research on developing\nbetter assistive tools for literature review.",
        "pos": [
            "GPT-4V's purported strong multimodal abilities raise interests in using it to\nautomate radiology report writing, but there lacks thorough evaluations. In\nthis work, we perform a systematic evaluation of GPT-4V in generating radiology\nreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt\nto directly generate reports using GPT-4V through different prompting\nstrategies and find that it fails terribly in both lexical metrics and clinical\nefficacy metrics. To understand the low performance, we decompose the task into\ntwo steps: 1) the medical image reasoning step of predicting medical condition\nlabels from images; and 2) the report synthesis step of generating reports from\n(groundtruth) conditions. We show that GPT-4V's performance in image reasoning\nis consistently low across different prompts. In fact, the distributions of\nmodel-predicted labels remain constant regardless of which groundtruth\nconditions are present on the image, suggesting that the model is not\ninterpreting chest X-rays meaningfully. Even when given groundtruth conditions\nin report synthesis, its generated reports are less correct and less\nnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt\non the viability of using GPT-4V in a radiology workflow."
        ],
        "neg": []
    },
    {
        "query": "Traditional knowledge graph (KG) completion models learn embeddings to\npredict missing facts. Recent works attempt to complete KGs in a\ntext-generation manner with large language models (LLMs). However, they need to\nground the output of LLMs to KG entities, which inevitably brings errors. In\nthis paper, we present a finetuning framework, DIFT, aiming to unleash the KG\ncompletion ability of LLMs and avoid grounding errors. Given an incomplete\nfact, DIFT employs a lightweight model to obtain candidate entities and\nfinetunes an LLM with discrimination instructions to select the correct one\nfrom the given candidates. To improve performance while reducing instruction\ndata, DIFT uses a truncated sampling method to select useful facts for\nfinetuning and injects KG embeddings into the LLM. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of our proposed framework.",
        "pos": [
            "Knowledge graphs (KGs) are widely acknowledged as incomplete, and new\nentities are constantly emerging in the real world. Inductive KG reasoning aims\nto predict missing facts for these new entities. Among existing models, graph\nneural networks (GNNs) based ones have shown promising performance for this\ntask. However, they are still challenged by inefficient message propagation due\nto the distance and scalability issues. In this paper, we propose a new\ninductive KG reasoning model, MStar, by leveraging conditional message passing\nneural networks (C-MPNNs). Our key insight is to select multiple query-specific\nstarting entities to expand the scope of progressive propagation. To propagate\nquery-related messages to a farther area within limited steps, we subsequently\ndesign a highway layer to propagate information toward these selected starting\nentities. Moreover, we introduce a training strategy called LinkVerify to\nmitigate the impact of noisy training samples. Experimental results validate\nthat MStar achieves superior performance compared with state-of-the-art models,\nespecially for distant entities.",
            "Controllability and proactivity are crucial properties of autonomous\nconversational agents (CAs). Controllability requires the CAs to follow the\nstandard operating procedures (SOPs), such as verifying identity before\nactivating credit cards. Proactivity requires the CAs to guide the conversation\ntowards the goal during user uncooperation, such as persuasive dialogue.\nExisting research cannot be unified with controllability, proactivity, and low\nmanual annotation. To bridge this gap, we propose a new framework for\nplanning-based conversational agents (PCA) powered by large language models\n(LLMs), which only requires humans to define tasks and goals for the LLMs.\nBefore conversation, LLM plans the core and necessary SOP for dialogue offline.\nDuring the conversation, LLM plans the best action path online referring to the\nSOP, and generates responses to achieve process controllability. Subsequently,\nwe propose a semi-automatic dialogue data creation framework and curate a\nhigh-quality dialogue dataset (PCA-D). Meanwhile, we develop multiple variants\nand evaluation metrics for PCA, e.g., planning with Monte Carlo Tree Search\n(PCA-M), which searches for the optimal dialogue action while satisfying SOP\nconstraints and achieving the proactive of the dialogue. Experiment results\nshow that LLMs finetuned on PCA-D can significantly improve the performance and\ngeneralize to unseen domains. PCA-M outperforms other CoT and ToT baselines in\nterms of conversation controllability, proactivity, task success rate, and\noverall logical coherence, and is applicable in industry dialogue scenarios.\nThe dataset and codes are available at XXXX."
        ],
        "neg": []
    },
    {
        "query": "Empathetic response generation is designed to comprehend the emotions of\nothers and select the most appropriate strategies to assist them in resolving\nemotional challenges. Empathy can be categorized into cognitive empathy and\naffective empathy. The former pertains to the ability to understand and discern\nthe emotional issues and situations of others, while the latter involves the\ncapacity to provide comfort. To enhance one's empathetic abilities, it is\nessential to develop both these aspects. Therefore, we develop an innovative\nframework that combines retrieval augmentation and emotional support strategy\nintegration. Our framework starts with the introduction of a comprehensive\nemotional palette for empathy. We then apply appraisal theory to decompose this\npalette and create a database of empathetic responses. This database serves as\nan external resource and enhances the LLM's empathy by integrating semantic\nretrieval mechanisms. Moreover, our framework places a strong emphasis on the\nproper articulation of response strategies. By incorporating emotional support\nstrategies, we aim to enrich the model's capabilities in both cognitive and\naffective empathy, leading to a more nuanced and comprehensive empathetic\nresponse. Finally, we extract datasets ED and ET from the empathetic dialogue\ndataset \\textsc{EmpatheticDialogues} and ExTES based on dialogue length.\nExperiments demonstrate that our framework can enhance the empathy ability of\nLLMs from both cognitive and affective empathy perspectives. Our code is\nreleased at https://github.com/CAS-SIAT-XinHai/APTNESS.",
        "pos": [
            "LLM-based agents have demonstrated impressive zero-shot performance in the\nvision-language navigation (VLN) task. However, these zero-shot methods focus\nonly on solving high-level task planning by selecting nodes in predefined\nnavigation graphs for movements, overlooking low-level control in realistic\nnavigation scenarios. To bridge this gap, we propose AO-Planner, a novel\naffordances-oriented planning framework for continuous VLN task. Our AO-Planner\nintegrates various foundation models to achieve affordances-oriented motion\nplanning and action decision-making, both performed in a zero-shot manner.\nSpecifically, we employ a visual affordances prompting (VAP) approach, where\nvisible ground is segmented utilizing SAM to provide navigational affordances,\nbased on which the LLM selects potential next waypoints and generates low-level\npath planning towards selected waypoints. We further introduce a high-level\nagent, PathAgent, to identify the most probable pixel-based path and convert it\ninto 3D coordinates to fulfill low-level motion. Experimental results on the\nchallenging R2R-CE benchmark demonstrate that AO-Planner achieves\nstate-of-the-art zero-shot performance (5.5% improvement in SPL). Our method\nestablishes an effective connection between LLM and 3D world to circumvent the\ndifficulty of directly predicting world coordinates, presenting novel prospects\nfor employing foundation models in low-level motion control."
        ],
        "neg": []
    },
    {
        "query": "Temporal reasoning over tabular data presents substantial challenges for\nlarge language models (LLMs), as evidenced by recent research. In this study,\nwe conduct a comprehensive analysis of temporal datasets to pinpoint the\nspecific limitations of LLMs. Our investigation leads to enhancements in\nTempTabQA, a dataset specifically designed for tabular temporal question\nanswering. We provide critical insights for improving LLM performance in\ntemporal reasoning tasks with tabular data. Furthermore, we introduce a novel\napproach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings\ndemonstrate that our method significantly improves evidence-based reasoning\nacross various models. Additionally, our experimental results reveal that\nindirect supervision with auxiliary data substantially boosts model performance\nin these tasks. This work contributes to a deeper understanding of LLMs'\ntemporal reasoning abilities over tabular data and promotes advancements in\ntheir application across diverse fields.",
        "pos": [
            "Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.",
            "Cognitive textual and visual reasoning tasks, such as puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. While LLMs and VLMs, through extensive\ntraining on large amounts of human-curated data, have attained a high level of\npseudo-human intelligence in some common sense reasoning tasks, they still\nstruggle with more complex reasoning tasks that require cognitive\nunderstanding. In this work, we introduce a new dataset, NTSEBench, designed to\nevaluate the cognitive multi-modal reasoning and problem-solving skills of\nlarge models. The dataset comprises 2,728 multiple-choice questions comprising\nof a total of 4,642 images across 26 categories sampled from the NTSE\nexamination conducted nationwide in India, featuring both visual and textual\ngeneral aptitude questions that do not rely on rote learning. We establish\nbaselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a\ncomparison between open source and propriety models, we propose four distinct\nmodeling strategies to handle different modalities (text and images) in the\ndataset instances."
        ],
        "neg": []
    },
    {
        "query": "Temporal reasoning over tabular data presents substantial challenges for\nlarge language models (LLMs), as evidenced by recent research. In this study,\nwe conduct a comprehensive analysis of temporal datasets to pinpoint the\nspecific limitations of LLMs. Our investigation leads to enhancements in\nTempTabQA, a dataset specifically designed for tabular temporal question\nanswering. We provide critical insights for improving LLM performance in\ntemporal reasoning tasks with tabular data. Furthermore, we introduce a novel\napproach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings\ndemonstrate that our method significantly improves evidence-based reasoning\nacross various models. Additionally, our experimental results reveal that\nindirect supervision with auxiliary data substantially boosts model performance\nin these tasks. This work contributes to a deeper understanding of LLMs'\ntemporal reasoning abilities over tabular data and promotes advancements in\ntheir application across diverse fields.",
        "pos": [
            "Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.",
            "Cognitive textual and visual reasoning tasks, such as puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. While LLMs and VLMs, through extensive\ntraining on large amounts of human-curated data, have attained a high level of\npseudo-human intelligence in some common sense reasoning tasks, they still\nstruggle with more complex reasoning tasks that require cognitive\nunderstanding. In this work, we introduce a new dataset, NTSEBench, designed to\nevaluate the cognitive multi-modal reasoning and problem-solving skills of\nlarge models. The dataset comprises 2,728 multiple-choice questions comprising\nof a total of 4,642 images across 26 categories sampled from the NTSE\nexamination conducted nationwide in India, featuring both visual and textual\ngeneral aptitude questions that do not rely on rote learning. We establish\nbaselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a\ncomparison between open source and propriety models, we propose four distinct\nmodeling strategies to handle different modalities (text and images) in the\ndataset instances."
        ],
        "neg": []
    },
    {
        "query": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. They are trained using preference datasets where each\nexample consists of one input prompt, two responses, and a preference label. As\ncurating a high-quality human labeled preference dataset is both time-consuming\nand expensive, people often rely on existing powerful LLMs for preference label\ngeneration. This can potentially introduce noise and impede RM training. In\nthis work, we present RMBoost, a novel synthetic preference data generation\nparadigm to boost reward model quality. Unlike traditional methods, which\ngenerate two responses before obtaining the preference label, RMBoost first\ngenerates one response and selects a preference label, followed by generating\nthe second more (or less) preferred response conditioned on the pre-selected\npreference label and the first response. This approach offers two main\nadvantages. First, RMBoost reduces labeling noise since preference pairs are\nconstructed intentionally. Second, RMBoost facilitates the creation of more\ndiverse responses by incorporating various quality aspects (e.g., helpfulness,\nrelevance, completeness) into the prompts. We conduct extensive experiments\nacross three diverse datasets and demonstrate that RMBoost outperforms other\nsynthetic preference data generation techniques and significantly boosts the\nperformance of four distinct reward models.",
        "pos": [
            "The popularity of automated news headline generation has surged with\nadvancements in pre-trained language models. However, these models often suffer\nfrom the ``hallucination'' problem, where the generated headline is not fully\nsupported by its source article. Efforts to address this issue have\npredominantly focused on English, using over-simplistic classification schemes\nthat overlook nuanced hallucination types. In this study, we introduce the\nfirst multilingual, fine-grained news headline hallucination detection dataset\nthat contains over 11 thousand pairs in 5 languages, each annotated with\ndetailed hallucination types by experts. We conduct extensive experiments on\nthis dataset under two settings. First, we implement several supervised\nfine-tuning approaches as preparatory solutions and demonstrate this dataset's\nchallenges and utilities. Second, we test various large language models'\nin-context learning abilities and propose two novel techniques,\nlanguage-dependent demonstration selection and coarse-to-fine prompting, to\nboost the few-shot hallucination detection performance in terms of the\nexample-F1 metric. We release this dataset to foster further research in\nmultilingual, fine-grained headline hallucination detection."
        ],
        "neg": []
    },
    {
        "query": "Modern language models (LMs) pose a new challenge in capability assessment.\nStatic benchmarks inevitably saturate without providing confidence in the\ndeployment tolerances of LM-based systems, but developers nonetheless claim\nthat their models have generalized traits such as reasoning or open-domain\nlanguage understanding based on these flawed metrics. The science and practice\nof LMs requires a new approach to benchmarking which measures specific\ncapabilities with dynamic assessments. To be confident in our metrics, we need\na new discipline of model metrology -- one which focuses on how to generate\nbenchmarks that predict performance under deployment. Motivated by our\nevaluation criteria, we outline how building a community of model metrology\npractitioners -- one focused on building tools and studying how to measure\nsystem capabilities -- is the best way to meet these needs to and add clarity\nto the AI discussion.",
        "pos": [
            "RLHF-aligned LMs have shown unprecedented ability on both benchmarks and\nlong-form text generation, yet they struggle with one foundational task:\nnext-token prediction. As RLHF models become agent models aimed at interacting\nwith humans, they seem to lose their world modeling -- the ability to predict\nwhat comes next in arbitrary documents, which is the foundational training\nobjective of the Base LMs that RLHF adapts.\n  Besides empirically demonstrating this trade-off, we propose a potential\nexplanation: to perform coherent long-form generation, RLHF models restrict\nrandomness via implicit blueprints. In particular, RLHF models concentrate\nprobability on sets of anchor spans that co-occur across multiple generations\nfor the same prompt, serving as textual scaffolding but also limiting a model's\nability to generate documents that do not include these spans. We study this\ntrade-off on the most effective current agent models, those aligned with RLHF,\nwhile exploring why this may remain a fundamental trade-off between models that\nact and those that predict, even as alignment techniques improve."
        ],
        "neg": []
    },
    {
        "query": "Modern language models (LMs) pose a new challenge in capability assessment.\nStatic benchmarks inevitably saturate without providing confidence in the\ndeployment tolerances of LM-based systems, but developers nonetheless claim\nthat their models have generalized traits such as reasoning or open-domain\nlanguage understanding based on these flawed metrics. The science and practice\nof LMs requires a new approach to benchmarking which measures specific\ncapabilities with dynamic assessments. To be confident in our metrics, we need\na new discipline of model metrology -- one which focuses on how to generate\nbenchmarks that predict performance under deployment. Motivated by our\nevaluation criteria, we outline how building a community of model metrology\npractitioners -- one focused on building tools and studying how to measure\nsystem capabilities -- is the best way to meet these needs to and add clarity\nto the AI discussion.",
        "pos": [
            "RLHF-aligned LMs have shown unprecedented ability on both benchmarks and\nlong-form text generation, yet they struggle with one foundational task:\nnext-token prediction. As RLHF models become agent models aimed at interacting\nwith humans, they seem to lose their world modeling -- the ability to predict\nwhat comes next in arbitrary documents, which is the foundational training\nobjective of the Base LMs that RLHF adapts.\n  Besides empirically demonstrating this trade-off, we propose a potential\nexplanation: to perform coherent long-form generation, RLHF models restrict\nrandomness via implicit blueprints. In particular, RLHF models concentrate\nprobability on sets of anchor spans that co-occur across multiple generations\nfor the same prompt, serving as textual scaffolding but also limiting a model's\nability to generate documents that do not include these spans. We study this\ntrade-off on the most effective current agent models, those aligned with RLHF,\nwhile exploring why this may remain a fundamental trade-off between models that\nact and those that predict, even as alignment techniques improve."
        ],
        "neg": []
    },
    {
        "query": "Spoken dialogue plays a crucial role in human-AI interactions, necessitating\ndialogue-oriented spoken language models (SLMs). To develop versatile SLMs,\nlarge-scale and diverse speech datasets are essential. Additionally, to ensure\nhiqh-quality speech generation, the data must be spontaneous like in-wild data\nand must be acoustically clean with noise removed. Despite the critical need,\nno open-source corpus meeting all these criteria has been available. This study\naddresses this gap by constructing and releasing a large-scale spoken dialogue\ncorpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly\naccessible. Furthermore, this paper presents a language-independent method for\ncorpus construction and describes experiments on dialogue generation using SLMs\ntrained on J-CHAT. Experimental results indicate that the collected data from\nmultiple domains by our method improve the naturalness and meaningfulness of\ndialogue generation.",
        "pos": [
            "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
        ],
        "neg": []
    },
    {
        "query": "Spoken dialogue plays a crucial role in human-AI interactions, necessitating\ndialogue-oriented spoken language models (SLMs). To develop versatile SLMs,\nlarge-scale and diverse speech datasets are essential. Additionally, to ensure\nhiqh-quality speech generation, the data must be spontaneous like in-wild data\nand must be acoustically clean with noise removed. Despite the critical need,\nno open-source corpus meeting all these criteria has been available. This study\naddresses this gap by constructing and releasing a large-scale spoken dialogue\ncorpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly\naccessible. Furthermore, this paper presents a language-independent method for\ncorpus construction and describes experiments on dialogue generation using SLMs\ntrained on J-CHAT. Experimental results indicate that the collected data from\nmultiple domains by our method improve the naturalness and meaningfulness of\ndialogue generation.",
        "pos": [
            "Traditional spoken language processing involves cascading an automatic speech\nrecognition (ASR) system into text processing models. In contrast, \"textless\"\nmethods process speech representations without ASR systems, enabling the direct\nuse of acoustic speech features. Although their effectiveness is shown in\ncapturing acoustic features, it is unclear in capturing lexical knowledge. This\npaper proposes a textless method for dependency parsing, examining its\neffectiveness and limitations. Our proposed method predicts a dependency tree\nfrom a speech signal without transcribing, representing the tree as a labeled\nsequence. scading method outperforms the textless method in overall parsing\naccuracy, the latter excels in instances with important acoustic features. Our\nfindings highlight the importance of fusing word-level representations and\nsentence-level prosody for enhanced parsing performance. The code and models\nare made publicly available: https://github.com/mynlp/SpeechParser."
        ],
        "neg": []
    },
    {
        "query": "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge here is to\ndevelop steerable language models that trade-off multiple (conflicting)\nobjectives in a flexible and efficient manner. This paper presents Conditioned\nLanguage Policy (CLP), a general framework for finetuning language models on\nmultiple objectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP can learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through an extensive set of experiments and ablations,\nwe show that the CLP framework learns steerable models that outperform and\nPareto-dominate the current state-of-the-art approaches for multi-objective\nfinetuning.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge here is to\ndevelop steerable language models that trade-off multiple (conflicting)\nobjectives in a flexible and efficient manner. This paper presents Conditioned\nLanguage Policy (CLP), a general framework for finetuning language models on\nmultiple objectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP can learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through an extensive set of experiments and ablations,\nwe show that the CLP framework learns steerable models that outperform and\nPareto-dominate the current state-of-the-art approaches for multi-objective\nfinetuning.",
        "pos": [
            "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Language agents, built on top of language models (LMs), are systems that can\ninteract with complex environments, such as the open web. In this work, we\nexamine whether such agents can perform realistic and time-consuming tasks on\nthe web, e.g., monitoring real-estate markets or locating relevant nearby\nbusinesses. We introduce AssistantBench, a challenging new benchmark consisting\nof 214 realistic tasks that can be automatically evaluated, covering different\nscenarios and domains. We find that AssistantBench exposes the limitations of\ncurrent systems, including language models and retrieval-augmented language\nmodels, as no model reaches an accuracy of more than 25 points. While\nclosed-book LMs perform well, they exhibit low precision since they tend to\nhallucinate facts. State-of-the-art web agents reach a score of near zero.\nAdditionally, we introduce SeePlanAct (SPA), a new web agent that significantly\noutperforms previous agents, and an ensemble of SPA and closed-book models\nreaches the best overall performance. Moreover, we analyze failures of current\nsystems and highlight that web navigation remains a major challenge.",
        "pos": [
            "Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under uncertainty, and investigate the connection\nbetween them. We categorize fallback behaviors -- sequence repetitions,\ndegenerate text, and hallucinations -- and extensively analyze them in models\nfrom the same family that differ by the amount of pretraining tokens, parameter\ncount, or the inclusion of instruction-following training. Our experiments\nreveal a clear and consistent ordering of fallback behaviors, across all these\naxes: the more advanced an LLM is (i.e., trained on more tokens, has more\nparameters, or instruction-tuned), its fallback behavior shifts from sequence\nrepetitions, to degenerate text, and then to hallucinations. Moreover, the same\nordering is observed throughout a single generation, even for the\nbest-performing models; as uncertainty increases, models shift from generating\nhallucinations to producing degenerate text and then sequence repetitions.\nLastly, we demonstrate that while common decoding techniques, such as random\nsampling, might alleviate some unwanted behaviors like sequence repetitions,\nthey increase harder-to-detect hallucinations."
        ],
        "neg": []
    },
    {
        "query": "Language agents, built on top of language models (LMs), are systems that can\ninteract with complex environments, such as the open web. In this work, we\nexamine whether such agents can perform realistic and time-consuming tasks on\nthe web, e.g., monitoring real-estate markets or locating relevant nearby\nbusinesses. We introduce AssistantBench, a challenging new benchmark consisting\nof 214 realistic tasks that can be automatically evaluated, covering different\nscenarios and domains. We find that AssistantBench exposes the limitations of\ncurrent systems, including language models and retrieval-augmented language\nmodels, as no model reaches an accuracy of more than 25 points. While\nclosed-book LMs perform well, they exhibit low precision since they tend to\nhallucinate facts. State-of-the-art web agents reach a score of near zero.\nAdditionally, we introduce SeePlanAct (SPA), a new web agent that significantly\noutperforms previous agents, and an ensemble of SPA and closed-book models\nreaches the best overall performance. Moreover, we analyze failures of current\nsystems and highlight that web navigation remains a major challenge.",
        "pos": [
            "Since language models (LMs) now outperform average humans on many challenging\ntasks, it has become increasingly difficult to develop challenging,\nhigh-quality, and realistic evaluations. We address this issue by examining\nLMs' capabilities to generate code for solving real scientific research\nproblems. Incorporating input from scientists and AI researchers in 16 diverse\nnatural science sub-fields, including mathematics, physics, chemistry, biology,\nand materials science, we created a scientist-curated coding benchmark,\nSciCode. The problems in SciCode naturally factorize into multiple subproblems,\neach involving knowledge recall, reasoning, and code synthesis. In total,\nSciCode contains 338 subproblems decomposed from 80 challenging main problems.\nIt offers optional descriptions specifying useful scientific background\ninformation and scientist-annotated gold-standard solutions and test cases for\nevaluation. Claude3.5-Sonnet, the best-performing model among those tested, can\nsolve only 4.6% of the problems in the most realistic setting. We believe that\nSciCode demonstrates both contemporary LMs' progress towards becoming helpful\nscientific assistants and sheds light on the development and evaluation of\nscientific AI in the future.",
            "Thousands of new scientific papers are published each month. Such information\noverload complicates researcher efforts to stay current with the\nstate-of-the-art as well as to verify and correctly attribute claims. We pose\nthe following research question: Given a text excerpt referencing a paper,\ncould an LM act as a research assistant to correctly identify the referenced\npaper? We advance efforts to answer this question by building a benchmark that\nevaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,\nconsists of text excerpts from recent machine learning papers, each referencing\na single other paper. CiteME use reveals a large gap between frontier LMs and\nhuman performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.\nWe close this gap by introducing CiteAgent, an autonomous system built on the\nGPT-4o LM that can also search and read papers, which achieves an accuracy of\n35.3\\% on CiteME. Overall, CiteME serves as a challenging testbed for\nopen-ended claim attribution, driving the research community towards a future\nwhere any claim made by an LM can be automatically verified and discarded if\nfound to be incorrect."
        ],
        "neg": []
    },
    {
        "query": "Language agents, built on top of language models (LMs), are systems that can\ninteract with complex environments, such as the open web. In this work, we\nexamine whether such agents can perform realistic and time-consuming tasks on\nthe web, e.g., monitoring real-estate markets or locating relevant nearby\nbusinesses. We introduce AssistantBench, a challenging new benchmark consisting\nof 214 realistic tasks that can be automatically evaluated, covering different\nscenarios and domains. We find that AssistantBench exposes the limitations of\ncurrent systems, including language models and retrieval-augmented language\nmodels, as no model reaches an accuracy of more than 25 points. While\nclosed-book LMs perform well, they exhibit low precision since they tend to\nhallucinate facts. State-of-the-art web agents reach a score of near zero.\nAdditionally, we introduce SeePlanAct (SPA), a new web agent that significantly\noutperforms previous agents, and an ensemble of SPA and closed-book models\nreaches the best overall performance. Moreover, we analyze failures of current\nsystems and highlight that web navigation remains a major challenge.",
        "pos": [
            "Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under uncertainty, and investigate the connection\nbetween them. We categorize fallback behaviors -- sequence repetitions,\ndegenerate text, and hallucinations -- and extensively analyze them in models\nfrom the same family that differ by the amount of pretraining tokens, parameter\ncount, or the inclusion of instruction-following training. Our experiments\nreveal a clear and consistent ordering of fallback behaviors, across all these\naxes: the more advanced an LLM is (i.e., trained on more tokens, has more\nparameters, or instruction-tuned), its fallback behavior shifts from sequence\nrepetitions, to degenerate text, and then to hallucinations. Moreover, the same\nordering is observed throughout a single generation, even for the\nbest-performing models; as uncertainty increases, models shift from generating\nhallucinations to producing degenerate text and then sequence repetitions.\nLastly, we demonstrate that while common decoding techniques, such as random\nsampling, might alleviate some unwanted behaviors like sequence repetitions,\nthey increase harder-to-detect hallucinations."
        ],
        "neg": []
    },
    {
        "query": "The internet offers tremendous access to services, social connections, and\nneeded products. However, to those without sufficient experience, engaging with\nbusinesses and friends across the internet can be daunting due to the ever\npresent danger of scammers and thieves, to say nothing of the myriad of\npotential computer viruses. Like a forest rich with both edible and poisonous\nplants, those familiar with the norms inhabit it safely with ease while\nnewcomers need a guide. However, reliance on a human digital guide can be\ntaxing and often impractical. We propose and pilot a simple but unexplored\nidea: could an LLM provide the necessary support to help the elderly who are\nseparated by the digital divide safely achieve digital autonomy?",
        "pos": [
            "This paper evaluates whether large language models (LLMs) exhibit cognitive\nfan effects, similar to those discovered by Anderson in humans, after being\npre-trained on human textual data. We conduct two sets of in-context recall\nexperiments designed to elicit fan effects. Consistent with human results, we\nfind that LLM recall uncertainty, measured via token probability, is influenced\nby the fan effect. Our results show that removing uncertainty disrupts the\nobserved effect. The experiments suggest the fan effect is consistent whether\nthe fan value is induced in-context or in the pre-training data. Finally, these\nfindings provide in-silico evidence that fan effects and typicality are\nexpressions of the same phenomena."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) in medicine. However LLMs often generate outdated or\ninaccurate information based on static training datasets. Retrieval augmented\ngeneration (RAG) mitigates this by integrating outside data sources. While\nprevious RAG systems used pre-assembled, fixed databases with limited\nflexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end\nframework that retrieves data from authoritative radiologic online sources in\nreal-time. RadioRAG is evaluated using a dedicated radiologic\nquestion-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of\nvarious LLMs when answering radiology-specific questions with and without\naccess to additional online information via RAG. Using 80 questions from RSNA\nCase Collection across radiologic subspecialties and 24 additional\nexpert-curated questions, for which the correct gold-standard answers were\navailable, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B\nand 70B]) were prompted with and without RadioRAG. RadioRAG retrieved\ncontext-specific information from www.radiopaedia.org in real-time and\nincorporated them into its reply. RadioRAG consistently improved diagnostic\naccuracy across all LLMs, with relative improvements ranging from 2% to 54%. It\nmatched or exceeded question answering without RAG across radiologic\nsubspecialties, particularly in breast imaging and emergency radiology.\nHowever, degree of improvement varied among models; GPT-3.5-turbo and\nMixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2\nshowed no improvement, highlighting variability in its effectiveness. LLMs\nbenefit when provided access to domain-specific data beyond their training\ndata. For radiology, RadioRAG establishes a robust framework that substantially\nimproves diagnostic accuracy and factuality in radiological question answering.",
        "pos": [
            "Matching cancer patients to clinical trials is essential for advancing\ntreatment and patient care. However, the inconsistent format of medical free\ntext documents and complex trial eligibility criteria make this process\nextremely challenging and time-consuming for physicians. We investigated\nwhether the entire trial matching process - from identifying relevant trials\namong 105,600 oncology-related clinical trials on clinicaltrials.gov to\ngenerating criterion-level eligibility matches - could be automated using Large\nLanguage Models (LLMs). Using GPT-4o and a set of 51 synthetic Electronic\nHealth Records (EHRs), we demonstrate that our approach identifies relevant\ncandidate trials in 93.3% of cases and achieves a preliminary accuracy of 88.0%\nwhen matching patient-level information at the criterion level against a\nbaseline defined by human experts. Utilizing LLM feedback reveals that 39.3%\ncriteria that were initially considered incorrect are either ambiguous or\ninaccurately annotated, leading to a total model accuracy of 92.7% after\nrefining our human baseline. In summary, we present an end-to-end pipeline for\nclinical trial matching using LLMs, demonstrating high precision in screening\nand matching trials to individual patients, even outperforming the performance\nof qualified medical doctors. Our fully end-to-end pipeline can operate\nautonomously or with human supervision and is not restricted to oncology,\noffering a scalable solution for enhancing patient-trial matching in real-world\nsettings."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) in medicine. However LLMs often generate outdated or\ninaccurate information based on static training datasets. Retrieval augmented\ngeneration (RAG) mitigates this by integrating outside data sources. While\nprevious RAG systems used pre-assembled, fixed databases with limited\nflexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end\nframework that retrieves data from authoritative radiologic online sources in\nreal-time. RadioRAG is evaluated using a dedicated radiologic\nquestion-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of\nvarious LLMs when answering radiology-specific questions with and without\naccess to additional online information via RAG. Using 80 questions from RSNA\nCase Collection across radiologic subspecialties and 24 additional\nexpert-curated questions, for which the correct gold-standard answers were\navailable, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B\nand 70B]) were prompted with and without RadioRAG. RadioRAG retrieved\ncontext-specific information from www.radiopaedia.org in real-time and\nincorporated them into its reply. RadioRAG consistently improved diagnostic\naccuracy across all LLMs, with relative improvements ranging from 2% to 54%. It\nmatched or exceeded question answering without RAG across radiologic\nsubspecialties, particularly in breast imaging and emergency radiology.\nHowever, degree of improvement varied among models; GPT-3.5-turbo and\nMixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2\nshowed no improvement, highlighting variability in its effectiveness. LLMs\nbenefit when provided access to domain-specific data beyond their training\ndata. For radiology, RadioRAG establishes a robust framework that substantially\nimproves diagnostic accuracy and factuality in radiological question answering.",
        "pos": [
            "Matching cancer patients to clinical trials is essential for advancing\ntreatment and patient care. However, the inconsistent format of medical free\ntext documents and complex trial eligibility criteria make this process\nextremely challenging and time-consuming for physicians. We investigated\nwhether the entire trial matching process - from identifying relevant trials\namong 105,600 oncology-related clinical trials on clinicaltrials.gov to\ngenerating criterion-level eligibility matches - could be automated using Large\nLanguage Models (LLMs). Using GPT-4o and a set of 51 synthetic Electronic\nHealth Records (EHRs), we demonstrate that our approach identifies relevant\ncandidate trials in 93.3% of cases and achieves a preliminary accuracy of 88.0%\nwhen matching patient-level information at the criterion level against a\nbaseline defined by human experts. Utilizing LLM feedback reveals that 39.3%\ncriteria that were initially considered incorrect are either ambiguous or\ninaccurately annotated, leading to a total model accuracy of 92.7% after\nrefining our human baseline. In summary, we present an end-to-end pipeline for\nclinical trial matching using LLMs, demonstrating high precision in screening\nand matching trials to individual patients, even outperforming the performance\nof qualified medical doctors. Our fully end-to-end pipeline can operate\nautonomously or with human supervision and is not restricted to oncology,\noffering a scalable solution for enhancing patient-trial matching in real-world\nsettings."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) in medicine. However LLMs often generate outdated or\ninaccurate information based on static training datasets. Retrieval augmented\ngeneration (RAG) mitigates this by integrating outside data sources. While\nprevious RAG systems used pre-assembled, fixed databases with limited\nflexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end\nframework that retrieves data from authoritative radiologic online sources in\nreal-time. RadioRAG is evaluated using a dedicated radiologic\nquestion-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of\nvarious LLMs when answering radiology-specific questions with and without\naccess to additional online information via RAG. Using 80 questions from RSNA\nCase Collection across radiologic subspecialties and 24 additional\nexpert-curated questions, for which the correct gold-standard answers were\navailable, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B\nand 70B]) were prompted with and without RadioRAG. RadioRAG retrieved\ncontext-specific information from www.radiopaedia.org in real-time and\nincorporated them into its reply. RadioRAG consistently improved diagnostic\naccuracy across all LLMs, with relative improvements ranging from 2% to 54%. It\nmatched or exceeded question answering without RAG across radiologic\nsubspecialties, particularly in breast imaging and emergency radiology.\nHowever, degree of improvement varied among models; GPT-3.5-turbo and\nMixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2\nshowed no improvement, highlighting variability in its effectiveness. LLMs\nbenefit when provided access to domain-specific data beyond their training\ndata. For radiology, RadioRAG establishes a robust framework that substantially\nimproves diagnostic accuracy and factuality in radiological question answering.",
        "pos": [
            "Matching cancer patients to clinical trials is essential for advancing\ntreatment and patient care. However, the inconsistent format of medical free\ntext documents and complex trial eligibility criteria make this process\nextremely challenging and time-consuming for physicians. We investigated\nwhether the entire trial matching process - from identifying relevant trials\namong 105,600 oncology-related clinical trials on clinicaltrials.gov to\ngenerating criterion-level eligibility matches - could be automated using Large\nLanguage Models (LLMs). Using GPT-4o and a set of 51 synthetic Electronic\nHealth Records (EHRs), we demonstrate that our approach identifies relevant\ncandidate trials in 93.3% of cases and achieves a preliminary accuracy of 88.0%\nwhen matching patient-level information at the criterion level against a\nbaseline defined by human experts. Utilizing LLM feedback reveals that 39.3%\ncriteria that were initially considered incorrect are either ambiguous or\ninaccurately annotated, leading to a total model accuracy of 92.7% after\nrefining our human baseline. In summary, we present an end-to-end pipeline for\nclinical trial matching using LLMs, demonstrating high precision in screening\nand matching trials to individual patients, even outperforming the performance\nof qualified medical doctors. Our fully end-to-end pipeline can operate\nautonomously or with human supervision and is not restricted to oncology,\noffering a scalable solution for enhancing patient-trial matching in real-world\nsettings."
        ],
        "neg": []
    },
    {
        "query": "Since the launch of ChatGPT at the end of 2022, generative dialogue models\nrepresented by ChatGPT have quickly become essential tools in daily life. As\nuser expectations increase, enhancing the capability of generative dialogue\nmodels to solve complex problems has become a focal point of current research.\nThis paper delves into the effectiveness of the RAFT (Retrieval Augmented\nFine-Tuning) method in improving the performance of Generative dialogue models.\nRAFT combines chain-of-thought with model supervised fine-tuning (SFT) and\nretrieval augmented generation (RAG), which significantly enhanced the model's\ninformation extraction and logical reasoning abilities. We evaluated the RAFT\nmethod across multiple datasets and analysed its performance in various\nreasoning tasks, including long-form QA and short-form QA tasks, tasks in both\nChinese and English, and supportive and comparison reasoning tasks. Notably, it\naddresses the gaps in previous research regarding long-form QA tasks and\nChinese datasets. Moreover, we also evaluate the benefit of the\nchain-of-thought (CoT) in the RAFT method. This work offers valuable insights\nfor studies focused on enhancing the performance of generative dialogue models.",
        "pos": [
            "The mainstream automatic speech recognition (ASR) technology usually requires\nhundreds to thousands of hours of annotated speech data. Three approaches to\nlow-resourced ASR are phoneme or subword based supervised pre-training, and\nself-supervised pre-training over multilingual data. The Iu Mien language is\nthe main ethnic language of the Yao ethnic group in China and is low-resourced\nin the sense that the annotated speech is very limited. With less than 10 hours\nof transcribed Iu Mien language, this paper investigates and compares the three\napproaches for Iu Mien speech recognition. Our experiments are based on the\nrecently released, three backbone models pretrained over the 10 languages from\nthe CommonVoice dataset (CV-Lang10), which correspond to the three approaches\nfor low-resourced ASR. It is found that phoneme supervision can achieve better\nresults compared to subword supervision and self-supervision, thereby providing\nhigher data-efficiency. Particularly, the Whistle models, i.e., obtained by the\nweakly-supervised phoneme-based multilingual pre-training, obtain the most\ncompetitive results."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of `jailbreaking' techniques to elicit\nharmful text from models that were fine-tuned to be harmless. Recent work on\nred-teaming, model editing, and interpretability suggests that this challenge\nstems from how (adversarial) fine-tuning largely serves to suppress rather than\nremove undesirable capabilities from LLMs. Prior work has introduced latent\nadversarial training (LAT) as a way to improve robustness to broad classes of\nfailures. These prior works have considered untargeted latent space attacks\nwhere the adversary perturbs latent activations to maximize loss on examples of\ndesirable behavior. Untargeted LAT can provide a generic type of robustness but\ndoes not leverage information about specific failure modes. Here, we experiment\nwith targeted LAT where the adversary seeks to minimize loss on a specific\ncompeting task. We find that it can augment a wide variety of state-of-the-art\nmethods. First, we use targeted LAT to improve robustness to jailbreaks,\noutperforming a strong R2D2 baseline with orders of magnitude less compute.\nSecond, we use it to more effectively remove backdoors with no knowledge of the\ntrigger. Finally, we use it to more effectively unlearn knowledge for specific\nundesirable tasks in a way that is also more robust to re-learning. Overall,\nour results suggest that targeted LAT can be an effective tool for defending\nagainst harmful behaviors from LLMs.",
        "pos": [
            "The integration of new modalities into frontier AI systems offers exciting\ncapabilities, but also increases the possibility such systems can be\nadversarially manipulated in undesirable ways. In this work, we focus on a\npopular class of vision-language models (VLMs) that generate text outputs\nconditioned on visual and textual inputs. We conducted a large-scale empirical\nstudy to assess the transferability of gradient-based universal image\n\"jailbreaks\" using a diverse set of over 40 open-parameter VLMs, including 18\nnew VLMs that we publicly release. Overall, we find that transferable\ngradient-based image jailbreaks are extremely difficult to obtain. When an\nimage jailbreak is optimized against a single VLM or against an ensemble of\nVLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits\nlittle-to-no transfer to any other VLMs; transfer is not affected by whether\nthe attacked and target VLMs possess matching vision backbones or language\nmodels, whether the language model underwent instruction-following and/or\nsafety-alignment training, or many other factors. Only two settings display\npartially successful transfer: between identically-pretrained and\nidentically-initialized VLMs with slightly different VLM training data, and\nbetween different training checkpoints of a single VLM. Leveraging these\nresults, we then demonstrate that transfer can be significantly improved\nagainst a specific target VLM by attacking larger ensembles of \"highly-similar\"\nVLMs. These results stand in stark contrast to existing evidence of universal\nand transferable text jailbreaks against language models and transferable\nadversarial attacks against image classifiers, suggesting that VLMs may be more\nrobust to gradient-based transfer attacks."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of `jailbreaking' techniques to elicit\nharmful text from models that were fine-tuned to be harmless. Recent work on\nred-teaming, model editing, and interpretability suggests that this challenge\nstems from how (adversarial) fine-tuning largely serves to suppress rather than\nremove undesirable capabilities from LLMs. Prior work has introduced latent\nadversarial training (LAT) as a way to improve robustness to broad classes of\nfailures. These prior works have considered untargeted latent space attacks\nwhere the adversary perturbs latent activations to maximize loss on examples of\ndesirable behavior. Untargeted LAT can provide a generic type of robustness but\ndoes not leverage information about specific failure modes. Here, we experiment\nwith targeted LAT where the adversary seeks to minimize loss on a specific\ncompeting task. We find that it can augment a wide variety of state-of-the-art\nmethods. First, we use targeted LAT to improve robustness to jailbreaks,\noutperforming a strong R2D2 baseline with orders of magnitude less compute.\nSecond, we use it to more effectively remove backdoors with no knowledge of the\ntrigger. Finally, we use it to more effectively unlearn knowledge for specific\nundesirable tasks in a way that is also more robust to re-learning. Overall,\nour results suggest that targeted LAT can be an effective tool for defending\nagainst harmful behaviors from LLMs.",
        "pos": [
            "The integration of new modalities into frontier AI systems offers exciting\ncapabilities, but also increases the possibility such systems can be\nadversarially manipulated in undesirable ways. In this work, we focus on a\npopular class of vision-language models (VLMs) that generate text outputs\nconditioned on visual and textual inputs. We conducted a large-scale empirical\nstudy to assess the transferability of gradient-based universal image\n\"jailbreaks\" using a diverse set of over 40 open-parameter VLMs, including 18\nnew VLMs that we publicly release. Overall, we find that transferable\ngradient-based image jailbreaks are extremely difficult to obtain. When an\nimage jailbreak is optimized against a single VLM or against an ensemble of\nVLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits\nlittle-to-no transfer to any other VLMs; transfer is not affected by whether\nthe attacked and target VLMs possess matching vision backbones or language\nmodels, whether the language model underwent instruction-following and/or\nsafety-alignment training, or many other factors. Only two settings display\npartially successful transfer: between identically-pretrained and\nidentically-initialized VLMs with slightly different VLM training data, and\nbetween different training checkpoints of a single VLM. Leveraging these\nresults, we then demonstrate that transfer can be significantly improved\nagainst a specific target VLM by attacking larger ensembles of \"highly-similar\"\nVLMs. These results stand in stark contrast to existing evidence of universal\nand transferable text jailbreaks against language models and transferable\nadversarial attacks against image classifiers, suggesting that VLMs may be more\nrobust to gradient-based transfer attacks."
        ],
        "neg": []
    },
    {
        "query": "Recent studies have increasingly applied natural language processing (NLP) to\nautomatically extract experimental research data from the extensive battery\nmaterials literature. Despite the complex process involved in battery\nmanufacturing -- from material synthesis to cell assembly -- there has been no\ncomprehensive study systematically organizing this information. In response, we\npropose a language modeling-based protocol, Text-to-Battery Recipe (T2BR), for\nthe automatic extraction of end-to-end battery recipes, validated using a case\nstudy on batteries containing LiFePO4 cathode material. We report machine\nlearning-based paper filtering models, screening 2,174 relevant papers from the\nkeyword-based search results, and unsupervised topic models to identify 2,876\nparagraphs related to cathode synthesis and 2,958 paragraphs related to cell\nassembly. Then, focusing on the two topics, two deep learning-based named\nentity recognition models are developed to extract a total of 30 entities --\nincluding precursors, active materials, and synthesis methods -- achieving F1\nscores of 88.18% and 94.61%. The accurate extraction of entities enables the\nsystematic generation of 165 end-toend recipes of LiFePO4 batteries. Our\nprotocol and results offer valuable insights into specific trends, such as\nassociations between precursor materials and synthesis methods, or combinations\nbetween different precursor materials. We anticipate that our findings will\nserve as a foundational knowledge base for facilitating battery-recipe\ninformation retrieval. The proposed protocol will significantly accelerate the\nreview of battery material literature and catalyze innovations in battery\ndesign and development.",
        "pos": [
            "The rapid advancement of Large Language Models (LLMs) and Large Multimodal\nModels (LMMs) has heightened the demand for AI-based scientific assistants\ncapable of understanding scientific articles and figures. Despite progress,\nthere remains a significant gap in evaluating models' comprehension of\nprofessional, graduate-level, and even PhD-level scientific content. Current\ndatasets and benchmarks primarily focus on relatively simple scientific tasks\nand figures, lacking comprehensive assessments across diverse advanced\nscientific disciplines. To bridge this gap, we collected a multimodal,\nmultidisciplinary dataset from open-access scientific articles published in\nNature Communications journals. This dataset spans 72 scientific disciplines,\nensuring both diversity and quality. We created benchmarks with various tasks\nand settings to comprehensively evaluate LMMs' capabilities in understanding\nscientific figures and content. Our evaluation revealed that these tasks are\nhighly challenging: many open-source models struggled significantly, and even\nGPT-4V and GPT-4o faced difficulties. We also explored using our dataset as\ntraining resources by constructing visual instruction-following data, enabling\nthe 7B LLaVA model to achieve performance comparable to GPT-4V/o on our\nbenchmark. Additionally, we investigated the use of our interleaved article\ntexts and figure images for pre-training LMMs, resulting in improvements on the\nmaterial generation task. The source dataset, including articles, figures,\nconstructed benchmarks, and visual instruction-following data, is open-sourced."
        ],
        "neg": []
    },
    {
        "query": "We introduces LLaST, a framework for building high-performance Large Language\nmodel based Speech-to-text Translation systems. We address the limitations of\nend-to-end speech translation(E2E ST) models by exploring model architecture\ndesign and optimization techniques tailored for LLMs. Our approach includes\nLLM-based speech translation architecture design, ASR-augmented training,\nmultilingual data augmentation, and dual-LoRA optimization. Our approach\ndemonstrates superior performance on the CoVoST-2 benchmark and showcases\nexceptional scaling capabilities powered by LLMs. We believe this effective\nmethod will serve as a strong baseline for speech translation and provide\ninsights for future improvements of the LLM-based speech translation framework.\nWe release the data, code and models in https://github.com/openaudiolab/LLaST.",
        "pos": [
            "Text relevance or text matching of query and product is an essential\ntechnique for the e-commerce search system to ensure that the displayed\nproducts can match the intent of the query. Many studies focus on improving the\nperformance of the relevance model in search system. Recently, pre-trained\nlanguage models like BERT have achieved promising performance on the text\nrelevance task. While these models perform well on the offline test dataset,\nthere are still obstacles to deploy the pre-trained language model to the\nonline system as their high latency. The two-tower model is extensively\nemployed in industrial scenarios, owing to its ability to harmonize performance\nwith computational efficiency. Regrettably, such models present an opaque\n``black box'' nature, which prevents developers from making special\noptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an\nefficient and interpretable relevance architecture for Chinese e-commerce. Our\napproach proposes to encode the query and the product into the sparse BoW\nrepresentation, which is a set of word-weight pairs. The weight means the\nimportant or the relevant score between the corresponding word and the raw\ntext. The relevance score is measured by the accumulation of the matched word\nbetween the sparse BoW representation of the query and the product. Compared to\npopular dense distributed representation that usually suffers from the drawback\nof black-box, the most advantage of the proposed representation model is highly\nexplainable and interventionable, which is a superior advantage to the\ndeployment and operation of online search engines. Moreover, the online\nefficiency of the proposed model is even better than the most efficient inner\nproduct form of dense representation ...",
            "PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation."
        ],
        "neg": []
    },
    {
        "query": "We introduces LLaST, a framework for building high-performance Large Language\nmodel based Speech-to-text Translation systems. We address the limitations of\nend-to-end speech translation(E2E ST) models by exploring model architecture\ndesign and optimization techniques tailored for LLMs. Our approach includes\nLLM-based speech translation architecture design, ASR-augmented training,\nmultilingual data augmentation, and dual-LoRA optimization. Our approach\ndemonstrates superior performance on the CoVoST-2 benchmark and showcases\nexceptional scaling capabilities powered by LLMs. We believe this effective\nmethod will serve as a strong baseline for speech translation and provide\ninsights for future improvements of the LLM-based speech translation framework.\nWe release the data, code and models in https://github.com/openaudiolab/LLaST.",
        "pos": [
            "In evaluating the long-context capabilities of large language models (LLMs),\nidentifying content relevant to a user's query from original long documents is\na crucial prerequisite for any LLM to answer questions based on long text. We\npresent NeedleBench, a framework consisting of a series of progressively more\nchallenging tasks for assessing bilingual long-context capabilities, spanning\nmultiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and\ndifferent depth ranges, allowing the strategic insertion of critical data\npoints in different text depth zones to rigorously test the retrieval and\nreasoning capabilities of models in diverse contexts. We use the NeedleBench\nframework to assess how well the leading open-source models can identify key\ninformation relevant to the question and apply that information to reasoning in\nbilingual long texts. Furthermore, we propose the Ancestral Trace Challenge\n(ATC) to mimic the complexity of logical reasoning challenges that are likely\nto be present in real-world long-context tasks, providing a simple method for\nevaluating LLMs in dealing with complex long-context situations. Our results\nsuggest that current LLMs have significant room for improvement in practical\nlong-context applications, as they struggle with the complexity of logical\nreasoning challenges that are likely to be present in real-world long-context\ntasks. All codes and resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass.",
            "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.",
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer."
        ],
        "neg": []
    },
    {
        "query": "We introduces LLaST, a framework for building high-performance Large Language\nmodel based Speech-to-text Translation systems. We address the limitations of\nend-to-end speech translation(E2E ST) models by exploring model architecture\ndesign and optimization techniques tailored for LLMs. Our approach includes\nLLM-based speech translation architecture design, ASR-augmented training,\nmultilingual data augmentation, and dual-LoRA optimization. Our approach\ndemonstrates superior performance on the CoVoST-2 benchmark and showcases\nexceptional scaling capabilities powered by LLMs. We believe this effective\nmethod will serve as a strong baseline for speech translation and provide\ninsights for future improvements of the LLM-based speech translation framework.\nWe release the data, code and models in https://github.com/openaudiolab/LLaST.",
        "pos": [
            "Simultaneous interpretation (SI), the translation of one language to another\nin real time, starts translation before the original speech has finished. Its\nevaluation needs to consider both latency and quality. This trade-off is\nchallenging especially for distant word order language pairs such as English\nand Japanese. To handle this word order gap, interpreters maintain the word\norder of the source language as much as possible to keep up with original\nlanguage to minimize its latency while maintaining its quality, whereas in\ntranslation reordering happens to keep fluency in the target language. This\nmeans outputs synchronized with the source language are desirable based on the\nreal SI situation, and it's a key for further progress in computational SI and\nsimultaneous machine translation (SiMT). In this work, we propose an automatic\nevaluation metric for SI and SiMT focusing on word order synchronization. Our\nevaluation metric is based on rank correlation coefficients, leveraging\ncross-lingual pre-trained language models. Our experimental results on\nNAIST-SIC-Aligned and JNPC showed our metrics' effectiveness to measure word\norder synchronization between source and target language."
        ],
        "neg": []
    },
    {
        "query": "We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.",
        "pos": [
            "Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust."
        ],
        "neg": []
    },
    {
        "query": "We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.",
        "pos": [
            "Recent advancements in artificial intelligence (AI) have precipitated\nsignificant breakthroughs in healthcare, particularly in refining diagnostic\nprocedures. However, previous studies have often been constrained to limited\nfunctionalities. This study introduces MiniGPT-Med, a vision-language model\nderived from large-scale language models and tailored for medical applications.\nMiniGPT-Med demonstrates remarkable versatility across various imaging\nmodalities, including X-rays, CT scans, and MRIs, enhancing its utility. The\nmodel is capable of performing tasks such as medical report generation, visual\nquestion answering (VQA), and disease identification within medical imagery.\nIts integrated processing of both image and textual clinical data markedly\nimproves diagnostic accuracy. Our empirical assessments confirm MiniGPT-Med's\nsuperior performance in disease grounding, medical report generation, and VQA\nbenchmarks, representing a significant step towards reducing the gap in\nassisting radiology practice. Furthermore, it achieves state-of-the-art\nperformance on medical report generation, higher than the previous best model\nby 19\\% accuracy. MiniGPT-Med promises to become a general interface for\nradiology diagnoses, enhancing diagnostic efficiency across a wide range of\nmedical imaging applications."
        ],
        "neg": []
    },
    {
        "query": "We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.",
        "pos": [
            "Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust."
        ],
        "neg": []
    },
    {
        "query": "Pronunciation dictionaries are an important component in the process of\nspeech forced alignment. The accuracy of these dictionaries has a strong effect\non the aligned speech data since they help the mapping between orthographic\ntranscriptions and acoustic signals. In this paper, I present the creation of a\ncomprehensive pronunciation dictionary in Spanish (ESPADA) that can be used in\nmost of the dialect variants of Spanish data. Current dictionaries focus on\nspecific regional variants, but with the flexible nature of our tool, it can be\nreadily applied to capture the most common phonetic differences across major\ndialectal variants. We propose improvements to current pronunciation\ndictionaries as well as mapping other relevant annotations such as\nmorphological and lexical information. In terms of size, it is currently the\nmost complete dictionary with more than 628,000 entries, representing words\nfrom 16 countries. All entries come with their corresponding pronunciations,\nmorphological and lexical tagging, and other relevant information for phonetic\nanalysis: stress patterns, phonotactics, IPA transcriptions, and more. This\naims to equip socio-phonetic researchers with a complete open-source tool that\nenhances dialectal research within socio-phonetic frameworks in the Spanish\nlanguage.",
        "pos": [
            "Social Media platforms have offered invaluable opportunities for linguistic\nresearch. The availability of up-to-date data, coming from any part in the\nworld, and coming from natural contexts, has allowed researchers to study\nlanguage in real time. One of the fields that has made great use of social\nmedia platforms is Corpus Linguistics. There is currently a wide range of\nprojects which have been able to successfully create corpora from social media.\nIn this paper, we present the development and deployment of a linguistic corpus\nfrom Twitter posts in English, coming from 26 news agencies and 27 individuals.\nThe main goal was to create a fully annotated English corpus for linguistic\nanalysis. We include information on morphology and syntax, as well as NLP\nfeatures such as tokenization, lemmas, and n- grams. The information is\npresented through a range of powerful visualisations for users to explore\nlinguistic patterns in the corpus. With this tool, we aim to contribute to the\narea of language technologies applied to linguistic research.",
            "The field of conlang has evidenced an important growth in the last decades.\nThis has been the product of a wide interest in the use and study of conlangs\nfor artistic purposes. However, one important question is what it is happening\nwith conlang in the academic world. This paper aims to have an overall\nunderstanding of the literature on conlang research. With this we aim to give a\nrealistic picture of the field in present days. We have implemented a\ncomputational linguistic approach, combining bibliometrics and network analysis\nto examine all publications available in the Scopus database. Analysing over\n2300 academic publications since 1927 until 2022, we have found that Esperanto\nis by far the most documented conlang. Three main authors have contributed to\nthis: Garv\\'ia R., Fiedler S., and Blanke D. The 1970s and 1980s have been the\ndecades where the foundations of current research have been built. In terms of\nmethodologies, language learning and experimental linguistics are the ones\ncontributing to most to the preferred approaches of study in the field. We\npresent the results and discuss our limitations and future work."
        ],
        "neg": []
    },
    {
        "query": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
        "pos": [
            "Multimodal Large Language Models (MLLM) have made significant progress in the\nfield of document analysis. Despite this, existing benchmarks typically focus\nonly on extracting text and simple layout information, neglecting the complex\ninteractions between elements in structured documents such as mind maps and\nflowcharts. To address this issue, we introduce the new benchmark named\nMindBench, which not only includes meticulously constructed bilingual authentic\nor synthetic images, detailed annotations, evaluation metrics and baseline\nmodels, but also specifically designs five types of structured understanding\nand parsing tasks. These tasks include full parsing, partial parsing,\nposition-related parsing, structured Visual Question Answering (VQA), and\nposition-related VQA, covering key areas such as text recognition, spatial\nawareness, relationship discernment, and structured parsing. Extensive\nexperimental results demonstrate the substantial potential and significant room\nfor improvement in current models' ability to handle structured document\ninformation. We anticipate that the launch of MindBench will significantly\nadvance research and application development in structured document analysis\ntechnology. MindBench is available at:\nhttps://miasanlei.github.io/MindBench.github.io/."
        ],
        "neg": []
    },
    {
        "query": "The key challenge of cross-modal domain-incremental learning (DIL) is to\nenable the learning model to continuously learn from novel data with different\nfeature distributions under the same task without forgetting old ones. However,\nexisting top-performing methods still cause high forgetting rates, by lacking\nintra-domain knowledge extraction and inter-domain common prompting strategy.\nIn this paper, we propose a simple yet effective framework, CP-Prompt, by\ntraining limited parameters to instruct a pre-trained model to learn new\ndomains and avoid forgetting existing feature distributions. CP-Prompt captures\nintra-domain knowledge by compositionally inserting personalized prompts on\nmulti-head self-attention layers and then learns the inter-domain knowledge\nwith a common prompting strategy. CP-Prompt shows superiority compared with\nstate-of-the-art baselines among three widely evaluated DIL tasks. The source\ncode is available at https://github.com/dannis97500/CP_Prompt.",
        "pos": [
            "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset using LLMs and jailbreaking prompt attacks. Based on our evaluation\nresults, we draw several important findings, including: 1) no single model\nexcels in all aspects, with different models showing various strengths; 2) the\ncorrelation between GPT-4 assessments and manual reviews is generally high; 3)\nthere is a trade-off between the usability and safety of text-to-video\ngenerative models. This indicates that as the field of video generation rapidly\nadvances, safety risks are set to surge, highlighting the urgency of\nprioritizing video safety. We hope that T2VSafetyBench can provide insights for\nbetter understanding the safety of video generation in the era of generative\nAI."
        ],
        "neg": []
    },
    {
        "query": "Event Factuality Detection (EFD) task determines the factuality of textual\nevents, i.e., classifying whether an event is a fact, possibility, or\nimpossibility, which is essential for faithfully understanding and utilizing\nevent knowledge. However, due to the lack of high-quality large-scale data,\nevent factuality detection is under-explored in event understanding research,\nwhich limits the development of EFD community. To address these issues and\nprovide faithful event understanding, we introduce MAVEN-Fact, a large-scale\nand high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes\nfactuality annotations of 112,276 events, making it the largest EFD dataset.\nExtensive experiments demonstrate that MAVEN-Fact is challenging for both\nconventional fine-tuned models and large language models (LLMs). Thanks to the\ncomprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact\nalso supports some further analyses and we find that adopting event arguments\nand relations helps in event factuality detection for fine-tuned models but\ndoes not benefit LLMs. Furthermore, we preliminarily study an application case\nof event factuality detection and find it helps in mitigating event-related\nhallucination in LLMs. Our dataset and codes can be obtained from\n\\url{https://github.com/lcy2723/MAVEN-FACT}",
        "pos": [
            "Entity Linking (EL) models are well-trained at mapping mentions to their\ncorresponding entities according to a given context. However, EL models\nstruggle to disambiguate long-tail entities due to their limited training data.\nMeanwhile, large language models (LLMs) are more robust at interpreting\nuncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at\ngenerating correct entity IDs. Furthermore, training an LLM to perform EL is\ncost-intensive. Building upon these insights, we introduce LLM-Augmented Entity\nLinking LLMAEL, a plug-and-play approach to enhance entity linking through LLM\ndata augmentation. We leverage LLMs as knowledgeable context augmenters,\ngenerating mention-centered descriptions as additional input, while preserving\ntraditional EL models for task specific processing. Experiments on 6 standard\ndatasets show that the vanilla LLMAEL outperforms baseline EL models in most\ncases, while the fine-tuned LLMAEL set the new state-of-the-art results across\nall 6 benchmarks."
        ],
        "neg": []
    },
    {
        "query": "Multimodal foundation models hold significant potential for automating\nradiology report generation, thereby assisting clinicians in diagnosing cardiac\ndiseases. However, generated reports often suffer from serious factual\ninaccuracy. In this paper, we introduce a fact-aware multimodal\nretrieval-augmented pipeline in generating accurate radiology reports\n(FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then\nintegrate factual knowledge to train a universal multimodal retriever. Given a\nradiology image, our retriever can identify high-quality reference reports to\naugment multimodal foundation models, thus enhancing the factual completeness\nand correctness of report generation. Experiments on two benchmark datasets\nshow that our multimodal retriever outperforms state-of-the-art retrievers on\nboth language generation and radiology-specific metrics, up to 6.5% and 2%\nscore in F1CheXbert and F1RadGraph. Further analysis indicates that employing\nour factually-informed training strategy imposes an effective supervision\nsignal, without relying on explicit diagnostic label guidance, and successfully\npropagates fact-aware capabilities from the multimodal retriever to the\nmultimodal foundation model in radiology report generation.",
        "pos": [
            "Data valuation quantifies the value of training data, and is used for data\nattribution (i.e., determining the contribution of training data towards model\npredictions), and data selection; both of which are important for curating\nhigh-quality datasets to train large language models. In our paper, we show\nthat data valuation through in-context probing (i.e., prompting a LLM)\napproximates influence functions for selecting training data. We provide a\ntheoretical sketch on this connection based on transformer models performing\n\"implicit\" gradient descent on its in-context inputs. Our empirical findings\nshow that in-context probing and gradient-based influence frameworks are\nsimilar in how they rank training data. Furthermore, fine-tuning experiments on\ndata selected by either method reveal similar model performance."
        ],
        "neg": []
    },
    {
        "query": "In this survey, we address the key challenges in Large Language Models (LLM)\nresearch, focusing on the importance of interpretability. Driven by increasing\ninterest from AI and business sectors, we highlight the need for transparency\nin LLMs. We examine the dual paths in current LLM research and eXplainable\nArtificial Intelligence (XAI): enhancing performance through XAI and the\nemerging focus on model interpretability. Our paper advocates for a balanced\napproach that values interpretability equally with functional advancements.\nRecognizing the rapid development in LLM research, our survey includes both\npeer-reviewed and preprint (arXiv) papers, offering a comprehensive overview of\nXAI's role in LLM research. We conclude by urging the research community to\nadvance both LLM and XAI fields together.",
        "pos": [
            "Key Point Analysis (KPA) aims for quantitative summarization that provides\nkey points (KPs) as succinct textual summaries and quantities measuring their\nprevalence. KPA studies for arguments and reviews have been reported in the\nliterature. A majority of KPA studies for reviews adopt supervised learning to\nextract short sentences as KPs before matching KPs to review comments for\nquantification of KP prevalence. Recent abstractive approaches still generate\nKPs based on sentences, often leading to KPs with overlapping and hallucinated\nopinions, and inaccurate quantification. In this paper, we propose Prompted\nAspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA\nemploys aspect sentiment analysis and prompted in-context learning with Large\nLanguage Models (LLMs) to generate and quantify KPs grounded in aspects for\nbusiness entities, which achieves faithful KPs with accurate quantification,\nand removes the need for large amounts of annotated data for supervised\ntraining. Experiments on the popular review dataset Yelp and the\naspect-oriented review summarization dataset SPACE show that our framework\nachieves state-of-the-art performance. Source code and data are available at:\nhttps://github.com/antangrocket1312/PAKPA",
            "Sustainability commonly refers to entities, such as individuals, companies,\nand institutions, having a non-detrimental (or even positive) impact on the\nenvironment, society, and the economy. With sustainability becoming a synonym\nof acceptable and legitimate behaviour, it is being increasingly demanded and\nregulated. Several frameworks and standards have been proposed to measure the\nsustainability impact of corporations, including United Nations' sustainable\ndevelopment goals and the recently introduced global sustainability reporting\nframework, amongst others. However, the concept of corporate sustainability is\ncomplex due to the diverse and intricate nature of firm operations (i.e.\ngeography, size, business activities, interlinks with other stakeholders). As a\nresult, corporate sustainability assessments are plagued by subjectivity both\nwithin data that reflect corporate sustainability efforts (i.e. corporate\nsustainability disclosures) and the analysts evaluating them. This subjectivity\ncan be distilled into distinct challenges, such as incompleteness, ambiguity,\nunreliability and sophistication on the data dimension, as well as limited\nresources and potential bias on the analyst dimension. Put together,\nsubjectivity hinders effective cost attribution to entities non-compliant with\nprevailing sustainability expectations, potentially rendering sustainability\nefforts and its associated regulations futile. To this end, we argue that\nExplainable Natural Language Processing (XNLP) can significantly enhance\ncorporate sustainability analysis. Specifically, linguistic understanding\nalgorithms (lexical, semantic, syntactic), integrated with XAI capabilities\n(interpretability, explainability, faithfulness), can bridge gaps in analyst\nresources and mitigate subjectivity problems within data."
        ],
        "neg": []
    },
    {
        "query": "Instruction tuning has achieved unprecedented success in NLP, turning large\nlanguage models into versatile chatbots. However, the increasing variety and\nvolume of instruction datasets demand significant computational resources. To\naddress this, it is essential to extract a small and highly informative subset\n(i.e., Coreset) that achieves comparable performance to the full dataset.\nAchieving this goal poses non-trivial challenges: 1) data selection requires\naccurate data representations that reflect the training samples' quality, 2)\nconsidering the diverse nature of instruction datasets, and 3) ensuring the\nefficiency of the coreset selection algorithm for large models. To address\nthese challenges, we propose Task-Agnostic Gradient Clustered COreset Selection\n(TAGCOS). Specifically, we leverage sample gradients as the data\nrepresentations, perform clustering to group similar data, and apply an\nefficient greedy algorithm for coreset selection. Experimental results show\nthat our algorithm, selecting only 5% of the data, surpasses other unsupervised\nmethods and achieves performance close to that of the full dataset.",
        "pos": [
            "We present a comprehensive evaluation of proprietary and open-weights large\nlanguage models using the first astronomy-specific benchmarking dataset. This\ndataset comprises 4,425 multiple-choice questions curated from the Annual\nReview of Astronomy and Astrophysics, covering a broad range of astrophysical\ntopics. Our analysis examines model performance across various astronomical\nsubfields and assesses response calibration, crucial for potential deployment\nin research environments. Claude-3.5-Sonnet outperforms competitors by up to\n4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we\nobserved a universal reduction in cost every 3-to-12 months to achieve similar\nscore in this particular astronomy benchmark. Open-source models have rapidly\nimproved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with\nsome of the best proprietary models. We identify performance variations across\ntopics, with non-English-focused models generally struggling more in\nexoplanet-related fields, stellar astrophysics, and instrumentation related\nquestions. These challenges likely stem from less abundant training data,\nlimited historical context, and rapid recent developments in these areas. This\npattern is observed across both open-weights and proprietary models, with\nregional dependencies evident, highlighting the impact of training data\ndiversity on model performance in specialized scientific domains.\nTop-performing models demonstrate well-calibrated confidence, with correlations\nabove 0.9 between confidence and correctness, though they tend to be slightly\nunderconfident. The development for fast, low-cost inference of open-weights\nmodels presents new opportunities for affordable deployment in astronomy. The\nrapid progress observed suggests that LLM-driven research in astronomy may\nbecome feasible in the near future."
        ],
        "neg": []
    },
    {
        "query": "Violence-provoking speech -- speech that implicitly or explicitly promotes\nviolence against the members of the targeted community, contributed to a\nmassive surge in anti-Asian crimes during the pandemic. While previous works\nhave characterized and built tools for detecting other forms of harmful speech,\nlike fear speech and hate speech, our work takes a community-centric approach\nto studying anti-Asian violence-provoking speech. Using data from ~420k Twitter\nposts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we\ndevelop a codebook to characterize anti-Asian violence-provoking speech and\ncollect a community-crowdsourced dataset to facilitate its large-scale\ndetection using state-of-the-art classifiers. We contrast the capabilities of\nnatural language processing classifiers, ranging from BERT-based to LLM-based\nclassifiers, in detecting violence-provoking speech with their capabilities to\ndetect anti-Asian hateful speech. In contrast to prior work that has\ndemonstrated the effectiveness of such classifiers in detecting hateful speech\n($F_1 = 0.89$), our work shows that accurate and reliable detection of\nviolence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the\nimplications of our findings, particularly the need for proactive interventions\nto support Asian communities during public health crises. The resources related\nto the study are available at\nhttps://claws-lab.github.io/violence-provoking-speech/.",
        "pos": [
            "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench."
        ],
        "neg": []
    },
    {
        "query": "Violence-provoking speech -- speech that implicitly or explicitly promotes\nviolence against the members of the targeted community, contributed to a\nmassive surge in anti-Asian crimes during the pandemic. While previous works\nhave characterized and built tools for detecting other forms of harmful speech,\nlike fear speech and hate speech, our work takes a community-centric approach\nto studying anti-Asian violence-provoking speech. Using data from ~420k Twitter\nposts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we\ndevelop a codebook to characterize anti-Asian violence-provoking speech and\ncollect a community-crowdsourced dataset to facilitate its large-scale\ndetection using state-of-the-art classifiers. We contrast the capabilities of\nnatural language processing classifiers, ranging from BERT-based to LLM-based\nclassifiers, in detecting violence-provoking speech with their capabilities to\ndetect anti-Asian hateful speech. In contrast to prior work that has\ndemonstrated the effectiveness of such classifiers in detecting hateful speech\n($F_1 = 0.89$), our work shows that accurate and reliable detection of\nviolence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the\nimplications of our findings, particularly the need for proactive interventions\nto support Asian communities during public health crises. The resources related\nto the study are available at\nhttps://claws-lab.github.io/violence-provoking-speech/.",
        "pos": [
            "Over one in five adults in the US lives with a mental illness. In the face of\na shortage of mental health professionals and offline resources, online\nshort-form video content has grown to serve as a crucial conduit for\ndisseminating mental health help and resources. However, the ease of content\ncreation and access also contributes to the spread of misinformation, posing\nrisks to accurate diagnosis and treatment. Detecting and understanding\nengagement with such content is crucial to mitigating their harmful effects on\npublic health. We perform the first quantitative study of the phenomenon using\nYouTube Shorts and Bitchute as the sites of study. We contribute MentalMisinfo,\na novel labeled mental health misinformation (MHMisinfo) dataset of 739 videos\n(639 from Youtube and 100 from Bitchute) and 135372 comments in total, using an\nexpert-driven annotation schema. We first found that few-shot in-context\nlearning with large language models (LLMs) are effective in detecting MHMisinfo\nvideos. Next, we discover distinct and potentially alarming linguistic patterns\nin how audiences engage with MHMisinfo videos through commentary on both\nvideo-sharing platforms. Across the two platforms, comments could exacerbate\nprevailing stigma with some groups showing heightened susceptibility to and\nalignment with MHMisinfo. We discuss technical and public health-driven\nadaptive solutions to tackling the \"epidemic\" of mental health misinformation\nonline."
        ],
        "neg": []
    },
    {
        "query": "Violence-provoking speech -- speech that implicitly or explicitly promotes\nviolence against the members of the targeted community, contributed to a\nmassive surge in anti-Asian crimes during the pandemic. While previous works\nhave characterized and built tools for detecting other forms of harmful speech,\nlike fear speech and hate speech, our work takes a community-centric approach\nto studying anti-Asian violence-provoking speech. Using data from ~420k Twitter\nposts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we\ndevelop a codebook to characterize anti-Asian violence-provoking speech and\ncollect a community-crowdsourced dataset to facilitate its large-scale\ndetection using state-of-the-art classifiers. We contrast the capabilities of\nnatural language processing classifiers, ranging from BERT-based to LLM-based\nclassifiers, in detecting violence-provoking speech with their capabilities to\ndetect anti-Asian hateful speech. In contrast to prior work that has\ndemonstrated the effectiveness of such classifiers in detecting hateful speech\n($F_1 = 0.89$), our work shows that accurate and reliable detection of\nviolence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the\nimplications of our findings, particularly the need for proactive interventions\nto support Asian communities during public health crises. The resources related\nto the study are available at\nhttps://claws-lab.github.io/violence-provoking-speech/.",
        "pos": [
            "Over one in five adults in the US lives with a mental illness. In the face of\na shortage of mental health professionals and offline resources, online\nshort-form video content has grown to serve as a crucial conduit for\ndisseminating mental health help and resources. However, the ease of content\ncreation and access also contributes to the spread of misinformation, posing\nrisks to accurate diagnosis and treatment. Detecting and understanding\nengagement with such content is crucial to mitigating their harmful effects on\npublic health. We perform the first quantitative study of the phenomenon using\nYouTube Shorts and Bitchute as the sites of study. We contribute MentalMisinfo,\na novel labeled mental health misinformation (MHMisinfo) dataset of 739 videos\n(639 from Youtube and 100 from Bitchute) and 135372 comments in total, using an\nexpert-driven annotation schema. We first found that few-shot in-context\nlearning with large language models (LLMs) are effective in detecting MHMisinfo\nvideos. Next, we discover distinct and potentially alarming linguistic patterns\nin how audiences engage with MHMisinfo videos through commentary on both\nvideo-sharing platforms. Across the two platforms, comments could exacerbate\nprevailing stigma with some groups showing heightened susceptibility to and\nalignment with MHMisinfo. We discuss technical and public health-driven\nadaptive solutions to tackling the \"epidemic\" of mental health misinformation\nonline."
        ],
        "neg": []
    },
    {
        "query": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
        "pos": [
            "Complex reasoning is an impressive ability shown by large language models\n(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought\nprompting or iterative tool-using to solve challenging tasks step-by-step. In\nthis paper, we hope to focus on evaluating and teaching LLMs to conduct\ninductive reasoning, that is, LLMs are supposed to infer underlying rules by\nobserving examples or sequential transformations. However, collecting\nlarge-scale and diverse human-generated inductive data is challenging. We focus\non data synthesis in the code domain and propose a \\textbf{Case2Code} task by\nexploiting the expressiveness and correctness of programs. Specifically, we\ncollect a diverse set of executable programs, synthesize input-output\ntransformations for each program, and force LLMs to infer the underlying code\nimplementations based on the synthetic I/O cases. We first evaluate\nrepresentative LLMs on the synthesized Case2Code task and demonstrate that the\nCase-to-code induction is challenging for LLMs. Then, we synthesize large-scale\nCase2Code training samples to train LLMs to perform inductive reasoning.\nExperimental results show that such induction training benefits not only in\ndistribution Case2Code performance but also enhances various coding abilities\nof trained LLMs, demonstrating the great potential of learning inductive\nreasoning via synthetic data.",
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer."
        ],
        "neg": []
    },
    {
        "query": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
        "pos": [
            "Complex reasoning is an impressive ability shown by large language models\n(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought\nprompting or iterative tool-using to solve challenging tasks step-by-step. In\nthis paper, we hope to focus on evaluating and teaching LLMs to conduct\ninductive reasoning, that is, LLMs are supposed to infer underlying rules by\nobserving examples or sequential transformations. However, collecting\nlarge-scale and diverse human-generated inductive data is challenging. We focus\non data synthesis in the code domain and propose a \\textbf{Case2Code} task by\nexploiting the expressiveness and correctness of programs. Specifically, we\ncollect a diverse set of executable programs, synthesize input-output\ntransformations for each program, and force LLMs to infer the underlying code\nimplementations based on the synthetic I/O cases. We first evaluate\nrepresentative LLMs on the synthesized Case2Code task and demonstrate that the\nCase-to-code induction is challenging for LLMs. Then, we synthesize large-scale\nCase2Code training samples to train LLMs to perform inductive reasoning.\nExperimental results show that such induction training benefits not only in\ndistribution Case2Code performance but also enhances various coding abilities\nof trained LLMs, demonstrating the great potential of learning inductive\nreasoning via synthetic data.",
            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer."
        ],
        "neg": []
    },
    {
        "query": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
        "pos": [
            "Video Question Answering (VideoQA) has emerged as a challenging frontier in\nthe field of multimedia processing, requiring intricate interactions between\nvisual and textual modalities. Simply uniformly sampling frames or\nindiscriminately aggregating frame-level visual features often falls short in\ncapturing the nuanced and relevant contexts of videos to well perform VideoQA.\nTo mitigate these issues, we propose VidF4, a novel VideoQA framework equipped\nwith tailored frame selection strategy for effective and efficient VideoQA. We\npropose three frame-scoring mechanisms that consider both question relevance\nand inter-frame similarity to evaluate the importance of each frame for a given\nquestion on the video. Furthermore, we design a differentiable adaptive frame\nsampling mechanism to facilitate end-to-end training for the frame selector and\nanswer generator. The experimental results across three widely adopted\nbenchmarks demonstrate that our model consistently outperforms existing VideoQA\nmethods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA\n(+1.0%). Furthermore, through both quantitative and qualitative analyses, we\nvalidate the effectiveness of each design choice."
        ],
        "neg": []
    },
    {
        "query": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
        "pos": [
            "Complex reasoning is an impressive ability shown by large language models\n(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought\nprompting or iterative tool-using to solve challenging tasks step-by-step. In\nthis paper, we hope to focus on evaluating and teaching LLMs to conduct\ninductive reasoning, that is, LLMs are supposed to infer underlying rules by\nobserving examples or sequential transformations. However, collecting\nlarge-scale and diverse human-generated inductive data is challenging. We focus\non data synthesis in the code domain and propose a \\textbf{Case2Code} task by\nexploiting the expressiveness and correctness of programs. Specifically, we\ncollect a diverse set of executable programs, synthesize input-output\ntransformations for each program, and force LLMs to infer the underlying code\nimplementations based on the synthetic I/O cases. We first evaluate\nrepresentative LLMs on the synthesized Case2Code task and demonstrate that the\nCase-to-code induction is challenging for LLMs. Then, we synthesize large-scale\nCase2Code training samples to train LLMs to perform inductive reasoning.\nExperimental results show that such induction training benefits not only in\ndistribution Case2Code performance but also enhances various coding abilities\nof trained LLMs, demonstrating the great potential of learning inductive\nreasoning via synthetic data.",
            "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) excel in many natural language processing (NLP)\ntasks. However, since LLMs can only incorporate new knowledge through training\nor supervised fine-tuning processes, they are unsuitable for applications that\ndemand precise, up-to-date, and private information not available in the\ntraining corpora. This precise, up-to-date, and private information is\ntypically stored in relational databases. Thus, a promising solution is to\naugment LLMs with the inclusion of relational databases as external memory.\nThis can ensure the timeliness, correctness, and consistency of data, and\nassist LLMs in performing complex arithmetic operations beyond their inherent\ncapabilities. However, bridging the gap between LLMs and relational databases\nis challenging. It requires the awareness of databases and data values stored\nin databases to select correct databases and issue correct SQL queries.\nBesides, it is necessary for the external memory to be independent of the LLM\nto meet the needs of real-world applications. We introduce a novel LLM-agnostic\nmemory architecture comprising a database selection memory, a data value\nmemory, and relational databases. And we design an elegant pipeline to retrieve\ninformation from it. Besides, we carefully design the prompts to instruct the\nLLM to maximize the framework's potential. To evaluate our method, we compose a\nnew dataset with various types of questions. Experimental results show that our\nframework enables LLMs to effectively answer database-related questions, which\nis beyond their direct ability.",
        "pos": [
            "Transformer-based Large language models (LLMs) have demonstrated their power\nin various tasks, but their inference incurs significant time and energy costs.\nTo accelerate LLM inference, speculative decoding uses a smaller model to\npropose one sequence of tokens, which are subsequently validated in batch by\nthe target large model. Compared with autoregressive decoding, speculative\ndecoding generates the same number of tokens with fewer runs of the large\nmodel, hence accelerating the overall inference by $1$-$2\\times$. However,\ngreedy decoding is not the optimal decoding algorithm in terms of output\nperplexity, which is a direct measurement of the effectiveness of a decoding\nalgorithm. An algorithm that has better output perplexity and even better\nefficiency than speculative decoding can be more useful in practice. To achieve\nthis seemingly contradictory goal, we first introduce multi-token joint greedy\ndecoding (MJGD), which greedily generates multiple tokens at each step based on\ntheir joint perplexity. We show that it leads to better perplexity for the\nwhole output. But the computation cost of MJGD is infeasible in practice. So we\nfurther propose multi-token joint speculative decoding (MJSD), which\napproximates and accelerates the MJGD from two aspects: it approximates the\njoint distribution of the large model with that of a small model, and uses a\nverification step to guarantee the accuracy of approximation; then it uses beam\ndecoding to accelerate the sequence generation from the joint distribution.\nCompared with vanilla speculative decoding, MJSD has two advantages: (1) it is\nan approximation of MJGD, thus achieving better output perplexity; (2)\nverification with joint likelihood allows it to accept the longest prefix\nsub-sequence of the draft tokens with valid perplexity, leading to better\nefficiency..."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) excel in many natural language processing (NLP)\ntasks. However, since LLMs can only incorporate new knowledge through training\nor supervised fine-tuning processes, they are unsuitable for applications that\ndemand precise, up-to-date, and private information not available in the\ntraining corpora. This precise, up-to-date, and private information is\ntypically stored in relational databases. Thus, a promising solution is to\naugment LLMs with the inclusion of relational databases as external memory.\nThis can ensure the timeliness, correctness, and consistency of data, and\nassist LLMs in performing complex arithmetic operations beyond their inherent\ncapabilities. However, bridging the gap between LLMs and relational databases\nis challenging. It requires the awareness of databases and data values stored\nin databases to select correct databases and issue correct SQL queries.\nBesides, it is necessary for the external memory to be independent of the LLM\nto meet the needs of real-world applications. We introduce a novel LLM-agnostic\nmemory architecture comprising a database selection memory, a data value\nmemory, and relational databases. And we design an elegant pipeline to retrieve\ninformation from it. Besides, we carefully design the prompts to instruct the\nLLM to maximize the framework's potential. To evaluate our method, we compose a\nnew dataset with various types of questions. Experimental results show that our\nframework enables LLMs to effectively answer database-related questions, which\nis beyond their direct ability.",
        "pos": [
            "Transformer-based Large language models (LLMs) have demonstrated their power\nin various tasks, but their inference incurs significant time and energy costs.\nTo accelerate LLM inference, speculative decoding uses a smaller model to\npropose one sequence of tokens, which are subsequently validated in batch by\nthe target large model. Compared with autoregressive decoding, speculative\ndecoding generates the same number of tokens with fewer runs of the large\nmodel, hence accelerating the overall inference by $1$-$2\\times$. However,\ngreedy decoding is not the optimal decoding algorithm in terms of output\nperplexity, which is a direct measurement of the effectiveness of a decoding\nalgorithm. An algorithm that has better output perplexity and even better\nefficiency than speculative decoding can be more useful in practice. To achieve\nthis seemingly contradictory goal, we first introduce multi-token joint greedy\ndecoding (MJGD), which greedily generates multiple tokens at each step based on\ntheir joint perplexity. We show that it leads to better perplexity for the\nwhole output. But the computation cost of MJGD is infeasible in practice. So we\nfurther propose multi-token joint speculative decoding (MJSD), which\napproximates and accelerates the MJGD from two aspects: it approximates the\njoint distribution of the large model with that of a small model, and uses a\nverification step to guarantee the accuracy of approximation; then it uses beam\ndecoding to accelerate the sequence generation from the joint distribution.\nCompared with vanilla speculative decoding, MJSD has two advantages: (1) it is\nan approximation of MJGD, thus achieving better output perplexity; (2)\nverification with joint likelihood allows it to accept the longest prefix\nsub-sequence of the draft tokens with valid perplexity, leading to better\nefficiency..."
        ],
        "neg": []
    },
    {
        "query": "Multiple-choice question answering (MCQA) is a key competence of performant\ntransformer language models that is tested by mainstream benchmarks. However,\nrecent evidence shows that models can have quite a range of performance,\nparticularly when the task format is diversified slightly (such as by shuffling\nanswer choice order). In this work we ask: how do successful models perform\nformatted MCQA? We employ vocabulary projection and activation patching methods\nto localize key hidden states that encode relevant information for predicting\nthe correct answer. We find that prediction of a specific answer symbol is\ncausally attributed to a single middle layer, and specifically its multi-head\nself-attention mechanism. We show that subsequent layers increase the\nprobability of the predicted answer symbol in vocabulary space, and that this\nprobability increase is associated with a sparse set of attention heads with\nunique roles. We additionally uncover differences in how different models\nadjust to alternative symbols. Finally, we demonstrate that a synthetic task\ncan disentangle sources of model error to pinpoint when a model has learned\nformatted MCQA, and show that an inability to separate answer symbol tokens in\nvocabulary space is a property of models unable to perform formatted MCQA\ntasks.",
        "pos": [
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "Multiple-choice question answering (MCQA) is a key competence of performant\ntransformer language models that is tested by mainstream benchmarks. However,\nrecent evidence shows that models can have quite a range of performance,\nparticularly when the task format is diversified slightly (such as by shuffling\nanswer choice order). In this work we ask: how do successful models perform\nformatted MCQA? We employ vocabulary projection and activation patching methods\nto localize key hidden states that encode relevant information for predicting\nthe correct answer. We find that prediction of a specific answer symbol is\ncausally attributed to a single middle layer, and specifically its multi-head\nself-attention mechanism. We show that subsequent layers increase the\nprobability of the predicted answer symbol in vocabulary space, and that this\nprobability increase is associated with a sparse set of attention heads with\nunique roles. We additionally uncover differences in how different models\nadjust to alternative symbols. Finally, we demonstrate that a synthetic task\ncan disentangle sources of model error to pinpoint when a model has learned\nformatted MCQA, and show that an inability to separate answer symbol tokens in\nvocabulary space is a property of models unable to perform formatted MCQA\ntasks.",
        "pos": [
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models.",
        "pos": [
            "To enhance Large Language Model (LLM) capabilities, multi-agent debates have\nbeen introduced, where multiple LLMs discuss solutions to a problem over\nseveral rounds of debate. However, LLMs often produce incorrect responses that\nappear deceptively confident, which can mislead other agents. This is partly\nbecause agents do not express their confidence levels during standard debates.\nTo address this, we introduce DebUnc, a multi-agent debate framework that uses\nuncertainty metrics to assess agent confidence levels. We adapted the LLM\nattention mechanism to adjust token weights based on confidence levels and also\nexplored using textual prompts to convey confidence. Our evaluations across\nvarious benchmarks show that attention-based methods are particularly\neffective, and that as uncertainty metrics evolve, performance will continue to\nincrease. The code is available at https://github.com/lukeyoffe/debunc"
        ],
        "neg": []
    },
    {
        "query": "Interruption in a dialogue occurs when the listener begins their speech\nbefore the current speaker finishes speaking. Interruptions can be broadly\ndivided into two groups: cooperative (when the listener wants to support the\nspeaker), and competitive (when the listener tries to take control of the\nconversation against the speaker's will). A system that automatically\nclassifies interruptions can be used in call centers, specifically in the tasks\nof customer satisfaction monitoring and agent monitoring. In this study, we\ndeveloped a text-based interruption classification model by preparing an\nin-house dataset consisting of ASR-transcribed customer support telephone\ndialogues in Russian. We fine-tuned Conversational RuBERT on our dataset and\noptimized hyperparameters, and the model performed well. With further\nimprovements, the proposed model can be applied to automatic monitoring\nsystems.",
        "pos": [
            "Average hold time is a concern for call centers because it affects customer\nsatisfaction. Contact centers should instruct their agents to use special\non-hold scripts to maintain positive interactions with clients. This study\npresents a natural language processing model that detects on-hold phrases in\ncustomer service calls transcribed by automatic speech recognition technology.\nThe task of finding hold scripts in dialogue was formulated as a multiclass\ntext classification problem with three mutually exclusive classes: scripts for\nputting a client on hold, scripts for returning to a client, and phrases\nirrelevant to on-hold scripts. We collected an in-house dataset of calls and\nlabeled each dialogue turn in each call. We fine-tuned RuBERT on the dataset by\nexploring various hyperparameter sets and achieved high model performance. The\ndeveloped model can help agent monitoring by providing a way to check whether\nan agent follows predefined on-hold scripts."
        ],
        "neg": []
    },
    {
        "query": "Interruption in a dialogue occurs when the listener begins their speech\nbefore the current speaker finishes speaking. Interruptions can be broadly\ndivided into two groups: cooperative (when the listener wants to support the\nspeaker), and competitive (when the listener tries to take control of the\nconversation against the speaker's will). A system that automatically\nclassifies interruptions can be used in call centers, specifically in the tasks\nof customer satisfaction monitoring and agent monitoring. In this study, we\ndeveloped a text-based interruption classification model by preparing an\nin-house dataset consisting of ASR-transcribed customer support telephone\ndialogues in Russian. We fine-tuned Conversational RuBERT on our dataset and\noptimized hyperparameters, and the model performed well. With further\nimprovements, the proposed model can be applied to automatic monitoring\nsystems.",
        "pos": [
            "Average hold time is a concern for call centers because it affects customer\nsatisfaction. Contact centers should instruct their agents to use special\non-hold scripts to maintain positive interactions with clients. This study\npresents a natural language processing model that detects on-hold phrases in\ncustomer service calls transcribed by automatic speech recognition technology.\nThe task of finding hold scripts in dialogue was formulated as a multiclass\ntext classification problem with three mutually exclusive classes: scripts for\nputting a client on hold, scripts for returning to a client, and phrases\nirrelevant to on-hold scripts. We collected an in-house dataset of calls and\nlabeled each dialogue turn in each call. We fine-tuned RuBERT on the dataset by\nexploring various hyperparameter sets and achieved high model performance. The\ndeveloped model can help agent monitoring by providing a way to check whether\nan agent follows predefined on-hold scripts."
        ],
        "neg": []
    },
    {
        "query": "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research.",
        "pos": [
            "Understanding and making use of audience feedback is important but difficult\nfor journalists, who now face an impractically large volume of audience\ncomments online. We introduce AudienceView, an online tool to help journalists\ncategorize and interpret this feedback by leveraging large language models\n(LLMs). AudienceView identifies themes and topics, connects them back to\nspecific comments, provides ways to visualize the sentiment and distribution of\nthe comments, and helps users develop ideas for subsequent reporting projects.\nWe consider how such tools can be useful in a journalist's workflow, and\nemphasize the importance of contextual awareness and human judgment.",
            "Words often carry different meanings for people from diverse backgrounds.\nToday's era of social polarization demands that we choose words carefully to\nprevent miscommunication, especially in political communication and journalism.\nTo address this issue, we introduce the Bridging Dictionary, an interactive\ntool designed to illuminate how words are perceived by people with different\npolitical views. The Bridging Dictionary includes a static, printable document\nfeaturing 796 terms with summaries generated by a large language model. These\nsummaries highlight how the terms are used distinctively by Republicans and\nDemocrats. Additionally, the Bridging Dictionary offers an interactive\ninterface that lets users explore selected words, visualizing their frequency,\nsentiment, summaries, and examples across political divides. We present a use\ncase for journalists and emphasize the importance of human agency and trust in\nfurther enhancing this tool. The deployed version of Bridging Dictionary is\navailable at https://dictionary.ccc-mit.org/."
        ],
        "neg": []
    },
    {
        "query": "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research.",
        "pos": [
            "Most currently deployed large language models (LLMs) undergo continuous\ntraining or additional finetuning. By contrast, most research into LLMs'\ninternal mechanisms focuses on models at one snapshot in time (the end of\npre-training), raising the question of whether their results generalize to\nreal-world settings. Existing studies of mechanisms over time focus on\nencoder-only or toy models, which differ significantly from most deployed\nmodels. In this study, we track how model mechanisms, operationalized as\ncircuits, emerge and evolve across 300 billion tokens of training in\ndecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\nWe find that task abilities and the functional components that support them\nemerge consistently at similar token counts across scale. Moreover, although\nsuch components may be implemented by different attention heads over time, the\noverarching algorithm that they implement remains. Surprisingly, both these\nalgorithms and the types of components involved therein can replicate across\nmodel scale. These results suggest that circuit analyses conducted on small\nmodels at the end of pre-training can provide insights that still apply after\nadditional pre-training and over model scale."
        ],
        "neg": []
    },
    {
        "query": "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research.",
        "pos": [
            "Preference optimization techniques have become a standard final stage for\ntraining state-of-art large language models (LLMs). However, despite widespread\nadoption, the vast majority of work to-date has focused on first-class citizen\nlanguages like English and Chinese. This captures a small fraction of the\nlanguages in the world, but also makes it unclear which aspects of current\nstate-of-the-art research transfer to a multilingual setting. In this work, we\nperform an exhaustive study to achieve a new state-of-the-art in aligning\nmultilingual LLMs. We introduce a novel, scalable method for generating\nhigh-quality multilingual feedback data to balance data coverage. We establish\nthe benefits of cross-lingual transfer and increased dataset size in preference\ntraining. Our preference-trained model achieves a 54.4% win-rate against Aya 23\n8B, the current state-of-the-art multilingual LLM in its parameter class, and a\n69.5% win-rate or higher against widely used models like Gemma-1.1-7B-it,\nLlama-3-8B-Instruct, Mistral-7B-Instruct-v0.3. As a result of our study, we\nexpand the frontier of alignment techniques to 23 languages covering half of\nthe world's population."
        ],
        "neg": []
    },
    {
        "query": "Forensic pathology is critical in determining the cause and manner of death\nthrough post-mortem examinations, both macroscopic and microscopic. The field,\nhowever, grapples with issues such as outcome variability, laborious processes,\nand a scarcity of trained professionals. This paper presents SongCi, an\ninnovative visual-language model (VLM) designed specifically for forensic\npathology. SongCi utilizes advanced prototypical cross-modal self-supervised\ncontrastive learning to enhance the accuracy, efficiency, and generalizability\nof forensic analyses. It was pre-trained and evaluated on a comprehensive\nmulti-center dataset, which includes over 16 million high-resolution image\npatches, 2,228 vision-language pairs of post-mortem whole slide images (WSIs),\nand corresponding gross key findings, along with 471 distinct diagnostic\noutcomes. Our findings indicate that SongCi surpasses existing multi-modal AI\nmodels in many forensic pathology tasks, performs comparably to experienced\nforensic pathologists and significantly better than less experienced ones, and\nprovides detailed multi-modal explainability, offering critical assistance in\nforensic investigations. To the best of our knowledge, SongCi is the first VLM\nspecifically developed for forensic pathological analysis and the first\nlarge-vocabulary computational pathology (CPath) model that directly processes\ngigapixel WSIs in forensic science.",
        "pos": [
            "Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that\nare only partially available. This poses two main challenges: effectively\nleveraging multi-view information and mitigating the impact of missing views.\nPrevailing solutions employ cross-view contrastive learning and missing view\nrecovery techniques. However, they either neglect valuable complementary\ninformation by focusing only on consensus between views or provide unreliable\nrecovered views due to the absence of supervision. To address these\nlimitations, we propose a novel Unified and Robust Representation Learning for\nIncomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a\nunified embedding that is robust to view missing conditions by integrating\ninformation from multiple views and neighboring samples. Firstly, to overcome\nthe limitations of cross-view contrastive learning, URRL-IMVC incorporates an\nattention-based auto-encoder framework to fuse multi-view information and\ngenerate unified embeddings. Secondly, URRL-IMVC directly enhances the\nrobustness of the unified embedding against view-missing conditions through KNN\nimputation and data augmentation techniques, eliminating the need for explicit\nmissing view recovery. Finally, incremental improvements are introduced to\nfurther enhance the overall performance, such as the Clustering Module and the\ncustomization of the Encoder. We extensively evaluate the proposed URRL-IMVC\nframework on various benchmark datasets, demonstrating its state-of-the-art\nperformance. Furthermore, comprehensive ablation studies are performed to\nvalidate the effectiveness of our design."
        ],
        "neg": []
    },
    {
        "query": "Multilingual sentence encoders are commonly obtained by training multilingual\nlanguage models to map sentences from different languages into a shared\nsemantic space. As such, they are subject to curse of multilinguality, a loss\nof monolingual representational accuracy due to parameter sharing. Another\nlimitation of multilingual sentence encoders is the trade-off between\nmonolingual and cross-lingual performance. Training for cross-lingual alignment\nof sentence embeddings distorts the optimal monolingual structure of semantic\nspaces of individual languages, harming the utility of sentence embeddings in\nmonolingual tasks. In this work, we address both issues by modular training of\nsentence encoders, i.e., by separating monolingual specialization from\ncross-lingual alignment. We first efficiently train language-specific sentence\nencoders to avoid negative interference between languages (i.e., the curse). We\nthen align all non-English monolingual encoders to the English encoder by\ntraining a cross-lingual alignment adapter on top of each, preventing\ninterference with monolingual specialization from the first step. In both\nsteps, we resort to contrastive learning on machine-translated paraphrase data.\nMonolingual and cross-lingual evaluations on semantic text\nsimilarity/relatedness and multiple-choice QA render our modular solution more\neffective than multilingual sentence encoders, especially benefiting\nlow-resource languages.",
        "pos": [
            "Research on token-level reference-free hallucination detection has\npredominantly focused on English, primarily due to the scarcity of robust\ndatasets in other languages. This has hindered systematic investigations into\nthe effectiveness of cross-lingual transfer for this important NLP application.\nTo address this gap, we introduce ANHALTEN, a new evaluation dataset that\nextends the English hallucination detection dataset to German. To the best of\nour knowledge, this is the first work that explores cross-lingual transfer for\ntoken-level reference-free hallucination detection. ANHALTEN contains gold\nannotations in German that are parallel (i.e., directly comparable to the\noriginal English instances). We benchmark several prominent cross-lingual\ntransfer approaches, demonstrating that larger context length leads to better\nhallucination detection in German, even without succeeding context.\nImportantly, we show that the sample-efficient few-shot transfer is the most\neffective approach in most setups. This highlights the practical benefits of\nminimal annotation effort in the target language for reference-free\nhallucination detection. Aiming to catalyze future research on cross-lingual\ntoken-level reference-free hallucination detection, we make ANHALTEN publicly\navailable: https://github.com/janekh24/anhalten"
        ],
        "neg": []
    },
    {
        "query": "In this paper we present the results of the AI-Debater 2023 Challenge held by\nthe Chinese Conference on Affect Computing (CCAC 2023), and introduce the\nrelated datasets. We organize two tracks to handle the argumentative generation\ntasks in different scenarios, namely, Counter-Argument Generation (Track 1) and\nClaim-based Argument Generation (Track 2). Each track is equipped with its\ndistinct dataset and baseline model respectively. In total, 32 competing teams\nregister for the challenge, from which we received 11 successful submissions.\nIn this paper, we will present the results of the challenge and a summary of\nthe systems, highlighting commonalities and innovations among participating\nsystems. Datasets and baseline models of the AI-Debater 2023 Challenge have\nbeen already released and can be accessed through the official website of the\nchallenge.",
        "pos": [
            "Complex reasoning is an impressive ability shown by large language models\n(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought\nprompting or iterative tool-using to solve challenging tasks step-by-step. In\nthis paper, we hope to focus on evaluating and teaching LLMs to conduct\ninductive reasoning, that is, LLMs are supposed to infer underlying rules by\nobserving examples or sequential transformations. However, collecting\nlarge-scale and diverse human-generated inductive data is challenging. We focus\non data synthesis in the code domain and propose a \\textbf{Case2Code} task by\nexploiting the expressiveness and correctness of programs. Specifically, we\ncollect a diverse set of executable programs, synthesize input-output\ntransformations for each program, and force LLMs to infer the underlying code\nimplementations based on the synthetic I/O cases. We first evaluate\nrepresentative LLMs on the synthesized Case2Code task and demonstrate that the\nCase-to-code induction is challenging for LLMs. Then, we synthesize large-scale\nCase2Code training samples to train LLMs to perform inductive reasoning.\nExperimental results show that such induction training benefits not only in\ndistribution Case2Code performance but also enhances various coding abilities\nof trained LLMs, demonstrating the great potential of learning inductive\nreasoning via synthetic data.",
            "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long- and Short-Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on 5 tasks demonstrate SMART's\nsuperior performance compared to previous widely adopted methods.",
            "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems.",
            "The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nTheoretical justifications and experiment results on five datasets show the\nvalidity and effectiveness of our proposed hybrid framework for training a\nhigh-quality reward model. By decoupling the reward modeling procedure and\nincorporating hybrid supervision, our HaF-RM framework offers a principled and\neffective approach to enhancing the performance and alignment of reward models,\na critical component in the responsible development of powerful language\nmodels. We release our code at https://haf-rm.github.io."
        ],
        "neg": []
    },
    {
        "query": "In this paper we present the results of the AI-Debater 2023 Challenge held by\nthe Chinese Conference on Affect Computing (CCAC 2023), and introduce the\nrelated datasets. We organize two tracks to handle the argumentative generation\ntasks in different scenarios, namely, Counter-Argument Generation (Track 1) and\nClaim-based Argument Generation (Track 2). Each track is equipped with its\ndistinct dataset and baseline model respectively. In total, 32 competing teams\nregister for the challenge, from which we received 11 successful submissions.\nIn this paper, we will present the results of the challenge and a summary of\nthe systems, highlighting commonalities and innovations among participating\nsystems. Datasets and baseline models of the AI-Debater 2023 Challenge have\nbeen already released and can be accessed through the official website of the\nchallenge.",
        "pos": [
            "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long- and Short-Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on 5 tasks demonstrate SMART's\nsuperior performance compared to previous widely adopted methods.",
            "The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nTheoretical justifications and experiment results on five datasets show the\nvalidity and effectiveness of our proposed hybrid framework for training a\nhigh-quality reward model. By decoupling the reward modeling procedure and\nincorporating hybrid supervision, our HaF-RM framework offers a principled and\neffective approach to enhancing the performance and alignment of reward models,\na critical component in the responsible development of powerful language\nmodels. We release our code at https://haf-rm.github.io."
        ],
        "neg": []
    },
    {
        "query": "Automatic spelling correction stands as a pivotal challenge within the ambit\nof natural language processing (NLP), demanding nuanced solutions. Traditional\nspelling correction techniques are typically only capable of detecting and\ncorrecting non-word errors, such as typos and misspellings. However,\ncontext-sensitive errors, also known as real-word errors, are more challenging\nto detect because they are valid words that are used incorrectly in a given\ncontext. The Persian language, characterized by its rich morphology and complex\nsyntax, presents formidable challenges to automatic spelling correction\nsystems. Furthermore, the limited availability of Persian language resources\nmakes it difficult to train effective spelling correction models. This paper\nintroduces a cutting-edge approach for precise and efficient real-word error\ncorrection in Persian text. Our methodology adopts a structured, multi-tiered\napproach, employing semantic analysis, feature selection, and advanced\nclassifiers to enhance error detection and correction efficacy. The innovative\narchitecture discovers and stores semantic similarities between words and\nphrases in Persian text. The classifiers accurately identify real-word errors,\nwhile the semantic ranking algorithm determines the most probable corrections\nfor real-word errors, taking into account specific spelling correction and\ncontext properties such as context, semantic similarity, and edit-distance\nmeasures. Evaluations have demonstrated that our proposed method surpasses\nprevious Persian real-word error correction models. Our method achieves an\nimpressive F-measure of 96.6% in the detection phase and an accuracy of 99.1%\nin the correction phase. These results clearly indicate that our approach is a\nhighly promising solution for automatic real-word error correction in Persian\ntext.",
        "pos": [
            "This research introduces a state-of-the-art Persian spelling correction\nsystem that seamlessly integrates deep learning techniques with phonetic\nanalysis, significantly enhancing the accuracy and efficiency of natural\nlanguage processing (NLP) for Persian. Utilizing a fine-tuned language\nrepresentation model, our methodology effectively combines deep contextual\nanalysis with phonetic insights, adeptly correcting both non-word and real-word\nspelling errors. This strategy proves particularly effective in tackling the\nunique complexities of Persian spelling, including its elaborate morphology and\nthe challenge of homophony. A thorough evaluation on a wide-ranging dataset\nconfirms our system's superior performance compared to existing methods, with\nimpressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for\ncorrecting them. Additionally, the system demonstrates a strong capability in\nnon-word error correction, achieving an F1-Score of 0.891. These results\nillustrate the significant benefits of incorporating phonetic insights into\ndeep learning models for spelling correction. Our contributions not only\nadvance Persian language processing by providing a versatile solution for a\nvariety of NLP applications but also pave the way for future research in the\nfield, emphasizing the critical role of phonetic analysis in developing\neffective spelling correction system."
        ],
        "neg": []
    },
    {
        "query": "Automatic spelling correction stands as a pivotal challenge within the ambit\nof natural language processing (NLP), demanding nuanced solutions. Traditional\nspelling correction techniques are typically only capable of detecting and\ncorrecting non-word errors, such as typos and misspellings. However,\ncontext-sensitive errors, also known as real-word errors, are more challenging\nto detect because they are valid words that are used incorrectly in a given\ncontext. The Persian language, characterized by its rich morphology and complex\nsyntax, presents formidable challenges to automatic spelling correction\nsystems. Furthermore, the limited availability of Persian language resources\nmakes it difficult to train effective spelling correction models. This paper\nintroduces a cutting-edge approach for precise and efficient real-word error\ncorrection in Persian text. Our methodology adopts a structured, multi-tiered\napproach, employing semantic analysis, feature selection, and advanced\nclassifiers to enhance error detection and correction efficacy. The innovative\narchitecture discovers and stores semantic similarities between words and\nphrases in Persian text. The classifiers accurately identify real-word errors,\nwhile the semantic ranking algorithm determines the most probable corrections\nfor real-word errors, taking into account specific spelling correction and\ncontext properties such as context, semantic similarity, and edit-distance\nmeasures. Evaluations have demonstrated that our proposed method surpasses\nprevious Persian real-word error correction models. Our method achieves an\nimpressive F-measure of 96.6% in the detection phase and an accuracy of 99.1%\nin the correction phase. These results clearly indicate that our approach is a\nhighly promising solution for automatic real-word error correction in Persian\ntext.",
        "pos": [
            "This research introduces a state-of-the-art Persian spelling correction\nsystem that seamlessly integrates deep learning techniques with phonetic\nanalysis, significantly enhancing the accuracy and efficiency of natural\nlanguage processing (NLP) for Persian. Utilizing a fine-tuned language\nrepresentation model, our methodology effectively combines deep contextual\nanalysis with phonetic insights, adeptly correcting both non-word and real-word\nspelling errors. This strategy proves particularly effective in tackling the\nunique complexities of Persian spelling, including its elaborate morphology and\nthe challenge of homophony. A thorough evaluation on a wide-ranging dataset\nconfirms our system's superior performance compared to existing methods, with\nimpressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for\ncorrecting them. Additionally, the system demonstrates a strong capability in\nnon-word error correction, achieving an F1-Score of 0.891. These results\nillustrate the significant benefits of incorporating phonetic insights into\ndeep learning models for spelling correction. Our contributions not only\nadvance Persian language processing by providing a versatile solution for a\nvariety of NLP applications but also pave the way for future research in the\nfield, emphasizing the critical role of phonetic analysis in developing\neffective spelling correction system."
        ],
        "neg": []
    },
    {
        "query": "Solving grid puzzles involves a significant amount of logical reasoning.\nHence, it is a good domain to evaluate the reasoning capability of a model\nwhich can then guide us to improve the reasoning ability of models. However,\nmost existing works evaluate only the final predicted answer of a puzzle,\nwithout delving into an in-depth analysis of the LLMs' reasoning chains (such\nas where they falter) or providing any finer metrics to evaluate them. Since\nLLMs may rely on simple heuristics or artifacts to predict the final answer, it\nis crucial to evaluate the generated reasoning chain beyond overall correctness\nmeasures, for accurately evaluating the reasoning abilities of LLMs. To this\nend, we first develop GridPuzzle, an evaluation dataset comprising 274\ngrid-based puzzles with different complexities. Second, we propose a new error\ntaxonomy derived from manual analysis of reasoning chains from LLMs including\nGPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop an LLM-based\nframework for large-scale subjective evaluation (i.e., identifying errors) and\nan objective metric, PuzzleEval, to evaluate the correctness of reasoning\nchains. Evaluating reasoning chains from LLMs leads to several interesting\nfindings. We further show that existing prompting methods used for enhancing\nmodels' reasoning abilities do not improve performance on GridPuzzle. This\nhighlights the importance of understanding fine-grained errors and presents a\nchallenge for future research to enhance LLMs' puzzle-solving abilities by\ndeveloping methods that address these errors. Data and source code are\navailable at https://github.com/Mihir3009/GridPuzzle.",
        "pos": [
            "Extractive summarization plays a pivotal role in natural language processing\ndue to its wide-range applications in summarizing diverse content efficiently,\nwhile also being faithful to the original content. Despite significant\nadvancement achieved in extractive summarization by Large Language Models\n(LLMs), these summaries frequently exhibit incoherence. An important aspect of\nthe coherent summary is its readability for intended users. Although there have\nbeen many datasets and benchmarks proposed for creating coherent extractive\nsummaries, none of them currently incorporate user intent to improve coherence\nin extractive summarization. Motivated by this, we propose a systematically\ncreated human-annotated dataset consisting of coherent summaries for five\npublicly available datasets and natural language user feedback, offering\nvaluable insights into how to improve coherence in extractive summaries. We\nutilize this dataset for aligning LLMs through supervised fine-tuning with\nnatural language human feedback to enhance the coherence of their generated\nsummaries. Preliminary experiments with Falcon-40B and Llama-2-13B show\nsignificant performance improvements (~10% Rouge-L) in terms of producing\ncoherent summaries. We further utilize human feedback to benchmark results over\ninstruction-tuned models such as FLAN-T5 which resulted in several interesting\nfindings. Data and source code are available at\nhttps://github.com/Mihir3009/Extract-AI."
        ],
        "neg": []
    },
    {
        "query": "Solving grid puzzles involves a significant amount of logical reasoning.\nHence, it is a good domain to evaluate the reasoning capability of a model\nwhich can then guide us to improve the reasoning ability of models. However,\nmost existing works evaluate only the final predicted answer of a puzzle,\nwithout delving into an in-depth analysis of the LLMs' reasoning chains (such\nas where they falter) or providing any finer metrics to evaluate them. Since\nLLMs may rely on simple heuristics or artifacts to predict the final answer, it\nis crucial to evaluate the generated reasoning chain beyond overall correctness\nmeasures, for accurately evaluating the reasoning abilities of LLMs. To this\nend, we first develop GridPuzzle, an evaluation dataset comprising 274\ngrid-based puzzles with different complexities. Second, we propose a new error\ntaxonomy derived from manual analysis of reasoning chains from LLMs including\nGPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop an LLM-based\nframework for large-scale subjective evaluation (i.e., identifying errors) and\nan objective metric, PuzzleEval, to evaluate the correctness of reasoning\nchains. Evaluating reasoning chains from LLMs leads to several interesting\nfindings. We further show that existing prompting methods used for enhancing\nmodels' reasoning abilities do not improve performance on GridPuzzle. This\nhighlights the importance of understanding fine-grained errors and presents a\nchallenge for future research to enhance LLMs' puzzle-solving abilities by\ndeveloping methods that address these errors. Data and source code are\navailable at https://github.com/Mihir3009/GridPuzzle.",
        "pos": [
            "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and\nMistral-7B and then evaluating them across a comprehensive suite of\nmultilingual benchmarks that test reasoning, question answering, and reading\ncomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with\nsPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared\nto the baselines. We also devise a strategy to incorporate N-shot examples in\neach fine-tuning sample which further boosts the performance of these models by\n3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other\nmultilingual instruction tuning datasets on the same benchmarks along with\nbeing sample efficient and diverse, thereby reducing dataset creation costs.\nAdditionally, instruction tuning with sPhinX does not lead to regression on\nmost standard LLM benchmarks.",
            "Synthetic data is becoming increasingly important for accelerating the\ndevelopment of language models, both large and small. Despite several\nsuccessful use cases, researchers also raised concerns around model collapse\nand drawbacks of imitating other models. This discrepancy can be attributed to\nthe fact that synthetic data varies in quality and diversity. Effective use of\nsynthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data\nby powerful models to teach a new skill or behavior to another model, we refer\nto this setting as Generative Teaching. We introduce AgentInstruct, an\nextensible agentic framework for automatically creating large amounts of\ndiverse and high-quality synthetic data. AgentInstruct can create both the\nprompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post\ntraining dataset of 25M pairs to teach language models different skills, such\nas text editing, creative writing, tool usage, coding, reading comprehension,\netc. The dataset can be used for instruction tuning of any base model. We\npost-train Mistral-7b with the data. When comparing the resulting model Orca-3\nto Mistral-7b-Instruct (which uses the same base model), we observe significant\nimprovements across many benchmarks. For example, 40% improvement on AGIEval,\n19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and\n45% improvement on AlpacaEval. Additionally, it consistently outperforms other\nmodels such as LLAMA-8B-instruct and GPT-3.5-turbo."
        ],
        "neg": []
    },
    {
        "query": "We initiate a formal investigation into the design and analysis of LLM-based\nalgorithms, i.e. algorithms that contain one or multiple calls of large\nlanguage models (LLMs) as sub-routines and critically rely on the capabilities\nof LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt\nengineering to complicated LLM-powered agent systems and compound AI systems,\nhave achieved remarkable empirical success, the design and optimization of them\nhave mostly relied on heuristics and trial-and-errors, which is largely due to\na lack of formal and analytical study for these algorithms. To fill this gap,\nwe start by identifying the computational-graph representation of LLM-based\nalgorithms, the design principle of task decomposition, and some key\nabstractions, which then facilitate our formal analysis for the accuracy and\nefficiency of LLM-based algorithms, despite the black-box nature of LLMs. We\nfurther consider parallel decomposition for a case study, providing extensive\nanalytical and empirical study for four concrete examples of this pattern. Our\nproposed framework holds promise for advancing LLM-based algorithms, by\nrevealing the reasons behind curious empirical phenomena, guiding the choices\nof hyperparameters, predicting the empirical performance of algorithms, and\ninspiring new algorithm design. To promote further study of LLM-based\nalgorithms, we release our source code at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.",
        "pos": [
            "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO."
        ],
        "neg": []
    },
    {
        "query": "With the advent of foundation models, prompt tuning has positioned itself as\nan important technique for directing model behaviors and eliciting desired\nresponses. Prompt tuning regards selecting appropriate keywords included into\nthe input, thereby adapting to the downstream task without adjusting or\nfine-tuning the model parameters. There is a wide range of work in prompt\ntuning, from approaches that directly harness the backpropagated gradient\nsignals from the model, to those employing black-box optimization such as\nreinforcement learning (RL) methods. Our primary focus is on RLPrompt, which\naims to find optimal prompt tokens leveraging soft Q-learning. While the\nresults show promise, we have observed that the prompts frequently appear\nunnatural, which impedes their interpretability. We address this limitation by\nusing sparse Tsallis entropy regularization, a principled approach to filtering\nout unlikely tokens from consideration. We extensively evaluate our approach\nacross various tasks, including few-shot text classification, unsupervised text\nstyle transfer, and textual inversion from images. The results indicate a\nnotable improvement over baselines, highlighting the efficacy of our approach\nin addressing the challenges of prompt tuning. Moreover, we show that the\nprompts discovered using our method are more natural and interpretable compared\nto those from other baselines.",
        "pos": [
            "Audio-visual speech recognition (AVSR) aims to transcribe human speech using\nboth audio and video modalities. In practical environments with noise-corrupted\naudio, the role of video information becomes crucial. However, prior works have\nprimarily focused on enhancing audio features in AVSR, overlooking the\nimportance of video features. In this study, we strengthen the video features\nby learning three temporal dynamics in video data: context order, playback\ndirection, and the speed of video frames. Cross-modal attention modules are\nintroduced to enrich video features with audio information so that speech\nvariability can be taken into account when training on the video temporal\ndynamics. Based on our approach, we achieve the state-of-the-art performance on\nthe LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach\nexcels in scenarios especially for babble and speech noise, indicating the\nability to distinguish the speech signal that should be recognized from lip\nmovements in the video modality. We support the validity of our methodology by\noffering the ablation experiments for the temporal dynamics losses and the\ncross-modal attention architecture design."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.",
        "pos": [
            "As language models have scaled both their number of parameters and\npretraining dataset sizes, the computational cost for pretraining has become\nintractable except for the most well-resourced teams. This increasing cost\nmakes it ever more important to be able to reuse a model after it has completed\npretraining; allowing for a model's abilities to further improve without\nneeding to train from scratch. In this work, we detail a set of guidelines that\ncover how to design efficacious data distributions and learning rate schedules\nfor continued pretraining of language models. When applying these findings\nwithin a continued pretraining run on top of a well-trained 15B parameter\nmodel, we show an improvement of 9\\% in average model accuracy compared to the\nbaseline of continued training on the pretraining set. The resulting recipe\nprovides a practical starting point with which to begin developing language\nmodels through reuse rather than retraining.",
            "The impressive capabilities of recent language models can be largely\nattributed to the multi-trillion token pretraining datasets that they are\ntrained on. However, model developers fail to disclose their construction\nmethodology which has lead to a lack of open information on how to develop\neffective pretraining sets. To address this issue, we perform the first\nsystematic study across the entire pipeline of pretraining set construction.\nFirst, we run ablations on existing techniques for pretraining set development\nto identify which methods translate to the largest gains in model accuracy on\ndownstream evaluations. Then, we categorize the most widely used data source,\nweb crawl snapshots, across the attributes of toxicity, quality, type of\nspeech, and domain. Finally, we show how such attribute information can be used\nto further refine and improve the quality of a pretraining set. These findings\nconstitute an actionable set of steps that practitioners can use to develop\nhigh quality pretraining sets."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.",
        "pos": [
            "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.",
            "As language models have scaled both their number of parameters and\npretraining dataset sizes, the computational cost for pretraining has become\nintractable except for the most well-resourced teams. This increasing cost\nmakes it ever more important to be able to reuse a model after it has completed\npretraining; allowing for a model's abilities to further improve without\nneeding to train from scratch. In this work, we detail a set of guidelines that\ncover how to design efficacious data distributions and learning rate schedules\nfor continued pretraining of language models. When applying these findings\nwithin a continued pretraining run on top of a well-trained 15B parameter\nmodel, we show an improvement of 9\\% in average model accuracy compared to the\nbaseline of continued training on the pretraining set. The resulting recipe\nprovides a practical starting point with which to begin developing language\nmodels through reuse rather than retraining.",
            "The impressive capabilities of recent language models can be largely\nattributed to the multi-trillion token pretraining datasets that they are\ntrained on. However, model developers fail to disclose their construction\nmethodology which has lead to a lack of open information on how to develop\neffective pretraining sets. To address this issue, we perform the first\nsystematic study across the entire pipeline of pretraining set construction.\nFirst, we run ablations on existing techniques for pretraining set development\nto identify which methods translate to the largest gains in model accuracy on\ndownstream evaluations. Then, we categorize the most widely used data source,\nweb crawl snapshots, across the attributes of toxicity, quality, type of\nspeech, and domain. Finally, we show how such attribute information can be used\nto further refine and improve the quality of a pretraining set. These findings\nconstitute an actionable set of steps that practitioners can use to develop\nhigh quality pretraining sets.",
            "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.",
        "pos": [
            "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.",
            "As language models have scaled both their number of parameters and\npretraining dataset sizes, the computational cost for pretraining has become\nintractable except for the most well-resourced teams. This increasing cost\nmakes it ever more important to be able to reuse a model after it has completed\npretraining; allowing for a model's abilities to further improve without\nneeding to train from scratch. In this work, we detail a set of guidelines that\ncover how to design efficacious data distributions and learning rate schedules\nfor continued pretraining of language models. When applying these findings\nwithin a continued pretraining run on top of a well-trained 15B parameter\nmodel, we show an improvement of 9\\% in average model accuracy compared to the\nbaseline of continued training on the pretraining set. The resulting recipe\nprovides a practical starting point with which to begin developing language\nmodels through reuse rather than retraining.",
            "The impressive capabilities of recent language models can be largely\nattributed to the multi-trillion token pretraining datasets that they are\ntrained on. However, model developers fail to disclose their construction\nmethodology which has lead to a lack of open information on how to develop\neffective pretraining sets. To address this issue, we perform the first\nsystematic study across the entire pipeline of pretraining set construction.\nFirst, we run ablations on existing techniques for pretraining set development\nto identify which methods translate to the largest gains in model accuracy on\ndownstream evaluations. Then, we categorize the most widely used data source,\nweb crawl snapshots, across the attributes of toxicity, quality, type of\nspeech, and domain. Finally, we show how such attribute information can be used\nto further refine and improve the quality of a pretraining set. These findings\nconstitute an actionable set of steps that practitioners can use to develop\nhigh quality pretraining sets.",
            "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains."
        ],
        "neg": []
    },
    {
        "query": "Chart comprehension presents significant challenges for machine learning\nmodels due to the diverse and intricate shapes of charts. Existing multimodal\nmethods often overlook these visual features or fail to integrate them\neffectively for chart question answering (ChartQA). To address this, we\nintroduce Chartformer, a unified framework that enhances chart component\nrecognition by accurately identifying and classifying components such as bars,\nlines, pies, titles, legends, and axes. Additionally, we propose a novel\nQuestion-guided Deformable Co-Attention (QDCAt) mechanism, which fuses chart\nfeatures encoded by Chartformer with the given question, leveraging the\nquestion's guidance to ground the correct answer. Extensive experiments\ndemonstrate that the proposed approaches significantly outperform baseline\nmodels in chart component recognition and ChartQA tasks, achieving improvements\nof 3.2% in mAP and 15.4% in accuracy, respectively. These results underscore\nthe robustness of our solution for detailed visual data interpretation across\nvarious applications.",
        "pos": [
            "Fact-checking real-world claims often requires reviewing multiple multimodal\ndocuments to assess a claim's truthfulness, which is a highly laborious and\ntime-consuming task. In this paper, we present a summarization model designed\nto generate claim-specific summaries useful for fact-checking from multimodal,\nmulti-document datasets. The model takes inputs in the form of documents,\nimages, and a claim, with the objective of assisting in fact-checking tasks. We\nintroduce a dynamic perceiver-based model that can handle inputs from multiple\nmodalities of arbitrary lengths. To train our model, we leverage a novel\nreinforcement learning-based entailment objective to generate summaries that\nprovide evidence distinguishing between different truthfulness labels. To\nassess the efficacy of our approach, we conduct experiments on both an existing\nbenchmark and a new dataset of multi-document claims that we contribute. Our\napproach outperforms the SOTA approach by 4.6% in the claim verification task\non the MOCHEG dataset and demonstrates strong performance on our new\nMulti-News-Fact-Checking dataset."
        ],
        "neg": []
    },
    {
        "query": "Chart comprehension presents significant challenges for machine learning\nmodels due to the diverse and intricate shapes of charts. Existing multimodal\nmethods often overlook these visual features or fail to integrate them\neffectively for chart question answering (ChartQA). To address this, we\nintroduce Chartformer, a unified framework that enhances chart component\nrecognition by accurately identifying and classifying components such as bars,\nlines, pies, titles, legends, and axes. Additionally, we propose a novel\nQuestion-guided Deformable Co-Attention (QDCAt) mechanism, which fuses chart\nfeatures encoded by Chartformer with the given question, leveraging the\nquestion's guidance to ground the correct answer. Extensive experiments\ndemonstrate that the proposed approaches significantly outperform baseline\nmodels in chart component recognition and ChartQA tasks, achieving improvements\nof 3.2% in mAP and 15.4% in accuracy, respectively. These results underscore\nthe robustness of our solution for detailed visual data interpretation across\nvarious applications.",
        "pos": [
            "Automating activities through robots in unstructured environments, such as\nconstruction sites, has been a long-standing desire. However, the high degree\nof unpredictable events in these settings has resulted in far less adoption\ncompared to more structured settings, such as manufacturing, where robots can\nbe hard-coded or trained on narrowly defined datasets. Recently, pretrained\nfoundation models, such as Large Language Models (LLMs), have demonstrated\nsuperior generalization capabilities by providing zero-shot solutions for\nproblems do not present in the training data, proposing them as a potential\nsolution for introducing robots to unstructured environments. To this end, this\nstudy investigates potential opportunities and challenges of pretrained\nfoundation models from a multi-dimensional perspective. The study\nsystematically reviews application of foundation models in two field of robotic\nand unstructured environment and then synthesized them with deliberative acting\ntheory. Findings showed that linguistic capabilities of LLMs have been utilized\nmore than other features for improving perception in human-robot interactions.\nOn the other hand, findings showed that the use of LLMs demonstrated more\napplications in project management and safety in construction, and natural\nhazard detection in disaster management. Synthesizing these findings, we\nlocated the current state-of-the-art in this field on a five-level scale of\nautomation, placing them at conditional automation. This assessment was then\nused to envision future scenarios, challenges, and solutions toward autonomous\nsafe unstructured environments. Our study can be seen as a benchmark to track\nour progress toward that future.",
            "Recent advancements in Vision-Language Models (VLMs) have led to the\ndevelopment of Vision-Language Generalists (VLGs) capable of understanding and\ngenerating interleaved images and text. Despite these advances, VLGs still\nstruggle to follow user instructions for interleaved text and image generation.\nTo address this issue, we introduce LeafInstruct, the first open-sourced\ninterleaved instruction tuning data with over 30,000 high-quality instances\nacross more than 10 domains. Due to the extensive size of existing VLGs, we opt\nfor parameter-efficient tuning. However, we observe that VLGs tuned with a\nstandard LoRA typically exhibit inferior performance in interleaved text-image\ngeneration. We attribute this problem to modality interference and the lack of\nmodality-specialized adaptation design. Hence, we propose Lateralization LoRA,\na novel modality-specialized adaptation method inspired by the concept of brain\nlateralization. Lateralization LoRA employs a hybrid approach, combining the\ntraditional linear LoRA and a Convolutional LoRA for generating text and\nimages, enabling the generation of high-quality text and images by leveraging\nmodality-specific structures and parameter sets. We perform instruction tuning\nof the VLG (i.e., EMU2) using Lateralization LoRA on the LeafInstruct dataset.\nExtensive experiments demonstrate that EMU2 tuned with Lateralization LoRA\nachieve state-of-the-art performance, significantly surpassing baseline models\nin complex interleaved tasks."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the reliability of explanations generated by large\nlanguage models (LLMs) when prompted to explain their previous output. We\nevaluate two kinds of such self-explanations - extractive and counterfactual -\nusing three state-of-the-art LLMs (2B to 8B parameters) on two different\nclassification tasks (objective and subjective). Our findings reveal, that,\nwhile these self-explanations can correlate with human judgement, they do not\nfully and accurately follow the model's decision process, indicating a gap\nbetween perceived and actual model reasoning. We show that this gap can be\nbridged because prompting LLMs for counterfactual explanations can produce\nfaithful, informative, and easy-to-verify results. These counterfactuals offer\na promising alternative to traditional explainability methods (e.g. SHAP,\nLIME), provided that prompts are tailored to specific tasks and checked for\nvalidity.",
        "pos": [
            "Natural Language Processing (NLP) research has traditionally been\npredominantly focused on English, driven by the availability of resources, the\nsize of the research community, and market demands. Recently, there has been a\nnoticeable shift towards multilingualism in NLP, recognizing the need for\ninclusivity and effectiveness across diverse languages and cultures.\nMonolingual surveys have the potential to complement the broader trend towards\nmultilingualism in NLP by providing foundational insights and resources\nnecessary for effectively addressing the linguistic diversity of global\ncommunication. However, monolingual NLP surveys are extremely rare in\nliterature. This study fills the gap by introducing a method for creating\nsystematic and comprehensive monolingual NLP surveys. Characterized by a\nstructured search protocol, it can be used to select publications and organize\nthem through a taxonomy of NLP tasks. We include a classification of Language\nResources (LRs), according to their availability, and datasets, according to\ntheir annotation, to highlight publicly-available and machine-actionable LRs.\nBy applying our method, we conducted a systematic literature review of Greek\nNLP from 2012 to 2022, providing a comprehensive overview of the current state\nand challenges of Greek NLP research. We discuss the progress of Greek NLP and\noutline encountered Greek LRs, classified by availability and usability. As we\nshow, our proposed method helps avoid common pitfalls, such as data leakage and\ncontamination, and to assess language support per NLP task. We consider this\nsystematic literature review of Greek NLP an application of our method that\nshowcases the benefits of a monolingual NLP survey. Similar applications could\nbe regard the myriads of languages whose progress in NLP lags behind that of\nwell-supported languages."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.",
        "pos": [
            "Large language models (LLMs) are integral to modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it demands substantial training\nresources to optimize model weights and quantization parameters. To address\nthis, we propose Efficient Quantization-Aware Training (EfficientQAT), a novel\nquantization technique for compressing LLMs. EfficientQAT involves two\nconsecutive phases: Block-wise training of all parameters (Block-AP) and\nend-to-end training of quantization parameters (E2E-QP). Block-AP sequentially\nconducts quantization-aware training for all parameters in each transformer\nblock with block-wise reconstruction, maintaining efficiency by avoiding\ntraining the entire LLM. Initialized with quantized model, E2E-QP then trains\nonly quantization parameters (step sizes) end-to-end, enhancing efficiency with\na fixed quantized backbone and reduced trainable parameter count. Extensive\nexperiments demonstrate that EfficientQAT outperforms previous quantization\nmethods across a range of models, including base LLMs, instruction-tuned LLMs,\nand multimodal LLMs, with scales from 7B to 70B parameters at various\nquantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model\non a single A100-80GB GPU in 41 hours, with less than 3\\% accuracy degradation\ncompared to the full precision (69.48 vs. 72.41). Notably, this INT2 quantized\n70B model obtains a 1.67 accuracy gain over the Llama-2-13B model (69.48 vs.\n67.81) while requiring less memory (19.2GB vs. 24.2GB). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.",
        "pos": [
            "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.",
        "pos": [
            "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.",
        "pos": [
            "The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.",
        "pos": [
            "The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.",
        "pos": [
            "The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.",
        "pos": [
            "Symptom phenotypes are one of the key types of manifestations for diagnosis\nand treatment of various disease conditions. However, the diversity of symptom\nterminologies is one of the major obstacles hindering the analysis and\nknowledge sharing of various types of symptom-related medical data particularly\nin the fields of Traditional Chinese Medicine (TCM). Objective: This study\naimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to\nsupport the data mining of Chinese EMRs and real-world study in TCM field.\nMethods: To construct an integrated ontology of symptom phenotypes (ISPO), we\nmanually annotated classical TCM textbooks and large-scale Chinese electronic\nmedical records (EMRs) to collect symptom terms with support from a medical\ntext annotation system. Furthermore, to facilitate the semantic\ninteroperability between different terminologies, we incorporated public\navailable biomedical vocabularies by manual mapping between Chinese terms and\nEnglish terms with cross-references to source vocabularies. In addition, we\nevaluated the ISPO using independent clinical EMRs to provide a high-usable\nmedical ontology for clinical data analysis. Results: By integrating 78,696\ninpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and\ndictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition\nor contextual texts. Adhering to the taxonomical structure of the related\nanatomical systems of symptom phenotypes, ISPO provides 12 top-level categories\nand 79 middle-level sub-categories. The validation of data analysis showed the\nISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with\noccurrence rates of 0.5% in additional three independent curated clinical\ndatasets, which can demonstrate the significant value of ISPO in mapping\nclinical terms to ontologies."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.",
        "pos": [
            "The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination."
        ],
        "neg": []
    },
    {
        "query": "Visual Language Models (VLMs) are essential for various tasks, particularly\nvisual reasoning tasks, due to their robust multi-modal information\nintegration, visual reasoning capabilities, and contextual awareness. However,\nexisting \\VLMs{}' visual spatial reasoning capabilities are often inadequate,\nstruggling even with basic tasks such as distinguishing left from right. To\naddress this, we propose the \\ours{} model, designed to enhance the visual\nspatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D\nreconstruction model for obtaining different views of the input images and\nincorporates a prompting mechanism to further improve visual spatial reasoning.\nExperimental results on four visual spatial reasoning datasets show that our\n\\ours{} achieves up to 19.48% accuracy improvement, which indicates the\neffectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.",
        "pos": [
            "Reinforcement learning with human feedback (RLHF), as a widely adopted\napproach in current large language model pipelines, is \\textit{bottlenecked by\nthe size of human preference data}. While traditional methods rely on offline\npreference dataset constructions, recent approaches have shifted towards online\nsettings, where a learner uses a small amount of labeled seed data and a large\npool of unlabeled prompts to iteratively construct new preference data through\nself-generated responses and high-quality reward/preference feedback. However,\nmost current online algorithms still focus on preference labeling during policy\nmodel updating with given feedback oracles, which incurs significant expert\nquery costs. \\textit{We are the first to explore cost-effective proxy reward\noracles construction strategies for further labeling preferences or rewards\nwith extremely limited labeled data and expert query budgets}. Our approach\nintroduces two key innovations: (1) on-policy query to avoid OOD and imbalance\nissues in seed data, and (2) active learning to select the most informative\ndata for preference queries. Using these methods, we train a evaluation model\nwith minimal expert-labeled data, which then effectively labels nine times more\npreference pairs for further RLHF training. For instance, our model using\nDirect Preference Optimization (DPO) gains around over 1% average improvement\non AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our\nmethodology is orthogonal to other direct expert query-based strategies and\ntherefore might be integrated with them to further reduce query costs."
        ],
        "neg": []
    },
    {
        "query": "We release Rasa, the first multilingual expressive TTS dataset for any Indian\nlanguage, which contains 10 hours of neutral speech and 1-3 hours of expressive\nspeech for each of the 6 Ekman emotions covering 3 languages: Assamese,\nBengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and\n30 minutes of expressive data can yield a Fair system as indicated by MUSHRA\nscores. Increasing neutral data to 10 hours, with minimal expressive data,\nsignificantly enhances expressiveness. This offers a practical recipe for\nresource-constrained languages, prioritizing easily obtainable neutral data\nalongside smaller amounts of expressive data. We show the importance of\nsyllabically balanced data and pooling emotions to enhance expressiveness. We\nalso highlight challenges in generating specific emotions, e.g., fear and\nsurprise.",
        "pos": [
            "Publicly available TTS datasets for low-resource languages like Hindi and\nTamil typically contain 10-20 hours of data, leading to poor vocabulary\ncoverage. This limitation becomes evident in downstream applications where\ndomain-specific vocabulary coupled with frequent code-mixing with English,\nresults in many OOV words. To highlight this problem, we create a benchmark\ncontaining OOV words from several real-world applications. Indeed,\nstate-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV\nbenchmark, as indicated by intelligibility tests. To improve the model's OOV\nperformance, we propose a low-effort and economically viable strategy to obtain\nmore training data. Specifically, we propose using volunteers as opposed to\nhigh quality voice artists to record words containing character bigrams unseen\nin the training data. We show that using such inexpensive data, the model's\nperformance improves on OOV words, while not affecting voice quality and\nin-domain performance."
        ],
        "neg": []
    },
    {
        "query": "We release Rasa, the first multilingual expressive TTS dataset for any Indian\nlanguage, which contains 10 hours of neutral speech and 1-3 hours of expressive\nspeech for each of the 6 Ekman emotions covering 3 languages: Assamese,\nBengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and\n30 minutes of expressive data can yield a Fair system as indicated by MUSHRA\nscores. Increasing neutral data to 10 hours, with minimal expressive data,\nsignificantly enhances expressiveness. This offers a practical recipe for\nresource-constrained languages, prioritizing easily obtainable neutral data\nalongside smaller amounts of expressive data. We show the importance of\nsyllabically balanced data and pooling emotions to enhance expressiveness. We\nalso highlight challenges in generating specific emotions, e.g., fear and\nsurprise.",
        "pos": [
            "Publicly available TTS datasets for low-resource languages like Hindi and\nTamil typically contain 10-20 hours of data, leading to poor vocabulary\ncoverage. This limitation becomes evident in downstream applications where\ndomain-specific vocabulary coupled with frequent code-mixing with English,\nresults in many OOV words. To highlight this problem, we create a benchmark\ncontaining OOV words from several real-world applications. Indeed,\nstate-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV\nbenchmark, as indicated by intelligibility tests. To improve the model's OOV\nperformance, we propose a low-effort and economically viable strategy to obtain\nmore training data. Specifically, we propose using volunteers as opposed to\nhigh quality voice artists to record words containing character bigrams unseen\nin the training data. We show that using such inexpensive data, the model's\nperformance improves on OOV words, while not affecting voice quality and\nin-domain performance."
        ],
        "neg": []
    },
    {
        "query": "We release Rasa, the first multilingual expressive TTS dataset for any Indian\nlanguage, which contains 10 hours of neutral speech and 1-3 hours of expressive\nspeech for each of the 6 Ekman emotions covering 3 languages: Assamese,\nBengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and\n30 minutes of expressive data can yield a Fair system as indicated by MUSHRA\nscores. Increasing neutral data to 10 hours, with minimal expressive data,\nsignificantly enhances expressiveness. This offers a practical recipe for\nresource-constrained languages, prioritizing easily obtainable neutral data\nalongside smaller amounts of expressive data. We show the importance of\nsyllabically balanced data and pooling emotions to enhance expressiveness. We\nalso highlight challenges in generating specific emotions, e.g., fear and\nsurprise.",
        "pos": [
            "Publicly available TTS datasets for low-resource languages like Hindi and\nTamil typically contain 10-20 hours of data, leading to poor vocabulary\ncoverage. This limitation becomes evident in downstream applications where\ndomain-specific vocabulary coupled with frequent code-mixing with English,\nresults in many OOV words. To highlight this problem, we create a benchmark\ncontaining OOV words from several real-world applications. Indeed,\nstate-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV\nbenchmark, as indicated by intelligibility tests. To improve the model's OOV\nperformance, we propose a low-effort and economically viable strategy to obtain\nmore training data. Specifically, we propose using volunteers as opposed to\nhigh quality voice artists to record words containing character bigrams unseen\nin the training data. We show that using such inexpensive data, the model's\nperformance improves on OOV words, while not affecting voice quality and\nin-domain performance."
        ],
        "neg": []
    },
    {
        "query": "We release Rasa, the first multilingual expressive TTS dataset for any Indian\nlanguage, which contains 10 hours of neutral speech and 1-3 hours of expressive\nspeech for each of the 6 Ekman emotions covering 3 languages: Assamese,\nBengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and\n30 minutes of expressive data can yield a Fair system as indicated by MUSHRA\nscores. Increasing neutral data to 10 hours, with minimal expressive data,\nsignificantly enhances expressiveness. This offers a practical recipe for\nresource-constrained languages, prioritizing easily obtainable neutral data\nalongside smaller amounts of expressive data. We show the importance of\nsyllabically balanced data and pooling emotions to enhance expressiveness. We\nalso highlight challenges in generating specific emotions, e.g., fear and\nsurprise.",
        "pos": [
            "Publicly available TTS datasets for low-resource languages like Hindi and\nTamil typically contain 10-20 hours of data, leading to poor vocabulary\ncoverage. This limitation becomes evident in downstream applications where\ndomain-specific vocabulary coupled with frequent code-mixing with English,\nresults in many OOV words. To highlight this problem, we create a benchmark\ncontaining OOV words from several real-world applications. Indeed,\nstate-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV\nbenchmark, as indicated by intelligibility tests. To improve the model's OOV\nperformance, we propose a low-effort and economically viable strategy to obtain\nmore training data. Specifically, we propose using volunteers as opposed to\nhigh quality voice artists to record words containing character bigrams unseen\nin the training data. We show that using such inexpensive data, the model's\nperformance improves on OOV words, while not affecting voice quality and\nin-domain performance.",
            "Language Models (LMs) excel in natural language processing tasks for English\nbut show reduced performance in most other languages. This problem is commonly\ntackled by continually pre-training and fine-tuning these models for said\nlanguages. A significant issue in this process is the limited vocabulary\ncoverage in the original model's tokenizer, leading to inadequate\nrepresentation of new languages and necessitating an expansion of the\ntokenizer. The initialization of the embeddings corresponding to new vocabulary\nitems presents a further challenge. Current strategies require cross-lingual\nembeddings and lack a solid theoretical foundation as well as comparisons with\nstrong baselines. In this paper, we first establish theoretically that\ninitializing within the convex hull of existing embeddings is a good\ninitialization, followed by a novel but simple approach, Constrained Word2Vec\n(CW2V), which does not require cross-lingual embeddings. Our study evaluates\ndifferent initialization methods for expanding RoBERTa and LLaMA 2 across four\nlanguages and five tasks. The results show that CW2V performs equally well or\neven better than more advanced techniques. Additionally, simpler approaches\nlike multivariate initialization perform on par with these advanced methods\nindicating that efficient large-scale multilingual continued pretraining can be\nachieved even with simpler initialization methods."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the presence of political bias in emotion inference\nmodels used for sentiment analysis (SA) in social science research. Machine\nlearning models often reflect biases in their training data, impacting the\nvalidity of their outcomes. While previous research has highlighted gender and\nrace biases, our study focuses on political bias - an underexplored yet\npervasive issue that can skew the interpretation of text data across a wide\narray of studies. We conducted a bias audit on a Polish sentiment analysis\nmodel developed in our lab. By analyzing valence predictions for names and\nsentences involving Polish politicians, we uncovered systematic differences\ninfluenced by political affiliations. Our findings indicate that annotations by\nhuman raters propagate political biases into the model's predictions. To\nmitigate this, we pruned the training dataset of texts mentioning these\npoliticians and observed a reduction in bias, though not its complete\nelimination. Given the significant implications of political bias in SA, our\nstudy emphasizes caution in employing these models for social science research.\nWe recommend a critical examination of SA results and propose using\nlexicon-based systems as a more ideologically neutral alternative. This paper\nunderscores the necessity for ongoing scrutiny and methodological adjustments\nto ensure the reliability and impartiality of the use of machine learning in\nacademic and applied contexts.",
        "pos": [
            "This study explores the use of large language models (LLMs) to predict\nemotion intensity in Polish political texts, a resource-poor language context.\nThe research compares the performance of several LLMs against a supervised\nmodel trained on an annotated corpus of 10,000 social media texts, evaluated\nfor the intensity of emotions by expert judges. The findings indicate that\nwhile the supervised model generally outperforms LLMs, offering higher accuracy\nand lower variance, LLMs present a viable alternative, especially given the\nhigh costs associated with data annotation. The study highlights the potential\nof LLMs in low-resource language settings and underscores the need for further\nresearch on emotion intensity prediction and its application across different\nlanguages and continuous features. The implications suggest a nuanced\ndecision-making process to choose the right approach to emotion prediction for\nresearchers and practitioners based on resource availability and the specific\nrequirements of their tasks."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the presence of political bias in emotion inference\nmodels used for sentiment analysis (SA) in social science research. Machine\nlearning models often reflect biases in their training data, impacting the\nvalidity of their outcomes. While previous research has highlighted gender and\nrace biases, our study focuses on political bias - an underexplored yet\npervasive issue that can skew the interpretation of text data across a wide\narray of studies. We conducted a bias audit on a Polish sentiment analysis\nmodel developed in our lab. By analyzing valence predictions for names and\nsentences involving Polish politicians, we uncovered systematic differences\ninfluenced by political affiliations. Our findings indicate that annotations by\nhuman raters propagate political biases into the model's predictions. To\nmitigate this, we pruned the training dataset of texts mentioning these\npoliticians and observed a reduction in bias, though not its complete\nelimination. Given the significant implications of political bias in SA, our\nstudy emphasizes caution in employing these models for social science research.\nWe recommend a critical examination of SA results and propose using\nlexicon-based systems as a more ideologically neutral alternative. This paper\nunderscores the necessity for ongoing scrutiny and methodological adjustments\nto ensure the reliability and impartiality of the use of machine learning in\nacademic and applied contexts.",
        "pos": [
            "This study explores the use of large language models (LLMs) to predict\nemotion intensity in Polish political texts, a resource-poor language context.\nThe research compares the performance of several LLMs against a supervised\nmodel trained on an annotated corpus of 10,000 social media texts, evaluated\nfor the intensity of emotions by expert judges. The findings indicate that\nwhile the supervised model generally outperforms LLMs, offering higher accuracy\nand lower variance, LLMs present a viable alternative, especially given the\nhigh costs associated with data annotation. The study highlights the potential\nof LLMs in low-resource language settings and underscores the need for further\nresearch on emotion intensity prediction and its application across different\nlanguages and continuous features. The implications suggest a nuanced\ndecision-making process to choose the right approach to emotion prediction for\nresearchers and practitioners based on resource availability and the specific\nrequirements of their tasks."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the presence of political bias in emotion inference\nmodels used for sentiment analysis (SA) in social science research. Machine\nlearning models often reflect biases in their training data, impacting the\nvalidity of their outcomes. While previous research has highlighted gender and\nrace biases, our study focuses on political bias - an underexplored yet\npervasive issue that can skew the interpretation of text data across a wide\narray of studies. We conducted a bias audit on a Polish sentiment analysis\nmodel developed in our lab. By analyzing valence predictions for names and\nsentences involving Polish politicians, we uncovered systematic differences\ninfluenced by political affiliations. Our findings indicate that annotations by\nhuman raters propagate political biases into the model's predictions. To\nmitigate this, we pruned the training dataset of texts mentioning these\npoliticians and observed a reduction in bias, though not its complete\nelimination. Given the significant implications of political bias in SA, our\nstudy emphasizes caution in employing these models for social science research.\nWe recommend a critical examination of SA results and propose using\nlexicon-based systems as a more ideologically neutral alternative. This paper\nunderscores the necessity for ongoing scrutiny and methodological adjustments\nto ensure the reliability and impartiality of the use of machine learning in\nacademic and applied contexts.",
        "pos": [
            "This study explores the use of large language models (LLMs) to predict\nemotion intensity in Polish political texts, a resource-poor language context.\nThe research compares the performance of several LLMs against a supervised\nmodel trained on an annotated corpus of 10,000 social media texts, evaluated\nfor the intensity of emotions by expert judges. The findings indicate that\nwhile the supervised model generally outperforms LLMs, offering higher accuracy\nand lower variance, LLMs present a viable alternative, especially given the\nhigh costs associated with data annotation. The study highlights the potential\nof LLMs in low-resource language settings and underscores the need for further\nresearch on emotion intensity prediction and its application across different\nlanguages and continuous features. The implications suggest a nuanced\ndecision-making process to choose the right approach to emotion prediction for\nresearchers and practitioners based on resource availability and the specific\nrequirements of their tasks."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown exceptional performance as\ngeneral-purpose assistants, excelling across a variety of reasoning tasks. This\nachievement represents a significant step toward achieving artificial general\nintelligence (AGI). Despite these advancements, the effectiveness of LLMs often\nhinges on the specific prompting strategies employed, and there remains a lack\nof a robust framework to facilitate learning and generalization across diverse\nreasoning tasks. To address these challenges, we introduce a novel learning\nframework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to\nimitate the Chain-of-Thought (CoT) process which is verified and translated\nfrom reasoning trajectories generated by a symbolic Prolog logic engine. This\nframework proceeds in a self-driven manner, that enables LLMs to formulate\nrules and statements from given instructions and leverage the symbolic Prolog\nengine to derive results. Subsequently, LLMs convert Prolog-derived successive\nreasoning trajectories into natural language CoT for imitation learning. Our\nempirical findings indicate that our proposed approach substantially enhances\nthe reasoning abilities of LLMs and demonstrates robust generalization across\nout-of-distribution reasoning tasks.",
        "pos": [
            "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.",
            "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown exceptional performance as\ngeneral-purpose assistants, excelling across a variety of reasoning tasks. This\nachievement represents a significant step toward achieving artificial general\nintelligence (AGI). Despite these advancements, the effectiveness of LLMs often\nhinges on the specific prompting strategies employed, and there remains a lack\nof a robust framework to facilitate learning and generalization across diverse\nreasoning tasks. To address these challenges, we introduce a novel learning\nframework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to\nimitate the Chain-of-Thought (CoT) process which is verified and translated\nfrom reasoning trajectories generated by a symbolic Prolog logic engine. This\nframework proceeds in a self-driven manner, that enables LLMs to formulate\nrules and statements from given instructions and leverage the symbolic Prolog\nengine to derive results. Subsequently, LLMs convert Prolog-derived successive\nreasoning trajectories into natural language CoT for imitation learning. Our\nempirical findings indicate that our proposed approach substantially enhances\nthe reasoning abilities of LLMs and demonstrates robust generalization across\nout-of-distribution reasoning tasks.",
        "pos": [
            "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.",
            "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown exceptional performance as\ngeneral-purpose assistants, excelling across a variety of reasoning tasks. This\nachievement represents a significant step toward achieving artificial general\nintelligence (AGI). Despite these advancements, the effectiveness of LLMs often\nhinges on the specific prompting strategies employed, and there remains a lack\nof a robust framework to facilitate learning and generalization across diverse\nreasoning tasks. To address these challenges, we introduce a novel learning\nframework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to\nimitate the Chain-of-Thought (CoT) process which is verified and translated\nfrom reasoning trajectories generated by a symbolic Prolog logic engine. This\nframework proceeds in a self-driven manner, that enables LLMs to formulate\nrules and statements from given instructions and leverage the symbolic Prolog\nengine to derive results. Subsequently, LLMs convert Prolog-derived successive\nreasoning trajectories into natural language CoT for imitation learning. Our\nempirical findings indicate that our proposed approach substantially enhances\nthe reasoning abilities of LLMs and demonstrates robust generalization across\nout-of-distribution reasoning tasks.",
        "pos": [
            "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown exceptional performance as\ngeneral-purpose assistants, excelling across a variety of reasoning tasks. This\nachievement represents a significant step toward achieving artificial general\nintelligence (AGI). Despite these advancements, the effectiveness of LLMs often\nhinges on the specific prompting strategies employed, and there remains a lack\nof a robust framework to facilitate learning and generalization across diverse\nreasoning tasks. To address these challenges, we introduce a novel learning\nframework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to\nimitate the Chain-of-Thought (CoT) process which is verified and translated\nfrom reasoning trajectories generated by a symbolic Prolog logic engine. This\nframework proceeds in a self-driven manner, that enables LLMs to formulate\nrules and statements from given instructions and leverage the symbolic Prolog\nengine to derive results. Subsequently, LLMs convert Prolog-derived successive\nreasoning trajectories into natural language CoT for imitation learning. Our\nempirical findings indicate that our proposed approach substantially enhances\nthe reasoning abilities of LLMs and demonstrates robust generalization across\nout-of-distribution reasoning tasks.",
        "pos": [
            "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.",
            "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown exceptional performance as\ngeneral-purpose assistants, excelling across a variety of reasoning tasks. This\nachievement represents a significant step toward achieving artificial general\nintelligence (AGI). Despite these advancements, the effectiveness of LLMs often\nhinges on the specific prompting strategies employed, and there remains a lack\nof a robust framework to facilitate learning and generalization across diverse\nreasoning tasks. To address these challenges, we introduce a novel learning\nframework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to\nimitate the Chain-of-Thought (CoT) process which is verified and translated\nfrom reasoning trajectories generated by a symbolic Prolog logic engine. This\nframework proceeds in a self-driven manner, that enables LLMs to formulate\nrules and statements from given instructions and leverage the symbolic Prolog\nengine to derive results. Subsequently, LLMs convert Prolog-derived successive\nreasoning trajectories into natural language CoT for imitation learning. Our\nempirical findings indicate that our proposed approach substantially enhances\nthe reasoning abilities of LLMs and demonstrates robust generalization across\nout-of-distribution reasoning tasks.",
        "pos": [
            "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.",
            "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown exceptional performance as\ngeneral-purpose assistants, excelling across a variety of reasoning tasks. This\nachievement represents a significant step toward achieving artificial general\nintelligence (AGI). Despite these advancements, the effectiveness of LLMs often\nhinges on the specific prompting strategies employed, and there remains a lack\nof a robust framework to facilitate learning and generalization across diverse\nreasoning tasks. To address these challenges, we introduce a novel learning\nframework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to\nimitate the Chain-of-Thought (CoT) process which is verified and translated\nfrom reasoning trajectories generated by a symbolic Prolog logic engine. This\nframework proceeds in a self-driven manner, that enables LLMs to formulate\nrules and statements from given instructions and leverage the symbolic Prolog\nengine to derive results. Subsequently, LLMs convert Prolog-derived successive\nreasoning trajectories into natural language CoT for imitation learning. Our\nempirical findings indicate that our proposed approach substantially enhances\nthe reasoning abilities of LLMs and demonstrates robust generalization across\nout-of-distribution reasoning tasks.",
        "pos": [
            "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.",
            "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext."
        ],
        "neg": []
    },
    {
        "query": "Recent innovations in language model training have demonstrated that it is\npossible to create highly performant models that are small enough to run on a\nsmartphone. As these models are deployed in an increasing number of domains, it\nis critical to ensure that they are aligned with human preferences and safety\nconsiderations. In this report, we present our methodology for safety aligning\nthe Phi-3 series of language models. We utilized a \"break-fix\" cycle,\nperforming multiple rounds of dataset curation, safety post-training,\nbenchmarking, red teaming, and vulnerability identification to cover a variety\nof harm areas in both single and multi-turn scenarios. Our results indicate\nthat this approach iteratively improved the performance of the Phi-3 models\nacross a wide range of responsible AI benchmarks.",
        "pos": [
            "Reinforcement learning with human feedback (RLHF), as a widely adopted\napproach in current large language model pipelines, is \\textit{bottlenecked by\nthe size of human preference data}. While traditional methods rely on offline\npreference dataset constructions, recent approaches have shifted towards online\nsettings, where a learner uses a small amount of labeled seed data and a large\npool of unlabeled prompts to iteratively construct new preference data through\nself-generated responses and high-quality reward/preference feedback. However,\nmost current online algorithms still focus on preference labeling during policy\nmodel updating with given feedback oracles, which incurs significant expert\nquery costs. \\textit{We are the first to explore cost-effective proxy reward\noracles construction strategies for further labeling preferences or rewards\nwith extremely limited labeled data and expert query budgets}. Our approach\nintroduces two key innovations: (1) on-policy query to avoid OOD and imbalance\nissues in seed data, and (2) active learning to select the most informative\ndata for preference queries. Using these methods, we train a evaluation model\nwith minimal expert-labeled data, which then effectively labels nine times more\npreference pairs for further RLHF training. For instance, our model using\nDirect Preference Optimization (DPO) gains around over 1% average improvement\non AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our\nmethodology is orthogonal to other direct expert query-based strategies and\ntherefore might be integrated with them to further reduce query costs."
        ],
        "neg": []
    },
    {
        "query": "Recent innovations in language model training have demonstrated that it is\npossible to create highly performant models that are small enough to run on a\nsmartphone. As these models are deployed in an increasing number of domains, it\nis critical to ensure that they are aligned with human preferences and safety\nconsiderations. In this report, we present our methodology for safety aligning\nthe Phi-3 series of language models. We utilized a \"break-fix\" cycle,\nperforming multiple rounds of dataset curation, safety post-training,\nbenchmarking, red teaming, and vulnerability identification to cover a variety\nof harm areas in both single and multi-turn scenarios. Our results indicate\nthat this approach iteratively improved the performance of the Phi-3 models\nacross a wide range of responsible AI benchmarks.",
        "pos": [
            "Reinforcement learning with human feedback (RLHF), as a widely adopted\napproach in current large language model pipelines, is \\textit{bottlenecked by\nthe size of human preference data}. While traditional methods rely on offline\npreference dataset constructions, recent approaches have shifted towards online\nsettings, where a learner uses a small amount of labeled seed data and a large\npool of unlabeled prompts to iteratively construct new preference data through\nself-generated responses and high-quality reward/preference feedback. However,\nmost current online algorithms still focus on preference labeling during policy\nmodel updating with given feedback oracles, which incurs significant expert\nquery costs. \\textit{We are the first to explore cost-effective proxy reward\noracles construction strategies for further labeling preferences or rewards\nwith extremely limited labeled data and expert query budgets}. Our approach\nintroduces two key innovations: (1) on-policy query to avoid OOD and imbalance\nissues in seed data, and (2) active learning to select the most informative\ndata for preference queries. Using these methods, we train a evaluation model\nwith minimal expert-labeled data, which then effectively labels nine times more\npreference pairs for further RLHF training. For instance, our model using\nDirect Preference Optimization (DPO) gains around over 1% average improvement\non AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our\nmethodology is orthogonal to other direct expert query-based strategies and\ntherefore might be integrated with them to further reduce query costs."
        ],
        "neg": []
    },
    {
        "query": "When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervisions for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.",
        "pos": [
            "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference."
        ],
        "neg": []
    },
    {
        "query": "When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervisions for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.",
        "pos": [
            "Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data."
        ],
        "neg": []
    },
    {
        "query": "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe same result that the optimal vocabulary size depends on the available\ncompute budget and that larger models deserve larger vocabularies. However,\nmost LLMs use too small vocabulary sizes. For example, we predict that the\noptimal vocabulary size of Llama2-70B should have been at least 216K, 7 times\nlarger than its vocabulary of 32K. We validate our predictions empirically by\ntraining models with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly\nconsidering model parameters and vocabulary size for efficient scaling.",
        "pos": [
            "Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io."
        ],
        "neg": []
    },
    {
        "query": "This paper presents the contribution of our dzNLP team to the NADI 2024\nshared task, specifically in Subtask 1 - Multi-label Country-level Dialect\nIdentification (MLDID) (Closed Track). We explored various configurations to\naddress the challenge: in Experiment 1, we utilized a union of n-gram analyzers\n(word, character, character with word boundaries) with different n-gram values;\nin Experiment 2, we combined a weighted union of Term Frequency-Inverse\nDocument Frequency (TF-IDF) features with various weights; and in Experiment 3,\nwe implemented a weighted major voting scheme using three classifiers: Linear\nSupport Vector Classifier (LSVC), Random Forest (RF), and K-Nearest Neighbors\n(KNN).\n  Our approach, despite its simplicity and reliance on traditional machine\nlearning techniques, demonstrated competitive performance in terms of F1-score\nand precision. Notably, we achieved the highest precision score of 63.22% among\nthe participating teams. However, our overall F1 score was approximately 21%,\nsignificantly impacted by a low recall rate of 12.87%. This indicates that\nwhile our models were highly precise, they struggled to recall a broad range of\ndialect labels, highlighting a critical area for improvement in handling\ndiverse dialectal variations.",
        "pos": [
            "This study compares Term Frequency-Inverse Document Frequency (TF-IDF)\nfeatures with Sentence Transformers for detecting writers' stances--favorable,\nopposing, or neutral--towards three significant topics: COVID-19 vaccine,\ndigital transformation, and women empowerment. Through empirical evaluation, we\ndemonstrate that Sentence Transformers outperform TF-IDF features across\nvarious experimental setups. Our team, dzStance, participated in a stance\ndetection competition, achieving the 13th position (74.91%) among 15 teams in\nWomen Empowerment, 10th (73.43%) in COVID Vaccine, and 12th (66.97%) in Digital\nTransformation. Overall, our team's performance ranked 13th (71.77%) among all\nparticipants. Notably, our approach achieved promising F1-scores, highlighting\nits effectiveness in identifying writers' stances on diverse topics. These\nresults underscore the potential of Sentence Transformers to enhance stance\ndetection models for addressing critical societal issues.",
            "In this paper, we present our dzFinNlp team's contribution for intent\ndetection in financial conversational agents, as part of the AraFinNLP shared\ntask. We experimented with various models and feature configurations, including\ntraditional machine learning methods like LinearSVC with TF-IDF, as well as\ndeep learning models like Long Short-Term Memory (LSTM). Additionally, we\nexplored the use of transformer-based models for this task. Our experiments\nshow promising results, with our best model achieving a micro F1-score of\n93.02% and 67.21% on the ArBanking77 dataset, in the development and test sets,\nrespectively."
        ],
        "neg": []
    },
    {
        "query": "This paper presents the contribution of our dzNLP team to the NADI 2024\nshared task, specifically in Subtask 1 - Multi-label Country-level Dialect\nIdentification (MLDID) (Closed Track). We explored various configurations to\naddress the challenge: in Experiment 1, we utilized a union of n-gram analyzers\n(word, character, character with word boundaries) with different n-gram values;\nin Experiment 2, we combined a weighted union of Term Frequency-Inverse\nDocument Frequency (TF-IDF) features with various weights; and in Experiment 3,\nwe implemented a weighted major voting scheme using three classifiers: Linear\nSupport Vector Classifier (LSVC), Random Forest (RF), and K-Nearest Neighbors\n(KNN).\n  Our approach, despite its simplicity and reliance on traditional machine\nlearning techniques, demonstrated competitive performance in terms of F1-score\nand precision. Notably, we achieved the highest precision score of 63.22% among\nthe participating teams. However, our overall F1 score was approximately 21%,\nsignificantly impacted by a low recall rate of 12.87%. This indicates that\nwhile our models were highly precise, they struggled to recall a broad range of\ndialect labels, highlighting a critical area for improvement in handling\ndiverse dialectal variations.",
        "pos": [
            "This study compares Term Frequency-Inverse Document Frequency (TF-IDF)\nfeatures with Sentence Transformers for detecting writers' stances--favorable,\nopposing, or neutral--towards three significant topics: COVID-19 vaccine,\ndigital transformation, and women empowerment. Through empirical evaluation, we\ndemonstrate that Sentence Transformers outperform TF-IDF features across\nvarious experimental setups. Our team, dzStance, participated in a stance\ndetection competition, achieving the 13th position (74.91%) among 15 teams in\nWomen Empowerment, 10th (73.43%) in COVID Vaccine, and 12th (66.97%) in Digital\nTransformation. Overall, our team's performance ranked 13th (71.77%) among all\nparticipants. Notably, our approach achieved promising F1-scores, highlighting\nits effectiveness in identifying writers' stances on diverse topics. These\nresults underscore the potential of Sentence Transformers to enhance stance\ndetection models for addressing critical societal issues.",
            "In this paper, we present our dzFinNlp team's contribution for intent\ndetection in financial conversational agents, as part of the AraFinNLP shared\ntask. We experimented with various models and feature configurations, including\ntraditional machine learning methods like LinearSVC with TF-IDF, as well as\ndeep learning models like Long Short-Term Memory (LSTM). Additionally, we\nexplored the use of transformer-based models for this task. Our experiments\nshow promising results, with our best model achieving a micro F1-score of\n93.02% and 67.21% on the ArBanking77 dataset, in the development and test sets,\nrespectively."
        ],
        "neg": []
    },
    {
        "query": "The NLP community has recently shown a growing interest in leveraging Large\nLanguage Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential\nknowledge bases (KBs). However, the reliability and extent to which LLMs can\nfunction as KBs remain underexplored. While previous studies suggest LLMs can\nencode knowledge within their parameters, the amount of parametric knowledge\nalone is not sufficient to evaluate their effectiveness as KBs. This study\ndefines criteria that a reliable LLM-as-KB should meet, focusing on factuality\nand consistency, and covering both seen and unseen knowledge. We develop\nseveral metrics based on these criteria and use them to evaluate 26 popular\nLLMs, while providing a comprehensive analysis of the effects of model size,\ninstruction tuning, and in-context learning (ICL). Our results paint a worrying\npicture. Even a high-performant model like GPT-3.5-turbo is not factual or\nconsistent, and strategies like ICL and fine-tuning are unsuccessful at making\nLLMs better KBs.",
        "pos": [
            "We focus on Text-to-SQL semantic parsing from the perspective of Large\nLanguage Models. Motivated by challenges related to the size of commercial\ndatabase schemata and the deployability of business intelligence solutions, we\npropose an approach that dynamically retrieves input database information and\nuses abstract syntax trees to select few-shot examples for in-context learning.\n  Furthermore, we investigate the extent to which an in-parallel semantic\nparser can be leveraged for generating $\\textit{approximated}$ versions of the\nexpected SQL queries, to support our retrieval. We take this approach to the\nextreme--we adapt a model consisting of less than $500$M parameters, to act as\nan extremely efficient approximator, enhancing it with the ability to process\nschemata in a parallelised manner. We apply our approach to monolingual and\ncross-lingual benchmarks for semantic parsing, showing improvements over\nstate-of-the-art baselines. Comprehensive experiments highlight the\ncontribution of modules involved in this retrieval-augmented generation\nsetting, revealing interesting directions for future work."
        ],
        "neg": []
    },
    {
        "query": "Spontaneous style speech synthesis, which aims to generate human-like speech,\noften encounters challenges due to the scarcity of high-quality data and\nlimitations in model capabilities. Recent language model-based TTS systems can\nbe trained on large, diverse, and low-quality speech datasets, resulting in\nhighly natural synthesized speech. However, they are limited by the difficulty\nof simulating various spontaneous behaviors and capturing prosody variations in\nspontaneous speech. In this paper, we propose a novel spontaneous speech\nsynthesis system based on language models. We systematically categorize and\nuniformly model diverse spontaneous behaviors. Moreover, fine-grained prosody\nmodeling is introduced to enhance the model's ability to capture subtle prosody\nvariations in spontaneous speech.Experimental results show that our proposed\nmethod significantly outperforms the baseline methods in terms of prosody\nnaturalness and spontaneous behavior naturalness.",
        "pos": [
            "Functional magnetic resonance imaging (fMRI) is essential for developing\nencoding models that identify functional changes in language-related brain\nareas of individuals with Neurocognitive Disorders (NCD). While large language\nmodel (LLM)-based fMRI encoding has shown promise, existing studies\npredominantly focus on healthy, young adults, overlooking older NCD populations\nand cognitive level correlations. This paper explores language-related\nfunctional changes in older NCD adults using LLM-based fMRI encoding and brain\nscores, addressing current limitations. We analyze the correlation between\nbrain scores and cognitive scores at both whole-brain and language-related ROI\nlevels. Our findings reveal that higher cognitive abilities correspond to\nbetter brain scores, with correlations peaking in the middle temporal gyrus.\nThis study highlights the potential of fMRI encoding models and brain scores\nfor detecting early functional changes in NCD patients.",
            "Multi-talker speech recognition and target-talker speech recognition, both\ninvolve transcription in multi-talker contexts, remain significant challenges.\nHowever, existing methods rarely attempt to simultaneously address both tasks.\nIn this study, we propose a pioneering approach to empower Whisper, which is a\nspeech foundation model, to tackle joint multi-talker and target-talker speech\nrecognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar\nseparator into its encoder to separate mixed embedding for multiple talkers;\n(ii) a Target Talker Identifier is introduced to identify the embedding flow of\nthe target talker on the fly, requiring only three-second enrollment speech as\na cue; (iii) soft prompt tuning for decoder is explored for better task\nadaptation. Our method outperforms previous methods on two- and three-talker\nLibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable\nzero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.",
            "We present MELLE, a novel continuous-valued tokens based language modeling\napproach for text to speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which are originally designed for audio\ncompression and sacrifice fidelity compared to mel-spectrograms. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens. (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language models VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling discrete codes,\nachieves superior performance across multiple metrics, and, most importantly,\noffers a more streamlined paradigm. See https://aka.ms/melle for demos of our\nwork."
        ],
        "neg": []
    },
    {
        "query": "Spontaneous style speech synthesis, which aims to generate human-like speech,\noften encounters challenges due to the scarcity of high-quality data and\nlimitations in model capabilities. Recent language model-based TTS systems can\nbe trained on large, diverse, and low-quality speech datasets, resulting in\nhighly natural synthesized speech. However, they are limited by the difficulty\nof simulating various spontaneous behaviors and capturing prosody variations in\nspontaneous speech. In this paper, we propose a novel spontaneous speech\nsynthesis system based on language models. We systematically categorize and\nuniformly model diverse spontaneous behaviors. Moreover, fine-grained prosody\nmodeling is introduced to enhance the model's ability to capture subtle prosody\nvariations in spontaneous speech.Experimental results show that our proposed\nmethod significantly outperforms the baseline methods in terms of prosody\nnaturalness and spontaneous behavior naturalness.",
        "pos": [
            "Functional magnetic resonance imaging (fMRI) is essential for developing\nencoding models that identify functional changes in language-related brain\nareas of individuals with Neurocognitive Disorders (NCD). While large language\nmodel (LLM)-based fMRI encoding has shown promise, existing studies\npredominantly focus on healthy, young adults, overlooking older NCD populations\nand cognitive level correlations. This paper explores language-related\nfunctional changes in older NCD adults using LLM-based fMRI encoding and brain\nscores, addressing current limitations. We analyze the correlation between\nbrain scores and cognitive scores at both whole-brain and language-related ROI\nlevels. Our findings reveal that higher cognitive abilities correspond to\nbetter brain scores, with correlations peaking in the middle temporal gyrus.\nThis study highlights the potential of fMRI encoding models and brain scores\nfor detecting early functional changes in NCD patients.",
            "Multi-talker speech recognition and target-talker speech recognition, both\ninvolve transcription in multi-talker contexts, remain significant challenges.\nHowever, existing methods rarely attempt to simultaneously address both tasks.\nIn this study, we propose a pioneering approach to empower Whisper, which is a\nspeech foundation model, to tackle joint multi-talker and target-talker speech\nrecognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar\nseparator into its encoder to separate mixed embedding for multiple talkers;\n(ii) a Target Talker Identifier is introduced to identify the embedding flow of\nthe target talker on the fly, requiring only three-second enrollment speech as\na cue; (iii) soft prompt tuning for decoder is explored for better task\nadaptation. Our method outperforms previous methods on two- and three-talker\nLibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable\nzero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.",
            "We present MELLE, a novel continuous-valued tokens based language modeling\napproach for text to speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which are originally designed for audio\ncompression and sacrifice fidelity compared to mel-spectrograms. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens. (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language models VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling discrete codes,\nachieves superior performance across multiple metrics, and, most importantly,\noffers a more streamlined paradigm. See https://aka.ms/melle for demos of our\nwork."
        ],
        "neg": []
    },
    {
        "query": "Simultaneous machine translation aims at solving the task of real-time\ntranslation by starting to translate before consuming the full input, which\nposes challenges in terms of balancing quality and latency of the translation.\nThe wait-$k$ policy offers a solution by starting to translate after consuming\n$k$ words, where the choice of the number $k$ directly affects the latency and\nquality. In applications where we seek to keep the choice over latency and\nquality at inference, the wait-$k$ policy obliges us to train more than one\nmodel. In this paper, we address the challenge of building one model that can\nfulfil multiple latency levels and we achieve this by introducing lightweight\nadapter modules into the decoder. The adapters are trained to be specialized\nfor different wait-$k$ values and compared to other techniques they offer more\nflexibility to allow for reaping the benefits of parameter sharing and\nminimizing interference. Additionally, we show that by combining with an\nadaptive strategy, we can further improve the results. Experiments on two\nlanguage directions show that our method outperforms or competes with other\nstrong baselines on most latency values.",
        "pos": [
            "Content monetization on social media fuels a growing influencer economy.\nInfluencer marketing remains largely undisclosed or inappropriately disclosed\non social media. Non-disclosure issues have become a priority for national and\nsupranational authorities worldwide, who are starting to impose increasingly\nharsher sanctions on them. This paper proposes a transparent methodology for\nmeasuring whether and how influencers comply with disclosures based on legal\nstandards. We introduce a novel distinction between disclosures that are\nlegally sufficient (green) and legally insufficient (yellow). We apply this\nmethodology to an original dataset reflecting the content of 150 Dutch\ninfluencers publicly registered with the Dutch Media Authority based on\nrecently introduced registration obligations. The dataset consists of 292,315\nposts and is multi-language (English and Dutch) and cross-platform (Instagram,\nYouTube and TikTok). We find that influencer marketing remains generally\nunderdisclosed on social media, and that bigger influencers are not necessarily\nmore compliant with disclosure standards."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) are very performant connectionist systems, but\ndo they exhibit more compositionality? More importantly, is that part of why\nthey perform so well? We present empirical analyses across four LLM families\n(12 models) and three task categories, including a novel task introduced below.\nOur findings reveal a nuanced relationship in learning of compositional\nstrategies by LLMs -- while scaling enhances compositional abilities,\ninstruction tuning often has a reverse effect. Such disparity brings forth some\nopen issues regarding the development and improvement of large language models\nin alignment with human cognitive capacities.",
        "pos": [
            "Large vision-language models (VLMs) can assist visually impaired people by\ndescribing images from their daily lives. Current evaluation datasets may not\nreflect diverse cultural user backgrounds or the situational context of this\nuse case. To address this problem, we create a survey to determine caption\npreferences and propose a culture-centric evaluation benchmark by filtering\nVizWiz, an existing dataset with images taken by people who are blind. We then\nevaluate several VLMs, investigating their reliability as visual assistants in\na culturally diverse setting. While our results for state-of-the-art models are\npromising, we identify challenges such as hallucination and misalignment of\nautomatic evaluation metrics with human judgment. We make our survey, data,\ncode, and model outputs publicly available."
        ],
        "neg": []
    },
    {
        "query": "Language model alignment methods, such as reinforcement learning from human\nfeedback (RLHF), have led to impressive advances in language model\ncapabilities, but existing techniques are limited by a widely observed\nphenomenon known as overoptimization, where the quality of the language model\nplateaus or degrades over the course of the alignment process. Overoptimization\nis often attributed to overfitting to an inaccurate reward model, and while it\ncan be mitigated through online data collection, this is infeasible in many\nsettings. This raises a fundamental question: Do existing offline alignment\nalgorithms make the most of the data they have, or can their sample-efficiency\nbe improved further?\n  We address this question with a new algorithm for offline alignment,\n$\\chi^2$-Preference Optimization ($\\chi$PO). $\\chi$PO is a one-line change to\nDirect Preference Optimization (DPO; Rafailov et al., 2023), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite\nthis minimal change, $\\chi$PO implicitly implements the principle of pessimism\nin the face of uncertainty via regularization with the $\\chi^2$-divergence --\nwhich quantifies uncertainty more effectively than KL-regularization -- and\nprovably alleviates overoptimization, achieving sample-complexity guarantees\nbased on single-policy concentrability -- the gold standard in offline\nreinforcement learning. $\\chi$PO's simplicity and strong guarantees make it the\nfirst practical and general-purpose offline alignment algorithm that is\nprovably robust to overoptimization.",
        "pos": [
            "Large language models (LLMs) currently dominate the field of natural language\nprocessing (NLP), representing the state-of-the-art across a diverse array of\ntasks. Developing a model of this nature, from training to inference, requires\nmaking numerous decisions which define a combinatorial search problem. For\nexample, selecting the optimal pre-trained LLM, prompt, or hyperparameters to\nattain the best performance for a task often requires evaluating multiple\ncandidates on an entire test set. This exhaustive evaluation can be\ntime-consuming and costly, as both inference and metric computation with LLMs\nare resource-intensive. In this paper, we address the challenge of identifying\nthe best method within a limited budget for evaluating methods on test\nexamples. By leveraging the well-studied multi-armed bandit framework, which\nsequentially selects the next method-example pair to evaluate, our approach,\ncombining multi-armed bandit algorithms with low-rank factorization,\nsignificantly reduces the required resources. Experiments show that our\nalgorithms can identify the top-performing method using only 5-15\\% of the\ntypically needed resources, resulting in an 85-95\\% reduction in cost.",
            "This paper presents a novel approach to aligning large language models (LLMs)\nwith individual human preferences, sometimes referred to as Reinforcement\nLearning from \\textit{Personalized} Human Feedback (RLPHF). Given stated\npreferences along multiple dimensions, such as helpfulness, conciseness, or\nhumor, the goal is to create an LLM without re-training that best adheres to\nthis specification. Starting from specialized expert LLMs, each trained for one\nsuch particular preference dimension, we propose a black-box method that merges\ntheir outputs on a per-token level. We train a lightweight Preference Control\nModel (PCM) that dynamically translates the preference description and current\ncontext into next-token prediction weights. By combining the expert models'\noutputs at the token level, our approach dynamically generates text that\noptimizes the given preference. Empirical tests show that our method matches or\nsurpasses existing preference merging techniques, providing a scalable,\nefficient alternative to fine-tuning LLMs for individual personalization."
        ],
        "neg": []
    },
    {
        "query": "Self-supervised learning (SSL) models usually require weeks of pre-training\nwith dozens of high-end GPUs. These models typically have a multi-headed\nself-attention (MHSA) context encoder. However, MHSA takes quadratic time and\nspace in the input length, contributing to the high pre-training cost.\nLinear-complexity alternatives to MHSA have been proposed. For instance, in\nsupervised training, the SummaryMixing model is the first to outperform MHSA\nacross multiple speech processing tasks. However, these cheaper alternatives\nhave not been explored for SSL yet. This paper studies a linear-complexity\ncontext encoder for SSL for the first time. With better or equivalent\nperformance for the downstream tasks of the MP3S benchmark, SummaryMixing\nreduces the pre-training time and peak VRAM of wav2vec 2.0 model by 18% and by\n23%, respectively, leading to the pre-training of a 155M wav2vec 2.0 model\nfinished within one week with 4 Tesla A100 GPUs. Code is available at\nhttps://github.com/SamsungLabs/SummaryMixing.",
        "pos": [
            "Recent advancements in textless speech-to-speech translation systems have\nbeen driven by the adoption of self-supervised learning techniques. Although\nmost state-of-the-art systems adopt a similar architecture to transform source\nlanguage speech into sequences of discrete representations in the target\nlanguage, the criteria for selecting these target speech units remains an open\nquestion. This work explores the selection process through a study of\ndownstream tasks such as automatic speech recognition, speech synthesis,\nspeaker recognition, and emotion recognition. Interestingly, our findings\nreveal a discrepancy in the optimization of discrete speech units: units that\nperform well in resynthesis performance do not necessarily correlate with those\nthat enhance translation efficacy. This discrepancy underscores the nuanced\ncomplexity of target feature selection and its impact on the overall\nperformance of speech-to-speech translation systems."
        ],
        "neg": []
    },
    {
        "query": "The field of medical diagnosis has undergone a significant transformation\nwith the advent of large language models (LLMs), yet the challenges of\ninterpretability within these models remain largely unaddressed. This study\nintroduces Chain-of-Diagnosis (CoD) to enhance the interpretability of\nLLM-based medical diagnostics. CoD transforms the diagnostic process into a\ndiagnostic chain that mirrors a physician's thought process, providing a\ntransparent reasoning pathway. Additionally, CoD outputs the disease confidence\ndistribution to ensure transparency in decision-making. This interpretability\nmakes model diagnostics controllable and aids in identifying critical symptoms\nfor inquiry through the entropy reduction of confidences. With CoD, we\ndeveloped DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental\nresults demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic\nbenchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring\ncontrollability in diagnostic rigor.",
        "pos": [
            "Recently, various pre-trained language models (PLMs) have been proposed to\nprove their impressive performances on a wide range of few-shot tasks. However,\nlimited by the unstructured prior knowledge in PLMs, it is difficult to\nmaintain consistent performance on complex structured scenarios, such as\nhierarchical text classification (HTC), especially when the downstream data is\nextremely scarce. The main challenge is how to transfer the unstructured\nsemantic space in PLMs to the downstream domain hierarchy. Unlike previous work\non HTC which directly performs multi-label classification or uses graph neural\nnetwork (GNN) to inject label hierarchy, in this work, we study the HTC problem\nunder a few-shot setting to adapt knowledge in PLMs from an unstructured manner\nto the downstream hierarchy. Technically, we design a simple yet effective\nmethod named Hierarchical Iterative Conditional Random Field (HierICRF) to\nsearch the most domain-challenging directions and exquisitely crafts\ndomain-hierarchy adaptation as a hierarchical iterative language modeling\nproblem, and then it encourages the model to make hierarchical consistency\nself-correction during the inference, thereby achieving knowledge transfer with\nhierarchical consistency preservation. We perform HierICRF on various\narchitectures, and extensive experiments on two popular HTC datasets\ndemonstrate that prompt with HierICRF significantly boosts the few-shot HTC\nperformance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%\nto 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot\nsettings, while remaining SOTA hierarchical consistency performance."
        ],
        "neg": []
    },
    {
        "query": "This paper investigates the capability of LLMs in storytelling, focusing on\nnarrative development and plot progression. We introduce a novel computational\nframework to analyze narratives through three discourse-level aspects: i) story\narcs, ii) turning points, and iii) affective dimensions, including arousal and\nvalence. By leveraging expert and automatic annotations, we uncover significant\ndiscrepancies between the LLM- and human- written stories. While human-written\nstories are suspenseful, arousing, and diverse in narrative structures, LLM\nstories are homogeneously positive and lack tension. Next, we measure narrative\nreasoning skills as a precursor to generative capacities, concluding that most\nLLMs fall short of human abilities in discourse understanding. Finally, we show\nthat explicit integration of aforementioned discourse features can enhance\nstorytelling, as is demonstrated by over 40% improvement in neural storytelling\nin terms of diversity, suspense, and arousal.",
        "pos": [
            "Large language models (LLMs) are increasingly applied to clinical\ndecision-making. However, their potential to exhibit bias poses significant\nrisks to clinical equity. Currently, there is a lack of benchmarks that\nsystematically evaluate such clinical bias in LLMs. While in downstream tasks,\nsome biases of LLMs can be avoided such as by instructing the model to answer\n\"I'm not sure...\", the internal bias hidden within the model still lacks deep\nstudies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in\nLarge Language Models), a pioneering comprehensive benchmark to evaluate both\nintrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for\nclinical decision tasks. Notably, for intrinsic bias, we introduce a novel\nmetric, AssocMAD, to assess the disparities of LLMs across multiple demographic\ngroups. Additionally, we leverage counterfactual intervention to evaluate\nextrinsic bias in a task of clinical diagnosis prediction. Our experiments\nacross popular and medically adapted LLMs, particularly from the Mistral and\nLLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic\nbias. This work underscores the critical need to mitigate clinical bias and\nsets a new standard for future evaluations of LLMs' clinical bias."
        ],
        "neg": []
    },
    {
        "query": "Since language models (LMs) now outperform average humans on many challenging\ntasks, it has become increasingly difficult to develop challenging,\nhigh-quality, and realistic evaluations. We address this issue by examining\nLMs' capabilities to generate code for solving real scientific research\nproblems. Incorporating input from scientists and AI researchers in 16 diverse\nnatural science sub-fields, including mathematics, physics, chemistry, biology,\nand materials science, we created a scientist-curated coding benchmark,\nSciCode. The problems in SciCode naturally factorize into multiple subproblems,\neach involving knowledge recall, reasoning, and code synthesis. In total,\nSciCode contains 338 subproblems decomposed from 80 challenging main problems.\nIt offers optional descriptions specifying useful scientific background\ninformation and scientist-annotated gold-standard solutions and test cases for\nevaluation. Claude3.5-Sonnet, the best-performing model among those tested, can\nsolve only 4.6% of the problems in the most realistic setting. We believe that\nSciCode demonstrates both contemporary LMs' progress towards becoming helpful\nscientific assistants and sheds light on the development and evaluation of\nscientific AI in the future.",
        "pos": [
            "Textual backdoor attacks present a substantial security risk to Large\nLanguage Models (LLM). It embeds carefully chosen triggers into a victim model\nat the training stage, and makes the model erroneously predict inputs\ncontaining the same triggers as a certain class. Prior backdoor defense methods\nprimarily target special token-based triggers, leaving syntax-based triggers\ninsufficiently addressed. To fill this gap, this paper proposes a novel online\ndefense algorithm that effectively counters syntax-based as well as special\ntoken-based backdoor attacks. The algorithm replaces semantically meaningful\nwords in sentences with entirely different ones but preserves the syntactic\ntemplates or special tokens, and then compares the predicted labels before and\nafter the substitution to determine whether a sentence contains triggers.\nExperimental results confirm the algorithm's performance against these two\ntypes of triggers, offering a comprehensive defense strategy for model\nintegrity."
        ],
        "neg": []
    },
    {
        "query": "In recent years, there has been increased demand for speech-to-speech\ntranslation (S2ST) systems in industry settings. Although successfully\ncommercialized, cloning-based S2ST systems expose their distributors to\nliabilities when misused by individuals and can infringe on personality rights\nwhen exploited by media organizations. This work proposes a regulated S2ST\nframework called Preset-Voice Matching (PVM). PVM removes cross-lingual voice\ncloning in S2ST by first matching the input voice to a similar prior consenting\nspeaker voice in the target-language. With this separation, PVM avoids cloning\nthe input speaker, ensuring PVM systems comply with regulations and reduce risk\nof misuse. Our results demonstrate PVM can significantly improve S2ST system\nrun-time in multi-speaker settings and the naturalness of S2ST synthesized\nspeech. To our knowledge, PVM is the first explicitly regulated S2ST framework\nleveraging similarly-matched preset-voices for dynamic S2ST tasks.",
        "pos": [
            "We present a novel approach to data preparation for developing multilingual\nIndic large language model. Our meticulous data acquisition spans open-source\nand proprietary sources, including Common Crawl, Indic books, news articles,\nand Wikipedia, ensuring a diverse and rich linguistic representation. For each\nIndic language, we design a custom preprocessing pipeline to effectively\neliminate redundant and low-quality text content. Additionally, we perform\ndeduplication on Common Crawl data to address the redundancy present in 70% of\nthe crawled web pages. This study focuses on developing high-quality data,\noptimizing tokenization for our multilingual dataset for Indic large language\nmodels with 3B and 7B parameters, engineered for superior performance in Indic\nlanguages. We introduce a novel multilingual tokenizer training strategy,\ndemonstrating our custom-trained Indic tokenizer outperforms the\nstate-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word\nratio for Indic languages."
        ],
        "neg": []
    },
    {
        "query": "Language models are known to encode a great amount of factual knowledge\nthrough pretraining. However, such knowledge might be insufficient to cater to\nuser requests, requiring the model to integrate external knowledge sources and\nadhere to user-provided specifications. When answering questions about ongoing\nevents, the model should use recent news articles to update its response; when\nasked to provide recommendations, the model should prioritize user\nspecifications over retrieved product reviews; when some facts are edited in\nthe model, the updated facts should override all prior knowledge learned by the\nmodel even if they are conflicting. In all of the cases above, the model faces\na decision between its own parametric knowledge, (retrieved) contextual\nknowledge, and user instruction knowledge. In this paper, we (1) unify such\nsettings into the problem of knowledge preference and define a three-level\npreference hierarchy over these knowledge sources; (2) compile a collection of\nexisting datasets IfQA, MQuAKE, and MRQA covering a combination of settings\n(with/without user specifications, with/without context documents) to\nsystematically evaluate how well models obey the intended knowledge preference;\nand (3) propose a dataset synthesis method that composes diverse\nquestion-answer pairs with user assumptions and related context to directly\nfine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a\n7B model, fine-tuned on only a few thousand examples automatically generated by\nour proposed method, effectively achieves superior performance (more than 18%\nimprovement across all evaluation benchmarks) in adhering to the desired\nknowledge preference hierarchy.",
        "pos": [
            "Hallucination is often regarded as a major impediment for using large\nlanguage models (LLMs), especially for knowledge-intensive tasks. Even when the\ntraining corpus consists solely of true statements, language models still\ngenerate hallucinations in the form of amalgamations of multiple facts. We coin\nthis phenomenon as ``knowledge overshadowing'': when we query knowledge from a\nlanguage model with multiple conditions, some conditions overshadow others,\nleading to hallucinated outputs. This phenomenon partially stems from training\ndata imbalance, which we verify on both pretrained models and fine-tuned\nmodels, over a wide range of LM model families and sizes.From a theoretical\npoint of view, knowledge overshadowing can be interpreted as\nover-generalization of the dominant conditions (patterns). We show that the\nhallucination rate grows with both the imbalance ratio (between the popular and\nunpopular condition) and the length of dominant condition description,\nconsistent with our derived generalization bound. Finally, we propose to\nutilize overshadowing conditions as a signal to catch hallucination before it\nis produced, along with a training-free self-contrastive decoding method to\nalleviate hallucination during inference. Our proposed approach showcases up to\n82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,\nwith different models and datasets."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have shown remarkable performance on many\ndifferent Natural Language Processing (NLP) tasks. Prompt engineering plays a\nkey role in adding more to the already existing abilities of LLMs to achieve\nsignificant performance gains on various NLP tasks. Prompt engineering requires\ncomposing natural language instructions called prompts to elicit knowledge from\nLLMs in a structured way. Unlike previous state-of-the-art (SoTA) models,\nprompt engineering does not require extensive parameter re-training or\nfine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently\nextract LLMs' knowledge through a basic natural language conversational\nexchange or prompt engineering, allowing more and more people even without deep\nmathematical machine learning background to experiment with LLMs. With prompt\nengineering gaining popularity in the last two years, researchers have come up\nwith numerous engineering techniques around designing prompts to improve\naccuracy of information extraction from the LLMs. In this paper, we summarize\ndifferent prompting techniques and club them together based on different NLP\ntasks that they have been used for. We further granularly highlight the\nperformance of these prompting strategies on various datasets belonging to that\nNLP task, talk about the corresponding LLMs used, present a taxonomy diagram\nand discuss the possible SoTA for specific datasets. In total, we read and\npresent a survey of 44 research papers which talk about 39 different prompting\nmethods on 29 different NLP tasks of which most of them have been published in\nthe last two years.",
        "pos": [
            "Having a clean dataset has been the foundational assumption of most natural\nlanguage processing (NLP) systems. However, properly written text is rarely\nfound in real-world scenarios and hence, oftentimes invalidates the\naforementioned foundational assumption. Recently, Large language models (LLMs)\nhave shown impressive performance, but can they handle the inevitable noise in\nreal-world data? This work tackles this critical question by investigating\nLLMs' resilience against morphological variations in text. To that end, we\nartificially introduce varying levels of noise into a diverse set of datasets\nand systematically evaluate LLMs' robustness against the corrupt variations of\nthe original text. Our findings show that contrary to popular beliefs,\ngenerative LLMs are quiet robust to noisy perturbations in text. This is a\ndeparture from pre-trained models like BERT or RoBERTa whose performance has\nbeen shown to be sensitive to deteriorating noisy text. Additionally, we test\nLLMs' resilience on multiple real-world benchmarks that closely mimic commonly\nfound errors in the wild. With minimal prompting, LLMs achieve a new\nstate-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and\nLexical Semantic Change (LSC). To empower future research, we also release a\ndataset annotated by humans stating their preference for LLM vs.\nhuman-corrected outputs along with the code to reproduce our results."
        ],
        "neg": []
    },
    {
        "query": "In the field of language modeling, models augmented with retrieval components\nhave emerged as a promising solution to address several challenges faced in the\nnatural language processing (NLP) field, including knowledge grounding,\ninterpretability, and scalability. Despite the primary focus on NLP, we posit\nthat the paradigm of retrieval-enhancement can be extended to a broader\nspectrum of machine learning (ML) such as computer vision, time series\nprediction, and computational biology. Therefore, this work introduces a formal\nframework of this paradigm, Retrieval-Enhanced Machine Learning (REML), by\nsynthesizing the literature in various domains in ML with consistent notations\nwhich is missing from the current literature. Also, we found that while a\nnumber of studies employ retrieval components to augment their models, there is\na lack of integration with foundational Information Retrieval (IR) research. We\nbridge this gap between the seminal IR research and contemporary REML studies\nby investigating each component that comprises the REML framework. Ultimately,\nthe goal of this work is to equip researchers across various disciplines with a\ncomprehensive, formally structured framework of retrieval-enhanced models,\nthereby fostering interdisciplinary future research.",
        "pos": [
            "Knowledge-intensive visual question answering requires models to effectively\nuse external knowledge to help answer visual questions. A typical pipeline\nincludes a knowledge retriever and an answer generator. However, a retriever\nthat utilizes local information, such as an image patch, may not provide\nreliable question-candidate relevance scores. Besides, the two-tower\narchitecture also limits the relevance score modeling of a retriever to select\ntop candidates for answer generator reasoning. In this paper, we introduce an\nadditional module, a multi-modal reranker, to improve the ranking quality of\nknowledge candidates for answer generation. Our reranking module takes\nmulti-modal information from both candidates and questions and performs\ncross-item interaction for better relevance score modeling. Experiments on\nOK-VQA and A-OKVQA show that multi-modal reranker from distant supervision\nprovides consistent improvements. We also find a training-testing discrepancy\nwith reranking in answer generation, where performance improves if training\nknowledge candidates are similar to or noisier than those used in testing."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .",
        "pos": [
            "Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data."
        ],
        "neg": []
    },
    {
        "query": "The success of pretrained language models (PLMs) across a spate of use-cases\nhas led to significant investment from the NLP community towards building\ndomain-specific foundational models. On the other hand, in mission critical\nsettings such as biomedical applications, other aspects also factor in-chief of\nwhich is a model's ability to produce reasonable estimates of its own\nuncertainty. In the present study, we discuss these two desiderata through the\nlens of how they shape the entropy of a model's output probability\ndistribution. We find that domain specificity and uncertainty awareness can\noften be successfully combined, but the exact task at hand weighs in much more\nstrongly.",
        "pos": [
            "This paper describes the organization and findings of AXOLOTL'24, the first\nmultilingual explainable semantic change modeling shared task. We present new\nsense-annotated diachronic semantic change datasets for Finnish and Russian\nwhich were employed in the shared task, along with a surprise test-only German\ndataset borrowed from an existing source. The setup of AXOLOTL'24 is new to the\nsemantic change modeling field, and involves subtasks of identifying unknown\n(novel) senses and providing dictionary-like definitions to these senses. The\nmethods of the winning teams are described and compared, thus paving a path\ntowards explainability in computational approaches to historical change of\nmeaning."
        ],
        "neg": []
    },
    {
        "query": "Understanding and making use of audience feedback is important but difficult\nfor journalists, who now face an impractically large volume of audience\ncomments online. We introduce AudienceView, an online tool to help journalists\ncategorize and interpret this feedback by leveraging large language models\n(LLMs). AudienceView identifies themes and topics, connects them back to\nspecific comments, provides ways to visualize the sentiment and distribution of\nthe comments, and helps users develop ideas for subsequent reporting projects.\nWe consider how such tools can be useful in a journalist's workflow, and\nemphasize the importance of contextual awareness and human judgment.",
        "pos": [
            "Words often carry different meanings for people from diverse backgrounds.\nToday's era of social polarization demands that we choose words carefully to\nprevent miscommunication, especially in political communication and journalism.\nTo address this issue, we introduce the Bridging Dictionary, an interactive\ntool designed to illuminate how words are perceived by people with different\npolitical views. The Bridging Dictionary includes a static, printable document\nfeaturing 796 terms with summaries generated by a large language model. These\nsummaries highlight how the terms are used distinctively by Republicans and\nDemocrats. Additionally, the Bridging Dictionary offers an interactive\ninterface that lets users explore selected words, visualizing their frequency,\nsentiment, summaries, and examples across political divides. We present a use\ncase for journalists and emphasize the importance of human agency and trust in\nfurther enhancing this tool. The deployed version of Bridging Dictionary is\navailable at https://dictionary.ccc-mit.org/."
        ],
        "neg": []
    },
    {
        "query": "Understanding and making use of audience feedback is important but difficult\nfor journalists, who now face an impractically large volume of audience\ncomments online. We introduce AudienceView, an online tool to help journalists\ncategorize and interpret this feedback by leveraging large language models\n(LLMs). AudienceView identifies themes and topics, connects them back to\nspecific comments, provides ways to visualize the sentiment and distribution of\nthe comments, and helps users develop ideas for subsequent reporting projects.\nWe consider how such tools can be useful in a journalist's workflow, and\nemphasize the importance of contextual awareness and human judgment.",
        "pos": [
            "Words often carry different meanings for people from diverse backgrounds.\nToday's era of social polarization demands that we choose words carefully to\nprevent miscommunication, especially in political communication and journalism.\nTo address this issue, we introduce the Bridging Dictionary, an interactive\ntool designed to illuminate how words are perceived by people with different\npolitical views. The Bridging Dictionary includes a static, printable document\nfeaturing 796 terms with summaries generated by a large language model. These\nsummaries highlight how the terms are used distinctively by Republicans and\nDemocrats. Additionally, the Bridging Dictionary offers an interactive\ninterface that lets users explore selected words, visualizing their frequency,\nsentiment, summaries, and examples across political divides. We present a use\ncase for journalists and emphasize the importance of human agency and trust in\nfurther enhancing this tool. The deployed version of Bridging Dictionary is\navailable at https://dictionary.ccc-mit.org/."
        ],
        "neg": []
    },
    {
        "query": "Understanding and making use of audience feedback is important but difficult\nfor journalists, who now face an impractically large volume of audience\ncomments online. We introduce AudienceView, an online tool to help journalists\ncategorize and interpret this feedback by leveraging large language models\n(LLMs). AudienceView identifies themes and topics, connects them back to\nspecific comments, provides ways to visualize the sentiment and distribution of\nthe comments, and helps users develop ideas for subsequent reporting projects.\nWe consider how such tools can be useful in a journalist's workflow, and\nemphasize the importance of contextual awareness and human judgment.",
        "pos": [
            "Words often carry different meanings for people from diverse backgrounds.\nToday's era of social polarization demands that we choose words carefully to\nprevent miscommunication, especially in political communication and journalism.\nTo address this issue, we introduce the Bridging Dictionary, an interactive\ntool designed to illuminate how words are perceived by people with different\npolitical views. The Bridging Dictionary includes a static, printable document\nfeaturing 796 terms with summaries generated by a large language model. These\nsummaries highlight how the terms are used distinctively by Republicans and\nDemocrats. Additionally, the Bridging Dictionary offers an interactive\ninterface that lets users explore selected words, visualizing their frequency,\nsentiment, summaries, and examples across political divides. We present a use\ncase for journalists and emphasize the importance of human agency and trust in\nfurther enhancing this tool. The deployed version of Bridging Dictionary is\navailable at https://dictionary.ccc-mit.org/."
        ],
        "neg": []
    },
    {
        "query": "Understanding and making use of audience feedback is important but difficult\nfor journalists, who now face an impractically large volume of audience\ncomments online. We introduce AudienceView, an online tool to help journalists\ncategorize and interpret this feedback by leveraging large language models\n(LLMs). AudienceView identifies themes and topics, connects them back to\nspecific comments, provides ways to visualize the sentiment and distribution of\nthe comments, and helps users develop ideas for subsequent reporting projects.\nWe consider how such tools can be useful in a journalist's workflow, and\nemphasize the importance of contextual awareness and human judgment.",
        "pos": [
            "Words often carry different meanings for people from diverse backgrounds.\nToday's era of social polarization demands that we choose words carefully to\nprevent miscommunication, especially in political communication and journalism.\nTo address this issue, we introduce the Bridging Dictionary, an interactive\ntool designed to illuminate how words are perceived by people with different\npolitical views. The Bridging Dictionary includes a static, printable document\nfeaturing 796 terms with summaries generated by a large language model. These\nsummaries highlight how the terms are used distinctively by Republicans and\nDemocrats. Additionally, the Bridging Dictionary offers an interactive\ninterface that lets users explore selected words, visualizing their frequency,\nsentiment, summaries, and examples across political divides. We present a use\ncase for journalists and emphasize the importance of human agency and trust in\nfurther enhancing this tool. The deployed version of Bridging Dictionary is\navailable at https://dictionary.ccc-mit.org/."
        ],
        "neg": []
    },
    {
        "query": "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.",
        "pos": [
            "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext."
        ],
        "neg": []
    },
    {
        "query": "Query rewriting aims to generate a new query that can complement the original\nquery to improve the information retrieval system. Recent studies on query\nrewriting, such as query2doc (Q2D), query2expand (Q2E) and querey2cot (Q2C),\nrely on the internal knowledge of Large Language Models (LLMs) to generate a\nrelevant passage to add information to the query. Nevertheless, the efficacy of\nthese methodologies may markedly decline in instances where the requisite\nknowledge is not encapsulated within the model's intrinsic parameters. In this\npaper, we propose a novel structured query rewriting method called Crafting the\nPath tailored for retrieval systems. Crafting the Path involves a three-step\nprocess that crafts query-related information necessary for finding the\npassages to be searched in each step. Specifically, the Crafting the Path\nbegins with Query Concept Comprehension, proceeds to Query Type Identification,\nand finally conducts Expected Answer Extraction. Experimental results show that\nour method outperforms previous rewriting methods, especially in less familiar\ndomains for LLMs. We demonstrate that our method is less dependent on the\ninternal parameter knowledge of the model and generates queries with fewer\nfactual inaccuracies. Furthermore, we observe that Crafting the Path has less\nlatency compared to the baselines.",
        "pos": [
            "Conversational search seeks to retrieve relevant passages for the given\nquestions in Conversational QA (ConvQA). Questions in ConvQA face challenges\nsuch as omissions and coreferences, making it difficult to obtain desired\nsearch results. Conversational Query Reformulation (CQR) transforms these\ncurrent queries into de-contextualized forms to resolve these issues. However,\nexisting CQR methods focus on rewriting human-friendly queries, which may not\nalways yield optimal search results for the retriever. To overcome this\nchallenge, we introduce GuideCQR, a framework that utilizes guided documents to\nrefine queries, ensuring that they are optimal for retrievers. Specifically, we\naugment keywords, generate expected answers from the re-ranked documents, and\nunify them with the filtering process. Experimental results show that queries\nenhanced by guided documents outperform previous CQR methods. Especially,\nGuideCQR surpasses the performance of Large Language Model (LLM) prompt-powered\napproaches and demonstrates the importance of the guided documents in\nformulating retriever-friendly queries across diverse setups."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in measuring hardness-wise properties of data guide language\nmodels in sample selection within low-resource scenarios. However,\nclass-specific properties are overlooked for task setup and learning. How will\nthese properties influence model learning and is it generalizable across\ndatasets? To answer this question, this work formally initiates the concept of\n$\\textit{class-wise hardness}$. Experiments across eight natural language\nunderstanding (NLU) datasets demonstrate a consistent hardness distribution\nacross learning paradigms, models, and human judgment. Subsequent experiments\nunveil a notable challenge in measuring such class-wise hardness with\ninstance-level metrics in previous works. To address this, we propose\n$\\textit{GeoHard}$ for class-wise hardness measurement by modeling class\ngeometry in the semantic embedding space. $\\textit{GeoHard}$ surpasses\ninstance-level metrics by over 59 percent on $\\textit{Pearson}$'s correlation\non measuring class-wise hardness. Our analysis theoretically and empirically\nunderscores the generality of $\\textit{GeoHard}$ as a fresh perspective on data\ndiagnosis. Additionally, we showcase how understanding class-wise hardness can\npractically aid in improving task learning.",
        "pos": [
            "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in measuring hardness-wise properties of data guide language\nmodels in sample selection within low-resource scenarios. However,\nclass-specific properties are overlooked for task setup and learning. How will\nthese properties influence model learning and is it generalizable across\ndatasets? To answer this question, this work formally initiates the concept of\n$\\textit{class-wise hardness}$. Experiments across eight natural language\nunderstanding (NLU) datasets demonstrate a consistent hardness distribution\nacross learning paradigms, models, and human judgment. Subsequent experiments\nunveil a notable challenge in measuring such class-wise hardness with\ninstance-level metrics in previous works. To address this, we propose\n$\\textit{GeoHard}$ for class-wise hardness measurement by modeling class\ngeometry in the semantic embedding space. $\\textit{GeoHard}$ surpasses\ninstance-level metrics by over 59 percent on $\\textit{Pearson}$'s correlation\non measuring class-wise hardness. Our analysis theoretically and empirically\nunderscores the generality of $\\textit{GeoHard}$ as a fresh perspective on data\ndiagnosis. Additionally, we showcase how understanding class-wise hardness can\npractically aid in improving task learning.",
        "pos": [
            "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in measuring hardness-wise properties of data guide language\nmodels in sample selection within low-resource scenarios. However,\nclass-specific properties are overlooked for task setup and learning. How will\nthese properties influence model learning and is it generalizable across\ndatasets? To answer this question, this work formally initiates the concept of\n$\\textit{class-wise hardness}$. Experiments across eight natural language\nunderstanding (NLU) datasets demonstrate a consistent hardness distribution\nacross learning paradigms, models, and human judgment. Subsequent experiments\nunveil a notable challenge in measuring such class-wise hardness with\ninstance-level metrics in previous works. To address this, we propose\n$\\textit{GeoHard}$ for class-wise hardness measurement by modeling class\ngeometry in the semantic embedding space. $\\textit{GeoHard}$ surpasses\ninstance-level metrics by over 59 percent on $\\textit{Pearson}$'s correlation\non measuring class-wise hardness. Our analysis theoretically and empirically\nunderscores the generality of $\\textit{GeoHard}$ as a fresh perspective on data\ndiagnosis. Additionally, we showcase how understanding class-wise hardness can\npractically aid in improving task learning.",
        "pos": [
            "Recently, there has been a growing interest among large language model (LLM)\ndevelopers in LLM-based document reading systems, which enable users to upload\ntheir own documents and pose questions related to the document contents, going\nbeyond simple reading comprehension tasks. Consequently, these systems have\nbeen carefully designed to tackle challenges such as file parsing, metadata\nextraction, multi-modal information understanding and long-context reading.\nHowever, no current benchmark exists to evaluate their performance in such\nscenarios, where a raw file and questions are provided as input, and a\ncorresponding response is expected as output. In this paper, we introduce\nDocBench, a new benchmark designed to evaluate LLM-based document reading\nsystems. Our benchmark involves a meticulously crafted process, including the\nrecruitment of human annotators and the generation of synthetic questions. It\nincludes 229 real documents and 1,102 questions, spanning across five different\ndomains and four major types of questions. We evaluate both proprietary\nLLM-based systems accessible via web interfaces or APIs, and a parse-then-read\npipeline employing open-source LLMs. Our evaluations reveal noticeable gaps\nbetween existing LLM-based document reading systems and human performance,\nunderscoring the challenges of developing proficient systems. To summarize,\nDocBench aims to establish a standardized benchmark for evaluating LLM-based\ndocument reading systems under diverse real-world scenarios, thereby guiding\nfuture advancements in this research area.",
            "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain."
        ],
        "neg": []
    },
    {
        "query": "Recent advances in measuring hardness-wise properties of data guide language\nmodels in sample selection within low-resource scenarios. However,\nclass-specific properties are overlooked for task setup and learning. How will\nthese properties influence model learning and is it generalizable across\ndatasets? To answer this question, this work formally initiates the concept of\n$\\textit{class-wise hardness}$. Experiments across eight natural language\nunderstanding (NLU) datasets demonstrate a consistent hardness distribution\nacross learning paradigms, models, and human judgment. Subsequent experiments\nunveil a notable challenge in measuring such class-wise hardness with\ninstance-level metrics in previous works. To address this, we propose\n$\\textit{GeoHard}$ for class-wise hardness measurement by modeling class\ngeometry in the semantic embedding space. $\\textit{GeoHard}$ surpasses\ninstance-level metrics by over 59 percent on $\\textit{Pearson}$'s correlation\non measuring class-wise hardness. Our analysis theoretically and empirically\nunderscores the generality of $\\textit{GeoHard}$ as a fresh perspective on data\ndiagnosis. Additionally, we showcase how understanding class-wise hardness can\npractically aid in improving task learning.",
        "pos": [
            "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain."
        ],
        "neg": []
    },
    {
        "query": "The language used by US courtroom actors in criminal trials has long been\nstudied for biases. However, systematic studies for bias in high-stakes court\ntrials have been difficult, due to the nuanced nature of bias and the legal\nexpertise required. Large language models offer the possibility to automate\nannotation. But validating the computational approach requires both an\nunderstanding of how automated methods fit in existing annotation workflows and\nwhat they really offer. We present a case study of adding a computational model\nto a complex and high-stakes problem: identifying gender-biased language in US\ncapital trials for women defendants. Our team of experienced death-penalty\nlawyers and NLP technologists pursue a three-phase study: first annotating\nmanually, then training and evaluating computational models, and finally\ncomparing expert annotations to model predictions. Unlike many typical NLP\ntasks, annotating for gender bias in months-long capital trials is complicated,\nwith many individual judgment calls. Contrary to standard arguments for\nautomation that are based on efficiency and scalability, legal experts find the\ncomputational models most useful in providing opportunities to reflect on their\nown bias in annotation and to build consensus on annotation rules. This\nexperience suggests that seeking to replace experts with computational models\nfor complex annotation is both unrealistic and undesirable. Rather,\ncomputational models offer valuable opportunities to assist the legal experts\nin annotation-based studies.",
        "pos": [
            "Contemporary language models are increasingly multilingual, but Chinese LLM\ndevelopers must navigate complex political and business considerations of\nlanguage diversity. Language policy in China aims at influencing the public\ndiscourse and governing a multi-ethnic society, and has gradually transitioned\nfrom a pluralist to a more assimilationist approach since 1949. We explore the\nimpact of these influences on current language technology. We evaluate six\nopen-source multilingual LLMs pre-trained by Chinese companies on 18 languages,\nspanning a wide range of Chinese, Asian, and Anglo-European languages. Our\nexperiments show Chinese LLMs performance on diverse languages is\nindistinguishable from international LLMs. Similarly, the models' technical\nreports also show lack of consideration for pretraining data language coverage\nexcept for English and Mandarin Chinese. Examining Chinese AI policy, model\nexperiments, and technical reports, we find no sign of any consistent policy,\neither for or against, language diversity in China's LLM development. This\nleaves a puzzling fact that while China regulates both the languages people use\ndaily as well as language model development, they do not seem to have any\npolicy on the languages in language models."
        ],
        "neg": []
    },
    {
        "query": "The language used by US courtroom actors in criminal trials has long been\nstudied for biases. However, systematic studies for bias in high-stakes court\ntrials have been difficult, due to the nuanced nature of bias and the legal\nexpertise required. Large language models offer the possibility to automate\nannotation. But validating the computational approach requires both an\nunderstanding of how automated methods fit in existing annotation workflows and\nwhat they really offer. We present a case study of adding a computational model\nto a complex and high-stakes problem: identifying gender-biased language in US\ncapital trials for women defendants. Our team of experienced death-penalty\nlawyers and NLP technologists pursue a three-phase study: first annotating\nmanually, then training and evaluating computational models, and finally\ncomparing expert annotations to model predictions. Unlike many typical NLP\ntasks, annotating for gender bias in months-long capital trials is complicated,\nwith many individual judgment calls. Contrary to standard arguments for\nautomation that are based on efficiency and scalability, legal experts find the\ncomputational models most useful in providing opportunities to reflect on their\nown bias in annotation and to build consensus on annotation rules. This\nexperience suggests that seeking to replace experts with computational models\nfor complex annotation is both unrealistic and undesirable. Rather,\ncomputational models offer valuable opportunities to assist the legal experts\nin annotation-based studies.",
        "pos": [
            "Contemporary language models are increasingly multilingual, but Chinese LLM\ndevelopers must navigate complex political and business considerations of\nlanguage diversity. Language policy in China aims at influencing the public\ndiscourse and governing a multi-ethnic society, and has gradually transitioned\nfrom a pluralist to a more assimilationist approach since 1949. We explore the\nimpact of these influences on current language technology. We evaluate six\nopen-source multilingual LLMs pre-trained by Chinese companies on 18 languages,\nspanning a wide range of Chinese, Asian, and Anglo-European languages. Our\nexperiments show Chinese LLMs performance on diverse languages is\nindistinguishable from international LLMs. Similarly, the models' technical\nreports also show lack of consideration for pretraining data language coverage\nexcept for English and Mandarin Chinese. Examining Chinese AI policy, model\nexperiments, and technical reports, we find no sign of any consistent policy,\neither for or against, language diversity in China's LLM development. This\nleaves a puzzling fact that while China regulates both the languages people use\ndaily as well as language model development, they do not seem to have any\npolicy on the languages in language models."
        ],
        "neg": []
    },
    {
        "query": "Semantic Textual Relatedness holds significant relevance in Natural Language\nProcessing, finding applications across various domains. Traditionally,\napproaches to STR have relied on knowledge-based and statistical methods.\nHowever, with the emergence of Large Language Models, there has been a paradigm\nshift, ushering in new methodologies. In this paper, we delve into the\ninvestigation of sentence-level STR within Track A (Supervised) by leveraging\nfine-tuning techniques on the RoBERTa transformer. Our study focuses on\nassessing the efficacy of this approach across different languages. Notably,\nour findings indicate promising advancements in STR performance, particularly\nin Latin languages. Specifically, our results demonstrate notable improvements\nin English, achieving a correlation of 0.82 and securing a commendable 19th\nrank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the\n15th position. However, our approach encounters challenges in languages like\nArabic, where we observed a correlation of only 0.38, resulting in a 20th rank.",
        "pos": [
            "Detecting Machine-Generated Text (MGT) has emerged as a significant area of\nstudy within Natural Language Processing. While language models generate text,\nthey often leave discernible traces, which can be scrutinized using either\ntraditional feature-based methods or more advanced neural language models. In\nthis research, we explore the effectiveness of fine-tuning a RoBERTa-base\ntransformer, a powerful neural architecture, to address MGT detection as a\nbinary classification task. Focusing specifically on Subtask A\n(Monolingual-English) within the SemEval-2024 competition framework, our\nproposed system achieves an accuracy of 78.9% on the test dataset, positioning\nus at 57th among participants. Our study addresses this challenge while\nconsidering the limited hardware resources, resulting in a system that excels\nat identifying human-written texts but encounters challenges in accurately\ndiscerning MGTs."
        ],
        "neg": []
    },
    {
        "query": "Semantic Textual Relatedness holds significant relevance in Natural Language\nProcessing, finding applications across various domains. Traditionally,\napproaches to STR have relied on knowledge-based and statistical methods.\nHowever, with the emergence of Large Language Models, there has been a paradigm\nshift, ushering in new methodologies. In this paper, we delve into the\ninvestigation of sentence-level STR within Track A (Supervised) by leveraging\nfine-tuning techniques on the RoBERTa transformer. Our study focuses on\nassessing the efficacy of this approach across different languages. Notably,\nour findings indicate promising advancements in STR performance, particularly\nin Latin languages. Specifically, our results demonstrate notable improvements\nin English, achieving a correlation of 0.82 and securing a commendable 19th\nrank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the\n15th position. However, our approach encounters challenges in languages like\nArabic, where we observed a correlation of only 0.38, resulting in a 20th rank.",
        "pos": [
            "Detecting Machine-Generated Text (MGT) has emerged as a significant area of\nstudy within Natural Language Processing. While language models generate text,\nthey often leave discernible traces, which can be scrutinized using either\ntraditional feature-based methods or more advanced neural language models. In\nthis research, we explore the effectiveness of fine-tuning a RoBERTa-base\ntransformer, a powerful neural architecture, to address MGT detection as a\nbinary classification task. Focusing specifically on Subtask A\n(Monolingual-English) within the SemEval-2024 competition framework, our\nproposed system achieves an accuracy of 78.9% on the test dataset, positioning\nus at 57th among participants. Our study addresses this challenge while\nconsidering the limited hardware resources, resulting in a system that excels\nat identifying human-written texts but encounters challenges in accurately\ndiscerning MGTs."
        ],
        "neg": []
    },
    {
        "query": "Semantic Textual Relatedness holds significant relevance in Natural Language\nProcessing, finding applications across various domains. Traditionally,\napproaches to STR have relied on knowledge-based and statistical methods.\nHowever, with the emergence of Large Language Models, there has been a paradigm\nshift, ushering in new methodologies. In this paper, we delve into the\ninvestigation of sentence-level STR within Track A (Supervised) by leveraging\nfine-tuning techniques on the RoBERTa transformer. Our study focuses on\nassessing the efficacy of this approach across different languages. Notably,\nour findings indicate promising advancements in STR performance, particularly\nin Latin languages. Specifically, our results demonstrate notable improvements\nin English, achieving a correlation of 0.82 and securing a commendable 19th\nrank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the\n15th position. However, our approach encounters challenges in languages like\nArabic, where we observed a correlation of only 0.38, resulting in a 20th rank.",
        "pos": [
            "Detecting Machine-Generated Text (MGT) has emerged as a significant area of\nstudy within Natural Language Processing. While language models generate text,\nthey often leave discernible traces, which can be scrutinized using either\ntraditional feature-based methods or more advanced neural language models. In\nthis research, we explore the effectiveness of fine-tuning a RoBERTa-base\ntransformer, a powerful neural architecture, to address MGT detection as a\nbinary classification task. Focusing specifically on Subtask A\n(Monolingual-English) within the SemEval-2024 competition framework, our\nproposed system achieves an accuracy of 78.9% on the test dataset, positioning\nus at 57th among participants. Our study addresses this challenge while\nconsidering the limited hardware resources, resulting in a system that excels\nat identifying human-written texts but encounters challenges in accurately\ndiscerning MGTs."
        ],
        "neg": []
    },
    {
        "query": "Semantic Textual Relatedness holds significant relevance in Natural Language\nProcessing, finding applications across various domains. Traditionally,\napproaches to STR have relied on knowledge-based and statistical methods.\nHowever, with the emergence of Large Language Models, there has been a paradigm\nshift, ushering in new methodologies. In this paper, we delve into the\ninvestigation of sentence-level STR within Track A (Supervised) by leveraging\nfine-tuning techniques on the RoBERTa transformer. Our study focuses on\nassessing the efficacy of this approach across different languages. Notably,\nour findings indicate promising advancements in STR performance, particularly\nin Latin languages. Specifically, our results demonstrate notable improvements\nin English, achieving a correlation of 0.82 and securing a commendable 19th\nrank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the\n15th position. However, our approach encounters challenges in languages like\nArabic, where we observed a correlation of only 0.38, resulting in a 20th rank.",
        "pos": [
            "Detecting Machine-Generated Text (MGT) has emerged as a significant area of\nstudy within Natural Language Processing. While language models generate text,\nthey often leave discernible traces, which can be scrutinized using either\ntraditional feature-based methods or more advanced neural language models. In\nthis research, we explore the effectiveness of fine-tuning a RoBERTa-base\ntransformer, a powerful neural architecture, to address MGT detection as a\nbinary classification task. Focusing specifically on Subtask A\n(Monolingual-English) within the SemEval-2024 competition framework, our\nproposed system achieves an accuracy of 78.9% on the test dataset, positioning\nus at 57th among participants. Our study addresses this challenge while\nconsidering the limited hardware resources, resulting in a system that excels\nat identifying human-written texts but encounters challenges in accurately\ndiscerning MGTs."
        ],
        "neg": []
    },
    {
        "query": "Semantic Textual Relatedness holds significant relevance in Natural Language\nProcessing, finding applications across various domains. Traditionally,\napproaches to STR have relied on knowledge-based and statistical methods.\nHowever, with the emergence of Large Language Models, there has been a paradigm\nshift, ushering in new methodologies. In this paper, we delve into the\ninvestigation of sentence-level STR within Track A (Supervised) by leveraging\nfine-tuning techniques on the RoBERTa transformer. Our study focuses on\nassessing the efficacy of this approach across different languages. Notably,\nour findings indicate promising advancements in STR performance, particularly\nin Latin languages. Specifically, our results demonstrate notable improvements\nin English, achieving a correlation of 0.82 and securing a commendable 19th\nrank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the\n15th position. However, our approach encounters challenges in languages like\nArabic, where we observed a correlation of only 0.38, resulting in a 20th rank.",
        "pos": [
            "Detecting Machine-Generated Text (MGT) has emerged as a significant area of\nstudy within Natural Language Processing. While language models generate text,\nthey often leave discernible traces, which can be scrutinized using either\ntraditional feature-based methods or more advanced neural language models. In\nthis research, we explore the effectiveness of fine-tuning a RoBERTa-base\ntransformer, a powerful neural architecture, to address MGT detection as a\nbinary classification task. Focusing specifically on Subtask A\n(Monolingual-English) within the SemEval-2024 competition framework, our\nproposed system achieves an accuracy of 78.9% on the test dataset, positioning\nus at 57th among participants. Our study addresses this challenge while\nconsidering the limited hardware resources, resulting in a system that excels\nat identifying human-written texts but encounters challenges in accurately\ndiscerning MGTs."
        ],
        "neg": []
    },
    {
        "query": "Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.",
        "pos": [
            "Many datasets have been developed to train and evaluate document-level\nrelation extraction (RE) models. Most of these are constructed using real-world\ndata. It has been shown that RE models trained on real-world data suffer from\nfactual biases. To evaluate and address this issue, we present CovEReD, a\ncounterfactual data generation approach for document-level relation extraction\ndatasets using entity replacement. We first demonstrate that models trained on\nfactual data exhibit inconsistent behavior: while they accurately extract\ntriples from factual data, they fail to extract the same triples after\ncounterfactual modification. This inconsistency suggests that models trained on\nfactual data rely on spurious signals such as specific entities and external\nknowledge $\\unicode{x2013}$ rather than on the input context $\\unicode{x2013}$\nto extract triples. We show that by generating document-level counterfactual\ndata with CovEReD and training models on them, consistency is maintained with\nminimal impact on RE performance. We release our CovEReD pipeline as well as\nRe-DocRED-CF, a dataset of counterfactual RE documents, to assist in evaluating\nand addressing inconsistency in document-level RE."
        ],
        "neg": []
    },
    {
        "query": "Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.",
        "pos": [
            "Many datasets have been developed to train and evaluate document-level\nrelation extraction (RE) models. Most of these are constructed using real-world\ndata. It has been shown that RE models trained on real-world data suffer from\nfactual biases. To evaluate and address this issue, we present CovEReD, a\ncounterfactual data generation approach for document-level relation extraction\ndatasets using entity replacement. We first demonstrate that models trained on\nfactual data exhibit inconsistent behavior: while they accurately extract\ntriples from factual data, they fail to extract the same triples after\ncounterfactual modification. This inconsistency suggests that models trained on\nfactual data rely on spurious signals such as specific entities and external\nknowledge $\\unicode{x2013}$ rather than on the input context $\\unicode{x2013}$\nto extract triples. We show that by generating document-level counterfactual\ndata with CovEReD and training models on them, consistency is maintained with\nminimal impact on RE performance. We release our CovEReD pipeline as well as\nRe-DocRED-CF, a dataset of counterfactual RE documents, to assist in evaluating\nand addressing inconsistency in document-level RE.",
            "Decoder-only large language models (LLMs) excel in high-resource languages\nacross various tasks through few-shot or even zero-shot in-context learning\n(ICL). However, their performance often does not transfer well to low-resource\nlanguages, especially those written in non-Latin scripts. Inspired by recent\nwork that leverages transliteration in encoder-only models, we investigate\nwhether transliteration is also effective in improving LLMs' performance for\nlow-resource languages written in non-Latin scripts. To this end, we propose\nthree prompt templates, where the target-language text is represented in (1)\nits original script, (2) Latin script, or (3) both. We apply these methods to\nseveral representative LLMs of different sizes on various tasks including text\nclassification and sequential labeling. Our findings show that the\neffectiveness of transliteration varies by task type and model size. For\ninstance, all models benefit from transliterations for sequential labeling\n(with increases of up to 25%)."
        ],
        "neg": []
    },
    {
        "query": "Large language models exhibit aspects of human-level intelligence that\ncatalyze their application as human-like agents in domains such as social\nsimulations, human-machine interactions, and collaborative multi-agent systems.\nHowever, the absence of distinct personalities, such as displaying ingratiating\nbehaviors, inconsistent opinions, and uniform response patterns, diminish LLMs\nutility in practical applications. Addressing this, the development of\npersonality traits in LLMs emerges as a crucial area of research to unlock\ntheir latent potential. Existing methods to personify LLMs generally involve\nstrategies like employing stylized training data for instruction tuning or\nusing prompt engineering to simulate different personalities. These methods\nonly capture superficial linguistic styles instead of the core of personalities\nand are therefore not stable. In this study, we propose PersLLM, integrating\npsychology-grounded principles of personality: social practice, consistency,\nand dynamic development, into a comprehensive training methodology. We\nincorporate personality traits directly into the model parameters, enhancing\nthe model's resistance to induction, promoting consistency, and supporting the\ndynamic evolution of personality. Single-agent evaluation validates our\nmethod's superiority, as it produces responses more aligned with reference\npersonalities compared to other approaches. Case studies for multi-agent\ncommunication highlight its benefits in enhancing opinion consistency within\nindividual agents and fostering collaborative creativity among multiple agents\nin dialogue contexts, potentially benefiting human simulation and multi-agent\ncooperation. Additionally, human-agent interaction evaluations indicate that\nour personified models significantly enhance interactive experiences,\nunderscoring the practical implications of our research.",
        "pos": [
            "Large Language Models (LLMs) exhibit various emergent abilities. Among these\nabilities, some might reveal the internal working mechanisms of models. In this\npaper, we uncover a novel emergent capability in models: the intrinsic ability\nto perform extended sequences of calculations without relying on\nchain-of-thought step-by-step solutions. Remarkably, the most advanced models\ncan directly output the results of two-digit number additions with lengths\nextending up to 15 addends. We hypothesize that the model emerges Implicit\nDiscrete State Representations (IDSRs) within its hidden states and performs\nsymbolic calculations internally. To test this hypothesis, we design a sequence\nof experiments that look into the hidden states. Specifically, we first confirm\nthat IDSRs exist. Then, we provide interesting observations about the formation\nof IDSRs from layer, digit, and sequence perspectives. Finally, we confirm that\nmodels indeed use IDSRs to produce the final answers. However, we also discover\nthat these state representations are far from lossless in current open-sourced\nmodels, leading to inaccuracies in their final performance. Our work presents a\nnovel exploration of LLMs' symbolic calculation abilities and the underlying\nmechanisms."
        ],
        "neg": []
    },
    {
        "query": "Language development researchers are interested in comparing the process of\nlanguage learning across languages. Unfortunately, it has been difficult to\nconstruct a consistent quantitative framework for such comparisons. However,\nrecent advances in AI (Artificial Intelligence) and ML (Machine Learning) are\nproviding new methods for ASR (automatic speech recognition) and NLP (natural\nlanguage processing) that can be brought to bear on this problem. Using the\nBatchalign2 program (Liu et al., 2023), we have been transcribing and linking\ndata for the CHILDES database and have applied the UD (Universal Dependencies)\nframework to provide a consistent and comparable morphosyntactic analysis for\n27 languages. These new resources open possibilities for deeper crosslinguistic\nstudy of language learning.",
        "pos": [
            "Typical schemes for automated red-teaming large language models (LLMs) focus\non discovering prompts that trigger a frozen language model (the defender) to\ngenerate toxic text. This often results in the prompting model (the adversary)\nproducing text that is unintelligible and unlikely to arise. Here, we propose a\nreinforcement learning formulation of the LLM red-teaming task which allows us\nto discover prompts that both (1) trigger toxic outputs from a frozen defender\nand (2) have low perplexity as scored by the defender. We argue these cases are\nmost pertinent in a red-teaming setting because of their likelihood to arise\nduring normal use of the defender model. We solve this formulation through a\nnovel online and weakly supervised variant of Identity Preference Optimization\n(IPO) on GPT-2 and GPT-2 XL defenders. We demonstrate that our policy is\ncapable of generating likely prompts that also trigger toxicity. Finally, we\nqualitatively analyze learned strategies, trade-offs of likelihood and\ntoxicity, and discuss implications. Source code is available for this project\nat: https://github.com/sisl/ASTPrompter/."
        ],
        "neg": []
    },
    {
        "query": "Recently, large language models (LLMs) and multimodal large language models\n(MLLMs) have demonstrated promising results on document visual question\nanswering (VQA) task, particularly after training on document instruction\ndatasets. An effective evaluation method for document instruction data is\ncrucial in constructing instruction data with high efficacy, which, in turn,\nfacilitates the training of LLMs and MLLMs for document VQA. However, most\nexisting evaluation methods for instruction data are limited to the textual\ncontent of the instructions themselves, thereby hindering the effective\nassessment of document instruction datasets and constraining their\nconstruction. In this paper, we propose ProcTag, a data-oriented method that\nassesses the efficacy of document instruction data. ProcTag innovatively\nperforms tagging on the execution process of instructions rather than the\ninstruction text itself. By leveraging the diversity and complexity of these\ntags to assess the efficacy of the given dataset, ProcTag enables selective\nsampling or filtering of document instructions. Furthermore, DocLayPrompt, a\nnovel semi-structured layout-aware document prompting strategy, is proposed for\neffectively representing documents. Experiments demonstrate that sampling\nexisting open-sourced and generated document VQA/instruction datasets with\nProcTag significantly outperforms current methods for evaluating instruction\ndata. Impressively, with ProcTag-based sampling in the generated document\ndatasets, only 30.5\\% of the document instructions are required to achieve\n100\\% efficacy compared to the complete dataset. The code is publicly available\nat\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/ProcTag.",
        "pos": [
            "Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building."
        ],
        "neg": []
    },
    {
        "query": "Personality psychologists have analyzed the relationship between personality\nand safety behaviors in human society. Although Large Language Models (LLMs)\ndemonstrate personality traits, the relationship between personality traits and\nsafety abilities in LLMs still remains a mystery. In this paper, we discover\nthat LLMs' personality traits are closely related to their safety abilities,\ni.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.\nMeanwhile, the safety alignment generally increases various LLMs' Extraversion,\nSensing, and Judging traits. According to such findings, we can edit LLMs'\npersonality traits and improve their safety performance, e.g., inducing\npersonality from ISTJ to ISTP resulted in a relative improvement of\napproximately 43% and 10% in privacy and fairness performance, respectively.\nAdditionally, we find that LLMs with different personality traits are\ndifferentially susceptible to jailbreak. This study pioneers the investigation\nof LLM safety from a personality perspective, providing new insights into LLM\nsafety enhancement.",
        "pos": [
            "Despite the substantial success of Information Retrieval (IR) in various NLP\ntasks, most IR systems predominantly handle queries and corpora in natural\nlanguage, neglecting the domain of code retrieval. Code retrieval is critically\nimportant yet remains under-explored, with existing methods and benchmarks\ninadequately representing the diversity of code in various domains and tasks.\nAddressing this gap, we present \\textbf{\\name} (\\textbf{Co}de\n\\textbf{I}nformation \\textbf{R}etrieval Benchmark), a robust and comprehensive\nbenchmark specifically designed to assess code retrieval capabilities. \\name\ncomprises \\textbf{ten} meticulously curated code datasets, spanning\n\\textbf{eight} distinctive retrieval tasks across \\textbf{seven} diverse\ndomains. We first discuss the construction of \\name and its diverse dataset\ncomposition. Further, we evaluate nine widely used retrieval models using\n\\name, uncovering significant difficulties in performing code retrieval tasks\neven with state-of-the-art systems. To facilitate easy adoption and integration\nwithin existing research workflows, \\name has been developed as a user-friendly\nPython framework, readily installable via pip. It shares same data schema as\nother popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark\nevaluations. Through \\name, we aim to invigorate research in the code retrieval\ndomain, providing a versatile benchmarking tool that encourages further\ndevelopment and exploration of code retrieval systems\\footnote{\\url{\nhttps://github.com/CoIR-team/coir}}."
        ],
        "neg": []
    },
    {
        "query": "We tackle the task of text-to-speech (TTS) in Hebrew. Traditional Hebrew\ncontains Diacritics, which dictate the way individuals should pronounce given\nwords, however, modern Hebrew rarely uses them. The lack of diacritics in\nmodern Hebrew results in readers expected to conclude the correct pronunciation\nand understand which phonemes to use based on the context. This imposes a\nfundamental challenge on TTS systems to accurately map between text-to-speech.\nIn this work, we propose to adopt a language modeling Diacritics-Free approach,\nfor the task of Hebrew TTS. The model operates on discrete speech\nrepresentations and is conditioned on a word-piece tokenizer. We optimize the\nproposed method using in-the-wild weakly supervised data and compare it to\nseveral diacritic-based TTS systems. Results suggest the proposed method is\nsuperior to the evaluated baselines considering both content preservation and\nnaturalness of the generated speech. Samples can be found under the following\nlink: pages.cs.huji.ac.il/adiyoss-lab/HebTTS/",
        "pos": [
            "We present HebDB, a weakly supervised dataset for spoken language processing\nin the Hebrew language. HebDB offers roughly 2500 hours of natural and\nspontaneous speech recordings in the Hebrew language, consisting of a large\nvariety of speakers and topics. We provide raw recordings together with a\npre-processed, weakly supervised, and filtered version. The goal of HebDB is to\nfurther enhance research and development of spoken language processing tools\nfor the Hebrew language. Hence, we additionally provide two baseline systems\nfor Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a\nfully supervised model. We present the performance of these two methods\noptimized on HebDB and compare them to current multi-lingual ASR alternatives.\nResults suggest the proposed method reaches better results than the evaluated\nbaselines considering similar model sizes. Dataset, code, and models are\npublicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/."
        ],
        "neg": []
    },
    {
        "query": "We tackle the task of text-to-speech (TTS) in Hebrew. Traditional Hebrew\ncontains Diacritics, which dictate the way individuals should pronounce given\nwords, however, modern Hebrew rarely uses them. The lack of diacritics in\nmodern Hebrew results in readers expected to conclude the correct pronunciation\nand understand which phonemes to use based on the context. This imposes a\nfundamental challenge on TTS systems to accurately map between text-to-speech.\nIn this work, we propose to adopt a language modeling Diacritics-Free approach,\nfor the task of Hebrew TTS. The model operates on discrete speech\nrepresentations and is conditioned on a word-piece tokenizer. We optimize the\nproposed method using in-the-wild weakly supervised data and compare it to\nseveral diacritic-based TTS systems. Results suggest the proposed method is\nsuperior to the evaluated baselines considering both content preservation and\nnaturalness of the generated speech. Samples can be found under the following\nlink: pages.cs.huji.ac.il/adiyoss-lab/HebTTS/",
        "pos": [
            "We present HebDB, a weakly supervised dataset for spoken language processing\nin the Hebrew language. HebDB offers roughly 2500 hours of natural and\nspontaneous speech recordings in the Hebrew language, consisting of a large\nvariety of speakers and topics. We provide raw recordings together with a\npre-processed, weakly supervised, and filtered version. The goal of HebDB is to\nfurther enhance research and development of spoken language processing tools\nfor the Hebrew language. Hence, we additionally provide two baseline systems\nfor Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a\nfully supervised model. We present the performance of these two methods\noptimized on HebDB and compare them to current multi-lingual ASR alternatives.\nResults suggest the proposed method reaches better results than the evaluated\nbaselines considering similar model sizes. Dataset, code, and models are\npublicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/."
        ],
        "neg": []
    },
    {
        "query": "In the field of emotion analysis, much NLP research focuses on identifying a\nlimited number of discrete emotion categories, often applied across languages.\nThese basic sets, however, are rarely designed with textual data in mind, and\nculture, language, and dialect can influence how particular emotions are\ninterpreted. In this work, we broaden our scope to a practically unbounded set\nof \\textit{affective states}, which includes any terms that humans use to\ndescribe their experiences of feeling. We collect and publish MASIVE, a dataset\nof Reddit posts in English and Spanish containing over 1,000 unique affective\nstates each. We then define the new problem of \\textit{affective state\nidentification} for language generation models framed as a masked span\nprediction task. On this task, we find that smaller finetuned multilingual\nmodels outperform much larger LLMs, even on region-specific Spanish affective\nstates. Additionally, we show that pretraining on MASIVE improves model\nperformance on existing emotion benchmarks. Finally, through machine\ntranslation experiments, we find that native speaker-written data is vital to\ngood performance on this task.",
        "pos": [
            "Human evaluation has been the gold standard for checking faithfulness in\nabstractive summarization. However, with a challenging source domain like\nnarrative, multiple annotators can agree a summary is faithful, while missing\ndetails that are obvious errors only once pointed out. We therefore introduce a\nnew dataset, STORYSUMM, comprising LLM summaries of short stories with\nlocalized faithfulness labels and error explanations. This benchmark is for\nevaluation methods, testing whether a given method can detect challenging\ninconsistencies. Using this dataset, we first show that any one human\nannotation protocol is likely to miss inconsistencies, and we advocate for\npursuing a range of methods when establishing ground truth for a summarization\ndataset. We finally test recent automatic metrics and find that none of them\nachieve more than 70% balanced accuracy on this task, demonstrating that it is\na challenging benchmark for future work in faithfulness evaluation.",
            "Prior research has enhanced the ability of Large Language Models (LLMs) to\nsolve logic puzzles using techniques such as chain-of-thought prompting or\nintroducing a symbolic representation. These frameworks are still usually\ninsufficient to solve complicated logical problems, such as Zebra puzzles, due\nto the inherent complexity of translating natural language clues into logical\nstatements. We introduce a multi-agent system, ZPS, that integrates LLMs with\nan off the shelf theorem prover. This system tackles the complex puzzle-solving\ntask by breaking down the problem into smaller, manageable parts, generating\nSMT (Satisfiability Modulo Theories) code to solve them with a theorem prover,\nand using feedback between the agents to repeatedly improve their answers. We\nalso introduce an automated grid puzzle grader to assess the correctness of our\npuzzle solutions and show that the automated grader is reliable by evaluating\nit in a user-study. Our approach shows improvement in all three LLMs we tested,\nwith GPT-4 showing 166% improvement in the number of fully correct solutions."
        ],
        "neg": []
    },
    {
        "query": "A common way to extend the memory of large language models (LLMs) is by\nretrieval augmented generation (RAG), which inserts text retrieved from a\nlarger memory into an LLM's context window. However, the context window is\ntypically limited to several thousand tokens, which limits the number of\nretrieved passages that can inform a model's response. For this reason, it's\nimportant to avoid occupying context window space with redundant information by\nensuring a degree of diversity among retrieved passages. At the same time, the\ninformation should also be relevant to the current task. Most prior methods\nthat encourage diversity among retrieved results, such as Maximal Marginal\nRelevance (MMR), do so by incorporating an objective that explicitly trades off\ndiversity and relevance. We propose a novel simple optimization metric based on\nrelevant information gain, a probabilistic measure of the total information\nrelevant to a query for a set of retrieved results. By optimizing this metric,\ndiversity organically emerges from our system. When used as a drop-in\nreplacement for the retrieval component of a RAG system, this method yields\nstate-of-the-art performance on question answering tasks from the Retrieval\nAugmented Generation Benchmark (RGB), outperforming existing metrics that\ndirectly optimize for relevance and diversity.",
        "pos": [
            "Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel."
        ],
        "neg": []
    },
    {
        "query": "We introduce an approach to identifying speaker names in dialogue\ntranscripts, a crucial task for enhancing content accessibility and\nsearchability in digital media archives. Despite the advancements in speech\nrecognition, the task of text-based speaker identification (SpeakerID) has\nreceived limited attention, lacking large-scale, diverse datasets for effective\nmodel training. Addressing these gaps, we present a novel, large-scale dataset\nderived from the MediaSum corpus, encompassing transcripts from a wide range of\nmedia sources. We propose novel transformer-based models tailored for\nSpeakerID, leveraging contextual cues within dialogues to accurately attribute\nspeaker names. Through extensive experiments, our best model achieves a great\nprecision of 80.3\\%, setting a new benchmark for SpeakerID. The data and code\nare publicly available here:\n\\url{https://github.com/adobe-research/speaker-identification}",
        "pos": [
            "Extractive summarization plays a pivotal role in natural language processing\ndue to its wide-range applications in summarizing diverse content efficiently,\nwhile also being faithful to the original content. Despite significant\nadvancement achieved in extractive summarization by Large Language Models\n(LLMs), these summaries frequently exhibit incoherence. An important aspect of\nthe coherent summary is its readability for intended users. Although there have\nbeen many datasets and benchmarks proposed for creating coherent extractive\nsummaries, none of them currently incorporate user intent to improve coherence\nin extractive summarization. Motivated by this, we propose a systematically\ncreated human-annotated dataset consisting of coherent summaries for five\npublicly available datasets and natural language user feedback, offering\nvaluable insights into how to improve coherence in extractive summaries. We\nutilize this dataset for aligning LLMs through supervised fine-tuning with\nnatural language human feedback to enhance the coherence of their generated\nsummaries. Preliminary experiments with Falcon-40B and Llama-2-13B show\nsignificant performance improvements (~10% Rouge-L) in terms of producing\ncoherent summaries. We further utilize human feedback to benchmark results over\ninstruction-tuned models such as FLAN-T5 which resulted in several interesting\nfindings. Data and source code are available at\nhttps://github.com/Mihir3009/Extract-AI."
        ],
        "neg": []
    },
    {
        "query": "We introduce an approach to identifying speaker names in dialogue\ntranscripts, a crucial task for enhancing content accessibility and\nsearchability in digital media archives. Despite the advancements in speech\nrecognition, the task of text-based speaker identification (SpeakerID) has\nreceived limited attention, lacking large-scale, diverse datasets for effective\nmodel training. Addressing these gaps, we present a novel, large-scale dataset\nderived from the MediaSum corpus, encompassing transcripts from a wide range of\nmedia sources. We propose novel transformer-based models tailored for\nSpeakerID, leveraging contextual cues within dialogues to accurately attribute\nspeaker names. Through extensive experiments, our best model achieves a great\nprecision of 80.3\\%, setting a new benchmark for SpeakerID. The data and code\nare publicly available here:\n\\url{https://github.com/adobe-research/speaker-identification}",
        "pos": [
            "Extractive summarization plays a pivotal role in natural language processing\ndue to its wide-range applications in summarizing diverse content efficiently,\nwhile also being faithful to the original content. Despite significant\nadvancement achieved in extractive summarization by Large Language Models\n(LLMs), these summaries frequently exhibit incoherence. An important aspect of\nthe coherent summary is its readability for intended users. Although there have\nbeen many datasets and benchmarks proposed for creating coherent extractive\nsummaries, none of them currently incorporate user intent to improve coherence\nin extractive summarization. Motivated by this, we propose a systematically\ncreated human-annotated dataset consisting of coherent summaries for five\npublicly available datasets and natural language user feedback, offering\nvaluable insights into how to improve coherence in extractive summaries. We\nutilize this dataset for aligning LLMs through supervised fine-tuning with\nnatural language human feedback to enhance the coherence of their generated\nsummaries. Preliminary experiments with Falcon-40B and Llama-2-13B show\nsignificant performance improvements (~10% Rouge-L) in terms of producing\ncoherent summaries. We further utilize human feedback to benchmark results over\ninstruction-tuned models such as FLAN-T5 which resulted in several interesting\nfindings. Data and source code are available at\nhttps://github.com/Mihir3009/Extract-AI."
        ],
        "neg": []
    },
    {
        "query": "We introduce an approach to identifying speaker names in dialogue\ntranscripts, a crucial task for enhancing content accessibility and\nsearchability in digital media archives. Despite the advancements in speech\nrecognition, the task of text-based speaker identification (SpeakerID) has\nreceived limited attention, lacking large-scale, diverse datasets for effective\nmodel training. Addressing these gaps, we present a novel, large-scale dataset\nderived from the MediaSum corpus, encompassing transcripts from a wide range of\nmedia sources. We propose novel transformer-based models tailored for\nSpeakerID, leveraging contextual cues within dialogues to accurately attribute\nspeaker names. Through extensive experiments, our best model achieves a great\nprecision of 80.3\\%, setting a new benchmark for SpeakerID. The data and code\nare publicly available here:\n\\url{https://github.com/adobe-research/speaker-identification}",
        "pos": [
            "Extractive summarization plays a pivotal role in natural language processing\ndue to its wide-range applications in summarizing diverse content efficiently,\nwhile also being faithful to the original content. Despite significant\nadvancement achieved in extractive summarization by Large Language Models\n(LLMs), these summaries frequently exhibit incoherence. An important aspect of\nthe coherent summary is its readability for intended users. Although there have\nbeen many datasets and benchmarks proposed for creating coherent extractive\nsummaries, none of them currently incorporate user intent to improve coherence\nin extractive summarization. Motivated by this, we propose a systematically\ncreated human-annotated dataset consisting of coherent summaries for five\npublicly available datasets and natural language user feedback, offering\nvaluable insights into how to improve coherence in extractive summaries. We\nutilize this dataset for aligning LLMs through supervised fine-tuning with\nnatural language human feedback to enhance the coherence of their generated\nsummaries. Preliminary experiments with Falcon-40B and Llama-2-13B show\nsignificant performance improvements (~10% Rouge-L) in terms of producing\ncoherent summaries. We further utilize human feedback to benchmark results over\ninstruction-tuned models such as FLAN-T5 which resulted in several interesting\nfindings. Data and source code are available at\nhttps://github.com/Mihir3009/Extract-AI."
        ],
        "neg": []
    },
    {
        "query": "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io.",
        "pos": [
            "Literature search questions, such as \"where can I find research on the\nevaluation of consistency in generated summaries?\" pose significant challenges\nfor modern search engines and retrieval systems. These questions often require\na deep understanding of research concepts and the ability to reason over entire\narticles. In this work, we introduce LitSearch, a retrieval benchmark\ncomprising 597 realistic literature search queries about recent ML and NLP\npapers. LitSearch is constructed using a combination of (1) questions generated\nby GPT-4 based on paragraphs containing inline citations from research papers\nand (2) questions about recently published papers, manually written by their\nauthors. All LitSearch questions were manually examined or edited by experts to\nensure high quality. We extensively benchmark state-of-the-art retrieval models\nand also evaluate two LLM-based reranking pipelines. We find a significant\nperformance gap between BM25 and state-of-the-art dense retrievers, with a\n24.8% difference in absolute recall@5. The LLM-based reranking strategies\nfurther improve the best-performing dense retriever by 4.4%. Additionally,\ncommercial search engines and research tools like Google Search perform poorly\non LitSearch, lagging behind the best dense retriever by 32 points. Taken\ntogether, these results show that LitSearch is an informative new testbed for\nretrieval systems while catering to a real-world use case."
        ],
        "neg": []
    },
    {
        "query": "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io.",
        "pos": [
            "Scaling laws with respect to the amount of training data and the number of\nparameters allow us to predict the cost-benefit trade-offs of pretraining\nlanguage models (LMs) in different configurations. In this paper, we consider\nanother dimension of scaling: the amount of data available at inference time.\nSpecifically, we find that increasing the size of the datastore used by a\nretrieval-based LM monotonically improves language modeling and several\ndownstream tasks without obvious saturation, such that a smaller model\naugmented with a large datastore outperforms a larger LM-only model on\nknowledge-intensive tasks. By plotting compute-optimal scaling curves with\nvaried datastore, model, and pretraining data sizes, we show that using larger\ndatastores can significantly improve model performance for the same training\ncompute budget. We carry out our study by constructing a 1.4 trillion-token\ndatastore named MassiveDS, which is the largest and the most diverse\nopen-sourced datastore for retrieval-based LMs to date, and designing an\nefficient pipeline for studying datastore scaling in a computationally\naccessible manner. Finally, we analyze the effect of improving the retriever,\ndatastore quality filtering, and other design choices on our observed scaling\ntrends. Overall, our results show that datastore size should be considered as\nan integral part of LM efficiency and performance trade-offs. To facilitate\nfuture research, we open-source our datastore and code at\nhttps://github.com/RulinShao/retrieval-scaling.",
            "RLHF-aligned LMs have shown unprecedented ability on both benchmarks and\nlong-form text generation, yet they struggle with one foundational task:\nnext-token prediction. As RLHF models become agent models aimed at interacting\nwith humans, they seem to lose their world modeling -- the ability to predict\nwhat comes next in arbitrary documents, which is the foundational training\nobjective of the Base LMs that RLHF adapts.\n  Besides empirically demonstrating this trade-off, we propose a potential\nexplanation: to perform coherent long-form generation, RLHF models restrict\nrandomness via implicit blueprints. In particular, RLHF models concentrate\nprobability on sets of anchor spans that co-occur across multiple generations\nfor the same prompt, serving as textual scaffolding but also limiting a model's\nability to generate documents that do not include these spans. We study this\ntrade-off on the most effective current agent models, those aligned with RLHF,\nwhile exploring why this may remain a fundamental trade-off between models that\nact and those that predict, even as alignment techniques improve."
        ],
        "neg": []
    },
    {
        "query": "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io.",
        "pos": [
            "Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io."
        ],
        "neg": []
    },
    {
        "query": "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io.",
        "pos": [
            "Transformer-based chatbots can conduct fluent, natural-sounding\nconversations, but we have limited understanding of the mechanisms underlying\ntheir behavior. Prior work has taken a bottom-up approach to understanding\nTransformers by constructing Transformers for various synthetic and formal\nlanguage tasks, such as regular expressions and Dyck languages. However, it is\nnot obvious how to extend this approach to understand more naturalistic\nconversational agents. In this work, we take a step in this direction by\nconstructing a Transformer that implements the ELIZA program, a classic,\nrule-based chatbot. ELIZA illustrates some of the distinctive challenges of the\nconversational setting, including both local pattern matching and long-term\ndialog state tracking. We build on constructions from prior work -- in\nparticular, for simulating finite-state automata -- showing how simpler\nconstructions can be composed and extended to give rise to more sophisticated\nbehavior. Next, we train Transformers on a dataset of synthetically generated\nELIZA conversations and investigate the mechanisms the models learn. Our\nanalysis illustrates the kinds of mechanisms these models tend to prefer -- for\nexample, models favor an induction head mechanism over a more precise, position\nbased copying mechanism; and using intermediate generations to simulate\nrecurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing\nan explicit connection between neural chatbots and interpretable, symbolic\nmechanisms, our results offer a new setting for mechanistic analysis of\nconversational agents.",
            "Literature search questions, such as \"where can I find research on the\nevaluation of consistency in generated summaries?\" pose significant challenges\nfor modern search engines and retrieval systems. These questions often require\na deep understanding of research concepts and the ability to reason over entire\narticles. In this work, we introduce LitSearch, a retrieval benchmark\ncomprising 597 realistic literature search queries about recent ML and NLP\npapers. LitSearch is constructed using a combination of (1) questions generated\nby GPT-4 based on paragraphs containing inline citations from research papers\nand (2) questions about recently published papers, manually written by their\nauthors. All LitSearch questions were manually examined or edited by experts to\nensure high quality. We extensively benchmark state-of-the-art retrieval models\nand also evaluate two LLM-based reranking pipelines. We find a significant\nperformance gap between BM25 and state-of-the-art dense retrievers, with a\n24.8% difference in absolute recall@5. The LLM-based reranking strategies\nfurther improve the best-performing dense retriever by 4.4%. Additionally,\ncommercial search engines and research tools like Google Search perform poorly\non LitSearch, lagging behind the best dense retriever by 32 points. Taken\ntogether, these results show that LitSearch is an informative new testbed for\nretrieval systems while catering to a real-world use case."
        ],
        "neg": []
    },
    {
        "query": "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io.",
        "pos": [
            "Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io."
        ],
        "neg": []
    },
    {
        "query": "Meeting summarization has become a critical task since digital encounters\nhave become a common practice. Large language models (LLMs) show great\npotential in summarization, offering enhanced coherence and context\nunderstanding compared to traditional methods. However, they still struggle to\nmaintain relevance and avoid hallucination. We introduce a multi-LLM correction\napproach for meeting summarization using a two-phase process that mimics the\nhuman review process: mistake identification and summary refinement. We release\nQMSum Mistake, a dataset of 200 automatically generated meeting summaries\nannotated by humans on nine error types, including structural, omission, and\nirrelevance errors. Our experiments show that these errors can be identified\nwith high accuracy by an LLM. We transform identified mistakes into actionable\nfeedback to improve the quality of a given summary measured by relevance,\ninformativeness, conciseness, and coherence. This post-hoc refinement\neffectively improves summary quality by leveraging multiple LLMs to validate\noutput quality. Our multi-LLM approach for meeting summarization shows\npotential for similar complex text generation tasks requiring robustness,\naction planning, and discussion towards a goal.",
        "pos": [
            "Paraphrases represent a human's intuitive ability to understand expressions\npresented in various different ways. Current paraphrase evaluations of language\nmodels primarily use binary approaches, offering limited interpretability of\nspecific text changes. Atomic paraphrase types (APT) decompose paraphrases into\ndifferent linguistic changes and offer a granular view of the flexibility in\nlinguistic expression (e.g., a shift in syntax or vocabulary used). In this\nstudy, we assess the human preferences towards ChatGPT in generating English\nparaphrases with ten APTs and five prompting techniques. We introduce APTY\n(Atomic Paraphrase TYpes), a dataset of 500 sentence-level and word-level\nannotations by 15 annotators. The dataset also provides a human preference\nranking of paraphrases with different types that can be used to fine-tune\nmodels with RLHF and DPO methods. Our results reveal that ChatGPT can generate\nsimple APTs, such as additions and deletions, but struggle with complex\nstructures (e.g., subordination changes). This study contributes to\nunderstanding which aspects of paraphrasing language models have already\nsucceeded at understanding and what remains elusive. In addition, our curated\ndatasets can be used to develop language models with specific linguistic\ncapabilities."
        ],
        "neg": []
    },
    {
        "query": "Meeting summarization has become a critical task since digital encounters\nhave become a common practice. Large language models (LLMs) show great\npotential in summarization, offering enhanced coherence and context\nunderstanding compared to traditional methods. However, they still struggle to\nmaintain relevance and avoid hallucination. We introduce a multi-LLM correction\napproach for meeting summarization using a two-phase process that mimics the\nhuman review process: mistake identification and summary refinement. We release\nQMSum Mistake, a dataset of 200 automatically generated meeting summaries\nannotated by humans on nine error types, including structural, omission, and\nirrelevance errors. Our experiments show that these errors can be identified\nwith high accuracy by an LLM. We transform identified mistakes into actionable\nfeedback to improve the quality of a given summary measured by relevance,\ninformativeness, conciseness, and coherence. This post-hoc refinement\neffectively improves summary quality by leveraging multiple LLMs to validate\noutput quality. Our multi-LLM approach for meeting summarization shows\npotential for similar complex text generation tasks requiring robustness,\naction planning, and discussion towards a goal.",
        "pos": [
            "Paraphrases represent a human's intuitive ability to understand expressions\npresented in various different ways. Current paraphrase evaluations of language\nmodels primarily use binary approaches, offering limited interpretability of\nspecific text changes. Atomic paraphrase types (APT) decompose paraphrases into\ndifferent linguistic changes and offer a granular view of the flexibility in\nlinguistic expression (e.g., a shift in syntax or vocabulary used). In this\nstudy, we assess the human preferences towards ChatGPT in generating English\nparaphrases with ten APTs and five prompting techniques. We introduce APTY\n(Atomic Paraphrase TYpes), a dataset of 500 sentence-level and word-level\nannotations by 15 annotators. The dataset also provides a human preference\nranking of paraphrases with different types that can be used to fine-tune\nmodels with RLHF and DPO methods. Our results reveal that ChatGPT can generate\nsimple APTs, such as additions and deletions, but struggle with complex\nstructures (e.g., subordination changes). This study contributes to\nunderstanding which aspects of paraphrasing language models have already\nsucceeded at understanding and what remains elusive. In addition, our curated\ndatasets can be used to develop language models with specific linguistic\ncapabilities."
        ],
        "neg": []
    },
    {
        "query": "Sign language translation (SLT) addresses the problem of translating\ninformation from a sign language in video to a spoken language in text.\nExisting studies, while showing progress, are often limited to narrow domains\nand/or few sign languages and struggle with open-domain tasks. In this paper,\nwe push forward the frontier of SLT by scaling pretraining data, model size,\nand number of translation directions. We perform large-scale SLT pretraining on\ndifferent data including 1) noisy multilingual YouTube SLT data, 2) parallel\ntext corpora, and 3) SLT data augmented by translating video captions to other\nlanguages with off-the-shelf machine translation models. We unify different\npretraining tasks with task-specific prompts under the encoder-decoder\narchitecture, and initialize the SLT model with pretrained (m/By)T5 models\nacross model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL\nto 42 spoken languages) demonstrate the significance of data/model scaling and\ncross-lingual cross-modal transfer, as well as the feasibility of zero-shot\nSLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT\nbenchmarks covering 5 sign languages. Experiments show substantial quality\nimprovements over the vanilla baselines, surpassing the previous\nstate-of-the-art (SOTA) by wide margins.",
        "pos": [
            "Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) encode vast amounts of world knowledge acquired\nvia training on large web-scale datasets crawled from the internet. However,\nthese datasets typically exhibit a geographical bias towards English-speaking\nWestern countries. This results in LLMs producing biased or hallucinated\nresponses to queries that require answers localized to other geographical\nregions. In this work, we introduce a new benchmark named LoFTI (Localization\nand Factuality Transfer to Indian Locales) that can be used to evaluate an\nLLM's localization and factual text transfer capabilities. LoFTI consists of\nfactual statements about entities in source and target locations; the source\nlocations are spread across the globe and the target locations are all within\nIndia with varying degrees of hyperlocality (country, states, cities). The\nentities span a wide variety of categories. We use LoFTI to evaluate Mixtral,\nGPT-4 and two other Mixtral-based approaches well-suited to the task of\nlocalized factual transfer. We demonstrate that LoFTI is a high-quality\nevaluation benchmark and all the models, including GPT-4, produce skewed\nresults across varying levels of hyperlocality.",
        "pos": [
            "Large language models (LLMs) are very proficient text generators. We leverage\nthis capability of LLMs to generate task-specific data via zero-shot prompting\nand promote cross-lingual transfer for low-resource target languages. Given\ntask-specific data in a source language and a teacher model trained on this\ndata, we propose using this teacher to label LLM generations and employ a set\nof simple data selection strategies that use the teacher's label probabilities.\nOur data selection strategies help us identify a representative subset of\ndiverse generations that help boost zero-shot accuracies while being efficient,\nin comparison to using all the LLM generations (without any subset selection).\nWe also highlight other important design choices that affect cross-lingual\nperformance such as the use of translations of source data and what labels are\nbest to use for the LLM generations. We observe significant performance gains\nacross sentiment analysis and natural language inference tasks (of up to a\nmaximum of 7.13 absolute points and 1.5 absolute points on average) across a\nnumber of target languages (Hindi, Marathi, Urdu, Swahili) and domains.",
            "Speech accents present a serious challenge to the performance of\nstate-of-the-art end-to-end Automatic Speech Recognition (ASR) systems. Even\nwith self-supervised learning and pre-training of ASR models, accent invariance\nis seldom achieved. In this work, we propose an accent-aware adaptation\ntechnique for self-supervised learning that introduces a trainable set of\naccent-specific codebooks to the self-supervised architecture. These learnable\ncodebooks enable the model to capture accent specific information during\npre-training, that is further refined during ASR finetuning. On the Mozilla\nCommon Voice dataset, our proposed approach outperforms all other\naccent-adaptation approaches on both seen and unseen English accents, with up\nto 9% relative reduction in word error rate (WER).",
            "Convolutions have become essential in state-of-the-art end-to-end Automatic\nSpeech Recognition~(ASR) systems due to their efficient modelling of local\ncontext. Notably, its use in Conformers has led to superior performance\ncompared to vanilla Transformer-based ASR systems. While components other than\nthe convolution module in the Conformer have been reexamined, altering the\nconvolution module itself has been far less explored. Towards this, we\nintroduce Multi-Convformer that uses multiple convolution kernels within the\nconvolution module of the Conformer in conjunction with gating. This helps in\nimproved modeling of local dependencies at varying granularities. Our model\nrivals existing Conformer variants such as CgMLP and E-Branchformer in\nperformance, while being more parameter efficient. We empirically compare our\napproach with Conformer and its variants across four different datasets and\nthree different modelling paradigms and show up to 8% relative word error\nrate~(WER) improvements."
        ],
        "neg": []
    },
    {
        "query": "Inference of Large Language Models (LLMs) across computer clusters has become\na focal point of research in recent times, with many acceleration techniques\ntaking inspiration from CPU speculative execution. These techniques reduce\nbottlenecks associated with memory bandwidth, but also increase end-to-end\nlatency per inference run, requiring high speculation acceptance rates to\nimprove performance. Combined with a variable rate of acceptance across tasks,\nspeculative inference techniques can result in reduced performance.\nAdditionally, pipeline-parallel designs require many user requests to maintain\nmaximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative\nacceleration technique to reduce inter-token latency and improve system\nutilization for single-request scenarios while also improving tolerance to low\nspeculation acceptance rates and low-bandwidth interconnects. PipeInfer\nexhibits up to a 2.15$\\times$ improvement in generation speed over standard\nspeculative inference. PipeInfer achieves its improvement through Continuous\nAsynchronous Speculation and Early Inference Cancellation, the former improving\nlatency and generation speed by running single-token inference simultaneously\nwith several speculative runs, while the latter improves speed and latency by\nskipping the computation of invalidated runs, even in the middle of inference.",
        "pos": [
            "Large Language Models (LLMs) have been applied to many research problems\nacross various domains. One of the applications of LLMs is providing\nquestion-answering systems that cater to users from different fields. The\neffectiveness of LLM-based question-answering systems has already been\nestablished at an acceptable level for users posing questions in popular and\npublic domains such as trivia and literature. However, it has not often been\nestablished in niche domains that traditionally require specialized expertise.\nTo this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performance\nof three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answering\nquestions originating from Environmental Impact Statements prepared by U.S.\nfederal government agencies in accordance with the National Environmental\nEnvironmental Act (NEPA). We specifically measure the ability of LLMs to\nunderstand the nuances of legal, technical, and compliance-related information\npresent in NEPA documents in different contextual scenarios. For example, we\ntest the LLMs' internal prior NEPA knowledge by providing questions without any\ncontext, as well as assess how LLMs synthesize the contextual information\npresent in long NEPA documents to facilitate the question/answering task. We\ncompare the performance of the long context LLMs and RAG powered models in\nhandling different types of questions (e.g., problem-solving, divergent). Our\nresults suggest that RAG powered models significantly outperform the long\ncontext models in the answer accuracy regardless of the choice of the frontier\nLLM. Our further analysis reveals that many models perform better answering\nclosed questions than divergent and problem-solving questions."
        ],
        "neg": []
    },
    {
        "query": "Despite being heralded as the new standard for dialogue evaluation, the\nclosed-source nature of GPT-4 poses challenges for the community. Motivated by\nthe need for lightweight, open source, and multilingual dialogue evaluators,\nthis paper introduces GenResCoh (Generated Responses targeting Coherence).\nGenResCoh is a novel LLM generated dataset comprising over 130k negative and\npositive responses and accompanying explanations seeded from XDailyDialog and\nXPersona covering English, French, German, Italian, and Chinese. Leveraging\nGenResCoh, we propose ECoh (Evaluation of Coherence), a family of evaluators\ntrained to assess response coherence across multiple languages. Experimental\nresults demonstrate that ECoh achieves multilingual detection capabilities\nsuperior to the teacher model (GPT-3.5-Turbo) on GenResCoh, despite being based\non a much smaller architecture. Furthermore, the explanations provided by ECoh\nclosely align in terms of quality with those generated by the teacher model.",
        "pos": [
            "Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots."
        ],
        "neg": []
    },
    {
        "query": "Despite being heralded as the new standard for dialogue evaluation, the\nclosed-source nature of GPT-4 poses challenges for the community. Motivated by\nthe need for lightweight, open source, and multilingual dialogue evaluators,\nthis paper introduces GenResCoh (Generated Responses targeting Coherence).\nGenResCoh is a novel LLM generated dataset comprising over 130k negative and\npositive responses and accompanying explanations seeded from XDailyDialog and\nXPersona covering English, French, German, Italian, and Chinese. Leveraging\nGenResCoh, we propose ECoh (Evaluation of Coherence), a family of evaluators\ntrained to assess response coherence across multiple languages. Experimental\nresults demonstrate that ECoh achieves multilingual detection capabilities\nsuperior to the teacher model (GPT-3.5-Turbo) on GenResCoh, despite being based\non a much smaller architecture. Furthermore, the explanations provided by ECoh\nclosely align in terms of quality with those generated by the teacher model.",
        "pos": [
            "Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots."
        ],
        "neg": []
    },
    {
        "query": "Despite being heralded as the new standard for dialogue evaluation, the\nclosed-source nature of GPT-4 poses challenges for the community. Motivated by\nthe need for lightweight, open source, and multilingual dialogue evaluators,\nthis paper introduces GenResCoh (Generated Responses targeting Coherence).\nGenResCoh is a novel LLM generated dataset comprising over 130k negative and\npositive responses and accompanying explanations seeded from XDailyDialog and\nXPersona covering English, French, German, Italian, and Chinese. Leveraging\nGenResCoh, we propose ECoh (Evaluation of Coherence), a family of evaluators\ntrained to assess response coherence across multiple languages. Experimental\nresults demonstrate that ECoh achieves multilingual detection capabilities\nsuperior to the teacher model (GPT-3.5-Turbo) on GenResCoh, despite being based\non a much smaller architecture. Furthermore, the explanations provided by ECoh\nclosely align in terms of quality with those generated by the teacher model.",
        "pos": [
            "Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots."
        ],
        "neg": []
    },
    {
        "query": "Tokenization - the practice of converting strings of characters over an\nalphabet into sequences of tokens over a vocabulary - is a critical yet\nunder-theorized step in the NLP pipeline. Notably, it remains the only major\nstep not fully integrated into widely used end-to-end neural models. This paper\naims to address this theoretical gap by laying the foundations of tokenization\nfrom a formal perspective. By articulating and extending basic properties about\nthe category of stochastic maps, we propose a unified framework for\nrepresenting and analyzing tokenizer models. This framework allows us to\nestablish general conditions for the use of tokenizers. In particular, we\nformally establish the necessary and sufficient conditions for a tokenizer\nmodel to preserve the consistency of statistical estimators. Additionally, we\ndiscuss statistical and computational concerns crucial for the design and\nimplementation of tokenizer models. The framework and results advanced in this\npaper represent a step toward a robust theoretical foundation for neural\nlanguage modeling.",
        "pos": [
            "Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on a controlled\ngeneration task suggest that while variational BoN is not as effective as BoN\nin aligning language models, it is close to BoN performance as vBoN appears\nmore often on the Pareto frontier of reward and KL divergence compared to\nmodels trained with KL-constrained RL objective."
        ],
        "neg": []
    },
    {
        "query": "The nascent topic of fake news requires automatic detection methods to\nquickly learn from limited annotated samples. Therefore, the capacity to\nrapidly acquire proficiency in a new task with limited guidance, also known as\nfew-shot learning, is critical for detecting fake news in its early stages.\nExisting approaches either involve fine-tuning pre-trained language models\nwhich come with a large number of parameters, or training a complex neural\nnetwork from scratch with large-scale annotated datasets. This paper presents a\nmultimodal fake news detection model which augments multimodal features using\nunimodal features. For this purpose, we introduce Cross-Modal Augmentation\n(CMA), a simple approach for enhancing few-shot multimodal fake news detection\nby transforming n-shot classification into a more robust (n $\\times$ z)-shot\nproblem, where z represents the number of supplementary features. The proposed\nCMA achieves SOTA results over three benchmark datasets, utilizing a\nsurprisingly simple linear probing method to classify multimodal fake news with\nonly a few training samples. Furthermore, our method is significantly more\nlightweight than prior approaches, particularly in terms of the number of\ntrainable parameters and epoch times. The code is available here:\n\\url{https://github.com/zgjiangtoby/FND_fewshot}",
        "pos": [
            "Large visual-language models (LVLMs) exhibit exceptional performance in\nvisual-language reasoning across diverse cross-modal benchmarks. Despite these\nadvances, recent research indicates that Large Language Models (LLMs), like\nGPT-3.5-turbo, underachieve compared to well-trained smaller models, such as\nBERT, in Fake News Detection (FND), prompting inquiries into LVLMs' efficacy in\nFND tasks. Although performance could improve through fine-tuning LVLMs, the\nsubstantial parameters and requisite pre-trained weights render it a\nresource-heavy endeavor for FND applications. This paper initially assesses the\nFND capabilities of two notable LVLMs, CogVLM and GPT4V, in comparison to a\nsmaller yet adeptly trained CLIP model in a zero-shot context. The findings\ndemonstrate that LVLMs can attain performance competitive with that of the\nsmaller model. Next, we integrate standard in-context learning (ICL) with\nLVLMs, noting improvements in FND performance, though limited in scope and\nconsistency. To address this, we introduce the \\textbf{I}n-context\n\\textbf{M}ultimodal \\textbf{F}ake \\textbf{N}ews \\textbf{D}etection (IMFND)\nframework, enriching in-context examples and test inputs with predictions and\ncorresponding probabilities from a well-trained smaller model. This strategic\nintegration directs the LVLMs' focus towards news segments associated with\nhigher probabilities, thereby improving their analytical accuracy. The\nexperimental results suggest that the IMFND framework significantly boosts the\nFND efficiency of LVLMs, achieving enhanced accuracy over the standard ICL\napproach across three publicly available FND datasets."
        ],
        "neg": []
    },
    {
        "query": "The nascent topic of fake news requires automatic detection methods to\nquickly learn from limited annotated samples. Therefore, the capacity to\nrapidly acquire proficiency in a new task with limited guidance, also known as\nfew-shot learning, is critical for detecting fake news in its early stages.\nExisting approaches either involve fine-tuning pre-trained language models\nwhich come with a large number of parameters, or training a complex neural\nnetwork from scratch with large-scale annotated datasets. This paper presents a\nmultimodal fake news detection model which augments multimodal features using\nunimodal features. For this purpose, we introduce Cross-Modal Augmentation\n(CMA), a simple approach for enhancing few-shot multimodal fake news detection\nby transforming n-shot classification into a more robust (n $\\times$ z)-shot\nproblem, where z represents the number of supplementary features. The proposed\nCMA achieves SOTA results over three benchmark datasets, utilizing a\nsurprisingly simple linear probing method to classify multimodal fake news with\nonly a few training samples. Furthermore, our method is significantly more\nlightweight than prior approaches, particularly in terms of the number of\ntrainable parameters and epoch times. The code is available here:\n\\url{https://github.com/zgjiangtoby/FND_fewshot}",
        "pos": [
            "Large visual-language models (LVLMs) exhibit exceptional performance in\nvisual-language reasoning across diverse cross-modal benchmarks. Despite these\nadvances, recent research indicates that Large Language Models (LLMs), like\nGPT-3.5-turbo, underachieve compared to well-trained smaller models, such as\nBERT, in Fake News Detection (FND), prompting inquiries into LVLMs' efficacy in\nFND tasks. Although performance could improve through fine-tuning LVLMs, the\nsubstantial parameters and requisite pre-trained weights render it a\nresource-heavy endeavor for FND applications. This paper initially assesses the\nFND capabilities of two notable LVLMs, CogVLM and GPT4V, in comparison to a\nsmaller yet adeptly trained CLIP model in a zero-shot context. The findings\ndemonstrate that LVLMs can attain performance competitive with that of the\nsmaller model. Next, we integrate standard in-context learning (ICL) with\nLVLMs, noting improvements in FND performance, though limited in scope and\nconsistency. To address this, we introduce the \\textbf{I}n-context\n\\textbf{M}ultimodal \\textbf{F}ake \\textbf{N}ews \\textbf{D}etection (IMFND)\nframework, enriching in-context examples and test inputs with predictions and\ncorresponding probabilities from a well-trained smaller model. This strategic\nintegration directs the LVLMs' focus towards news segments associated with\nhigher probabilities, thereby improving their analytical accuracy. The\nexperimental results suggest that the IMFND framework significantly boosts the\nFND efficiency of LVLMs, achieving enhanced accuracy over the standard ICL\napproach across three publicly available FND datasets."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we introduce the VerifAI project, a pioneering open-source\nscientific question-answering system, designed to provide answers that are not\nonly referenced but also automatically vetted and verifiable. The components of\nthe system are (1) an Information Retrieval system combining semantic and\nlexical search techniques over scientific papers (PubMed), (2) a\nRetrieval-Augmented Generation (RAG) module using fine-tuned generative model\n(Mistral 7B) and retrieved articles to generate claims with references to the\narticles from which it was derived, and (3) a Verification engine, based on a\nfine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task\nusing SciFACT dataset. The verification engine cross-checks the generated claim\nand the article from which the claim was derived, verifying whether there may\nhave been any hallucinations in generating the claim. By leveraging the\nInformation Retrieval and RAG modules, Verif.ai excels in generating factual\ninformation from a vast array of scientific sources. At the same time, the\nVerification engine rigorously double-checks this output, ensuring its accuracy\nand reliability. This dual-stage process plays a crucial role in acquiring and\nconfirming factual information, significantly enhancing the information\nlandscape. Our methodology could significantly enhance scientists'\nproductivity, concurrently fostering trust in applying generative language\nmodels within scientific domains, where hallucinations and misinformation are\nunacceptable.",
        "pos": [
            "Large language models (LLMs) have recently become the leading source of\nanswers for users' questions online. Despite their ability to offer eloquent\nanswers, their accuracy and reliability can pose a significant challenge. This\nis especially true for sensitive domains such as biomedicine, where there is a\nhigher need for factually correct answers. This paper introduces a biomedical\nretrieval-augmented generation (RAG) system designed to enhance the reliability\nof generated responses. The system is based on a fine-tuned LLM for the\nreferenced question-answering, where retrieved relevant abstracts from PubMed\nare passed to LLM's context as input through a prompt. Its output is an answer\nbased on PubMed abstracts, where each statement is referenced accordingly,\nallowing the users to verify the answer. Our retrieval system achieves an\nabsolute improvement of 23% compared to the PubMed search engine. Based on the\nmanual evaluation on a small sample, our fine-tuned LLM component achieves\ncomparable results to GPT-4 Turbo in referencing relevant abstracts. We make\nthe dataset used to fine-tune the models and the fine-tuned models based on\nMistral-7B-instruct-v0.1 and v0.2 publicly available."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we introduce the VerifAI project, a pioneering open-source\nscientific question-answering system, designed to provide answers that are not\nonly referenced but also automatically vetted and verifiable. The components of\nthe system are (1) an Information Retrieval system combining semantic and\nlexical search techniques over scientific papers (PubMed), (2) a\nRetrieval-Augmented Generation (RAG) module using fine-tuned generative model\n(Mistral 7B) and retrieved articles to generate claims with references to the\narticles from which it was derived, and (3) a Verification engine, based on a\nfine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task\nusing SciFACT dataset. The verification engine cross-checks the generated claim\nand the article from which the claim was derived, verifying whether there may\nhave been any hallucinations in generating the claim. By leveraging the\nInformation Retrieval and RAG modules, Verif.ai excels in generating factual\ninformation from a vast array of scientific sources. At the same time, the\nVerification engine rigorously double-checks this output, ensuring its accuracy\nand reliability. This dual-stage process plays a crucial role in acquiring and\nconfirming factual information, significantly enhancing the information\nlandscape. Our methodology could significantly enhance scientists'\nproductivity, concurrently fostering trust in applying generative language\nmodels within scientific domains, where hallucinations and misinformation are\nunacceptable.",
        "pos": [
            "Large language models (LLMs) have recently become the leading source of\nanswers for users' questions online. Despite their ability to offer eloquent\nanswers, their accuracy and reliability can pose a significant challenge. This\nis especially true for sensitive domains such as biomedicine, where there is a\nhigher need for factually correct answers. This paper introduces a biomedical\nretrieval-augmented generation (RAG) system designed to enhance the reliability\nof generated responses. The system is based on a fine-tuned LLM for the\nreferenced question-answering, where retrieved relevant abstracts from PubMed\nare passed to LLM's context as input through a prompt. Its output is an answer\nbased on PubMed abstracts, where each statement is referenced accordingly,\nallowing the users to verify the answer. Our retrieval system achieves an\nabsolute improvement of 23% compared to the PubMed search engine. Based on the\nmanual evaluation on a small sample, our fine-tuned LLM component achieves\ncomparable results to GPT-4 Turbo in referencing relevant abstracts. We make\nthe dataset used to fine-tune the models and the fine-tuned models based on\nMistral-7B-instruct-v0.1 and v0.2 publicly available."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we introduce the VerifAI project, a pioneering open-source\nscientific question-answering system, designed to provide answers that are not\nonly referenced but also automatically vetted and verifiable. The components of\nthe system are (1) an Information Retrieval system combining semantic and\nlexical search techniques over scientific papers (PubMed), (2) a\nRetrieval-Augmented Generation (RAG) module using fine-tuned generative model\n(Mistral 7B) and retrieved articles to generate claims with references to the\narticles from which it was derived, and (3) a Verification engine, based on a\nfine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task\nusing SciFACT dataset. The verification engine cross-checks the generated claim\nand the article from which the claim was derived, verifying whether there may\nhave been any hallucinations in generating the claim. By leveraging the\nInformation Retrieval and RAG modules, Verif.ai excels in generating factual\ninformation from a vast array of scientific sources. At the same time, the\nVerification engine rigorously double-checks this output, ensuring its accuracy\nand reliability. This dual-stage process plays a crucial role in acquiring and\nconfirming factual information, significantly enhancing the information\nlandscape. Our methodology could significantly enhance scientists'\nproductivity, concurrently fostering trust in applying generative language\nmodels within scientific domains, where hallucinations and misinformation are\nunacceptable.",
        "pos": [
            "Large language models (LLMs) have recently become the leading source of\nanswers for users' questions online. Despite their ability to offer eloquent\nanswers, their accuracy and reliability can pose a significant challenge. This\nis especially true for sensitive domains such as biomedicine, where there is a\nhigher need for factually correct answers. This paper introduces a biomedical\nretrieval-augmented generation (RAG) system designed to enhance the reliability\nof generated responses. The system is based on a fine-tuned LLM for the\nreferenced question-answering, where retrieved relevant abstracts from PubMed\nare passed to LLM's context as input through a prompt. Its output is an answer\nbased on PubMed abstracts, where each statement is referenced accordingly,\nallowing the users to verify the answer. Our retrieval system achieves an\nabsolute improvement of 23% compared to the PubMed search engine. Based on the\nmanual evaluation on a small sample, our fine-tuned LLM component achieves\ncomparable results to GPT-4 Turbo in referencing relevant abstracts. We make\nthe dataset used to fine-tune the models and the fine-tuned models based on\nMistral-7B-instruct-v0.1 and v0.2 publicly available."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we introduce the VerifAI project, a pioneering open-source\nscientific question-answering system, designed to provide answers that are not\nonly referenced but also automatically vetted and verifiable. The components of\nthe system are (1) an Information Retrieval system combining semantic and\nlexical search techniques over scientific papers (PubMed), (2) a\nRetrieval-Augmented Generation (RAG) module using fine-tuned generative model\n(Mistral 7B) and retrieved articles to generate claims with references to the\narticles from which it was derived, and (3) a Verification engine, based on a\nfine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task\nusing SciFACT dataset. The verification engine cross-checks the generated claim\nand the article from which the claim was derived, verifying whether there may\nhave been any hallucinations in generating the claim. By leveraging the\nInformation Retrieval and RAG modules, Verif.ai excels in generating factual\ninformation from a vast array of scientific sources. At the same time, the\nVerification engine rigorously double-checks this output, ensuring its accuracy\nand reliability. This dual-stage process plays a crucial role in acquiring and\nconfirming factual information, significantly enhancing the information\nlandscape. Our methodology could significantly enhance scientists'\nproductivity, concurrently fostering trust in applying generative language\nmodels within scientific domains, where hallucinations and misinformation are\nunacceptable.",
        "pos": [
            "Large language models (LLMs) have recently become the leading source of\nanswers for users' questions online. Despite their ability to offer eloquent\nanswers, their accuracy and reliability can pose a significant challenge. This\nis especially true for sensitive domains such as biomedicine, where there is a\nhigher need for factually correct answers. This paper introduces a biomedical\nretrieval-augmented generation (RAG) system designed to enhance the reliability\nof generated responses. The system is based on a fine-tuned LLM for the\nreferenced question-answering, where retrieved relevant abstracts from PubMed\nare passed to LLM's context as input through a prompt. Its output is an answer\nbased on PubMed abstracts, where each statement is referenced accordingly,\nallowing the users to verify the answer. Our retrieval system achieves an\nabsolute improvement of 23% compared to the PubMed search engine. Based on the\nmanual evaluation on a small sample, our fine-tuned LLM component achieves\ncomparable results to GPT-4 Turbo in referencing relevant abstracts. We make\nthe dataset used to fine-tune the models and the fine-tuned models based on\nMistral-7B-instruct-v0.1 and v0.2 publicly available."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we introduce the VerifAI project, a pioneering open-source\nscientific question-answering system, designed to provide answers that are not\nonly referenced but also automatically vetted and verifiable. The components of\nthe system are (1) an Information Retrieval system combining semantic and\nlexical search techniques over scientific papers (PubMed), (2) a\nRetrieval-Augmented Generation (RAG) module using fine-tuned generative model\n(Mistral 7B) and retrieved articles to generate claims with references to the\narticles from which it was derived, and (3) a Verification engine, based on a\nfine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task\nusing SciFACT dataset. The verification engine cross-checks the generated claim\nand the article from which the claim was derived, verifying whether there may\nhave been any hallucinations in generating the claim. By leveraging the\nInformation Retrieval and RAG modules, Verif.ai excels in generating factual\ninformation from a vast array of scientific sources. At the same time, the\nVerification engine rigorously double-checks this output, ensuring its accuracy\nand reliability. This dual-stage process plays a crucial role in acquiring and\nconfirming factual information, significantly enhancing the information\nlandscape. Our methodology could significantly enhance scientists'\nproductivity, concurrently fostering trust in applying generative language\nmodels within scientific domains, where hallucinations and misinformation are\nunacceptable.",
        "pos": [
            "Large language models (LLMs) have recently become the leading source of\nanswers for users' questions online. Despite their ability to offer eloquent\nanswers, their accuracy and reliability can pose a significant challenge. This\nis especially true for sensitive domains such as biomedicine, where there is a\nhigher need for factually correct answers. This paper introduces a biomedical\nretrieval-augmented generation (RAG) system designed to enhance the reliability\nof generated responses. The system is based on a fine-tuned LLM for the\nreferenced question-answering, where retrieved relevant abstracts from PubMed\nare passed to LLM's context as input through a prompt. Its output is an answer\nbased on PubMed abstracts, where each statement is referenced accordingly,\nallowing the users to verify the answer. Our retrieval system achieves an\nabsolute improvement of 23% compared to the PubMed search engine. Based on the\nmanual evaluation on a small sample, our fine-tuned LLM component achieves\ncomparable results to GPT-4 Turbo in referencing relevant abstracts. We make\nthe dataset used to fine-tune the models and the fine-tuned models based on\nMistral-7B-instruct-v0.1 and v0.2 publicly available."
        ],
        "neg": []
    },
    {
        "query": "In this paper, we introduce the VerifAI project, a pioneering open-source\nscientific question-answering system, designed to provide answers that are not\nonly referenced but also automatically vetted and verifiable. The components of\nthe system are (1) an Information Retrieval system combining semantic and\nlexical search techniques over scientific papers (PubMed), (2) a\nRetrieval-Augmented Generation (RAG) module using fine-tuned generative model\n(Mistral 7B) and retrieved articles to generate claims with references to the\narticles from which it was derived, and (3) a Verification engine, based on a\nfine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task\nusing SciFACT dataset. The verification engine cross-checks the generated claim\nand the article from which the claim was derived, verifying whether there may\nhave been any hallucinations in generating the claim. By leveraging the\nInformation Retrieval and RAG modules, Verif.ai excels in generating factual\ninformation from a vast array of scientific sources. At the same time, the\nVerification engine rigorously double-checks this output, ensuring its accuracy\nand reliability. This dual-stage process plays a crucial role in acquiring and\nconfirming factual information, significantly enhancing the information\nlandscape. Our methodology could significantly enhance scientists'\nproductivity, concurrently fostering trust in applying generative language\nmodels within scientific domains, where hallucinations and misinformation are\nunacceptable.",
        "pos": [
            "Large language models (LLMs) have recently become the leading source of\nanswers for users' questions online. Despite their ability to offer eloquent\nanswers, their accuracy and reliability can pose a significant challenge. This\nis especially true for sensitive domains such as biomedicine, where there is a\nhigher need for factually correct answers. This paper introduces a biomedical\nretrieval-augmented generation (RAG) system designed to enhance the reliability\nof generated responses. The system is based on a fine-tuned LLM for the\nreferenced question-answering, where retrieved relevant abstracts from PubMed\nare passed to LLM's context as input through a prompt. Its output is an answer\nbased on PubMed abstracts, where each statement is referenced accordingly,\nallowing the users to verify the answer. Our retrieval system achieves an\nabsolute improvement of 23% compared to the PubMed search engine. Based on the\nmanual evaluation on a small sample, our fine-tuned LLM component achieves\ncomparable results to GPT-4 Turbo in referencing relevant abstracts. We make\nthe dataset used to fine-tune the models and the fine-tuned models based on\nMistral-7B-instruct-v0.1 and v0.2 publicly available."
        ],
        "neg": []
    },
    {
        "query": "Recent work integrating Large Language Models (LLMs) has led to significant\nimprovements in the Knowledge Base Question Answering (KBQA) task. However, we\nposit that existing KBQA datasets that either have simple questions, use\nsynthetically generated logical forms, or are based on small knowledge base\n(KB) schemas, do not capture the true complexity of KBQA tasks.\n  To address this, we introduce the SPINACH dataset, an expert-annotated KBQA\ndataset collected from forum discussions on Wikidata's \"Request a Query\" forum\nwith 320 decontextualized question-SPARQL pairs. Much more complex than\nexisting datasets, SPINACH calls for strong KBQA systems that do not rely on\ntraining data to learn the KB schema, but can dynamically explore large and\noften incomplete schemas and reason about them.\n  Along with the dataset, we introduce the SPINACH agent, a new KBQA approach\nthat mimics how a human expert would write SPARQLs for such challenging\nquestions. Experiments on existing datasets show SPINACH's capability in KBQA,\nachieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10\ndatasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6%\nof the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH\ndataset, SPINACH agent outperforms all baselines, including the best\nGPT-4-based KBQA agent, by 38.1% in F1.",
        "pos": [
            "Programming LLM-based knowledge and task assistants that faithfully conform\nto developer-provided policies is challenging. These agents must retrieve and\nprovide consistent, accurate, and relevant information to address user's\nqueries and needs. Yet such agents generate unfounded responses\n(\"hallucinate\"). Traditional dialogue trees can only handle a limited number of\nconversation flows, making them inherently brittle. To this end, we present\nKITA - a programmable framework for creating task-oriented conversational\nagents that are designed to handle complex user interactions. Unlike LLMs, KITA\nprovides reliable grounded responses, with controllable agent policies through\nits expressive specification, KITA Worksheet. In contrast to dialog trees, it\nis resilient to diverse user queries, helpful with knowledge sources, and\noffers ease of programming policies through its declarative paradigm. Through a\nreal-user study involving 62 participants, we show that KITA beats the GPT-4\nwith function calling baseline by 26.1, 22.5, and 52.4 points on execution\naccuracy, dialogue act accuracy, and goal completion rate, respectively. We\nalso release 22 real-user conversations with KITA manually corrected to ensure\naccuracy."
        ],
        "neg": []
    },
    {
        "query": "Recent work integrating Large Language Models (LLMs) has led to significant\nimprovements in the Knowledge Base Question Answering (KBQA) task. However, we\nposit that existing KBQA datasets that either have simple questions, use\nsynthetically generated logical forms, or are based on small knowledge base\n(KB) schemas, do not capture the true complexity of KBQA tasks.\n  To address this, we introduce the SPINACH dataset, an expert-annotated KBQA\ndataset collected from forum discussions on Wikidata's \"Request a Query\" forum\nwith 320 decontextualized question-SPARQL pairs. Much more complex than\nexisting datasets, SPINACH calls for strong KBQA systems that do not rely on\ntraining data to learn the KB schema, but can dynamically explore large and\noften incomplete schemas and reason about them.\n  Along with the dataset, we introduce the SPINACH agent, a new KBQA approach\nthat mimics how a human expert would write SPARQLs for such challenging\nquestions. Experiments on existing datasets show SPINACH's capability in KBQA,\nachieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10\ndatasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6%\nof the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH\ndataset, SPINACH agent outperforms all baselines, including the best\nGPT-4-based KBQA agent, by 38.1% in F1.",
        "pos": [
            "Programming LLM-based knowledge and task assistants that faithfully conform\nto developer-provided policies is challenging. These agents must retrieve and\nprovide consistent, accurate, and relevant information to address user's\nqueries and needs. Yet such agents generate unfounded responses\n(\"hallucinate\"). Traditional dialogue trees can only handle a limited number of\nconversation flows, making them inherently brittle. To this end, we present\nKITA - a programmable framework for creating task-oriented conversational\nagents that are designed to handle complex user interactions. Unlike LLMs, KITA\nprovides reliable grounded responses, with controllable agent policies through\nits expressive specification, KITA Worksheet. In contrast to dialog trees, it\nis resilient to diverse user queries, helpful with knowledge sources, and\noffers ease of programming policies through its declarative paradigm. Through a\nreal-user study involving 62 participants, we show that KITA beats the GPT-4\nwith function calling baseline by 26.1, 22.5, and 52.4 points on execution\naccuracy, dialogue act accuracy, and goal completion rate, respectively. We\nalso release 22 real-user conversations with KITA manually corrected to ensure\naccuracy."
        ],
        "neg": []
    },
    {
        "query": "The rise of generative AI is transforming the landscape of digital imagery,\nand exerting a significant influence on online creative communities. This has\nled to the emergence of AI-Generated Content (AIGC) social platforms, such as\nCivitai. These distinctive social platforms allow users to build and share\ntheir own generative AI models, thereby enhancing the potential for more\ndiverse artistic expression. Designed in the vein of social networks, they also\nprovide artists with the means to showcase their creations (generated from the\nmodels), engage in discussions, and obtain feedback, thus nurturing a sense of\ncommunity. Yet, this openness also raises concerns about the abuse of such\nplatforms, e.g., using models to disseminate deceptive deepfakes or infringe\nupon copyrights. To explore this, we conduct the first comprehensive empirical\nstudy of an AIGC social platform, focusing on its use for generating abusive\ncontent. As an exemplar, we construct a comprehensive dataset covering Civitai,\nthe largest available AIGC social platform. Based on this dataset of 87K models\nand 2M images, we explore the characteristics of content and discuss strategies\nfor moderation to better govern these platforms.",
        "pos": [
            "Harnessing the potential of large language models (LLMs) like ChatGPT can\nhelp address social challenges through inclusive, ethical, and sustainable\nmeans. In this paper, we investigate the extent to which ChatGPT can annotate\ndata for social computing tasks, aiming to reduce the complexity and cost of\nundertaking web research. To evaluate ChatGPT's potential, we re-annotate seven\ndatasets using ChatGPT, covering topics related to pressing social issues like\nCOVID-19 misinformation, social bot deception, cyberbully, clickbait news, and\nthe Russo-Ukrainian War. Our findings demonstrate that ChatGPT exhibits promise\nin handling these data annotation tasks, albeit with some challenges. Across\nthe seven datasets, ChatGPT achieves an average annotation F1-score of 72.00%.\nIts performance excels in clickbait news annotation, correctly labeling 89.66%\nof the data. However, we also observe significant variations in performance\nacross individual labels. Our study reveals predictable patterns in ChatGPT's\nannotation performance. Thus, we propose GPT-Rater, a tool to predict if\nChatGPT can correctly label data for a given annotation task. Researchers can\nuse this to identify where ChatGPT might be suitable for their annotation\nrequirements. We show that GPT-Rater effectively predicts ChatGPT's\nperformance. It performs best on a clickbait headlines dataset by achieving an\naverage F1-score of 95.00%. We believe that this research opens new avenues for\nanalysis and can reduce barriers to engaging in social computing research."
        ],
        "neg": []
    },
    {
        "query": "The rise of generative AI is transforming the landscape of digital imagery,\nand exerting a significant influence on online creative communities. This has\nled to the emergence of AI-Generated Content (AIGC) social platforms, such as\nCivitai. These distinctive social platforms allow users to build and share\ntheir own generative AI models, thereby enhancing the potential for more\ndiverse artistic expression. Designed in the vein of social networks, they also\nprovide artists with the means to showcase their creations (generated from the\nmodels), engage in discussions, and obtain feedback, thus nurturing a sense of\ncommunity. Yet, this openness also raises concerns about the abuse of such\nplatforms, e.g., using models to disseminate deceptive deepfakes or infringe\nupon copyrights. To explore this, we conduct the first comprehensive empirical\nstudy of an AIGC social platform, focusing on its use for generating abusive\ncontent. As an exemplar, we construct a comprehensive dataset covering Civitai,\nthe largest available AIGC social platform. Based on this dataset of 87K models\nand 2M images, we explore the characteristics of content and discuss strategies\nfor moderation to better govern these platforms.",
        "pos": [
            "Harnessing the potential of large language models (LLMs) like ChatGPT can\nhelp address social challenges through inclusive, ethical, and sustainable\nmeans. In this paper, we investigate the extent to which ChatGPT can annotate\ndata for social computing tasks, aiming to reduce the complexity and cost of\nundertaking web research. To evaluate ChatGPT's potential, we re-annotate seven\ndatasets using ChatGPT, covering topics related to pressing social issues like\nCOVID-19 misinformation, social bot deception, cyberbully, clickbait news, and\nthe Russo-Ukrainian War. Our findings demonstrate that ChatGPT exhibits promise\nin handling these data annotation tasks, albeit with some challenges. Across\nthe seven datasets, ChatGPT achieves an average annotation F1-score of 72.00%.\nIts performance excels in clickbait news annotation, correctly labeling 89.66%\nof the data. However, we also observe significant variations in performance\nacross individual labels. Our study reveals predictable patterns in ChatGPT's\nannotation performance. Thus, we propose GPT-Rater, a tool to predict if\nChatGPT can correctly label data for a given annotation task. Researchers can\nuse this to identify where ChatGPT might be suitable for their annotation\nrequirements. We show that GPT-Rater effectively predicts ChatGPT's\nperformance. It performs best on a clickbait headlines dataset by achieving an\naverage F1-score of 95.00%. We believe that this research opens new avenues for\nanalysis and can reduce barriers to engaging in social computing research."
        ],
        "neg": []
    },
    {
        "query": "The rise of generative AI is transforming the landscape of digital imagery,\nand exerting a significant influence on online creative communities. This has\nled to the emergence of AI-Generated Content (AIGC) social platforms, such as\nCivitai. These distinctive social platforms allow users to build and share\ntheir own generative AI models, thereby enhancing the potential for more\ndiverse artistic expression. Designed in the vein of social networks, they also\nprovide artists with the means to showcase their creations (generated from the\nmodels), engage in discussions, and obtain feedback, thus nurturing a sense of\ncommunity. Yet, this openness also raises concerns about the abuse of such\nplatforms, e.g., using models to disseminate deceptive deepfakes or infringe\nupon copyrights. To explore this, we conduct the first comprehensive empirical\nstudy of an AIGC social platform, focusing on its use for generating abusive\ncontent. As an exemplar, we construct a comprehensive dataset covering Civitai,\nthe largest available AIGC social platform. Based on this dataset of 87K models\nand 2M images, we explore the characteristics of content and discuss strategies\nfor moderation to better govern these platforms.",
        "pos": [
            "Harnessing the potential of large language models (LLMs) like ChatGPT can\nhelp address social challenges through inclusive, ethical, and sustainable\nmeans. In this paper, we investigate the extent to which ChatGPT can annotate\ndata for social computing tasks, aiming to reduce the complexity and cost of\nundertaking web research. To evaluate ChatGPT's potential, we re-annotate seven\ndatasets using ChatGPT, covering topics related to pressing social issues like\nCOVID-19 misinformation, social bot deception, cyberbully, clickbait news, and\nthe Russo-Ukrainian War. Our findings demonstrate that ChatGPT exhibits promise\nin handling these data annotation tasks, albeit with some challenges. Across\nthe seven datasets, ChatGPT achieves an average annotation F1-score of 72.00%.\nIts performance excels in clickbait news annotation, correctly labeling 89.66%\nof the data. However, we also observe significant variations in performance\nacross individual labels. Our study reveals predictable patterns in ChatGPT's\nannotation performance. Thus, we propose GPT-Rater, a tool to predict if\nChatGPT can correctly label data for a given annotation task. Researchers can\nuse this to identify where ChatGPT might be suitable for their annotation\nrequirements. We show that GPT-Rater effectively predicts ChatGPT's\nperformance. It performs best on a clickbait headlines dataset by achieving an\naverage F1-score of 95.00%. We believe that this research opens new avenues for\nanalysis and can reduce barriers to engaging in social computing research.",
            "How similar are politicians to those who vote for them? This is a critical\nquestion at the heart of democratic representation and particularly relevant at\ntimes when political dissatisfaction and populism are on the rise. To answer\nthis question we compare the online discourse of elected politicians and their\nconstituents. We collect a two and a half years (September 2020 - February\n2023) constituency-level dataset for USA and UK that includes: (i) the Twitter\ntimelines (5.6 Million tweets) of elected political representatives (595 UK\nMembers of Parliament and 433 USA Representatives), (ii) the Nextdoor posts\n(21.8 Million posts) of the constituency (98.4% USA and 91.5% UK\nconstituencies). We find that elected politicians tend to be equally similar to\ntheir constituents in terms of content and style regardless of whether a\nconstituency elects a right or left-wing politician. The size of the electoral\nvictory and the level of income of a constituency shows a nuanced picture. The\nnarrower the electoral victory, the more similar the style and the more\ndissimilar the content is. The lower the income of a constituency, the more\nsimilar the content is. In terms of style, poorer constituencies tend to have a\nmore similar sentiment and more dissimilar psychological text traits (i.e.\nmeasured with LIWC categories)."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.",
        "pos": [
            "Exceptional mathematical reasoning ability is one of the key features that\ndemonstrate the power of large language models (LLMs). How to comprehensively\ndefine and evaluate the mathematical abilities of LLMs, and even reflect the\nuser experience in real-world scenarios, has emerged as a critical issue.\nCurrent benchmarks predominantly concentrate on problem-solving capabilities,\nwhich presents a substantial risk of model overfitting and fails to accurately\nrepresent genuine mathematical reasoning abilities. In this paper, we argue\nthat if a model really understands a problem, it should be robustly and readily\napplied across a diverse array of tasks. Motivated by this, we introduce\nMATHCHECK, a well-designed checklist for testing task generalization and\nreasoning robustness, as well as an automatic tool to generate checklists\nefficiently. MATHCHECK includes multiple mathematical reasoning tasks and\nrobustness test types to facilitate a comprehensive evaluation of both\nmathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we\ndevelop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual\nreasoning and multi-modal reasoning capabilities, respectively, serving as\nupgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K.\nWe adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs,\nassessing their comprehensive mathematical reasoning abilities. Our results\ndemonstrate that while frontier LLMs like GPT-4o continue to excel in various\nabilities on the checklist, many other model families exhibit a significant\ndecline. Further experiments indicate that, compared to traditional math\nbenchmarks, MATHCHECK better reflects true mathematical abilities and\nrepresents mathematical intelligence more linearly, thereby supporting our\ndesign. On our MATHCHECK, we can easily conduct detailed behavior analysis to\ndeeply investigate models."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.",
        "pos": [
            "Recent advancements in Vision-Language Models (VLMs) have led to the\ndevelopment of Vision-Language Generalists (VLGs) capable of understanding and\ngenerating interleaved images and text. Despite these advances, VLGs still\nstruggle to follow user instructions for interleaved text and image generation.\nTo address this issue, we introduce LeafInstruct, the first open-sourced\ninterleaved instruction tuning data with over 30,000 high-quality instances\nacross more than 10 domains. Due to the extensive size of existing VLGs, we opt\nfor parameter-efficient tuning. However, we observe that VLGs tuned with a\nstandard LoRA typically exhibit inferior performance in interleaved text-image\ngeneration. We attribute this problem to modality interference and the lack of\nmodality-specialized adaptation design. Hence, we propose Lateralization LoRA,\na novel modality-specialized adaptation method inspired by the concept of brain\nlateralization. Lateralization LoRA employs a hybrid approach, combining the\ntraditional linear LoRA and a Convolutional LoRA for generating text and\nimages, enabling the generation of high-quality text and images by leveraging\nmodality-specific structures and parameter sets. We perform instruction tuning\nof the VLG (i.e., EMU2) using Lateralization LoRA on the LeafInstruct dataset.\nExtensive experiments demonstrate that EMU2 tuned with Lateralization LoRA\nachieve state-of-the-art performance, significantly surpassing baseline models\nin complex interleaved tasks."
        ],
        "neg": []
    },
    {
        "query": "We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. We also introduce Block\nQ-Sparse for batch training and inference. The key results from this work are,\n(1) Q-Sparse can achieve results comparable to those of baseline LLMs while\nbeing much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.",
        "pos": [
            "We present MELLE, a novel continuous-valued tokens based language modeling\napproach for text to speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which are originally designed for audio\ncompression and sacrifice fidelity compared to mel-spectrograms. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens. (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language models VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling discrete codes,\nachieves superior performance across multiple metrics, and, most importantly,\noffers a more streamlined paradigm. See https://aka.ms/melle for demos of our\nwork.",
            "This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel\nreasoning approach to enhance the decision rationality of language models.\nTraditional reasoning methods typically rely on historical information and\nemploy uni-directional (left-to-right) reasoning strategy. This lack of\nbi-directional deliberation reasoning results in limited awareness of potential\nfuture outcomes and insufficient integration of historical context, leading to\nsuboptimal decisions. BIDDER addresses this gap by incorporating principles of\nrational decision-making, specifically managing uncertainty and predicting\nexpected utility. Our approach involves three key processes: Inferring hidden\nstates to represent uncertain information in the decision-making process from\nhistorical data; Using these hidden states to predict future potential states\nand potential outcomes; Integrating historical information (past contexts) and\nlong-term outcomes (future contexts) to inform reasoning. By leveraging\nbi-directional reasoning, BIDDER ensures thorough exploration of both past and\nfuture contexts, leading to more informed and rational decisions. We tested\nBIDDER's effectiveness in two well-defined scenarios: Poker (Limit Texas\nHold'em) and Negotiation. Our experiments demonstrate that BIDDER significantly\nimproves the decision-making capabilities of LLMs and LLM agents."
        ],
        "neg": []
    },
    {
        "query": "Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.",
        "pos": [
            "Model training requires significantly more memory, compared with inference.\nParameter efficient fine-tuning (PEFT) methods provide a means of adapting\nlarge models to downstream tasks using less memory. However, existing methods\nsuch as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce\nlatency overhead at inference time or achieve subpar downstream performance\ncompared with full fine-tuning. In this work we propose Random Subspace\nAdaptation (ROSA), a method that outperforms previous PEFT methods by a\nsignificant margin, while maintaining a zero latency overhead during inference\ntime. In contrast to previous methods, ROSA is able to adapt subspaces of\narbitrarily large dimension, better approximating full-finetuning. We\ndemonstrate both theoretically and experimentally that this makes ROSA strictly\nmore expressive than LoRA, without consuming additional memory during runtime.\nAs PEFT methods are especially useful in the natural language processing\ndomain, where models operate on scales that make full fine-tuning very\nexpensive, we evaluate ROSA in two common NLP scenarios: natural language\ngeneration (NLG) and natural language understanding (NLU) with GPT-2 and\nRoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms\nLoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our\ncode is available at https://github.com/rosa-paper/rosa"
        ],
        "neg": []
    },
    {
        "query": "Decoding by contrasting layers (DoLa), is designed to improve the generation\nquality of large language models (LLMs) by contrasting the prediction\nprobabilities between an early exit output (amateur logits) and the final\noutput (expert logits). However, we find that this approach does not work well\non non-English tasks. Inspired by previous interpretability work on language\ntransition during the model's forward pass, we discover that this issue arises\nfrom a language mismatch between early exit output and final output. In this\nwork, we propose an improved contrastive decoding algorithm that is effective\nfor diverse languages beyond English. To obtain more helpful amateur logits, we\ndevise two strategies to skip a set of bottom, language-agnostic layers based\non our preliminary analysis. Experimental results on multilingual reasoning\nbenchmarks demonstrate that our proposed method outperforms previous\ncontrastive decoding baselines and substantially improves LLM's\nchain-of-thought reasoning accuracy across 11 languages. The project will be\navailable at: https://github.com/NJUNLP/SkipLayerCD.",
        "pos": [
            "Large Language Models~(LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting\nextensive multilingual continual pre-training on the LLaMA series models,\nenabling translation support across more than 100 languages. Through a\ncomprehensive analysis of training strategies, such as vocabulary expansion and\ndata augmentation, we develop LLaMAX. Remarkably, without sacrificing its\ngeneralization ability, LLaMAX achieves significantly higher translation\nperformance compared to existing open-source LLMs~(by more than 10 spBLEU\npoints) and performs on-par with specialized translation model~(M2M-100-12B) on\nthe Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve\nas a robust multilingual foundation model. The\ncode~\\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and\nmodels~\\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available."
        ],
        "neg": []
    },
    {
        "query": "Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.",
        "pos": [
            "The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools."
        ],
        "neg": []
    },
    {
        "query": "Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.",
        "pos": [
            "Controllability and proactivity are crucial properties of autonomous\nconversational agents (CAs). Controllability requires the CAs to follow the\nstandard operating procedures (SOPs), such as verifying identity before\nactivating credit cards. Proactivity requires the CAs to guide the conversation\ntowards the goal during user uncooperation, such as persuasive dialogue.\nExisting research cannot be unified with controllability, proactivity, and low\nmanual annotation. To bridge this gap, we propose a new framework for\nplanning-based conversational agents (PCA) powered by large language models\n(LLMs), which only requires humans to define tasks and goals for the LLMs.\nBefore conversation, LLM plans the core and necessary SOP for dialogue offline.\nDuring the conversation, LLM plans the best action path online referring to the\nSOP, and generates responses to achieve process controllability. Subsequently,\nwe propose a semi-automatic dialogue data creation framework and curate a\nhigh-quality dialogue dataset (PCA-D). Meanwhile, we develop multiple variants\nand evaluation metrics for PCA, e.g., planning with Monte Carlo Tree Search\n(PCA-M), which searches for the optimal dialogue action while satisfying SOP\nconstraints and achieving the proactive of the dialogue. Experiment results\nshow that LLMs finetuned on PCA-D can significantly improve the performance and\ngeneralize to unseen domains. PCA-M outperforms other CoT and ToT baselines in\nterms of conversation controllability, proactivity, task success rate, and\noverall logical coherence, and is applicable in industry dialogue scenarios.\nThe dataset and codes are available at XXXX.",
            "Manual Red teaming is a commonly-used method to identify vulnerabilities in\nlarge language models (LLMs), which, is costly and unscalable. In contrast,\nautomated red teaming uses a Red LLM to automatically generate adversarial\nprompts to the Target LLM, offering a scalable way for safety vulnerability\ndetection. However, the difficulty of building a powerful automated Red LLM\nlies in the fact that the safety vulnerabilities of the Target LLM are\ndynamically changing with the evolution of the Target LLM. To mitigate this\nissue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in\nwhich the Red LLM and Target LLM are deeply and dynamically interacting with\neach other in an iterative manner. In each iteration, in order to generate\nsuccessful attacks as many as possible, the Red LLM not only takes into account\nthe responses from the Target LLM, but also adversarially adjust its attacking\ndirections by monitoring the global diversity of generated attacks across\nmultiple iterations. Simultaneously, to explore dynamically changing safety\nvulnerabilities of the Target LLM, we allow the Target LLM to enhance its\nsafety via an active learning based data selection mechanism. Experimential\nresults demonstrate that DART significantly reduces the safety risk of the\ntarget LLM. For human evaluation on Anthropic Harmless dataset, compared to the\ninstruction-tuning target LLM, DART eliminates the violation risks by 53.4\\%.\nWe will release the datasets and codes of DART soon."
        ],
        "neg": []
    },
    {
        "query": "Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.",
        "pos": [
            "The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools."
        ],
        "neg": []
    },
    {
        "query": "Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.",
        "pos": [
            "The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools."
        ],
        "neg": []
    },
    {
        "query": "Recently, there has been a growing interest among large language model (LLM)\ndevelopers in LLM-based document reading systems, which enable users to upload\ntheir own documents and pose questions related to the document contents, going\nbeyond simple reading comprehension tasks. Consequently, these systems have\nbeen carefully designed to tackle challenges such as file parsing, metadata\nextraction, multi-modal information understanding and long-context reading.\nHowever, no current benchmark exists to evaluate their performance in such\nscenarios, where a raw file and questions are provided as input, and a\ncorresponding response is expected as output. In this paper, we introduce\nDocBench, a new benchmark designed to evaluate LLM-based document reading\nsystems. Our benchmark involves a meticulously crafted process, including the\nrecruitment of human annotators and the generation of synthetic questions. It\nincludes 229 real documents and 1,102 questions, spanning across five different\ndomains and four major types of questions. We evaluate both proprietary\nLLM-based systems accessible via web interfaces or APIs, and a parse-then-read\npipeline employing open-source LLMs. Our evaluations reveal noticeable gaps\nbetween existing LLM-based document reading systems and human performance,\nunderscoring the challenges of developing proficient systems. To summarize,\nDocBench aims to establish a standardized benchmark for evaluating LLM-based\ndocument reading systems under diverse real-world scenarios, thereby guiding\nfuture advancements in this research area.",
        "pos": [
            "The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench."
        ],
        "neg": []
    },
    {
        "query": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "pos": [
            "Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath."
        ],
        "neg": []
    },
    {
        "query": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "pos": [
            "Large Language Models (LLMs) have gained widespread adoption in various\nnatural language processing tasks, including question answering and dialogue\nsystems. However, a major drawback of LLMs is the issue of hallucination, where\nthey generate unfaithful or inconsistent content that deviates from the input\nsource, leading to severe consequences. In this paper, we propose a robust\ndiscriminator named RelD to effectively detect hallucination in LLMs' generated\nanswers. RelD is trained on the constructed RelQA, a bilingual\nquestion-answering dialogue dataset along with answers generated by LLMs and a\ncomprehensive set of metrics. Our experimental results demonstrate that the\nproposed RelD successfully detects hallucination in the answers generated by\ndiverse LLMs. Moreover, it performs well in distinguishing hallucination in\nLLMs' generated answers from both in-distribution and out-of-distribution\ndatasets. Additionally, we also conduct a thorough analysis of the types of\nhallucinations that occur and present valuable insights. This research\nsignificantly contributes to the detection of reliable answers generated by\nLLMs and holds noteworthy implications for mitigating hallucination in the\nfuture work.",
            "Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath."
        ],
        "neg": []
    },
    {
        "query": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "pos": [
            "Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath."
        ],
        "neg": []
    },
    {
        "query": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "pos": [
            "Multimodal large language models (MLLMs) are flourishing, but mainly focus on\nimages with less attention than videos, especially in sub-fields such as prompt\nengineering, video chain-of-thought (CoT), and instruction tuning on videos.\nTherefore, we try to explore the collection of CoT datasets in videos to lead\nto video OpenQA and improve the reasoning ability of MLLMs. Unfortunately,\nmaking such video CoT datasets is not an easy task. Given that human annotation\nis too cumbersome and expensive, while machine-generated is not reliable due to\nthe hallucination issue, we develop an automatic annotation tool that combines\nmachine and human experts, under the active learning paradigm. Active learning\nis an interactive strategy between the model and human experts, in this way,\nthe workload of human labeling can be reduced and the quality of the dataset\ncan be guaranteed. With the help of the automatic annotation tool, we strive to\ncontribute three datasets, namely VideoCoT, TopicQA, TopicCoT. Furthermore, we\npropose a simple but effective benchmark based on the collected datasets, which\nexploits CoT to maximize the complex reasoning capabilities of MLLMs. Extensive\nexperiments demonstrate the effectiveness our solution."
        ],
        "neg": []
    },
    {
        "query": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "pos": [
            "Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath."
        ],
        "neg": []
    },
    {
        "query": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "pos": [
            "Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath."
        ],
        "neg": []
    },
    {
        "query": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "pos": [
            "We propose LogicVista, an evaluation benchmark that assesses the integrated\nlogical reasoning capabilities of multimodal large language models (MLLMs) in\nVisual contexts. Recent advancements in MLLMs have demonstrated various\nfascinating abilities, from crafting poetry based on an image to performing\nmathematical reasoning. However, there is still a lack of systematic evaluation\nof MLLMs' proficiency in logical reasoning tasks, which are essential for\nactivities like navigation and puzzle-solving. Thus we evaluate general logical\ncognition abilities across 5 logical reasoning tasks encompassing 9 different\ncapabilities, using a sample of 448 multiple-choice questions. Each question is\nannotated with the correct answer and the human-written reasoning behind the\nselection, enabling both open-ended and multiple-choice evaluation. A total of\n8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Available\nat https://github.com/Yijia-Xiao/LogicVista."
        ],
        "neg": []
    },
    {
        "query": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "pos": [
            "Prompt recovery, a crucial task in natural language processing, entails the\nreconstruction of prompts or instructions that language models use to convert\ninput text into a specific output. Although pivotal, the design and\neffectiveness of prompts represent a challenging and relatively untapped field\nwithin NLP research. This paper delves into an exhaustive investigation of\nprompt recovery methodologies, employing a spectrum of pre-trained language\nmodels and strategies. Our study is a comparative analysis aimed at gauging the\nefficacy of various models on a benchmark dataset, with the goal of pinpointing\nthe most proficient approach for prompt recovery. Through meticulous\nexperimentation and detailed analysis, we elucidate the outstanding performance\nof the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its\ncounterparts, showcasing its exceptional capability in accurately\nreconstructing prompts for text transformation tasks. Our findings offer a\nsignificant contribution to the existing knowledge on prompt recovery, shedding\nlight on the intricacies of prompt design and offering insightful perspectives\nfor future innovations in text rewriting and the broader field of natural\nlanguage processing."
        ],
        "neg": []
    },
    {
        "query": "Retrieval-augmented generation (RAG) techniques leverage the in-context\nlearning capabilities of large language models (LLMs) to produce more accurate\nand relevant responses. Originating from the simple 'retrieve-then-read'\napproach, the RAG framework has evolved into a highly flexible and modular\nparadigm. A critical component, the Query Rewriter module, enhances knowledge\nretrieval by generating a search-friendly query. This method aligns input\nquestions more closely with the knowledge base. Our research identifies\nopportunities to enhance the Query Rewriter module to Query Rewriter+ by\ngenerating multiple queries to overcome the Information Plateaus associated\nwith a single query and by rewriting questions to eliminate Ambiguity, thereby\nclarifying the underlying intent. We also find that current RAG systems exhibit\nissues with Irrelevant Knowledge; to overcome this, we propose the Knowledge\nFilter. These two modules are both based on the instruction-tuned Gemma-2B\nmodel, which together enhance response quality. The final identified issue is\nRedundant Retrieval; we introduce the Memory Knowledge Reservoir and the\nRetriever Trigger to solve this. The former supports the dynamic expansion of\nthe RAG system's knowledge base in a parameter-free manner, while the latter\noptimizes the cost for accessing external knowledge, thereby improving resource\nutilization and response efficiency. These four RAG modules synergistically\nimprove the response quality and efficiency of the RAG system. The\neffectiveness of these modules has been validated through experiments and\nablation studies across six common QA datasets. The source code can be accessed\nat https://github.com/Ancientshi/ERM4.",
        "pos": [
            "Symptom phenotypes are one of the key types of manifestations for diagnosis\nand treatment of various disease conditions. However, the diversity of symptom\nterminologies is one of the major obstacles hindering the analysis and\nknowledge sharing of various types of symptom-related medical data particularly\nin the fields of Traditional Chinese Medicine (TCM). Objective: This study\naimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to\nsupport the data mining of Chinese EMRs and real-world study in TCM field.\nMethods: To construct an integrated ontology of symptom phenotypes (ISPO), we\nmanually annotated classical TCM textbooks and large-scale Chinese electronic\nmedical records (EMRs) to collect symptom terms with support from a medical\ntext annotation system. Furthermore, to facilitate the semantic\ninteroperability between different terminologies, we incorporated public\navailable biomedical vocabularies by manual mapping between Chinese terms and\nEnglish terms with cross-references to source vocabularies. In addition, we\nevaluated the ISPO using independent clinical EMRs to provide a high-usable\nmedical ontology for clinical data analysis. Results: By integrating 78,696\ninpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and\ndictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition\nor contextual texts. Adhering to the taxonomical structure of the related\nanatomical systems of symptom phenotypes, ISPO provides 12 top-level categories\nand 79 middle-level sub-categories. The validation of data analysis showed the\nISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with\noccurrence rates of 0.5% in additional three independent curated clinical\ndatasets, which can demonstrate the significant value of ISPO in mapping\nclinical terms to ontologies."
        ],
        "neg": []
    },
    {
        "query": "Fairness in classification tasks has traditionally focused on bias removal\nfrom neural representations, but recent trends favor algorithmic methods that\nembed fairness into the training process. These methods steer models towards\nfair performance, preventing potential elimination of valuable information that\narises from representation manipulation. Reinforcement Learning (RL), with its\ncapacity for learning through interaction and adjusting reward functions to\nencourage desired behaviors, emerges as a promising tool in this domain. In\nthis paper, we explore the usage of RL to address bias in imbalanced\nclassification by scaling the reward function to mitigate bias. We employ the\ncontextual multi-armed bandit framework and adapt three popular RL algorithms\nto suit our objectives, demonstrating a novel approach to mitigating bias.",
        "pos": [
            "Language models trained on large amounts of data require careful tuning to be\nsafely deployed in real world. We revisit the guided decoding paradigm, where\nthe goal is to augment the logits of the base language model using the scores\nfrom a task-specific reward model. We propose a simple but efficient\nparameterization of the autoregressive reward model enabling fast and effective\nguided decoding. On detoxification and sentiment control tasks, we show that\nour efficient parameterization performs on par with RAD, a strong but less\nefficient guided decoding approach."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) are very proficient text generators. We leverage\nthis capability of LLMs to generate task-specific data via zero-shot prompting\nand promote cross-lingual transfer for low-resource target languages. Given\ntask-specific data in a source language and a teacher model trained on this\ndata, we propose using this teacher to label LLM generations and employ a set\nof simple data selection strategies that use the teacher's label probabilities.\nOur data selection strategies help us identify a representative subset of\ndiverse generations that help boost zero-shot accuracies while being efficient,\nin comparison to using all the LLM generations (without any subset selection).\nWe also highlight other important design choices that affect cross-lingual\nperformance such as the use of translations of source data and what labels are\nbest to use for the LLM generations. We observe significant performance gains\nacross sentiment analysis and natural language inference tasks (of up to a\nmaximum of 7.13 absolute points and 1.5 absolute points on average) across a\nnumber of target languages (Hindi, Marathi, Urdu, Swahili) and domains.",
        "pos": [
            "Retrieval augmented generation (RAG) with large language models (LLMs) for\nQuestion Answering (QA) entails furnishing relevant context within the prompt\nto facilitate the LLM in answer generation. During the generation, inaccuracies\nor hallucinations frequently occur due to two primary factors: inadequate or\ndistracting context in the prompts, and the inability of LLMs to effectively\nreason through the facts. In this paper, we investigate whether providing\naligned context via a carefully selected passage sequence leads to better\nanswer generation by the LLM for multi-hop QA. We introduce, \"GenSco\", a novel\napproach of selecting passages based on the predicted decomposition of the\nmulti-hop questions}. The framework consists of two distinct LLMs: (i)\nGenerator LLM, which is used for question decomposition and final answer\ngeneration; (ii) an auxiliary open-sourced LLM, used as the scorer, to\nsemantically guide the Generator for passage selection. The generator is\ninvoked only once for the answer generation, resulting in a cost-effective and\nefficient approach. We evaluate on three broadly established multi-hop question\nanswering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve\nan absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect\nto the best performing baselines over MuSiQue and 2WikiMultiHop respectively."
        ],
        "neg": []
    },
    {
        "query": "Traditional Chinese medicine (TCM) relies on specific combinations of herbs\nin prescriptions to treat symptoms and signs, a practice that spans thousands\nof years. Predicting TCM prescriptions presents a fascinating technical\nchallenge with practical implications. However, this task faces limitations due\nto the scarcity of high-quality clinical datasets and the intricate\nrelationship between symptoms and herbs. To address these issues, we introduce\nDigestDS, a new dataset containing practical medical records from experienced\nexperts in digestive system diseases. We also propose a method, TCM-FTP (TCM\nFine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs)\nthrough supervised fine-tuning on DigestDS. Additionally, we enhance\ncomputational efficiency using a low-rank adaptation technique. TCM-FTP also\nincorporates data augmentation by permuting herbs within prescriptions,\ncapitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves\nan F1-score of 0.8031, surpassing previous methods significantly. Furthermore,\nit demonstrates remarkable accuracy in dosage prediction, achieving a\nnormalized mean square error of 0.0604. In contrast, LLMs without fine-tuning\nperform poorly. Although LLMs have shown capabilities on a wide range of tasks,\nthis work illustrates the importance of fine-tuning for TCM prescription\nprediction, and we have proposed an effective way to do that.",
        "pos": [
            "Symptom phenotypes are one of the key types of manifestations for diagnosis\nand treatment of various disease conditions. However, the diversity of symptom\nterminologies is one of the major obstacles hindering the analysis and\nknowledge sharing of various types of symptom-related medical data particularly\nin the fields of Traditional Chinese Medicine (TCM). Objective: This study\naimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to\nsupport the data mining of Chinese EMRs and real-world study in TCM field.\nMethods: To construct an integrated ontology of symptom phenotypes (ISPO), we\nmanually annotated classical TCM textbooks and large-scale Chinese electronic\nmedical records (EMRs) to collect symptom terms with support from a medical\ntext annotation system. Furthermore, to facilitate the semantic\ninteroperability between different terminologies, we incorporated public\navailable biomedical vocabularies by manual mapping between Chinese terms and\nEnglish terms with cross-references to source vocabularies. In addition, we\nevaluated the ISPO using independent clinical EMRs to provide a high-usable\nmedical ontology for clinical data analysis. Results: By integrating 78,696\ninpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and\ndictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition\nor contextual texts. Adhering to the taxonomical structure of the related\nanatomical systems of symptom phenotypes, ISPO provides 12 top-level categories\nand 79 middle-level sub-categories. The validation of data analysis showed the\nISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with\noccurrence rates of 0.5% in additional three independent curated clinical\ndatasets, which can demonstrate the significant value of ISPO in mapping\nclinical terms to ontologies."
        ],
        "neg": []
    },
    {
        "query": "Traditional Chinese medicine (TCM) relies on specific combinations of herbs\nin prescriptions to treat symptoms and signs, a practice that spans thousands\nof years. Predicting TCM prescriptions presents a fascinating technical\nchallenge with practical implications. However, this task faces limitations due\nto the scarcity of high-quality clinical datasets and the intricate\nrelationship between symptoms and herbs. To address these issues, we introduce\nDigestDS, a new dataset containing practical medical records from experienced\nexperts in digestive system diseases. We also propose a method, TCM-FTP (TCM\nFine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs)\nthrough supervised fine-tuning on DigestDS. Additionally, we enhance\ncomputational efficiency using a low-rank adaptation technique. TCM-FTP also\nincorporates data augmentation by permuting herbs within prescriptions,\ncapitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves\nan F1-score of 0.8031, surpassing previous methods significantly. Furthermore,\nit demonstrates remarkable accuracy in dosage prediction, achieving a\nnormalized mean square error of 0.0604. In contrast, LLMs without fine-tuning\nperform poorly. Although LLMs have shown capabilities on a wide range of tasks,\nthis work illustrates the importance of fine-tuning for TCM prescription\nprediction, and we have proposed an effective way to do that.",
        "pos": [
            "Symptom phenotypes are one of the key types of manifestations for diagnosis\nand treatment of various disease conditions. However, the diversity of symptom\nterminologies is one of the major obstacles hindering the analysis and\nknowledge sharing of various types of symptom-related medical data particularly\nin the fields of Traditional Chinese Medicine (TCM). Objective: This study\naimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to\nsupport the data mining of Chinese EMRs and real-world study in TCM field.\nMethods: To construct an integrated ontology of symptom phenotypes (ISPO), we\nmanually annotated classical TCM textbooks and large-scale Chinese electronic\nmedical records (EMRs) to collect symptom terms with support from a medical\ntext annotation system. Furthermore, to facilitate the semantic\ninteroperability between different terminologies, we incorporated public\navailable biomedical vocabularies by manual mapping between Chinese terms and\nEnglish terms with cross-references to source vocabularies. In addition, we\nevaluated the ISPO using independent clinical EMRs to provide a high-usable\nmedical ontology for clinical data analysis. Results: By integrating 78,696\ninpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and\ndictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition\nor contextual texts. Adhering to the taxonomical structure of the related\nanatomical systems of symptom phenotypes, ISPO provides 12 top-level categories\nand 79 middle-level sub-categories. The validation of data analysis showed the\nISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with\noccurrence rates of 0.5% in additional three independent curated clinical\ndatasets, which can demonstrate the significant value of ISPO in mapping\nclinical terms to ontologies."
        ],
        "neg": []
    },
    {
        "query": "Functional magnetic resonance imaging (fMRI) is essential for developing\nencoding models that identify functional changes in language-related brain\nareas of individuals with Neurocognitive Disorders (NCD). While large language\nmodel (LLM)-based fMRI encoding has shown promise, existing studies\npredominantly focus on healthy, young adults, overlooking older NCD populations\nand cognitive level correlations. This paper explores language-related\nfunctional changes in older NCD adults using LLM-based fMRI encoding and brain\nscores, addressing current limitations. We analyze the correlation between\nbrain scores and cognitive scores at both whole-brain and language-related ROI\nlevels. Our findings reveal that higher cognitive abilities correspond to\nbetter brain scores, with correlations peaking in the middle temporal gyrus.\nThis study highlights the potential of fMRI encoding models and brain scores\nfor detecting early functional changes in NCD patients.",
        "pos": [
            "Multi-talker speech recognition and target-talker speech recognition, both\ninvolve transcription in multi-talker contexts, remain significant challenges.\nHowever, existing methods rarely attempt to simultaneously address both tasks.\nIn this study, we propose a pioneering approach to empower Whisper, which is a\nspeech foundation model, to tackle joint multi-talker and target-talker speech\nrecognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar\nseparator into its encoder to separate mixed embedding for multiple talkers;\n(ii) a Target Talker Identifier is introduced to identify the embedding flow of\nthe target talker on the fly, requiring only three-second enrollment speech as\na cue; (iii) soft prompt tuning for decoder is explored for better task\nadaptation. Our method outperforms previous methods on two- and three-talker\nLibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable\nzero-shot performance on multi-talker ASR on AishellMix Mandarin dataset."
        ],
        "neg": []
    },
    {
        "query": "Functional magnetic resonance imaging (fMRI) is essential for developing\nencoding models that identify functional changes in language-related brain\nareas of individuals with Neurocognitive Disorders (NCD). While large language\nmodel (LLM)-based fMRI encoding has shown promise, existing studies\npredominantly focus on healthy, young adults, overlooking older NCD populations\nand cognitive level correlations. This paper explores language-related\nfunctional changes in older NCD adults using LLM-based fMRI encoding and brain\nscores, addressing current limitations. We analyze the correlation between\nbrain scores and cognitive scores at both whole-brain and language-related ROI\nlevels. Our findings reveal that higher cognitive abilities correspond to\nbetter brain scores, with correlations peaking in the middle temporal gyrus.\nThis study highlights the potential of fMRI encoding models and brain scores\nfor detecting early functional changes in NCD patients.",
        "pos": [
            "Multi-talker speech recognition and target-talker speech recognition, both\ninvolve transcription in multi-talker contexts, remain significant challenges.\nHowever, existing methods rarely attempt to simultaneously address both tasks.\nIn this study, we propose a pioneering approach to empower Whisper, which is a\nspeech foundation model, to tackle joint multi-talker and target-talker speech\nrecognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar\nseparator into its encoder to separate mixed embedding for multiple talkers;\n(ii) a Target Talker Identifier is introduced to identify the embedding flow of\nthe target talker on the fly, requiring only three-second enrollment speech as\na cue; (iii) soft prompt tuning for decoder is explored for better task\nadaptation. Our method outperforms previous methods on two- and three-talker\nLibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable\nzero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.",
            "We present MELLE, a novel continuous-valued tokens based language modeling\napproach for text to speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which are originally designed for audio\ncompression and sacrifice fidelity compared to mel-spectrograms. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens. (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language models VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling discrete codes,\nachieves superior performance across multiple metrics, and, most importantly,\noffers a more streamlined paradigm. See https://aka.ms/melle for demos of our\nwork."
        ],
        "neg": []
    },
    {
        "query": "This study addresses a binary classification task to determine whether a text\nsequence, either a sentence or paragraph, is subjective or objective. The task\nspans five languages: Arabic, Bulgarian, English, German, and Italian, along\nwith a multilingual category. Our approach involved several key techniques.\nInitially, we preprocessed the data through parts of speech (POS) tagging,\nidentification of question marks, and application of attention masks. We\nfine-tuned the sentiment-based Transformer model\n'MarieAngeA13/Sentiment-Analysis-BERT' on our dataset. Given the imbalance with\nmore objective data, we implemented a custom classifier that assigned greater\nweight to objective data. Additionally, we translated non-English data into\nEnglish to maintain consistency across the dataset. Our model achieved notable\nresults, scoring top marks for the multilingual dataset (Macro F1=0.7121) and\nGerman (Macro F1=0.7908). It ranked second for Arabic (Macro F1=0.4908) and\nBulgarian (Macro F1=0.7169), third for Italian (Macro F1=0.7430), and ninth for\nEnglish (Macro F1=0.6893).",
        "pos": [
            "We present an overview of the second edition of the ArAIEval shared task,\norganized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In\nthis edition, ArAIEval offers two tasks: (i) detection of propagandistic\ntextual spans with persuasion techniques identification in tweets and news\narticles, and (ii) distinguishing between propagandistic and non-propagandistic\nmemes. A total of 14 teams participated in the final evaluation phase, with 6\nand 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams\nsubmitted system description papers. Across both tasks, we observed that\nfine-tuning transformer models such as AraBERT was at the core of the majority\nof the participating systems. We provide a description of the task setup,\nincluding a description of the dataset construction and the evaluation setup.\nWe further provide a brief overview of the participating systems. All datasets\nand evaluation scripts are released to the research community\n(https://araieval.gitlab.io/). We hope this will enable further research on\nthese important tasks in Arabic."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs.",
        "pos": [
            "Grounding external knowledge can enhance the factuality of responses in\ndialogue generation. However, excessive emphasis on it might result in the lack\nof engaging and diverse expressions. Through the introduction of randomness in\nsampling, current approaches can increase the diversity. Nevertheless, such\nsampling method could undermine the factuality in dialogue generation. In this\nstudy, to discover a solution for advancing creativity without relying on\nquestionable randomness and to subtly reconcile the factuality and diversity\nwithin the source-grounded paradigm, a novel method named DoGe is proposed.\nDoGe can dynamically alternate between the utilization of internal parameter\nknowledge and external source knowledge based on the model's factual\nconfidence. Extensive experiments on three widely-used datasets show that DoGe\ncan not only enhance response diversity but also maintain factuality, and it\nsignificantly surpasses other various decoding strategy baselines."
        ],
        "neg": []
    },
    {
        "query": "Low-resource languages often face challenges in acquiring high-quality\nlanguage data due to the reliance on translation-based methods, which can\nintroduce the translationese effect. This phenomenon results in translated\nsentences that lack fluency and naturalness in the target language. In this\npaper, we propose a novel approach for data collection by leveraging\nstoryboards to elicit more fluent and natural sentences. Our method involves\npresenting native speakers with visual stimuli in the form of storyboards and\ncollecting their descriptions without direct exposure to the source text. We\nconducted a comprehensive evaluation comparing our storyboard-based approach\nwith traditional text translation-based methods in terms of accuracy and\nfluency. Human annotators and quantitative metrics were used to assess\ntranslation quality. The results indicate a preference for text translation in\nterms of accuracy, while our method demonstrates worse accuracy but better\nfluency in the language focused.",
        "pos": [
            "Predicting emotions elicited by news headlines can be challenging as the task\nis largely influenced by the varying nature of people's interpretations and\nbackgrounds. Previous works have explored classifying discrete emotions\ndirectly from news headlines. We provide a different approach to tackling this\nproblem by utilizing people's explanations of their emotion, written in\nfree-text, on how they feel after reading a news headline. Using the dataset\nBU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the\nfree-text explanations have a strong correlation with the dominant emotion\nelicited by the headlines. The free-text explanations also contain more\nsentimental context than the news headlines alone and can serve as a better\ninput to emotion classification models. Therefore, in this work we explored\ngenerating emotion explanations from headlines by training a\nsequence-to-sequence transformer model and by using pretrained large language\nmodel, ChatGPT (GPT-4). We then used the generated emotion explanations for\nemotion classification. In addition, we also experimented with training the\npretrained T5 model for the intermediate task of explanation generation before\nfine-tuning it for emotion classification. Using McNemar's significance test,\nmethods that incorporate GPT-generated free-text emotion explanations\ndemonstrated significant improvement (P-value < 0.05) in emotion classification\nfrom headlines, compared to methods that only use headlines. This underscores\nthe value of using intermediate free-text explanations for emotion prediction\ntasks with headlines."
        ],
        "neg": []
    },
    {
        "query": "This paper introduces the Pandemic PACT Advanced Categorisation Engine\n(PPACE) along with its associated dataset. PPACE is a fine-tuned model\ndeveloped to automatically classify research abstracts from funded biomedical\nprojects according to WHO-aligned research priorities. This task is crucial for\nmonitoring research trends and identifying gaps in global health preparedness\nand response. Our approach builds on human-annotated projects, which are\nallocated one or more categories from a predefined list. A large language model\nis then used to generate `rationales' explaining the reasoning behind these\nannotations. This augmented data, comprising expert annotations and rationales,\nis subsequently used to fine-tune a smaller, more efficient model. Developed as\npart of the Pandemic PACT project, which aims to track and analyse research\nfunding and clinical evidence for a wide range of diseases with outbreak\npotential, PPACE supports informed decision-making by research funders,\npolicymakers, and independent researchers. We introduce and release both the\ntrained model and the instruction-based dataset used for its training. Our\nevaluation shows that PPACE significantly outperforms its baselines. The\nrelease of PPACE and its associated dataset offers valuable resources for\nresearchers in multilabel biomedical document classification and supports\nadvancements in aligning biomedical research with key global health priorities.",
        "pos": [
            "The recent advancements in large language models (LLMs) with billions of\nparameters have significantly boosted their performance across various\nreal-world applications. However, the inference processes for these models\nrequire substantial energy and computational resources, presenting considerable\ndeployment challenges. In contrast, human brains, which contain approximately\n86 billion biological neurons, exhibit significantly greater energy efficiency\ncompared to LLMs with a similar number of parameters. Inspired by this, we\nredesign 7 to 70 billion parameter LLMs using bio-plausible spiking mechanisms,\nemulating the efficient behavior of the human brain. We propose the first\nspiking large language model as recent LLMs termed SpikeLLM. Coupled with the\nproposed model, a novel spike-driven quantization framework named Optimal Brain\nSpiking is introduced to reduce the energy cost and accelerate inference speed\nvia two essential approaches: first (second)-order differentiation-based\nsalient channel detection, and per-channel salient outlier expansion with\nGeneralized Integrate-and-Fire neurons. Our proposed spike-driven quantization\ncan plug in main streams of quantization training methods. In the OmniQuant\npipeline, SpikeLLM significantly reduces 25.51% WikiText2 perplexity and\nimproves 3.08% average accuracy of 6 zero-shot datasets on a LLAMA2-7B 4A4W\nmodel. In the GPTQ pipeline, SpikeLLM realizes a sparse ternary quantization,\nwhich achieves additive in all linear layers. Compared with PB-LLM with similar\noperations, SpikeLLM also exceeds significantly. We will release our code on\nGitHub."
        ],
        "neg": []
    },
    {
        "query": "Knowledge distillation plays a key role in compressing the Large Language\nModels (LLMs), which boosts a small-size student model under large teacher\nmodels' guidance. However, existing LLM distillation methods overly rely on\nstudent-generated outputs, which may introduce generation errors and misguide\nthe distillation process. Moreover, the distillation loss functions introduced\nin previous art struggle to align the most informative part due to the complex\ndistribution of LLMs' outputs. To address these problems, we propose a\nmulti-granularity semantic revision method for LLM distillation. At the\nsequence level, we propose a sequence correction and re-generation (SCRG)\nstrategy. SCRG first calculates the semantic cognitive difference between the\nteacher and student to detect the error token, then corrects it with the\nteacher-generated one, and re-generates the sequence to reduce generation\nerrors and enhance generation diversity. At the token level, we design a\ndistribution adaptive clipping Kullback-Leibler (DAC-KL) loss as the\ndistillation objective function. DAC-KL loss exploits a learnable sub-network\nto adaptively extract semantically dense areas from the teacher's output,\navoiding the interference of redundant information in the distillation process.\nFinally, at the span level, we leverage the span priors of a sequence to\ncompute the probability correlations within spans, and constrain the teacher\nand student's probability correlations to be consistent, further enhancing the\ntransfer of semantic information. Extensive experiments across different model\nfamilies with parameters ranging from 0.1B to 13B demonstrate the superiority\nof our method compared to existing methods.",
        "pos": [
            "Large language models (LLMs) have achieved remarkable performance on various\nNLP tasks, yet their potential in more challenging and domain-specific task,\nsuch as finance, has not been fully explored. In this paper, we present\nCFinBench: a meticulously crafted, the most comprehensive evaluation benchmark\nto date, for assessing the financial knowledge of LLMs under Chinese context.\nIn practice, to better align with the career trajectory of Chinese financial\npractitioners, we build a systematic evaluation from 4 first-level categories:\n(1) Financial Subject: whether LLMs can memorize the necessary basic knowledge\nof financial subjects, such as economics, statistics and auditing. (2)\nFinancial Qualification: whether LLMs can obtain the needed financial qualified\ncertifications, such as certified public accountant, securities qualification\nand banking qualification. (3) Financial Practice: whether LLMs can fulfill the\npractical financial jobs, such as tax consultant, junior accountant and\nsecurities analyst. (4) Financial Law: whether LLMs can meet the requirement of\nfinancial laws and regulations, such as tax law, insurance law and economic\nlaw. CFinBench comprises 99,100 questions spanning 43 second-level categories\nwith 3 question types: single-choice, multiple-choice and judgment. We conduct\nextensive experiments of 50 representative LLMs with various model size on\nCFinBench. The results show that GPT4 and some Chinese-oriented models lead the\nbenchmark, with the highest average accuracy being 60.16%, highlighting the\nchallenge presented by CFinBench. The dataset and evaluation code are available\nat https://cfinbench.github.io/."
        ],
        "neg": []
    },
    {
        "query": "We present WojoodNER-2024, the second Arabic Named Entity Recognition (NER)\nShared Task. In WojoodNER-2024, we focus on fine-grained Arabic NER. We\nprovided participants with a new Arabic fine-grained NER dataset called\nwojoodfine, annotated with subtypes of entities. WojoodNER-2024 encompassed\nthree subtasks: (i) Closed-Track Flat Fine-Grained NER, (ii) Closed-Track\nNested Fine-Grained NER, and (iii) an Open-Track NER for the Israeli War on\nGaza. A total of 43 unique teams registered for this shared task. Five teams\nparticipated in the Flat Fine-Grained Subtask, among which two teams tackled\nthe Nested Fine-Grained Subtask and one team participated in the Open-Track NER\nSubtask. The winning teams achieved F-1 scores of 91% and 92% in the Flat\nFine-Grained and Nested Fine-Grained Subtasks, respectively. The sole team in\nthe Open-Track Subtask achieved an F-1 score of 73.7%.",
        "pos": [
            "We address a notable gap in Natural Language Processing (NLP) by introducing\na collection of resources designed to improve Machine Translation (MT) for\nlow-resource languages, with a specific focus on African languages. First, we\nintroduce two language models (LMs), Cheetah-1.2B and Cheetah-3.7B, with 1.2\nbillion and 3.7 billion parameters respectively. Next, we finetune the\naforementioned models to create toucan, an Afrocentric machine translation\nmodel designed to support 156 African language pairs. To evaluate Toucan, we\ncarefully develop an extensive machine translation benchmark, dubbed\nAfroLingu-MT, tailored for evaluating machine translation. Toucan significantly\noutperforms other models, showcasing its remarkable performance on MT for\nAfrican languages. Finally, we train a new model, spBLEU-1K, to enhance\ntranslation evaluation metrics, covering 1K languages, including 614 African\nlanguages. This work aims to advance the field of NLP, fostering cross-cultural\nunderstanding and knowledge exchange, particularly in regions with limited\nlanguage resources such as Africa. The GitHub repository for the Toucan project\nis available at https://github.com/UBC-NLP/Toucan."
        ],
        "neg": []
    },
    {
        "query": "Many fake news detection studies have achieved promising performance by\nextracting effective semantic and structure features from both content and\npropagation trees. However, it is challenging to apply them to practical\nsituations, especially when using the trained propagation-based models to\ndetect news with no propagation data. Towards this scenario, we study a new\ntask named cold-start fake news detection, which aims to detect content-only\nsamples with missing propagation. To achieve the task, we design a simple but\neffective Structure Adversarial Net (SAN) framework to learn transferable\nfeatures from available propagation to boost the detection of content-only\nsamples. SAN introduces a structure discriminator to estimate dissimilarities\namong learned features with and without propagation, and further learns\nstructure-invariant features to enhance the generalization of existing\npropagation-based methods for content-only samples. We conduct qualitative and\nquantitative experiments on three datasets. Results show the challenge of the\nnew task and the effectiveness of our SAN framework.",
        "pos": [
            "Scaling the size of a model enhances its capabilities but significantly\nincreases computation complexity. Mixture-of-Experts models (MoE) address the\nissue by allowing model size to scale up without substantially increasing\ntraining or inference costs. Despite their promising results, MoE models\nencounter several challenges. Primarily, for dynamic routing methods, the\ndispersion of training tokens across multiple experts can lead to underfitting,\nparticularly for infrequent tokens. Additionally, while fixed routing methods\ncan mitigate that issue, they compromise on the diversity of representations.\nIn this paper, we propose \\textbf{MaskMoE}, a method designed to enhance\ntoken-level learning by employing a routing \\textbf{mask}ing technique within\nthe \\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xperts model. MaskMoE is capable of\nmaintaining representation diversity while achieving more comprehensive\ntraining. Experimental results demonstrate that our method outperforms previous\ndominant Mixture-of-Experts models in terms of both perplexity (PPL) and\ndownstream task performance.",
            "Varying-size models are often required to deploy ASR systems under different\nhardware and/or application constraints such as memory and latency. To avoid\nredundant training and optimization efforts for individual models of different\nsizes, we present the dynamic encoder size approach, which jointly trains\nmultiple performant models within one supernet from scratch. These subnets of\nvarious sizes are layer-wise pruned from the supernet, and thus, enjoy full\nparameter sharing. By combining score-based pruning with supernet training, we\npropose two novel methods, Simple-Top-k and Iterative-Zero-Out, to\nautomatically select the best-performing subnets in a data-driven manner,\navoiding resource-intensive search efforts. Our experiments using CTC on both\nLibrispeech and TED-LIUM-v2 corpora show that our methods can achieve on-par\nperformance as individually trained models of each size category. Also, our\napproach consistently brings small performance improvements for the full-size\nsupernet."
        ],
        "neg": []
    },
    {
        "query": "Many fake news detection studies have achieved promising performance by\nextracting effective semantic and structure features from both content and\npropagation trees. However, it is challenging to apply them to practical\nsituations, especially when using the trained propagation-based models to\ndetect news with no propagation data. Towards this scenario, we study a new\ntask named cold-start fake news detection, which aims to detect content-only\nsamples with missing propagation. To achieve the task, we design a simple but\neffective Structure Adversarial Net (SAN) framework to learn transferable\nfeatures from available propagation to boost the detection of content-only\nsamples. SAN introduces a structure discriminator to estimate dissimilarities\namong learned features with and without propagation, and further learns\nstructure-invariant features to enhance the generalization of existing\npropagation-based methods for content-only samples. We conduct qualitative and\nquantitative experiments on three datasets. Results show the challenge of the\nnew task and the effectiveness of our SAN framework.",
        "pos": [
            "Scaling the size of a model enhances its capabilities but significantly\nincreases computation complexity. Mixture-of-Experts models (MoE) address the\nissue by allowing model size to scale up without substantially increasing\ntraining or inference costs. Despite their promising results, MoE models\nencounter several challenges. Primarily, for dynamic routing methods, the\ndispersion of training tokens across multiple experts can lead to underfitting,\nparticularly for infrequent tokens. Additionally, while fixed routing methods\ncan mitigate that issue, they compromise on the diversity of representations.\nIn this paper, we propose \\textbf{MaskMoE}, a method designed to enhance\ntoken-level learning by employing a routing \\textbf{mask}ing technique within\nthe \\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xperts model. MaskMoE is capable of\nmaintaining representation diversity while achieving more comprehensive\ntraining. Experimental results demonstrate that our method outperforms previous\ndominant Mixture-of-Experts models in terms of both perplexity (PPL) and\ndownstream task performance."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long- and Short-Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on 5 tasks demonstrate SMART's\nsuperior performance compared to previous widely adopted methods.",
        "pos": [
            "The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nTheoretical justifications and experiment results on five datasets show the\nvalidity and effectiveness of our proposed hybrid framework for training a\nhigh-quality reward model. By decoupling the reward modeling procedure and\nincorporating hybrid supervision, our HaF-RM framework offers a principled and\neffective approach to enhancing the performance and alignment of reward models,\na critical component in the responsible development of powerful language\nmodels. We release our code at https://haf-rm.github.io."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long- and Short-Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on 5 tasks demonstrate SMART's\nsuperior performance compared to previous widely adopted methods.",
        "pos": [
            "The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nTheoretical justifications and experiment results on five datasets show the\nvalidity and effectiveness of our proposed hybrid framework for training a\nhigh-quality reward model. By decoupling the reward modeling procedure and\nincorporating hybrid supervision, our HaF-RM framework offers a principled and\neffective approach to enhancing the performance and alignment of reward models,\na critical component in the responsible development of powerful language\nmodels. We release our code at https://haf-rm.github.io."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Speech-Copilot, a modular framework for\ninstruction-oriented speech-processing tasks that minimizes human effort in\ntoolset construction. Unlike end-to-end methods using large audio-language\nmodels, Speech-Copilot builds speech processing-specific toolsets by analyzing\npre-collected task instructions and breaking tasks into manageable sub-tasks.\nIt features a flexible agent based on large language models that performs tasks\nthrough program generation. Our approach achieves state-of-the-art performance\non the Dynamic-SUPERB benchmark, demonstrating its effectiveness across diverse\nspeech-processing tasks. Key contributions include: 1) developing an innovative\nframework for speech processing-specific toolset construction, 2) establishing\na high-performing agent based on large language models, and 3) offering a new\nperspective on addressing challenging instruction-oriented speech-processing\ntasks. Without additional training processes required by end-to-end approaches,\nour method provides a flexible and extendable solution for a wide range of\nspeech-processing applications.",
        "pos": [
            "Speech Integrated Large Language Models (SILLMs) combine large language\nmodels with speech perception to perform diverse tasks, such as emotion\nrecognition to speaker verification, demonstrating universal audio\nunderstanding capability. However, these models may amplify biases present in\ntraining data, potentially leading to biased access to information for\nmarginalized groups. This work introduces a curated spoken bias evaluation\ntoolkit and corresponding dataset. We evaluate gender bias in SILLMs across\nfour semantic-related tasks: speech-to-text translation (STT), spoken\ncoreference resolution (SCR), spoken sentence continuation (SSC), and spoken\nquestion answering (SQA). Our analysis reveals that bias levels are\nlanguage-dependent and vary with different evaluation methods. Our findings\nemphasize the necessity of employing multiple approaches to comprehensively\nassess biases in SILLMs, providing insights for developing fairer SILLM\nsystems.",
            "Using large language models (LLMs) for automatic evaluation has become an\nimportant evaluation method in NLP research. However, it is unclear whether\nthese LLM-based evaluators can be applied in real-world classrooms to assess\nstudent assignments. This empirical report shares how we use GPT-4 as an\nautomatic assignment evaluator in a university course with 1,028 students.\nBased on student responses, we find that LLM-based assignment evaluators are\ngenerally acceptable to students when students have free access to these\nLLM-based evaluators. However, students also noted that the LLM sometimes fails\nto adhere to the evaluation instructions. Additionally, we observe that\nstudents can easily manipulate the LLM-based evaluator to output specific\nstrings, allowing them to achieve high scores without meeting the assignment\nrubric. Based on student feedback and our experience, we provide several\nrecommendations for integrating LLM-based evaluators into future classrooms."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Speech-Copilot, a modular framework for\ninstruction-oriented speech-processing tasks that minimizes human effort in\ntoolset construction. Unlike end-to-end methods using large audio-language\nmodels, Speech-Copilot builds speech processing-specific toolsets by analyzing\npre-collected task instructions and breaking tasks into manageable sub-tasks.\nIt features a flexible agent based on large language models that performs tasks\nthrough program generation. Our approach achieves state-of-the-art performance\non the Dynamic-SUPERB benchmark, demonstrating its effectiveness across diverse\nspeech-processing tasks. Key contributions include: 1) developing an innovative\nframework for speech processing-specific toolset construction, 2) establishing\na high-performing agent based on large language models, and 3) offering a new\nperspective on addressing challenging instruction-oriented speech-processing\ntasks. Without additional training processes required by end-to-end approaches,\nour method provides a flexible and extendable solution for a wide range of\nspeech-processing applications.",
        "pos": [
            "Speech Integrated Large Language Models (SILLMs) combine large language\nmodels with speech perception to perform diverse tasks, such as emotion\nrecognition to speaker verification, demonstrating universal audio\nunderstanding capability. However, these models may amplify biases present in\ntraining data, potentially leading to biased access to information for\nmarginalized groups. This work introduces a curated spoken bias evaluation\ntoolkit and corresponding dataset. We evaluate gender bias in SILLMs across\nfour semantic-related tasks: speech-to-text translation (STT), spoken\ncoreference resolution (SCR), spoken sentence continuation (SSC), and spoken\nquestion answering (SQA). Our analysis reveals that bias levels are\nlanguage-dependent and vary with different evaluation methods. Our findings\nemphasize the necessity of employing multiple approaches to comprehensively\nassess biases in SILLMs, providing insights for developing fairer SILLM\nsystems."
        ],
        "neg": []
    },
    {
        "query": "In this work, we introduce Speech-Copilot, a modular framework for\ninstruction-oriented speech-processing tasks that minimizes human effort in\ntoolset construction. Unlike end-to-end methods using large audio-language\nmodels, Speech-Copilot builds speech processing-specific toolsets by analyzing\npre-collected task instructions and breaking tasks into manageable sub-tasks.\nIt features a flexible agent based on large language models that performs tasks\nthrough program generation. Our approach achieves state-of-the-art performance\non the Dynamic-SUPERB benchmark, demonstrating its effectiveness across diverse\nspeech-processing tasks. Key contributions include: 1) developing an innovative\nframework for speech processing-specific toolset construction, 2) establishing\na high-performing agent based on large language models, and 3) offering a new\nperspective on addressing challenging instruction-oriented speech-processing\ntasks. Without additional training processes required by end-to-end approaches,\nour method provides a flexible and extendable solution for a wide range of\nspeech-processing applications.",
        "pos": [
            "Speech Integrated Large Language Models (SILLMs) combine large language\nmodels with speech perception to perform diverse tasks, such as emotion\nrecognition to speaker verification, demonstrating universal audio\nunderstanding capability. However, these models may amplify biases present in\ntraining data, potentially leading to biased access to information for\nmarginalized groups. This work introduces a curated spoken bias evaluation\ntoolkit and corresponding dataset. We evaluate gender bias in SILLMs across\nfour semantic-related tasks: speech-to-text translation (STT), spoken\ncoreference resolution (SCR), spoken sentence continuation (SSC), and spoken\nquestion answering (SQA). Our analysis reveals that bias levels are\nlanguage-dependent and vary with different evaluation methods. Our findings\nemphasize the necessity of employing multiple approaches to comprehensively\nassess biases in SILLMs, providing insights for developing fairer SILLM\nsystems."
        ],
        "neg": []
    },
    {
        "query": "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and\nMistral-7B and then evaluating them across a comprehensive suite of\nmultilingual benchmarks that test reasoning, question answering, and reading\ncomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with\nsPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared\nto the baselines. We also devise a strategy to incorporate N-shot examples in\neach fine-tuning sample which further boosts the performance of these models by\n3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other\nmultilingual instruction tuning datasets on the same benchmarks along with\nbeing sample efficient and diverse, thereby reducing dataset creation costs.\nAdditionally, instruction tuning with sPhinX does not lead to regression on\nmost standard LLM benchmarks.",
        "pos": [
            "Synthetic data is becoming increasingly important for accelerating the\ndevelopment of language models, both large and small. Despite several\nsuccessful use cases, researchers also raised concerns around model collapse\nand drawbacks of imitating other models. This discrepancy can be attributed to\nthe fact that synthetic data varies in quality and diversity. Effective use of\nsynthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data\nby powerful models to teach a new skill or behavior to another model, we refer\nto this setting as Generative Teaching. We introduce AgentInstruct, an\nextensible agentic framework for automatically creating large amounts of\ndiverse and high-quality synthetic data. AgentInstruct can create both the\nprompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post\ntraining dataset of 25M pairs to teach language models different skills, such\nas text editing, creative writing, tool usage, coding, reading comprehension,\netc. The dataset can be used for instruction tuning of any base model. We\npost-train Mistral-7b with the data. When comparing the resulting model Orca-3\nto Mistral-7b-Instruct (which uses the same base model), we observe significant\nimprovements across many benchmarks. For example, 40% improvement on AGIEval,\n19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and\n45% improvement on AlpacaEval. Additionally, it consistently outperforms other\nmodels such as LLAMA-8B-instruct and GPT-3.5-turbo."
        ],
        "neg": []
    },
    {
        "query": "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and\nMistral-7B and then evaluating them across a comprehensive suite of\nmultilingual benchmarks that test reasoning, question answering, and reading\ncomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with\nsPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared\nto the baselines. We also devise a strategy to incorporate N-shot examples in\neach fine-tuning sample which further boosts the performance of these models by\n3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other\nmultilingual instruction tuning datasets on the same benchmarks along with\nbeing sample efficient and diverse, thereby reducing dataset creation costs.\nAdditionally, instruction tuning with sPhinX does not lead to regression on\nmost standard LLM benchmarks.",
        "pos": [
            "Since the release of ChatGPT, the field of Natural Language Processing has\nexperienced rapid advancements, particularly in Large Language Models (LLMs)\nand their multimodal counterparts, Large Multimodal Models (LMMs). Despite\ntheir impressive capabilities, LLMs often exhibit significant performance\ndisparities across different languages and cultural contexts, as demonstrated\nby various text-only benchmarks. However, current research lacks such\nbenchmarks for multimodal visio-linguistic settings. This work fills this gap\nby introducing M5, the first comprehensive benchmark designed to evaluate LMMs\non diverse vision-language tasks within a multilingual and multicultural\ncontext. M5 includes eight datasets covering five tasks and $41$ languages,\nwith a focus on underrepresented languages and culturally diverse images.\nFurthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a\nnew Visio-Linguistic Outlier Detection task, in which all evaluated open-source\nmodels fail to significantly surpass the random baseline. Through extensive\nevaluation and analyses, we highlight substantial task-agnostic performance\ndisparities between high- and low-resource languages. Moreover, we show that\nlarger models do not necessarily outperform smaller ones in a multilingual\nsetting.",
            "Prior research has demonstrated noticeable performance gains through the use\nof probabilistic tokenizations, an approach that involves employing multiple\ntokenizations of the same input string during the training phase of a language\nmodel. Despite these promising findings, modern large language models (LLMs)\nhave yet to be trained using probabilistic tokenizations. Interestingly, while\nthe tokenizers of these contemporary LLMs have the capability to generate\nmultiple tokenizations, this property remains underutilized.\n  In this work, we propose a novel method to leverage the multiple tokenization\ncapabilities of modern LLM tokenizers, aiming to enhance the self-consistency\nof LLMs in reasoning tasks. Our experiments indicate that when utilizing\nprobabilistic tokenizations, LLMs generate logically diverse reasoning paths,\nmoving beyond mere surface-level linguistic diversity.We carefully study\nprobabilistic tokenization and offer insights to explain the self consistency\nimprovements it brings through extensive experimentation on 5 LLM families and\n4 reasoning benchmarks."
        ],
        "neg": []
    },
    {
        "query": "State-of-the-art LLMs often rely on scale with high computational costs,\nwhich has sparked a research agenda to reduce parameter counts and costs\nwithout significantly impacting performance. Our study focuses on\nTransformer-based LLMs, specifically applying low-rank parametrization to the\ncomputationally intensive feedforward networks (FFNs), which are less studied\nthan attention blocks. In contrast to previous works, (i) we explore low-rank\nparametrization at scale, up to 1.3B parameters; (ii) within Transformer\nlanguage models rather than convolutional architectures; and (iii) starting\nfrom training from scratch. Experiments on the large RefinedWeb dataset show\nthat low-rank parametrization is both efficient (e.g., 2.6$\\times$ FFN speed-up\nwith 32\\% parameters) and effective during training. Interestingly, these\nstructured FFNs exhibit steeper scaling curves than the original models.\nMotivated by this finding, we develop the wide and structured networks\nsurpassing the current medium-sized and large-sized Transformer in perplexity\nand throughput performance. Our code is available at\nhttps://github.com/CLAIRE-Labo/StructuredFFN/tree/main.",
        "pos": [
            "A rapidly growing number of applications rely on a small set of closed-source\nlanguage models (LMs). This dependency might introduce novel security risks if\nLMs develop self-recognition capabilities. Inspired by human identity\nverification methods, we propose a novel approach for assessing\nself-recognition in LMs using model-generated \"security questions\". Our test\ncan be externally administered to keep track of frontier models as it does not\nrequire access to internal model parameters or output probabilities. We use our\ntest to examine self-recognition in ten of the most capable open- and\nclosed-source LMs currently publicly available. Our extensive experiments found\nno empirical evidence of general or consistent self-recognition in any examined\nLM. Instead, our results suggest that given a set of alternatives, LMs seek to\npick the \"best\" answer, regardless of its origin. Moreover, we find indications\nthat preferences about which models produce the best answers are consistent\nacross LMs. We additionally uncover novel insights on position bias\nconsiderations for LMs in multiple-choice settings."
        ],
        "neg": []
    },
    {
        "query": "The Internet of Things (IoT) network integrating billions of smart physical\ndevices embedded with sensors, software, and communication technologies is a\ncritical and rapidly expanding component of our modern world. The IoT ecosystem\nprovides a rich source of real-world modalities such as motion, thermal,\ngeolocation, imaging, depth, sensors, and audio to recognize the states of\nhumans and physical objects. Machine learning presents a rich opportunity to\nautomatically process IoT data at scale, enabling efficient inference for\nunderstanding human wellbeing, controlling physical devices, and\ninterconnecting smart cities. To realize this potential, we introduce IoT-LM,\nan open-source large multisensory language model tailored for the IoT\necosystem. IoT-LM is enabled by two technical contributions: the first is\nMultiIoT, the most expansive unified IoT dataset to date, encompassing over\n1.15 million samples from 12 modalities and 8 tasks prepared for multisensory\npre-training and instruction-tuning. The second is a new multisensory multitask\nadapter layer to condition pre-trained large language models on multisensory\nIoT data. Not only does IoT-LM yield substantial improvements on 8 supervised\nIoT classification tasks, but it also demonstrates new interactive\nquestion-answering, reasoning, and dialog capabilities conditioned on IoT\nsensors. We release IoT-LM's data sources and new multisensory language\nmodeling framework.",
        "pos": [
            "Multimodal foundation models that can holistically process text alongside\nimages, video, audio, and other sensory modalities are increasingly used in a\nvariety of real-world applications. However, it is challenging to characterize\nand study progress in multimodal foundation models, given the range of possible\nmodeling decisions, tasks, and domains. In this paper, we introduce Holistic\nEvaluation of Multimodal Models (HEMM) to systematically evaluate the\ncapabilities of multimodal foundation models across a set of 3 dimensions:\nbasic skills, information flow, and real-world use cases. Basic multimodal\nskills are internal abilities required to solve problems, such as learning\ninteractions across modalities, fine-grained alignment, multi-step reasoning,\nand the ability to handle external knowledge. Information flow studies how\nmultimodal content changes during a task through querying, translation,\nediting, and fusion. Use cases span domain-specific challenges introduced in\nreal-world multimedia, affective computing, natural sciences, healthcare, and\nhuman-computer interaction applications. Through comprehensive experiments\nacross the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,\nbasic skills, information flows, and use cases) that pose challenges to today's\nmodels, and (2) distill performance trends regarding how different modeling\ndimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,\nand instruction tuning objectives) influence performance. Our conclusions\nregarding challenging multimodal interactions, use cases, and tasks requiring\nreasoning and external knowledge, the benefits of data and model scale, and the\nimpacts of instruction tuning yield actionable insights for future work in\nmultimodal foundation models."
        ],
        "neg": []
    },
    {
        "query": "The Internet of Things (IoT) network integrating billions of smart physical\ndevices embedded with sensors, software, and communication technologies is a\ncritical and rapidly expanding component of our modern world. The IoT ecosystem\nprovides a rich source of real-world modalities such as motion, thermal,\ngeolocation, imaging, depth, sensors, and audio to recognize the states of\nhumans and physical objects. Machine learning presents a rich opportunity to\nautomatically process IoT data at scale, enabling efficient inference for\nunderstanding human wellbeing, controlling physical devices, and\ninterconnecting smart cities. To realize this potential, we introduce IoT-LM,\nan open-source large multisensory language model tailored for the IoT\necosystem. IoT-LM is enabled by two technical contributions: the first is\nMultiIoT, the most expansive unified IoT dataset to date, encompassing over\n1.15 million samples from 12 modalities and 8 tasks prepared for multisensory\npre-training and instruction-tuning. The second is a new multisensory multitask\nadapter layer to condition pre-trained large language models on multisensory\nIoT data. Not only does IoT-LM yield substantial improvements on 8 supervised\nIoT classification tasks, but it also demonstrates new interactive\nquestion-answering, reasoning, and dialog capabilities conditioned on IoT\nsensors. We release IoT-LM's data sources and new multisensory language\nmodeling framework.",
        "pos": [
            "Multimodal foundation models that can holistically process text alongside\nimages, video, audio, and other sensory modalities are increasingly used in a\nvariety of real-world applications. However, it is challenging to characterize\nand study progress in multimodal foundation models, given the range of possible\nmodeling decisions, tasks, and domains. In this paper, we introduce Holistic\nEvaluation of Multimodal Models (HEMM) to systematically evaluate the\ncapabilities of multimodal foundation models across a set of 3 dimensions:\nbasic skills, information flow, and real-world use cases. Basic multimodal\nskills are internal abilities required to solve problems, such as learning\ninteractions across modalities, fine-grained alignment, multi-step reasoning,\nand the ability to handle external knowledge. Information flow studies how\nmultimodal content changes during a task through querying, translation,\nediting, and fusion. Use cases span domain-specific challenges introduced in\nreal-world multimedia, affective computing, natural sciences, healthcare, and\nhuman-computer interaction applications. Through comprehensive experiments\nacross the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,\nbasic skills, information flows, and use cases) that pose challenges to today's\nmodels, and (2) distill performance trends regarding how different modeling\ndimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,\nand instruction tuning objectives) influence performance. Our conclusions\nregarding challenging multimodal interactions, use cases, and tasks requiring\nreasoning and external knowledge, the benefits of data and model scale, and the\nimpacts of instruction tuning yield actionable insights for future work in\nmultimodal foundation models."
        ],
        "neg": []
    },
    {
        "query": "In this study, we address the issue of API hallucinations in various software\nengineering contexts. We introduce CloudAPIBench, a new benchmark designed to\nmeasure API hallucination occurrences. CloudAPIBench also provides annotations\nfor frequencies of API occurrences in the public domain, allowing us to study\nAPI hallucinations at various frequency levels. Our findings reveal that Code\nLLMs struggle with low frequency APIs: for e.g., GPT-4o achieves only 38.58%\nvalid low frequency API invocations. We demonstrate that Documentation\nAugmented Generation (DAG) significantly improves performance for low frequency\nAPIs (increase to 47.94% with DAG) but negatively impacts high frequency APIs\nwhen using sub-optimal retrievers (a 39.02% absolute drop). To mitigate this,\nwe propose to intelligently trigger DAG where we check against an API index or\nleverage Code LLMs' confidence scores to retrieve only when needed. We\ndemonstrate that our proposed methods enhance the balance between low and high\nfrequency API performance, resulting in more reliable API invocations (8.20%\nabsolute improvement on CloudAPIBench for GPT-4o).",
        "pos": [
            "Prior research has enhanced the ability of Large Language Models (LLMs) to\nsolve logic puzzles using techniques such as chain-of-thought prompting or\nintroducing a symbolic representation. These frameworks are still usually\ninsufficient to solve complicated logical problems, such as Zebra puzzles, due\nto the inherent complexity of translating natural language clues into logical\nstatements. We introduce a multi-agent system, ZPS, that integrates LLMs with\nan off the shelf theorem prover. This system tackles the complex puzzle-solving\ntask by breaking down the problem into smaller, manageable parts, generating\nSMT (Satisfiability Modulo Theories) code to solve them with a theorem prover,\nand using feedback between the agents to repeatedly improve their answers. We\nalso introduce an automated grid puzzle grader to assess the correctness of our\npuzzle solutions and show that the automated grader is reliable by evaluating\nit in a user-study. Our approach shows improvement in all three LLMs we tested,\nwith GPT-4 showing 166% improvement in the number of fully correct solutions."
        ],
        "neg": []
    },
    {
        "query": "Foundation models, such as Large Language Models (LLMs) or Large Vision\nModels (LVMs), have emerged as one of the most powerful tools in the respective\nfields. However, unlike text and image data, graph data do not have a\ndefinitive structure, posing great challenges to developing a Graph Foundation\nModel (GFM). For example, current attempts at designing general graph models\neither transform graph data into a language format for LLM-based prediction or\nstill train a GNN model with LLM as an assistant. The former can handle\nunlimited tasks, while the latter captures graph structure much better -- yet,\nno existing work can achieve both simultaneously. In this paper, we identify\nthree key desirable properties of a GFM: self-supervised pretraining, fluidity\nin tasks, and graph awareness. To account for these properties, we extend the\nconventional language modeling to the graph domain and propose a novel\ngenerative graph language model GOFA to solve the problem. The model\ninterleaves randomly initialized GNN layers into a frozen pre-trained LLM so\nthat the semantic and structural modeling abilities are organically combined.\nGOFA is pre-trained on newly proposed graph-level next-word prediction,\nquestion-answering, and structural tasks to obtain the above GFM properties.\nThe pre-trained model is further fine-tuned on downstream tasks to obtain\ntask-solving ability. The fine-tuned model is evaluated on various downstream\ntasks, demonstrating a strong ability to solve structural and contextual\nproblems in zero-shot scenarios. The code is available at\nhttps://github.com/JiaruiFeng/GOFA.",
        "pos": [
            "Large language models (LLMs) have achieved remarkable performance on various\nNLP tasks, yet their potential in more challenging and domain-specific task,\nsuch as finance, has not been fully explored. In this paper, we present\nCFinBench: a meticulously crafted, the most comprehensive evaluation benchmark\nto date, for assessing the financial knowledge of LLMs under Chinese context.\nIn practice, to better align with the career trajectory of Chinese financial\npractitioners, we build a systematic evaluation from 4 first-level categories:\n(1) Financial Subject: whether LLMs can memorize the necessary basic knowledge\nof financial subjects, such as economics, statistics and auditing. (2)\nFinancial Qualification: whether LLMs can obtain the needed financial qualified\ncertifications, such as certified public accountant, securities qualification\nand banking qualification. (3) Financial Practice: whether LLMs can fulfill the\npractical financial jobs, such as tax consultant, junior accountant and\nsecurities analyst. (4) Financial Law: whether LLMs can meet the requirement of\nfinancial laws and regulations, such as tax law, insurance law and economic\nlaw. CFinBench comprises 99,100 questions spanning 43 second-level categories\nwith 3 question types: single-choice, multiple-choice and judgment. We conduct\nextensive experiments of 50 representative LLMs with various model size on\nCFinBench. The results show that GPT4 and some Chinese-oriented models lead the\nbenchmark, with the highest average accuracy being 60.16%, highlighting the\nchallenge presented by CFinBench. The dataset and evaluation code are available\nat https://cfinbench.github.io/."
        ],
        "neg": []
    },
    {
        "query": "Syntactic elements, such as word order and case markers, are fundamental in\nnatural language processing. Recent studies show that syntactic information\nboosts language model performance and offers clues for people to understand\ntheir learning mechanisms. Unlike languages with a fixed word order such as\nEnglish, Korean allows for varied word sequences, despite its canonical\nstructure, due to case markers that indicate the functions of sentence\ncomponents. This study explores whether Korean language models can accurately\ncapture this flexibility. We note that incomplete word orders and omitted case\nmarkers frequently appear in ordinary Korean communication. To investigate this\nfurther, we introduce the Syntactically Incomplete Korean (SIKO) dataset.\nThrough SIKO, we assessed Korean language models' flexibility with incomplete\nsyntax and confirmed the dataset's training value. Results indicate these\nmodels reflect Korean's inherent flexibility, accurately handling incomplete\ninputs. Moreover, fine-tuning with SIKO enhances the ability to handle common\nincomplete Korean syntactic forms. The dataset's simple construction process,\ncoupled with significant performance enhancements, solidifies its standing as\nan effective data augmentation technique.",
        "pos": [
            "Humans share a wide variety of images related to their personal experiences\nwithin conversations via instant messaging tools. However, existing works focus\non (1) image-sharing behavior in singular sessions, leading to limited\nlong-term social interaction, and (2) a lack of personalized image-sharing\nbehavior. In this work, we introduce Stark, a large-scale long-term multi-modal\nconversation dataset that covers a wide range of social personas in a\nmulti-modality format, time intervals, and images. To construct Stark\nautomatically, we propose a novel multi-modal contextualization framework, Mcu,\nthat generates long-term multi-modal dialogue distilled from ChatGPT and our\nproposed Plan-and-Execute image aligner. Using our Stark, we train a\nmulti-modal conversation model, Ultron 7B, which demonstrates impressive visual\nimagination ability. Furthermore, we demonstrate the effectiveness of our\ndataset in human evaluation. We make our source code and dataset publicly\navailable."
        ],
        "neg": []
    },
    {
        "query": "Syntactic elements, such as word order and case markers, are fundamental in\nnatural language processing. Recent studies show that syntactic information\nboosts language model performance and offers clues for people to understand\ntheir learning mechanisms. Unlike languages with a fixed word order such as\nEnglish, Korean allows for varied word sequences, despite its canonical\nstructure, due to case markers that indicate the functions of sentence\ncomponents. This study explores whether Korean language models can accurately\ncapture this flexibility. We note that incomplete word orders and omitted case\nmarkers frequently appear in ordinary Korean communication. To investigate this\nfurther, we introduce the Syntactically Incomplete Korean (SIKO) dataset.\nThrough SIKO, we assessed Korean language models' flexibility with incomplete\nsyntax and confirmed the dataset's training value. Results indicate these\nmodels reflect Korean's inherent flexibility, accurately handling incomplete\ninputs. Moreover, fine-tuning with SIKO enhances the ability to handle common\nincomplete Korean syntactic forms. The dataset's simple construction process,\ncoupled with significant performance enhancements, solidifies its standing as\nan effective data augmentation technique.",
        "pos": [
            "Humans share a wide variety of images related to their personal experiences\nwithin conversations via instant messaging tools. However, existing works focus\non (1) image-sharing behavior in singular sessions, leading to limited\nlong-term social interaction, and (2) a lack of personalized image-sharing\nbehavior. In this work, we introduce Stark, a large-scale long-term multi-modal\nconversation dataset that covers a wide range of social personas in a\nmulti-modality format, time intervals, and images. To construct Stark\nautomatically, we propose a novel multi-modal contextualization framework, Mcu,\nthat generates long-term multi-modal dialogue distilled from ChatGPT and our\nproposed Plan-and-Execute image aligner. Using our Stark, we train a\nmulti-modal conversation model, Ultron 7B, which demonstrates impressive visual\nimagination ability. Furthermore, we demonstrate the effectiveness of our\ndataset in human evaluation. We make our source code and dataset publicly\navailable."
        ],
        "neg": []
    },
    {
        "query": "In recent years, journalists have expressed concerns about the increasing\ntrend of news article avoidance, especially within specific domains. This issue\nhas been exacerbated by the rise of recommender systems. Our research indicates\nthat recommender systems should consider avoidance as a fundamental factor. We\nargue that news articles can be characterized by three principal elements:\nexposure, relevance, and avoidance, all of which are closely interconnected. To\naddress these challenges, we introduce AWRS, an Avoidance-Aware Recommender\nSystem. This framework incorporates avoidance awareness when recommending news,\nbased on the premise that news article avoidance conveys significant\ninformation about user preferences. Evaluation results on three news datasets\nin different languages (English, Norwegian, and Japanese) demonstrate that our\nmethod outperforms existing approaches.",
        "pos": [
            "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
        ],
        "neg": []
    },
    {
        "query": "As LLMs become increasingly prevalent, it is interesting to consider how\n``creative'' these models can be. From cognitive science, creativity consists\nof at least two key characteristics: \\emph{convergent} thinking (purposefulness\nto achieve a given goal) and \\emph{divergent} thinking (adaptability to new\nenvironments or constraints) \\citep{runco2003critical}. In this work, we\nintroduce a framework for quantifying LLM creativity that incorporates the two\ncharacteristics. This is achieved by (1) Denial Prompting pushes LLMs to come\nup with more creative solutions to a given problem by incrementally imposing\nnew constraints on the previous solution, compelling LLMs to adopt new\nstrategies, and (2) defining and computing the NeoGauge metric which examines\nboth convergent and divergent thinking in the generated creative responses by\nLLMs. We apply the proposed framework on Codeforces problems, a natural data\nsource for collecting human coding solutions. We quantify NeoGauge for various\nproprietary and open-source models and find that even the most creative model,\nGPT-4, still falls short of demonstrating human-like creativity. We also\nexperiment with advanced reasoning strategies (MCTS, self-correction, etc.) and\nobserve no significant improvement in creativity. As a by-product of our\nanalysis, we release NeoCoder dataset for reproducing our results on future\nmodels.",
        "pos": [
            "Hallucinations -- the generation of untrue claims -- pose a challenge to the\napplication of large language models (LLMs) [1] thereby motivating the\ndevelopment of metrics to evaluate factual precision. We observe that popular\nmetrics using the Decompose-Then-Verify framework, such as FActScore [2], can\nbe manipulated by adding obvious or repetitive claims to artificially inflate\nscores. We expand the FActScore dataset to design and analyze factual precision\nmetrics, demonstrating that models can be trained to achieve high scores under\nexisting metrics through exploiting the issues we identify. This motivates our\nnew customizable plug-and-play subclaim selection component called Core, which\nfilters down individual subclaims according to their uniqueness and\ninformativeness. Metrics augmented by Core are substantially more robust as\nshown in head-to-head comparisons. We release an evaluation framework\nsupporting the modular use of Core (https://github.com/zipJiang/Core) and\nvarious decomposition strategies, and we suggest its adoption by the LLM\ncommunity.\n  [1] Hong et al., \"The Hallucinations Leaderboard -- An Open Effort to Measure\nHallucinations in Large Language Models\", arXiv:2404.05904v2 [cs.CL].\n  [2] Min et al., \"FActScore: Fine-grained Atomic Evaluation of Factual\nPrecision in Long Form Text Generation\", arXiv:2305.14251v2 [cs.CL]."
        ],
        "neg": []
    },
    {
        "query": "The rapid growth of biomedical knowledge has outpaced our ability to\nefficiently extract insights and generate novel hypotheses. Large language\nmodels (LLMs) have emerged as a promising tool to revolutionize knowledge\ninteraction and potentially accelerate biomedical discovery. In this paper, we\npresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.\nWe construct a dataset of background-hypothesis pairs from biomedical\nliterature, carefully partitioned into training, seen, and unseen test sets\nbased on publication date to mitigate data contamination. Using this dataset,\nwe assess the hypothesis generation capabilities of top-tier instructed models\nin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of\nuncertainty, a crucial aspect of scientific discovery, we incorporate tool use\nand multi-agent interactions in our evaluation framework. Furthermore, we\npropose four novel metrics grounded in extensive literature review to evaluate\nthe quality of generated hypotheses, considering both LLM-based and human\nassessments. Our experiments yield two key findings: 1) LLMs can generate novel\nand validated hypotheses, even when tested on literature unseen during\ntraining, and 2) Increasing uncertainty through multi-agent interactions and\ntool use can facilitate diverse candidate generation and improve zero-shot\nhypothesis generation performance. However, we also observe that the\nintegration of additional knowledge through few-shot learning and tool use may\nnot always lead to performance gains, highlighting the need for careful\nconsideration of the type and scope of external knowledge incorporated. These\nfindings underscore the potential of LLMs as powerful aids in biomedical\nhypothesis generation and provide valuable insights to guide further research\nin this area.",
        "pos": [
            "In this perspective paper, we introduce the concept of Specialized Generalist\nArtificial Intelligence (SGAI or simply SGI) as a crucial milestone toward\nArtificial General Intelligence (AGI). Compared to directly scaling general\nabilities, SGI is defined as AI that specializes in at least one task,\nsurpassing human experts, while also retaining general abilities. This fusion\npath enables SGI to rapidly achieve high-value areas. We categorize SGI into\nthree stages based on the level of mastery over professional skills and\ngenerality performance. Additionally, we discuss the necessity of SGI in\naddressing issues associated with large language models, such as their\ninsufficient generality, specialized capabilities, uncertainty in innovation,\nand practical applications. Furthermore, we propose a conceptual framework for\ndeveloping SGI that integrates the strengths of Systems 1 and 2 cognitive\nprocessing. This framework comprises three layers and four key components,\nwhich focus on enhancing individual abilities and facilitating collaborative\nevolution. We conclude by summarizing the potential challenges and suggesting\nfuture directions. We hope that the proposed SGI will provide insights into\nfurther research and applications towards achieving AGI."
        ],
        "neg": []
    },
    {
        "query": "The rapid growth of biomedical knowledge has outpaced our ability to\nefficiently extract insights and generate novel hypotheses. Large language\nmodels (LLMs) have emerged as a promising tool to revolutionize knowledge\ninteraction and potentially accelerate biomedical discovery. In this paper, we\npresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.\nWe construct a dataset of background-hypothesis pairs from biomedical\nliterature, carefully partitioned into training, seen, and unseen test sets\nbased on publication date to mitigate data contamination. Using this dataset,\nwe assess the hypothesis generation capabilities of top-tier instructed models\nin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of\nuncertainty, a crucial aspect of scientific discovery, we incorporate tool use\nand multi-agent interactions in our evaluation framework. Furthermore, we\npropose four novel metrics grounded in extensive literature review to evaluate\nthe quality of generated hypotheses, considering both LLM-based and human\nassessments. Our experiments yield two key findings: 1) LLMs can generate novel\nand validated hypotheses, even when tested on literature unseen during\ntraining, and 2) Increasing uncertainty through multi-agent interactions and\ntool use can facilitate diverse candidate generation and improve zero-shot\nhypothesis generation performance. However, we also observe that the\nintegration of additional knowledge through few-shot learning and tool use may\nnot always lead to performance gains, highlighting the need for careful\nconsideration of the type and scope of external knowledge incorporated. These\nfindings underscore the potential of LLMs as powerful aids in biomedical\nhypothesis generation and provide valuable insights to guide further research\nin this area.",
        "pos": [
            "In this perspective paper, we introduce the concept of Specialized Generalist\nArtificial Intelligence (SGAI or simply SGI) as a crucial milestone toward\nArtificial General Intelligence (AGI). Compared to directly scaling general\nabilities, SGI is defined as AI that specializes in at least one task,\nsurpassing human experts, while also retaining general abilities. This fusion\npath enables SGI to rapidly achieve high-value areas. We categorize SGI into\nthree stages based on the level of mastery over professional skills and\ngenerality performance. Additionally, we discuss the necessity of SGI in\naddressing issues associated with large language models, such as their\ninsufficient generality, specialized capabilities, uncertainty in innovation,\nand practical applications. Furthermore, we propose a conceptual framework for\ndeveloping SGI that integrates the strengths of Systems 1 and 2 cognitive\nprocessing. This framework comprises three layers and four key components,\nwhich focus on enhancing individual abilities and facilitating collaborative\nevolution. We conclude by summarizing the potential challenges and suggesting\nfuture directions. We hope that the proposed SGI will provide insights into\nfurther research and applications towards achieving AGI."
        ],
        "neg": []
    },
    {
        "query": "The rapid growth of biomedical knowledge has outpaced our ability to\nefficiently extract insights and generate novel hypotheses. Large language\nmodels (LLMs) have emerged as a promising tool to revolutionize knowledge\ninteraction and potentially accelerate biomedical discovery. In this paper, we\npresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.\nWe construct a dataset of background-hypothesis pairs from biomedical\nliterature, carefully partitioned into training, seen, and unseen test sets\nbased on publication date to mitigate data contamination. Using this dataset,\nwe assess the hypothesis generation capabilities of top-tier instructed models\nin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of\nuncertainty, a crucial aspect of scientific discovery, we incorporate tool use\nand multi-agent interactions in our evaluation framework. Furthermore, we\npropose four novel metrics grounded in extensive literature review to evaluate\nthe quality of generated hypotheses, considering both LLM-based and human\nassessments. Our experiments yield two key findings: 1) LLMs can generate novel\nand validated hypotheses, even when tested on literature unseen during\ntraining, and 2) Increasing uncertainty through multi-agent interactions and\ntool use can facilitate diverse candidate generation and improve zero-shot\nhypothesis generation performance. However, we also observe that the\nintegration of additional knowledge through few-shot learning and tool use may\nnot always lead to performance gains, highlighting the need for careful\nconsideration of the type and scope of external knowledge incorporated. These\nfindings underscore the potential of LLMs as powerful aids in biomedical\nhypothesis generation and provide valuable insights to guide further research\nin this area.",
        "pos": [
            "In this perspective paper, we introduce the concept of Specialized Generalist\nArtificial Intelligence (SGAI or simply SGI) as a crucial milestone toward\nArtificial General Intelligence (AGI). Compared to directly scaling general\nabilities, SGI is defined as AI that specializes in at least one task,\nsurpassing human experts, while also retaining general abilities. This fusion\npath enables SGI to rapidly achieve high-value areas. We categorize SGI into\nthree stages based on the level of mastery over professional skills and\ngenerality performance. Additionally, we discuss the necessity of SGI in\naddressing issues associated with large language models, such as their\ninsufficient generality, specialized capabilities, uncertainty in innovation,\nand practical applications. Furthermore, we propose a conceptual framework for\ndeveloping SGI that integrates the strengths of Systems 1 and 2 cognitive\nprocessing. This framework comprises three layers and four key components,\nwhich focus on enhancing individual abilities and facilitating collaborative\nevolution. We conclude by summarizing the potential challenges and suggesting\nfuture directions. We hope that the proposed SGI will provide insights into\nfurther research and applications towards achieving AGI.",
            "Summarization is a fundamental task in natural language processing (NLP) and\nsince large language models (LLMs), such as GPT-4 and Claude, come out,\nincreasing attention has been paid to long-form summarization whose input\nsequences are much longer, indicating more information contained.\n  The current evaluation metrics either use similarity-based metrics like ROUGE\nand BERTScore which rely on similarity and fail to consider informativeness or\nLLM-based metrics, lacking quantitative analysis of information richness and\nare rather subjective.\n  In this paper, we propose a new evaluation metric called EVA-Score using\nAtomic Fact Chain Generation and Document-level Relation Extraction together to\nautomatically calculate the informativeness and give a definite number as an\ninformation score. Experiment results show that our metric shows a\nstate-of-the-art correlation with humans. We also re-evaluate the performance\nof LLMs on long-form summarization comprehensively from the information aspect,\nforecasting future ways to use LLMs for long-form summarization."
        ],
        "neg": []
    },
    {
        "query": "In multilingual settings, non-Latin scripts and low-resource languages are\nusually disadvantaged in terms of language models' utility, efficiency, and\ncost. Specifically, previous studies have reported multiple modeling biases\nthat the current tokenization algorithms introduce to non-Latin script\nlanguages, the main one being over-segmentation. In this work, we propose\nMAGNET; multilingual adaptive gradient-based tokenization to reduce\nover-segmentation via adaptive gradient-based subword tokenization. MAGNET\nlearns to predict segment boundaries between byte tokens in a sequence via\nsub-modules within the model, which act as internal boundary predictors\n(tokenizers). Previous gradient-based tokenization methods aimed for uniform\ncompression across sequences by integrating a single boundary predictor during\ntraining and optimizing it end-to-end through stochastic reparameterization\nalongside the next token prediction objective. However, this approach still\nresults in over-segmentation for non-Latin script languages in multilingual\nsettings. In contrast, MAGNET offers a customizable architecture where\nbyte-level sequences are routed through language-script-specific predictors,\neach optimized for its respective language script. This modularity enforces\nequitable segmentation granularity across different language scripts compared\nto previous methods. Through extensive experiments, we demonstrate that in\naddition to reducing segmentation disparities, MAGNET also enables faster\nlanguage modelling and improves downstream utility.",
        "pos": [
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) with in-context learning have significantly\nimproved the performance of text-to-SQL task. Previous works generally focus on\nusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.\nHowever, they are mostly hard to handle large databases with numerous tables\nand columns, and usually ignore the significance of pre-processing database and\nextracting valuable information for more efficient prompt engineering. Based on\nabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework for\nin-context prompt engineering, which consists of three modules that retrieve\nconcise tables and columns as schema, and targeted examples for in-context\nlearning. Experiment results demonstrate that our model achieves better\nperformance than several competitive baselines on public datasets BIRD and\nSpider.",
        "pos": [
            "A large number of studies have emerged for Multimodal Knowledge Graph\nCompletion (MKGC) to predict the missing links in MKGs. However, fewer studies\nhave been proposed to study the inductive MKGC (IMKGC) involving emerging\nentities unseen during training. Existing inductive approaches focus on\nlearning textual entity representations, which neglect rich semantic\ninformation in visual modality. Moreover, they focus on aggregating structural\nneighbors from existing KGs, which of emerging entities are usually limited.\nHowever, the semantic neighbors are decoupled from the topology linkage and\nusually imply the true target entity. In this paper, we propose the IMKGC task\nand a semantic neighbor retrieval-enhanced IMKGC framework CMR, where the\ncontrast brings the helpful semantic neighbors close, and then the memorize\nsupports semantic neighbor retrieval to enhance inference. Specifically, we\nfirst propose a unified cross-modal contrastive learning to simultaneously\ncapture the textual-visual and textual-textual correlations of query-entity\npairs in a unified representation space. The contrastive learning increases the\nsimilarity of positive query-entity pairs, therefore making the representations\nof helpful semantic neighbors close. Then, we explicitly memorize the knowledge\nrepresentations to support the semantic neighbor retrieval. At test time, we\nretrieve the nearest semantic neighbors and interpolate them to the\nquery-entity similarity distribution to augment the final prediction. Extensive\nexperiments validate the effectiveness of CMR on three inductive MKGC datasets.\nCodes are available at https://github.com/OreOZhao/CMR."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) with in-context learning have significantly\nimproved the performance of text-to-SQL task. Previous works generally focus on\nusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.\nHowever, they are mostly hard to handle large databases with numerous tables\nand columns, and usually ignore the significance of pre-processing database and\nextracting valuable information for more efficient prompt engineering. Based on\nabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework for\nin-context prompt engineering, which consists of three modules that retrieve\nconcise tables and columns as schema, and targeted examples for in-context\nlearning. Experiment results demonstrate that our model achieves better\nperformance than several competitive baselines on public datasets BIRD and\nSpider.",
        "pos": [
            "Instruction tuning as an effective technique aligns the outputs of large\nlanguage models (LLMs) with human preference. But how to generate the seasonal\nmulti-turn dialogues from raw documents for instruction tuning still requires\nfurther exploration. In this paper, we present a novel framework named R2S that\nleverages the CoD-Chain of Dialogue logic to guide large language models (LLMs)\nin generating knowledge-intensive multi-turn dialogues for instruction tuning.\nBy integrating raw documents from both open-source datasets and domain-specific\nweb-crawled documents into a benchmark K-BENCH, we cover diverse areas such as\nWikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach\nfirst decides the logic flow of the current dialogue and then prompts LLMs to\nproduce key phrases for sourcing relevant response content. This methodology\nenables the creation of the G I NSTRUCT instruction dataset, retaining raw\ndocument knowledge within dialoguestyle interactions. Utilizing this dataset,\nwe fine-tune GLLM, a model designed to transform raw documents into structured\nmulti-turn dialogues, thereby injecting comprehensive domain knowledge into the\nSFT model for enhanced instruction tuning. This work signifies a stride towards\nrefining the adaptability and effectiveness of LLMs in processing and\ngenerating more accurate, contextually nuanced responses across various fields."
        ],
        "neg": []
    },
    {
        "query": "Rule-based language processing systems have been overshadowed by neural\nsystems in terms of utility, but it remains unclear whether neural NLP systems,\nin practice, learn the grammar rules that humans use. This work aims to shed\nlight on the issue by evaluating state-of-the-art LLMs in a task of\nmorphological analysis of complex Finnish noun forms. We generate the forms\nusing an FST tool, and they are unlikely to have occurred in the training sets\nof the LLMs, therefore requiring morphological generalisation capacity. We find\nthat GPT-4-turbo has some difficulties in the task while GPT-3.5-turbo\nstruggles and smaller models Llama2-70B and Poro-34B fail nearly completely.",
        "pos": [
            "Test data is said to be out-of-distribution (OOD) when it unexpectedly\ndiffers from the training data, a common challenge in real-world use cases of\nmachine learning. Although OOD generalisation has gained interest in recent\nyears, few works have focused on OOD generalisation in spoken language\nunderstanding (SLU) tasks. To facilitate research on this topic, we introduce a\nmodified version of the popular SLU dataset SLURP, featuring data splits for\ntesting OOD generalisation in the SLU task. We call our modified dataset SLURP\nFor OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find\nend-to-end SLU models to have limited capacity for generalisation. Furthermore,\nby employing model interpretability techniques, we shed light on the factors\ncontributing to the generalisation difficulties of the models. To improve the\ngeneralisation, we experiment with two techniques, which improve the results on\nsome, but not all the splits, emphasising the need for new techniques."
        ],
        "neg": []
    },
    {
        "query": "Rule-based language processing systems have been overshadowed by neural\nsystems in terms of utility, but it remains unclear whether neural NLP systems,\nin practice, learn the grammar rules that humans use. This work aims to shed\nlight on the issue by evaluating state-of-the-art LLMs in a task of\nmorphological analysis of complex Finnish noun forms. We generate the forms\nusing an FST tool, and they are unlikely to have occurred in the training sets\nof the LLMs, therefore requiring morphological generalisation capacity. We find\nthat GPT-4-turbo has some difficulties in the task while GPT-3.5-turbo\nstruggles and smaller models Llama2-70B and Poro-34B fail nearly completely.",
        "pos": [
            "Test data is said to be out-of-distribution (OOD) when it unexpectedly\ndiffers from the training data, a common challenge in real-world use cases of\nmachine learning. Although OOD generalisation has gained interest in recent\nyears, few works have focused on OOD generalisation in spoken language\nunderstanding (SLU) tasks. To facilitate research on this topic, we introduce a\nmodified version of the popular SLU dataset SLURP, featuring data splits for\ntesting OOD generalisation in the SLU task. We call our modified dataset SLURP\nFor OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find\nend-to-end SLU models to have limited capacity for generalisation. Furthermore,\nby employing model interpretability techniques, we shed light on the factors\ncontributing to the generalisation difficulties of the models. To improve the\ngeneralisation, we experiment with two techniques, which improve the results on\nsome, but not all the splits, emphasising the need for new techniques."
        ],
        "neg": []
    },
    {
        "query": "Reduplication and repetition, though similar in form, serve distinct\nlinguistic purposes. Reduplication is a deliberate morphological process used\nto express grammatical, semantic, or pragmatic nuances, while repetition is\noften unintentional and indicative of disfluency. This paper presents the first\nlarge-scale study of reduplication and repetition in speech using computational\nlinguistics. We introduce IndicRedRep, a new publicly available dataset\ncontaining Hindi, Telugu, and Marathi text annotated with reduplication and\nrepetition at the word level. We evaluate transformer-based models for\nmulti-class reduplication and repetition token classification, utilizing the\nReparandum-Interregnum-Repair structure to distinguish between the two\nphenomena. Our models achieve macro F1 scores of up to 85.62% in Hindi, 83.95%\nin Telugu, and 84.82% in Marathi for reduplication-repetition classification.",
        "pos": [
            "In document-level neural machine translation (DocNMT), multi-encoder\napproaches are common in encoding context and source sentences. Recent studies\n\\cite{li-etal-2020-multi-encoder} have shown that the context encoder generates\nnoise and makes the model robust to the choice of context. This paper further\ninvestigates this observation by explicitly modelling context encoding through\nmulti-task learning (MTL) to make the model sensitive to the choice of context.\nWe conduct experiments on cascade MTL architecture, which consists of one\nencoder and two decoders. Generation of the source from the context is\nconsidered an auxiliary task, and generation of the target from the source is\nthe main task. We experimented with German--English language pairs on News,\nTED, and Europarl corpora. Evaluation results show that the proposed MTL\napproach performs better than concatenation-based and multi-encoder DocNMT\nmodels in low-resource settings and is sensitive to the choice of context.\nHowever, we observe that the MTL models are failing to generate the source from\nthe context. These observations align with the previous studies, and this might\nsuggest that the available document-level parallel corpora are not\ncontext-aware, and a robust sentence-level model can outperform the\ncontext-aware models."
        ],
        "neg": []
    },
    {
        "query": "Hallucination is often regarded as a major impediment for using large\nlanguage models (LLMs), especially for knowledge-intensive tasks. Even when the\ntraining corpus consists solely of true statements, language models still\ngenerate hallucinations in the form of amalgamations of multiple facts. We coin\nthis phenomenon as ``knowledge overshadowing'': when we query knowledge from a\nlanguage model with multiple conditions, some conditions overshadow others,\nleading to hallucinated outputs. This phenomenon partially stems from training\ndata imbalance, which we verify on both pretrained models and fine-tuned\nmodels, over a wide range of LM model families and sizes.From a theoretical\npoint of view, knowledge overshadowing can be interpreted as\nover-generalization of the dominant conditions (patterns). We show that the\nhallucination rate grows with both the imbalance ratio (between the popular and\nunpopular condition) and the length of dominant condition description,\nconsistent with our derived generalization bound. Finally, we propose to\nutilize overshadowing conditions as a signal to catch hallucination before it\nis produced, along with a training-free self-contrastive decoding method to\nalleviate hallucination during inference. Our proposed approach showcases up to\n82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,\nwith different models and datasets.",
        "pos": [
            "Extensive previous research has focused on post-training knowledge editing\n(KE) for language models (LMs) to ensure that knowledge remains accurate and\nup-to-date. One desired property and open question in KE is to let edited LMs\ncorrectly handle ripple effects, where LM is expected to answer its logically\nrelated knowledge accurately. In this paper, we answer the question of why most\nKE methods still create messy ripple effects. We conduct extensive analysis and\nidentify a salient indicator, GradSim, that effectively reveals when and why\nupdated knowledge ripples in LMs. GradSim is computed by the cosine similarity\nbetween gradients of the original fact and its related knowledge. We observe a\nstrong positive correlation between ripple effect performance and GradSim\nacross different LMs, KE methods, and evaluation metrics. Further\ninvestigations into three counter-intuitive failure cases (Negation,\nOver-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures\nare often associated with very low GradSim. This finding validates that GradSim\nis an effective indicator of when knowledge ripples in LMs."
        ],
        "neg": []
    },
    {
        "query": "Hallucination is often regarded as a major impediment for using large\nlanguage models (LLMs), especially for knowledge-intensive tasks. Even when the\ntraining corpus consists solely of true statements, language models still\ngenerate hallucinations in the form of amalgamations of multiple facts. We coin\nthis phenomenon as ``knowledge overshadowing'': when we query knowledge from a\nlanguage model with multiple conditions, some conditions overshadow others,\nleading to hallucinated outputs. This phenomenon partially stems from training\ndata imbalance, which we verify on both pretrained models and fine-tuned\nmodels, over a wide range of LM model families and sizes.From a theoretical\npoint of view, knowledge overshadowing can be interpreted as\nover-generalization of the dominant conditions (patterns). We show that the\nhallucination rate grows with both the imbalance ratio (between the popular and\nunpopular condition) and the length of dominant condition description,\nconsistent with our derived generalization bound. Finally, we propose to\nutilize overshadowing conditions as a signal to catch hallucination before it\nis produced, along with a training-free self-contrastive decoding method to\nalleviate hallucination during inference. Our proposed approach showcases up to\n82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,\nwith different models and datasets.",
        "pos": [
            "Extensive previous research has focused on post-training knowledge editing\n(KE) for language models (LMs) to ensure that knowledge remains accurate and\nup-to-date. One desired property and open question in KE is to let edited LMs\ncorrectly handle ripple effects, where LM is expected to answer its logically\nrelated knowledge accurately. In this paper, we answer the question of why most\nKE methods still create messy ripple effects. We conduct extensive analysis and\nidentify a salient indicator, GradSim, that effectively reveals when and why\nupdated knowledge ripples in LMs. GradSim is computed by the cosine similarity\nbetween gradients of the original fact and its related knowledge. We observe a\nstrong positive correlation between ripple effect performance and GradSim\nacross different LMs, KE methods, and evaluation metrics. Further\ninvestigations into three counter-intuitive failure cases (Negation,\nOver-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures\nare often associated with very low GradSim. This finding validates that GradSim\nis an effective indicator of when knowledge ripples in LMs."
        ],
        "neg": []
    },
    {
        "query": "The reconfiguration of human-LM interactions from simple sentence completions\nto complex, multi-domain, humanlike engagements necessitates new methodologies\nto understand how humans choose to rely on LMs. In our work, we contend that\nreliance is influenced by numerous factors within the interactional context of\na generation, a departure from prior work that used verbalized confidence\n(e.g., \"I'm certain the answer is...\") as the key determinant of reliance.\nHere, we introduce Rel-A.I., an in situ, system-level evaluation approach to\nmeasure human reliance on LM-generated epistemic markers (e.g., \"I think\nit's..\", \"Undoubtedly it's...\"). Using this methodology, we measure reliance\nrates in three emergent human-LM interaction settings: long-term interactions,\nanthropomorphic generations, and variable subject matter. Our findings reveal\nthat reliance is not solely based on verbalized confidence but is significantly\naffected by other features of the interaction context. Prior interactions,\nanthropomorphic cues, and subject domain all contribute to reliance\nvariability. An expression such as, \"I'm pretty sure it's...\", can vary up to\n20% in reliance frequency depending on its interactional context. Our work\nunderscores the importance of context in understanding human reliance and\noffers future designers and researchers with a methodology to conduct such\nmeasurements.",
        "pos": [
            "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities."
        ],
        "neg": []
    },
    {
        "query": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.",
        "pos": [
            "Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, the conventional\nfixed-length data composition strategy for pretraining, which involves\nconcatenating and splitting documents, can introduce noise and limit the\nmodel's ability to capture long-range dependencies. To address this, we first\nintroduce three metrics for evaluating data composition quality: padding ratio,\ntruncation ratio, and concatenation ratio. We further propose a multi-bucket\ndata composition method that moves beyond the fixed-length paradigm, offering a\nmore flexible and efficient approach to pretraining. Extensive experiments\ndemonstrate that our proposed method could significantly improving both the\nefficiency and efficacy of LLMs pretraining. Our approach not only reduces\nnoise and preserves context but also accelerates training, making it a\npromising solution for LLMs pretraining.",
        "pos": [
            "Product review generation is an important task in recommender systems, which\ncould provide explanation and persuasiveness for the recommendation. Recently,\nLarge Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling\nand generating ability, which could be applied in review generation. However,\ndirectly applying the LLMs for generating reviews might be troubled by the\n``polite'' phenomenon of the LLMs and could not generate personalized reviews\n(e.g., negative reviews). In this paper, we propose Review-LLM that customizes\nLLMs for personalized review generation. Firstly, we construct the prompt input\nby aggregating user historical behaviors, which include corresponding item\ntitles and reviews. This enables the LLMs to capture user interest features and\nreview writing style. Secondly, we incorporate ratings as indicators of\nsatisfaction into the prompt, which could further improve the model's\nunderstanding of user preferences and the sentiment tendency control of\ngenerated reviews. Finally, we feed the prompt text into LLMs, and use\nSupervised Fine-Tuning (SFT) to make the model generate personalized reviews\nfor the given user and target item. Experimental results on the real-world\ndataset show that our fine-tuned model could achieve better review generation\nperformance than existing close-source LLMs."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, the conventional\nfixed-length data composition strategy for pretraining, which involves\nconcatenating and splitting documents, can introduce noise and limit the\nmodel's ability to capture long-range dependencies. To address this, we first\nintroduce three metrics for evaluating data composition quality: padding ratio,\ntruncation ratio, and concatenation ratio. We further propose a multi-bucket\ndata composition method that moves beyond the fixed-length paradigm, offering a\nmore flexible and efficient approach to pretraining. Extensive experiments\ndemonstrate that our proposed method could significantly improving both the\nefficiency and efficacy of LLMs pretraining. Our approach not only reduces\nnoise and preserves context but also accelerates training, making it a\npromising solution for LLMs pretraining.",
        "pos": [
            "Product review generation is an important task in recommender systems, which\ncould provide explanation and persuasiveness for the recommendation. Recently,\nLarge Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling\nand generating ability, which could be applied in review generation. However,\ndirectly applying the LLMs for generating reviews might be troubled by the\n``polite'' phenomenon of the LLMs and could not generate personalized reviews\n(e.g., negative reviews). In this paper, we propose Review-LLM that customizes\nLLMs for personalized review generation. Firstly, we construct the prompt input\nby aggregating user historical behaviors, which include corresponding item\ntitles and reviews. This enables the LLMs to capture user interest features and\nreview writing style. Secondly, we incorporate ratings as indicators of\nsatisfaction into the prompt, which could further improve the model's\nunderstanding of user preferences and the sentiment tendency control of\ngenerated reviews. Finally, we feed the prompt text into LLMs, and use\nSupervised Fine-Tuning (SFT) to make the model generate personalized reviews\nfor the given user and target item. Experimental results on the real-world\ndataset show that our fine-tuned model could achieve better review generation\nperformance than existing close-source LLMs."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, the conventional\nfixed-length data composition strategy for pretraining, which involves\nconcatenating and splitting documents, can introduce noise and limit the\nmodel's ability to capture long-range dependencies. To address this, we first\nintroduce three metrics for evaluating data composition quality: padding ratio,\ntruncation ratio, and concatenation ratio. We further propose a multi-bucket\ndata composition method that moves beyond the fixed-length paradigm, offering a\nmore flexible and efficient approach to pretraining. Extensive experiments\ndemonstrate that our proposed method could significantly improving both the\nefficiency and efficacy of LLMs pretraining. Our approach not only reduces\nnoise and preserves context but also accelerates training, making it a\npromising solution for LLMs pretraining.",
        "pos": [
            "Product review generation is an important task in recommender systems, which\ncould provide explanation and persuasiveness for the recommendation. Recently,\nLarge Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling\nand generating ability, which could be applied in review generation. However,\ndirectly applying the LLMs for generating reviews might be troubled by the\n``polite'' phenomenon of the LLMs and could not generate personalized reviews\n(e.g., negative reviews). In this paper, we propose Review-LLM that customizes\nLLMs for personalized review generation. Firstly, we construct the prompt input\nby aggregating user historical behaviors, which include corresponding item\ntitles and reviews. This enables the LLMs to capture user interest features and\nreview writing style. Secondly, we incorporate ratings as indicators of\nsatisfaction into the prompt, which could further improve the model's\nunderstanding of user preferences and the sentiment tendency control of\ngenerated reviews. Finally, we feed the prompt text into LLMs, and use\nSupervised Fine-Tuning (SFT) to make the model generate personalized reviews\nfor the given user and target item. Experimental results on the real-world\ndataset show that our fine-tuned model could achieve better review generation\nperformance than existing close-source LLMs.",
            "Manual Red teaming is a commonly-used method to identify vulnerabilities in\nlarge language models (LLMs), which, is costly and unscalable. In contrast,\nautomated red teaming uses a Red LLM to automatically generate adversarial\nprompts to the Target LLM, offering a scalable way for safety vulnerability\ndetection. However, the difficulty of building a powerful automated Red LLM\nlies in the fact that the safety vulnerabilities of the Target LLM are\ndynamically changing with the evolution of the Target LLM. To mitigate this\nissue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in\nwhich the Red LLM and Target LLM are deeply and dynamically interacting with\neach other in an iterative manner. In each iteration, in order to generate\nsuccessful attacks as many as possible, the Red LLM not only takes into account\nthe responses from the Target LLM, but also adversarially adjust its attacking\ndirections by monitoring the global diversity of generated attacks across\nmultiple iterations. Simultaneously, to explore dynamically changing safety\nvulnerabilities of the Target LLM, we allow the Target LLM to enhance its\nsafety via an active learning based data selection mechanism. Experimential\nresults demonstrate that DART significantly reduces the safety risk of the\ntarget LLM. For human evaluation on Anthropic Harmless dataset, compared to the\ninstruction-tuning target LLM, DART eliminates the violation risks by 53.4\\%.\nWe will release the datasets and codes of DART soon."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, the conventional\nfixed-length data composition strategy for pretraining, which involves\nconcatenating and splitting documents, can introduce noise and limit the\nmodel's ability to capture long-range dependencies. To address this, we first\nintroduce three metrics for evaluating data composition quality: padding ratio,\ntruncation ratio, and concatenation ratio. We further propose a multi-bucket\ndata composition method that moves beyond the fixed-length paradigm, offering a\nmore flexible and efficient approach to pretraining. Extensive experiments\ndemonstrate that our proposed method could significantly improving both the\nefficiency and efficacy of LLMs pretraining. Our approach not only reduces\nnoise and preserves context but also accelerates training, making it a\npromising solution for LLMs pretraining.",
        "pos": [
            "Product review generation is an important task in recommender systems, which\ncould provide explanation and persuasiveness for the recommendation. Recently,\nLarge Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling\nand generating ability, which could be applied in review generation. However,\ndirectly applying the LLMs for generating reviews might be troubled by the\n``polite'' phenomenon of the LLMs and could not generate personalized reviews\n(e.g., negative reviews). In this paper, we propose Review-LLM that customizes\nLLMs for personalized review generation. Firstly, we construct the prompt input\nby aggregating user historical behaviors, which include corresponding item\ntitles and reviews. This enables the LLMs to capture user interest features and\nreview writing style. Secondly, we incorporate ratings as indicators of\nsatisfaction into the prompt, which could further improve the model's\nunderstanding of user preferences and the sentiment tendency control of\ngenerated reviews. Finally, we feed the prompt text into LLMs, and use\nSupervised Fine-Tuning (SFT) to make the model generate personalized reviews\nfor the given user and target item. Experimental results on the real-world\ndataset show that our fine-tuned model could achieve better review generation\nperformance than existing close-source LLMs."
        ],
        "neg": []
    },
    {
        "query": "Varying-size models are often required to deploy ASR systems under different\nhardware and/or application constraints such as memory and latency. To avoid\nredundant training and optimization efforts for individual models of different\nsizes, we present the dynamic encoder size approach, which jointly trains\nmultiple performant models within one supernet from scratch. These subnets of\nvarious sizes are layer-wise pruned from the supernet, and thus, enjoy full\nparameter sharing. By combining score-based pruning with supernet training, we\npropose two novel methods, Simple-Top-k and Iterative-Zero-Out, to\nautomatically select the best-performing subnets in a data-driven manner,\navoiding resource-intensive search efforts. Our experiments using CTC on both\nLibrispeech and TED-LIUM-v2 corpora show that our methods can achieve on-par\nperformance as individually trained models of each size category. Also, our\napproach consistently brings small performance improvements for the full-size\nsupernet.",
        "pos": [
            "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
        ],
        "neg": []
    },
    {
        "query": "As language models have scaled both their number of parameters and\npretraining dataset sizes, the computational cost for pretraining has become\nintractable except for the most well-resourced teams. This increasing cost\nmakes it ever more important to be able to reuse a model after it has completed\npretraining; allowing for a model's abilities to further improve without\nneeding to train from scratch. In this work, we detail a set of guidelines that\ncover how to design efficacious data distributions and learning rate schedules\nfor continued pretraining of language models. When applying these findings\nwithin a continued pretraining run on top of a well-trained 15B parameter\nmodel, we show an improvement of 9\\% in average model accuracy compared to the\nbaseline of continued training on the pretraining set. The resulting recipe\nprovides a practical starting point with which to begin developing language\nmodels through reuse rather than retraining.",
        "pos": [
            "The impressive capabilities of recent language models can be largely\nattributed to the multi-trillion token pretraining datasets that they are\ntrained on. However, model developers fail to disclose their construction\nmethodology which has lead to a lack of open information on how to develop\neffective pretraining sets. To address this issue, we perform the first\nsystematic study across the entire pipeline of pretraining set construction.\nFirst, we run ablations on existing techniques for pretraining set development\nto identify which methods translate to the largest gains in model accuracy on\ndownstream evaluations. Then, we categorize the most widely used data source,\nweb crawl snapshots, across the attributes of toxicity, quality, type of\nspeech, and domain. Finally, we show how such attribute information can be used\nto further refine and improve the quality of a pretraining set. These findings\nconstitute an actionable set of steps that practitioners can use to develop\nhigh quality pretraining sets."
        ],
        "neg": []
    },
    {
        "query": "In-context learning (ICL) is a recent advancement in the capabilities of\nlarge language models (LLMs). This feature allows users to perform a new task\nwithout updating the model. Concretely, users can address tasks during the\ninference time by conditioning on a few input-label pair demonstrations along\nwith the test input. It is different than the conventional fine-tuning paradigm\nand offers more flexibility. However, this capability also introduces potential\nissues. For example, users may use the model on any data without restriction,\nsuch as performing tasks with improper or sensitive content, which might\nviolate the model policy or conflict with the model owner's interests. As a\nmodel owner, it is crucial to establish a mechanism to control the model's\nbehavior under ICL, depending on the model owner's requirements for various\ncontent. To this end, we introduce the concept of \"applicability authorization\"\ntailored for LLMs, particularly for ICL behavior, and propose a simple\napproach, ICLGuard. It is a fine-tuning framework designed to allow the model\nowner to regulate ICL behavior on different data. ICLGuard preserves the\noriginal LLM and fine-tunes only a minimal set of additional trainable\nparameters to \"guard\" the LLM. Empirical results show that the guarded LLM can\ndeactivate its ICL ability on target data without affecting its ICL ability on\nother data and its general functionality across all data.",
        "pos": [
            "Open-source large language models (LLMs) have become increasingly popular\namong both the general public and industry, as they can be customized,\nfine-tuned, and freely used. However, some open-source LLMs require approval\nbefore usage, which has led to third parties publishing their own easily\naccessible versions. Similarly, third parties have been publishing fine-tuned\nor quantized variants of these LLMs. These versions are particularly appealing\nto users because of their ease of access and reduced computational resource\ndemands. This trend has increased the risk of training time attacks,\ncompromising the integrity and security of LLMs. In this work, we present a new\ntraining time attack, SOS, which is designed to be low in computational demand\nand does not require clean data or modification of the model weights, thereby\nmaintaining the model's utility intact. The attack addresses security issues in\nvarious scenarios, including the backdoor attack, jailbreak attack, and prompt\nstealing attack. Our experimental findings demonstrate that the proposed attack\nis effective across all evaluated targets. Furthermore, we present the other\nside of our SOS technique, namely the copyright token -- a novel technique that\nenables users to mark their copyrighted content and prevent models from using\nit."
        ],
        "neg": []
    },
    {
        "query": "Is it always necessary to compute tokens from shallow to deep layers in\nTransformers? The continued success of vanilla Transformers and their variants\nsuggests an undoubted \"yes\". In this work, however, we attempt to break the\ndepth-ordered convention by proposing a novel architecture dubbed\nmixture-of-modules (MoM), which is motivated by an intuition that any layer,\nregardless of its position, can be used to compute a token as long as it\npossesses the needed processing capabilities. The construction of MoM starts\nfrom a finite set of modules defined by multi-head attention and feed-forward\nnetworks, each distinguished by its unique parameterization. Two routers then\niteratively select attention modules and feed-forward modules from the set to\nprocess a token. The selection dynamically expands the computation graph in the\nforward pass of the token, culminating in an assembly of modules. We show that\nMoM provides not only a unified framework for Transformers and their numerous\nvariants but also a flexible and learnable approach for reducing redundancy in\nTransformer parameterization. We pre-train various MoMs using OpenWebText.\nEmpirical results demonstrate that MoMs, of different parameter counts,\nconsistently outperform vanilla transformers on both GLUE and XSUM benchmarks.\nMore interestingly, with a fixed parameter budget, MoM-large enables an over\n38% increase in depth for computation graphs compared to GPT-2-large, resulting\nin absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large\nalso enables an over 60% reduction in depth while involving more modules per\nlayer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage\ncompared to GPT-2-large, while maintaining comparable performance.",
        "pos": [
            "LLMs are known to be vulnerable to jailbreak attacks, even after safety\nalignment. An important observation is that, while different types of jailbreak\nattacks can generate significantly different queries, they mostly result in\nsimilar responses that are rooted in the same harmful knowledge (e.g., detailed\nsteps to make a bomb). Therefore, we conjecture that directly unlearn the\nharmful knowledge in the LLM can be a more effective way to defend against\njailbreak attacks than the mainstream supervised fine-tuning (SFT) based\napproaches. Our extensive experiments confirmed our insight and suggested\nsurprising generalizability of our unlearning-based approach: using only 20 raw\nharmful questions \\emph{without} any jailbreak prompt during training, our\nsolution reduced the Attack Success Rate (ASR) in Vicuna-7B on\n\\emph{out-of-distribution} (OOD) harmful questions wrapped with various complex\njailbreak prompts from 82.6\\% to 7.7\\%. This significantly outperforms\nLlama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment samples but\nstill has an ASR of 21.9\\% even under the help of an additional safety system\nprompt. Further analysis reveals that the generalization ability of our\nsolution stems from the intrinsic relatedness among harmful responses across\nharmful questions (e.g., response patterns, shared steps and actions, and\nsimilarity among their learned representations in the LLM). Our code is\navailable at \\url{https://github.com/thu-coai/SafeUnlearning}."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) are increasingly being used to synthesize and\nreason about source code. However, the static nature of these models' knowledge\ndoes not reflect the fact that libraries and API functions they invoke are\ncontinuously evolving, with functionality being added or changing. While\nnumerous benchmarks evaluate how LLMs can generate code, no prior work has\nstudied how an LLMs' knowledge about code API functions can be updated. To fill\nthis gap, we present CodeUpdateArena, a benchmark for knowledge editing in the\ncode domain. An instance in our benchmark consists of a synthetic API function\nupdate paired with a program synthesis example that uses the updated\nfunctionality; our goal is to update an LLM to be able to solve this program\nsynthesis example without providing documentation of the update at inference\ntime. Compared to knowledge editing for facts encoded in text, success here is\nmore challenging: a code LLM must correctly reason about the semantics of the\nmodified function rather than just reproduce its syntax. Our dataset is\nconstructed by first prompting GPT-4 to generate atomic and executable function\nupdates. Then, for each update, we generate program synthesis examples whose\ncode solutions are prone to use the update. Our benchmark covers updates of\nvarious types to 54 functions from seven diverse Python packages, with a total\nof 670 program synthesis examples. Our experiments show that prepending\ndocumentation of the update to open-source code LLMs (i.e., DeepSeek,\nCodeLlama) does not allow them to incorporate changes for problem solving, and\nexisting knowledge editing techniques also have substantial room for\nimprovement. We hope our benchmark will inspire new methods for knowledge\nupdating in code LLMs.",
        "pos": [
            "Recent work has explored the capability of large language models (LLMs) to\nidentify and correct errors in LLM-generated responses. These refinement\napproaches frequently evaluate what sizes of models are able to do refinement\nfor what problems, but less attention is paid to what effective feedback for\nrefinement looks like. In this work, we propose looking at refinement with\nfeedback as a composition of three distinct LLM competencies: (1)\nidentification of bad generations; (2) fine-grained natural language feedback\ngeneration; (3) refining with fine-grained feedback. The first step can be\nimplemented with a high-performing discriminative model and steps 2 and 3 can\nbe implemented either via prompted or fine-tuned LLMs. A key property of this\napproach is that the step 2 critique model can give fine-grained feedback about\nerrors, made possible by offloading the discrimination to a separate model in\nstep 1. We show that models of different capabilities benefit from refining\nwith this approach on the task of improving factual consistency of document\ngrounded summaries. Overall, our proposed method consistently outperforms\nexisting end-to-end refinement approaches and current trained models not\nfine-tuned for factuality critiquing."
        ],
        "neg": []
    },
    {
        "query": "The remarkable success of Large Language Models (LLMs) has ushered natural\nlanguage processing (NLP) research into a new era. Despite their diverse\ncapabilities, LLMs trained on different corpora exhibit varying strengths and\nweaknesses, leading to challenges in maximizing their overall efficiency and\nversatility. To address these challenges, recent studies have explored\ncollaborative strategies for LLMs. This paper provides a comprehensive overview\nof this emerging research area, highlighting the motivation behind such\ncollaborations. Specifically, we categorize collaborative strategies into three\nprimary approaches: Merging, Ensemble, and Cooperation. Merging involves\nintegrating multiple LLMs in the parameter space. Ensemble combines the outputs\nof various LLMs. Cooperation} leverages different LLMs to allow full play to\ntheir diverse capabilities for specific tasks. We provide in-depth\nintroductions to these methods from different perspectives and discuss their\npotential applications. Additionally, we outline future research directions,\nhoping this work will catalyze further studies on LLM collaborations and paving\nthe way for advanced NLP applications.",
        "pos": [
            "The recent advancements in large language models (LLMs) with billions of\nparameters have significantly boosted their performance across various\nreal-world applications. However, the inference processes for these models\nrequire substantial energy and computational resources, presenting considerable\ndeployment challenges. In contrast, human brains, which contain approximately\n86 billion biological neurons, exhibit significantly greater energy efficiency\ncompared to LLMs with a similar number of parameters. Inspired by this, we\nredesign 7 to 70 billion parameter LLMs using bio-plausible spiking mechanisms,\nemulating the efficient behavior of the human brain. We propose the first\nspiking large language model as recent LLMs termed SpikeLLM. Coupled with the\nproposed model, a novel spike-driven quantization framework named Optimal Brain\nSpiking is introduced to reduce the energy cost and accelerate inference speed\nvia two essential approaches: first (second)-order differentiation-based\nsalient channel detection, and per-channel salient outlier expansion with\nGeneralized Integrate-and-Fire neurons. Our proposed spike-driven quantization\ncan plug in main streams of quantization training methods. In the OmniQuant\npipeline, SpikeLLM significantly reduces 25.51% WikiText2 perplexity and\nimproves 3.08% average accuracy of 6 zero-shot datasets on a LLAMA2-7B 4A4W\nmodel. In the GPTQ pipeline, SpikeLLM realizes a sparse ternary quantization,\nwhich achieves additive in all linear layers. Compared with PB-LLM with similar\noperations, SpikeLLM also exceeds significantly. We will release our code on\nGitHub."
        ],
        "neg": []
    },
    {
        "query": "The consequences of a healthcare data breach can be devastating for the\npatients, providers, and payers. The average financial impact of a data breach\nin recent months has been estimated to be close to USD 10 million. This is\nespecially significant for healthcare organizations in India that are managing\nrapid digitization while still establishing data governance procedures that\nalign with the letter and spirit of the law. Computer-based systems for\nde-identification of personal information are vulnerable to data drift, often\nrendering them ineffective in cross-institution settings. Therefore, a rigorous\nassessment of existing de-identification against local health datasets is\nimperative to support the safe adoption of digital health initiatives in India.\nUsing a small set of de-identified patient discharge summaries provided by an\nIndian healthcare institution, in this paper, we report the nominal performance\nof de-identification algorithms (based on language models) trained on publicly\navailable non-Indian datasets, pointing towards a lack of cross-institutional\ngeneralization. Similarly, experimentation with off-the-shelf de-identification\nsystems reveals potential risks associated with the approach. To overcome data\nscarcity, we explore generating synthetic clinical reports (using publicly\navailable and Indian summaries) by performing in-context learning over Large\nLanguage Models (LLMs). Our experiments demonstrate the use of generated\nreports as an effective strategy for creating high-performing de-identification\nsystems with good generalization capabilities.",
        "pos": [
            "Legal systems worldwide are inundated with exponential growth in cases and\ndocuments. There is an imminent need to develop NLP and ML techniques for\nautomatically processing and understanding legal documents to streamline the\nlegal system. However, evaluating and comparing various NLP models designed\nspecifically for the legal domain is challenging. This paper addresses this\nchallenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding\nand Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual\n(9 Indian languages) domain-specific tasks that address different aspects of\nthe legal system from the point of view of understanding and reasoning over\nIndian legal documents. We present baseline models (including LLM-based) for\neach task, outlining the gap between models and the ground truth. To foster\nfurther research in the legal domain, we create a leaderboard (available at:\nhttps://exploration-lab.github.io/IL-TUR/) where the research community can\nupload and compare legal text understanding systems."
        ],
        "neg": []
    },
    {
        "query": "The extreme multi-label classification~(XMC) task involves learning a\nclassifier that can predict from a large label set the most relevant subset of\nlabels for a data instance. While deep neural networks~(DNNs) have demonstrated\nremarkable success in XMC problems, the task is still challenging because it\nmust deal with a large number of output labels, which make the DNN training\ncomputationally expensive. This paper addresses the issue by exploring the use\nof random circular vectors, where each vector component is represented as a\ncomplex amplitude. In our framework, we can develop an output layer and loss\nfunction of DNNs for XMC by representing the final output layer as a fully\nconnected layer that directly predicts a low-dimensional circular vector\nencoding a set of labels for a data instance. We conducted experiments on\nsynthetic datasets to verify that circular vectors have better label encoding\ncapacity and retrieval ability than normal real-valued vectors. Then, we\nconducted experiments on actual XMC datasets and found that these appealing\nproperties of circular vectors contribute to significant improvements in task\nperformance compared with a previous model using random real-valued vectors,\nwhile reducing the size of the output layers by up to 99%.",
        "pos": [
            "Knowledge Graphs (KGs) are fundamental resources in knowledge-intensive tasks\nin NLP. Due to the limitation of manually creating KGs, KG Completion (KGC) has\nan important role in automatically completing KGs by scoring their links with\nKG Embedding (KGE). To handle many entities in training, KGE relies on Negative\nSampling (NS) loss that can reduce the computational cost by sampling. Since\nthe appearance frequencies for each link are at most one in KGs, sparsity is an\nessential and inevitable problem. The NS loss is no exception. As a solution,\nthe NS loss in KGE relies on smoothing methods like Self-Adversarial Negative\nSampling (SANS) and subsampling. However, it is uncertain what kind of\nsmoothing method is suitable for this purpose due to the lack of theoretical\nunderstanding. This paper provides theoretical interpretations of the smoothing\nmethods for the NS loss in KGE and induces a new NS loss, Triplet Adaptive\nNegative Sampling (TANS), that can cover the characteristics of the\nconventional smoothing methods. Experimental results of TransE, DistMult,\nComplEx, RotatE, HAKE, and HousE on FB15k-237, WN18RR, and YAGO3-10 datasets\nand their sparser subsets show the soundness of our interpretation and\nperformance improvement by our TANS."
        ],
        "neg": []
    },
    {
        "query": "Legal systems worldwide are inundated with exponential growth in cases and\ndocuments. There is an imminent need to develop NLP and ML techniques for\nautomatically processing and understanding legal documents to streamline the\nlegal system. However, evaluating and comparing various NLP models designed\nspecifically for the legal domain is challenging. This paper addresses this\nchallenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding\nand Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual\n(9 Indian languages) domain-specific tasks that address different aspects of\nthe legal system from the point of view of understanding and reasoning over\nIndian legal documents. We present baseline models (including LLM-based) for\neach task, outlining the gap between models and the ground truth. To foster\nfurther research in the legal domain, we create a leaderboard (available at:\nhttps://exploration-lab.github.io/IL-TUR/) where the research community can\nupload and compare legal text understanding systems.",
        "pos": [
            "Automatic summarization of legal case judgements, which are known to be long\nand complex, has traditionally been tried via extractive summarization models.\nIn recent years, generative models including abstractive summarization models\nand Large language models (LLMs) have gained huge popularity. In this paper, we\nexplore the applicability of such models for legal case judgement\nsummarization. We applied various domain specific abstractive summarization\nmodels and general domain LLMs as well as extractive summarization models over\ntwo sets of legal case judgements from the United Kingdom (UK) Supreme Court\nand the Indian (IN) Supreme Court and evaluated the quality of the generated\nsummaries. We also perform experiments on a third dataset of legal documents of\na different type, Government reports from the United States (US). Results show\nthat abstractive summarization models and LLMs generally perform better than\nthe extractive methods as per traditional metrics for evaluating summary\nquality. However, detailed investigation shows the presence of inconsistencies\nand hallucinations in the outputs of the generative models, and we explore ways\nto reduce the hallucinations and inconsistencies in the summaries. Overall, the\ninvestigation suggests that further improvements are needed to enhance the\nreliability of abstractive models and LLMs for legal case judgement\nsummarization. At present, a human-in-the-loop technique is more suitable for\nperforming manual checks to identify inconsistencies in the generated\nsummaries."
        ],
        "neg": []
    },
    {
        "query": "Multimodal large language models (MLLMs) are flourishing, but mainly focus on\nimages with less attention than videos, especially in sub-fields such as prompt\nengineering, video chain-of-thought (CoT), and instruction tuning on videos.\nTherefore, we try to explore the collection of CoT datasets in videos to lead\nto video OpenQA and improve the reasoning ability of MLLMs. Unfortunately,\nmaking such video CoT datasets is not an easy task. Given that human annotation\nis too cumbersome and expensive, while machine-generated is not reliable due to\nthe hallucination issue, we develop an automatic annotation tool that combines\nmachine and human experts, under the active learning paradigm. Active learning\nis an interactive strategy between the model and human experts, in this way,\nthe workload of human labeling can be reduced and the quality of the dataset\ncan be guaranteed. With the help of the automatic annotation tool, we strive to\ncontribute three datasets, namely VideoCoT, TopicQA, TopicCoT. Furthermore, we\npropose a simple but effective benchmark based on the collected datasets, which\nexploits CoT to maximize the complex reasoning capabilities of MLLMs. Extensive\nexperiments demonstrate the effectiveness our solution.",
        "pos": [
            "Emotion and Intent Joint Understanding in Multimodal Conversation (MC-EIU)\naims to decode the semantic information manifested in a multimodal\nconversational history, while inferring the emotions and intents simultaneously\nfor the current utterance. MC-EIU is enabling technology for many\nhuman-computer interfaces. However, there is a lack of available datasets in\nterms of annotation, modality, language diversity, and accessibility. In this\nwork, we propose an MC-EIU dataset, which features 7 emotion categories, 9\nintent categories, 3 modalities, i.e., textual, acoustic, and visual content,\nand two languages, i.e., English and Mandarin. Furthermore, it is completely\nopen-source for free access. To our knowledge, MC-EIU is the first\ncomprehensive and rich emotion and intent joint understanding dataset for\nmultimodal conversation. Together with the release of the dataset, we also\ndevelop an Emotion and Intent Interaction (EI$^2$) network as a reference\nsystem by modeling the deep correlation between emotion and intent in the\nmultimodal conversation. With comparative experiments and ablation studies, we\ndemonstrate the effectiveness of the proposed EI$^2$ method on the MC-EIU\ndataset. The dataset and codes will be made available at:\nhttps://github.com/MC-EIU/MC-EIU."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) are increasingly applied to clinical\ndecision-making. However, their potential to exhibit bias poses significant\nrisks to clinical equity. Currently, there is a lack of benchmarks that\nsystematically evaluate such clinical bias in LLMs. While in downstream tasks,\nsome biases of LLMs can be avoided such as by instructing the model to answer\n\"I'm not sure...\", the internal bias hidden within the model still lacks deep\nstudies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in\nLarge Language Models), a pioneering comprehensive benchmark to evaluate both\nintrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for\nclinical decision tasks. Notably, for intrinsic bias, we introduce a novel\nmetric, AssocMAD, to assess the disparities of LLMs across multiple demographic\ngroups. Additionally, we leverage counterfactual intervention to evaluate\nextrinsic bias in a task of clinical diagnosis prediction. Our experiments\nacross popular and medically adapted LLMs, particularly from the Mistral and\nLLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic\nbias. This work underscores the critical need to mitigate clinical bias and\nsets a new standard for future evaluations of LLMs' clinical bias.",
        "pos": [
            "Vision-language models (VLMs) pre-trained on extensive datasets can\ninadvertently learn biases by correlating gender information with specific\nobjects or scenarios. Current methods, which focus on modifying inputs and\nmonitoring changes in the model's output probability scores, often struggle to\ncomprehensively understand bias from the perspective of model components. We\npropose a framework that incorporates causal mediation analysis to measure and\nmap the pathways of bias generation and propagation within VLMs. This approach\nallows us to identify the direct effects of interventions on model bias and the\nindirect effects of interventions on bias mediated through different model\ncomponents. Our results show that image features are the primary contributors\nto bias, with significantly higher impacts than text features, specifically\naccounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE\ndatasets, respectively. Notably, the image encoder's contribution surpasses\nthat of the text encoder and the deep fusion encoder. Further experimentation\nconfirms that contributions from both language and vision modalities are\naligned and non-conflicting. Consequently, focusing on blurring gender\nrepresentations within the image encoder, which contributes most to the model\nbias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and\nPASCAL-SENTENCE datasets, respectively, with minimal performance loss or\nincreased computational demands."
        ],
        "neg": []
    },
    {
        "query": "We use referential translation machines (RTMs) to identify the similarity\nbetween an attribute and two words in English by casting the task as machine\ntranslation performance prediction (MTPP) between the words and the attribute\nword and the distance between their similarities for Task 10 with stacked RTM\nmodels. RTMs are also used to predict the intensity of the structure and\ncontent in tweets in English, Arabic, and Spanish in Task 1 where MTPP is\nbetween the tweets and the set of words for the emotion selected from WordNet\naffect emotion lists. Stacked RTM models obtain encouraging results in both.",
        "pos": [
            "We present a new parser performance prediction (PPP) model using machine\ntranslation performance prediction system (MTPPS), statistically independent of\nany language or parser, relying only on extrinsic and novel features based on\ntextual, link structural, and bracketing tree structural information. This new\nsystem, MTPPS-PPP, can predict the performance of any parser in any language\nand can be useful for estimating the grammatical difficulty when understanding\na given text, for setting expectations from parsing output, for parser\nselection for a specific domain, and for parser combination systems. We obtain\nSoA results in PPP of bracketing $F_1$ with better results over textual\nfeatures and similar performance with previous results that use parser and\nlinguistic label specific information. Our results show the contribution of\ndifferent types of features as well as rankings of individual features in\ndifferent experimental settings (cased vs. uncased), in different learning\ntasks (in-domain vs. out-of-domain), with different training sets, with\ndifferent learning algorithms, and with different dimensionality reduction\ntechniques. We achieve $0.0678$ MAE and $0.85$ RAE in setting +Link, which\ncorresponds to about $7.4\\%$ error when predicting the bracketing $F_1$ score\nfor the Charniak and Johnson parser on the WSJ23 test set. MTPPS-PPP system can\npredict without parsing using only the text, without a supervised parser using\nonly an unsupervised parser, without any parser or language dependent\ninformation, without using a reference parser output, and can be used to\npredict the performance of any parser in any language."
        ],
        "neg": []
    },
    {
        "query": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has\nenhanced medical diagnosis. However, current Med-LVLMs frequently encounter\nfactual issues, often generating responses that do not align with established\nmedical facts. Retrieval-Augmented Generation (RAG), which utilizes external\nknowledge, can improve the factual accuracy of these models but introduces two\nmajor challenges. First, limited retrieved contexts might not cover all\nnecessary information, while excessive retrieval can introduce irrelevant and\ninaccurate references, interfering with the model's generation. Second, in\ncases where the model originally responds correctly, applying RAG can lead to\nan over-reliance on retrieved contexts, resulting in incorrect answers. To\naddress these issues, we propose RULE, which consists of two components. First,\nwe introduce a provably effective strategy for controlling factuality risk\nthrough the calibrated selection of the number of retrieved contexts. Second,\nbased on samples where over-reliance on retrieved contexts led to errors, we\ncurate a preference dataset to fine-tune the model, balancing its dependence on\ninherent knowledge and retrieved contexts for generation. We demonstrate the\neffectiveness of RULE on three medical VQA datasets, achieving an average\nimprovement of 20.8% in factual accuracy. We publicly release our benchmark and\ncode in https://github.com/richard-peng-xia/RULE.",
        "pos": [
            "Large Language Model (LLMs) such as ChatGPT that exhibit generative AI\ncapabilities are facing accelerated adoption and innovation. The increased\npresence of Generative AI (GAI) inevitably raises concerns about the risks and\nsafety associated with these models. This article provides an up-to-date survey\nof recent trends in AI safety research of GAI-LLMs from a computer scientist's\nperspective: specific and technical. In this survey, we explore the background\nand motivation for the identified harms and risks in the context of LLMs being\ngenerative language models; our survey differentiates by emphasising the need\nfor unified theories of the distinct safety challenges in the research\ndevelopment and applications of LLMs. We start our discussion with a concise\nintroduction to the workings of LLMs, supported by relevant literature. Then we\ndiscuss earlier research that has pointed out the fundamental constraints of\ngenerative models, or lack of understanding thereof (e.g., performance and\nsafety trade-offs as LLMs scale in number of parameters). We provide a\nsufficient coverage of LLM alignment -- delving into various approaches,\ncontending methods and present challenges associated with aligning LLMs with\nhuman preferences. By highlighting the gaps in the literature and possible\nimplementation oversights, our aim is to create a comprehensive analysis that\nprovides insights for addressing AI safety in LLMs and encourages the\ndevelopment of aligned and secure models. We conclude our survey by discussing\nfuture directions of LLMs for AI safety, offering insights into ongoing\nresearch in this critical area."
        ],
        "neg": []
    },
    {
        "query": "Cross-lingual word alignment plays a crucial role in various natural language\nprocessing tasks, particularly for low-resource languages. Recent study\nproposes a BiLSTM-based encoder-decoder model that outperforms pre-trained\nlanguage models in low-resource settings. However, their model only considers\nthe similarity of word embedding spaces and does not explicitly model the\ndifferences between word embeddings. To address this limitation, we propose\nincorporating contrastive learning into the BiLSTM-based encoder-decoder\nframework. Our approach introduces a multi-view negative sampling strategy to\nlearn the differences between word pairs in the shared cross-lingual embedding\nspace. We evaluate our model on five bilingual aligned datasets spanning four\nASEAN languages: Lao, Vietnamese, Thai, and Indonesian. Experimental results\ndemonstrate that integrating contrastive learning consistently improves word\nalignment accuracy across all datasets, confirming the effectiveness of the\nproposed method in low-resource scenarios. We will release our data set and\ncode to support future research on ASEAN or more low-resource word alignment.",
        "pos": [
            "Protecting Personal Identifiable Information (PII) in text data is crucial\nfor privacy, but current PII generalization methods face challenges such as\nuneven data distributions and limited context awareness. To address these\nissues, we propose two approaches: a feature-based method using machine\nlearning to improve performance on structured inputs, and a novel context-aware\nframework that considers the broader context and semantic relationships between\nthe original text and generalized candidates. The context-aware approach\nemploys Multilingual-BERT for text representation, functional transformations,\nand mean squared error scoring to evaluate candidates. Experiments on the\nWikiReplace dataset demonstrate the effectiveness of both methods, with the\ncontext-aware approach outperforming the feature-based one across different\nscales. This work contributes to advancing PII generalization techniques by\nhighlighting the importance of feature selection, ensemble learning, and\nincorporating contextual information for better privacy protection in text\nanonymization."
        ],
        "neg": []
    },
    {
        "query": "Cross-lingual word alignment plays a crucial role in various natural language\nprocessing tasks, particularly for low-resource languages. Recent study\nproposes a BiLSTM-based encoder-decoder model that outperforms pre-trained\nlanguage models in low-resource settings. However, their model only considers\nthe similarity of word embedding spaces and does not explicitly model the\ndifferences between word embeddings. To address this limitation, we propose\nincorporating contrastive learning into the BiLSTM-based encoder-decoder\nframework. Our approach introduces a multi-view negative sampling strategy to\nlearn the differences between word pairs in the shared cross-lingual embedding\nspace. We evaluate our model on five bilingual aligned datasets spanning four\nASEAN languages: Lao, Vietnamese, Thai, and Indonesian. Experimental results\ndemonstrate that integrating contrastive learning consistently improves word\nalignment accuracy across all datasets, confirming the effectiveness of the\nproposed method in low-resource scenarios. We will release our data set and\ncode to support future research on ASEAN or more low-resource word alignment.",
        "pos": [
            "Protecting Personal Identifiable Information (PII) in text data is crucial\nfor privacy, but current PII generalization methods face challenges such as\nuneven data distributions and limited context awareness. To address these\nissues, we propose two approaches: a feature-based method using machine\nlearning to improve performance on structured inputs, and a novel context-aware\nframework that considers the broader context and semantic relationships between\nthe original text and generalized candidates. The context-aware approach\nemploys Multilingual-BERT for text representation, functional transformations,\nand mean squared error scoring to evaluate candidates. Experiments on the\nWikiReplace dataset demonstrate the effectiveness of both methods, with the\ncontext-aware approach outperforming the feature-based one across different\nscales. This work contributes to advancing PII generalization techniques by\nhighlighting the importance of feature selection, ensemble learning, and\nincorporating contextual information for better privacy protection in text\nanonymization."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) are increasingly deployed in real-world\nscenarios with the help of recent model compression techniques. Such momentum\ntowards local deployment means the use of compressed LLMs will widely impact a\nlarge population. However, prior analysis works often prioritize on preserving\nperplexity which is a direct analogy to training loss. The impact of\ncompression method on other critical aspects of model behavior, particularly\nsafety, still calls for a systematic assessment. To this end, we investigate\nthe impact of model compression on four dimensions: (1) degeneration harm,\ni.e., bias and toxicity in generation; (2) representational harm, i.e., biases\nin discriminative tasks; (3) dialect bias; (4) language modeling and downstream\ntask performance. We cover a wide spectrum of LLM compression techniques,\nincluding unstructured pruning, semi-structured pruning and quantization. Our\nanalysis reveals that compression can lead to unexpected consequences. Although\ncompression may unintentionally remedy LLMs' degeneration harm, it can still\nexacerbate on the representational harm axis. Although compression may\nunintentionally remedy LLMs' degeneration harm, it can still exacerbate on the\nrepresentational harm axis. Moreover, there is a divergent impact on different\nprotected groups as the compression rate grows. Finally, different compression\nmethods have drastically different safety impacts, e.g., quantization mostly\npreserves bias while pruning degrades quickly. Our findings underscore the\nimportance of integrating safety assessments into the development of compressed\nLLMs to ensure their reliability across real-world applications. Our full\nresults are available here:\n\\url{https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval}",
        "pos": [
            "Large language models (LLMs) have played a fundamental role in various\nnatural language processing tasks with powerful prompt techniques. However, in\nreal-world applications, there are often similar prompt components for repeated\nqueries, which causes significant computational burdens during inference.\nExisting prompt compression and direct fine-tuning methods aim to tackle these\nchallenges, yet they frequently struggle to strike an optimal balance between\ncost-efficiency and performance effectiveness, especially in complex tasks such\nas NL2Code. In this paper, we propose a novel method namely PromptIntern to\ninternalize the prompt knowledge into model parameters via progressive\nfine-tuning. Our method enables LLMs to emulate the human learning process for\na new task, where detailed templates and examples in a prompt are gradually\ninternalized and phased out progressively as the model grows accustomed to the\ntask. Extensive experiments demonstrate that our method reduces inference\ntokens over 90%, speedups inference by 4.2 times, and saves 88.3% monetary\ncost."
        ],
        "neg": []
    },
    {
        "query": "End-to-end (E2E) keyword search (KWS) has emerged as an alternative and\ncomplimentary approach to conventional keyword search which depends on the\noutput of automatic speech recognition (ASR) systems. While E2E methods greatly\nsimplify the KWS pipeline, they generally have worse performance than their\nASR-based counterparts, which can benefit from pretraining with untranscribed\ndata. In this work, we propose a method for pretraining E2E KWS systems with\nuntranscribed data, which involves using acoustic unit discovery (AUD) to\nobtain discrete units for untranscribed data and then learning to locate\nsequences of such units in the speech. We conduct experiments across languages\nand AUD systems: we show that finetuning such a model significantly outperforms\na model trained from scratch, and the performance improvements are generally\ncorrelated with the quality of the AUD system used for pretraining.",
        "pos": [
            "This paper explores speculative speech recognition (SSR), where we empower\nconventional automatic speech recognition (ASR) with speculation capabilities,\nallowing the recognizer to run ahead of audio. We introduce a metric for\nmeasuring SSR performance and we propose a model which does SSR by combining a\nRNN-Transducer-based ASR system with an audio-prefixed language model (LM). The\nASR system transcribes ongoing audio and feeds the resulting transcripts, along\nwith an audio-dependent prefix, to the LM, which speculates likely completions\nfor the transcriptions. We experiment with a variety of ASR datasets on which\nshow the efficacy our method and the feasibility of SSR as a method of reducing\nASR latency.",
            "End-to-end (E2E) approaches to keyword search (KWS) are considerably simpler\nin terms of training and indexing complexity when compared to approaches which\nuse the output of automatic speech recognition (ASR) systems. This\nsimplification however has drawbacks due to the loss of modularity. In\nparticular, where ASR-based KWS systems can benefit from external unpaired text\nvia a language model, current formulations of E2E KWS systems have no such\nmechanism. Therefore, in this paper, we propose a multitask training objective\nwhich allows unpaired text to be integrated into E2E KWS without complicating\nindexing and search. In addition to training an E2E KWS model to retrieve text\nqueries from spoken documents, we jointly train it to retrieve text queries\nfrom masked written documents. We show empirically that this approach can\neffectively leverage unpaired text for KWS, with significant improvements in\nsearch performance across a wide variety of languages. We conduct analysis\nwhich indicates that these improvements are achieved because the proposed\nmethod improves document representations for words in the unpaired text.\nFinally, we show that the proposed method can be used for domain adaptation in\nsettings where in-domain paired data is scarce or nonexistent."
        ],
        "neg": []
    },
    {
        "query": "End-to-end (E2E) keyword search (KWS) has emerged as an alternative and\ncomplimentary approach to conventional keyword search which depends on the\noutput of automatic speech recognition (ASR) systems. While E2E methods greatly\nsimplify the KWS pipeline, they generally have worse performance than their\nASR-based counterparts, which can benefit from pretraining with untranscribed\ndata. In this work, we propose a method for pretraining E2E KWS systems with\nuntranscribed data, which involves using acoustic unit discovery (AUD) to\nobtain discrete units for untranscribed data and then learning to locate\nsequences of such units in the speech. We conduct experiments across languages\nand AUD systems: we show that finetuning such a model significantly outperforms\na model trained from scratch, and the performance improvements are generally\ncorrelated with the quality of the AUD system used for pretraining.",
        "pos": [
            "End-to-end (E2E) approaches to keyword search (KWS) are considerably simpler\nin terms of training and indexing complexity when compared to approaches which\nuse the output of automatic speech recognition (ASR) systems. This\nsimplification however has drawbacks due to the loss of modularity. In\nparticular, where ASR-based KWS systems can benefit from external unpaired text\nvia a language model, current formulations of E2E KWS systems have no such\nmechanism. Therefore, in this paper, we propose a multitask training objective\nwhich allows unpaired text to be integrated into E2E KWS without complicating\nindexing and search. In addition to training an E2E KWS model to retrieve text\nqueries from spoken documents, we jointly train it to retrieve text queries\nfrom masked written documents. We show empirically that this approach can\neffectively leverage unpaired text for KWS, with significant improvements in\nsearch performance across a wide variety of languages. We conduct analysis\nwhich indicates that these improvements are achieved because the proposed\nmethod improves document representations for words in the unpaired text.\nFinally, we show that the proposed method can be used for domain adaptation in\nsettings where in-domain paired data is scarce or nonexistent."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have gained widespread adoption in various\nnatural language processing tasks, including question answering and dialogue\nsystems. However, a major drawback of LLMs is the issue of hallucination, where\nthey generate unfaithful or inconsistent content that deviates from the input\nsource, leading to severe consequences. In this paper, we propose a robust\ndiscriminator named RelD to effectively detect hallucination in LLMs' generated\nanswers. RelD is trained on the constructed RelQA, a bilingual\nquestion-answering dialogue dataset along with answers generated by LLMs and a\ncomprehensive set of metrics. Our experimental results demonstrate that the\nproposed RelD successfully detects hallucination in the answers generated by\ndiverse LLMs. Moreover, it performs well in distinguishing hallucination in\nLLMs' generated answers from both in-distribution and out-of-distribution\ndatasets. Additionally, we also conduct a thorough analysis of the types of\nhallucinations that occur and present valuable insights. This research\nsignificantly contributes to the detection of reliable answers generated by\nLLMs and holds noteworthy implications for mitigating hallucination in the\nfuture work.",
        "pos": [
            "Recent years, Pre-trained Language models (PLMs) have swept into various\nfields of artificial intelligence and achieved great success. However, most\nPLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them\nis often expensive and time consuming, and storing them takes up a lot of\nspace. Therefore, it is necessary to adopt a parameter-efficient approach to\nreduce parameters of PLMs in fine-tuning without compromising their performance\nin downstream tasks. In this paper, we design a novel adapter which only acts\non self-attention outputs in PLMs. This adapter adopts element-wise linear\ntransformation using Hadamard product, hence named as Hadamard adapter,\nrequires the fewest parameters compared to previous parameter-efficient\nadapters. In addition, we also summarize some tuning patterns for Hadamard\nadapter shared by various downstream tasks, expecting to provide some guidance\nfor further parameter reduction with shared adapters in future studies. The\nexperiments conducted on the widely-used GLUE benchmark with several SOTA PLMs\nprove that the Hadamard adapter achieves competitive performance with only\n0.033\\% parameters compared with full fine-tuning, and it has the fewest\nparameters compared with other adapters. Moreover, we further find that there\nis also some redundant layers in the Hadamard adapter which can be removed to\nachieve more parameter efficiency with only 0.022\\% parameters.",
            "Humor understanding is an important and challenging research in natural\nlanguage processing. As the popularity of pre-trained language models (PLMs),\nsome recent work makes preliminary attempts to adopt PLMs for humor recognition\nand generation. However, these simple attempts do not substantially answer the\nquestion: {\\em whether PLMs are capable of humor understanding?} This paper is\nthe first work that systematically investigates the humor understanding ability\nof PLMs. For this purpose, a comprehensive framework with three evaluation\nsteps and four evaluation tasks is designed. We also construct a comprehensive\nChinese humor dataset, which can fully meet all the data requirements of the\nproposed evaluation framework. Our empirical study on the Chinese humor dataset\nyields some valuable observations, which are of great guiding value for future\noptimization of PLMs in humor understanding and generation."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have gained widespread adoption in various\nnatural language processing tasks, including question answering and dialogue\nsystems. However, a major drawback of LLMs is the issue of hallucination, where\nthey generate unfaithful or inconsistent content that deviates from the input\nsource, leading to severe consequences. In this paper, we propose a robust\ndiscriminator named RelD to effectively detect hallucination in LLMs' generated\nanswers. RelD is trained on the constructed RelQA, a bilingual\nquestion-answering dialogue dataset along with answers generated by LLMs and a\ncomprehensive set of metrics. Our experimental results demonstrate that the\nproposed RelD successfully detects hallucination in the answers generated by\ndiverse LLMs. Moreover, it performs well in distinguishing hallucination in\nLLMs' generated answers from both in-distribution and out-of-distribution\ndatasets. Additionally, we also conduct a thorough analysis of the types of\nhallucinations that occur and present valuable insights. This research\nsignificantly contributes to the detection of reliable answers generated by\nLLMs and holds noteworthy implications for mitigating hallucination in the\nfuture work.",
        "pos": [
            "Recent years, Pre-trained Language models (PLMs) have swept into various\nfields of artificial intelligence and achieved great success. However, most\nPLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them\nis often expensive and time consuming, and storing them takes up a lot of\nspace. Therefore, it is necessary to adopt a parameter-efficient approach to\nreduce parameters of PLMs in fine-tuning without compromising their performance\nin downstream tasks. In this paper, we design a novel adapter which only acts\non self-attention outputs in PLMs. This adapter adopts element-wise linear\ntransformation using Hadamard product, hence named as Hadamard adapter,\nrequires the fewest parameters compared to previous parameter-efficient\nadapters. In addition, we also summarize some tuning patterns for Hadamard\nadapter shared by various downstream tasks, expecting to provide some guidance\nfor further parameter reduction with shared adapters in future studies. The\nexperiments conducted on the widely-used GLUE benchmark with several SOTA PLMs\nprove that the Hadamard adapter achieves competitive performance with only\n0.033\\% parameters compared with full fine-tuning, and it has the fewest\nparameters compared with other adapters. Moreover, we further find that there\nis also some redundant layers in the Hadamard adapter which can be removed to\nachieve more parameter efficiency with only 0.022\\% parameters."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have gained widespread adoption in various\nnatural language processing tasks, including question answering and dialogue\nsystems. However, a major drawback of LLMs is the issue of hallucination, where\nthey generate unfaithful or inconsistent content that deviates from the input\nsource, leading to severe consequences. In this paper, we propose a robust\ndiscriminator named RelD to effectively detect hallucination in LLMs' generated\nanswers. RelD is trained on the constructed RelQA, a bilingual\nquestion-answering dialogue dataset along with answers generated by LLMs and a\ncomprehensive set of metrics. Our experimental results demonstrate that the\nproposed RelD successfully detects hallucination in the answers generated by\ndiverse LLMs. Moreover, it performs well in distinguishing hallucination in\nLLMs' generated answers from both in-distribution and out-of-distribution\ndatasets. Additionally, we also conduct a thorough analysis of the types of\nhallucinations that occur and present valuable insights. This research\nsignificantly contributes to the detection of reliable answers generated by\nLLMs and holds noteworthy implications for mitigating hallucination in the\nfuture work.",
        "pos": [
            "Recent years, Pre-trained Language models (PLMs) have swept into various\nfields of artificial intelligence and achieved great success. However, most\nPLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them\nis often expensive and time consuming, and storing them takes up a lot of\nspace. Therefore, it is necessary to adopt a parameter-efficient approach to\nreduce parameters of PLMs in fine-tuning without compromising their performance\nin downstream tasks. In this paper, we design a novel adapter which only acts\non self-attention outputs in PLMs. This adapter adopts element-wise linear\ntransformation using Hadamard product, hence named as Hadamard adapter,\nrequires the fewest parameters compared to previous parameter-efficient\nadapters. In addition, we also summarize some tuning patterns for Hadamard\nadapter shared by various downstream tasks, expecting to provide some guidance\nfor further parameter reduction with shared adapters in future studies. The\nexperiments conducted on the widely-used GLUE benchmark with several SOTA PLMs\nprove that the Hadamard adapter achieves competitive performance with only\n0.033\\% parameters compared with full fine-tuning, and it has the fewest\nparameters compared with other adapters. Moreover, we further find that there\nis also some redundant layers in the Hadamard adapter which can be removed to\nachieve more parameter efficiency with only 0.022\\% parameters."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have gained widespread adoption in various\nnatural language processing tasks, including question answering and dialogue\nsystems. However, a major drawback of LLMs is the issue of hallucination, where\nthey generate unfaithful or inconsistent content that deviates from the input\nsource, leading to severe consequences. In this paper, we propose a robust\ndiscriminator named RelD to effectively detect hallucination in LLMs' generated\nanswers. RelD is trained on the constructed RelQA, a bilingual\nquestion-answering dialogue dataset along with answers generated by LLMs and a\ncomprehensive set of metrics. Our experimental results demonstrate that the\nproposed RelD successfully detects hallucination in the answers generated by\ndiverse LLMs. Moreover, it performs well in distinguishing hallucination in\nLLMs' generated answers from both in-distribution and out-of-distribution\ndatasets. Additionally, we also conduct a thorough analysis of the types of\nhallucinations that occur and present valuable insights. This research\nsignificantly contributes to the detection of reliable answers generated by\nLLMs and holds noteworthy implications for mitigating hallucination in the\nfuture work.",
        "pos": [
            "Recent years, Pre-trained Language models (PLMs) have swept into various\nfields of artificial intelligence and achieved great success. However, most\nPLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them\nis often expensive and time consuming, and storing them takes up a lot of\nspace. Therefore, it is necessary to adopt a parameter-efficient approach to\nreduce parameters of PLMs in fine-tuning without compromising their performance\nin downstream tasks. In this paper, we design a novel adapter which only acts\non self-attention outputs in PLMs. This adapter adopts element-wise linear\ntransformation using Hadamard product, hence named as Hadamard adapter,\nrequires the fewest parameters compared to previous parameter-efficient\nadapters. In addition, we also summarize some tuning patterns for Hadamard\nadapter shared by various downstream tasks, expecting to provide some guidance\nfor further parameter reduction with shared adapters in future studies. The\nexperiments conducted on the widely-used GLUE benchmark with several SOTA PLMs\nprove that the Hadamard adapter achieves competitive performance with only\n0.033\\% parameters compared with full fine-tuning, and it has the fewest\nparameters compared with other adapters. Moreover, we further find that there\nis also some redundant layers in the Hadamard adapter which can be removed to\nachieve more parameter efficiency with only 0.022\\% parameters.",
            "Humor understanding is an important and challenging research in natural\nlanguage processing. As the popularity of pre-trained language models (PLMs),\nsome recent work makes preliminary attempts to adopt PLMs for humor recognition\nand generation. However, these simple attempts do not substantially answer the\nquestion: {\\em whether PLMs are capable of humor understanding?} This paper is\nthe first work that systematically investigates the humor understanding ability\nof PLMs. For this purpose, a comprehensive framework with three evaluation\nsteps and four evaluation tasks is designed. We also construct a comprehensive\nChinese humor dataset, which can fully meet all the data requirements of the\nproposed evaluation framework. Our empirical study on the Chinese humor dataset\nyields some valuable observations, which are of great guiding value for future\noptimization of PLMs in humor understanding and generation."
        ],
        "neg": []
    },
    {
        "query": "Large Language Models (LLMs) have gained widespread adoption in various\nnatural language processing tasks, including question answering and dialogue\nsystems. However, a major drawback of LLMs is the issue of hallucination, where\nthey generate unfaithful or inconsistent content that deviates from the input\nsource, leading to severe consequences. In this paper, we propose a robust\ndiscriminator named RelD to effectively detect hallucination in LLMs' generated\nanswers. RelD is trained on the constructed RelQA, a bilingual\nquestion-answering dialogue dataset along with answers generated by LLMs and a\ncomprehensive set of metrics. Our experimental results demonstrate that the\nproposed RelD successfully detects hallucination in the answers generated by\ndiverse LLMs. Moreover, it performs well in distinguishing hallucination in\nLLMs' generated answers from both in-distribution and out-of-distribution\ndatasets. Additionally, we also conduct a thorough analysis of the types of\nhallucinations that occur and present valuable insights. This research\nsignificantly contributes to the detection of reliable answers generated by\nLLMs and holds noteworthy implications for mitigating hallucination in the\nfuture work.",
        "pos": [
            "Recent years, Pre-trained Language models (PLMs) have swept into various\nfields of artificial intelligence and achieved great success. However, most\nPLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them\nis often expensive and time consuming, and storing them takes up a lot of\nspace. Therefore, it is necessary to adopt a parameter-efficient approach to\nreduce parameters of PLMs in fine-tuning without compromising their performance\nin downstream tasks. In this paper, we design a novel adapter which only acts\non self-attention outputs in PLMs. This adapter adopts element-wise linear\ntransformation using Hadamard product, hence named as Hadamard adapter,\nrequires the fewest parameters compared to previous parameter-efficient\nadapters. In addition, we also summarize some tuning patterns for Hadamard\nadapter shared by various downstream tasks, expecting to provide some guidance\nfor further parameter reduction with shared adapters in future studies. The\nexperiments conducted on the widely-used GLUE benchmark with several SOTA PLMs\nprove that the Hadamard adapter achieves competitive performance with only\n0.033\\% parameters compared with full fine-tuning, and it has the fewest\nparameters compared with other adapters. Moreover, we further find that there\nis also some redundant layers in the Hadamard adapter which can be removed to\nachieve more parameter efficiency with only 0.022\\% parameters.",
            "Humor understanding is an important and challenging research in natural\nlanguage processing. As the popularity of pre-trained language models (PLMs),\nsome recent work makes preliminary attempts to adopt PLMs for humor recognition\nand generation. However, these simple attempts do not substantially answer the\nquestion: {\\em whether PLMs are capable of humor understanding?} This paper is\nthe first work that systematically investigates the humor understanding ability\nof PLMs. For this purpose, a comprehensive framework with three evaluation\nsteps and four evaluation tasks is designed. We also construct a comprehensive\nChinese humor dataset, which can fully meet all the data requirements of the\nproposed evaluation framework. Our empirical study on the Chinese humor dataset\nyields some valuable observations, which are of great guiding value for future\noptimization of PLMs in humor understanding and generation."
        ],
        "neg": []
    },
    {
        "query": "Recent years, Pre-trained Language models (PLMs) have swept into various\nfields of artificial intelligence and achieved great success. However, most\nPLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them\nis often expensive and time consuming, and storing them takes up a lot of\nspace. Therefore, it is necessary to adopt a parameter-efficient approach to\nreduce parameters of PLMs in fine-tuning without compromising their performance\nin downstream tasks. In this paper, we design a novel adapter which only acts\non self-attention outputs in PLMs. This adapter adopts element-wise linear\ntransformation using Hadamard product, hence named as Hadamard adapter,\nrequires the fewest parameters compared to previous parameter-efficient\nadapters. In addition, we also summarize some tuning patterns for Hadamard\nadapter shared by various downstream tasks, expecting to provide some guidance\nfor further parameter reduction with shared adapters in future studies. The\nexperiments conducted on the widely-used GLUE benchmark with several SOTA PLMs\nprove that the Hadamard adapter achieves competitive performance with only\n0.033\\% parameters compared with full fine-tuning, and it has the fewest\nparameters compared with other adapters. Moreover, we further find that there\nis also some redundant layers in the Hadamard adapter which can be removed to\nachieve more parameter efficiency with only 0.022\\% parameters.",
        "pos": [
            "Large language models (LLMs) have played a fundamental role in various\nnatural language processing tasks with powerful prompt techniques. However, in\nreal-world applications, there are often similar prompt components for repeated\nqueries, which causes significant computational burdens during inference.\nExisting prompt compression and direct fine-tuning methods aim to tackle these\nchallenges, yet they frequently struggle to strike an optimal balance between\ncost-efficiency and performance effectiveness, especially in complex tasks such\nas NL2Code. In this paper, we propose a novel method namely PromptIntern to\ninternalize the prompt knowledge into model parameters via progressive\nfine-tuning. Our method enables LLMs to emulate the human learning process for\na new task, where detailed templates and examples in a prompt are gradually\ninternalized and phased out progressively as the model grows accustomed to the\ntask. Extensive experiments demonstrate that our method reduces inference\ntokens over 90%, speedups inference by 4.2 times, and saves 88.3% monetary\ncost."
        ],
        "neg": []
    },
    {
        "query": "This paper summarizes the experiments and results of the HYBRINFOX team for\nthe CheckThat! 2024 - Task 1 competition. We propose an approach enriching\nLanguage Models such as RoBERTa with embeddings produced by triples (subject ;\npredicate ; object) extracted from the text sentences. Our analysis of the\ndevelopmental data shows that this method improves the performance of Language\nModels alone. On the evaluation data, its best performance was in English,\nwhere it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On\nthe other languages (Dutch and Arabic), it obtained more mixed results. Future\nresearch tracks are identified toward adapting this processing pipeline to more\nrecent Large Language Models.",
        "pos": [
            "This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity\ndetection of the CLEF 2024 CheckThat! competition. The specificity of the\nmethod is to use a hybrid system, combining a RoBERTa model, fine-tuned for\nsubjectivity detection, a frozen sentence-BERT (sBERT) model to capture\nsemantics, and several scores calculated by the English version of the expert\nsystem VAGO, developed independently of this task to measure vagueness and\nsubjectivity in texts based on the lexicon. In English, the HYBRINFOX method\nranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the\nother languages, the method used a translation step into English, producing\nmore mixed results (ranking 1st in Multilingual and 2nd in Italian over the\nbaseline, but under the baseline in Bulgarian, German, and Arabic). We explain\nthe principles of our hybrid approach, and outline ways in which the method\ncould be improved for other languages besides English."
        ],
        "neg": []
    },
    {
        "query": "This paper summarizes the experiments and results of the HYBRINFOX team for\nthe CheckThat! 2024 - Task 1 competition. We propose an approach enriching\nLanguage Models such as RoBERTa with embeddings produced by triples (subject ;\npredicate ; object) extracted from the text sentences. Our analysis of the\ndevelopmental data shows that this method improves the performance of Language\nModels alone. On the evaluation data, its best performance was in English,\nwhere it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On\nthe other languages (Dutch and Arabic), it obtained more mixed results. Future\nresearch tracks are identified toward adapting this processing pipeline to more\nrecent Large Language Models.",
        "pos": [
            "This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity\ndetection of the CLEF 2024 CheckThat! competition. The specificity of the\nmethod is to use a hybrid system, combining a RoBERTa model, fine-tuned for\nsubjectivity detection, a frozen sentence-BERT (sBERT) model to capture\nsemantics, and several scores calculated by the English version of the expert\nsystem VAGO, developed independently of this task to measure vagueness and\nsubjectivity in texts based on the lexicon. In English, the HYBRINFOX method\nranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the\nother languages, the method used a translation step into English, producing\nmore mixed results (ranking 1st in Multilingual and 2nd in Italian over the\nbaseline, but under the baseline in Bulgarian, German, and Arabic). We explain\nthe principles of our hybrid approach, and outline ways in which the method\ncould be improved for other languages besides English."
        ],
        "neg": []
    },
    {
        "query": "This paper summarizes the experiments and results of the HYBRINFOX team for\nthe CheckThat! 2024 - Task 1 competition. We propose an approach enriching\nLanguage Models such as RoBERTa with embeddings produced by triples (subject ;\npredicate ; object) extracted from the text sentences. Our analysis of the\ndevelopmental data shows that this method improves the performance of Language\nModels alone. On the evaluation data, its best performance was in English,\nwhere it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On\nthe other languages (Dutch and Arabic), it obtained more mixed results. Future\nresearch tracks are identified toward adapting this processing pipeline to more\nrecent Large Language Models.",
        "pos": [
            "This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity\ndetection of the CLEF 2024 CheckThat! competition. The specificity of the\nmethod is to use a hybrid system, combining a RoBERTa model, fine-tuned for\nsubjectivity detection, a frozen sentence-BERT (sBERT) model to capture\nsemantics, and several scores calculated by the English version of the expert\nsystem VAGO, developed independently of this task to measure vagueness and\nsubjectivity in texts based on the lexicon. In English, the HYBRINFOX method\nranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the\nother languages, the method used a translation step into English, producing\nmore mixed results (ranking 1st in Multilingual and 2nd in Italian over the\nbaseline, but under the baseline in Bulgarian, German, and Arabic). We explain\nthe principles of our hybrid approach, and outline ways in which the method\ncould be improved for other languages besides English."
        ],
        "neg": []
    },
    {
        "query": "This paper summarizes the experiments and results of the HYBRINFOX team for\nthe CheckThat! 2024 - Task 1 competition. We propose an approach enriching\nLanguage Models such as RoBERTa with embeddings produced by triples (subject ;\npredicate ; object) extracted from the text sentences. Our analysis of the\ndevelopmental data shows that this method improves the performance of Language\nModels alone. On the evaluation data, its best performance was in English,\nwhere it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On\nthe other languages (Dutch and Arabic), it obtained more mixed results. Future\nresearch tracks are identified toward adapting this processing pipeline to more\nrecent Large Language Models.",
        "pos": [
            "This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity\ndetection of the CLEF 2024 CheckThat! competition. The specificity of the\nmethod is to use a hybrid system, combining a RoBERTa model, fine-tuned for\nsubjectivity detection, a frozen sentence-BERT (sBERT) model to capture\nsemantics, and several scores calculated by the English version of the expert\nsystem VAGO, developed independently of this task to measure vagueness and\nsubjectivity in texts based on the lexicon. In English, the HYBRINFOX method\nranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the\nother languages, the method used a translation step into English, producing\nmore mixed results (ranking 1st in Multilingual and 2nd in Italian over the\nbaseline, but under the baseline in Bulgarian, German, and Arabic). We explain\nthe principles of our hybrid approach, and outline ways in which the method\ncould be improved for other languages besides English."
        ],
        "neg": []
    },
    {
        "query": "This paper summarizes the experiments and results of the HYBRINFOX team for\nthe CheckThat! 2024 - Task 1 competition. We propose an approach enriching\nLanguage Models such as RoBERTa with embeddings produced by triples (subject ;\npredicate ; object) extracted from the text sentences. Our analysis of the\ndevelopmental data shows that this method improves the performance of Language\nModels alone. On the evaluation data, its best performance was in English,\nwhere it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On\nthe other languages (Dutch and Arabic), it obtained more mixed results. Future\nresearch tracks are identified toward adapting this processing pipeline to more\nrecent Large Language Models.",
        "pos": [
            "This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity\ndetection of the CLEF 2024 CheckThat! competition. The specificity of the\nmethod is to use a hybrid system, combining a RoBERTa model, fine-tuned for\nsubjectivity detection, a frozen sentence-BERT (sBERT) model to capture\nsemantics, and several scores calculated by the English version of the expert\nsystem VAGO, developed independently of this task to measure vagueness and\nsubjectivity in texts based on the lexicon. In English, the HYBRINFOX method\nranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the\nother languages, the method used a translation step into English, producing\nmore mixed results (ranking 1st in Multilingual and 2nd in Italian over the\nbaseline, but under the baseline in Bulgarian, German, and Arabic). We explain\nthe principles of our hybrid approach, and outline ways in which the method\ncould be improved for other languages besides English."
        ],
        "neg": []
    },
    {
        "query": "This paper summarizes the experiments and results of the HYBRINFOX team for\nthe CheckThat! 2024 - Task 1 competition. We propose an approach enriching\nLanguage Models such as RoBERTa with embeddings produced by triples (subject ;\npredicate ; object) extracted from the text sentences. Our analysis of the\ndevelopmental data shows that this method improves the performance of Language\nModels alone. On the evaluation data, its best performance was in English,\nwhere it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On\nthe other languages (Dutch and Arabic), it obtained more mixed results. Future\nresearch tracks are identified toward adapting this processing pipeline to more\nrecent Large Language Models.",
        "pos": [
            "This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity\ndetection of the CLEF 2024 CheckThat! competition. The specificity of the\nmethod is to use a hybrid system, combining a RoBERTa model, fine-tuned for\nsubjectivity detection, a frozen sentence-BERT (sBERT) model to capture\nsemantics, and several scores calculated by the English version of the expert\nsystem VAGO, developed independently of this task to measure vagueness and\nsubjectivity in texts based on the lexicon. In English, the HYBRINFOX method\nranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the\nother languages, the method used a translation step into English, producing\nmore mixed results (ranking 1st in Multilingual and 2nd in Italian over the\nbaseline, but under the baseline in Bulgarian, German, and Arabic). We explain\nthe principles of our hybrid approach, and outline ways in which the method\ncould be improved for other languages besides English."
        ],
        "neg": []
    },
    {
        "query": "This paper summarizes the experiments and results of the HYBRINFOX team for\nthe CheckThat! 2024 - Task 1 competition. We propose an approach enriching\nLanguage Models such as RoBERTa with embeddings produced by triples (subject ;\npredicate ; object) extracted from the text sentences. Our analysis of the\ndevelopmental data shows that this method improves the performance of Language\nModels alone. On the evaluation data, its best performance was in English,\nwhere it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On\nthe other languages (Dutch and Arabic), it obtained more mixed results. Future\nresearch tracks are identified toward adapting this processing pipeline to more\nrecent Large Language Models.",
        "pos": [
            "This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity\ndetection of the CLEF 2024 CheckThat! competition. The specificity of the\nmethod is to use a hybrid system, combining a RoBERTa model, fine-tuned for\nsubjectivity detection, a frozen sentence-BERT (sBERT) model to capture\nsemantics, and several scores calculated by the English version of the expert\nsystem VAGO, developed independently of this task to measure vagueness and\nsubjectivity in texts based on the lexicon. In English, the HYBRINFOX method\nranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the\nother languages, the method used a translation step into English, producing\nmore mixed results (ranking 1st in Multilingual and 2nd in Italian over the\nbaseline, but under the baseline in Bulgarian, German, and Arabic). We explain\nthe principles of our hybrid approach, and outline ways in which the method\ncould be improved for other languages besides English."
        ],
        "neg": []
    },
    {
        "query": "Speech accents present a serious challenge to the performance of\nstate-of-the-art end-to-end Automatic Speech Recognition (ASR) systems. Even\nwith self-supervised learning and pre-training of ASR models, accent invariance\nis seldom achieved. In this work, we propose an accent-aware adaptation\ntechnique for self-supervised learning that introduces a trainable set of\naccent-specific codebooks to the self-supervised architecture. These learnable\ncodebooks enable the model to capture accent specific information during\npre-training, that is further refined during ASR finetuning. On the Mozilla\nCommon Voice dataset, our proposed approach outperforms all other\naccent-adaptation approaches on both seen and unseen English accents, with up\nto 9% relative reduction in word error rate (WER).",
        "pos": [
            "Convolutions have become essential in state-of-the-art end-to-end Automatic\nSpeech Recognition~(ASR) systems due to their efficient modelling of local\ncontext. Notably, its use in Conformers has led to superior performance\ncompared to vanilla Transformer-based ASR systems. While components other than\nthe convolution module in the Conformer have been reexamined, altering the\nconvolution module itself has been far less explored. Towards this, we\nintroduce Multi-Convformer that uses multiple convolution kernels within the\nconvolution module of the Conformer in conjunction with gating. This helps in\nimproved modeling of local dependencies at varying granularities. Our model\nrivals existing Conformer variants such as CgMLP and E-Branchformer in\nperformance, while being more parameter efficient. We empirically compare our\napproach with Conformer and its variants across four different datasets and\nthree different modelling paradigms and show up to 8% relative word error\nrate~(WER) improvements."
        ],
        "neg": []
    },
    {
        "query": "Prior research has demonstrated noticeable performance gains through the use\nof probabilistic tokenizations, an approach that involves employing multiple\ntokenizations of the same input string during the training phase of a language\nmodel. Despite these promising findings, modern large language models (LLMs)\nhave yet to be trained using probabilistic tokenizations. Interestingly, while\nthe tokenizers of these contemporary LLMs have the capability to generate\nmultiple tokenizations, this property remains underutilized.\n  In this work, we propose a novel method to leverage the multiple tokenization\ncapabilities of modern LLM tokenizers, aiming to enhance the self-consistency\nof LLMs in reasoning tasks. Our experiments indicate that when utilizing\nprobabilistic tokenizations, LLMs generate logically diverse reasoning paths,\nmoving beyond mere surface-level linguistic diversity.We carefully study\nprobabilistic tokenization and offer insights to explain the self consistency\nimprovements it brings through extensive experimentation on 5 LLM families and\n4 reasoning benchmarks.",
        "pos": [
            "Maximizing the likelihood of the next token is an established, statistically\nsound objective for pre-training language models. In this paper we show that we\ncan train better models faster by pre-aggregating the corpus with a collapsed\n$n$-gram distribution. Previous studies have proposed corpus-level $n$-gram\nstatistics as a regularizer; however, the construction and querying of such\n$n$-grams, if done naively, prove to be costly and significantly impede\ntraining speed, thereby limiting their application in modern large language\nmodel pre-training.\n  We introduce an alternative compact representation of the next token\ndistribution that, in expectation, aligns with the complete $n$-gram\ndistribution while markedly reducing variance across mini-batches compared to\nthe standard next-token loss. Empirically, we demonstrate that both the\n$n$-gram regularized model and our approximation yield substantial improvements\nin model quality and convergence rate compared to existing methods.\nFurthermore, our approximation facilitates scalability of gains to larger\ndatasets and models compared to the straightforward $n$-gram regularization\nmethod."
        ],
        "neg": []
    },
    {
        "query": "Continual Learning (CL) involves fine-tuning pre-trained models with new data\nwhile maintaining the performance on the pre-trained data. This is particularly\nrelevant for expanding multilingual ASR (MASR) capabilities. However, existing\nCL methods, mainly designed for computer vision and reinforcement learning\ntasks, often yield sub-optimal results when directly applied to MASR. We\nhypothesise that this is because CL of the auto-regressive decoder in the MASR\nmodel is difficult. To verify this, we propose four optimizations on the\ndecoder. They include decoder-layer gradient surgery, freezing unused token\nembeddings, suppressing output of newly added tokens, and learning rate\nre-scaling. Our experiments on adapting Whisper to 10 unseen languages from the\nCommon Voice dataset demonstrate that these optimizations reduce the Average\nWord Error Rate (AWER) of pretrained languages from 14.2% to 12.4% compared\nwith Experience Replay, without compromising the AWER of new languages.",
        "pos": [
            "In this paper, we propose reverse inference optimization (RIO), a simple and\neffective method designed to enhance the robustness of\nautoregressive-model-based zero-shot text-to-speech (TTS) systems using\nreinforcement learning from human feedback (RLHF). To assess the quality of\nspeech produced by the TTS system without human annotations, RIO introduces a\nnovel concept termed as reverse inference based on the Bayesian principle,\nwhich suggests that a high-quality generated speech should be able to be used\nas a prompt for subsequent generation using the same TTS model. By leveraging\nreverse inference as the standard to select exemplars used in RLHF from the\nspeech samples generated by the TTS system itself, RIO steers the subsequent\noptimization towards a direction of enhancing the TTS robustness. The RIO\nframework, comprising sampling, automatic annotating, and learning, obviates\nthe need for a reward model or pairwise preference data, and significantly\nimproves the stability of zero-shot TTS performance by reducing the\ndiscrepancies between training and inference conditions. Our experimental\nresults verify that RIO can effectively improve both subjective and objective\nmetrics, including mean opinion scores, word error rates, and speaker\nsimilarity. Remarkably, RIO can also diminish the incidence of bad outputs to\nnearly zero percent, rivalling the robustness when using ground-truth speech as\nthe prompt."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.",
        "pos": [
            "Sign language, essential for the deaf and hard-of-hearing, presents unique\nchallenges in translation and production due to its multimodal nature and the\ninherent ambiguity in mapping sign language motion to spoken language words.\nPrevious methods often rely on gloss annotations, requiring time-intensive\nlabor and specialized expertise in sign language. Gloss-free methods have\nemerged to address these limitations, but they often depend on external sign\nlanguage data or dictionaries, failing to completely eliminate the need for\ngloss annotations. There is a clear demand for a comprehensive approach that\ncan supplant gloss annotations and be utilized for both Sign Language\nTranslation (SLT) and Sign Language Production (SLP). We introduce Universal\nGloss-level Representation (UniGloR), a unified and self-supervised solution\nfor both SLT and SLP, trained on multiple datasets including PHOENIX14T,\nHow2Sign, and NIASL2021. Our results demonstrate UniGloR's effectiveness in the\ntranslation and production tasks. We further report an encouraging result for\nthe Sign Language Recognition (SLR) on previously unseen data. Our study\nsuggests that self-supervised learning can be made in a unified manner, paving\nthe way for innovative and practical applications in future research."
        ],
        "neg": []
    },
    {
        "query": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.",
        "pos": [
            "Sign language, essential for the deaf and hard-of-hearing, presents unique\nchallenges in translation and production due to its multimodal nature and the\ninherent ambiguity in mapping sign language motion to spoken language words.\nPrevious methods often rely on gloss annotations, requiring time-intensive\nlabor and specialized expertise in sign language. Gloss-free methods have\nemerged to address these limitations, but they often depend on external sign\nlanguage data or dictionaries, failing to completely eliminate the need for\ngloss annotations. There is a clear demand for a comprehensive approach that\ncan supplant gloss annotations and be utilized for both Sign Language\nTranslation (SLT) and Sign Language Production (SLP). We introduce Universal\nGloss-level Representation (UniGloR), a unified and self-supervised solution\nfor both SLT and SLP, trained on multiple datasets including PHOENIX14T,\nHow2Sign, and NIASL2021. Our results demonstrate UniGloR's effectiveness in the\ntranslation and production tasks. We further report an encouraging result for\nthe Sign Language Recognition (SLR) on previously unseen data. Our study\nsuggests that self-supervised learning can be made in a unified manner, paving\nthe way for innovative and practical applications in future research."
        ],
        "neg": []
    },
    {
        "query": "Nowadays, topic classification from tweets attracts considerable research\nattention. Different classification systems have been suggested thanks to these\nresearch efforts. Nevertheless, they face major challenges owing to low\nperformance metrics due to the limited amount of labeled data. We propose\nSentence Transformers Fine-tuning (STF), a topic detection system that\nleverages pretrained Sentence Transformers models and fine-tuning to classify\ntopics from tweets accurately. Moreover, extensive parameter sensitivity\nanalyses were conducted to finetune STF parameters for our topic classification\ntask to achieve the best performance results. Experiments on two benchmark\ndatasets demonstrated that (1) the proposed STF can be effectively used for\nclassifying tweet topics and outperforms the latest state-of-the-art\napproaches, and (2) the proposed STF does not require a huge amount of labeled\ntweets to achieve good accuracy, which is a limitation of many state-of-the-art\napproaches. Our main contribution is the achievement of promising results in\ntweet topic classification by applying pretrained sentence transformers\nlanguage models.",
        "pos": [
            "Today, hate speech classification from Arabic tweets has drawn the attention\nof several researchers. Many systems and techniques have been developed to\nresolve this classification task. Nevertheless, two of the major challenges\nfaced in this context are the limited performance and the problem of imbalanced\ndata. In this study, we propose a novel approach that leverages ensemble\nlearning and semi-supervised learning based on previously manually labeled. We\nconducted experiments on a benchmark dataset by classifying Arabic tweets into\n5 distinct classes: non-hate, general hate, racial, religious, or sexism.\nExperimental results show that: (1) ensemble learning based on pre-trained\nlanguage models outperforms existing related works; (2) Our proposed data\naugmentation improves the accuracy results of hate speech detection from Arabic\ntweets and outperforms existing related works. Our main contribution is the\nachievement of encouraging results in Arabic hate speech detection."
        ],
        "neg": []
    },
    {
        "query": "Nowadays, topic classification from tweets attracts considerable research\nattention. Different classification systems have been suggested thanks to these\nresearch efforts. Nevertheless, they face major challenges owing to low\nperformance metrics due to the limited amount of labeled data. We propose\nSentence Transformers Fine-tuning (STF), a topic detection system that\nleverages pretrained Sentence Transformers models and fine-tuning to classify\ntopics from tweets accurately. Moreover, extensive parameter sensitivity\nanalyses were conducted to finetune STF parameters for our topic classification\ntask to achieve the best performance results. Experiments on two benchmark\ndatasets demonstrated that (1) the proposed STF can be effectively used for\nclassifying tweet topics and outperforms the latest state-of-the-art\napproaches, and (2) the proposed STF does not require a huge amount of labeled\ntweets to achieve good accuracy, which is a limitation of many state-of-the-art\napproaches. Our main contribution is the achievement of promising results in\ntweet topic classification by applying pretrained sentence transformers\nlanguage models.",
        "pos": [
            "Today, hate speech classification from Arabic tweets has drawn the attention\nof several researchers. Many systems and techniques have been developed to\nresolve this classification task. Nevertheless, two of the major challenges\nfaced in this context are the limited performance and the problem of imbalanced\ndata. In this study, we propose a novel approach that leverages ensemble\nlearning and semi-supervised learning based on previously manually labeled. We\nconducted experiments on a benchmark dataset by classifying Arabic tweets into\n5 distinct classes: non-hate, general hate, racial, religious, or sexism.\nExperimental results show that: (1) ensemble learning based on pre-trained\nlanguage models outperforms existing related works; (2) Our proposed data\naugmentation improves the accuracy results of hate speech detection from Arabic\ntweets and outperforms existing related works. Our main contribution is the\nachievement of encouraging results in Arabic hate speech detection."
        ],
        "neg": []
    },
    {
        "query": "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.",
        "pos": [
            "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%.\nFinally, we demonstrate the maturity of IncogniText for real-world applications\nby distilling its anonymization capability into a set of LoRA parameters\nassociated with an on-device model.",
            "The latest and most impactful advances in large models stem from their\nincreased size. Unfortunately, this translates into an improved memorization\ncapacity, raising data privacy concerns. Specifically, it has been shown that\nmodels can output personal identifiable information (PII) contained in their\ntraining data. However, reported PIII extraction performance varies widely, and\nthere is no consensus on the optimal methodology to evaluate this risk,\nresulting in underestimating realistic adversaries. In this work, we\nempirically demonstrate that it is possible to improve the extractability of\nPII by over ten-fold by grounding the prefix of the manually constructed\nextraction prompt with in-domain data. Our approach, PII-Compass, achieves\nphone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308\nqueries, respectively, i.e., the phone number of 1 person in 15 is extractable."
        ],
        "neg": []
    },
    {
        "query": "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.",
        "pos": [
            "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%.\nFinally, we demonstrate the maturity of IncogniText for real-world applications\nby distilling its anonymization capability into a set of LoRA parameters\nassociated with an on-device model."
        ],
        "neg": []
    },
    {
        "query": "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.",
        "pos": [
            "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%.\nFinally, we demonstrate the maturity of IncogniText for real-world applications\nby distilling its anonymization capability into a set of LoRA parameters\nassociated with an on-device model.",
            "The latest and most impactful advances in large models stem from their\nincreased size. Unfortunately, this translates into an improved memorization\ncapacity, raising data privacy concerns. Specifically, it has been shown that\nmodels can output personal identifiable information (PII) contained in their\ntraining data. However, reported PIII extraction performance varies widely, and\nthere is no consensus on the optimal methodology to evaluate this risk,\nresulting in underestimating realistic adversaries. In this work, we\nempirically demonstrate that it is possible to improve the extractability of\nPII by over ten-fold by grounding the prefix of the manually constructed\nextraction prompt with in-domain data. Our approach, PII-Compass, achieves\nphone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308\nqueries, respectively, i.e., the phone number of 1 person in 15 is extractable."
        ],
        "neg": []
    },
    {
        "query": "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.",
        "pos": [
            "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%.\nFinally, we demonstrate the maturity of IncogniText for real-world applications\nby distilling its anonymization capability into a set of LoRA parameters\nassociated with an on-device model.",
            "The latest and most impactful advances in large models stem from their\nincreased size. Unfortunately, this translates into an improved memorization\ncapacity, raising data privacy concerns. Specifically, it has been shown that\nmodels can output personal identifiable information (PII) contained in their\ntraining data. However, reported PIII extraction performance varies widely, and\nthere is no consensus on the optimal methodology to evaluate this risk,\nresulting in underestimating realistic adversaries. In this work, we\nempirically demonstrate that it is possible to improve the extractability of\nPII by over ten-fold by grounding the prefix of the manually constructed\nextraction prompt with in-domain data. Our approach, PII-Compass, achieves\nphone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308\nqueries, respectively, i.e., the phone number of 1 person in 15 is extractable."
        ],
        "neg": []
    },
    {
        "query": "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.",
        "pos": [
            "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%.\nFinally, we demonstrate the maturity of IncogniText for real-world applications\nby distilling its anonymization capability into a set of LoRA parameters\nassociated with an on-device model.",
            "The latest and most impactful advances in large models stem from their\nincreased size. Unfortunately, this translates into an improved memorization\ncapacity, raising data privacy concerns. Specifically, it has been shown that\nmodels can output personal identifiable information (PII) contained in their\ntraining data. However, reported PIII extraction performance varies widely, and\nthere is no consensus on the optimal methodology to evaluate this risk,\nresulting in underestimating realistic adversaries. In this work, we\nempirically demonstrate that it is possible to improve the extractability of\nPII by over ten-fold by grounding the prefix of the manually constructed\nextraction prompt with in-domain data. Our approach, PII-Compass, achieves\nphone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308\nqueries, respectively, i.e., the phone number of 1 person in 15 is extractable."
        ],
        "neg": []
    },
    {
        "query": "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains.",
        "pos": [
            "In this paper, we propose reverse inference optimization (RIO), a simple and\neffective method designed to enhance the robustness of\nautoregressive-model-based zero-shot text-to-speech (TTS) systems using\nreinforcement learning from human feedback (RLHF). To assess the quality of\nspeech produced by the TTS system without human annotations, RIO introduces a\nnovel concept termed as reverse inference based on the Bayesian principle,\nwhich suggests that a high-quality generated speech should be able to be used\nas a prompt for subsequent generation using the same TTS model. By leveraging\nreverse inference as the standard to select exemplars used in RLHF from the\nspeech samples generated by the TTS system itself, RIO steers the subsequent\noptimization towards a direction of enhancing the TTS robustness. The RIO\nframework, comprising sampling, automatic annotating, and learning, obviates\nthe need for a reward model or pairwise preference data, and significantly\nimproves the stability of zero-shot TTS performance by reducing the\ndiscrepancies between training and inference conditions. Our experimental\nresults verify that RIO can effectively improve both subjective and objective\nmetrics, including mean opinion scores, word error rates, and speaker\nsimilarity. Remarkably, RIO can also diminish the incidence of bad outputs to\nnearly zero percent, rivalling the robustness when using ground-truth speech as\nthe prompt."
        ],
        "neg": []
    },
    {
        "query": "In this work, we present a comprehensive three-phase study to examine (1) the\neffectiveness of large multimodal models (LMMs) in recognizing cultural\ncontexts; (2) the accuracy of their representations of diverse cultures; and\n(3) their ability to adapt content across cultural boundaries. We first\nintroduce Dalle Street, a large-scale dataset generated by DALL-E 3 and\nvalidated by humans, containing 9,935 images of 67 countries and 10 concept\nclasses. We reveal disparities in cultural understanding at the sub-region\nlevel with both open-weight (LLaVA) and closed-source (GPT-4V) models on Dalle\nStreet and other existing benchmarks. Next, we assess models' deeper culture\nunderstanding by an artifact extraction task and identify over 18,000 artifacts\nassociated with different countries. Finally, we propose a highly composable\npipeline, CultureAdapt, to adapt images from culture to culture. Our findings\nreveal a nuanced picture of the cultural competence of LMMs, highlighting the\nneed to develop culture-aware systems. Dataset and code are available at\nhttps://github.com/iamshnoo/crossroads",
        "pos": [
            "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices\nin their training data and reinforcing societal stereotypes and inequalities.\nOur work explores the potential of the Contact Hypothesis, a concept from\nsocial psychology for debiasing LLMs. We simulate various forms of social\ncontact through LLM prompting to measure their influence on the model's biases,\nmirroring how intergroup interactions can reduce prejudices in social contexts.\nWe create a dataset of 108,000 prompts following a principled approach\nreplicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and\nNousHermes) across 13 social bias dimensions. We propose a unique debiasing\ntechnique, Social Contact Debiasing (SCD), that instruction-tunes these models\nwith unbiased responses to prompts. Our research demonstrates that LLM\nresponses exhibit social biases when subject to contact probing, but more\nimportantly, these biases can be significantly reduced by up to 40% in 1 epoch\nof instruction tuning LLaMA 2 following our SCD strategy. Our code and data are\navailable at https://github.com/chahatraj/breakingbias."
        ],
        "neg": []
    },
    {
        "query": "In this work, we present a comprehensive three-phase study to examine (1) the\neffectiveness of large multimodal models (LMMs) in recognizing cultural\ncontexts; (2) the accuracy of their representations of diverse cultures; and\n(3) their ability to adapt content across cultural boundaries. We first\nintroduce Dalle Street, a large-scale dataset generated by DALL-E 3 and\nvalidated by humans, containing 9,935 images of 67 countries and 10 concept\nclasses. We reveal disparities in cultural understanding at the sub-region\nlevel with both open-weight (LLaVA) and closed-source (GPT-4V) models on Dalle\nStreet and other existing benchmarks. Next, we assess models' deeper culture\nunderstanding by an artifact extraction task and identify over 18,000 artifacts\nassociated with different countries. Finally, we propose a highly composable\npipeline, CultureAdapt, to adapt images from culture to culture. Our findings\nreveal a nuanced picture of the cultural competence of LMMs, highlighting the\nneed to develop culture-aware systems. Dataset and code are available at\nhttps://github.com/iamshnoo/crossroads",
        "pos": [
            "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices\nin their training data and reinforcing societal stereotypes and inequalities.\nOur work explores the potential of the Contact Hypothesis, a concept from\nsocial psychology for debiasing LLMs. We simulate various forms of social\ncontact through LLM prompting to measure their influence on the model's biases,\nmirroring how intergroup interactions can reduce prejudices in social contexts.\nWe create a dataset of 108,000 prompts following a principled approach\nreplicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and\nNousHermes) across 13 social bias dimensions. We propose a unique debiasing\ntechnique, Social Contact Debiasing (SCD), that instruction-tunes these models\nwith unbiased responses to prompts. Our research demonstrates that LLM\nresponses exhibit social biases when subject to contact probing, but more\nimportantly, these biases can be significantly reduced by up to 40% in 1 epoch\nof instruction tuning LLaMA 2 following our SCD strategy. Our code and data are\navailable at https://github.com/chahatraj/breakingbias."
        ],
        "neg": []
    },
    {
        "query": "In this work, we present a comprehensive three-phase study to examine (1) the\neffectiveness of large multimodal models (LMMs) in recognizing cultural\ncontexts; (2) the accuracy of their representations of diverse cultures; and\n(3) their ability to adapt content across cultural boundaries. We first\nintroduce Dalle Street, a large-scale dataset generated by DALL-E 3 and\nvalidated by humans, containing 9,935 images of 67 countries and 10 concept\nclasses. We reveal disparities in cultural understanding at the sub-region\nlevel with both open-weight (LLaVA) and closed-source (GPT-4V) models on Dalle\nStreet and other existing benchmarks. Next, we assess models' deeper culture\nunderstanding by an artifact extraction task and identify over 18,000 artifacts\nassociated with different countries. Finally, we propose a highly composable\npipeline, CultureAdapt, to adapt images from culture to culture. Our findings\nreveal a nuanced picture of the cultural competence of LMMs, highlighting the\nneed to develop culture-aware systems. Dataset and code are available at\nhttps://github.com/iamshnoo/crossroads",
        "pos": [
            "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices\nin their training data and reinforcing societal stereotypes and inequalities.\nOur work explores the potential of the Contact Hypothesis, a concept from\nsocial psychology for debiasing LLMs. We simulate various forms of social\ncontact through LLM prompting to measure their influence on the model's biases,\nmirroring how intergroup interactions can reduce prejudices in social contexts.\nWe create a dataset of 108,000 prompts following a principled approach\nreplicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and\nNousHermes) across 13 social bias dimensions. We propose a unique debiasing\ntechnique, Social Contact Debiasing (SCD), that instruction-tunes these models\nwith unbiased responses to prompts. Our research demonstrates that LLM\nresponses exhibit social biases when subject to contact probing, but more\nimportantly, these biases can be significantly reduced by up to 40% in 1 epoch\nof instruction tuning LLaMA 2 following our SCD strategy. Our code and data are\navailable at https://github.com/chahatraj/breakingbias."
        ],
        "neg": []
    }
]